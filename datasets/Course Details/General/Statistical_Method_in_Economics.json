{
  "course_name": "Statistical Method in Economics",
  "course_description": "No description found.",
  "topics": [
    "Mathematics",
    "Probability and Statistics",
    "Social Science",
    "Economics",
    "Econometrics",
    "Mathematics",
    "Probability and Statistics",
    "Social Science",
    "Economics",
    "Econometrics"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 1.5 hrs / session; 2 sessions / week\n\nRecitations: 1.5 hrs / session; 1 session / week\n\nPrerequisites\n\n18.02 Multivariable Calculus\n\nCourse Description\n\nThis course is divided into two sections, Part I and Part II. Part I, found here, provides an introduction to statistical theory. A brief review of probability will be given mainly as background material, however, it is assumed to be known. Topics include normal distribution, limit theorems, Bayesian concepts, and testing, among others.\n\nTextbooks\n\nCasella, George and Roger Berger.\nStatistical Inference\n. 2nd Edition. Cengage Learning, 2001. ISBN: 9780534243128.\n\nThis book covers all of the material of the course and, in addition, provides many problems for practice as well as excellent references.\n\nCourse Outline\n\nNumbers after each section refer to sections of the text.\n\nSamples and Their Characteristics: Sample vs. Population, Histogram, Sample Moments, Order Statistics (5.1-5.4)\n\nTypes of Convergence and Limit Theorems: LLN, CLT, Slutsky Theorem, Chebyshev's Inequality (5.5)\n\nSummarizing Data: Sufficient Statistics, Minimal Sufficient Statistic, Ancillary Statistics (6.1-6.4)\n\nPoint Estimates and Their Comparison: Unbiasness, MSE, Rao-Cramer Bound, Information Matrix; Asymptotic Behavior: Consistency, Asymptotic Normality, Asymptotic Efficiency (7.3)\n\nMethod of Moments (7.2.1)\n\nMaximum Likelihood (7.2.2)\n\nTesting: Size and Power, UMP Test and Neyman-Pearson Lemma, Wald Test (8.1-8.3)\n\nConfidence Sets Construction (9.1-9.3)\n\nGrading\n\nActivity\n\nPercentage\n\nMidterm\n\n35%\n\nProblem Sets\n\n15%\n\nPart I consists of 50% of the course. The other 50% is determined in Part II.\n\nThere will be a midterm worth 35%. There will be 6 problem sets. A solution to one problem (marked) from each problem set should be handed in to the Teaching Assistant (TA) at the beginning of the lecture or sent to the TA via e-mail before the lecture. This will constitute 15% of the grade. The solution to this problem will be posted after the due date. No late assignments will be accepted. All other problems are for your own study; the solutions to them won't be posted, but will be discussed during the recitations. One problem from the problem sets will appear on the mid-term exam.",
  "files": [
    {
      "category": "Assignment",
      "title": "14.381 Statistical Method in Economics, Problem Set 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-381-statistical-method-in-economics-fall-2018/53b55110a93f8268fa3b445612651e2b_MIT14_381F18_PS1.pdf",
      "content": "14.381: Statistics\nProblem Set 1\nProblems 1-2 and 4-5 are for practice. They will be discussed at recitation.\nOne of the problems from the problem sets will appear on the mid-term exam.\n1. Let X and Y be random variables with finite variances.\n(i) Show that\nmin E (Y - g(X))2 = E (Y - E (Y | X))2 ,\ng(·)\nwhere g(·) ranges over all functions.\n(ii) Assume m(X) = E(Y |X) and write Y = m(X) + e. Show that V ar(Y ) =\nV ar(m(X)) + V ar(e).\n(iii) If E(Y |X = x) = a + bx find E(Y X) as a function of moments of X.\n2. Show that if a sequence of random variables ξi converges in distribution to a\nconstant c, then ξi →\np c.\n3. (The required problem) Let {Xi} be independent Bernoulli (p). Then EXi = p,\n∑\nV ar(Xi) = p(1 - p). Let Yn = 1\nn Xi.\nn\ni=1\n(a) Describe the asymptotic behavior of Yn.\n(b) Show that for p = 2\n1 the estimated variance Yn(1 - Yn) has the following\nlimit behavior\n√ n(Yn(1 - Yn) - p(1 - p)) ⇒ N(0, (1 - 2p)2 p(1 - p)).\n√\n(c) Prove that if (i) σ\nn (ξn - μ) ⇒ N(0, 1) (ii) g is twice continuously differ\n′′ (μ)\nentiable: g ′ (μ) = 0, g\n= 0, then\n′′ (μ)\nn(g(ξn) - g(μ)) ⇒ σ2 g\nχ1\n2 .\n\nNote. You may assume that g has more derivatives, if it simplifies your\nlife. Use Op and op notation wherever possible.\nNote:\nχ2\n1 is a chi-square distribution with 1 degree of freedom.\nLet\n∑\nξ1, . . . , ξp be i.i.d. N(0, 1), then χ2 =\np\nξi\n2 .\np\ni=1\n(d) Show that for p = 1\n[\n]\nn Yn(1 - Yn) -\n⇒- χ1\nCurious fact: Note that Yn(1 - Yn) ≤ 4\n1 , that is, we always underestimate\nthe variance for p = 1\n2 .\n4. (Multivariate limit theorems) Let X = (X1, ..., Xm) ′ and Xn = (Xn1, ..., Xnm) ′\n√\nbe m-dimensional random vectors. Define a norm ∥X∥ =\nX1\n2 + ... + Xm\n2 .\n(a) Show that E∥X∥ < inf if and only if E|Xi| < inf for all i = 1, .., m.\n(b) Define Xn →p X if for any ε > 0, limn→inf P {∥Xn - X∥ > ε} = 0. Show\nXn →p X if and only if Xni →p Xi for all i = 1, ..., m.\n(c) Define Xn ⇒ X if and only if for any non-random m-dimensional vec\ntor λ such that ∥λ∥ = 1 we have λ ′ Xn ⇒ λ ′ X. Formulate and prove\nsome multi-dimensional Central Limit Theorem for independent but not\nidentically distributed random vectors. Hint: use some formulation of one-\ndimensional Linderberg-Fuller's theorem.\n5. Prove the following statements:\n(a) If Xn = Op(n-δ) for some δ > 0 then Xn = op(1);\n(b) If Xn = op(bn) then Xn = Op(bn);\n(c) If Xn = Op(nα) and Yn = Op(nβ ), then XnYn = Op(nα+β) and Xn + Yn =\nOp(max{nα, nβ });\n(d) If Xn = Op(nα) and Yn = op(nβ ), then XnYn = op(nα+β).\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n14.381 Statistical Method in Economics\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Assignment",
      "title": "14.381 Statistical Method in Economics, Problem Set 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-381-statistical-method-in-economics-fall-2018/d3ffb3331f85e81e2e453e6f88a966b0_MIT14_381F18_PS2.pdf",
      "content": "14.381: Statistics\nProblem Set 2\nYou should hand the solution for problem 4. Problems 1-3 are for practice.\nOne of the problems from the problem sets will be on the mid-term exam.\n1. Let X1, X2, . . . , Xn be iid observations. Find minimal sufficient statistics\n(a) f(x | θ) = 2\nθ\nx\n2 , 0 < x < θ, θ > 0;\n{\n}\n-(x-θ)\n-(x-θ)\n(b) f(x | θ) = e\n· exp -e\n, -inf < x < inf, -inf < θ < inf;\n(c) f(x | θ) =\n2!\nθx(1 - θ)2-x , x ∈{0, 1, 2} , 0 ≤ θ ≤ 1.\nx!(2-x)!\n2. Let X1, . . . , Xn be a random sample from a Poisson distribution with parameter\nλ\n-λλj\ne\nP {X = j} =\nj = 0, 1, . . .\nj!\n(a) Find a minimal sufficient statistic.\n(b) Assume that we are interested in estimating probability of a count of zero\nθ = P {X = 0} = exp{-λ}. Find an unbiased estimator of θ. Hint:\nθ = P {X = 0} = EI{X = 0}.\n(c) Is the estimator in (b) a function of a minimal sufficient statistics? Modify\nthe estimator to make sure it is a function of a minimal sufficient statistics,\nwhile it is still unbiased. You may try to do analytical derivation (it can\nbe done here). However, if it is too hard, then explain a Monte-Carlo\nprocedure that you may use instead.\n3. Assume X1, . . . , Xn are iid with mean μ and variance σ2 (both unknown). Let\nus estimate mean by\nn\n∑\nμˆ =\nωiXi\ni=1\nin\n\n(i) Under what condition is μˆ unbiased?\n(ii) Among all unbiased μˆ find the one with the smallest variance.\n(iii) What {ωi} whould lead to the smallest MSE?\n4. (Required problem) Suppose that the random variables Y1, ..., Yn satisfy\nYi = βxi + ei, i = 1, ..., n,\nwhere x1, ..., xn are fixed constants and e1, ..., en are i.i.d. normals with mean 0\nand variance σ2 (variance is unknown).\n(a) Find a two-dimensional sufficient statistic for (β, σ2).\n(b) Find the MLE of β and show that it is unbiased.\n(c) Find the distribution of the MLE of β.\n∑\ni=1 Yi\n(d) Is βˆ 1 = ∑\nn\nn\nan unbiased estimator for β? Find its variance.\ni=1 xi\n(e) Is βˆ 2 = 1 ∑ n\nYi an unbiased estimator for β? Find its variance.\nn\ni=1 xi\n(f) Which of the three estimator βˆ MLE , βˆ 1 and βˆ 2 has the smallest variance?\nHint: you may need the following inequalities. For any numbers a1, .., an\nwe have\n(\n)-1\n∑\n∑\n∑\n∑\n(\nai)2 ≤ n\na 2\nand\n≤\na 2\ni\ni\nn\na\nn\ni\ni\ni\ni\ni\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n14.381 Statistical Method in Economics\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Assignment",
      "title": "14.381 Statistical Method in Economics, Problem Set 3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-381-statistical-method-in-economics-fall-2018/3bf53900c554c727e36e116844c00d43_MIT14_381F18_PS3.pdf",
      "content": "14.381: Statistics\nProblem Set 3\nYou should hand in the solution for problem 4. Problems 1- 3 are for practice.\n1. Let X1, . . . , Xn be a random sample from a Poisson distribution with parameter\nλ\n-λλj\ne\nP {X = j} =\nj = 0, 1, . . .\nj!\n(a) Find the MLE of λ and its asymptotic distribution.\n(b) Assume that we are interested in estimating the probability of a count of\nzero θ = P {X = 0} = exp{-λ}. Find the MLE of θ and its asymptotic\ndistribution. Hint: you may use the delta-method.\n(c) Is the MLE of θ you derived in (b) unbiased? Describe a bootstrap bias-\ncorrection you may do here.\n(d) Now consider a question of variance estimation. Find the asymptotic dis\n∑ n\nσ2\nX )2\ntribution of estimator ˆ = n\ni=1(Xi -\n. You may use the following\nfacts about Poisson distribution: EX = λ, V ar(X) = λ, E(X - EX)3 =\nλ, E(X - EX)4 = λ(1 + 3λ).\n(e) Notice that since V ar(X) = λ, the estimator in (a) is the MLE for variance\nin this model. How would you compare asymptotic efficiency of estimators\nin (a) and (d)?\n2. Assume one observes random variables {Xi,t, t = 1, 2, i = 1, ..., n} which are\nindependent of each other and come from the following model:\nXi,t = μi + εi,t, where εi,t ∼ N(0, σ2)\n\nThe unknown parameter here is (σ2, μ1, μ2, ...., μn). This is the simplest panel\ndata, and you can treat this situation as you observe each entity (i) for two\nperiods t = 1 and t = 2, keeping in mind that each entity has its own unknown\nmean μi. This is also known as fixed effects model.\n(a) Write down the likelihood function.\n(b) Find MLE for the unknown parameters.\n(c) Is the estimator for μi unbiased? Consistent as n →inf?\n(d) Is the MLE for σ2 unbiased? Consistent as n →inf?\n(e) Why does asymptotic MLE theory fail to work in this case?\nThe described problem is known as the incidental parameter problem and is\nextremely important for panel data analysis.\n3. Let X1, . . . , Xn be a sample from the following distributions. In each case find\nthe asymptotic variance of the MLE.\n(a) f(x | θ) = θxθ-1 ,\n0 < x < 1,\n0 < θ < inf.\n{\n}\n-x\n(b) f(x | θ) = 1 exp\n,\n0 < x < inf,\n0 < θ < inf.\nθ\nθ\n4. (Required problem) Suppose that income Y is distributed as a Pareto distribu\ntion: f(y) = αy-(α+1) for 1 ≤ y, with α > 1.\n(a) It is quite common to not observe all incomes, but only those that are\nhigher than some threshold (so-called truncated variables). Assume that\nyou observe only those individuals with an income greater than or equal\nto $9,000, and their income is described by a random variable Y ∗ . How is\nY ∗ distributed?\n(b) Your have a sample of size N drawn from the population of persons with\nincomes greater than or equal to $9,000. What is the MLE of α?\n(c) What is asymptotic distribution of the estimator in (b)?\n\n(d) Suppose you believe that the mean of the Pareto distribution out of which\nyou draw an observation is affected linearly by a variable w, that is,\nE(Yi|wi) = β1 + β2wi . Assume that you have a sample of (Y ∗ , wi) of\ni\nsize N. Explain how you would estimate the parameters β1 and β2. Hint:\ncalculate the mean of the Pareto distribution. How is it related to α?\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n14.381 Statistical Method in Economics\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Assignment",
      "title": "14.381 Statistical Method in Economics, Problem Set 4",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-381-statistical-method-in-economics-fall-2018/2db23e9d53b56fc062d9d88e1d1d9409_MIT14_381F18_PS4.pdf",
      "content": "14.381: Statistics\nProblem Set 4\nProblem 3 is required.\n1. Suppose we have a response variable s that can take k possible values - for\nconvenience labeled 1,2,... k with probability distribution\nk\n∑\nP {s = i} = θi,\nθi = 1\ni=1\nAssume that we have a sample s1, ..., sN of such responses.\n(a) Find a minimal sufficient statistics.\n(b) Let Xi stay for the number of i's observed in the sample (we refer to\nXi's as counts form from the sample). What is the distribution of vector\n(X1, ..., Xk)?\n(c) Find the MLE of (θ1, ..., θk).\n(d) Since the dimensionality of the parameter space is k-1, let us re-parameterize\nthe problem by considering as a new parameter θe only first k - 1 compo\n∑k-1\ne\nnents: θ = (θ1, ..., θk-1). (Remember that θk = 1 -\nθi.) Find out the\nFisher information matrix for θe .\n(e) What is the limit behavior of the MLE estimator in this case? Take a\nsimpler case when k = 3 and calculate the asymptotic variance of ξi =\n√ n(θb i,ML - θi,0), and the asymptotic covariance of ξ1 and ξ2. Could you\nexplain the sign of the covariance?\n2. Let X1, . . . , Xn be iid Poisson (λ).\n(a) Find the UMP test for H0 : λ ≤ λ0 vs. H1 : λ > λ0\n\n(b) Consider the specific case H0 : λ ≤ 1 vs. H1 : λ > 1.\nDetermine the\nsample size n so that the UMP satisfies two conditions:\nPλ=1(reject H0) ≈ 0.05\nPλ=2(reject H0) ≈ 0.9\nHere \"≈\" stays for \" approximately equal\". Please, use the CLT as ap\nproximation device.\n3. (Required problem) There is a theory that people can postpone their death until\nafter an important event. To test the theory, Phillips and King (1988) collected\ndata on deaths around the Jewish holiday Passover. Of 1919 deaths, 922 died\nthe week before the holiday and 997 dies the week after. Think of this as a\nbinomial and test the null hypothesis that θ = 1/2. Report and interpret the\np-value. There are a number of different test you may suggest, any would be\nfine.\n4. Suppose that we have two independent samples: X1, . . . , Xn are iid exponential(θ)\nand Y1, . . . , Ym are iid exponential(μ). Both θ and μ are unknown. We want to\ntest H0 : μ = θ vs H1 : μ = θ. The goal of this problem is to write down a LR\ntest statistic.\n(a) Write down the likelihood function and find the unrestricted ML estimates\nof μ and θ\n(b) Find the restricted ML estimate (via imposing the null)\n(c) Write down LR test statistic\n∑\ni Xi\n(d) Show that it's a function of test statistic ∑\n∑\ni Xi+\nj Yj\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n14.381 Statistical Method in Economics\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Assignment",
      "title": "14.381 Statistical Method in Economics, Problem Set 6",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-381-statistical-method-in-economics-fall-2018/0b6c13b5fcee3d6d74ec4c28b560da8a_MIT14_381F18_PS6.pdf",
      "content": "14.381: Statistics\nn\ni\nn\ni\n∑\nProblem Set 6\nNo due date\nThis problem set is intended for your own practice.\n1. Let X1, . . . , Xn be iid Poisson (λ) and let λ have a Gamma (α, β) distribution\n(the conjugate family for Poisson)\nπ(λ) = λα-1 exp{-λ/β}\nΓ(α)βα\n(a) Find the posterior distribution for λ.\n(b) Calculate posterior mean and variance. Hint: mean of Gamma (α, β) is\nαβ; the variance is αβ2 .\n(c) Discuss whether the prior vanishes asymptotically.\n2(nβ+1)\n(d) Assume that α is an integer. Show that the posterior for\nβ\nλ given\nX is χ2(2(α + ΣXi)).\n(e) Using result of (d), suggest a 95%-credible interval for λ.\n2. Suppose that the random variables Y1, ..., Yn satisfy\nYi = βxi + ei,\ni = 1, ..., n,\nwhere x1, ..., xn are fixed constants and e1, ..., en are i.i.d. normals with mean 0\nand known variance σ2 . The prior for β is normal N(β0, τ 2).\n(a) Find the posterior for β.\n∑\n=1\n=1 x\n(b) The maximum likelihood estimator is the OLS estimator, βˆ OLS\nYixi\n=\n.\ni\nWhat is the variance of the OLS estimator? How is βˆ OLS distributed?\n(c) What is the posterior mean of β? How is it related to βˆ OLS ?\n(d) Construct posterior credible set for β.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n14.381 Statistical Method in Economics\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Assignment",
      "title": "14.381 Statistical Methods in Economics, Problem Set 5",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-381-statistical-method-in-economics-fall-2018/15b44ab3fe6ca93c4fe2a25da55d222e_MIT14_381F18_PS5.pdf",
      "content": "14.381: Statistics\nProblem Set 5\nProblem 1 is required. All other problems are for your own practice.\n1. (Required problem) Suppose that the random variables Y1, ..., Yn satisfy\nyi = βxi + ei,\ni = 1, ..., n,\nwhere x1, ..., xn are fixed constants and e1, ..., en are i.i.d. normals with mean 0\nand unknown variance σ2 . Assume that the hypothesis of interest is H0 : β = 0.\n(a) Write the likelihood function (treating both β and σ2 as unknown). Write\ndown score and information matrix.\n(b) Find the unrestricted maximum likelihood estimator. Write the Wald test\nfor the null hypothesis.\n(c) Solve the restricted maximization problem. Write the Lagrange Multiplier\ntest.\n(d) Write down the LR test.\n(e) Introduce sample correlation between yi and xi:\n∑ n yixi\nrb = √∑\ni=1√∑\n.\nn\nn\ny\nx\ni=1 i\ni=1\ni\nRe-write the Wald, LM and LR statistics as functions of rb2 and n only.\n(f) Notice, that for any x < 1 the following inequality holds: x < -ln(1-x) <\nx\nThis implies some ordering of the statistics discussed above. What is\n1-x .\nit? Apparently, it holds in general for linear hypothesis in OLS models.\n2. Assume that n1 people are given treatment 1 and n2 people are given treatment\n2. Let X1 be the number of people on treatment 1 who respond favorably to\n\nthe treatment and let X2 be the number of people on treatment 2 who respond\nfavorably. Assume that X1 ∼ Binomial(n1, p1), X2 ∼ Binomial(n2, p2) and\nn1 = γn, n2 = (1 - γ)n. Let ψ = p1 - p2.\n(a) Assume that the unknown parameters are (p1, p2), write the likelihood.\nFind the MLE estimator of ψ. Would your answer change if you write\nlikelihood in terms of parameters (ψ, p2)?\n(b) Find the Fisher information matrix I(p1, p2).\n(c) Use the multiparameter delta-method to find the asymptotic variance of\nˆψ assuming that n →inf.\n(d) Construct a Wald confidence set for ψ. Is it asymptotic? Why or why not?\n(e) Test the null hypothesis ψ = 0 using the asymptotic LR test. Describe all\nthe details.\n3. Suppose that X1, . . . , Xn is a random sample from N(μ, σ2) with known σ2 .\nFind a minimum value of n to guarantee that a 0.95 confidence interval for μ\nwill have length no more than σ\n4 .\n4. Assume that X1, . . . , Xn are iid Poisson (λ)\n(a) Construct a Wald type confience set for λ.\n(b) Construct a confidence set for λ by inverting Lagrange multiplier (score)\ntest.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n14.381 Statistical Method in Economics\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Lecture Notes",
      "title": "14.381 Stastical Method in Economics, Lec 4: Sufficient Statistics, Introduction to Estimation",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-381-statistical-method-in-economics-fall-2018/1c6d8a7fe4fbdd22c659af374a830e6a_MIT14_381F18_lec4.pdf",
      "content": "Lecture 4\nSu°cient Statistics. Introduction to Estimation\nSu°cient statistics\nLet f(x|θ) with θ ∈ Θ be some parametric family. Let X = (X1, ..., Xn) be a random sample from distribution\nf(x|θ).\nSuppose we would like to learn parameter value θ from our sample.\nThe concept of su°cient\nstatistic allows us to separate information contained in X into two parts. One part contains all the valuable\ninformation as long as we are concerned with parameter θ, while the other part contains pure noise in the\nsense that this part has no valuable information. Thus, we can ignore the latter part.\nDe nition 1. Statistic T (X) is su°cient for θ if the conditional distribution of X given T (X) does not\ndepend on θ.\nLet T (X) be a su°cient statistic. Consider the pair (X, T (X)). Obviously, (X, T (X)) contains the same\ninformation about θ as X alone, since T (X) is a function of X. But if we know T (X), then X itself has\nno value for us since its conditional distribution given T (X) is independent of θ. Thus, by observing X (in\naddition to T (X)), we cannot say whether one particular value of parameter θ is more likely than another.\nTherefore, once we know T (X), we can discard X completely.\nExample\nLet X = (X1, ..., Xn) be a random sample from N(μ, σ2). Suppose that σ2 is known. Thus, the\nonly parameter is μ (θ = μ). We have already seen that T (X) = Xn ∼ N(μ, σ2/n). Let us calculate the\nconditional distribution of X given T (X) = t. First, note that\nn\nn\n∑\n∑\n(xi - μ)2 - n(xn - μ)2\n=\n(xi - xn + xn - μ)2 - n(xn - μ)2\ni=1\ni=1\nn\nn\n∑\n∑\n=\n(xi - xn)2 + 2\n(xi - xn)(xn - μ)\ni=1\ni=1\nn\n∑\n=\n(xi - xn)2 .\ni=1\n\nTherefore\nfX (x)\nfX|T (X)(x|T (X) = T (x)) =\nfT (T (x))\n∑n\nexp{-\n(xi - μ)2/(2σ2)}/((2π)n/2σn)\ni=1\n=\nexp{-n(xn - μ)2/(2σ2)}/((2π)1/2σ/n1/2)\nn\n∑\n= exp{-\n(xi - xn)2/(2σ2)}/((2π)(n-1)/2σn-1/n1/2),\ni=1\nwhich is independent of μ. We conclude that T (X) = Xn is a su°cient statistic for our parametric family.\nNote, however, that Xn is not su°cient if σ2 is not known.\nFactorization Theorem\nThe Factorization Theorem gives a general approach for how to nd a su°cient statistic:\nTheorem 2 (Factorization Theorem). Let f(x|θ) be the pdf of X. Then T (X) is a su°cient statistic if and\nonly if there exist functions g(t|θ) and h(x) such that f(x|θ) = g(T (x)|θ)h(x).\nProof. Let l(t|θ) be the pdf of T (X).\nSuppose T (X) is a su°cient statistic. Then fX|T (X)(x|T (X) = T (x)) = fX (x|θ)/l(T (x)|θ) does not\ndepend on θ. Denote it by h(x). Then f(x|θ) = l(T (x)|θ)h(x). Denoting l by g yields the result in one\ndirection.\nIn the other direction we will give a sloppy proof. Denote A(x) = {y : T (y) = T (x)}. Then\n∫\n∫\n∫\nl(T (x)|θ) =\nf(y|θ)dy =\ng(T (y)|θ)h(y)dy = g(T (x)|θ)\nh(y)dy.\nA(x)\nA(x)\nA(x)\nSo\nf(x|θ)\nfX|T (X)(x|T (X) = T (x)) =\nl(T (x)|θ)\ng(T (x)|θ)h(x)\n=\n∫\ng(T (x)|θ)\nh(y)dy\nA(x)\nh(x)\n=\n∫\n,\nh(y)dy\nA(x)\nwhich is independent of θ. We conclude that T (X) is a su°cient statistic.\n\nExample\nLet us show how to use the factorization theorem in practice. Let X1, ..., Xn be a random sample\nfrom N(μ, σ2) where both μ and σ2 are unknown, i.e. θ = (μ, σ2). Then\nn\n∑\nf(x|θ) = exp{-\n(xi - μ)2/(2σ2)}/((2π)n/2σn)\ni=1\nn\nn\n∑\n∑\n= exp{-[\nxi - 2μ\nxi + nμ 2]/(2σ2)}/((2π)n/2σn).\ni=1\ni=1\n∑\n∑\nn\nn\nThus, T (X) = (\nXi\n2 ,\nXi) is a su°cient statistic (here h(x) = 1 and g is the whole thing). Note\ni=1\ni=1\nthat in this example we actually have a pair of su°cient statistics. In addition, as we have seen before,\nn\n∑\nf(x|θ) = exp{-[\n(xi - xn)2 + n(xn - μ)2]/(2σ2)}/((2π)n/2σn)\ni=1\n= exp{-[(n - 1)s + n(xn - μ)2]/(2σ2)}/((2π)n/2σn).\nn\nThus, T (X) = (Xn, s ) is another su°cient statistic. Yet another su°cient statistic is T (X) = (X1, ..., Xn).\nn\nNote that Xn is not su°cient in this example.\nExample\nA less trivial example: let X1, ..., Xn be a random sample from U[θ, 1 + θ]. Then f(x|θ) = 1 if\nθ ≤ mini Xi ≤ maxi Xi ≤ 1 + θ and 0 otherwise. In other words, f(x|θ) = I{θ ≤ X(1)}I{1 + θ ≥ X(n)}. So\nT (X) = (X(1), X(n)) is su°cient.\nMinimal Su°cient Statistics\nCould we reduce su°cient statistic T (X) in the previous example even more? Suppose we have two statistics,\n⋆\nsay, T (X) and T ⋆(X).\nWe say that T\nis not bigger than T if there exists some function r such that\nT ⋆(X) = r(T (X)). In other words, we can calculate T ⋆(X) whenever we know T (X). In this case when\nT ∗ changes its value, statistic T must change its value as well. In this sense T ∗ does not give less of an\ninformation reduction than T .\nDe nition 3. A su°cient statistic T ⋆(X) is called minimal if for any su°cient statistic T (X) there exists\nsome function r such that T ⋆(X) = r(T (X)).\nThus, in some sense, the minimal su°cient statistic gives us the greatest data reduction without a loss of\ninformation about parameters. The following theorem gives a characterization of minimal su°cient statistics:\nTheorem 4. Let f(x|θ) be the pdf of X and T (X) be such that, for any x, y, statement {f(x|θ)/f(y|θ) does\nnot depend on θ} is equivalent to statement {T (x) = T (y)}. Then T (X) is minimal su°cient.\nWe will leave this statement unproven here.\nExample\nLet us now go back to the example with X1, ..., Xn ∼ U[θ, 1 + θ].\nRatio f(x|θ)/f(y|θ) is\nindependent of θ if and only if x(1) = y(1) and x(n) = y(n) which is the case if and only if T (x) = T (y).\nTherefore T (X) = (X(1), X(n)) is minimal su°cient.\n\nExample\nLet X1, ..., Xn be a random sample from the Cauchy distribution with parameter θ, i.e. the\n∏n\ndistribution with the pdf f(x|θ) = 1/(π(x - θ)2). Then f(x1, ..., xn|θ) = 1/(πn\n(xi - θ)2). By the\ni=1\ntheorem above, T (X) = (X(1), ..., X(n)) is minimal su°cient.\nEstimators. Properties of estimators.\nAn estimator is a function of the data (statistic). If we have a parametric family with parameter θ, then an\nestimator of θ is usually denoted by θˆ .\nExample\nFor example, if X1, ..., Xn is a random sample from some distribution with mean μ and variance\nσ2 , then sample average μˆ = Xn is an estimator of the population mean, and sample variance σˆ2 = s =\n∑n (Xi - Xn)2/(n - 1) is an estimator of the population variance.\ni=1\n4.1\nUnbiasness\nLet X be our data. Let θˆ = T (X) be an estimator where T is some function.\nWe say that θˆ is unbiased for θ if Eθ[T (X)] = θ for all possible values of θ where Eθ denotes the\nexpectation when θ is the true parameter value. The bias of θˆ is de ned by Bias(θˆ) = Eθ[θˆ] - θ.\nThus, the concept of unbiasness means that we are on average correct. For example, if X is a random\nsample X1, ..., Xn from some distribution with mean μ and variance σ2 , then, as we have already seen,\nE[ˆμ] = μ and E[s2] = σ2 . Thus, sample average and sample variance are unbiased estimators of population\nmean and population variance correspondingly.\n4.2\nE°ciency: MSE\nAnother of the concepts that evaluates performance of estimators is the MSE (Mean Squared Error). By\nde nition, MSE(θˆ) = Eθ[(θˆ - θ)2]. The theorem below gives a useful decomposition for MSE:\nTheorem 5. MSE(θˆ) = Bias2(θˆ) + V (θˆ).\nProof.\nE[(θˆ - θ)2]\n= E[(θˆ - E[θˆ] + E[θˆ] - θ)2]\n= E[(θˆ - E[θˆ])2 + (E[θˆ] - θ)2 + 2(θˆ - E[θˆ])(E[θˆ] - θ)]\n= V (θˆ) + Bias2(θˆ) + 2E[θˆ - E[θˆ]](E[θˆ] - θ)\n= V (θˆ) + Bias2(θˆ).\nEstimators with smaller MSE are considered to be better, or more e°cient.\n\n4.3\nConnection between e°ciency and su°cient statistics\nLet X = (X1, ..., Xn) be a random sample from distribution fθ. Let θˆ = δ(X) be an estimator of θ. Let\nT (X) be a su°cient statistic for θ. As we have seen already, an MSE provides one way to compare the\nquality of di erent estimators. In particular, estimators with smaller MSE are said to be more e°cient. On\nthe other hand, once we know T (X), we can discard X. How do these concepts relate to each other? The\ntheorem below shows that for any estimator θˆ = δ(X), there is another estimator which depends on data X\nˆ\nonly through T (X) and is at least as e°cient as θ:\nTheorem 6 (Rao-Blackwell). In the setting above, de ne φ(T ) = E[δ(X)|T ]. Then θˆ 2 = φ(T (X)) is an\nestimator for θ and MSE(θˆ 2) ≤ MSE(θˆ). In addition, if θˆ is unbiased, then θˆ 2 is unbiased as well.\nProof. To show that θˆ 2 is an estimator, we have to check that it does not depend on θ. Indeed, since T is\nsu°cient for θ, the conditional distribution of X given T is independent of θ. So the conditional distribution\nof δ(X) given T is independent of θ as well. In particular, the conditional expectation E[δ(X)|T ] does not\ndepend on θ. Thus, φ(T (X)) depends only on the data X and θˆ 2 is an estimator.\nMSE(θˆ) = E[(θˆ - θˆ 2 + θˆ 2 - θ)2]\n= E[(θˆ - θˆ 2)2] + 2E[(θˆ - θˆ 2)(θˆ 2 - θ)] + E[(θˆ 2 - θ)2]\n= E[(θˆ - θˆ 2)2] + 2E[(θˆ - θˆ 2)(θˆ 2 - θ)] + MSE(θˆ 2)\n= E[(θˆ - θˆ 2)2] + MSE(θˆ 2),\nwhere in the last line we used\nE[(θˆ - θˆ 2)(θˆ 2 - θ)] = E[(δ(X) - φ(T (X)))(φ(T (X)) - θ)]\n= E[E[(δ(X) - φ(T (X)))(φ(T (X)) - θ)|T ]]\n= E[(φ(T (X)) - θ)E[(δ(X) - φ(T (X)))|T ]]\n= E[(φ(T (X)) - θ) · (E[δ(X)|T ] - φ(T (X)))]\n=\n0,\nsince E[δ(X)|T ] = φ(T (X)).\nTo show the last result, we have\nE[φ(T (X))] = E[E[δ(X)|T ]] = E[δ(X)] = θ\nby the law of iterated expectations.\nExample\nLet X1, ..., Xn be a random sample from Binomial(p, k), i.e.\nP {Xj = m} = (k!/(m!(k -\nm)!))pm(1 - p)k-m for any integer m ≥ 0. Suppose our parameter of interest is the probability of one\n∑n\nsuccess, i.e. θ = P {Xj = 1} = kp(1 - p)k-1 . One possible estimator is θˆ =\nI(Xi = 1)/n. This\ni=1\n\nestimator is unbiased, i.e. E[θˆ] = θ. Let us nd a su°cient statistic. The joint density of the data is\nn\n∏\nf(x1, ..., xn)\n=\n(k!/(xi!(k - xi)!))p xi (1 - p)k-xi\ni=1\n∑\n∑ xi\n= function(x1, ..., xn)p\nxi (1 - p)nk-\n.\n∑n\nThus, T =\ni=1 Xi is su°cient. In fact, it is minimal su°cient.\nUsing the Rao-Blackwell theorem, we can improve θˆ by considering its conditional expectation given T .\nLet φ = E[θˆ|T ] denote this estimator. Then, for any nonnegative integer t,\nn\nn\n∑\n∑\nφ(t)\n= E[\nI(Xi = 1)/n|\nXi = t]\ni=1\ni=1\nn\nn\n∑\n∑\n=\nP {Xi = 1|\nXj = t}/n\ni=1\nj=1\nn\n∑\n= P {X1 = 1|\nXj = t}\nj=1\n∑n\nP {X1 = 1,\nXj = t}\nj=1\n=\n∑n\nP {\nXj = t}\nj=1∑n\nP {X1 = 1,\nXj = t - 1}\nj=2\n=\n∑n\nP {\nXj = t}\nj=1∑n\nP {X1 = 1}P {\nXj = t - 1}\nj=2\n=\nP { ∑n\nXj = t}\nj=1\nkp(1 - p)k-1 · (k(n - 1))!/((t - 1)!(k(n - 1) - (t - 1))!)pt-1(1 - p)k(n-1)-(t-1)\n=\n(kn)!/(t!(kn - t)!)pt(1 - p)kn-t\nk(k(n - 1))!/((t - 1)!(k(n - 1) - (t - 1))!)\n=\n(kn)!/(t!(kn - t)!)\nk(k(n - 1))!(kn - t)!t\n=\n(kn)!(kn - k + 1 - t)!\n∑\n∑\nn\nn\nwhere we used the fact that X1 is independent of (X2, ..., Xn),\nXi ∼ Binomial(kn, p), and\nXi ∼\ni=1\ni=2\nBinomial(k(n - 1), p). So our new estimator is\n∑\n∑\nn\nn\nk(k(n - 1))!(kn -\nXi)!(\nXi)\nˆ\ni=1\ni=1\nθ2 = φ(X1, ..., Xn) =\n∑\n.\nn\n(kn)!(kn - k + 1 -\nXi)!\ni=1\nˆ\nBy the theorem above, it is unbiased and at least as e°cient as θ. The procedure we just applied is sometimes\ninformally referred to as Rao-Blackwellization.\n∑n\nNote on implementation. One may say this is too complicated. We have derived φ(t) = E(θˆ|\nXi =\ni=1\nn\nt) analytically in order to calculate a new estimate θˆ 2 = φ(T ) = φ( ∑\ni=1 Xi), but in real life you may just\ndo this with Monte-Carlo simulations. Note, that we do not need to calculate the whole φ(t) function we\nneed only φ(T ), that is evaluated in one point (the realized value of T ). Note also, that the result does not\ndepend on p, so we are free to choose any p (choosing p close to T/(kn) will give faster calculations).\n\nChoose some p ∈ (0, 1). For b = 1, ..., B repeat the following:\n∑n\n- Draw X∗ , ..., X∗\nX∗\nnb as independent variables from Binomial (p, k), if\n= T , discard this sample.\n1b\ni=1\nib\n∑n\nRepeat drawing samples until you get\nX∗ = T .\ni=1\nib\n∑ n\n- Calculate Yb =\nI(X∗ = 1).\nn\ni=1\nib\n∑B\nThe new estimator is θˆ 2 ≈\nYb. The accuracy is better for larger number of simulation B.\nB\nb=1\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n14.381 Statistical Method in Economics\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Lecture Notes",
      "title": "14.381 Statistical Method in Economics, Lec 1: Distributions and Normal Random Variables",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-381-statistical-method-in-economics-fall-2018/f09db96df409f70d2c874cd5402d19e5_MIT14_381F18_lec1.pdf",
      "content": "Lecture 1\nDistributions and Normal Random Variables\nRandom variables\n1.1\nBasic De nitions\nGiven a random variable X, we de ne a cumulative distribution function (cdf ), FX : R → [0, 1], such that\nFX (t) = P {X ≤ t} for all t ∈ R. Here P {X ≤ t} denotes the probability that X ≤ t. To emphasize that\nrandom variable X has cdf FX , we write X ∼ FX . Note that FX (t) is a nondecreasing function of t.\nThere are 3 types of random variables: discrete, continuous, and mixed.\nDiscrete random variable, X, is characterized by a list of possible values, X = {x1, ..., xn}, and their\nprobabilities, p = {p1, ..., pn}, where pi denotes the probability that X will take value xi, i.e. pi = P {X = xi}\nfor all i = 1, ..., n. Note that p1 + ... + pn = 1 and pi ≥ 0 for all i = 1, ..., n by de nition of probability. Then\n∑\nthe cdf of X is given by FX (t) =\npj .\nj=1,...,n: xj ≤t\nContinuous random variable, Y , is characterized by its probability density function (pdf), fY : R → R,\n∫\n∫ b\n+inf\nsuch that P {a < Y ≤ b} =\nfY (s)ds.\nNote that\nfY (s)ds = 1 and fY (s) ≥ 0 for all s ∈ R by\na\n-inf\n∫ t\nde nition of probability. Then the cdf of Y is given by FY (t) = -inf fY (s)ds. By the Fundamental Theorem\nof Calculus, fY (t) = dFY (t)/dt.\nA random variable is referred to as mixed if it is not discrete and not continuous.\nIf cdf F of some random variable X is strictly increasing and continuous then it has inverse, q(x) =\nF -1(x). It is de ned for all x ∈ (0, 1). Note that\nP {X ≤ q(x)} = P {X ≤ F -1(x)} = F (F -1(x)) = x\nfor all x ∈ (0, 1). Therefore q(x) is called the x-quantile of X. It is such a number that random variable X\ntakes a value smaller or equal to this number with probability x. If F is not strictly increasing or continuous,\nthen we de ne q(x) as a generalized inverse of F , i.e. q(x) = inf{t ∈ R : F (t) ≥ x} for all x ∈ (0, 1). In\nother words, q(x) is a number such that F (q(x) + ε) ≥ x and F (q(x) - ε) < x for any ε > 0. As an exercise,\ncheck that P {X ≤ q(x)} ≥ x.\n\n1.2\nFunctions of Random Variables\nSuppose we have random variable X and function g : R → R. Then we can de ne another random variable\nY = g(X). The cdf of Y can be calculated as follows\nFY (t) = P {Y ≤ t} = P {g(X) ≤ t} = P {X ∈ g -1(-inf, t]},\n-1\nwhere g\nmay be the set-valued inverse of g. The set g-1(-inf, t] consists of all s ∈ R such that g(s) ∈\n(-inf, t], i.e. g(s) ≤ t. If g is strictly increasing and continuously di erentiable then it has strictly increasing\nand continuously di erentiable inverse g-1 de ned on set g(R). In this case P {X ∈ g-1(-inf, t]} = P {X ≤\ng-1(t)} = FX (g-1(t)) for all t ∈ g(R). If, in addition, X is a continuous random variable, then\n(\n)\n(\n)-1\n(\n)-1\ndFY (t)\ndFX (g-1(t))\ndFX (s)\ndg(s)\ndg(s)\nfY (t) =\n=\n=\n= fX (g -1(t))\ndt\ndt\nds\nds\nds\ns=g-1(t)\ns=g-1(t)\ns=g-1(t)\nfor all t ∈ g(R) . If t ∈/ g(R), then fY (t) = 0.\nOne important type of function is a linear transformation. If Y = X - a for some a ∈ R, then\nFY (t) = P {Y ≤ t} = P {X - a ≤ t} = P {X ≤ t + a} = FX (t + a).\nIn particular, if X is continuous, then Y is also continuous with fY (t) = fX (t + a). If Y = bX with b > 0,\nthen\nFY (t) = P {bX ≤ t} = P {X ≤ t/b} = FX (t/b).\nIn particular, if X is continuous, then Y is also continuous with fY (t) = fX (t/b)/b.\n1.3\nExpected Value\nInformally, the expected value of some random variable can be interpreted as its average. Formally, if X is\na random variable and g : R → R is some function, then, by de nition,\n∑\nE[g(X)] =\ng(xi)pi\ni\nfor discrete random variables and\n∫ +inf\nE[g(X)] =\ng(x)fX (x)dx\n-inf\nfor continuous random variables.\nExpected values for some functions g deserve special names:\n- mean: g(x) = x, E[X]\n- second moment: g(x) = x , E[X2]\n- variance: g(x) = (x - E[X])2 , E[(X - E[X])2]\n\nk\n- k-th moment: g(x) = x , E[Xk]\n- k-th central moment: E[(X - EX)k]\nThe variance of random variable X is commonly denoted by V (X).\n1.3.1\nProperties of expectation\n1) For any constant a (non-random), E[a] = a.\n2) The most useful property of an expectation is its linearity: if X and Y are two random variables and\na and b are two constants, then E[aX + bY ] = aE[X] + bE[Y ].\n3)If X is a random variable, then V (X) = E[X2] - (E[X])2 . Indeed,\nV (X)\n= E[(X - E[X])2]\n= E[X2 - 2XE[X] + (E[X])2]\n= E[X2] - E[2XE[X]] + E[(E[X])2]\n= E[X2] - 2E[X]E[X] + (E[X])2\n= E[X2] - (E[X])2 .\n4) If X is a random variable and a is a constant, then V (aX) = a2V (X) and V (X + a) = V (X).\n1.4\nExamples of Random Variables\nDiscrete random variables:\n- Bernoulli(p): random variable X has Bernoully(p) distribution if it takes values from X = {0, 1},\nP {X = 0} = 1 -p and P {X = 1} = p. Its expectation E[X] = 1 · p+0 · (1 - p) = p. Its second moment\nE[X2] = 12 · p + 02 · (1 - p) = p. Thus, its variance V (X) = E[X2] - (E[X])2 = p - p = p(1 - p).\nNotation: X ∼ Bernoulli(p).\n- Poisson(λ): random variable X has a Poisson(λ) distribution if it takes values from X = {0, 1, 2, ...}\nand P {X = j} = e-λλj /j!. As an exercise, check that E[X] = λ and V (X) = λ. Notation: X ∼\nPoisson(λ}.\nContinuous random variables:\n- Uniform(a, b): random variable X has a Uniform(a, b) distribution if its density fX (x) = 1/(b - a) for\nx ∈ (a, b) and fX (x) = 0 otherwise. Notation: X ∼ U(a, b).\n- Normal (μ, σ2): random variable X has a Normal(μ, σ2) distribution if its density fX (x) = exp(-(x -\n√\nμ)2/(2σ2))/(\n2πσ) for all x ∈ R. Its expectation E[X] = μ and its variance V (X) = σ2 . Notation:\nX ∼ N(μ, σ2). As an exercise, check that if X ∼ N(μ, σ2), then Y = (X - μ)/σ ∼ N(0, 1). Y is\nsaid to have a standard normal distribution. It is known that the cdf of N(μ, σ2) is not analytical,\ni.e. it can not be written as a composition of simple functions. However, there exist tables that give\n\nits approximate values. The cdf of a standard normal distribution is commonly denoted by Φ, i.e. if\nY ∼ N(0, 1), then FY (t) = P {Y ≤ t} = Φ(t).\nBivariate (multivariate) distributions\n2.1\nJoint, marginal, conditional\nIf X and Y are two random variables, then FX,Y (x, y) = P {X ≤ x, Y ≤ y} denotes their joint cdf. X and Y\n∫\n∫\nx\ny\nare said to have joint pdf fX,Y if fX,Y (x, y) ≥ 0 for all x, y ∈ R and FX,Y (x, y) =\nfX,Y (s, t)dtds.\n-inf -inf\nUnder some mild regularity conditions (for example, if fX,Y (x, y) is continuous),\n∂2FX,Y (x, y)\nfX,Y (x, y) =\n∂x∂y\nFrom the joint pdf fX,Y one can calculate the pdf of, say, X. Indeed,\n∫\n∫\nx\n+inf\nFX (x) = P {X ≤ x} =\nf(s, t)dtds\n-inf\n-inf\n∫ +inf\nTherefore fX (s) = -inf f(s, t)dt. The pdf of X is called marginal to emphasize that it comes from a joint\npdf of X and Y .\nIf X and Y have a joint pdf, then we can de ne a conditional pdf of Y given X = x (for x such that\nfX (x) > 0): fY |X (y|x) = fX,Y (x, y)/fX (x). Conditional probability is a full characterization of how Y is\ndistributed for any given given X = x. The probability that Y ∈ A for some set A given that X = x can\n∫\nbe calculated as P {Y ∈ A|X = x} =\nfY |X (y|x)dy. In a similar manner we can calculate the conditional\nA\n∫ +inf\nexpectation of Y given X = x: E[Y |X = x] =\nyfY |X (y|x)dy. As an exercise, think how we can de ne\n-inf\nthe conditional distribution of Y given X = x if X and Y are discrete random variables.\nTwo extremely useful properties of a conditional expectation are: for any random variables X and Y ,\n- E[f(X)Y |X = x] = f(x)E[Y |X = x];\n- the law of iterated expectations: E[E[Y |X = x]] = E[Y ].\n2.2\nIndependence\nRandom variables X and Y are said to be independent if fY |X (y|x) = fY (y) for all x ∈ R, i.e. if the marginal\npdf of Y equals conditional pdf Y given X = x for all x ∈ R. Note that fY |X (y|x) = fY (y) if and only if\nfX,Y (x, y) = fX (x)fY (y). If X and Y are independent, then g(X) and f(Y ) are also independent for any\nfunctions g : R → R and f : R → R. In addition, if X and Y are independent, then E[XY ] = E[X]E[Y ].\n\nIndeed,\n∫\n∫\n+inf\n+inf\nE[XY ]\n=\nxyfX,Y (x, y)dxdy\n-inf\n-inf\n∫\n∫\n+inf\n+inf\n=\nxyfX (x)fY (y)dxdy\n-inf\n-inf\n∫\n∫\n+inf\n+inf\n=\nxfX (x)dx\nyfY (y)dy\n-inf\n-inf\n= E[X]E[Y ]\n2.3\nCovariance\nFor any two random variables X and Y we can de ne covariance as\ncov(X, Y ) = E[(X - E[X])(Y - E[Y ])].\nAs an exercise, check that cov(X, Y ) = E[XY ] - E[X]E[Y ].\nCovariances have several useful properties:\n1. cov(X, Y ) = 0 whenever X and Y are independent\n2. cov(aX, bY ) = abcov(X, Y ) for any random variables X and Y and any constants a and b\n3. cov(X + a, Y ) = cov(X, Y ) for any random variables X and Y and any constant a\n4. cov(X, Y ) = cov(Y, X) for any random variables X and Y\n√\n5. |cov(X, Y )| ≤\nV (X)V (Y ) for any random variables X and Y\n6. V (X + Y ) = V (X) + V (Y ) + 2cov(X, Y ) for any random variables X and Y\n∑\n∑\nn\nn\n7. V (\nXi) =\nV (Xi) whenever X1, ..., Xn are independent\ni=1\ni=1\nTo prove property 5, consider random variable X - aY with a = cov(X, Y )/V (Y ). On the one hand, its\nvariance V (X - aY ) ≥ 0. On the other hand,\nV (X - aY )\n= V (X) - 2acov(X, Y ) + a 2V (Y )\n= V (X) - 2(cov(X, Y ))2/V (Y ) + (cov(X, Y )2/V (Y )\nThus, the last expression is nonnegative as well. Multiplying it by V (Y ) yields the result.\n√\nThe correlation of two random variables X and Y is de ned by corr(X, Y ) = cov(X, Y )/\nV (X)V (Y ).\nBy property 5 as before, |corr(X, Y )| ≤ 1. If |corr(X, Y )| = 1, then X and Y are linearly dependent, i.e.\nthere exist constants a and b such that X = a + bY .\n\nNormal Random Variables\nLet us begin with the de nition of a multivariate normal distribution. Let Σ be a positive de nite n × n\nmatrix. Remember that the n × n matrix Σ is positive de nite if aT Σa > 0 for any non-zero n × 1 vector a.\nHere superindex T denotes transposition. Let μ be n × 1 vector. Then X ∼ N(μ, Σ) if X is continuous and\nits pdf is given by\nexp(-(x - μ)T Σ-1(x - μ)/2)\nfX (x) =\n√\n(2π)n/2\ndet(Σ)\nfor any n × 1 vector x.\nA normal distribution has several useful properties:\n1. if X ∼ N(μ, Σ), then Σij = cov(Xi, Xj ) for any i, j = 1, ..., n where X = (X1, ..., Xn)T\n2. if X ∼ N(μ, Σ), then μi = E[Xi] for any i = 1, ..., n\n3. if X ∼ N(μ, Σ), then any subset of components of X is normal as well. In particular, Xi ∼ N(μi, Σii)\n4. if X and Y are uncorrelated normal random variables, then X and Y are independent. As an exercise,\ncheck this statement\n5. if X ∼ N(μX , σ2 ), Y ∼ N(μY , σ2 ), and X and Y are independent, then X +Y ∼ N(μX +μY , σX\n2 +σ2 )\nX\nY\nY\n6. Any linear combination of normals is normal. That is, if X ∼ N(μ, Σ) is an n × 1 dimensional normal\nvector, and A is a xed k × n full-rank matrix with k ≤ n, then Y = AX is a normal k × 1 vector:\nY ∼ N(Aμ, AΣAT ).\n3.1\nConditional distribution\nAnother useful property of a normal distribution is that its conditional distribution is normal as well. If\n[\n]\n([\n] [\n])\nX1\nμ1\nΣ11\nΣ12\nX =\n∼ N\n,\nX2\nμ2\nΣ21\nΣ22\nthen X1|X2 = x2 ∼ N( μ, Σ) with μ = μ1+Σ12Σ-1(x2 -μ2) and\nIf X1 and X2 are both\n\nΣ = Σ11-Σ12Σ-1Σ21.\nrandom variables (as opposed to random vectors), then E[X1|X2 = x2] = μ1 + cov(X1, X2)(x2 - μ2)/V (X2).\nLet us prove the last statement. Let\n[\n]\nσ11\nσ12\nΣ =\nσ12\nσ22\nbe the covariance matrix of 2 × 1 normal random vector X = (X1, X2)T with mean μ = (μ1, μ2)T . Note that\nΣ12 = Σ21\n= σ12 since cov(X1, X2) = cov(X1, X2). From linear algebra, we know that det(Σ) = σ11σ22 -σ2\nand\n[\n]\nσ22\n-σ12\nΣ-1 =\n.\ndet(Σ)\n-σ12\nσ11\n\nThus the pdf of X is\nexp{-[(x1 - μ1)2σ22 + (x2 - μ2)2σ11 - 2(x1 - μ1)(x2 - μ2)σ12]/(2 det(Σ)}\nfX (x1, x2) =\n√\n,\n2π det(Σ)\nand the pdf of X2 is\nexp{-(x2 - μ2)2/(2σ22)}\nfX2 (x2) =\n√\n.\n2πσ22\nNote that\nσ2\nσ11\n12)\nσ11σ22 - (σ11σ22 - σ2\n-\n=\n=\n.\ndet(Σ)\nσ22\ndet(Σ)σ22\ndet(Σ)σ22\nTherefore the conditional pdf of X1, given X2 = x2, is\nfX (x1, x2)\nfX1|X2 (x1|X2 = x2)\n=\nfX2 (x2)\nexp{-[(x1 - μ1)2σ22 + (x2 - μ2)2σ2\n12/σ22 - 2(x1 - μ1)(x2 - μ2)σ12]/(2 det(Σ))}\n=\n√\n√\n2π det(Σ)/σ22\nexp{-[(x1 - μ1)2 + (x2 - μ2)2σ2\n22 - 2(x1 - μ1)(x2 - μ2)σ12/σ22]/(2 det(Σ)/σ22)}\n12/σ2\n√\n2π det(Σ)/σ22\nexp{-[x1 - μ1 - (x2 - μ2)σ12/σ22]2/(2 det(Σ)/σ22)}\n=\n√\n=\n√\n√\n2π det(Σ)/σ22\nexp{-(x1 - μ )2/(2 σ)}\n=\n√\n√\n,\n2π\nσ\nwhere μ = μ1 + (x2 - μ2)σ12/σ22 and σ = det(Σ)/σ22. Note, that the last expression equals the pdf of a\nnormal random variable with mean μ and variance σ yields the result.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n14.381 Statistical Method in Economics\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Lecture Notes",
      "title": "14.381 Statistical Method in Economics, Lec 10: Large Sample Tests",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-381-statistical-method-in-economics-fall-2018/6e7aeb6dd0a1e8daffd560e9dabb8299_MIT14_381F18_lec10.pdf",
      "content": "Lecture 10\nLarge Sample Tests.\nLikelihood Ratio Test\nLet X1, ..., Xn be a random sample from a distribution with pdf f(x|θ) where θ is some one dimensional\n(unknown) parameter. Suppose we want to test the null hypothesis, H0, that θ = θ0 against the alternative\nhypothesis, Ha, that θ = θ0. Assume the same regularity conditions hold as in the MLE theory. Then\nlikelihood ratio test (LRT) statistic is\nL(θ0|x)\nλ(x) =\nL(θˆ ML|x)\nwhere x = (x1, ..., xn) is a realization of the data set and θˆ ML is the ML estimator. Then we have\nTheorem 1. Under the same regularity conditions as the MLE theory and if H0 : θ = θ0 holds, we have:\n-2 log λ(X) ⇒ χ2\n1.\nProof. Denote l(θ|x) = log L(θ|x). By the Taylor theorem, for some θ⋆ between θ0 and θˆ ML,\n-2 log λ(X)\n= -2((l(θ0|X) - l(θˆ ML|X))\n)\n∂l(θˆ ML|X)\n1 ∂2l(θ⋆|X)\n= -2\n(θ0 - θˆ ML) +\n(θ0 - θˆ ML)2\n∂θ\n∂θ2\n∂2l(θ⋆|X)\n= -\n(θ0 - θˆ ML)2\n∂θ2\nsince ∂l(θˆ ML|X)/∂θ = 0 by FOC.\nBy the MLE theory, θˆ ML →p θ0. So, θ⋆ →p θ0. As will be shown in 14.385, by the uniform law of large\nnumbers,\n1 ∂2l(θ⋆|X)\n-n\n∂θ2\n= - n\nn\n∑ ∂2 log f(Xi|θ∗)\n∂θ2\n→p I1(θ0)\ni=1\nwhere I1(θ) denotes the information for one observation, i.e.\nI1(θ) = -Eθ[∂2 log f(Xi|θ)/∂θ2].\nBy the\nSlutsky theorem,\n∂2l(θ⋆|X)\n-nI1(θ0)\n∂θ2\n→p 1\n\nIn addition, from the MLE theory,\n√ n(θˆ ML - θ0) ⇒ N(0, I-1(θ0)).\nSo, by Continuous mapping theorem,\nI(θ0)n(θˆ ML - θ0)2 ⇒ χ2\n1.\nBy the Slutsky theorem,\n∂2l(θ⋆|X)\n-2 log λ(X) = -\nI(θ0)n(θ0 - θˆ ML)2 ⇒ χ1\n2 .\nnI(θ0)\n∂θ2\nIt follows from this theorem that the large sample LR test of level α rejects the null hypothesis if and\nonly if -2 log λ(x) > χ2\n1(1 - α), where χ1\n2(1 - α) denotes 1 - α-quantile of χ2\n1. Note that in nite samples,\nthe size of this test may be greater than α but as the sample size increases, the size will converge to α.\n1.1\nFormulation for multi-dimensional case\nIn general, let θ be a multidimensional parameter (say dimensionality is k). Suppose that the null hypothesis\nΘ0 can be written in the form {θ ∈ Θ : g1(θ) = 0, ..., gp(θ) = 0} where g1, ..., gp denote some nonlinear\nfunctions of θ.\nEquations g1(θ) = 0, ..., gp(θ) = 0 are called restrictions of the model (and k ≥ p, if\nk > p then it is a composite hypothesis, if k = p then simple). Assume that restrictions are jointly linear\nindependent in the sense that we cannot drop any subset of restrictions without changing set Θ0. Then,\nunder some regularity conditions (mainly smoothness of g1, ..., gp),\n(\n)\n-2 log λ(X) = 2 max l(θ|X) - max l(θ|X) ⇒ χ2\np\nθ∈Θ\nθ∈Θ0\nunder the assumption that the null hypothesis hold. So, large sample LR test of level α rejects the null\nhypothesis if and only if -2 log λ(X) > χ2(1 - α). Often, we denote LR = -2 log λ(X). LR is called the\np\nlikelihood ratio statistic. Let us denote θˆ 0 = arg maxθ∈Θ0 l(θ|X) to be the restricted estimate (estimates\nassuming the null is true), then\nLR = 2(l(θˆ ML|X) - l(θˆ 0|X))\nExample Let X1, ..., Xn be a random sample from a Poisson(λ) distribution. Recall that the pmf of the\nPoisson(λ) distribution is f(x|λ) = λxe-λ/x! for x = 0, 1, 2, .... Suppose we want to test the null hypothesis,\nH0, that λ = λ0 = 6 against the alternative hypothesis, Ha, that λ =\nλ0. Suppose we observe Xn = 5 while\nour sample size n = 100. Let us derive the result of the large sample LR test. Likelihood function is\n∑\nXi e-nλ\ni=1\nλ\nn\nL(λ|X) =\n∏n\ni=1 Xi!\n\nwhere X = (X1, ..., Xn). The log-likelihood is\nn\nn\n∑\n∏\nl(λ|X) =\nXi log λ - nλ - log\nXi!\ni=1\ni=1\nSo, the ML estimator λˆ ML solves\nn\n∑\nXi/λˆ ML - n = 0\ni=1\nor, equivalently,\nˆλML = Xn\nSo, LRT statistic is\n∑\ni=1 XI -n(λ0-λˆ ML)\nλ(x) = (λ0/λˆ ML)\nn\ne\n.\nThen\nLR = -2 log λ(x)\n(\n)\nn\n∑\n= -2\nXi log(λ0/λˆ ML) - n(λ0 - λˆ ML)\ni=1\n= -2n(Xn log(λ0/Xn) - λ0 + Xn)\n= -200(5 log(6/5) - 6 + 5)\n≈ 17.6,\nwhile χ2\n1(0.95) = 3.98. So large sample LR test rejects the null hypothesis.\nLarge Sample Tests: Wald\n2.1\nSimplistic 1-dimensional case\nOnce we know the asymptotic distribution of some statistic, say, δ(X1, ..., Xn), we can construct a large\nsample test based on this asymptotic distribution. Suppose we can show that\n√ n(δ(X1, ..., Xn) - τ) ⇒ N(0, σ2)\nwhere τ is some 1-dimensional parameter. Suppose we have a consistent estimator σˆ2 of σ2 , i.e. σˆ2 →p σ2 .\nBy the Slutsky theorem,\n√ n(δ(X1, ..., Xn) - τ)/σˆ ⇒ N(0, 1)\nSuppose we want to test the null hypothesis, H0, that τ = τ0 against the two-sided alternative. Under the\nnull hypothesis,\n√ n(δ(X1, ..., Xn) - τ0)/σˆ ⇒ N(0, 1).\n\n√ δ(X1,...,Xn)-τ0\nSo, one test of level α will be to reject the null hypothesis if t =\nn\nis smaller than zα/2 or\nσˆ\nlarger than z1-α/2. Which is equivalent to calculating statistic\n(\n)2\nδ(X) - τ0\nW = n\nσˆ\nand comparing it to 1 - α quantile of χ2\n1 distribution.\n2.2\nMulti-dimensional case\nNotice that this logic could be easily extended to multi-dimensional parameters.\nAssume that τ is p-\ndimensional and\n√ n(δ(X) - τ ) ⇒ N(0, Σ),\nand we can construct a consistent estimate Σˆ of the covariance matrix Σ, that is Σˆ →p Σ, then\nW = n(δ(X) - τ0) ′ Σˆ -1(δ(X) - τ0) ⇒ χ2\np\nif H0 : τ = τ0.\n2.3\nSpecial case: 1-dimensional MLE\nWe can specialize this to the MLE case, for example, and see how this test compares to the LR introduced\nbefore.\nLet θˆ ML be the ML estimator of 1-dimensional parameter θ ∈ R.\nWe know that, under some\nregularity conditions,\n√ n(θˆ ML - θ) ⇒ N(0, I-1(θ))\nUnder some regularity conditions, I-1(θ) may be consistently estimated by I-1(θˆ ML). Suppose that our\nnull hypothesis is H0 : θ = θ0. Then, under the null hypothesis,\n√ nI1/2(θˆ ML)(θˆ ML - θ0) ⇒ N(0, 1)\nUnder the null hypothesis\nW = nI1(θˆ ML)(θˆ ML - θ0)2 ⇒ χ1\n2 .\nRecall that LR-statistic is given by\n(\n)\n1 ∂2l(θ⋆|X)\nLR = n(θˆ ML - θ0)2 - n\n∂θ2\nwhere θ⋆ is between θ0 and θˆ ML. As in the case of Wald statistic, under the null hypothesis,\nLR ⇒ χ2\n\nMoreover,\nW - LR →p 0\nsince I(θˆ ML) →p I(θ0) and -(1/n)∂2l(θ⋆|X)/∂θ2 →p I(θ0). Thus, LR and Wald statistics are asymptotically\nequivalent. They are di erent in nite samples though. In particular, it is known that W ≥ LR in the case\nof normal likelihood.\nAn advantage of the Wald statistic in comparison with the LR statistic is that it only includes calculations\nbased on the unrestricted estimator θˆ ML. On the other hand, in order to calculate the Wald statistic, we\nhave to estimate the information matrix.\nExample (cont.) Let us calculate the Wald statistic in our example with a random sample from the\nPoisson(λ) distribution. The log-likelihood is\nn\nn\n∑\n∏\nl(λ|X) =\nXi log λ - nλ - log\nXi!\ni=1\ni=1\nSo,\nn\n∑\n∂l(λ|X)/∂λ =\nXi/λ - n\ni=1\nand\nn\n∑\n∂2l(λ|X)/∂λ2 = -\nXi/λ2 .\ni=1\nThus,\n[\n]\n1 ∂2l(λ)\nI1(λ) = -E\n=\n.\nn ∂λ2\nλ\nSo, the Wald statistic is\nW = n(λˆ - λ0)2/λˆ = 100 · (5 - 6)2 · (1/5) = 20.\nSo, the test based on the Wald statistic rejects the null hypothesis with an even smaller p-value than the\ntest based on the LR statistic.\nScore Test\n3.1\n1-dimensional case\nRecall that the score is de ned by\nn\n∑\n∂l\n∂ log L\n∂ log f(Xi|θ)\nS(θ) =\n(θ|X) =\n(θ|X) =\n.\n∂θ\n∂θ\n∂θ\ni=1\n\nBy the rst order condition for the ML estimator, S(θˆ ML) = 0. By the rst information equality,\nn\n[\n]\n∑\n∂ log f(Xi|θ0)\nE[S(θ0)] =\nE\n= 0.\n∂θ\ni=1\nBy de nition of Fisher information,\n[(\n)2]\n∂ log f(Xi|θ)\nE\n= I1(θ0).\n∂θ\nSo, by the Central limit theorem, under the null hypothesis (if θ is 1-dimensional)\nS(θ0)\n√ √\n⇒ N(0, 1).\nn\nI1(θ0)\nBy the continuous mapping theorem,\nLM = S(θ0)2/(nI1(θ0)) ⇒ χ2\n1.\nThe LM is called Lagrange Multiplier (LM) statistic. Let us show where the name comes from. Consider\nthe constrained optimization problem log L(θ|x) → max s.t. θ = θ0. The lagrangian is\nH = log L(θ|x) - λ(θ - θ0).\nThe FOC is\nS(θ0) = λ.\nSo, indeed, the score is connected to the lagrange multiplier.\nComparison to LR and Wald Let us show that LM - LR →p 0. By the Taylor's expansion,\n∂2l\nS(θ0) = S(θ0) - S(θˆ ML) =\n(θ ⋆|X)(θ0 - θˆ ML),\n∂θ2\nwhere θ⋆ is between θ0 and θˆ ML. As before, -(1/n)∂2l(θ⋆|X)/∂θ2 →p I1(θ0). By the Slutsky theorem,\n(\n)2\n1 ∂2l\nLM = n\n(θ ⋆|X)\n(θ0 - θˆ ML)2/(nI1(θ0)) = nI1(θ0)(θ0 - θˆ ML)2(1 + op(1))\nn ∂θ2\nThus, we have shown that LR, Wald, and LM statistics are all asymptotically equivalent under the null\nhypothesis. However, they di er in nite samples. For example, in the case of normal likelihood, we have\nLM ≤ LR ≤ W .\n\n3.2\nMulti-dimensional case\nAssume that the unknown parameter θ is k-dimensional, while the null hypothesis is imposing p-dimensional\n∂l\nrestriction Θ0 = {θ ∈ Θ : g1(θ) = 0, ..., gp(θ) = 0}. Score function is k × 1- vector function S(θ) =\n(θ|X).\n∂θ\nDenote θˆ 0 to be restricted estimator: θˆ 0 = arg maxθ∈Θ0 l(θ|X). Then\nLM = 1 S(θˆ 0)I1(θˆ 0)-1S(θˆ 0) ⇒ χ2\np\nn\nif the null holds.\nAn advantage of the LM statistic is that it only includes calculations based on the restricted estimator\nθ0. On the other hand, in order to nd the LM statistic, we have to estimate Fisher information.\nExample (cont.) Let us calculate the LM statistic in our example with a random sample from Poisson(λ)\ndistribution. We have\nn\n∑\nS(λ0) =\nXi/λ0 - n = 500/6 - 100 = -100/6\ni=1\nand I(λ0) = 1/λ0 = 1/6. So,\n(\n)2\nS(λ0)2\nLM =\n=\n·\n· 6 =\n≈ 17\nnI(λ0)\nGeneralizations and Summary\nLet x = (X1, ..., Xn) be a random sample from distribution f(X|θ) with θ ∈ Θ. Suppose we want to test\nthe null hypothesis, H0, that θ ∈ Θ0 against the alternative hypothesis, Ha, that θ ∈/ Θ0. Let θˆ 0 be a\nrestricted estimator, i.e. θˆ 0 solves maxθ∈Θ0 L(θ|x), and θˆ ML an unrestricted (ML) estimator, i.e. θˆ ML solves\nmaxθ∈Θ L(θ|x). Assume for simplicity that the null can be formulated as g(θ) = 0, where g is p-dimensional\nfunction. Then, under the null hypothesis,\nLR = 2(l(θˆ ML|X) - l(θˆ 0|X)) ⇒ χ2\np\nW = (g(θˆ ML) - 0)Σˆ -1(g(θˆ ML) - 0) ⇒ χ2\np\nLM = S(θˆ 0)I-1(θˆ 0)S(θˆ 0) ⇒ χ2\nn\np\n(\n)′\n(\n)\n∂g\n∂g\nwhere Σˆ =\n(θˆ ML) I1\n-1(θˆ ML)\n∂θ (θˆ ML)\nis a natural delta-method inspired estimate of asymptotic\n∂θ\nvariance. Under a proper regularity conditions all of these tests are asymptotically equivalent to each other.\nNotice, that LR and LM are invariant to formulation of the null hypothesis, while Wald is not.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n14.381 Statistical Method in Economics\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Lecture Notes",
      "title": "14.381 Statistical Method in Economics, Lec 11: Confidence Sets",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-381-statistical-method-in-economics-fall-2018/6a7fdee0c759b8ac985bf28a553acb24_MIT14_381F18_lec11.pdf",
      "content": "Lecture 11\nCon dence Sets\nIntroduction\nSo far, we have been considering point estimation. In this lecture, we will study interval estimation. Let\nX denote our data. Let θ ∈ R be our parameter of interest. Our task is to construct a data-dependent\ninterval [l(X), r(X)] so that it contains θ with large probability. One possibility is to set l(X) = -inf and\nr(X) = +inf. Such an interval will contain θ with probability 1. Of course, the problem with this interval\nis that it is too long. So, we want to construct an interval that will be shorter. More generally, instead\nof intervals, we can consider con dence set C(X) ⊂ R such that it contains θ with large probability. The\nconcept of con dence sets can be also applied to any set of possible parameter values Θ, not just for R.\nLet us introduce the basic concepts related to con dence sets.\nDe nition 1. Coverage probability of the set C(X) ⊂ Θ is the probability (under the assumption that the\ntrue value is θ) that con dence set C(X) contains θ, i.e. Coverage Probability(θ) = Pθ{θ ∈ C(X)}.\nOf course, in practice, we are interested in con dence sets that contain the true parameter value with\nlarge probability uniformly over the set of possible parameters values.\nDe nition 2. Con dence level is the minimum of coverage probabilities over the set of possible parameter\nvalues, i.e. Con dence Level = infθ∈Θ Pθ{θ ∈ C(X)}. We say that con dence set C(X) has con dence level\nα if infθ∈Θ Pθ{θ ∈ C(X)} ≥ α.\nLet us consider how we can construct con dence sets.\nTest Inversion\nFor each possible parameter value θ0 ∈ Θ, consider the problem of testing the null hypothesis, H0 : θ = θ0\nagainst the alternative, Ha : θ =\nθ0. Suppose that for each such hypothesis we have a test of size α. Then\nthe con dence set C(X) = {θ0 ∈ Θ : the null hypothesis that θ = θ0 is not rejected} is of con dence level\n1 - α. Indeed, suppose that the true value of the parameter is θ0. Since the test of θ = θ0 against θ = θ0 has\nlevel α by construction, Pθ0 {the test rejects θ = θ0} ≤ α. So, with probability of at least 1 - α, θ0 ∈ C(X).\nIn other words, Pθ0 {θ0 ∈ C(X)} ≥ 1 - α. The same holds for all θ0 ∈ Θ. So, infθ∈Θ Pθ{θ ∈ C(X)} ≥ 1 - α.\nThis procedure is known as test inversion.\n\nWe may nd that the con dence set construction is a dual problem for testing. Note that if we have a\nway to construct a con dence set, we can construct a test for any hypothesis H0 : θ = θ0. Indeed, once we\nhave a con dence set C(X) of level 1 - α, we can form a test of the null hypothesis, H0, that θ = θ0 against\nthe alternative, Ha, that θ = θ0 by accepting the null hypothesis if and only if θ0 ∈ C(X). This test will be\nof size α.\nExample 1\nLet X1, ..., Xn be a random sample from a distribution with two nite moments. Let us use\ntest inversion to construct a con dence set for μ = EXi of (asymptotic) level 1 - α. Let us consider the\nproblem of testing the null hypothesis, H0 : μ = μ0 against the alternative, Ha : μ = μ0.\nUnder the\n√ X-μ0\nnull hypothesis we have the following asymptotic statement: t(μ0) =\n√ n\ns\n⇒N(0, 1). One possible\nn(X-μ0)\ntest of size α is to accept the null hypothesis if and only if zα/2 ≤\n≤ z1-α/2, where zα is the\ns\nquantile of the standard normal distribution. This test will accept the null hypothesis μ = μ0 if and only if\n[\n]\ns\ns\ns\ns\nX - z1-α/2 √ ≤ μ0 ≤ X - zα/2 √ . So, the con dence set is\nX - z1-α/2 √ , X - zα/2 √\n. Note that we\nn\nn\nn\nn\nactually end up with an interval in this example.\nExample 2\nLet X1, ..., Xn be a random sample from the distribution N(μ, σ2). Let us use test inversion\nto construct a con dence set for σ2 of level 1 - α. Consider the problem of testing the null hypothesis,\nH0 : σ2 = σ2 against the alternative, Ha : σ2 = σ0\n2 . Under the null hypothesis, (n - 1)s2/σ2 ∼ χ2(n - 1).\nOne possible test of size α is to accept the null hypothesis if and only if\nχ2\n(n - 1) ≤ (n - 1)s 2/σ0\n2 ≤ χ2\n(n - 1).\nα/2\n1-α/2\nThis test will accept the null hypothesis σ2 = σ2 if and only if\n(n - 1)s 2/χ2\n(n - 1) ≤ σ0\n2 ≤ (n - 1)s 2/χ2\n(n - 1).\n1-α/2\nα/2\nSo, the con dence set is\n[\n]\n(n - 1)s\n(n - 1)s\n,\n.\nχ2\n(n - 1) χ2\n(n - 1)\n1-α/2\nα/2\nThis is not the shortest interval, apparently we may cut o unequal tails, but tails whose probability would\nsum up to α: say\n[\n]\n(n - 1)s\n(n - 1)s\nχ2\n, χ2\n(n - 1)\n(n - 1)\n1-α+δ\nδ\nfor 0 < δ < α is also a valid con dence set, we may try to optimize over δ to nd the shortest interval.\nIn general, if we can nd a (asymptotically) pivotal quantity Q = q(X1, ..., Xn, θ0) such that distribution\nof Q under the null hypothesis θ = θ0 does not depend on the choice of θ0 (in nite samples or asymptotically),\nthen we can use Q for testing and con dence set construction. Indeed, since distribution of Q is independent\nof the true parameter value, we can nd numbers a and b such that Pθ0 {a ≤ Q ≤ b} = 1 - α for all θ0 ∈ Θ.\nThen one possible test is to accept the null hypothesis that θ = θ0 if and only if a ≤ q(X1, ..., Xn, θ0) ≤ b.\n\nThe con dence set will consists of all parameter values θ0 which are accepted:\nC(X) = {θ0 : a ≤ q(X1, ..., Xn, θ0) ≤ b}.\nIf q(X, θ0) considered as a function of θ0 is continuous monotonic and there exists a unique inverse, then we\nwould have C(X) as an interval with end points q-1(X1, ..., Xn, a) and q-1(X1, ..., Xn, b). Though in many\ninteresting cases inversion of a test will not lead to an interval.\nPratt's Theorem\nInformally, the theorem states that if we use a uniformly-most-powerful test (UMP) for the con dence set\nconstruction, the expected length of the con dence set will be the shortest among all con dence sets of a\ngiven level.\nTheorem 3. Let X ∼ f(x|θ) be our data. Let C(X) be our con dence set for θ. Then, under some regularity\nconditions, for any θ0,\n∫\nEθ0 [length of C(X)] =\nPθ0 {θ ∈ C(X)}dθ.\nMoreover, if C(X) is constructed by inverting a UMP test of size α, then C(X) has the shortest expected\nlength among all con dence sets of level 1 - α for any θ0.\nProof. The rst result follows from\n∫\nEθ0 [length of C(X)] = Eθ0 [\nI{θ ∈ C(X)}dθ]\nθ\n∫∫\n=\nI{θ ∈ C(X)}dθf(x|θ0)dx\nx\nθ\n∫∫\n=\nI{θ ∈ C(X)}f(x|θ0)dxdθ\nθ\nx\n∫\n=\nPθ0 {θ ∈ C(X)}dθ.\n∫\n∫\nNote that\nPθ0 {θ ∈ C(X)}dθ =\nPθ0 {θ ∈ C(X)}dθ and, for any θ = θ0, Pθ0 {θ ∈ C(X)} equals 1 minus\nθ=θ0\nthe power of the test based on con dence set C(X). So, if C (X) denotes the con dence set constructed by\ninverting a UMP test,\nPθ0 {θ ∈ C(X)} ≥ Pθ0 {θ ∈ C (X)}\nand\n∫\n∫\nPθ0 {θ ∈ C(X)}dθ ≥\nPθ0 {θ ∈ C (X)}dθ.\nCombining this inequality with the rst result yields the second result of Pratt's theorem.\nExample 3\nLet X1, ..., Xn be a random sample from distribution N(μ, σ2). We have already seen that\nthe UMP test of the null hypothesis, H0, that μ = μ0 against the alternative, Ha, that μ = μ0 accepts the\n\n√\nnull hypothesis if and only if |(Xn - μ)/\ns2/n| ≤ t1-α/2(n - 1). So, the con dence interval with shortest\nexpected length is\n[\n]\ns\ns\nXn -√ t1-α/2, Xn + √ t1-α/2 .\nn\nn\nAsymptotic Theory for Interval Construction\nLet X1, ..., Xn be a random sample from distribution f(x|θ) with θ ∈ Θ. Under some regularity conditions,\n√ n(θˆ ML - θ) ⇒ N(0, I-1(θ)).\nFor any function h : Θ → R, under some regularity conditions, by the delta-method,\n√ n(h(θˆ ML) - h(θ)) ⇒ N(0, (h ′ (θ))2I-1(θ)).\nWe can consistently estimate (h ′ (θ))2I-1(θ) by n(h ′ (θˆ ML))2(-∂2ln(θˆ ML)/∂θ2)-1 . Denote\nVˆ (h(θˆ ML)) = (h ′ (θˆ ML)2(-∂2ln(θˆ ML)/∂θ2)-1\nBy the Slutsky theorem,\nh(θˆ ML) - h(θ)\n√\n⇒ N(0, 1).\nVˆ (h(θˆ ML))\nSo, we can construct a con dence interval for h(θ) as\n[\n√\n√\n]\nˆ\nˆ\nh(θˆ ML) + zα/2\nV (h(θˆ ML)), h(θˆ ML) + z1-α/2\nV (h(θˆ ML)) .\nNote that this con dence set is essentially constructed based on the Wald statistic.\nExample 4\nLet X1, ..., Xn be a random sample from distribution Bernoulli(p).\nSuppose we want to\nconstruct a con dence set for h(p) = p/(1 - p). Denote pˆ = Xn. Then\n√ n(ˆp - p) ⇒ N(0, p(1 - p)).\nIn addition,\n(1 - p) + p\nh ′ (p) =\n=\n.\n(1 - p)2\n(1 - p)2\nBy delta-method,\n√ n(h(ˆp) - h(p)) ⇒ N(0, p/(1 - p)3).\nSo, Vˆ (h(ˆp)) = p/\nˆ ((1 - pˆ)3n). Thus, a con dence interval for p/(1 - p) is\n[\n√\n√\n]\npˆ\npˆ\npˆ\npˆ\n+ zα/2\n,\n+ z1-α/2\n.\n1 - pˆ\n(1 - pˆ)3n 1 - pˆ\n(1 - pˆ)3n\n\n4.1\nCon dence Sets Based on LM and LR Tests\nIn addition to the Wald statistic, we can invert tests based on the LM and LR statistics as well. However,\nthese con dence sets are usually more involved, as the inversion procedure is less straightforward.\nLet X1, ..., Xn be a random sample from the distribution Bernoulli(p). Then the joint log-likelihood is\n( ∑\n∑\n)\n∑\n∑\nXi\nln = log p\nXi (1 - p)n-\n=\nXi log p + (n -\nXi) log(1 - p).\nSo,\n∑\n∑\n∂ln\n∂p =\nXi\nn -\nXi\n-\n,\np\n1 - p\nand\nI(p) =\n.\np(1 - p)\nThus,\n(∑\n∑\n)2\nXi/p - (n -\nXi)/(1 - p)\nLM\n=\n√\nn/(p(1 - p))\n(\n)2\n∑\n∑\n(1 - p)\nXi - (n -\nXi)p\n=\n√\nnp(1 - p)\n( ∑\n)2\nXi - np\n=\n√\n.\nnp(1 - p)\nWe know that LM ⇒ χ2\n1. So, the con dence set based on inverting the LM test is\n{\n}\n∑Xi - np\np ∈ (0, 1) : √\n≤ z1-α/2\n,\nnp(1 - p)\nwhich is the solution to a quadratic inequality.\nAs for the LR test,\n∑\n∑\nlur - lr =\nXi log(ˆp/p0) + (n -\nXi) log((1 - pˆ)/((1 - p0)).\nn\nn\nSo, the con dence set based on inverting the LR test is\n{\n(∑\n∑\n)\n}\np ∈ (0, 1) : 2\nXi log(ˆp/p) + (n -\nXi) log((1 - pˆ)/((1 - p)) ≤ χ1\n-α(1) .\nIt is the solution to a nonlinear inequality.\n\nBootstrap con dence sets\nAssume one wants to create a con dence set for a parameter θ for which s/he has a consistent estimator\nˆθ = δ(X) where X is a random draw from unknown distribution F . One way to construct a bootstrap\nˆ\ncon dence set is by bootstrapping a statistic T (θ0) = θ - θ0 whenever testing H0 : θ = θ0. In particular, we\n∗\nwould draw a bootstrapped sample X∗ from an approximating distribution Fˆ and calculate T = δ(X∗) - θˆ .\n∗\nThen quantiles of T will serve as critical values for the corresponding test. Notice that inverting this test\nis exceptionally easy!\n- For b = 1, ..., B repeat the following:\nDraw a random sample X∗ from distribution Fˆ;\nb\n∗\nCalculate T = δ(X∗) - θˆ;\nb\nb\n∗\n- Order the bootstrapped statistics from smallest to largest: T\n≤ ... ≤ T(\n∗\nB).\n(1)\n∗\n∗\n- Test of H0 : θ = θ0 accepts if T\n≤ θˆ - θ0 ≤ T\n([ α B])\n([(1-α )B]).\n- Con dence set is θˆ - T ∗\n≤ θ0 ≤ θˆ - T ∗\n([(1- α\n([ α\n2 )B])\nB]).\nWhen does this work? When the di erence between the distributions of T and T ∗ converges almost surely to\nzero as the sample size increases. Notice that this interval implicitly bias-corrects. Most often the application\nof this method happens when θˆ is asymptotically Gaussian (and you choose not to calculate standard errors).\nThough if one decided to calculate standard errors then s/he may bootstrap a t-statistic. One would\nδ(X ∗ )-θˆ\nb\nthen bootstrap statistics Z∗ =\nand nd the proper quantiles of it. The resulting con dence set will\ns.e. ∗\nbe θˆ - Z∗\ns.e. ≤ θ0 ≤ θˆ - Z∗\ns.e.\n([(1- α\n([ α\n2 )B])\nB])\nGrid bootstrap\nOne may construct a con dence set by inverting other statistics as well, though the\ninversion is less obvious. This is called a grid bootstrap. One would impose a ne grid on the space of Θ and\nwill test each value θ0 on this grid. In particular, one would calculate the test statistic testing H0 : θ = θ0,\nsay, G(θ0, X), and nd its bootstrapped critical values. That is the hypothesis is accepted for example i\nG(θ0, X) < G∗\n(if the test uses only one-side of the nal distribution). Then on the grid we would\n([(1-α)B])\ndecide which θ0 are accepted, and the result does not have to be an interval.\nSome notes on joint con dence sets and the projection method\nImagine you are trying to estimate two parameters, α and β, from the same data set, and you have estimates\nαˆ and βˆ that are consistent and jointly gaussian, that is,\n(\n)\n(\n)\n√\nαˆ - α\nσ2\nσαβ\nα\nn\n⇒N(0, Σ),\nΣ =\n,\nˆ\nσ2\nβ - β\nσαβ\nβ\nˆ\nand you have a consistent estimator for Σ, say Σ. You can easily construct con dence sets for α: [ˆα -\n√σˆβ\n√σˆβ\n1.96 √\nσˆα , αˆ + 1.96 √\nσˆα ] and for β: [βˆ - 1.96\n, βˆ + 1.96\n]. However, these two con dence intervals are not\nn\nn\nn\nn\n\njointly valid, that is the probability that both of them cover the true values simultaneously is less than 95%.\nTo get a joint con dence set one should invert the test for the joint hypothesis H0 : α = α0, β = β0. For\nexample (if we still decide to stick with the Wald statistic), we may accept the null i :\n(\n)\n(\n)\nαˆ - α0\nW (α0, β0) = n αˆ - α0, βˆ - β0 Σˆ -1\n≤ χ2\n2,1-α.\nˆβ - β0\nThe set of (α0, β0) described by the inequality above is ellipse, call it A. Then to construct con dence sets\nfor α and β, which would be jointly valid, we may project it onto two axes. That is,\n[\n]\n√\n√\nσˆβ\nσˆβ\nˆ\nχ2\nˆ\nχ2\nCβ = {β0 : ∃α s.t. (α, β0) ∈ A} = {β0 : min W ald(α, β0) ≤ χ2\n2,1-α} = β -\n2,1-α √ , β +\n2,1-α √\n.\nα\nn\nn\nThis con dence set is constructed using the projection method. It is conservative, in the sense that\nPα0,β0 {α0 ∈ Cα and β0 ∈ Cβ } ≥ 1 - α,\nas the event under the probability sign is the rectangle containing the con dence ellipse A. Another idea,\nmay be to construct a rectangle to start with. For that we may consider other statistics, like:\n{\n}\nˆ\nαˆ - α0 √\nβ - β0 √\nS(α0, β0) = max\nn ,\nn\n.\nσˆα\nσˆβ\nWe may calculate the critical values from asymptotics (and they would depend on Σ) or by the bootstrap.\n√\nσα\n√\nσα\nAssuming that the bootstrap gives us critical value C, the con dence sets will be Cα = [ˆα - C ˆ , αˆ + C ˆ ]\nn\nn\n√σˆβ ˆ\n√σˆβ\nand Cβ = [βˆ - C\n, β + C\n].\nn\nn\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n14.381 Statistical Method in Economics\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Lecture Notes",
      "title": "14.381 Statistical Method in Economics, Lec 12: Bayesian Inference",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-381-statistical-method-in-economics-fall-2018/33bf335eca97c9d2da235fe0343ae800_MIT14_381F18_lec12.pdf",
      "content": "Lecture 12\nBayesian Inference.\nFrequentists and Bayesian Paradigms\nAccording to the frequentists theory, it is assumed that unknown parameter θ is some xed number or\nvector. Given parameter value θ, we observe sample data X from distribution f(·|θ). To estimate parameter\nθ, we introduce an estimator T (X) which is a statistic, i.e. function of the data. This statistic is a random\nvariable, since X. That is, the randomness here comes from randomness of sampling. Di erent properties of\nestimation characterize this randomness. For example, the concept of unbiasness means that if we observe\nmany samples from distribution f(·|θ), then the average of T (X) over the samples will be close to the true\nparameter value θ, i.e. Eθ[T (X)] = θ. The concept of consistency means that if we observe many samples\nfrom distribution f(·|θ), then the distribution of T (X) over these samples will be close in probability to the\ntrue parameter value θ, at least when we observe large samples, i.e. T (X) will be in a small neighborhood\nof θ in most observed samples.\nIn contrast, Bayesian theory assumes that θ is some random variable and we are interested in the real\nization of this random variable. This realization, say, θ0, is thought to be the true parameter value. It is\nassumed that we know the distribution of θ, or at least its approximation. This distribution is called a prior.\nIt usually comes from our subjective belief based on our past experience. Once θ0 is realized, we observe\na random sample X = (X1, ..., Xn) from distribution f(·|θ0). Once we have data, the best thing we can\ndo is to calculate conditional distribution of θ given X1, ..., Xn. This conditional distribution is called the\nposterior. The posterior is used to create an estimate for θ. Since we condition on the observations Xi, we\ntreat them as given. The randomness in posterior is an uncertainty about θ. The Bayesian approach is often\nused in the learning theory.\nBayesian Updating\nLet π(θ) denote our prior. In other words, parameter of interest θ is a random variable with distribution\nπ(·). Our model is f(·|θ). In other words, once we have a realization of a parameter value θ, the observed\nsample data X are drawn from the conditional distribution f(·|θ).\nThen the joint pdf of θ and X is\n∫\nf(x, θ) = π(θ)f(x|θ). The prior predictive distribution is m(x) =\nπ(θ )f(x|θ )dθ . Thus, the prior predictive\ndistribution means the marginal pdf of our sample data X. Once we observe X = x, we can calculate the\n\nposterior distribution for θ as\nf(x, θ)\nπ(θ)f(x|θ)\nπ(θ|X = x) =\n= ∫\n.\nm(x)\nπ(θ )f(x|θ )dθ\nExample Let X = (X1, ..., Xn) be a random sample from a Bernoulli(p) distribution. Then the joint pdf\n∑\n∑ xi\ni\nof the data is f(x|p) = p\ni xi (1 - p)n-\nwhere x = (x1, ..., xn). In classical theory, we would consider\nsome estimator of p. For example, we can take T (X) = Xn. Then we have already seen that Ep[T (X)] = p\nand T (X) →p p. In Bayesian theory we need some prior distribution of p. For example, suppose we believe\nthat all p are equally possible. Then we have a uniform prior, i.e. π(p) = 1 if p ∈ [0, 1] and 0 otherwise. Then\n∑\n∑\ni\ni\nthe joint pdf of p and X is f(x, p) = p\nxi (1 - p)n-\nxi I(0 ≤ p ≤ 1). The prior predictive distribution is\n∫ 1\nn\nn\n∑\n∑\n∑\n∑\ni\nm(x) =\np\nxi (1 - p)n-\ni xi dp = B(\nxi + 1, n -\nxi + 1),\ni=1\ni=1\n∫ 1\nwhere B(x, y) =\ntx-1(1 - t)y-1dt is the Beta-function. The posterior distribution is\n∑\n∑ xi\ni\ni\np\nxi (1 - p)n-\nπ(p|X = x) =\n∑\n∑\nI(0 ≤ p ≤ 1)\nn\nn\nB(\ni=1 xi + 1, n -\ni=1 xi + 1)\n∑\n∑\nn\nn\nThis distribution is called Beta B(α, β) distribution with parameters α =\ni=1 xi and β = n -\ni=1 xi + 1.\nIts mean is\n∑n\nα\ni=1 xi + 1\nE[p|X = x] =\n=\n.\n(α + β)\nn + 2\nIts variance is\n∑\n∑\nn\nn\nαβ\n(\ni=1 xi + 1)(n -\ni=1 xi + 1)\nV (p|X = x) =\n=\n.\n(α + β)2(α + β + 1)\n(n + 2)2(n + 3)\n2.1\nHow to calculate posterior distribution\nHere we consider a trick that may help to calculate the posterior analytically. It does not work always,\nthough.\nLet X = (X1, ...Xn) be a random sample from an N(μ, σ2) distribution. Suppose σ2 is known. Let\nN(μ0, τ 2) be the prior distribution for μ. Then the posterior distribution is π(μ|X = x) = π(μ)f(x|μ)/m(x)\nwhere π(μ) denotes the prior distribution and m(x) denotes the prior predictive distribution. Note that m(x)\n∫\ndoes not depend on μ. So m(x) is just a constant that normalizes\nπ(μ|X = x)dμ to 1. Therefore, when\nwe calculate π(μ)f(x|μ), we can denote all multiplicative terms which do not contain μ as some constant C\ninstead of keeping track of all these terms. Once we have an expression for π(μ)f(x|μ) as a function of μ,\nwe can integrate it in order to nd the normalizing constant m(x). In our case,\n{\n∑\n}\nn (xi - μ)2\n(μ - μ0)2\ni=1\nπ(μ|X = x) = Cπ(μ)f(x|μ) = C exp -\n-\n,\n2σ2\n2τ 2\n\nwhere C contains all terms which do not contain μ. Thus,\n{\n∑\n}\nn\nμ\nnμ\nμ\nμμ0\ni=1 xi\nπ(μ|X = x) ∝ exp\n-\n-\n+\nσ2\n2σ2\n2τ 2\nτ 2\n{\n(\n)\n(∑\n)}\nn\nn\ni=1 xi\nμ0\n∝ exp -μ\n+\n+ 2μ\n+\n2σ2\n2τ 2\n2σ2\n2τ 2\n{\n(\n) [\n(∑\n)]}\nn\nn\ni=1 xi/σ2 + μ0/τ 2\n∝ exp -\n+\nμ 2 - 2μ\nσ2\nτ 2\nn/σ2 + 1/τ 2\n{\n(\n)\n}\nn\n∝ exp -\n+\n[μ - μ ]\nσ2\nτ 2\n{\n}\n(μ - μ )2\n∝ exp -\n,\n2 σ2\n∑n\nwhere μ = (\ni=1 xi/σ2 + μ0/τ 2)/(n/σ2 + 1/τ 2) and σ 2 = 1/(n/σ2 + 1/τ 2). Note that, via some abuse of\nnotation, symbol ∝ stands proportional as function of μ\", thus di erent lines need di erent normalizing\nconstants. Thus, the conditional distribution of μ given X = x is N( μ, σ 2). Now, it is easy to nd a constant\n√\nin the last expression, namely, the missing constant is C = 1/(\n2πσ ).\nNote that in the example above, posterior mean μ is a weighted average of sample average xn and\nprior mean μ0, i.e.\nμ = ω1xn + ω2μ0 with ω1 + ω2 = 1 where ω1 = (n/σ2)/(n/σ2 + 1/τ 2) and ω2 =\n(1/τ 2)/(n/σ2 + 1/τ 2).\nHere 1/τ 2 may be interpreted as the precision of initial information.\nIf initial\ninformation is very precise, i.e. 1/τ 2 is large (or τ 2 is small), then the prior mean gets almost all weight,\nand μ is close to μ0. Thus, we almost ignore new information in the form of a sample X1, ..., Xn. If initial\ninformation is poor, i.e. 1/τ 2 is small, prior mean gets almost no weight, and μ is close to xn. Note that as\nn →inf, information from the sample dominates prior information and μ → xn . Moreover, at least in our\nexample, as n →inf, σ 2 → 0. Thus, as the sample size increases, the posterior distribution converges to a\ndegenerate distribution concentrated at the true parameter value.\nOnce we have the posterior distribution, we can construct an estimator of the parameter of interest.\nCommon examples include posterior mean and posterior mode (posterior mode denotes the point with the\ngreatest pdf value on the posterior distribution).\nAn important problem with Bayesian estimation is that if a prior distribution puts zero probability mass\non the true parameter value, then no matter how large our sample is, posterior distribution will put zero\nmass on the true parameter value as well. You will see examples of this phenomenon in 14.384.\n2.2\nConjugate Classes\nLet F be the class of distributions indexed by θ. Let P be the class of prior distributions of θ. Then we say\nthat P is conjugate to F if whenever data is distributed according to F and prior distribution is from P,\nthen the posterior distribution is from P as well. For example, we have already seen that the class of normal\ndistributions is conjugate to the class of normal distributions with known ( xed) variance. It is also known\nthat the class of B-distributions is conjugate to the class of binomial distributions. The concept of conjugate\nclasses is introduced because of its mathematical convenience. It is relatively easy to calculate the posterior\nwhen the prior lies in the conjugate class. Conjugate priors were almost the only priors used for a long time,\n\nas the others tend to be not analytically tractable.\n2.3\nSimulation Techniques\nConjugate priors were almost the only priors used for a long time, as the others tend to be not analytically\ntractable. Nowadays, there are numerical algorithms (MCMC- Markov Chain Monte-Carlo), that allow one\nto calculate posterior for priors outside the conjugate family. MCMC will be discussed in 14.384 and 14.385.\nThe goal of a typical MCMC algorithm is to get a sequence of random draws from posterior distribution.\nThat is to construct a numerical algorithm such that would produce θ1, ..., θB as simulated from π(θ|X = x),\n(not always independent draws but may be stationary and satisfying Law of Large Numbers). Then it allows\nto make inferences of any sort on θ or any function of θ. Imagine that you wish to use mean of posterior as an\n∑B\n∑B\nestimator than\nθb is an estimator for θ, while\nb=1 g(θb) is an estimator for parameter τ = g(θ).\nB\nb=1\nB\nIf you prefer to use median of the posterior as an estimate then you would order draws of θb in increasing\norder and take the middle one θ([B/2]) as an estimate for θ, etc.\n2.4\nCredible Intervals\nThe posterior distribution contains all the information that a researcher can get from the data. However,\nit is often impractical to report a posterior distribution, as it might be intractable or it might not have\nanalytic form at all. Therefore, it is a common practice to report only some characteristics of a posterior\ndistribution, such as its mean, variance, and mode. Mean and mode of the posterior serve as point estimators\nof the parameter while variance shows the precision of the posterior distribution. A better way to show the\nprecision of the posterior distribution is the concept of credible intervals. Let X be our data, θ ∈ Θ our\nparameter, and π(θ|X = x) the posterior distribution. Then for any α ∈ [0, 1], a set C(x) ⊂ Θ is called\n1 - α-credible if π(θ ∈ C(x)|X = x) ≥ 1 - α. In words, set C(x) contains true parameter value θ with\nprobability of at least 1 - α. Of course, the whole parameter space Θ is 1 - α-credible. But, apparently,\nreporting Θ as a 1 - α-credible set is not useful at all. The smallest γ-credible set contains only points with\nthe highest posterior density.\nAs an example, let X = (X1, ..., Xn) be a random sample from N(μ, σ2) with σ2 known and let N(μ0, τ 2)\nbe the prior distribution of μ. We have already seen that the conditional distribution of μ given X = x\nis N( μ, σ 2) with μ and σ 2 de ned as above.\nThen (μ - μ )/σ ∼ N(0, 1).\nLet zγ be the γ-quantile of\nthe standard normal distribution.\nThen π{(μ - μ )/σ ∈ [zα/2, -zα/2]|X = x} = 1 - α or, equivalently,\nπ{μ + zα/2σ ≤ μ ≤ μ - zα/2σ |X = x} = 1 - α. Since the pdf φ(x) of the standard normal distribution is\ndecreasing on x ≥ 0 and increasing on x ≤ 0, [ μ + zα/2σ,\nμ - zα/2σ ] is the shortest 1 - α-credible interval.\nLarge Sample Properties of Bayes' Procedures\nThere is a mathematical theorem which claims that under some regularity conditions the prior vanishes\nasymptotically and we get essentially MLE inferences.\nˆ\nTheorem 1. Under appropriate regularity conditions the posterior is approximately normal with mean θML\n\nand variance nI1(θˆ ML) when n is large. In particular, the asymptotic frequentist 1 - α interval\n[\n√\n√\n]\nC = θˆ ML - zα/2\n, θˆ ML - zα/2\nnI1(θˆ ML)\nnI1(θˆ ML)\nis also an approximate Bayesian credible set at level 1 - α:\nP (θ ∈ C|X = x) → 1 - α as n →inf\nTheorem says that the posterior concentrates around the asymptotic limit of frequentist MLE, and in\nany reasonable\" situation Bayes estimate in large samples will be close to MLE. However, you should be\ncautious about this theorem!\nCautions:\n- The theorem is about asymptotics. However, the prior can in uence inferences in nite samples.\n- One of the regularity assumption is the identi cation condition. If you are not identi ed, then where\nthe Bayesian estimator converges depends on your prior.\n- Prior should not restrict parameter space unnecessary\nThe strongest critique and objective people may have to Bayes inference is the question about where priors\ncome from.\nPriors are subjective by de nition, they summarize ones subjective opinion or belief about\npossible values of values of θ. There is a general desire for non-informative priors, which is a hard question.\n3.1\nBayesian Testing\nLet π(θ|X = x) be our posterior distribution. One can test the hypothesis H0 : θ ∈ Θ0 against the hypothesis\nthat H1 : θ ∈ Θ1 by looking on posterior odds and accept i :\nP (θ ∈ Θ0|X = x) ≥ P (θ ∈ Θ1|X = x).\nIt is interesting that the hypotheses are treated in a symmetric way here. You would face a di°culty if you\nwant to test a speci c parameter value, for example, H0 : θ = θ0 as the continuous posterior put zero weight\non any point. In such a case one has to put pass point weigh prior weight on this speci c value for testing.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n14.381 Statistical Method in Economics\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Lecture Notes",
      "title": "14.381 Statistical Method in Economics, Lec 2: Limit Theorems",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-381-statistical-method-in-economics-fall-2018/9684a62ef140ffcfb3258bce01b460bc_MIT14_381F18_lec2.pdf",
      "content": "Lecture 2\nLimit theorems\nUseful Inequalities\nTheorem 1. (Markov inequality) Let X be any nonnegative random variable such that E[X] exists. Then\nfor any t > 0, we have P {X ≥ t} ≤ E[X]/t.\nProof. Since X is nonnegative,\n∫\n∫ inf\nt\n∫ inf\nE[X] =\nxf(x)dx =\nxf(x)dx +\nxf(x)dx\nt\n∫ inf\n∫ inf\n≥\nxf(x)dx ≥ t\nf(x)dx = tP {X ≥ t}\nt\nt\nwhere f denotes the pdf of X. A similar argument works for other types of random variables (not-continuous)\nas well.\nTheorem 2. (Chebyshev inequality) For any random variable X with mean μ and nite variance and for\nany t > 0, we have P {|X - μ| ≥ t} ≤ V ar(X)/t2 .\nProof. Note that |X - μ| ≥ t if and only if |X - μ|2 ≥ t2 . Thus, P {|X - μ| ≥ t} = P {|X - μ|2 ≥ t2}. Since\n|X - μ|2 is a nonnegative random variable, P {|X - μ|2 ≥ t2} ≤ E[|X - μ|2]/t2 = V ar(X)/t2 by Markov\ninequality.\nTheorem 3. (Holder's inequality) If p > 1 and 1/p + 1/q = 1, and if E|X|p < inf and E|Y |q < inf, then\nE|XY | ≤ (E|X|p)1/p(E|Y |q )1/q .\nConvergence in probability and Law of Large Numbers\nDe nition 4. Let X1, ..., Xn, ... be a sequence of random variables. We say that {Xn}inf\nconverges to X\nn=1\nin probability if for any ε > 0 P { |Xn - X| > ε} → 0 as n →inf. In this case we write Xn →p X.\nTheorem 5. If E(Xn - X)2 → 0, then Xn →p X.\nProof. By Markov inequality, for any ε > 0\nP {|Xn - X| > ε} = P {|Xn - X|2 > ε2} ≤ E[|Xn - X|2]/ε2 → 0\n\n.\nTheorem 6. If {Xi}inf is a sequence of independent and identically distributed (i.i.d.) random variables\ni=1\n∑n\nwith E[Xi] = μ and V ar(Xi) = σ2 < inf, then Xn :=\nXi/n →p μ.\ni=1\n∑\n∑\nn\nn\nProof. By linearity of expectation, E[Xn] = E[\nXi/n] =\nE[Xi]/n = μ. Thus,\ni=1\ni=1\nn\nn\n∑\n∑\nE[|Xn - μ|2] = V (X) = V (\nXi)/n2 =\nV (Xi)/n2 = σ2/n.\ni=1\ni=1\nThus, E|Xn - μ|2 → 0 as n →inf.\nTheorem 6 uses very strong i.i.d. assumption, in Econometrics we often consider cases when it is not\nsatis ed. Limit theorems for dependent random variables are discussed in 14.384 Time Series. It is easy to\nget an extension for independent but non-identically distributed random variables. Assume that {Xi}inf\ni=1 are\n∑ n\nσ2\nindependent random variables with E[Xi] = μ but V ar(Xi) = σi\n2 . Show that LLN holds if n2\ni=1\ni → 0.\nAnother version of the law of large numbers is\nTheorem 7. If {Xn}inf\nis a sequence of iid random variables with EXn = μ and E|Xn| < inf, then\nn=1\nXn →p μ\nWeak convergence and Central Limit Theorem\nDe nition 8. We say that {Xn}inf\nconverges to X in distribution or weakly if limn→inf FXn (x) = FX (x)\nn=1\nfor all x ∈ R where FX (x) is continuous. In this case we write Xn ⇒ X.\nTheorem 9. If Xn →p X, then Xn ⇒ X.\nProof. Note that Xn ≤ x and X > x + ε implies |Xn - X| > ε. Thus,\nFXn (x)\n= P {Xn ≤ x}\n= P {Xn ≤ x, X ≤ x + ε} + P {Xn ≤ x, X > x + ε}\n≤ P {X ≤ x + ε} + P {|Xn - X| > ε}\n= FX (x + ε) + P {|Xn - X| > ε}.\nfor any x ∈ R and ε > 0. Similarly,\nFX (x - ε) ≤ FXn (x) + P {|Xn - X| > ε}.\nThus,\nFX (x - ε) - P {|Xn - X| > ε} ≤ FXn (x) ≤ FX (x + ε) + P {|Xn - X| > ε}.\n\nNext, if x is a point of continuity of FX , for any δ > 0, there exists ε(δ) > 0 such that\nFX (x + ε(δ)) - δ ≤ FX (x) ≤ FX (x - ε(δ)) + δ.\nTherefore\nFX (x) - δ - P {|Xn - X| > ε(δ)} ≤ FXn (x) ≤ FX (x) + δ + P {|Xn - X| > ε(δ)}.\nNext, since Xn →p X,by de nition:\nlim |FXn (x) - FX (x)| ≤ δ\nn\nSo, FXn (x) → FX (x) as n →inf for any x ∈ R where FX (x) is continuous.\nAs an exercise, prove that if c is some constant and Xn ⇒ c, then Xn →p c.\nTheorem 10. ( Central limit theorem) Let {Xi} be a sequence of i.i.d. random variables with mean μ and\n∑\n√\nn\nvariance σ2 . Then\n(Xi - μ)/\nn ⇒ N(0, σ2).\ni=1\n∑\n√\nn\nIn the multivariate case, if V ar(Xi) = E[(Xi - E[Xi])(Xi - E[Xi])T ] = Σ, then\n(Xi - μ)/\nn ⇒\ni=1\nN(0, Σ).\nWe often will need to consider non-identically distributed random variables, in such a case we should use\nLinderberg- Feller's CLT:\nTheorem 11. Let {Xi}inf\ni=1 be a sequence of independent random variables with EXi = μi and V (Xi) = σi\n2 .\n∑\n∑\nn\nn\nDenote c = V (\nXi) =\nσi\n2 . If for any ε > 0\nn\ni=1\ni=1\nn\n1 ∑\n(\n)\nlim\nE (Xi - μi)2I{|Xi - μi| > εcn} = 0\n(1)\nn→inf c2\nn i=1\n∑ n\n(Xi-μi)\nthen\ni=1\n⇒ N(0, 1).\ncn\nSometimes Linderberg's condition (1) is called asymptotic negligibility as in particular it implies that\nσ2\nmax1≤i≤n\ni → 0 and guarantees that a personal contribution of any Xi to the variance of the sum is\ncn\nsu°ciently small for large n. The following su°cient condition for (1) is called Lyapunov's:\nn\n∑\nE|Xi - μi|2+δ\nlim\n= 0 for some δ > 0.\n2+δ\nn→inf cn\ni=1\nAsymptotic statements derived from basic limit theorems\nTheorem 12. (Slutsky theorem and Continuous mapping theorem) Let X, X1, ..., Xn, ... and Y, Y1, ..., Yn, ...\nbe some random variables. Let g be some continuous function. Let c be some constant. Then\n1. If Xn →p X and Yn →p Y , then Xn + Yn →p X + Y and XnYn →p XY .\n2. If Xn ⇒ X and Y →p c, then Xn + Yn ⇒ X + c and XnYn ⇒ cX.\n\n3. If Xn →p X, then g(Xn) →p g(X).\n4. If Xn ⇒ X, then g(Xn) ⇒ g(X)\nThe rst and second statements are known as the Slutsky theorem. The third and forth statements are\nknown as the Continuous mapping theorem. These theorems are widely used in statistics.\n4.1\nSymbols op and Op\nFirst, let us talk about some notation for non-stochastic sequences xn and bn. We would write xn = o(bn)\nwhen limn→inf\nx\nbn\nn = 0. Often it is described as xn is asymptotically smaller than bn. We would write\n√\nxn\nxn = O(bn) when supn\nbn < inf. In general we will use some easy-to-describe sequences as bn, such as\nn,\n1 or 1. Notice that xn = o(1) means xn → 0 and xn = O(1) means that xn is a bounded sequence. Now let\nn\nus adopt similar notations for sequences of random variables {Xn}inf\nn=1:\nDe nition 13. We say that Xn = op(bn) i Xn →p 0.\nbn\n{\n}\nXn\nWe say that Xn = Op(bn) i for any ε > 0 there exists constant C < inf such that P\nbn\n> C < ε for\nall n.\nSometimes statement Xn = Op(bn) is described as sequence\nXn is stochastically bounded.\nSome\nbn\nexample of the use of these symbols are given below:\n- If Xn ⇒ N(0, 1) as n →inf then Xn = Op(1);\n- If Xn →p X then Xn = X + op(1);\n- Chebyshev's inequality for i.i.d. Xi with nite variance implies Xn = Op( √ )\nn\nWe will often use the following statements :\n- If Xn = Op(n-δ) for some δ > 0 then Xn = op(1);\n- If Xn = op(bn) then Xn = Op(bn);\nα\n- If Xn = Op(nα) and Yn = Op(nβ ), then XnYn = Op(nα+β ) and Xn + Yn = Op(max{n , nβ });\n- If Xn = Op(nα) and Yn = op(nβ ), then XnYn = op(nα+β );\n- If Xn = Op(nα) and Yn = op(nα), then Xn + Yn = Op(nα)\nDelta method\n√\nTheorem 14. Assume that for a sequence of random variables Xn and constants μ and σ we have\nn(Xn -\n√\nμ) ⇒ N(0, σ2). If g ′ (μ) =\n0, then\nn(g(Xn) - g(μ)) ⇒ N(0, σ2(g ′ (μ))2).\n\n⋆\nProof. By the mean value theorem, for any realization Xn(ω), there is some μ (ω) between μ and Xn(ω)\nn\nsuch that\ng(Xn(ω)) - g(μ) = g ′ (μ ∗ )(Xn(ω) - μ).\n(2)\n√\n⋆ }inf\nThus, we have de ned a new sequence of random variables, {μ\nn=1. By assumptions we have\nn(Xn -μ) =\nn\n→p\n⋆\n⋆\nOp(1), thus (Xn - μ) = op(1) and Xn\nμ. Since μ is between μ and Xn, μ →p μ as well. By the\nn\nn\n⋆\nContinuous mapping theorem, g ′ (μ ) →p g ′ (μ) since g ′ (x) is continuous. Moreover, by the Slutsky theorem\nn√\nand by the Central limit theorem,\nng ′ (μ ∗)(Xn(ω) - μ) ⇒ g ′ (μ)N(0, σ2).\nNote that this theorem also holds when g ′ (μ) = 0 but in this case the asymptotic distribution will be 0\n(constant), i.e. degenerate. I recommend that you remember the argument used in this theorem as it is very\ntypical in statistics and econometrics.\nThe Delta method has a multidimensional extension. Let X1, ..., Xn, ... be a sequence of iid k × 1 random\nvectors with mean μ and covariance matrix Σ.\nThen, by the multidimensional Central limit theorem,\n√ n(Xn - μ) ⇒ N(0, Σ).\nLet g : Rk → R be a twice continuously di erentiable function.\nLet τ 2 =\n(∂g(μ)/∂μ)T Σ(∂g(μ)/∂μ). Here ∂g(μ)/∂μ is a k × 1 vector with i-th component equals ∂g(μ)/∂μi. Then\n√ n(g(Xn) - g(μ)) ⇒ N(0, τ 2).\n5.1\nExample\nLet X1, ..., Xn, ... be a sequence of iid random variables with mean μ and variance σ2 . What is the limiting\ndistribution of (Xn)2? Let g(x) = x . Then g ′ (μ) = 2μ. Thus, by the Delta method, √ n((Xn)2 - μ2) ⇒\nN(0, 4μ2σ2). Note that if μ = 0, then the limit distribution is degenerate.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n14.381 Statistical Method in Economics\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Lecture Notes",
      "title": "14.381 Statistical Method in Economics, Lec 3: Intro to Statistics, Inferences for Normal Families",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-381-statistical-method-in-economics-fall-2018/f1053e5346041952ef00c3f55df03d43_MIT14_381F18_lec3.pdf",
      "content": "Lecture 3\nIntro to Statistics. Inferences for normal families.\nBasic Concepts: Population, Sample, Parameter, Statistics\nIn statistics we usually have a data set (or a sample), which can be described as a single draw of data from\nall potential realizations of that data. We may describe it as a realization x of random vector X . The\ndistribution FX of data vector X is often referred as population.\nIn Econometrics one encounters 3 types of data: cross-section, time series and panel. Cross-section is\nusually described as a set of iid (independent and identically distributed ) random vectors X1, ..., Xn (that\n∏n\nis, X = (X1, ..., Xn), x = (x1, ..., xn)). If we assume that Xi ∼ F , then FX (x) =\nF (xi). Time-series\ni=1\ndata Xt, t = 1, ..., T usually allow dependency between consecutive observations and describe (X1, ..., XT )\nas one realization of a path that results from a dynamic process. Panel data usually consider X = {Xit, i =\n1, .., n, t = 1, ..., T } that we have a draw from n independent identically distributed dynamic processes.\nThe object of interest here is usually some functional of the unknown distribution FX . Any such function\nis known as a parameter. In the case of cross-sectional data, it is usually some function of distribution of\none observation F ; for time series, the parameter may be related to the dependence between observations as\nwell as marginal distributions of observation. Notice that parameter is a population concept.\nThe goal of Statistics generally is to render some judgement about a parameter (or population FX ) based\non a single draw from this population. This is called inference. We will see three types of inference: esti\nmation, con dence set construction and testing. In performing each task we will sometimes make mistakes,\nand the quality of the procedure will be related to minimizing the size and/or probability of mistakes.\nWe refer to any function of a random sample as a statistic. Thus, Y = g(X ) = g(X1, ..., Xn) is a statistic.\nBy construction, it is random variable. When calculated for our speci c data set y = g(x) = g(x1, ..., xn) it\nproduces a single realization of this random variable. The distribution of a statistic is called the sampling\ndistribution.\nExample 1.\nAssume you want to gure out whether the penny you have is a fair coin by ipping it n = 10\ntimes and recording 0 for each tail and 1 for each head. In this case your data x = (x1, ..., x10) is a sequence\nof of 0s and 1s of the length 10, where xi is the result of i-th experiment. This is a single realization of\nrandom vector X = (X1, ..., X10), where Xi ∼ i.i.d. Bernoulli(p). The population here is described as\n∏10\nP (x1, ..., x10) =\nxi (1 - p)1-xi , and is known up to parameter p. So, the only goal is to make some\ni=1 p\njudgement about p. Remember, statistics refers to any function of the data set. We may consider many\n\ndi erent functions: say,\n∑\nY1 = {number of heads} =\nXi,\ni=1\nY2 = {the order number of the rst experiment resulting in heads, with 0 if no heads} = X1 + 2(1 -\nX1)X2 + 3(1 - X1)(1 - X2)X3 + ...,\nY3 = {the di erence between the number of heads in the rst 6 ips and the number of heads in the\n∑6\n∑10\nremaining 4} =\nXi -\nXi.\ni=1\ni=7\nIf one has data=(0,1,1,1,0,0,1,1,0,1), then y1 = 6, y2 = 2, y3 = 0. All statistics Y1, Y2, Y3 are random\nvariables with a distribution depending on p.\nExample 2.\nAssume you want to estimate the average income of a man aged between 25 and 65, who\nresides in Massachusetts. You were able to get a random sample of income for n such men. Your data\nis represented by random vector X = (X1, ..., Xn), where Xi ∼ i.i.d.F (·), where F (·) is the unknown\n∏n\ndistribution of incomes. The population is described by FX (x1, ..., xn) =\nF (xi). The parameter of\ni=1\n∫\ninterest is μ =\nudF (u)- the mean of the unknown income distribution. You have one realization of this\ndata set x = (x1, ..., xn), statistic is any function of the data set. Some examples:\n∑ n\nY1 =\nXi (average);\nn\ni=1\nY2 = (n/2)-th highest value among (X1, ..., Xn); (median)\nY3 = average of 80% of middle values (drop 10% smallest and 10% largest values)- a trimmed mean.\n1.1\nSample mean and sample variance.\n∑n\nThe two most commonly used statistics are the sample mean (Xn =\nXi/n) and the sample variance\ni=1\n∑ n\n(s =\n(Xi -Xn)2/(n-1)). These statistics have attractive properties as described in the lemma below:\ni=1\nLemma 1. If X1, ..., Xn ∼ i.i.dF is a random sample of size n from a population distribution with mean\nμ = EXi and variance σ2 = V ar(Xi), then E[Xn] = μ and E[s2] = σ2 .\nProof. By linearity of expectation,\nn\nn\nn\n∑\n∑\n∑\nE[Xn] = E[\nXi/n] =\nE[Xi]/n =\nμ/n = μ\ni=1\ni=1\ni=1\n∑n\n. To show the second part of the lemma, denote Yi = Xi - μ and Y n =\ni=1 Yi/n. Note that E[Yi] = 0.\nThus, E[Y 2] = V (Yi) = V (Xi) = σ2 and V (Y n) = σ2/n. Then\ni\nn\nn\n∑\n∑\nE[s 2] = E[\n(Xi - Xn)2/(n - 1)] = E[\n((Xi - μ) - (Xn - μ))2/(n - 1)]\ni=1\ni=1\nn\nn\n∑\n∑\n= E[\n(Yi - Y n)2/(n - 1)] = E[\n(Yi\n2 - 2YiY n + Y )]/(n - 1)\nn\ni=1\ni=1\n\nn\nn\n∑\n∑\n= E[\nY 2 - 2nY + nY ]/(n - 1) = E[\nY 2 - nY ]/(n - 1)\ni\nn\nn\ni\nn\ni=1\ni=1\nn\n∑\n= (\nE[Y 2] - nE[Y ])/(n - 1) = (nσ2 - σ2)/(n - 1) = σ2\ni\nn\ni=1\n1.2\nEmpirical distribution function\nˆ\nIf we have a random sample X1, ..., Xn of size n, then empirical distribution function Fn is the cdf of the\ndistribution that places mass 1/n at each data point Xi. Thus, by de nition,\nn\n∑\nˆFn(x) =\nI(Xi ≤ x)/n,\ni=1\nwhere I(·) stands for the indicator function, i.e. the function which equals 1 if the statement in brackets is\ntrue, and 0 otherwise. In other words, Fˆ n(x) shows the fraction of observations with a value smaller than or\nequal to x. An important property of an empirical distribution function is given in the lemma below.\nLemma 2. If we have a random sample X1, ..., Xn of size n from a distribution with cdf F , then for any\nˆ\nx ∈ R, E[Fˆ n(x)] = F (x) and V (Fˆ n(x)) → 0 as n →inf. As a consequence, Fn(x) →p F (x) as n →inf.\nProof. Note that I(Xi ≤ x) equals 1 with probability P {X ≤ x} and 0 otherwise. Thus, E[I(Xi ≤ x)] =\nP {X ≤ x} = F (x). Hence E[Fˆ n(x)] = F (x) by linearity of expectation. In addition, V (I(Xi ≤ x)) =\nF (x)(1 - F (x)) by the formula for variance of a Bernoulli (F (x)) distribution. Therefore,\nn\n∑\nV (Fˆ n(x)) =\nV (I(Xi ≤ x))/n2 = F (x)(1 - F (x))/n → 0.\ni=1\nTo prove the second part of the lemma, we have E[(Fˆ n(x) - F (x))2] = V (Fˆ n(x)) → 0 as n →inf since\nE[Fˆ n(x)] = F (x). Convergence in probability then follows from Chebyshev's inequality.\nActually, a much more strong result holds as well:\nTheorem 3 (Glivenko-Cantelli). If X1, ..., Xn is a random sample from a distribution with cdf F , then\nsup |Fˆ n(x) - F (x)| →p 0.\nx∈R\nWays to nd the distribution of a statistic\nIn order to make inferences we often will need to know the distribution of di erent statistics. There are\nseveral ways of getting them.\n\n2.1\nExact distribution\nIn rare cases one can actually calculate the exact distribution of a statistic.\n∑n\nExample 1(cont.)\nConsider statistic Y1 =\nXi, it take integer values between 0 and 10 with\ni=1\n10!\nk(1 - p)10-k\nP {Y1 = k} =\np\n.\nk!(10 - k)!\nThe distribution depends on an unknown p; once we know (or postulate p) we can calculate exact distribution\nof Y1.\n∑ n\nExample 2(cont)\nLet us try to gure out the distribution of Y1 =\nXi. If Xi ∼ F (·) with pdf f\nn\ni=1\n∫\nthen X1 + X2 ∼ f2(u) where f2(u) =\nf(x)f(u - x)dx. So, Y1 has pdf\n∫\n∫\nfY (y) =\n...\nf(10y - y1 - ... - yn-1)f(y1)...f(yn-1)dy1...dyn-1,\nwhich is a complicated expression and is not very helpful in most situations. It depends in a signi cant way\non the unknown distribution F (·). If we make some strong assumptions about the distribution of our data\nwe may end up with an exact distribution of some statistics. For example, assume Xi ∼ i.i.d.N(μ, σ2), then\nY1 ∼ N(μ, σ2/n). However, this is a strange assumption to make about income distribution (why?).\n2.2\nMonte-Carlo Method.\nLet us consider Example 2, but now with an alternative assumption of the log-normal distribution. Assume\nthat Xi has the following pdf:\n(log(x-γ)-μ)2\n-\n2σ2\nf(x; μ, σ2, γ) = (2πσ2)-1/2(x - γ)-1 e\nI{x > γ}.\n∑ n\nApparently, getting a (closed-form) exact distribution of Y1 =\nXi is not that easy. But we can do\nn\ni=1\nthis numerically by the use of Monte-Carlo method. A typical algorithm for given (μ, σ2, γ):\n- For b = 1, ..., B, simulate X ∗ = (X∗ , ..., X∗ ), where the Xib\n∗ are independently drawn from f(x; μ, σ2, γ);\nb\n1b\nnb\n∑ n\n- Calculate Y ∗ = g(X ∗), in our case\nX∗\nb\nb\nn\ni=1\nib;\n- If you are interested in:\n∑B\ncdf of Y , then FY (s) ≈\nI{Y ∗ ≤ s};\nB\nb=1\nb\n∑B\nprobability Y gets into the set A: P {Y ∈ A} ≈\nI{Y ∗ ∈ A};\nB\nb=1\nb\nα-quantile of Y : qY (α) ≈ Y(\n∗\n⌊αB⌋), here (·)- stands for the order statistics;\n∑B\nmean of Y : EY ≈ 1\nY ∗\nB\nb=1\nb ;\n(\n)2\n∑B\n∑B\nvariance of Y : V (Y ) ≈\nY ∗ - 1\nY ∗\n.\nB-1\nb=1\nb\nB\ns=1\ns\n\nHere all the Y ∗ are i.i.d. from the correct distribution. We control the accuracy here: larger B leads to\nb\nbetter accuracy; we are bounded only by power of our computers.\nThe important assumption here is we assume that we know (or postulate) the distribution of the data.\n2.3\nAsymptotic approximation\nExact nite-sample distributions of many statistics have complicated forms and cannot be calculated directly\nif we do not know (and do not want to assume) the distribution of the data. But at the same time that often\nmay be well-approximated if the sample size is large. This is known as asymptotic approximation and relies\non CLT, delta-methods, Slutsky theorem and alike.\nExample 2(cont)\nIf we are unwilling to assume the distribution of Xi at all, but are willing to assume\nthat it has nite variance, then\nn\n∑\n√\nn(\nXi - EXi) ⇒ N(0, V (Xi)).\nn i=1\n∑n\nThus the distribution of Y1 =\nXi is approximately gaussian with mean EXi and variance V (Xi)/n.\ni=1\nWe do not control the quality of this approximation and cannot improve it, but as the sample size grows the\napproximation should become more accurate.\nAsymptotic approximation is probably the most often used way of guring out the distribution of a\nstatistic in econometrics.\n2.4\nBootstrap\nThe distribution of statistic Y = g(X ) is a function of g(·) and the population distribution FX . The typical\nissue, as we have seen above, is that we do not know FX . One idea may be to approximate FX by some\nclose distribution. If statistic Y depends on the distribution of the data in a continuous enough fashion, we\n∏n\nmay get a good enough approximation. For example, in the i.i.d case FX (x) =\nF (xi), all we need is to\ni=1\napproximate F - the cdf of one observation.\nAs one example, Glivenko-Cantelli' theorem above suggests that empirical distribution Fˆ n(x) may be a\ngood approximation to F (x). So, the idea is to run the same algorithm as one does with Monte-Carlo simu\nlations, but simulate X∗ from distribution Fˆ n(x) rather than from an unknown F . This speci c procedure\nib\nis called the non-parametric bootstrap:\n- For b = 1, ..., B, simulate X ∗ = (X∗ , ..., X∗ ), where the X∗ are drawn independently and uniformly\nb\n1b\nnb\nib\nwith replacement from the set of initial observations {xi, i = 1, ..., n} (each xi has the same probability\nto be drawn);\n∑ n\n- Calculate Y ∗ = g(X ∗), in our case\nX∗\nb\nb\nn\ni=1\nib;\n- If you are interested in:\nthe cdf of Y , then FY (s) ≈\n∑B\nI{Y ∗ ≤ s};\nB\nb=1\nb\n\n∑B\nprobability Y gets into the set A: P {Y ∈ A} ≈\nI{Y ∗ ∈ A}.\nB\nb=1\nb\nα-quantile of Y : qY (α) ≈ Y(\n∗\n⌊αB⌋), here (·) - stands for the order statistics.\n∑B\nY ∗\nmean of Y : EY ≈ B\nb=1\nb .\n(\n)2\n∑B\n∑B\nY ∗ - 1\nvariance of Y : V (Y ) ≈\nb=1\nB\nYs\n∗\n.\nB-1\nb\ns=1\nLarger B will help eliminate some part of approximation error (simulation error), but some part (due to\napproximating F ) cannot be controlled.\nWe will see many other ways to approximate F (x), which would lead to a number of di erent bootstrap\nprocedures.\nThere are often two ways to justify using the bootstrap. Typically we wish to claim that the distance\nbetween true distribution of statistic FY and the bootstrapped one FY ∗ is converging to zero in some sense\nas the sample size increases to in nity. One way is to show that FY is continuous in F in some sense (we\nhave to be accurate as there are di erent metrics that can be introduced on the space of these distributions)\nand show that whatever approximation Fˆ we use, it converges to F in the proper metrics. The other way is\nto show that the distribution of Y converges somewhere (allow for asymptotic approximation) and that Y ∗\nconverges to the same distribution. We will discuss this many times in what follows.\nPlug-in estimators\nSuppose we have a random sample X1, ..., Xn of size n from a population distribution with cdf F . Suppose T\nis some function on the space of possible cdfs. Suppose we do not know F but we are interested in T (F ). Then\nwe can use some statistic g(X1, ..., Xn) to estimate T (F ). In this case g(X1, ..., Xn) is called an estimator\nof T (F ). Its realization g(x1, ..., xn) is called an estimate of T (F ). Here the x1, ..., xn stand for realizations\nof X1, ..., Xn. What is a good estimator of T (F )? By common sense, a good estimator g(X1, ..., Xn) should\nbe such that g(X1, ..., Xn) ≈ T (F ), at least with large probability. One possible estimator is T (Fˆ n), where\nFˆ n is the empirical cdf. T (Fˆ n) is called a plug-in estimator. From the Glivenko-Cantelli's theorem we know\nthat Fˆ n will be close to F with large probability in large samples. Thus, if T is continuous, then T (Fˆ n) will\nbe close to T (F ).\nAs an example, suppose we are interested in the mean of the population distribution, i.e. μ = T (F ) =\n∫ +inf\nE[X] = -inf xdF (x).Then\n∫ +inf\n∑\nμˆ = T (Fˆ n) =\nxdFˆ n(x) =\nXi/n = Xn.\n-inf\nThus, the plug-in estimator of the population mean is just the sample average.\nNext, suppose we are\n∫ +inf\ninterested in the variance of the population distribution, i.e. σ2 = T (F ) = E[(X - E[X])2] =\n(x -\n-inf\n\n∫ +inf xdF (x))2dF (x). Then\n-inf\nσˆ2\n= T (Fˆ n)\n∫\n∫\n+inf\n+inf\n=\n(x -\nxdFˆ n(x))2dFˆ n(x)\n-inf\n-inf\n∫ +inf\n=\n(x - Xn)2dFˆ n(x)\n-inf\nn\n∑\n=\n(Xi - Xn)2/n.\ni=1\nThus, the plug-in estimator of the population variance does not coincide with the sample variance. The\nreason we use n - 1 instead of n in the denominator of the sample variance is to make it unbiased for the\npopulation variance, i.e. E[s2] = σ2 . Note that E[ˆσ2] = (n - 1)σ2/n =\nσ2 .\nFinally, consider the plug-in estimator of quantiles. We already de ned the quantile of the distribution\nin lecture 1 as qp = inf{x : F (x) ≥ p} so that qp is the p-th quantile of distribution F . Thus, the plug-in\nˆ\nestimator of the p-th quantile is qˆp = inf{x : Fn(x) ≥ p}.\nParametric Families: Normal\nThe plug-in estimator considered above is a generic nonparametric estimator of some function T (F ) of\ndistribution F in the sense that it does not use any information about the class of possible distributions.\nHowever, in practice, it is sometimes assumed that the class of possible distributions form some parametric\nfamily. In other words, it is assumed that F = F (θ) with θ ∈ Θ where Θ is some nite-dimensional set.\nThen θ is called a parameter and Θ is a parameter space. In this case the cdf F and the corresponding pdf\nf are often denoted by F (x|θ) and f(x|θ). If X1, ..., Xn is a random sample from a distribution with pdf\n∏n\nf(x|θ), then joint pdf f(x1, ..., xn|θ) =\nf(xi|θ). For xed x1, ..., xn, f(x1, ..., xn|θ) as a function of θ is\ni=1\ncalled the likelihood function.\nOne of the most important parametric families is a normal family when θ = (μ, σ2) and the population\ndistribution is N(μ, σ2). Before considering normal family, let us give some de nitions related to normal\ndistributions.\n∑n\nIf X1, ..., Xn is a random sample from N(0, 1), then random variable χ2 =\nX2 is called a χ2 random\nn\ni=1\ni\nvariable with n degrees of freedom. Its distribution is known as a χ2 distribution with n degrees of freedom.\np/2-1\nIt is known that its pdf is given by f(x) = x\ne-x/2/(Γ(p/2)2p/2) if x > 0 and 0 otherwise. Here the\nΓ(x) denotes the gamma function. Its values can be found in special tables.\n√\nNext, if X0 is N(0, 1) and independent of X1, ..., Xn, then tn = X0/\nχ2 /n is called a t random variable\nn\nwith n degrees of freedom. Its distribution is called a t-distribution or a Student distribution.\nFinally, if χ2 and χ2 are independent χ2 random variables with n and m degrees of freedom correspond-\nn\nm\ningly, then Fn,m = (χ2 /n)/(χ2 /m) is called a Fisher random variable with (n, m) degrees of freedom. This\nn\nm\ndistribution is called a Fisher distribution with (n, m) degrees of freedom.\nThe following theorem gives some basic facts about the sample mean and the sample variance for random\n\nsample from normal distribution:\nTheorem 4. If X1, ..., Xn are iid random variables with N(μ, σ2) distribution, then (1) Xn and s are\nn\nindependent, (2) Xn ∼ N(μ, σ2/n), and (3) (n - 1)s2/σ2 ∼ χ2\nn-1.\nProof. Let Z = Xn, Y1 = X1 - Xn, Y2 = X2 - Xn, ..., Yn = Xn - Xn. Then Z, Y1, ..., Yn are jointly normal.\nObviously, E[Z] = μ and E[Yi] = μ - μ = 0 for all i = 1, ..., n. In addition, V (Z) = σ2/n. Thus statement\n(2) holds.\nFor any j = 1, ..., n,\ncov(Z, Yj )\n= cov(Xn, Xj - Xn)\n= cov(Xn, Xj ) - V (Xn)\n= σ2/n - σ2/n\n=\nSince uncorrelated jointly normal random variables are independent, we conclude that Z is independent of\n∑\n∑\nn\nn\nY1, Y2, ..., Yn. Moreover, s =\n(Xi - Xn)2/(n - 1) =\nYi\n2/(n - 1) and statement (1) holds since\ni=1\ni=1\nany functions of independent random variables are independent as well.\nThe proof of statement (3) is left for Problem set 1.\n√\nBy de nition, t = (Xn - μ)/(s/ n) is called the t-statistic. Using the theorem above,\nXn - μ\nXn - μ\nN(0, 1)\nt =\n√\n=\n√\n√\n∼ √\n= tn-1\ns/\nn\nσ/ n\ns2/σ2\nχ2\n/(n - 1)\nn-1\nsince N(0, 1) and χ2\nn-1 in the display above are independent. Thus, we proved that if X1, ..., Xn is a random\nsample from N(μ, σ2), then t-statistic has t-distribution with n - 1 degrees of freedom.\nFinally, let X1, ..., Xn be a random sample from N(μx, σ2) and Y1, ..., Ym be a random sample from\nx\nN(μy, σ2). Assume that X1, ..., Xn are independent of Y1, ..., Ym. Then F = (s /s2 )/(σ2/σ2) is called a\ny\nx\ny\nx\ny\nF -statistic. Using the theorem above,\nχ2\ns /s2\nn-1/(n - 1)\nF =\nx\ny ∼\n= Fn-1,m-1\nσ2/σ2\nχ2\n/(m - 1)\nx\ny\nm-1\nThus, the F -statistic has the F -distribution with (n - 1, m - 1) degrees of freedom.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n14.381 Statistical Method in Economics\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Lecture Notes",
      "title": "14.381 Statistical Method in Economics, Lec 5: Point Estimators",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-381-statistical-method-in-economics-fall-2018/848f2edc5d8bc940fcc988c955f02118_MIT14_381F18_lec5.pdf",
      "content": "Lecture 5\nPoint estimators.\nEstimators. Properties of estimators.\nAn estimator is a function of the data. If we have a parametric family with parameter θ, then an estimator\nof θ is usually denoted by θˆ .\n1.1\nUnbiasness\nLet X be our data. Let θˆ = T (X) be an estimator where T is some function.\nWe say that θˆ is unbiased for θ if Eθ[T (X)] = θ for all possible values of θ where Eθ denotes the expectation\nwhen θ is the true parameter value. Thus, the concept of unbiasness means that we are on average right.\nThe bias of θˆ is de ned by Bias(θˆ) = Eθ[θˆ] - θ. Thus, θˆ is unbiased if and only if its bias equals 0. Thus,\nsample average and sample variance are unbiased estimators of population mean and population variance\ncorrespondingly.\nThere are some cases when unbiased estimators do not exist. As an example, let X1, ..., Xn be a random\nsample from a Bernoulli(p) distribution. Suppose that our parameter of interest θ = 1/p. Let θˆ = T (X)\n∑\nbe some estimator. Then E[θˆ] =\n(x1,...,xn)∈{0,1}n T (x1, ..., xn)P {(X1, ..., Xn) = (x1, ...xn)}. We know that\n∑\n∑\nfor any (x1, ..., xn) ∈{0, 1}n , P {(X1, ..., Xn) = (x1, ...xn)} = p\nxi (1 - p)\n(1-xi) which is a polynomial of\ndegree n in p. Therefore, E[θˆ] is a polynomial of degree at most n in p. However, 1/p is not a polynomial\nat all. Hence, there are no unbiased estimators in this case.\n∑ n\nThe example above is very typical in the sense that parameter p has an unbiased estimator pˆ =\nXp,\nn\ni=1\nbut the parameter of interest is a non-linear function of p. Notice that E 1 =\n, and the bias appears from\nξ\nEξ\nthe non-linear transformation. This bias can be partially corrected by bootstrap.\n1.1.1\nBootstrap bias correction\nAnother task for which the bootstrap is used is bias-correction. Suppose, EZ = μ and, we're interested in\na non-linear function of μ, say θ = g(μ). Here Z may be a random variable coming from transformations of\n∑ n\nobserved: Zi = h(Xi). We do have an unbiased estimate of μ, say μˆ = Z =\nZi. We may try to use\nn\ni=1\nthis in order to estimate θ : θˆ = g(Z ). Estimator θˆ is reasonable but is biased unless g() is linear. The bias\nis Bias = Eθˆ - g(μ). We can estimate the bias using the bootstrap:\n1. For each b = 1, ..., B generate a bootstrap sample, {Z∗ } from set {Z1, ..., Zn} with replacement;\nib\n\n∗\n∑ n\n2. Calculate Z =\nZ∗ ;\nb\nn\ni=1\nib\n∗\n3. Estimate θ∗ = g( z );\nb\nb\n∑B\n4. Bias ∗ =\nθ∗ - θˆ ≈ Bias.\nB\nb=1 b\n5. Use θ = θˆ - Bias ∗ as your estimate.\nd2\ndg(μ)\ng(μ)\nWhy does it work? Let's denote G1(μ) =\nand G2(μ) =\n. Notice that if CLT works we have\ndμ\ndμ2\n√ n(Z - μ) ⇒ N(0, σ2), where σ2 = V ar(Zi); or Z - μ = Op(1/ √ n). Then\nz\nz\nθb- θ = g(Z) - g(μ) = G1(μ)(Z - μ) + G2(μ)(Z - μ)2 + op(\n),\nn\nσ2\nz\nBias = E(θb- θ) = G2(μ)E(Z - μ)2 = G2(μ)\n+ o(\n),\nn\nn\nand similarly\ns\nz\nBias ∗ = G2( z)\n+ op(\n).\nn\nn\nAs a result,\nBias ∗ - Bias = op(\n).\nn\nThis procedure eliminates the leading term in bias (O(1/n)), but not the whole of the bias. The remaining\nbias is of order o(1/n). Notice that in principle there was an asymptotic approach to eliminate bias as well\nz\n(as we did get the formula for the leading term\nG2(μ) σ2\n+o( 1 )). One in principle could have approximated\nn\nn\ns z\nit by 1 G2(Z)\n, but the bootstrap does this automatically.\nn\nE(X-EX)3\nExample. Assume we wish to estimate the skewness of a distribution θ =\nA natural estimate\n[V ar(X)]3/2 .\nis\nn\n1 ∑\nX )3\n(Xi -\nn\ni=1\nˆ\nθ =\n.\ns3\n\nn\nX2\nn\nThis is a non-linear function of Z = (X,\n1 ∑\ni ,\n∑\nXi\n3). As such it will most likely have bias, which\nn\ni=1\nn\ni=1\nwe can correct with the bootstrap.\n1.1.2\nE°ciency: MSE\nAnother concept that evaluates the performance of estimators is the MSE (Mean Squared Error).\nBy\nde nition, MSE(θˆ) = Eθ[(θˆ - θ)2]. Last time we showed a useful decomposition for MSE:\nMSE(θˆ) = Bias2(θˆ) + V (θˆ).\nEstimators with smaller MSE are considered to be better, meaning more e°cient. Quite often there is a\ntrade-o between the bias of the estimator and its variance. Thus, we may prefer a slightly biased estimator\nto an unbiased one if the former has much smaller variance in comparison to the latter one.\n∑ n\nσ2\nExample\nLet X1, ..., Xn be a random sample from N(μ, σ2). Let ˆ = s =\n(Xi - Xn)2/(n - 1) and\ni=1\n∑n\nσˆ2 =\n(Xi - Xn)2/n be two estimators of σ2 . We know that E[ˆσ1\n2] = σ2 . So E[ˆσ2\n2] = ((n - 1)/n)E[ˆσ1\n2] =\ni=1\n\n((n - 1)/n)σ2 , and Bias(ˆσ2\n2) = σ2/n. We also know that (n - 1)ˆσ1\n2/σ2 ∼ χ2(n - 1). What is V (χ2(n - 1))?\nLet ξ1, ..., ξn-1 be a random sample from N(0, 1). Then ξ = ξ1\n2 + ... + ξ2\n∼ χ2(n - 1). By linearity of\nn-1\nexpectation, E[ξ] = (n - 1). By independence,\nE[ξ2]\n= E[(ξ1\n2 + ... + ξn\n-1)2]\nn-1\n∑\n∑\n=\nE[ξi\n4] + 2\nE[ξi\n2ξj\n2]\ni=1\n1≤i<j≤n-1\n∑\n= 3(n - 1) + 2\nE[ξi\n2]E[ξj\n2]\n1≤i<j≤n-1\n= 3(n - 1) + (n - 1)(n - 2)\n=\n(n - 1)(n + 1),\nsince E[ξ4] = 3. So\ni\nV (ξ) = E[ξ2] - (E[ξ])2 = (n - 1)(n + 1) - (n - 1)2 = 2(n - 1).\nThus, V (ˆσ1\n2) = V (σ2ξ/(n - 1)) = 2σ4/(n - 1) and V (ˆσ2\n2) = ((n - 1)/n)2V (ˆσ1\n2) = 2σ4(n - 1)/n2 . Finally,\nMSE(ˆσ1\n2) = 2σ4/(n - 1) and\nMSE(ˆσ2\n2) = σ4/n2 + 2σ4(n - 1)/n2 = (2n - 1)σ2/n2 .\nSo, MSE(ˆσ1\n2) < MSE(ˆσ2\n2) if and only if 2/(n - 1) < (2n - 1)/n2 , which is equivalent to 3n < 1. So, for any\nn ≥ 1, MSE(ˆσ1\n2) > MSE(ˆσ2\n2) in spite of the fact that σˆ2 is unbiased.\nIn general, the idea of minimizing MSE is not in agreement with unbiasedness: one may get better\ne°ciency if we allow for some bias. Here is Stein's shrinkage idea. Assume that the parameter set Θ is\nbounded and θˆ = T (X) is an unbiased estimator of θ: ET (X) = θ. Take any xed point θ∗ ∈ Θ and shrink\nthe initial estimator towards it:\nθˆ 1 = (1 - c)T (X) + cθ ∗ .\nHere c characterize the amount of shrinkage. The new estimator is somewhat biased Bias(θˆ 1) = c(θ∗ - θ)\nbut is less dispersed V ar(θˆ 1) = (1 - c)2V ar(ˆ\nSo, we have\nΘ).\nMSE(θˆ 1) = c 2(θ ∗ - θ)2 + (1 - c)2V ar(θˆ).\nOne may calculate the derivative of MSE with respect to c at c = 0 and nd that it is negative, and thus\nsome small positive amount of shrinkage c > 0 will improve the e°ciency of the initial estimator.\n\n1.2\nAsymptotic properties.\n1.2.1\nConsistency\nImagine a thought experiment in which the number of observations n increases without bound, i.e. n →inf.\nSuppose that for each n, we have an estimator θˆ n.\nWe say that θˆ n is consistent for θ if θˆ n →p θ.\nExample\nLet X1, ..., Xn be a random sample from some distribution with mean μ and variance σ2 . Let\n∑ n\nμˆ = μˆn = Xn be our estimator of μ and s = s =\n(Xi - Xn)2/(n - 1) be our estimator of σ2 . By the\nn\ni=1\nLaw of large numbers, we know that μˆ →p μ as n →inf. In addition,\nn\n∑\ns\n=\n(Xi - Xn)2/(n - 1)\ni=1\nn\n∑\n=\n(Xi - μ)2/(n - 1) - (n/(n - 1))(Xn - μ)2\ni=1\nn\n∑\n=\n(n/(n - 1))(\n(Xi - μ)2/n) - (n/(n - 1))(Xn - μ)2\ni=1\n∑\n∑\nn\nn\nBy the Law of Large Numbers,\n(Xi - μ)2/n →p E[(Xi - μ)2 = σ2 and Xn - μ =\n(Xi - μ)/n →p\ni=1\ni=1\nE[Xi - μ] = 0. By using the Continuous Mapping Theorem, (Xn - μ)2 →p 0. In addition, n/(n - 1) →p 1.\nσ2\nSo, by the Slutsky theorem, s2 →p\n. So μˆ and s are consistent for μ and σ2 correspondingly.\n1.2.2\nAsymptotic Normality\nWe say that θˆ is asymptotically normal if there are sequences {an}inf\nand {rn}inf\nand constant σ2 such\nn=1\nn=1\nthat rn(θˆ - an) ⇒ N(0, σ2). Then rn is called the rate of convergence, an - the asymptotic mean, and σ2\n√\nthe asymptotic variance. In many cases, one can choose an = θ and rn =\nn. We will use the concept of\nasymptotic normality for con dence set construction later on. For now, let us consider an example.\nExample\nLet X1, ..., Xn be a random sample from some distribution with mean μ and variance σ2 . Let μˆ\nand s be the sample mean and the sample variance correspondingly. Then, by the Central limit theorem,\n√\nn(ˆμ - μ) ⇒ N(0, σ2). As for s ,\nn\n∑\n√\n√\n√\n√\nn(s 2 - σ2) = (n/(n - 1))[\n((Xi - μ)2 - σ2)/\nn - ( n(Xn - μ)/n1/4)2] + ( n/(n - 1))σ2\ni=1\n∑\n√\nn\nBy the Central limit theorem,\n((Xi -μ)2 -σ2)/\nn ⇒ N(0, τ 2) with τ 2 = E[((Xi -μ)2 -σ2)2]. Note that\ni=1\n√\nτ 2 = μ4-2σ2E[(Xi-μ)2]+σ4 = μ4-σ4 with μ4 = E[(Xi-μ)4]. By Slutsky theorem,\nn(Xn-μ)/n1/4 →p 0.\nIn addition, ( √ n/(n - 1))σ2 →p 0. So, by the Slutsky theorem again, √ n(s2 - σ2) ⇒ N(0, τ 2).\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n14.381 Statistical Method in Economics\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Lecture Notes",
      "title": "14.381 Statistical Method in Economics, Lec 6: Efficient Estimators, Rao-Cramer Bound",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-381-statistical-method-in-economics-fall-2018/15224973dfc1c2b4287804c8712681f7_MIT14_381F18_lec6.pdf",
      "content": "Lecture 6\nE°cient estimators. Rao-Cramer bound.\nCommon methods for constructing an estimator\n1.1\nMethod of Analogy (plug-in)\nA method of analogy is another name for the plug-in estimator we have seen before. If we are interested in\nestimating θ = θ(F ) where F denotes the population distribution, we can estimate θ by θˆ = θ(Fˆ) where Fˆ\n∫\nis some estimator of F . We have seen a number of plug-in estimators. For example, μ = EXi =\nxdF (x) is\n∫\n\na functional of the cdf. An analog estimator is μˆ =\nxdFˆ(x) = X. Another example: if we wish to estimate\n∑ n\nˆ\nθ = P {Xi ∈ A} we may use θ =\nI{Xi ∈ A}\nn\ni=1\n1.2\nMethod of Moments\nLet X1, ..., Xn be a random sample from some distribution.\nSuppose that the k-dimensional parameter\nof interest θ satis es the system of equations E[Xi] = m1(θ), E[X2] = m2(θ),..., E[Xk] = mk(θ) where\ni\ni\nm1, ..., mk are some known functions. Then the method-of-moments estimator θˆ MM of θ is the solution of the\n∑\n∑\n∑\nn\nn\nn\nabove system of equations when we substitute\nXi/n,\nX2/n,...,\nXk/n for E[Xi], E[X2],...,\ni=1\ni=1\ni\ni=1\ni\ni\nn\nE[Xi\nk] correspondingly. In other words θˆ MM solves the following system of equations: ∑\nXi/n = m1(θˆ),\ni=1\nn\nn\n∑\nX2/n = m2(θˆ),..., ∑\nXk/n = mk(θˆ). We implicitly assume here that the solution exists and is\ni=1\ni\ni=1\ni\nunique.\nExample\nLet X1, ..., Xn be a random sample from N(μ, σ2). Then E[Xi] = μ and E[Xi\n2] = μ +σ2 . Thus,\n∑\n∑\n∑\n∑\nn\nn\nn\nn\nσ2\nX2\nσ2\nX2\nμˆMM =\nXi/n and μˆ\n=\n/n. So ˆ\n=\n/n - (\nXi/n)2 .\ni=1\nMM + ˆMM\ni=1\ni\nMM\ni=1\ni\ni=1\nExample\nLet X1, ..., Xn be a random sample from an exponential distribution:\nf(x; λ) = λe-λxI{x > 0}.\nOne can calculate that EXi = λ . So one suggestion for an estimator is a solution to\nX = ˆλ\n\nor\nˆλ =\n.\nX\nWe may use higher moments as well. For example\nEX2 =\n.\ni\nλ2\nAs such, another method-of-moments estimator for λ is:\n√\nˆλ =\n∑ n\nn\ni=1\ni\nX2\nSo, the method of moments estimator is not unique and depends on the moments chosen. One can easily\nprove that if function m(θ) has a unique inverse which is continuous, then the method-of-moments estimator\nis consistent (do this for your own practice! Hint: use the continuous mapping theorem). And if the inverse\nis continuously di erentiable, we can use the delta method and prove asymptotic gaussianity (try to do this\nas well).\nThe idea of the method-of-moments is a very old one. There is a generalization of it which allows for\nmore moments than the dimensionality of the parameter and also allows for the data and parameter to be\nmixed up within the moment condition. It is called a GMM (Generalized Method of Moments) and will be\nstudied extensively later on, as the main workhorse of Econometrics.\n1.3\nMaximum Likelihood Estimator\nIn this section we consider parametric estimation. We have a parametric estimation problem when we know\nthe distribution of the data up to a nite-dimensional parameter θ (the only unknown part). We denote the\n∏n\njoint pdf of X = (X1, ..., Xn) as f(x|θ) = f(x1, ..., xn|θ) ( for i.i.d. sample we will havef(x|θ) =\nf1(xi|θ),\ni=1\nwhere f1(xi|θ) is the pdf of one observation). That is, if we knew θ we have known the exact distribution of\nthe data.\nLet x = (x1, ..., xn) denote the realization of X = (X1, ..., Xn). By de nition, the maximum likelihood\nestimator θˆ ML of θ is the value that maximizes f(x|θ), i.e.\nˆθML = arg max f(x1, ..., xn|θ).\nθ∈Θ\nThe function f(x|θ), when considered as a function of θ for xed values x = (x1, ..., xn), is called the likelihood\nfunction. It is usually denoted by L(θ|x). Thus, the maximum likelihood estimator maximizes the likelihood\nfunction, which explains the name of this estimator. Since log(x) is increasing in x, it is easy to see that\nˆθML also maximizes l(θ|x1, ..., xn) = log L(θ|x1, ..., xn). Function l(θ|x1, ..., xn) is called the log-likelihood.\nIf l(θ|x1, ..., xn) is di erentiable in θ, then θˆ ML satis es rst order condition (FOC): dl (θˆ ML|x1, ..., xn) = 0.\ndθ\n∑n\nIf the data comes from an i.i.d. sample then it is equivalent to\n∂ log f1(xi|θˆ ML)/∂θ = 0. The reason\ni=1\nwe took the log of the likelihood function now can be seen: it is easier to take the derivative of the sum than\nthe derivative of the product. Function S(θ|x) = ∂ log f(x|θ)/∂θ is called the score. Thus, θˆ ML solves the\n\nequation S(θ|x) = 0.\nExample\nLet X1, ...Xn be a random sample from N(μ, σ2). Then\n√\nlog f1(θ|xi) = - log 2π - (1/2) log σ2 - (xi - μ)2/(2σ2),\nwhere θ = (μ, σ2). So\nn\n√\n∑\nl(θ|x1, ..., xn) = -n log 2π - (n/2) log σ2 -\n(xi - μ)2/(2σ2)\ni=1\nFOCs are\nn\n∑\n∂l/∂μ =\n(xi - μ)/σ2 = 0,\ni=1\nn\n∑\n∂l/∂σ2 = -n/(2σ2) +\n(xi - μ)2/(2σ4) = 0.\ni=1\n∑n\nSo μˆML = Xn and σˆ2\n=\n(Xi - Xn)2/n.\nML\ni=1\nExample\nAs another example, let X1, ..., Xn be a random sample from U[0, θ].\nThen f1(xi|θ) = 1/θ\nif x ∈ [0, θ] and 0 otherwise.\nSo f(x1, ...xn|θ) = 1/θn if 0 ≤ x(1) ≤ x(n) ≤ θ and 0 otherwise.\nThus,\nL(θ|x) = (1/θn)I{θ ≥ x(n)}I{x(1) ≥ 0}. We conclude that θˆ ML = X(n).\nFisher information\nLet f(x|θ) with θ ∈ Θ be some parametric family.\nFor given θ ∈ Θ, let Suppθ = {x : f(x|θ) > 0}.\nSuppθ is usually called the support of distribution f(x|θ). Assume that Suppθ does not depend on θ. As\nbefore, l(θ|x) = log f(x|θ) is called the log-likelihood function. Assume that l(θ|x) is twice continuously\n∂2l(θ|x)\ndi erentiable in θ for all x ∈ Supp and\n∂θ2\nis bounded above by some function g(x) such that Eg(X) < inf\nfor random variable X with distribution f(x|θ). Then:\nDe nition 1. I(θ) = Eθ[(∂l(θ|X)/∂θ)2] is called Fisher information.\nFisher information plays an important role in maximum likelihood estimation. The theorem below gives\ntwo information equalities:\nTheorem 2. In the setting above,\n(1) Eθ[∂l(θ|X)/∂θ] = 0\n(2) I(θ) = -Eθ[∂2l(θ|X)/∂θ2].\nProof. Since l(θ|x) is twice di erentiable in θ, f(x|θ) is twice di erentiable in θ as well. Let us di erentiate\nthe following identity with respect to θ:\n∫\nf(x|θ)dx ≡ 1.\n\nThe restrictions on dominance by g(x) allow us to interchange di erentiation and integration signs below:\n∫ ∂f(x|θ) dx = 0 for all θ ∈ Θ.\n∂θ\nThe second di erentiation with respect to θ yields\n∫ ∂2f(x|θ) dx = 0 for all θ ∈ Θ.\n(1)\n∂θ2\nNow notice that\n∂l(θ|x)\n∂ log f(x|θ)\n∂f(x|θ)\n=\n=\n∂θ\n∂θ\nf(x|θ)\n∂θ\nand\n(\n)2\n∂2l(θ|x)\n∂f(x|θ)\n∂2f(x|θ)\n= -\n+\n.\n∂θ2\nf 2(x|θ)\n∂θ\nf(x|θ)\n∂θ2\nThe former equality yields\n[\n]\n[\n]\n∫\n∫\n∂l(θ|X)\n∂f(X|θ)\n∂f(x|θ)\n∂f(x|θ)\nEθ\n= Eθ\n=\nf(x|θ)dx =\ndx = 0,\n∂θ\nf(X|θ)\n∂θ\nf(x|θ)\n∂θ\n∂θ\nwhich is our rst result. The latter equality yields\n[\n]\n∫\n(\n)2\n∂2l(X, θ)\n∂f(x|θ)\nEθ\n= -\ndx,\n∂θ2\nf(x|θ)\n∂θ\nhere the second term disappears due to equation (1). So,\n[(\n)2]\n∫(\n)2\n∂l(θ|X)\n∂f(x|θ)\nI(θ) =\nEθ\n=\nf(x|θ)dx\n∂θ\nf(x|θ)\n∂θ\n(\n)2\n[\n]\n∫\n∂f (x|θ)\n∂2l(θ|X)\n=\ndx = -Eθ\n.\nf (x|θ)\n∂θ\n∂θ2\nExample\nLet us calculate Fisher information for one random draw from the N(μ, σ2) distribution where\nσ2 is known. Thus, our parameter θ = μ. The density of a normal distribution is f(x|μ) = exp(-(x -\n√\nμ)2/(2σ2))/ 2πσ2 .\nThe log-likelihood is l(μ|x) = - log(2πσ2)/2 - (x - μ)2/(2σ2).\nSo ∂l(μ|x)/∂μ =\n(x - μ)/σ2 and ∂2l(μ|x)/∂μ2 = -1/σ2 . So -Eθ[∂2l(μ|X)/∂μ2] = 1/σ2 . At the same time,\nI(θ) = Eμ[(∂l(μ|X)/∂μ)2] = Eμ[(X - μ)2/σ4] = 1/σ2 .\nSo, as was expected in view of the theorem above, I(θ) = -Eμ[∂2l(μ|X)/∂μ2] in this example.\nExample\nLet us calculate Fisher information for one random draw from a Bernoulli(θ) distribution. Note\nthat a Bernoulli distribution is discrete. So we use a probability mass function (pms) instead of a pdf. The\n\npms of Bernoulli(θ) is f(x|θ) = θx(1 - θ)1-x for x ∈{0, 1}. The log-likelihood is l(θ|x) = x log θ + (1 -\nx) log(1 - θ). So ∂l(θ|x)/∂θ = x/θ - (1 - x)/(1 - θ) and ∂2l(θ|x)/∂θ2 = -x/θ2 - (1 - x)/(1 - θ)2 . So\nEθ[(∂l(θ|X)/∂θ)2]\n= Eθ[(X/θ - (1 - X)/(1 - θ))2]\n= Eθ[X2/θ2] - 2Eθ[X(1 - X)/(θ(1 - θ))] + Eθ[(1 - X)2/(1 - θ)2]\n= Eθ[X/θ2] + Eθ[(1 - X)/(1 - θ)2]\n=\n1/(θ(1 - θ)),\nsince x = x , x(1 - x) = 0, and (1 - x) = (1 - x)2 if x ∈{0, 1}. At the same time,\n-Eθ[∂2l(θ|X)/∂θ2]\n= Eθ[X/θ2 + (1 - X)/(1 - θ)2]\n= θ/θ2 + (1 - θ)/(1 - θ)2\n=\n1/θ + 1/(1 - θ)\n=\n1/(θ(1 - θ)).\nSo I(θ) = -Eθ[∂2l(θ|X)/∂θ2], as it should be.\n2.1\nInformation for a random sample\nLet us now consider Fisher information for a random sample. Let X = (X1, ..., Xn) be an i.i.d. random\n∏n\nsample from distribution f1(xi|θ). Then the joint pdf is f(x) =\nf1(xi|θ) where x = (x1, ..., xn). The\ni=1\n∑n\njoint log-likelihood is l(x, θ) =\nl1(xi, θ). So Fisher information for the sample X is\ni=1\n[\n]\nn [\n]\n∑\n∂2ln(θ|X)\n∂2l1(θ|Xi)\nI(θ) = -Eθ\n= -Eθ\n= nI1(θ).\n∂θ2\n∂θ2\ni=1\nHere I1(θ) denotes Fisher information for one random draw from the distribution f1(xi|θ).\nRao-Cramer bound\nAn important question in the theory of statistical estimation is whether there is a nontrivial bound such\nthat no estimator can be more e°cient than this bound. The theorem below is a result of this sort:\nTheorem 3 (Rao-Cramer bound). Let X = (X1, ..., Xn) be a random sample from distribution f(x|θ) with\ninformation I(θ). Let W (X) be an estimator of θ such that\nd\n∫\n∂f (x|θ)\n(1)\nEθ[W (X)] =\nW (x)\ndx, where x = (x1, ...xn)\ndθ\n∂θ\n(2) V ar(W ) < inf.\nThen\n(\n)2\nd\nV ar(W ) ≥\nEθ[W (X)]\n.\ndθ\nI(θ)\n\nIn particular, if W is unbiased for θ, then V ar(W ) ≥\n=\n.\nI(θ)\nnI1(θ)\n[\n]\n∂l(θ|X)\nProof. The rst information equality gives ES(θ|X) = Eθ\n= 0. So,\n∂θ\ncov(W (X), S(θ|X)) =\n[\n]\n∂l(θ|X)\nE W (X)\n∂θ\n∫\n=\n∫\n∂l(θ|x)\nW (x)\nf(x|θ)dx\n∂θ\n=\n=\n∫\n∂f(x|θ)\nW (x)\n·\nf(x|θ)dx\n∂θ\nf(x|θ)\n∂f(x|θ)\nW (x)\ndx\n∂θ\nd\n=\nEθ[W (X)].\ndθ\nBy the Cauchy-Schwarz inequality,\n(cov(W (X), S(θ|X))2 ≤ V ar(W (X))V ar(S(θ|X)) = V ar(W (X))I(θ).\nThus,\n(\n)2\nd\nV ar(W (X)) ≥\nEθ[W (X)]\n/I(θ).\ndθ\nIf W is unbiased for θ, then Eθ[W (X)] = θ, dEθ[W (X)]/dθ = 1, and V ar(W (X)) ≥ 1/I(θ).\nExample\nLet us calculate the Rao-Cramer bound for random sample X1, ..., Xn from a Bernoulli(θ) dis\ntribution. We have already seen that I1(θ) = 1/(θ(1 - θ)) in this case. So Fisher information for the sample\nis I(θ) = n/(θ(1 - θ)). Thus, any unbiased estimator of θ, under some regularity conditions, has a variance\nn\nno smaller than θ(1 - θ)/n. On the other hand, let θˆ =\n= ∑\nXi/n be an estimator of θ. Then\nXn\ni=1\nEθ[θˆ] = θ, i.e. θˆ is unbiased, and V (θˆ) = θ(1 - θ)/n which coincides with the Rao-Cramer bound. Thus, Xn\nis the uniformly minimum variance unbiased (UMVU) estimator of θ. The word uniformly in this situation\nmeans that Xn has the smallest variance among unbiased estimators for all θ ∈ Θ.\nExample\nLet us now consider a counterexample to the Rao-Cramer theorem. Let X1, ..., Xn be a random\nsample from U[0, θ]. Then f(xi|θ) = 1/θ if xi ∈ [0, θ] and 0 otherwise. So l(xi, θ) = - log θ if xi ∈ [0, θ].\nThen ∂l/∂θ = -1/θ and ∂2l/∂θ2 = 1/θ2 . So I(θ) = 1/θ2 while -Eθ[∂2l(Xi, θ)/∂θ2] = -1/θ2 = I(θ). Thus,\nthe second information equality does not hold in this example. The reason is that support of the distribution\ndepends on θ in this example. Moreover, consider an estimator θˆ = ((n + 1)/n)X(n) of θ. Then Eθ[X(n)] = θ\nand\nV (θˆ) = ((n + 1)2/n2)V (X(n)) = θ2/(n(n + 2))\nas we saw when we considered order statistics. So θˆ is unbiased, but its variance is smaller than 1/In(θ) =\nθ2/n2 . Thus, the Rao-Cramer theorem does not work in this example either. Again, the reason is that the\nRao-Cramer theorem assumes that support is independent of parameter.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n14.381 Statistical Method in Economics\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    },
    {
      "category": "Lecture Notes",
      "title": "14.381 Statistical method in Economics, Lec 7: Maximum Likelihood Estimation",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-381-statistical-method-in-economics-fall-2018/2eb7af31864f5f340b0b62ec8d3368ab_MIT14_381F18_lec7.pdf",
      "content": "Lecture 7\nMaximum Likelihood Estimation.\nMLE\nLet f(·|θ) with θ ∈ Θ be a parametric family. Let X = (X1, ..., Xn) be a random sample from distribution\n∏n\nf1(·|θ0) with θ0 ∈ Θ. Then the joint pdf is f(x|θ) =\nf1(xi|θ) where x = (x1, ..., xn). The log-likelihood\ni=1\n∑n\nis l(θ|x) =\nlog f1(xi|θ). The maximum likelihood estimator is, by de nition,\ni=1\nˆθML = arg max l(θ|x).\nθ∈Θ\nThe FOC is\nn\n∑\n∂l1(θˆ ML|xi) = 0.\nn\n∂θ\ni=1\nNote that the rst information equality is E[∂l1(θ0|Xi)] = 0. Thus MLE is the method of moments estimator\ncorresponding to the rst information equality. So we can expect that the MLE is consistent. Indeed, the\ntheorem below gives the consistency result for MLE:\nTheorem 1 (MLE consistency). In the setting above, assume that (1) θ0 is identi able, i.e. for any θ =\nθ0,\nthere exists x such that f(x|θ) =\nf(x|θ0), (2) the support of f(·|θ) does not depend on θ, and (3) θ0 is an\ninterior point of parameter space Θ. Then θˆ ML →p θ0.\nThe proof of MLE consistency will be given in 14.382 and 14.385. What the proof does, it shows that\nfunction g(θ) = Eθ0 l1(θ|Xi) (here Xi ∼ f1(xi|θ0)) is maximized at θ = θ0 and random process\nl(θ|X)\nn\nconverges to the function g(θ) in a uniform manner in probability. Then it argues that the maximizer of the\nˆ\nprocess θML will converge to θ0.\nOnce we know that the estimator is consistent, we can think about the asymptotic distribution of the\nestimator. The next theorem gives the asymptotic distribution of MLE:\nTheorem 2 (MLE asymptotic normality). In the setting above, assume that conditions (1)-(3) in the\nMLE consistency theorem hold. In addition, assume that (4) f1(xi|θ) is thrice di erentiable with respect\nto θ and we can interchange integration with respect to x and di erentiation with respect to θ, and (5)\n|∂3 log f1(xi|θ)/∂θ3| ≤ M(x) and E[M(Xi)] < inf. Then\n√ n(θˆ ML - θ0) ⇒ N(0, I-1(θ0))\n\n∂l(θˆML,x)\nProof. This is a sketch of the proof as it misses and important step. By de nition,\n= 0. By the\n∂θ\nTaylor theorem with a remainder, there is some random variable θ with value between θ0 and θˆ ML such that\n∂l(θˆ ML)|X\n∂l(θ0|X)\n∂2l(θ |X)\n=\n+\n(θˆ ML - θ0).\n∂θ\n∂θ\n∂θ2\nSo,\n1 ∂l(θ0|X)\n- √ n\n∂θ\n√ n(θˆ ML - θ0) =\n.\n1 ∂2l(θ |X)\nn\n∂θ2\nSince θˆ ML →p θ0 and θ is between θ0 and θˆ ML, θ →p θ0 as well. From θ →p θ0, one can prove that\n1 ∂2l(θ |X)\n1 ∂2l(θ0|X)\n-\n= op(1).\nn\n∂θ2\nn\n∂θ2\nWe will not discuss this result here since it requires knowledge of the concept of asymptotic equicontinuity\nwhich we do not cover in this class. You will learn it in 14.385. Note, however, that this result does not\nfollow from the Continuous mapping theorem since we have a sequence of random functions l(θ|X) instead\nof just one non-random function. Suppose we believe in this result. Then, by the Law of large numbers,\nn\n[\n]\n∑\n1 ∂2l(θ0|X)\n∂2 log f1(Xi|θ0)\n∂2 log f1(Xi|θ0)\n=\n→p E\n= -I1(θ0).\nn\n∂θ2\nn\n∂θ2\n∂θ2\ni=1\n[\n]\n[\n]\n∂ log f1(Xi|θ0)\n∂ log f1(Xi|θ0)\nNext, by the rst information equality, E\n= 0 while V ar\n= I1(θ0). Thus,\n∂θ\n∂θ\nby the Central limit theorem,\nn\n∑\n1 ∂l(θ0|X)\n∂ log f1(Xi|θ0)\n√\n= √\n⇒ N(0, I(θ0)).\nn\n∂θ\nn\n∂θ\ni=1\nFinally, by the Slutsky theorem,\n√ n(θˆ ML - θ0) ⇒ N(0, I-1(θ0)).\nOne interpretation of MLE asymptotics is that the MLE is asymptotically e°cient (hit Rao-Cramer\nbound in very large samples).\nExample Let X1, ..., Xn be a random sample from a distribution with pdf f(x|λ) = λ exp(-λx). This\n∂l1(λ|xi)\ndistribution is called exponential. Its log-likelihood for one draw is l1(λ|xi) = log λ - λxi. So\n=\n∂λ\n∂2l1(λ|xi)\n1/λ - xi and\n= -1/λ2 . So Fisher information is I1(λ) = 1/λ2 . Let us nd the MLE for λ. The\n∂λ2\n∑\n∑\nn\nn\nn\nlog-likelihood for the whole is l(θ|x) = n log λ - λ\ni=1 xi. The FOC is λˆ ML -\ni=1 xi = 0. So λˆ ML =\n.\nXn\nIts asymptotic distribution is given by √ n(λˆ ML - λ) ⇒ N(0, λ2).\n\nInference using MLE\nWe will have a longer discussion about how to estimate the asymptotic variance of the MLE (I1(θ0))-1 later\nwhen we will discuss asymptotic tests. Right now I want to mention several suggestions.\nFirst of all, if I1(θ) is a continuous function in θ (which is needed for asymptotic results), then given that\n(\n)-1\n(\n)-1\nθˆ ML is consistent for θ0, the quantity\nI1(θˆ ML)\nis consistent for I1\n-1(θ0)\n.\nSecond, by de nition of Fisher information, it equals to the expectation of either negative second deriva\ntive of the likelihood or of the squared score. Instead of taking expectation one may approximate it by taking\naverages. For example\nn\n1 ∑ ∂2l1(θˆ|Xi)\nˆI = - n\n∂θ2\ni=1\nwill be a consistent estimator of the Fisher information.\nThe third idea to be used in this context is parametric bootstrap. Assume θˆ ML is the MLE we obtained\nfrom our sample of size n. For b = 1, ..., B do the following:\n- Simulate sample X∗ = (X∗ , ..., X∗ ) as i.i.d. draws from f1(xi|θˆ ML) (that is, assuming that ˆ\nis\nb\n1b\nnb\nθML\nthe true parameter value).\n- Find MLE using sample Xb\n∗ , denote it θb\n∗ .\nCalculate the sample variance of (θ1\n∗, ..., θ∗ ), it gives the bootstrap approximation to (nI1(θ0))-1 . You may\nB\nalso do bootstrap- bias correction using similar procedure.\nWhen MLE asymptotic theory fails us...\nExample A word of caution. For asymptotic normality of MLE, we should have common support. Let\nus see what might happen otherwise. Let X1, ..., Xn be a random sample from U[0, θ]. Then θˆ ML = X(n).\nSo √ n(θˆ ML - θ) is always nonpositive. So it does not converge to mean zero normal distribution. In fact,\nE[X(n)] = (n/(n + 1))θ and V (X(n)) = θ2n/((n + 1)2(n + 2)) ≈ θ2/n2 . On the other hand, if the theorem\nworked, we would have V (X(n)) ≈ 1/(nI(θ)). The MLE happens to be super-consistent here, means it\n√\nconverges to the true value at a faster speed than the regular parametric speed of 1/\nn.\nExample Now, let us consider what might happen if the true parameter value θ0 were on the boundary\nof Θ. Let X1, ..., Xn be a random sample from distribution N(μ, 1) with μ ≥ 0. As an exercise, check that\n√\nμˆML = Xn if Xn ≥ 0 and 0 otherwise. Suppose that μ0 = 0. Then\nn(ˆμML - μ0) is always nonnegative.\nSo it does not converge to mean zero normal distribution.\nExample Finally, note that it is implicitly assumed both in the consistency and asymptotic normality\ntheorems that parameter space Θ is xed, i.e. independent of n. In particular, the number of parameters\nshould not depend on n. Indeed, let\n(\n)\n((\n) (\n))\nX1i\nμi\nσ2\nXi =\n∼ N\n,\nX2i\nμi\nσ2\n\nfor i = 1, ..., n, and X1, ..., Xn be mutually independent. One can show that if the sample size (n) increases\nto in nity, the MLE for σ2 is inconsistent in this case, though a consistent estimator for σ2 exists.\nWhat is interesting, though we won't show it here is that a bootstrap does not help in this cases, that\nis, the bootstrap approximation to the distribution of θˆ ML is not close to the true nite-sample distribution\nof θˆ ML.\nPseudo-MLE\nLet us have a sample X = (X1, ..., Xn) i.i.d from some distribution. We do not know what distribution it\nis, let's assume it has pdf g(xi). But we wrongly assumed a speci c parametric family, that is, we assumed\nXi ∼ f1(xi|θ). What would happen if we do MLE. Apparently, MLE will be estimating a pseudo-true\nparameter value θ0 with minimizes in some sense the distance between g(·) and family f(·|θ). In particular:\n∫\nθ0 = arg max\nlog[f1(xi|θ)]g(xi)dxi = arg max E log f1(Xi|θ).\nθ\nθ\nParameter θ0 may be of interest or may be not. Under some regularity condition θˆ ML →p θ0, and in most\nparts the logic of the proof of theorem about normality will hold. However, the the information equality\nwould fail. De ne\n[(\n)2]\n∂ log f1(Xi|θ0)\nΣ1 = E\n,\n∂θ0\n[\n]\n∂2 log f1(Xi|θ0)\nΣ2 = -E\n,\n∂θ0\nwhere expectations in both cases are taken assuming that Xi ∼ g(·). If g is not in the parametric family,\nthen in general Σ1 = Σ2. But using the logic of the proof, we can prove that\n√ n(θˆ ML - θ0) ⇒ N(0, Σ-1Σ1Σ-1)\nThis asymptotic variance Σ-1Σ1Σ-1 is often called White's due to White's (1980) paper and thus White's\nstandard errors.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n14.381 Statistical Method in Economics\nFall 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
    }
  ]
}