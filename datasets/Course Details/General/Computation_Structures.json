{
  "course_name": "Computation Structures",
  "course_description": "6.004 offers an introduction to the engineering of digital systems. Starting with MOS transistors, the course develops a series of building blocks — logic gates, combinational and sequential circuits, finite-state machines, computers and finally complete systems. Both hardware and software mechanisms are explored through a series of design examples.\n6.004 is required material for any EECS undergraduate who wants to understand (and ultimately design) digital systems. A good grasp of the material is essential for later courses in digital design, computer architecture and systems. The problem sets and lab exercises are intended to give students “hands-on” experience in designing digital systems; each student completes a gate-level design for a reduced instruction set computer (RISC) processor during the semester.",
  "topics": [
    "Engineering",
    "Computer Science",
    "Computer Design and Engineering",
    "Electrical Engineering",
    "Digital Systems",
    "Mathematics",
    "Computation",
    "Engineering",
    "Computer Science",
    "Computer Design and Engineering",
    "Electrical Engineering",
    "Digital Systems",
    "Mathematics",
    "Computation"
  ],
  "syllabus_content": "This course makes use of Athena, MIT's UNIX-based computing environment. OCW does not provide access to this environment.\n\nCourse Meeting Times\n\nLectures: 2 sessions / week, 1 hour / session\n\nRecitations: 2 sessions / week, 1 hour / session\n\nCourse Description\n\n6.004 offers an introduction to the engineering of digital systems. Starting with MOS transistors, the course develops a series of building blocks -- logic gates, combinational and sequential circuits, finite-state machines, computers and finally complete systems. Both hardware and software mechanisms are explored through a series of design examples.\n\n6.004 is required material for any EECS undergraduate who wants to understand (and ultimately design) digital systems. A good grasp of the material is essential for later courses in digital design, computer architecture and systems. Before taking 6.004, students should feel comfortable using computers; a rudimentary knowledge of programming language concepts (6.001) and electrical fundamentals (6.002) is assumed.\n\nThe problem sets and lab exercises are intended to give students \"hands-on\" experience in designing digital systems; each student completes a gate-level design for a reduced instruction set computer (RISC) processor during the semester. Access to workstations as well as help from the course staff is provided in the lab but it is possible to complete the assignments using Athena machines or one's home computer. Students are encouraged to get help from others in understanding the material, but the designs and measurements they hand in must be their own work.\n\n6.004 Learning Objectives and Measurable Outcomes (\nPDF\n)\n\nPrerequisites\n\nStudents should feel comfortable using computers. A rudimentary knowledge of programming language concepts (C language or 6.001) and electrical fundamentals (6.002) is assumed. Each student must have an Athena account to access the software used to complete the lab assignments.\n\nText\n\nThere is no required text for the course this semester. Readings for some of the course material will be available on-line.\n\nProblem Sets\n\nThere are no weekly graded problem sets. Instead there are on-line tutorial problems with answers you can use to test your understanding of the material. The recitations give you a chance to work on these problems with the help of the course staff and to ask any questions that you may have.\n\nCollaboration\n\nThe assignments are intended to help you understand the material and should be done individually. You are welcome to get help from others but the work you hand in must be your own. Copying another person's work or allowing your work to be copied by others is a serious academic offense and will be treated as such. We do spot-check submissions to the on-line checkoff system for infractions of the collaboration policy. So please don't tempt fate by submitting someone else's work as your own; it will save us all a lot of grief.\n\nLabs\n\nThere are eight lab assignments due at various times during the term and an optional design project at the end of the term. Completing each part of a lab earns points that count toward your final grade. Points are determined during a short interview about each lab with a member of the course staff. Note that you can submit your work for a lab more than once, for example, as you complete each part. After completing the work on some of the labs, you'll be presented with some on-line lab questions to answer (these are different than the tutorial questions mentioned above), and you'll need to schedule a short lab checkoff meeting for each lab with a member of the course staff. This meeting can happen after the lab's due date but to receive full credit it must be completed within one week of the due date.\n\nYou must have a non-zero score for each required lab and all on-line lab questions must be checked-off as a prerequisite for passing the course. A missing required lab (i.e., a lab with a score of 0) will result in a failing grade; incompletes will not be given for unfinished laboratory work.\n\nThe lab gets crowded just before an assignment is due so plan accordingly. The lab will be staffed by the course staff during the late afternoon and evening Monday through Thursday, and during the afternoon on Friday.\n\nLate policy for labs: The on-line system will give you 50% of any points earned for submissions after the due date. So if your first submittal is late, you get 50% of the points. But if you submitted on-time for 15 points, and then late for 25 points, you'll get 20 points total for the lab. Note that points reported by JSim/BSim at check-in are for on-time submittals; you can check your on-line status page to see how many points count toward your total. This will be reported as \"0\" until you complete your checkoff meeting.\n\nLate policy for checkoffs: The on-line system will allow only 50% of any points earned by your lab (including any late penalties you incurred) if you don't complete your checkoff before the checkoff deadline. So if you miss both the lab deadline and the checkoff deadline, you'll only get 25% of the total points.\n\nQuizzes\n\nThere are five fifty-minute, closed-book quizzes. The questions will be similar (perhaps identical!) to the tutorial problems and will ask you to provide short, written answers and/or explanations. The quizzes are scheduled roughly every three weeks during recitation.\n\nTo ensure everyone has a seat, please attend your assigned section on quiz days. If exceptional circumstances make it impossible to take a quiz at your assigned time, please contact your recitation instructor before the quiz to see if other arrangements can be made. Requests for make-ups after the quiz has been given are unlikely to be successful.\n\nThere is no final exam.\n\nGrading\n\nThe final grade is determined by performance on the quizzes (25 points/quiz, 125 points total), the labs, and the design project (75 points total). In addition, you must have a non-zero score for each of the required labs and all the on-line lab questions must be checked off as a prerequisite for passing the course. A missing required lab will result in a failing grade; incompletes will not be given for unfinished laboratory work.\n\nOnce your combined score has been computed as explained above, here's how grades will be assigned:\n\nGRADES\n\nREQUIREMENTS\n\nA\n\n170 <= total points\n\nB\n\n150 <= total points < 170\n\nC\n\n130 <= total points < 150\n\nD\n\n110 <= total points < 130\n\nF\n\ntotal points < 110, missing required lab",
  "files": [
    {
      "category": "Exam",
      "title": "Quiz #1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/f445e9aecd4f6fa0446b6417eac0068a_MIT6_004s09_quiz01.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.004 Computation Structures\nSpring 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nM A S S A C H U S E T T S I N S T I T U T E O F T E C H N O L O G Y\nDEPARTMENT OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE\n6.004 Computation Structures\nSpring 2009\nQuiz #1: February 20, 2009\nName\nAthena login name\nScore\nProblem 1 (6 points): Quickies and Trickies\n(A) You are trying to guess a card picked at random from a standard 52-card deck. Sam tells\nyou the card is a spade; Nora tells you it's not an ace; Rita tells you it's a seven.\nWhat is the total amount of information about the card given by Sam, Nora, and\nRita? You may give your answer as either a number or a formula.\nBits of info (number or formula): ____________________\n(B) Give the four-bit two's complement representation of -6.\n4-bit 2's complement representation of -6: ____________________\n(C) Let w be an eight-bit code word. How many 8-bit code words are there whose Hamming\ndistance from w is exactly one?\nNumber of 8-bit code words distance 1 from w: ____________________\n(D) In Lecture 4, you saw that a 2k -input XOR could be implemented as a k-level tree of 2\ninput gates. Can every 2k-input, 1-output combinational function be implemented as\na k-level tree of (not necessarily identical) 2-input 1-output gates?\nCircle one:\nYES\nNO\n(E) The propagation delay specified for an inverter is less than its contamination delay\nCircle one:\nNEVER\nSOMETIMES\nALWAYS\n(F) A single CMOS gate, consisting of an output node connected to a single pullup circuit\ncontaining zero or more PFETs and a single pulldown circuit containing zero or more\nNFETs (as described in lecture), computes F(A, B, C, D).\nIt is observed that F(1,0,1,1) = 1. What can you say about the value of F(1,0,0,0)?\n(circle one): F(1,0,0,0) = 1 ... 0 ... can't tell\n6.004 Spring 2009\n- 1 of 4 -\nQuiz #1\n\nProblem 2. (5 points): Variable-length Codes\nGradeStore, Inc makes storage devices designed to hold only letter grades issued in college\ncourses. Each grade is one of A, B, C, D, F, or X (for dropped courses), and the grades are\ndistributed according to the following table:\nGrade\nProbability of occurrence\nA\n26%\nB\n34%\nC\n11%\nD\n10%\nF\n8%\nX\n11%\n(A) (1 point) How many bits would be required to store each grade using a fixed-length code?\nNumber of bits: ____________________\n(B) (3 points) Devise a Huffman code to optimize the storage of grades.\nEncoding for A: ____________________\nEncoding for B: ____________________\nEncoding for C: ____________________\nEncoding for D: ____________________\nEncoding for F: ____________________\nEncoding for X: ____________________\nA well-heeled customer wants GradeStore to customize their encoding to include a seventh grade,\nM, to identify students whose work was incomplete due to their being hit by a meteor. This is a\nvery rare event, and GradeStore expects its probability to be on the order of 0.00000000007%.\n(C) (1 point) How many bits would Huffman encoding allocate to the representation of an M\ngrade?\nBits for encoding of M grade: ____________________\n6.004 Spring 2009\n- 2 of 4 -\nQuiz #1\n\nProblem 3. (7 points): Saga of Helena Handbasket\nHelena Handbasket, who barely passed 6.004, has been hired to design CMOS gates\nfor Hapless Logic, Inc. Remembering something about PFETs in pullups and NFETs\nin pulldowns, her first design was a 3-input device whose circuit is shown to the right.\nHelena's intent was that these devices compute some useful 3-input Boolean function\nD=F(A,B,C); unfortunately, the devices don't seem to work as planned. To make\nmatters worse, she had 1,000,000,000 of the devices fabricated (thinking that the\norder to the fab must, of course, be in binary rather than decimal). The defective\ndevices, known within HLI as the Gates of Helena, have become a subject of ridicule.\nHelena has brought you in as a consultant. Your first task is to figure out how badly Helena blew\nthe design of her 3-input logic device - in particular, whether it drives the output D to a valid\nlogic level for every combination of the inputs A, B, and C.\n(A) (2 points) Are there logical (0/1) values of A, B, and C for which D is not driven at all?\nIf so, give values for A, B, and C that leave D undriven; else write NONE.\nA, B, C values for undriven D, or \"NONE\": ____________________\n(B) (2 points) Are there logical values of A, B, and C for which D is pulled down and up\nsimultaneously? If so, values for A, B, and C that cause such a conflict; else write NONE.\nA, B, C values for output conflict, or \"NONE\": ____________________\nNora Nanda, Helena's assistant, suggests that the devices might be salvaged by using them to\ncompute useful functions of fewer than 3 inputs. She proposes that a two-input function of X and\nY, for example, might be computed by connecting each of the three inputs A, B, and C to either\nX, Y, or the logical constants 0 (ground) or 1 (vdd), and reading the output on D.\n(C) (1 point) Can Nora's approach be used to compute NAND of X and Y? If so, choose\nvalues (X, Y, 0, or 1) for each of A, B, and C such that D is XY ; else circle NO.\nChoose A, B, C values or circle NO: A=____; B=____; C=____; or NO\n(D) (1 point) Can Nora's approach be used to compute NOR of X and Y? If so, choose\nvalues (X, Y, 0, or 1) for each of A, B, and C such that D is X + Y ; else circle NO.\nChoose A, B, C values or circle NO: A=____; B=____; C=____; or NO\n(E) (1 point) Can Nora's approach be used to compute X + Y ? If so, choose values (X, Y, 0,\nor 1) for each of A, B, and C such that D is X + Y ; else circle NO.\nChoose A, B, C values or circle NO: A=____; B=____; C=____; or NO\n6.004 Spring 2009\n- 3 of 4 -\nQuiz #1\n\nProblem 4 (7 points): Organic Logic\nOrganic Logic, Inc, is a Cambridge startup that has developed an interesting device built using\nunidentified organic sludge from the depths of the Charles River; they would like to use it to\nperform logic functions. Their device, termed a Slime Gate, has two inputs A and B, and one\noutput C (in addition to power and ground connections):\nWith a 3-volt power supply, they have noted that Slime Gates reliably behave as follows:\n-\nThe output C is always in the range 0 volts < C < 3 volts.\n-\nWhen both A and B have been at less than 1 volt for a\nnanosecond or more, the voltage at C is greater than 2.5\nvolts.\n-\nWhen either or both of A and B have been at more than 2\nvolts for at least a nanosecond, C carries a voltage of less\nthan 0.5 volts.\nAside from the above constraints, the voltage at C is generally unpredictable; it varies widely\nbetween individual Slime Gate devices.\nAs an O.L.I. consultant, you have proposed the circuit to the\nright as an inverter in the evolving family of Slime Gate logic:\n(A) (3 points) Give logic representation parameters\nyielding maximum noise margins and for which\nyour diagram depicts a valid inverter.\nVOL: ________; VIL: ________; VIH: ________; VOH: ________\n(B) (2 points) Give appropriate specifications for propagation and contamination delays\nfor this inverter:\ntpd: __________ ns; tcd: __________ ns\n(C) (1 point) Suppose the Slime Gate is used as a\n2-input logic gate in this same family, as\nshown to the right. What, if any, function of\nP and Q is represented by the output R?\nBoolean function of P and Q, or \"None\": _______________\n(D) (1 point) Is the Slime Gate, based on the description above, a lenient device?\n(Circle one:)\nYES\n/\nNO\nEND OF QUIZ!\n6.004 Spring 2009\n- 4 of 4 -\nQuiz #1"
    },
    {
      "category": "Exam",
      "title": "Quiz #1 Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/069499651244ea36ac78ef8e4c8f16dc_MIT6_004s09_quiz01_sol.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n6.004 Computation Structures\nSpring 2009\n\nM A S S A C H U S E T T S I N S T I T U T E O F T E C H N O L O G Y\nDEPARTMENT OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE\n\n6.004 Computation Structures\nSpring 2009\n\nQuiz #1: February 20, 2009\n\nName\nSolutions\nAthena login name\n\nScore\nAvg: 21.4\nProblem 1 (6 points): Quickies and Trickies\n\n(A) You are trying to guess a card picked at random from a standard 52-card deck. Sam tells\nyou the card is a spade; Nora tells you it's not an ace; Rita tells you it's a seven.\nWhat is the total amount of information about the card given by Sam, Nora, and\nRita? You may give your answer as either a number or a formula.\n\nBits of info (number or formula): ____________________\nlog 2 (52)\n\n(B) Give the four-bit two's complement representation of -6.\n\n4-bit 2's complement representation of -6: ____________________\n\n(C) Let w be an eight-bit code word. How many 8-bit code words are there whose Hamming\ndistance from w is exactly one?\n\nNumber of 8-bit code words distance 1 from w: ____________________\n\n(D) In Lecture 4, you saw that a 2k -input XOR could be implemented as a k-level tree of 2-\ninput gates. Can every 2k-input, 1-output combinational function be implemented as\na k-level tree of (not necessarily identical) 2-input 1-output gates?\n\nCircle one: YES NO\n\n(E) The propagation delay specified for an inverter is less than its contamination delay\n\nCircle one: NEVER SOMETIMES ALWAYS\n\n(F) A single CMOS gate, consisting of an output node connected to a single pullup circuit\ncontaining zero or more PFETs and a single pulldown circuit containing zero or more\nNFETs (as described in lecture), computes F(A, B, C, D).\n\nIt is observed that F(1,0,1,1) = 1. What can you say about the value of F(1,0,0,0)?\n\n(circle one): F(1,0,0,0) = 1 ... 0 ... can't tell\n6.004 Spring 2009\n- 1 of 4 -\nQuiz #1\n\nProblem 2. (5 points): Variable-length Codes\n\nGradeStore, Inc makes storage devices designed to hold only letter grades issued in college\ncourses. Each grade is one of A, B, C, D, F, or X (for dropped courses), and the grades are\ndistributed according to the following table:\n\nGrade\nProbability of occurrence\nA\n26%\nB\n34%\nC\n11%\nD\n10%\nF\n8%\nX\n11%\n\n(A) (1 point) How many bits would be required to store each grade using a fixed-length code?\n\nNumber of bits: ____________________\n\n(B) (3 points) Devise a Huffman code to optimize the storage of grades.\n\nEncoding for A: ____________________\nThe correct Huffman tree is\n\nB A\nC X D F\nThe specific answers at right are\njust one of the many possible\nencodings from this tree.\n\nEncoding for B: ____________________\n\nEncoding for C: ____________________\n\nEncoding for D: ____________________\n\nEncoding for F: ____________________\n\nEncoding for X: ____________________\n\nA well-heeled customer wants GradeStore to customize their encoding to include a seventh grade,\nM, to identify students whose work was incomplete due to their being hit by a meteor. This is a\nvery rare event, and GradeStore expects its probability to be on the order of 0.00000000007%.\n\n(C) (1 point) How many bits would Huffman encoding allocate to the representation of an M\ngrade?\n\nBits for encoding of M grade: ____________________\n6.004 Spring 2009\n- 2 of 4 -\nQuiz #1\n\nProblem 3. (7 points): Saga of Helena Handbasket\n\nHelena Handbasket, who barely passed 6.004, has been hired to design CMOS gates\nfor Hapless Logic, Inc. Remembering something about PFETs in pullups and NFETs\nin pulldowns, her first design was a 3-input device whose circuit is shown to the right.\n\nHelena's intent was that these devices compute some useful 3-input Boolean function\nD=F(A,B,C); unfortunately, the devices don't seem to work as planned. To make\nmatters worse, she had 1,000,000,000 of the devices fabricated (thinking that the\norder to the fab must, of course, be in binary rather than decimal). The defective\ndevices, known within HLI as the Gates of Helena, have become a subject of ridicule.\n\nHelena has brought you in as a consultant. Your first task is to figure out how badly Helena blew\nthe design of her 3-input logic device - in particular, whether it drives the output D to a valid\nlogic level for every combination of the inputs A, B, and C.\n\n(A) (2 points) Are there logical (0/1) values of A, B, and C for which D is not driven at all?\nIf so, give values for A, B, and C that leave D undriven; else write NONE.\n\nA, B, C values for undriven D, or \"NONE\": ____________________\n\n(B) (2 points) Are there logical values of A, B, and C for which D is pulled down and up\nsimultaneously? If so, values for A, B, and C that cause such a conflict; else write NONE.\n\nB = 0 and C = 1\n(A can be 0 or 1)\nA, B, C values for output conflict, or \"NONE\": ____________________\nNONE\n\nNora Nanda, Helena's assistant, suggests that the devices might be salvaged by using them to\ncompute useful functions of fewer than 3 inputs. She proposes that a two-input function of X and\nY, for example, might be computed by connecting each of the three inputs A, B, and C to either\nX, Y, or the logical constants 0 (ground) or 1 (vdd), and reading the output on D.\n\n(C) (1 point) Can Nora's approach be used to compute NAND of X and Y? If so, choose\nvalues (X, Y, 0, or 1) for each of A, B, and C such that D is XY ; else circle NO.\n\nChoose A, B, C values or circle NO: A=____; B=____; C=____; or NO\n\nX\nY\n\n(D) (1 point) Can Nora's approach be used to compute NOR of X and Y? If so, choose\nvalues (X, Y, 0, or 1) for each of A, B, and C such that D is X + Y ; else circle NO.\n\nChoose A, B, C values or circle NO: A=____; B=____; C=____; or NO\n\nX\n\n(E) (1 point) Can Nora's approach be used to compute X + Y ? If so, choose values (X, Y, 0,\nor 1) for each of A, B, and C such that D is X + Y ; else circle NO.\nChoose A, B, C values or circle NO: A=____; B=____; C=____; or NO\nY\n6.004 Spring 2009\n- 3 of 4 -\nQuiz #1\n\nProblem 4 (7 points): Organic Logic\n\nOrganic Logic, Inc, is a Cambridge startup that has developed an interesting device built using\nunidentified organic sludge from the depths of the Charles River; they would like to use it to\nperform logic functions. Their device, termed a Slime Gate, has two inputs A and B, and one\noutput C (in addition to power and ground connections):\n\nWith a 3-volt power supply, they have noted that Slime Gates reliably behave as follows:\n\n-\nThe output C is always in the range 0 volts < C < 3 volts.\n\n-\nWhen both A and B have been at less than 1 volt for a\nnanosecond or more, the voltage at C is greater than 2.5\nvolts.\n-\nWhen either or both of A and B have been at more than 2\nvolts for at least a nanosecond, C carries a voltage of less\nthan 0.5 volts.\n\nAside from the above constraints, the voltage at C is generally unpredictable; it varies widely\nbetween individual Slime Gate devices.\n\nX\nY\nInverter\nAs an O.L.I. consultant, you have proposed the circuit to the\nright as an inverter in the evolving family of Slime Gate logic:\n\n(A) (3 points) Give logic representation parameters\nyielding maximum noise margins and for which\nyour diagram depicts a valid inverter.\n\n6.004 Spring 2009\n- 4 of 4 -\nQuiz #1\nVOL: __\n_; V\n_____\n0.5 V\nIL: ___\n_; V\n____\n1 V\nIH: __\n_; V\n_____\n2 V\nOH: _\n_\n______\n2.5 V\n\n(B) (2 points) Give appropriate specifications for propagation and contamination delays\nfor this inverter:\n\ntpd: ____\n____ ns; t\n__1\ncd: _____\n___ ns\n__\n\nQ\nP\nR\n2-input gate\n(C) (1 point) Suppose the Slime Gate is used as a\n2-input logic gate in this same family, as\nshown to the right. What, if any, function of\nP and Q is represented by the output R?\n\nBoolean function of P and Q, or \"None\": ____\n____\n_______\n\n(D) (1 point) Is the Slime Gate, based on the description above, a lenient device?\n\n(Circle one:) YES / NO\n\nP + Q\n(NOR)\nEND OF QUIZ!"
    },
    {
      "category": "Exam",
      "title": "Quiz #2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/1083b2708ff095302f2f36dd895bd8c4_MIT6_004s09_quiz02.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n6.004 Computation Structures\nSpring 2009\n\nM A S S A C H U S E T T S I N S T I T U T E O F T E C H N O L O G Y\nDEPARTMENT OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE\n\n6.004 Computation Structures\nSpring 2009\n\nQuiz #2: March 13, 2009\n\nName\n\nAthena login name\n\nScore\n\nProblem 1 (4 points): Quickies and Trickies\n\n(A) A latch is constructed from a 2-input lenient MUX having a propagation delay of 200ps\nand a contamination delay of 20ps, using the design shown in lecture. Give the\nminimum appropriate setup time specification for this latch.\n\nSetup time (ps): ____________________\n\n(B) Give the best achievable asymptotic throughput for a pipelined multiplier capable of\nmultiplying two N-bit operands. Enter a number, a formula, or \"CAN'T TELL\".\n\nAsymptotic throughput: Θ(________________)\n\n(C) A complex combinational circuit is constructed entirely from 2-input NAND gates\nhaving a propagation delay of 1 ns. If this circuit is pipelined for maximal\nthroughput by adding registers whose setup time and propagation delay are each 1 ns,\nwhat is the throughput of the resulting pipeline? Enter a number, a formula, or\n\"CAN'T TELL\".\n\nThroughput (ns-1): ________________\n\n(D) A \"Thingee\" is a clocked device built out of 3 interconnected components, each of which\nis known to be a 4-state FSM. What bound, if any, can you put on the number of\nstates of a Thingee?\n\nMax # of states, or \"Can't Tell\": ________________\n\n6.004 Spring 2009\n- 1 of 5 -\nQuiz #2\n\nProblem 2 (6 points): Reliability measures\n\nYour thesis supervisor has offered you a number of candidate topics, each involving the\nconstruction of a gadget from reliable electronic components. To select a project, you decide to\nevaluate, for each choice, whether it can be made to meet its specification with perfect reliability.\nEach device takes two digital inputs A and B, each of which start at logical 0 and have a single\npositive transition at tA and tB, respectively. The propagation delay t\nB\npd is to be 1 nanosecond.\n\n(A) The output Q is guaranteed to be a valid 1 or 0 tpd after the second input transition, but\nthere is no constraint on which value is output.\n\nCan be made reliable? Circle one: YES ... NO ... CAN'T TELL\n\n(B) The output Q is guaranteed to be a valid 1 or 0 tpd after the second input transition, must\nbe 1 if tA is 10ns or more earlier than tB, must be 0 if t\nB\nA is 10ns or more later than tB\nB, or\notherwise can be either valid logic level.\n\nCan be made reliable? Circle one: YES ... NO ... CAN'T TELL\n\n(C) At tpd after the second input transition, the output Q must be a valid 1 if tA is 10ns or\nmore earlier than tB, must be a valid 0 if t\nB\nA is 10ns or more later than tB\nB, and is otherwise\nunspecified (may be invalid) if tA and tB are within 10 ns of each other.\nB\n\nCan be made reliable? Circle one: YES ... NO ... CAN'T TELL\n\n(D) Prior to tA and tB the output Q must be a valid 0, but it must become a valid 1 within t\nB\npd of\nthe first input transition.\n\nCan be made reliable? Circle one: YES ... NO ... CAN'T TELL\n\n(E) The output Q is guaranteed to be a valid 1 or 0 tpd after the second input transition, must\nbe 1 if the transitions are within 10ns of each other, and must be 0 if the transitions are\nseparated by more than 20ns.\n\nCan be made reliable? Circle one: YES ... NO ... CAN'T TELL\n\n(F) Two outputs, Q and R, where R is guaranteed to become a valid 1 sometime after the first\ninput transition. When R=1, the Q output must be a valid 1 if tA is 10ns or more earlier\nthan tB, must be a valid 0 if t\nB\nA is 10ns or more later than tB\nB, or otherwise may be either\nvalid logic level.\n\nCan be made reliable? Circle one: YES ... NO ... CAN'T TELL\n\n6.004 Spring 2009\n- 2 of 5 -\nQuiz #2\n\nProblem 3. (9 Points) Picking Locks, 6.004 style\n\nPerfectly Perplexing Padlocks makes an entry-level electronic lock, the P3b, built from an FSM\nwith two bits of state. The P3b has two buttons (\"0\" and \"1\") that when pressed cause the FSM\ncontrolling the lock to advance to a new state. In addition to advancing the FSM, each button\npress is encoded on the B signal (B=0 for button \"0\", B=1 for button \"1\"). The padlock unlocks\nwhen the FSM sets the UNLOCK output signal to 1, which it does whenever--and only\nwhenever--the last 3 button presses correspond to the 3-digit combination. The combination\nis unique, and will open the lock independently of the starting state.\nUnfortunately the design notes for the P3b are incomplete.\n\nS1\nS0\nB\nS'1\nS'0\nU\n\n(A) (1 Point) What is the 3-bit combination for the lock?\n\nlock combination:_____________________\n\n(B) (4 Points) Using the specification and clues from the partially completed diagrams above\nfill in the information that is missing from the state transition diagram and its\naccompanying truth table. When done:\n\n- each state in the transition diagram should be assigned a 2-bit state name S1S0 (note\nthat in this design the state name is not derived from the combination that opens the\nlock),\n- the arcs leaving each state should be mutually exclusive and collectively exhaustive,\n- the value for U should be specified for each state, and\n- the truth table should be completed.\n\n(complete above transition diagram and table)\n6.004 Spring 2009\n- 3 of 5 -\nQuiz #2\n\nThe circuit diagram for the P3b is shown below. You may assume that the B input has been\nappropriately synchronized with CLK.\n\n(C) (1 Point) How many bits are stored in the ROM?\n\nTotal size of ROM, in bits:____________________\n\n(D) (1 Point) What is the smallest value for the ROM's contamination delay that ensures the\nnecessary timing specifications are met?\n\nsmallest valid value for tCD,ROM (ns):____________________\n\n(E) (1 Point) What is the smallest value for the period of CLK that will meet the timing\nspecifications?\n\nsmallest value for tCLK (ns):____________________\n\n(F) (1 Point) What is the smallest setup time for the B input with respect to the active edge of\nCLK that ensures the necessary timing specifications are met?\n\nsmallest setup time for B input (ns):____________________\n\n6.004 Spring 2009\n- 4 of 5 -\nQuiz #2\n\nProblem 4 (6 Points) The Mysterious XYZ Machine\n\nAn unidentified government agency has a design for a combinational device depicted below:\n\nAlthough you don't know the function of each of the component modules, they are each\ncombinational and marked with their respective propagation delays. You have been hired to\nanalyze and improve the performance of this device.\n\nNOTE: Scratch copies of the above diagram appear on the back of the previous page.\n\n(A) (1 Point) What are the best throughput and latency that can be achieved with the\ncombinational device?\n\nLatency: ________ns; Throughput: ____________ ns-1\n\n(B) (4 Points) Show how to pipeline the above circuit for maximum throughput, by marking\nlocations in the diagram where registers are to be inserted. Use a minimum number of\nregisters, but be sure to include one on the output.\n\n(mark register locations in diagram above)\n\n(C) (1 Point) What are the latency and throughput of your pipelined circuit?\n\nLatency: ________ns; Throughput: ____________ ns-1\n\nEND OF QUIZ!\n6.004 Spring 2009\n- 5 of 5 -\nQuiz #2"
    },
    {
      "category": "Exam",
      "title": "Quiz #2 Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/c9d995f67312634823b472696a250420_MIT6_004s09_quiz02_sol.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n6.004 Computation Structures\nSpring 2009\n\nM A S S A C H U S E T T S I N S T I T U T E O F T E C H N O L O G Y\nDEPARTMENT OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE\n\n6.004 Computation Structures\nSpring 2009\n\nQuiz #2: March 13, 2009\n\nName\nSolutions\nAthena login name\n\nScore\nAvg: 19.0\n\nProblem 1 (4 points): Quickies and Trickies\n\n(A) A latch is constructed from a 2-input lenient MUX having a propagation delay of 200ps\nand a contamination delay of 20ps, using the design shown in lecture. Give the\nminimum appropriate setup time specification for this latch.\n\nSetup time (ps): ____________________\n\n(B) Give the best achievable asymptotic throughput for a pipelined multiplier capable of\nmultiplying two N-bit operands. Enter a number, a formula, or \"CAN'T TELL\".\n\nAsymptotic throughput: Θ(________________)\n\n(C) A complex combinational circuit is constructed entirely from 2-input NAND gates\nhaving a propagation delay of 1 ns. If this circuit is pipelined for maximal\nthroughput by adding registers whose setup time and propagation delay are each 1 ns,\nwhat is the throughput of the resulting pipeline? Enter a number, a formula, or\n\"CAN'T TELL\".\n\nThroughput (ns-1): ________________\n1/3\n\n(D) A \"Thingee\" is a clocked device built out of 3 interconnected components, each of which\nis known to be a 4-state FSM. What bound, if any, can you put on the number of\nstates of a Thingee?\n\nMax # of states, or \"Can't Tell\": ________________\n\n6.004 Spring 2009\n- 1 of 5 -\nQuiz #2\n\nProblem 2 (6 points): Reliability measures\n\nYour thesis supervisor has offered you a number of candidate topics, each involving the\nconstruction of a gadget from reliable electronic components. To select a project, you decide to\nevaluate, for each choice, whether it can be made to meet its specification with perfect reliability.\nEach device takes two digital inputs A and B, each of which start at logical 0 and have a single\npositive transition at tA and tB, respectively. The propagation delay t\nB\npd is to be 1 nanosecond.\n\n(A) The output Q is guaranteed to be a valid 1 or 0 tpd after the second input transition, but\nthere is no constraint on which value is output.\n\nCan be made reliable? Circle one: YES ... NO ... CAN'T TELL\n\n(B) The output Q is guaranteed to be a valid 1 or 0 tpd after the second input transition, must\nbe 1 if tA is 10ns or more earlier than tB, must be 0 if t\nB\nA is 10ns or more later than tB\nB, or\notherwise can be either valid logic level.\n\nCan be made reliable? Circle one: YES ... NO ... CAN'T TELL\n\n(C) At tpd after the second input transition, the output Q must be a valid 1 if tA is 10ns or\nmore earlier than tB, must be a valid 0 if t\nB\nA is 10ns or more later than tB\nB, and is otherwise\nunspecified (may be invalid) if tA and tB are within 10 ns of each other.\nB\n\nCan be made reliable? Circle one: YES ... NO ... CAN'T TELL\n\n(D) Prior to tA and tB the output Q must be a valid 0, but it must become a valid 1 within t\nB\npd of\nthe first input transition.\n\nCan be made reliable? Circle one: YES ... NO ... CAN'T TELL\n\n(E) The output Q is guaranteed to be a valid 1 or 0 tpd after the second input transition, must\nbe 1 if the transitions are within 10ns of each other, and must be 0 if the transitions are\nseparated by more than 20ns.\n\nCan be made reliable? Circle one: YES ... NO ... CAN'T TELL\n\n(F) Two outputs, Q and R, where R is guaranteed to become a valid 1 sometime after the first\ninput transition. When R=1, the Q output must be a valid 1 if tA is 10ns or more earlier\nthan tB, must be a valid 0 if t\nB\nA is 10ns or more later than tB\nB, or otherwise may be either\nvalid logic level.\n\nCan be made reliable? Circle one: YES ... NO ... CAN'T TELL\n\n6.004 Spring 2009\n- 2 of 5 -\nQuiz #2\n\nProblem 3. (9 Points) Picking Locks, 6.004 style\n\nPerfectly Perplexing Padlocks makes an entry-level electronic lock, the P3b, built from an FSM\nwith two bits of state. The P3b has two buttons (\"0\" and \"1\") that when pressed cause the FSM\ncontrolling the lock to advance to a new state. In addition to advancing the FSM, each button\npress is encoded on the B signal (B=0 for button \"0\", B=1 for button \"1\"). The padlock unlocks\nwhen the FSM sets the UNLOCK output signal to 1, which it does whenever--and only\nwhenever--the last 3 button presses correspond to the 3-digit combination. The combination\nis unique, and will open the lock independently of the starting state.\nUnfortunately the design notes for the P3b are incomplete.\n\nS1\nS0\nB\nS'1\nS'0\nU\n\n(A) (1 Point) What is the 3-bit combination for the lock?\n\nlock combination:_____________________\n\n(B) (4 Points) Using the specification and clues from the partially completed diagrams above\nfill in the information that is missing from the state transition diagram and its\naccompanying truth table. When done:\n\n- each state in the transition diagram should be assigned a 2-bit state name S1S0 (note\nthat in this design the state name is not derived from the combination that opens the\nlock),\n- the arcs leaving each state should be mutually exclusive and collectively exhaustive,\n- the value for U should be specified for each state, and\n- the truth table should be completed.\n\n(complete above transition diagram and table)\n6.004 Spring 2009\n- 3 of 5 -\nQuiz #2\n\nThe circuit diagram for the P3b is shown below. You may assume that the B input has been\nappropriately synchronized with CLK.\n\n(C) (1 Point) How many bits are stored in the ROM?\n\nTotal size of ROM, in bits:____________________\n\n(D) (1 Point) What is the smallest value for the ROM's contamination delay that ensures the\nnecessary timing specifications are met?\n\nsmallest valid value for tCD,ROM (ns):____________________\n0.1\n\n(E) (1 Point) What is the smallest value for the period of CLK that will meet the timing\nspecifications?\n\nsmallest value for tCLK (ns):____________________\n\n(F) (1 Point) What is the smallest setup time for the B input with respect to the active edge of\nCLK that ensures the necessary timing specifications are met?\n\nsmallest setup time for B input (ns):____________________\n8.5\n\n6.004 Spring 2009\n- 4 of 5 -\nQuiz #2\n\nProblem 4 (6 Points) The Mysterious XYZ Machine\n\nAn unidentified government agency has a design for a combinational device depicted below:\n\nAlthough you don't know the function of each of the component modules, they are each\ncombinational and marked with their respective propagation delays. You have been hired to\nanalyze and improve the performance of this device.\n\nNOTE: Scratch copies of the above diagram appear on the back of the previous page.\n\n(A) (1 Point) What are the best throughput and latency that can be achieved with the\ncombinational device?\n\nLatency: ________ns; Throughput: ____________ ns-1\n1/180\n\n(B) (4 Points) Show how to pipeline the above circuit for maximum throughput, by marking\nlocations in the diagram where registers are to be inserted. Use a minimum number of\nregisters, but be sure to include one on the output.\n\n(mark register locations in diagram above)\n\n(C) (1 Point) What are the latency and throughput of your pipelined circuit?\n\nLatency: ________ns; Throughput: ____________ ns-1\n1/60\n\nEND OF QUIZ!\n6.004 Spring 2009\n- 5 of 5 -\nQuiz #2"
    },
    {
      "category": "Exam",
      "title": "Quiz #3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/385890fca34776c85f3638d9f7337c3a_MIT6_004s09_quiz03.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.004 Computation Structures\nSpring 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nM A S S A C H U S E T T S I N S T I T U T E O F T E C H N O L O G Y\nDEPARTMENT OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE\n6.004 Computation Structures\nSpring 2009\nQuiz #3: April 10, 2009\nName\nAthena login name\nScore\nNOTE: Reference material and scratch copies of code appear on the backs of quiz pages.\nProblem 1 (5 points): Quickies and Trickies\n(A) (2 points) A student tries to optimize his Beta assembly program by replacing a line\ncontaining\nADDC(R0, 3*4+5, R1)\nby\nADDC(R0, 17, R1)\nIs the resulting binary program smaller? Does it run faster?\n(circle one) Binary program is SMALLER? yes ... no\n(circle one) FASTER? yes ... no\n(B) Which of the following best conveys Church's thesis?\nC1: Every integer function can be computed by some Turing machine.\nC2: Every computable function can be computed by some Turing machine.\nC3: No Turing machine can solve the halting problem.\nC4: There exists a single Turing machine that can compute every computable function.\n(circle one) Best conveys Church's thesis: C1 ... C2 ... C3 ... C4\n(C) What value will be found in the low 16 bits of the BEQ instruction resulting from the\nfollowing assembly language snippet?\n. = 0x100\nBEQ(R31, target, R31)\ntarget: ADDC(R31, 0, R31)\n16-bit offset portion of above BEQ instruction: ____________________\n(D) Can every SUBC instruction be replaced by an equivalent ADDC instruction with the\nconstant negated? If so, answer \"YES\"; if not, give an example of a SUBC\ninstruction that can't be replaced by an ADDC.\nSUBC(...) instruction, or \"YES\": _______________________________________\n6.004 Spring 2009\n- 1 of 5 -\nQuiz #3\n\nSummary of β Instruction Formats\nOperate Class:\n10xxxx\nRc\nRa\nRb\nunused\nOP(Ra,Rb,Rc):\nReg[Rc] ← Reg[Ra] op Reg[Rb]\nOpcodes: ADD (plus), SUB (minus), MUL (multiply), DIV (divided by)\nAND (bitwise and), OR (bitwise or), XOR (bitwise exclusive or)\nCMPEQ (equal), CMPLT (less than), CMPLE (less than or equal) [result = 1 if true, 0 if false]\nSHL (left shift), SHR (right shift w/o sign extension), SRA (right shift w/ sign extension)\n11xxxx\nRc\nRa\nliteral (two's complement)\nOPC(Ra,literal,Rc):\nReg[Rc] ← Reg[Ra] op SEXT(literal)\nOpcodes: ADDC (plus), SUBC (minus), MULC (multiply), DIVC (divided by)\nANDC (bitwise and), ORC (bitwise or), XORC (bitwise exclusive or)\nCMPEQC (equal), CMPLTC (less than), CMPLEC (less than or equal) [result = 1 if true, 0 if false]\nSHLC (left shift), SHRC (right shift w/o sign extension), SRAC (right shift w/ sign extension)\nOther:\n01xxxx\nRc\nRa\nliteral (two's complement)\nLD(Ra,literal,Rc):\nReg[Rc] ← Mem[Reg[Ra] + SEXT(literal)]\nST(Rc,literal,Ra):\nMem[Reg[Ra] + SEXT(literal)] ← Reg[Rc]\nJMP(Ra,Rc):\nReg[Rc] ← PC + 4; PC ← Reg[Ra]\nBEQ/BF(Ra,label,Rc):\nReg[Rc] ← PC + 4; if Reg[Ra] = 0 then PC ← PC + 4 + 4*SEXT(literal)\nBNE/BT(Ra,label,Rc):\nReg[Rc] ← PC + 4; if Reg[Ra] = 0 then PC ← PC + 4 + 4*SEXT(literal)\nLDR(label,Rc):\nReg[Rc] ← Mem[PC + 4 + 4*SEXT(literal)]\nOpcode Table: (*optional opcodes)\n2:0\n5:3\nLD\nST\nJMP\nBEQ\nBNE\nLDR\nADD\nSUB\nMUL*\nDIV*\nCMPEQ\nCMPLT\nCMPLE\nAND\nOR\nXOR\nSHL\nSHR\nSRA\nADDC\nSUBC\nMULC*\nDIVC*\nCMPEQC\nCMPLTC\nCMPLEC\nANDC\nORC\nXORC\nSHLC\nSHRC\nSRAC\n6.004 Spring 2009\n- Back of page 1 of 5 -\nQuiz #3\n\nProblem 2. (13 points): Parentheses Galore\nThe wfps procedure determines whether a string of left and right parentheses is well balanced,\nmuch as your Turing machine of Lab 4 did. Below is the code for the wfps (\"well-formed paren\nstring\") procedure in C, as well as its translation to Beta assembly code. This code is reproduced\non the backs of the following two pages for your use and/or annotation.\nint STR[100];\nint wfps(int i,\nint n)\n{ int c = STR[i];\nint new_n;\nif (c == 0)\nreturn (n == 0);\nelse if (c == 1)\nnew_n = n+1;\nelse {\nif (n == 0) return 0; // too many RPARENS!\nxxxxx; }\n// MYSTERY CODE!\nreturn wfps(i+1, new_n); // and recurse.\n}\nwfps expects to find a string of parentheses in the integer array stored at STR. The\nstring is encoded as a series of 32-bit integers having values of\n1 to indicate a left paren,\n2 to indicate a right paren, or\n0 to indicate the end of the string.\nThese integers are stored in consecutive 32-bit locations starting at the address\nSTR.\nwfps is called with two arguments:\n1. The first, i, is the index of the start of the part of STR that this call of\nwfps should examine. Note that indexes start at 0 in C. For example, if i\nis 0, then wfps should examine the entire string in STR (starting at the\nfirst character, or STR[0]). If i is 4, then wfps should ignore the first\nfour characters and start examining STR starting at the fifth character (the\ncharacter at STR[4]).\n2. The second argument, n, is zero in the original call; however, it may be\nnonzero in recursive calls.\nwfps returns 1 if the part of STR being examined represents a string of balanced\nparentheses if n additional left parentheses are prepended to its left, and returns 0\notherwise.\nNote that the compiler may use some simple optimizations to simplify the\nassembly-language version of the code, while preserving equivalent behavior.\nThe C code is incomplete; the missing expression is shown as xxxx.\n// string of parens\n// current index in STR\n// LPARENs to balance\n// next character\n// next value of n\n// if end of string,\n//\nreturn 1 iff n == 0\n// on LEFT PAREN,\n//\nincrement n\n// else must be RPAREN\nSTR: . = .+4*100\nwfps: PUSH(LP)\nPUSH(BP)\nMOVE(SP, BP)\nALLOCATE(1)\nPUSH(R1)\nLD(BP, -12, R0)\nMULC(R0, 4, R0)\nLD(R0, STR, R1)\nST(R1, 0, BP)\nBNE(R1, more)\nLD(BP, -16, R0)\nCMPEQC(R0, 0, R0)\nrtn: POP(R1)\nMOVE(BP, SP)\nPOP(BP)\nPOP(LP)\nJMP(LP)\nmore: CMPEQC(R1, 1, R0)\nBF(R0, rpar)\nLD(BP, -16, R0)\nADDC(R0, 1, R0)\nBR(par)\nrpar: LD(BP, -16, R0)\nBEQ(R0, rtn)\nADDC(R0, -1, R0)\npar: PUSH(R0)\nLD(BP, -12, R0)\nADDC(R0, 1, R0)\nPUSH(R0)\nBR(wfps, LP)\nDEALLOCATE(2)\nBR(rtn)\n6.004 Spring 2009\n- 2 of 5 -\nQuiz #3\n\n6.004 Spring 2009\n- Back of page 2 of 5 -\nQuiz #3\n \nSTR: . = .+4*100\n\nwfps: PUSH(LP)\nPUSH(BP)\nMOVE(SP, BP)\nALLOCATE(1)\nPUSH(R1)\n\nLD(BP, -12, R0)\nMULC(R0, 4, R0)\nLD(R0, STR, R1)\nST(R1, 0, BP)\nBNE(R1, more)\n\nLD(BP, -16, R0)\nCMPEQC(R0, 0, R0)\n\nrtn: POP(R1)\nMOVE(BP, SP)\nPOP(BP)\nPOP(LP)\nJMP(LP)\n\nmore: CMPEQC(R1, 1, R0)\nBF(R0, rpar)\nLD(BP, -16, R0)\nADDC(R0, 1, R0)\nBR(par)\n\nrpar: LD(BP, -16, R0)\nBEQ(R0, rtn)\nADDC(R0, -1, R0)\n\npar: PUSH(R0)\nLD(BP, -12, R0)\nADDC(R0, 1, R0)\nPUSH(R0)\nBR(wfps, LP)\nDEALLOCATE(2)\nBR(rtn)\nint STR[100]; // string of parens\n\nint wfps(int i, // current index in STR\nint n) // LPARENs to balance\n{ int c = STR[i]; // next character\nint new_n; // next value of n\nif (c == 0) // if end of string,\nreturn (n == 0); // return 1 iff n == 0\nelse if (c == 1) // on LEFT PAREN,\nnew_n = n+1; // increment n\nelse { // else must be RPAREN\nif (n == 0) return 0; // too many RPARENS!\nxxxxx; } // MYSTERY CODE!\nreturn wfps(i+1, new_n); // and recurse.\n}\nScratch copies of code and memory snippet for Problem 2:\nMemory\naddress: Data\n188: 7\n18C: 4A8\n190: 0\n194: 0\n198: 458\n19C: D4\n1A0: 1\n1A4: D8\n1A8: 1\n1AC: 1\n1B0: 3B8\n1B4: 1A0\n1B8: 2\n1BC: 1\n1C0: 0\n1C4: 2\n1C8: 3B8\n1CC: 1B8\nBP->1D0: 2\n1D4: 2\nSP->1D8: 0\n\nProblem 2 continued:\n(A) (3 points) In the space below, fill in the binary value of the instruction stored at the\nlocation tagged 'more:' in the above assembly-language program.\n(fill in missing 1s and 0s for instruction at more:)\n(B) (1 point) Is the value of the variable c from the C program stored in the local stack\nframe? If so, give its (signed) offset from BP; else write \"NO\".\nStack offset of variable c, or \"NO\": _________________\n(C) (1 point) Is the value of the variable new_n from the C program stored in the local\nstack frame? If so, give its (signed) offset from BP; else write \"NO\".\nStack offset of variable new_n, or \"NO\": _________________\n(D) (2 points) What is the missing C source code represented by xxxxx in the given C\nprogram?\n(give missing C code shown as xxxxx)\n6.004 Spring 2009\n- 3 of 5 -\nQuiz #3\n\n6.004 Spring 2009\n- Back of page 3 of 5 -\nQuiz #3\n \nSTR: . = .+4*100\n\nwfps: PUSH(LP)\nPUSH(BP)\nMOVE(SP, BP)\nALLOCATE(1)\nPUSH(R1)\n\nLD(BP, -12, R0)\nMULC(R0, 4, R0)\nLD(R0, STR, R1)\nST(R1, 0, BP)\nBNE(R1, more)\n\nLD(BP, -16, R0)\nCMPEQC(R0, 0, R0)\n\nrtn: POP(R1)\nMOVE(BP, SP)\nPOP(BP)\nPOP(LP)\nJMP(LP)\n\nmore: CMPEQC(R1, 1, R0)\nBF(R0, rpar)\nLD(BP, -16, R0)\nADDC(R0, 1, R0)\nBR(par)\n\nrpar: LD(BP, -16, R0)\nBEQ(R0, rtn)\nADDC(R0, -1, R0)\n\npar: PUSH(R0)\nLD(BP, -12, R0)\nADDC(R0, 1, R0)\nPUSH(R0)\nBR(wfps, LP)\nDEALLOCATE(2)\nBR(rtn)\nint STR[100]; // string of parens\n\nint wfps(int i, // current index in STR\nint n) // LPARENs to balance\n{ int c = STR[i]; // next character\nint new_n; // next value of n\nif (c == 0) // if end of string,\nreturn (n == 0); // return 1 iff n == 0\nelse if (c == 1) // on LEFT PAREN,\nnew_n = n+1; // increment n\nelse { // else must be RPAREN\nif (n == 0) return 0; // too many RPARENS!\nxxxxx; } // MYSTERY CODE!\nreturn wfps(i+1, new_n); // and recurse.\n}\nScratch copies of code and memory snippet for Problem 2:\nMemory\naddress: Data\n188: 7\n18C: 4A8\n190: 0\n194: 0\n198: 458\n19C: D4\n1A0: 1\n1A4: D8\n1A8: 1\n1AC: 1\n1B0: 3B8\n1B4: 1A0\n1B8: 2\n1BC: 1\n1C0: 0\n1C4: 2\n1C8: 3B8\n1CC: 1B8\nBP->1D0: 2\n1D4: 2\nSP->1D8: 0\n\nProblem 2 continued again:\nThe procedure wfps is called from an external procedure and its execution is interrupted during a\nrecursive call to wfps, just prior to the execution of the instruction labeled 'rtn:'. The contents\nof a region of memory are shown to below on the left. At this point, SP contains 0x1D8, and BP\ncontains 0x1D0.\nNOTE: All addresses and data values are shown in hexadecimal.\n188:\n(E) (1 point) What are the arguments to the most recent active call to wfps?\n18C:\n4A8\nMost recent arguments (HEX): i=________; n=________\n190:\n194:\n198:\n(F) (1 point) What are the arguments to the original call to wfps?\n19C:\nD4\nOriginal arguments (HEX): i=________; n=________\n1A0:\n1A4:\n1A8:\nD8\n(G) (1 point) What value is in R0 at this point?\n1AC:\nContents of R0 (HEX): ________\n1B0:\n3B8\n1B4:\n1A0\n(H) (1 point) How many parens (left and right) are in the string stored at STR\n1B8:\n1BC:\n(starting at index 0)? Give a number, or \"CAN'T TELL\" if the number\ncan't be determined from the given information.\n1C0:\nLength of string, or \"CAN'T TELL\": _______________\n1C4:\n1C8:\n1CC:\n3B8\n1B8\n(I) (1 point) What is the hex address of the instruction tagged par:?\nBP->1D0:\nAddress of par (HEX): ________\n1D4:\nSP->1D8:\n(J) (1 point) What is the hex address of the BR instruction that called wfps\noriginally?\nAddress of original call (HEX): ________\n6.004 Spring 2009\n- 4 of 5 -\nQuiz #3\n\nWDSEL\nControl logic:\n6.004 Spring 2009\n- Back of page 4 of 5\nQuiz #3\n\nProblem 3 (7 Points): Beta control signals\nFollowing is an incomplete table listing control signals for several instructions on an unpipelined\nBeta. You may wish to consult the Beta diagram on the back of the previous page and the\ninstruction set summary on the back of the first page.\nThe operations listed include two existing instructions and two proposed additions to the Beta\ninstruction set:\nLDX(Ra, Rb, Rc)\n// Load, double indexed\nEA ← Reg[Ra] + Reg[Rb]\nReg[Rc] ← Mem[EA]\nPC ← PC + 4\nMVZC(Ra, literal, Rc)\n// Move constant if zero\nIf Reg[Ra] == 0 then Reg[Rc] ← SEXT(literal)\nPC ← PC + 4\nIn the following table, φ represents a \"don't care\" or unspecified value; Z is the value (0 or 1)\noutput by the 32-input NOR in the unpipelined Beta diagram. Your job is to complete the table by\nfilling in each unshaded entry. In each case, enter an opcode, a value, an expression, or φ as\nappropriate.\nInstr\nALUFN WERF\nBSEL WDSEL\nWR RA2SEL PCSEL ASEL WASEL\nφ\nφ\nφ\nφ\nφ\nφ\nφ\nZ\nφ\nLDX\nA+B\nZ\nφ\n(Complete the above table)\nEND OF QUIZ!\n(phew!)\n6.004 Spring 2009\n- 5 of 5 -\nQuiz #3\n\nConvenience Macros\nWe augment the basic β instruction set with the following macros, making it easier to\nexpress certain common operations:\nMacro\nDefinition\nBEQ(Ra, label)\nBEQ(Ra, label, R31)\nBF(Ra, label)\nBF(Ra, label, R31)\nBNE(Ra, label)\nBNE(Ra, label, R31)\nBT(Ra, label)\nBT(Ra, label, R31)\nBR(label, Rc)\nBEQ(R31, label, Rc)\nBR(label)\nBR(label, R31)\nJMP(Ra)\nJMP(Ra, R31)\nLD(label, Rc)\nLD(R31, label, Rc)\nST(Rc, label)\nST(Rc, label, R31)\nMOVE(Ra, Rc)\nADD(Ra, R31, Rc)\nCMOVE(c, Rc)\nADDC(R31, c, Rc)\nPUSH(Ra)\nADDC(SP, 4, SP)\nST(Ra, -4, SP)\nPOP(Rc)\nLD(SP, -4, Rc)\nSUBC(SP, 4, SP)\nALLOCATE(k)\nADDC(SP, 4*k, SP)\nDEALLOCATE(k)\nSUBC(SP, 4*k, SP)\n6.004 Spring 2009\n- Back of page 5 of 5 -\nQuiz #3"
    },
    {
      "category": "Exam",
      "title": "Quiz #3 Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/1d890cbfba8ae3d05011db1c4f86de71_MIT6_004s09_quiz03_sol.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.004 Computation Structures\nSpring 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nM A S S A C H U S E T T S I N S T I T U T E O F T E C H N O L O G Y\nDEPARTMENT OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE\n6.004 Computation Structures\nSpring 2009\nQuiz #3: April 10, 2009\nName\nSolutions\nAthena login name\nScore\nAvg: 20.1\nNOTE: Reference material and scratch copies of code appear on the backs of quiz pages.\nProblem 1 (5 points): Quickies and Trickies\n(A) (2 points) A student tries to optimize his Beta assembly program by replacing a line\ncontaining\nADDC(R0, 3*4+5, R1)\nby\nADDC(R0, 17, R1)\nIs the resulting binary program smaller? Does it run faster?\n(circle one) Binary program is SMALLER? yes ... no\nno\n...\n(circle one) FASTER? yes\n(B) Which of the following best conveys Church's thesis?\nC1: Every integer function can be computed by some Turing machine.\nC2: Every computable function can be computed by some Turing machine.\nC3: No Turing machine can solve the halting problem.\nC4: There exists a single Turing machine that can compute every computable function.\n(circle one) Best conveys Church's thesis: C1 ... C2 ... C3 ... C4\n(C) What value will be found in the low 16 bits of the BEQ instruction resulting from the\nfollowing assembly language snippet?\n. = 0x100\nBEQ(R31, target, R31)\ntarget: ADDC(R31, 0, R31)\n0x0000\n16-bit offset portion of above BEQ instruction: ____________________\n(D) Can every SUBC instruction be replaced by an equivalent ADDC instruction with the\nconstant negated? If so, answer \"YES\"; if not, give an example of a SUBC\ninstruction that can't be replaced by an ADDC.\nSUBC(...) instruction, or \"YES\": _______________________________________\nSUBC(Ra, 0x8000, Rc)\n(Ra can be any of R0-R31;\nRc can be any of R0-R30.)\n6.004 Spring 2009\n- 1 of 5 -\nQuiz #3\n\nProblem 2. (13 points): Parentheses Galore\nThe wfps procedure determines whether a string of left and right parentheses is well balanced,\nmuch as your Turing machine of Lab 4 did. Below is the code for the wfps (\"well-formed paren\nstring\") procedure in C, as well as its translation to Beta assembly code. This code is reproduced\non the backs of the following two pages for your use and/or annotation.\nint STR[100];\nint wfps(int i,\nint n)\n{ int c = STR[i];\nint new_n;\nif (c == 0)\nreturn (n == 0);\nelse if (c == 1)\nnew_n = n+1;\nelse {\nif (n == 0) return 0; // too many RPARENS!\nxxxxx; }\n// MYSTERY CODE!\nreturn wfps(i+1, new_n); // and recurse.\n}\nwfps expects to find a string of parentheses in the integer array stored at STR. The\nstring is encoded as a series of 32-bit integers having values of\n1 to indicate a left paren,\n2 to indicate a right paren, or\n0 to indicate the end of the string.\nThese integers are stored in consecutive 32-bit locations starting at the address\nSTR.\nwfps is called with two arguments:\n1. The first, i, is the index of the start of the part of STR that this call of\nwfps should examine. Note that indexes start at 0 in C. For example, if i\nis 0, then wfps should examine the entire string in STR (starting at the\nfirst character, or STR[0]). If i is 4, then wfps should ignore the first\nfour characters and start examining STR starting at the fifth character (the\ncharacter at STR[4]).\n2. The second argument, n, is zero in the original call; however, it may be\nnonzero in recursive calls.\nwfps returns 1 if the part of STR being examined represents a string of balanced\nparentheses if n additional left parentheses are prepended to its left, and returns 0\notherwise.\nNote that the compiler may use some simple optimizations to simplify the\nassembly-language version of the code, while preserving equivalent behavior.\nThe C code is incomplete; the missing expression is shown as xxxx.\n// string of parens\n// current index in STR\n// LPARENs to balance\n// next character\n// next value of n\n// if end of string,\n//\nreturn 1 iff n == 0\n// on LEFT PAREN,\n//\nincrement n\n// else must be RPAREN\nSTR: . = .+4*100\nwfps: PUSH(LP)\nPUSH(BP)\nMOVE(SP, BP)\nALLOCATE(1)\nPUSH(R1)\nLD(BP, -12, R0)\nMULC(R0, 4, R0)\nLD(R0, STR, R1)\nST(R1, 0, BP)\nBNE(R1, more)\nLD(BP, -16, R0)\nCMPEQC(R0, 0, R0)\nrtn: POP(R1)\nMOVE(BP, SP)\nPOP(BP)\nPOP(LP)\nJMP(LP)\nmore: CMPEQC(R1, 1, R0)\nBF(R0, rpar)\nLD(BP, -16, R0)\nADDC(R0, 1, R0)\nBR(par)\nrpar: LD(BP, -16, R0)\nBEQ(R0, rtn)\nADDC(R0, -1, R0)\npar: PUSH(R0)\nLD(BP, -12, R0)\nADDC(R0, 1, R0)\nPUSH(R0)\nBR(wfps, LP)\nDEALLOCATE(2)\nBR(rtn)\n6.004 Spring 2009\n- 2 of 5 -\nQuiz #3\n\nProblem 2 continued:\n(A) (3 points) In the space below, fill in the binary value of the instruction stored at the\nlocation tagged 'more:' in the above assembly-language program.\n1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n(fill in missing 1s and 0s for instruction at more:)\n(B) (1 point) Is the value of the variable c from the C program stored in the local stack\nframe? If so, give its (signed) offset from BP; else write \"NO\".\nStack offset of variable c, or \"NO\": _________________\nBP+0\n(C) (1 point) Is the value of the variable new_n from the C program stored in the local\nstack frame? If so, give its (signed) offset from BP; else write \"NO\".\nStack offset of variable new_n, or \"NO\": _________________\nNO or BP+8\n(D) (2 points) What is the missing C source code represented by xxxxx in the given C\nprogram?\nnew_n = n - 1\n(give missing C code shown as xxxxx)\nThe original intention of this problem was that the variable\nnew_n was not stored on the local stack frame (unlike the\nvariable c, which did have a space specifically allocated on\nthe stack for it). However, it is true that when wfps makes\na recursive call, it pushes the value of new_n onto the\nstack, so we ended up also accepting BP+8 as an answer.\n6.004 Spring 2009\n- 3 of 5 -\nQuiz #3\n\nProblem 2 continued again:\nThe procedure wfps is called from an external procedure and its execution is interrupted during a\nrecursive call to wfps, just prior to the execution of the instruction labeled 'rtn:'. The contents\nof a region of memory are shown to below on the left. At this point, SP contains 0x1D8, and BP\ncontains 0x1D0.\nNOTE: All addresses and data values are shown in hexadecimal.\n188:\n(E) (1 point) What are the arguments to the most recent active call to wfps?\n18C:\n4A8\nMost recent arguments (HEX): i=________; n=________\n190:\n194:\n198:\n(F) (1 point) What are the arguments to the original call to wfps?\n19C:\nD4\nOriginal arguments (HEX): i=________; n=________\n1A0:\n1A4:\n1A8:\n1AC:\nD8\n(G) (1 point) What value is in R0 at this point?\nContents of R0 (HEX):\n________\n1B0:\n3B8\n1B4:\n1A0\n(H) (1 point) How many parens (left and right) are in the string stored at STR\n1B8:\n1BC:\n(starting at index 0)? Give a number, or \"CAN'T TELL\" if the number\ncan't be determined from the given information.\n1C0:\nLength of string, or \"CAN'T TELL\": CAN'T TELL\n_______________\n1C4:\n1C8:\n1CC:\n3B8\n1B8\n(I) (1 point) What is the hex address of the instruction tagged par:?\nBP->1D0:\nAddress of par (HEX): ________\n39C\n1D4:\nSP->1D8:\n(J) (1 point) What is the hex address of the BR instruction that called wfps\noriginally?\nAddress of original call (HEX): ________\n6.004 Spring 2009\n- 4 of 5 -\nQuiz #3\n\nProblem 3 (7 Points): Beta control signals\nFollowing is an incomplete table listing control signals for several instructions on an unpipelined\nBeta. You may wish to consult the Beta diagram on the back of the previous page and the\ninstruction set summary on the back of the first page.\nThe operations listed include two existing instructions and two proposed additions to the Beta\ninstruction set:\nLDX(Ra, Rb, Rc)\n// Load, double indexed\nEA ← Reg[Ra] + Reg[Rb]\nReg[Rc] ← Mem[EA]\nPC ← PC + 4\nMVZC(Ra, literal, Rc)\n// Move constant if zero\nIf Reg[Ra] == 0 then Reg[Rc] ← SEXT(literal)\nPC ← PC + 4\nIn the following table, φ represents a \"don't care\" or unspecified value; Z is the value (0 or 1)\noutput by the 32-input NOR in the unpipelined Beta diagram. Your job is to complete the table by\nfilling in each unshaded entry. In each case, enter an opcode, a value, an expression, or φ as\nappropriate.\nInstr\nALUFN WERF\nBSEL WDSEL\nWR RA2SEL PCSEL ASEL WASEL\nJMP\nφ\nφ\nφ\nφ\nBEQ\nφ\nφ\nφ\nZ\nφ\nLDX\nA+B\nMVZC\nA+B\nZ\nφ\n(Complete the above table)\nEND OF QUIZ!\n(phew!)\n6.004 Spring 2009\n- 5 of 5 -\nQuiz #3"
    },
    {
      "category": "Exam",
      "title": "Quiz #4 Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/ec293fb30bd3b9836de1c58888b5b6c8_MIT6_004s09_quiz04_sol.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.004 Computation Structures\nSpring 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nM A S S A C H U S E T T S I N S T I T U T E O F T E C H N O L O G Y\nDEPARTMENT OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE\n6.004 Computation Structures\nSpring 2009\nQuiz #4: April 24, 2009\nName\nSolutions\nAthena login name\nScore\nAvg: 16.5\nNOTE: Reference material appears on the backs of quiz pages.\nProblem 1 (7 points): Quickies and Trickies\n(A) (1 point) A Beta processor has an interrupt handler invoked by a periodic 60Hz clock.\nThe handler simply inspects the high-order bit of the XP register to see if it is a 1 or 0.\nGive your best estimate of the fraction of the time it finds a 0.\nCircle best answer: 0% ... 50% ... 100%\n(B) (2 points) An application program that stores data in the XP register might fail due to\n(circle YES or NO for each):\nInterrupt handler returns to wrong location: YES ... NO\nApplication data in XP changes unexpectedly: YES ... NO\nInterrupt executes wrong handler: YES ... NO\n(C) (2 points) Decrementing the saved PC of an interrupted program often indicates (circle\nYES or NO for each):\nre-entrant interrupts: YES ... NO\na cache miss: YES ... NO\n\"busy waiting\": YES ... NO\na programming error: YES ... NO\n(D) (2 points) Use of a Translation Lookaside Buffer (circle YES or NO for each):\ndecreases cache hit rate: YES ... NO\ndecreases average address translation time: YES ... NO\nprevents handler re-entrance: YES ... NO\n6.004 Spring 2009\n- 1 of 4 -\nQuiz #4\n\nProblem 2 (7 points): Cache Management\nFour otherwise identical Beta systems have slightly different cache configurations. Each cache\nhas a total of 8 lines each caching a single 32-bit data word, and caches both instruction and data\nfetches. However, the caches differ in their associativity as follows:\nCache C1: Direct mapped, 8-word cache.\nCache C2: 2-way set associative (4 sets of 2 lines), LRU replacement.\nCache C3: Fully associative, LRU replacement.\nYour task is to answer questions about the performance, measured by hit ratio, of these cache\ndesigns on the following tiny benchmarks. Note that each benchmark involves instruction fetches\nstarting at location 0 and data accesses in the neighborhood of location 1024 (= 210).\n.=0\n|| Benchmark B0\n.=0\n|| Benchmark B1\nCMOVE(100, R1)\nCMOVE(100, R1)\nLOOP: LD(R31, 1024, R0)\nLOOP: LD(R31, 1024+4, R0)\nSUBC(R1, 1, R1)\nSUBC(R1, 1, R1)\nBNE(R1, LOOP)\nBNE(R1, LOOP)\nHALT()\nHALT()\n.=0\n|| Benchmark B2\n.=0\n|| Benchmark B3\n.=0\n|| Benchmark B4\nCMOVE(100, R1)\nCMOVE(100, R1)\nCMOVE(100, R1)\nLOOP: LD(R31, 1024+4, R0)\nLOOP: LD(R31, 1024+4, R0)\nLOOP: LD(R31, 1024+4, R0)\nLD(R31, 1024+8, R0)\nLD(R31, 1024+8, R0)\nLD(R31, 1024+8, R0)\nSUBC(R1, 1, R1)\nLD(R31, 1024+12, R0)\nLD(R31, 1024+12, R0)\nBNE(R1, LOOP)\nSUBC(R1, 1, R1)\nLD(R31, 1024+16, R0)\nHALT()\nBNE(R1, LOOP)\nSUBC(R1, 1, R1)\nHALT()\nBNE(R1, LOOP)\nHALT()\n(A) (1 point) Which benchmark yields the best hit ratio with\ncache C1?\n(circle one): B0 ... B1 ... B2 ... B3 ... B4\n(B) (2 points) Select the value that best approximates the hit ratio with cache C1 on\nBenchmark B1.\n(select approx hit ratio): 0% ... 25% ... 50% ... 75% ... 100%\n(C) (2 points) Which cache yields the best hit ratio with benchmark B3?\n(circle one): C1 ... C2 ... C3\n(D) (2 points) Which cache, if any, yields a hit ratio of zero (0%) with benchmark B4?\n(circle one): C1 ... C2 ... C3 ... NONE\n6.004 Spring 2009\n- 2 of 4 -\nQuiz #4\n\nProblem 3 (8 points): Memory Systems\nA simple computer system using the Beta processor has memory components as diagrammed\nbelow:\nCPU\nCache\nMMU\n(PageMap)\nMain\nMemory\nRAM\nSome parameters of the system include:\nCache design: Direct mapped, 4-byte (1 word) block size, 214 words total data. Cache\naccess (hit) time: 5 ns\nMMU design: single-level PageMap, 230-byte virtual address space,\n228-byte physical address space, page size 214 bytes.\nMMU access (translation) time: 5ns.\nRAM (main memory) access (read or write) time: 40 ns.\nAssume that other times (e.g. gate delays) are insignificant compared to memory access times.\nNote that all memory accesses -- including LDs and STs as well as instruction fetches -- are\nmade via the above path.\n(A) (1 point) Assume that each page map entry includes Resident and Dirty bits as well as a\nphysical page number. What is the total size, in bits, of the pagemap storage?\n220 = 16 × 216\nTotal page map size (bits): ____________________________________\n(B) (2 points) What is the memory access time for a cache hit? A miss?\nAccess times on HIT: _________ns; on MISS: ________ns\n(C) (1 point) Assuming a 90% hit rate, what is the expected average access time?\nAverage access time for 90% hit rate: ________ns\n(D) (2 points) Which, if any, of the following operations must the operating system perform\non a context switch? Circle YES or NO for each:\nRead pages from disk corresponding to new virtual memory contents? YES ... NO\nReload PageMap with new context? YES ... NO\nClear cache Valid bits? YES ... NO\n6.004 Spring 2009\n- 3 of 4 -\nQuiz #4\n\nNoah Doze, one of the few who actually stayed awake during the entire VM lecture in 6.004,\nargues that a clever redesign of the memory system using a K-way set associative cache of the\nsame total (214-word) size, for some carefully chosen K, will allow the cache and page map\nlookups to happen concurrently.\n(E) (2 points) Fill in the details of Noah's argument in the blanks below. Continue to assume\na 90% hit ratio.\nAppropriate (minimal) value of K: ________\nNew average access time: ________ns.\nProblem 4 (3 points): Mystery Handler\nYou are exploring an operating system very much like the one sketched in lecture, and have\ndiscovered the following code for a mysterious, undocumented supervisor call handler:\nMystery_SVC_handler()\n{\nUser.Regs[0] = User.Regs[0] - 1;\nif (User.Regs[0] != 0)\n{\nUser.Regs[XP] = User.Regs[XP] - 4;\nScheduler();\n}\n}\nRecall that the C operator != means \"not equal\".\nYour task is to provide documentation explaining, to an application developer, the impact of\nexecuting this SVC on a running application and on other processes running on the system.\n(A) (3 points) Describe, in few sentences, the effect of executing this mystery SVC by an\napplication. Be sure to mention the role of and impact on any relevant register contents.\nThis SVC causes the current process to \"sleep\" (not execute any\nfurther instructions) for the next R0 - 1 scheduler cycles. R0 will be\nset to 0 when the SVC completes. If R0 is 1, it has no effect beyond\nsetting R0 to 0.\nWe gave full credit (3 points) for all answers that included at least the first sentence.\n(Brief documentation of mystery SVC)\nEND OF QUIZ!\n(phew!)\n6.004 Spring 2009\n- 4 of 4 -\nQuiz #4"
    },
    {
      "category": "Exam",
      "title": "Quiz #5",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/393b699df0170086df0583893648e1ba_MIT6_004s09_quiz05.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.004 Computation Structures\nSpring 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nM A S S A C H U S E T T S I N S T I T U T E O F T E C H N O L O G Y\nDEPARTMENT OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE\n6.004 Computation Structures\nSpring 2009\nQuiz #5: May 8, 2009\nName\nAthena login name\nScore\nNOTE: Reference material and scratch diagrams appear on the backs of quiz pages.\nProblem 1 (2 points): I should have stayed awake during that lecture...\n(A) (1 point) A trend in modern computers is to replace parallel, shared backplane buses with\n1) Hypercube networks\n2) Serial, point-to-point switched connections\n3) Wireless networks\n4) Ribbon cables\n5) Coaxial cables\n6) None of the above\nChoose best answer (1 thru 6): ______\n(B) (1 point) If we take into consideration physical realities like speed-of-light delays and\nminimum node size, the asymptotic worst-case latency between nodes of an N-node\nbinary tree network is:\n1) O(log(N))\n2)\n)\n3)\n)\n4) O(N)\n5) O(N 2)\n6) O(N 3)\n7) None of the above\nChoose best answer (1 thru 7): ______\nO( N\nO( N\n6.004 Spring 2009\n- 1 of 6 -\nQuiz #5\n\nProblem 2 (10 points): Process Synchronization\nGill Bates, who dropped out of MIT to found the fledgling Megahard Corporation, has decided to\nbuy electric trains for his three kids: Alice, Bobby, and Chip. Gill is strapped for cash, due to the\nstartup; he can't afford to buy a complete setup for each child. He vows that after Megahard\nmakes him a billionaire, he will buy each kid a complete set of real trains. But for now, he has\nWith this setup, each of Alice,\nBobby, and Chip have their own\ntrains, and each train travels in a\ncircular path in the indicated\n(counterclockwise) direction.\nNote that there is a section of\ntrack shared between each pair\nof trains. In order to avoid\nsibling battles due to train\nwrecks, Gill has devised a\nsystem for segmenting the\ntrack, and has programmed each\ntrain to use a semaphore to\navoid simultaneous use of the\nshared track sections ac, ab,\nand bc.\ndecided to compromise with three trains which run on a common track layout shaped as follows:\nGill's code is as follows:\nShared Memory :\nsemaphore S=???;\nProcess A:\nProcess B:\nProcess C:\nwhile (ARun)\nwhile (BRun)\nwhile (CRun)\n{ TravelTo(a2);\n{ TravelTo(b2);\n{ TravelTo(c2);\nwait(S);\nwait(S);\nwait(S);\nTravelTo(a1);\nTravelTo(b1);\nTravelTo(c1);\nsignal(S);\nsignal(S);\nsignal(S);\n}\n}\n}\nNote that TravelTo(x) moves the train forward until the train is entirely enclosed in the track\nsegment labled x. Trains A, B, and C start out in segments a1, b1, and c1 respectively.\n(A) (1 point) What should the initial value of the semaphore S be?\nInitial value for S: _____\n6.004 Spring 2009\n- 2 of 6 -\nQuiz #5\n\nAlice, a precocious 4-year-old, keeps asking her daddy why her train (marked \"A\") waits while B\nis traveling from segment bc to b1 and C is using segment c1, none of which are used by A.\n(B) (1 point) Choose the best explanation of the problem cited by Alice.\nE1: There's a deadlock.\nE2: Some unenforced essential precedence constraint.\nE3: Some nonessential precedence constraint enforced.\nE4: Some wait without a corresponding signal.\nE5: There's no problem, tell Alice to shut up and go to bed.\nGive number of best explanation: _______\nAfter some deliberation, Gill modifies the code in the trains as follows:\nShared Memory :\nsemaphore Sab=1, Sbc=1, Sac=1;\nProcess A:\nProcess B:\nProcessC:\nwhile (ARun)\nwhile (BRun)\nwhile (CRun)\n{ TravelTo(a2);\n{ TravelTo(b2);\n{ TravelTo(c2);\nwait(Sac);\nwait(Sab);\nwait(Sbc);\nTravelTo(ac);\nTravelTo(ab);\nTravelTo(bc);\nwait(Sab);\nwait(Sbc);\nwait(Sac);\nTravelTo(ab);\nTravelTo(bc);\nTravelTo(ac);\nsignal(Sac);\nsignal(Sab);\nsignal(Sbc);\nTravelTo(a1);\nTravelTo(b1);\nTravelTo(c1);\nsignal(Sab);\nsignal(Sbc);\nsignal(Sac);\n}\n}\n}\nAlice and Chip try the new setup; Bobby is busy painting his train with peanut butter and doesn't\nrun it during this test. The A and C trains run flawlessly for hours. Gill wonders which\ncombinations of TravelTo operations within processes A and C are prohibited by his semaphores.\n(C) (2 points) Which of the following operations might process C execute while\nprocess A is executing TravelTo(ac)? Circle YES or NO for each case.\nCan C be executing TravelTo(bc)? YES ... NO\nCan C be executing TravelTo(ac)? YES ... NO\nCan C be executing TravelTo(c1)? YES ... NO\n6.004 Spring 2009\n- 3 of 6 -\nQuiz #5\n\nBobby returns and adds his train to the setup; after a few minutes all three trains stop perma\nnently. Gill investigates the system, examining the state of each process.\n(D)(1 point) Which line of code does he find Process A executing?\n(give line of code)\n(E) (1 point) On which track segment has train C stopped?\nIndicate segement at which C has stopped: _______\n(F) (2 points) What values are in each of the semaphores?\nValues in Sab: _____; Sac: _____; Sbc: _____\nMeanwhile Chip, an 11-month old prodigy, insists on putting his train on the track so that it\ntravels in a clockwise direction (while the other trains travel counterclockwise). After\nunsuccessfully arguing with Chip, Gill finally changes the code in Process C to reflect the change\nin the direction of Chip's train. To Gill's amazement, the trains now run perfectly. Chip smiles\nwith great satisfaction at his fix. Unfortunately, he can't explain it to Gill since he hasn't learned\nto talk yet.\n(G)(1 point) Gill's modified process C code still contains two wait operations\nfollowed by two signal operations. What are the arguments to the wait\ncalls in the new code?\nFirst call: wait(_______)\nSecond call: wait(_______)\n(H)(1 point) Choose the best generalization of Chip's fix as a rule for allocating\nmultiple resources in a multiprocess system.\nR1: Never use more than one semaphore in a multiprocess\nsystem.\nR2: Make each process allocate required resources in a\nglobal, prescribed order.\nR3: Always release resources in the opposite order from\nthat with which they were allocated.\nR4: Never make more than two trains take\ncounterclockwise paths.\nGive number of best generalization: _______\n6.004 Spring 2009\n- 4 of 6 -\nQuiz #5\n\nProblem 3 (8 points): Pipelined Beta\nThis problem concerns the 5-stage Beta pipeline described in Lecture (a diagram of which can be\nfound on back of page 1). This Beta has full bypass and annulment logic.\nConsider the execution of the following sequence in kernel mode on the 5-stage pipelined Beta.\nThe loop sums the first 100 elements of the integer array X and stores the result in Y.\nADDC(R31,400,R1)\n| index = 100 * 4\nXORC(R31,0,R2)\n| clear sum\nA:\nLD(R1,X-4,R3)\n| load next array element\nADD(R3,R2,R2)\n| add it to sum\nSUBC(R1,4,R1)\n| decrement index\nBNE(R1,A,R31)\n| loop unless index is zero\nST(R2,Y,R31)\n| store result\n...\n(A) (5 Points) Fill the blank boxes in the pipeline diagram below by writing the appropriate\ninstruction opcode (ADDC, XORC, LD, ...) in each box, showing the first 12 clock cycles\nof execution. Use the opcode NOP to indicate what happens during pipeline stalls or branch\ndelay slot annulments. There are scratch copies of the diagram on the back of the previous\npage.\nCycle\nIF\nLD\nRF\nXORC\nLD\nALU\nADDC XORC\nLD\nMEM\nADDC XORC\nLD\nWB\nADDC XORC LD\n(B) (3 Points) Add arrows to your pipeline diagram above to indicate any active bypass paths for\neach clock cycle. The arrow should point from the stage where the bypassed value comes\nfrom, to the stage where the bypassed value is used. For example, in Cycle 4 there's an\nupward arrow pointing from the ADDC opcode in the MEM stage to the LD opcode in the\nRF stage .\n6.004 Spring 2009\n- 5 of 6 -\nQuiz #5\n\nProblem 4 (5 points): Broken pipeline\nYou've been given a 5-stage pipelined Beta processor as shown in lecture, whose diagram can be\nfound on the back of page 1. Unfortunately, the Beta you've been given is defective: it has no\nbypass paths, annulment of instructions in branch delay slots, or pipeline stalls.\nYou undertake to convert some existing code,\n...\ndesigned to run on an unpipelined Beta, to run\nNOP() NOP() NOP() NOP()\non your defective pipelined processor. The\nscrap of code on the left is a sample of the\nLoop:\nAA:\nBB:\nCC:\nLD(R0, 0, R1)\nMUL(R2, R3, R4)\nXOR(R1, R0, R0)\nBNE(R4, LOOP, R0)\nprogram to be converted. It doesn't make\nmuch sense to you - it doesn't to us either -\nbut you are to add the minimum number of\nNOP instructions at the various tagged points\nin this code to make it give the same results\non your defective pipelined Beta as it gives on\na normal, unpipelined Beta.\nXOR(R1, R2, R6)\nNOP() NOP() NOP() NOP()\nNote that the code scrap begins and ends with\nsequences of NOPs; thus you don't need to\n...\nworry about pipeline hazards involving\ninteractions with instructions outside of the\nregion shown.\nScratch instruction pipeline grids are provided for your convenience on the backs of this page and\nthe previous page.\n(A) (4 points) Specify the minimal number of NOP instructions (defined as\nADD(R31,R31,R31)) to be added at each of the labeled points in the above program.\nNOPs at Loop: ______\nNOPs at AA: ______\nNOPs at BB: ______\nNOPs at CC: ______\n(B) (1 point) On a fully functional 5-stage Beta pipeline (with working bypass, annul, and\nstall logic), the above code will run fine with no added NOPs. How many clock cycles\nof execution time are required by the fully functional 5-stage pipelined Beta for each\niteration through the loop?\nClocks per loop iteration: ________\nEND OF QUIZ!\n(phew!)\n6.004 Spring 2009\n- 6 of 6 -\nQuiz #5"
    },
    {
      "category": "Exam",
      "title": "Quiz #5 Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/0c061afd3797eff8a166068311e242c5_MIT6_004s09_quiz05_sol.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.004 Computation Structures\nSpring 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nM A S S A C H U S E T T S I N S T I T U T E O F T E C H N O L O G Y\nDEPARTMENT OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE\n6.004 Computation Structures\nSpring 2009\nQuiz #5: May 8, 2009\nName\nSolutions\nAthena login name\nScore\nAvg: 20.1\nNOTE: Reference material and scratch diagrams appear on the backs of quiz pages.\nProblem 1 (2 points): I should have stayed awake during that lecture...\n(A) (1 point) A trend in modern computers is to replace parallel, shared backplane buses with\n1) Hypercube networks\n2) Serial, point-to-point switched connections\n3) Wireless networks\n4) Ribbon cables\n5) Coaxial cables\n6) None of the above\nChoose best answer (1 thru 6): ______\n(B) (1 point) If we take into consideration physical realities like speed-of-light delays and\nminimum node size, the asymptotic worst-case latency between nodes of an N-node\nbinary tree network is:\n1) O(log(N))\n2)\n)\n3)\n)\n4) O(N)\n5) O(N 2)\n6) O(N 3)\n7) None of the above\nChoose best answer (1 thru 7): ______\nO( N\nO( N\n6.004 Spring 2009\n- 1 of 6 -\nQuiz #5\n\nProblem 2 (10 points): Process Synchronization\nGill Bates, who dropped out of MIT to found the fledgling Megahard Corporation, has decided to\nbuy electric trains for his three kids: Alice, Bobby, and Chip. Gill is strapped for cash, due to the\nstartup; he can't afford to buy a complete setup for each child. He vows that after Megahard\nmakes him a billionaire, he will buy each kid a complete set of real trains. But for now, he has\nWith this setup, each of Alice,\nBobby, and Chip have their own\ntrains, and each train travels in a\ncircular path in the indicated\n(counterclockwise) direction.\nNote that there is a section of\ntrack shared between each pair\nof trains. In order to avoid\nsibling battles due to train\nwrecks, Gill has devised a\nsystem for segmenting the\ntrack, and has programmed each\ntrain to use a semaphore to\navoid simultaneous use of the\nshared track sections ac, ab,\nand bc.\ndecided to compromise with three trains which run on a common track layout shaped as follows:\nGill's code is as follows:\nShared Memory :\nsemaphore S=???;\nProcess A:\nProcess B:\nProcess C:\nwhile (ARun)\nwhile (BRun)\nwhile (CRun)\n{ TravelTo(a2);\n{ TravelTo(b2);\n{ TravelTo(c2);\nwait(S);\nwait(S);\nwait(S);\nTravelTo(a1);\nTravelTo(b1);\nTravelTo(c1);\nsignal(S);\nsignal(S);\nsignal(S);\n}\n}\n}\nNote that TravelTo(x) moves the train forward until the train is entirely enclosed in the track\nsegment labled x. Trains A, B, and C start out in segments a1, b1, and c1 respectively.\n(A) (1 point) What should the initial value of the semaphore S be?\nInitial value for S: _____\n6.004 Spring 2009\n- 2 of 6 -\nQuiz #5\n\nAlice, a precocious 4-year-old, keeps asking her daddy why her train (marked \"A\") waits while B\nis traveling from segment bc to b1 and C is using segment c1, none of which are used by A.\n(B) (1 point) Choose the best explanation of the problem cited by Alice.\nE1: There's a deadlock.\nE2: Some unenforced essential precedence constraint.\nE3: Some nonessential precedence constraint enforced.\nE4: Some wait without a corresponding signal.\nE5: There's no problem, tell Alice to shut up and go to bed.\nE3\nGive number of best explanation: _______\nAfter some deliberation, Gill modifies the code in the trains as follows:\nShared Memory :\nsemaphore Sab=1, Sbc=1, Sac=1;\nProcess A:\nProcess B:\nProcessC:\nwhile (ARun)\nwhile (BRun)\nwhile (CRun)\n{ TravelTo(a2);\n{ TravelTo(b2);\n{ TravelTo(c2);\nwait(Sac);\nwait(Sab);\nwait(Sbc);\nTravelTo(ac);\nTravelTo(ab);\nTravelTo(bc);\nwait(Sab);\nwait(Sbc);\nwait(Sac);\nTravelTo(ab);\nTravelTo(bc);\nTravelTo(ac);\nsignal(Sac);\nsignal(Sab);\nsignal(Sbc);\nTravelTo(a1);\nTravelTo(b1);\nTravelTo(c1);\nsignal(Sab);\nsignal(Sbc);\nsignal(Sac);\n}\n}\n}\nAlice and Chip try the new setup; Bobby is busy painting his train with peanut butter and doesn't\nrun it during this test. The A and C trains run flawlessly for hours. Gill wonders which\ncombinations of TravelTo operations within processes A and C are prohibited by his semaphores.\n(C) (2 points) Which of the following operations might process C execute while\nprocess A is executing TravelTo(ac)? Circle YES or NO for each case.\nCan C be executing TravelTo(ac)? YES ... NO\nCan C be executing TravelTo(c1)? YES ... NO\nCan C be executing TravelTo(bc)? YES ... NO\n6.004 Spring 2009\n- 3 of 6 -\nQuiz #5\n\nBobby returns and adds his train to the setup; after a few minutes all three trains stop perma\nnently. Gill investigates the system, examining the state of each process.\n(D)(1 point) Which line of code does he find Process A executing?\nwait(Sab);\n(give line of code)\n(E) (1 point) On which track segment has train C stopped?\nbc\nIndicate segement at which C has stopped: _______\n(F) (2 points) What values are in each of the semaphores?\nValues in Sab: _____; Sac: _____; Sbc: _____\nMeanwhile Chip, an 11-month old prodigy, insists on putting his train on the track so that it\ntravels in a clockwise direction (while the other trains travel counterclockwise). After\nunsuccessfully arguing with Chip, Gill finally changes the code in Process C to reflect the change\nin the direction of Chip's train. To Gill's amazement, the trains now run perfectly. Chip smiles\nwith great satisfaction at his fix. Unfortunately, he can't explain it to Gill since he hasn't learned\nto talk yet.\n(G)(1 point) Gill's modified process C code still contains two wait operations\nfollowed by two signal operations. What are the arguments to the wait\ncalls in the new code?\nSac\nFirst call: wait(_______)\nSbc\nSecond call: wait(_______)\n(H)(1 point) Choose the best generalization of Chip's fix as a rule for allocating\nmultiple resources in a multiprocess system.\nR1: Never use more than one semaphore in a multiprocess\nsystem.\nR2: Make each process allocate required resources in a\nglobal, prescribed order.\nR3: Always release resources in the opposite order from\nthat with which they were allocated.\nR4: Never make more than two trains take\ncounterclockwise paths.\nR2\nGive number of best generalization: _______\n6.004 Spring 2009\n- 4 of 6 -\nQuiz #5\n\nProblem 3 (8 points): Pipelined Beta\nThis problem concerns the 5-stage Beta pipeline described in Lecture (a diagram of which can be\nfound on back of page 1). This Beta has full bypass and annulment logic.\nConsider the execution of the following sequence in kernel mode on the 5-stage pipelined Beta.\nThe loop sums the first 100 elements of the integer array X and stores the result in Y.\nADDC(R31,400,R1)\n| index = 100 * 4\nXORC(R31,0,R2)\n| clear sum\nA:\nLD(R1,X-4,R3)\n| load next array element\nADD(R3,R2,R2)\n| add it to sum\nSUBC(R1,4,R1)\n| decrement index\nBNE(R1,A,R31)\n| loop unless index is zero\nST(R2,Y,R31)\n| store result\n...\n(A) (5 Points) Fill the blank boxes in the pipeline diagram below by writing the appropriate\ninstruction opcode (ADDC, XORC, LD, ...) in each box, showing the first 12 clock cycles\nof execution. Use the opcode NOP to indicate what happens during pipeline stalls or branch\ndelay slot annulments. There are scratch copies of the diagram on the back of the previous\npage.\nCycle\nIF\nLD\nADD\nSUBC SUBC SUBC BNE\nST\nLD\nADD\nSUBC\nRF\nXORC\nLD\nADD\nADD\nADD SUBC BNE\nNOP\nLD\nADD\nALU\nADDC\nXORC\nLD\nNOP\nNOP\nADD\nSUBC BNE\nNOP\nLD\nMEM\nADDC\nXORC\nLD\nNOP\nNOP\nADD\nSUBC BNE\nNOP\nWB\nADDC\nXORC\nLD\nNOP\nNOP\nADD\nSUBC BNE\n(B) (3 Points) Add arrows to your pipeline diagram above to indicate any active bypass paths for\neach clock cycle. The arrow should point from the stage where the bypassed value comes\nfrom, to the stage where the bypassed value is used. For example, in Cycle 4 there's an\nupward arrow pointing from the ADDC opcode in the MEM stage to the LD opcode in the\nRF stage .\n6.004 Spring 2009\n- 5 of 6 -\nQuiz #5\n\nProblem 4 (5 points): Broken pipeline\nYou've been given a 5-stage pipelined Beta processor as shown in lecture, whose diagram can be\nfound on the back of page 1. Unfortunately, the Beta you've been given is defective: it has no\nbypass paths, annulment of instructions in branch delay slots, or pipeline stalls.\nYou undertake to convert some existing code,\n...\ndesigned to run on an unpipelined Beta, to run\nNOP() NOP() NOP() NOP()\non your defective pipelined processor. The\nscrap of code on the left is a sample of the\nLoop:\nAA:\nBB:\nCC:\nLD(R0, 0, R1)\nMUL(R2, R3, R4)\nXOR(R1, R0, R0)\nBNE(R4, LOOP, R0)\nprogram to be converted. It doesn't make\nmuch sense to you - it doesn't to us either -\nbut you are to add the minimum number of\nNOP instructions at the various tagged points\nin this code to make it give the same results\non your defective pipelined Beta as it gives on\na normal, unpipelined Beta.\nXOR(R1, R2, R6)\nNOP() NOP() NOP() NOP()\nNote that the code scrap begins and ends with\nsequences of NOPs; thus you don't need to\n...\nworry about pipeline hazards involving\ninteractions with instructions outside of the\nregion shown.\nScratch instruction pipeline grids are provided for your convenience on the backs of this page and\nthe previous page.\n(A) (4 points) Specify the minimal number of NOP instructions (defined as\nADD(R31,R31,R31)) to be added at each of the labeled points in the above program.\nNOPs at Loop: ______\nNOPs at AA: ______\nNOPs at BB: ______\nNOPs at CC: ______\n(B) (1 point) On a fully functional 5-stage Beta pipeline (with working bypass, annul, and\nstall logic), the above code will run fine with no added NOPs. How many clock cycles\nof execution time are required by the fully functional 5-stage pipelined Beta for each\niteration through the loop?\nClocks per loop iteration: ________\nEND OF QUIZ!\n(phew!)\n6.004 Spring 2009\n- 6 of 6 -\nQuiz #5"
    },
    {
      "category": "Resource",
      "title": "BSim",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/9082ab67c9d6bf44cdd39737b382b790_MIT6_004s09_lab_tool_bsim.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n6.004 Computation Structures\nSpring 2009\n\nM A S S A C H U S E T T S I N S T I T U T E O F T E C H N O L O G Y\nDEPARTMENT OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE\n\n6.004 Computation Structures\nBSim\n\nIntroduction to BSim\n\nBSim is a simulator for the 6.004 Beta architecture. The BSim user interface is very similar to\nJSim's: there's a simple editor for typing in your program and some tools for assembling the\nprogram into binary, loading the program into the simulated Beta's memory, executing the\nprogram and examining the results.\n\nTo run BSim, login to an Athena console. After signing onto the Athena station, add the 6.004\nlocker to gain access to the design tools and model files (you only have to do this once each\nsession):\n\nathena% add 6.004\n\nStart BSim running in a separate window by typing\n\nathena% bsim &\n\nIt can take a few moments for the Java runtime system to start up, please be patient! BSim takes\nas input a assembly language program to be executed. The initial BSim window is a very simple\neditor that lets you enter and modify your netlist. If you use a separate editor to create your\nnetlists, you can have BSim load your netlist files when it starts:\n\nathena% bsim filename ... filename &\n\nThere are various handy buttons on the BSim toolbar:\n\nExit. Asks if you want to save any modified file buffers and then exits BSim.\n\nNew file. Create a new edit buffer called \"untitled\". Any attempts to save this\nbuffer will prompt the user for a filename.\n\nOpen file. Prompts the user for a filename and then opens that file in its own edit\nbuffer. If the file has already been read into a buffer, the buffer will be reloaded\nfrom the file (after asking permission if the buffer has been modified).\n\nClose file. Closes the current edit buffer after asking permission if the buffer has\nbeen modified.\n\nReload file. Reload the current buffer from its source file after asking permission\nif the buffer has been modified. This button is useful if you are using an external\n6.004 Computation Structures\n- 1 -\nBSim\n\neditor to modify the netlist and simply want to reload a new version for\nsimulation.\n\nSave file. If any changes have been made, write the current buffer back to its\nsource file (prompting for a file name if this is an untitled buffer created with the\n\"new file\" command). If the save was successful, the old version of the file is\nsaved with a \".bak\" extension.\n\nSave file, specifying new file name. Like \"Save file\" but prompts for a new file\nname to use.\n\nSave all files. Like \"save file\" but applied to all edit buffers.\n\nAssemble the current buffer, i.e., convert it into binary and load it into the\nsimulated Beta's memory. Any errors detected will be flagged in the editor\nwindow and described in the message area at the bottom of the window. If the\nassembly completes successfully, a window showing the Beta datapath is created\nfrom which you can start execution of the program.\n\nAssemble the current buffer and output the resulting binary to a file whose name\nis the same as source file for the current buffer with \".bin\" appended.\n\nUsing information supplied in the checkoff file, check for specified memory\nvalues. If all the checks are successful, submit the program to the on-line\nassignment system.\n\nThe Display window has some additional toolbar buttons that are used to control the simulation.\nThe values shown in the window reflect the values on Beta signals after the current instruction\nhas been fetched and executed but just before the register file is updated at the end of the cycle.\n\nStop execution and update the datapath display.\n\nReset the contents of the PC and registers to 0, and memory locations to the\nvalues they had just after assembly was complete. You have to stop a running\nsimulation before a reset.\n\nStart simulation and run until a HALT() instruction is executed or a breakpoint is\nreached. You can stop a running simulation using the stop control described\nabove. For maximum simulation speed, the datapath display is not updated until\nthe simulation is stopped.\n6.004 Computation Structures\n- 2 -\nBSim\n\nExecute the program for a single cycle and then update the display. Very useful\nfor following your program's operation instruction-by-instruction.\n\nToggle visualization between the programmer's panel (the default) and the\nanimated datapath.\n\nBring up a window that let's you configure the cache parameters for main\nmemory.\n\nIf \".options tty\" is specified by the program, a small 5-line typeout window appears at the bottom\nof the datapath window. You can output characters to this window by executing a WRCHAR()\ninstruction after placing the character value in R0. The tty option also allows for type-in: any\ncharacter typed by the user causes an interrupt to location 12; RDCHAR() can be used to fetch\nthe character value into R0. Clicking the mouse will cause an interrupts to location 16; CLICK()\ncan be used to fetch the coordinates of the last click into R0. The coordinates are encoded as\n(x<<16)+y, or -1 if there has been no mouse click since the last call to CLICK().\n\nIf \".options clock\" is specified by the program, an interrupt to location 8 is generated every\n10,000 cycles. (Remember though that interrupts are disabled until the program enters user mode\n- see section 6.3 of the Beta documentation.)\n\nIntroduction to assembly language\n\nBSim incorporates an assembler: a program that converts text files into binary memory data.\nThe simplest assembly language program is a sequence of numerical values which are\nconverted to binary and placed in successive byte locations in memory:\n\n| Comments begin with vertical bar and end at a newline\n\n37 3 255 | decimal (the default radix)\n0b100101 | binary (note the 0b prefix)\n0x25 | hexadecimal (note the 0x prefix)\n\n'a' | character constants\n\nValues can also be expressions; e.g., the source file\n\n37+0b10-0x10 24 - 0x1 4*0b110-1 0xF7 % 0x20\n\ngenerates 4 bytes of binary output, each with the value 23. Note the operators have no\nprecedence - you have to use parentheses to avoid simple left-to-right evaluation. The\navailable operators are\n\n-\nunary minus\n~\nbit-wise complement\n+\naddition\n-\nsubtraction\n*\nmultiplication\n6.004 Computation Structures\n- 3 -\nBSim\n\n6.004 Computation Structures\n- 4 -\nBSim\n\n/\ndivision\n%\nmodulo (result is always positive!)\n>>\nright shift\n<<\nleft shift\n\nWe can also define symbols for use in expressions:\n\nx = 0x1000 | address in memory of variable X\ny = 0x10004 | another address\n\n| Symbolic names for registers\nR0 = 0\n\nR1 = 1\n\n...\n\nR31 = 31\n\nNote that symbols are case-sensitive: \"Foo\" and \"foo\" are different symbols. A special\nsymbol named \".\" (period) means the address of the next byte to be filled by the assembler:\n\n. = 0x100 | assemble into location 0x100\n1 2 3 4\nfive = . | symbol five has the value 0x104\n5 6 7 8\n. = . + 16 | skip 16 bytes\n9 10 11 12\n\nLabels are symbols that represent memory address. They can be set with the following\nspecial syntax:\n\nX: | this is an abbreviation for X = .\n\nFor example the table on the left shows what main memory will contain after assembling the\nprogram on the right.\n\n---- MAIN MEMORY ----\nbyte: 3 2 1 0\n. = 0x1000\n1000: 09 04 01 00 sqrs: 0 1 4 9\n1004: 31 24 19 10 16 25 36 49\n1008: 79 64 51 40 64 81 100 121\n100C: E1 C4 A9 90 144 169 196 225\n1010: 00 00 00 10 slen: LONG(. - sqrs)\n\nMacros are parameterized abbreviations:\n\n| macro to generate 4 consecutive bytes\n.macro consec(n) n n+1 n+2 n+3\n\n| invocation of above macro\nconsec(37)\n\nThe macro invocation above has the same effect as\n\n37 38 39 40\n\n6.004 Computation Structures\n- 5 -\nBSim\n\nNote that macros evaluate their arguments and substitute the resulting value for occurrences\nof the corresponding formal parameter in the body of the macro. Here are some macros for\nbreaking multi-byte data into byte-size chunks\n\n| assemble into bytes, little-endian format\n.macro WORD(x) x%256 (x/256)%256\n.macro LONG(x) WORD(x) WORD(x>>16)\nLONG(0xdeadbeef)\n\nHas the same effect as\n\n0xef 0xbe 0xad 0xde\n\nThe body of the macro includes the remainder of the line on which the .macro directive\nappears. Multi-line macros can be defined by enclosing the body in \"{\" and \"}\".\n\nbeta.uasm contains symbol definitions for all the registers (R0, ..., R31, BP, LP, SP, XP, r0,\n..., r31, bp, lp, sp, xp) and macro definitions for all the Beta instructions:\n\nOP(Ra,Rb,Rc)\nReg[Rc] ← Reg[Ra] op Reg[Rb]\nOpcodes:\nADD, SUB, MUL, DIV, AND, OR, XOR\n\nCMPEQ, CMPLT, CMPLE, SHL, SHR, SRA\nOPC(Ra,literal,Rc) Reg[Rc] ← Reg[Ra] op SEXT(literal15:0)\nOpcodes:\nADDC, SUBC, MULC, DIVC, ANDC, ORC, XORC\n\nCMPEQC, CMPLTC, CMPLEC, SHLC, SHRC, SRAC\nLD(Ra,literal,Rc)\nReg[Rc] ← Mem[Reg[Ra] + SEXT(literal)]\nST(Rc,literal,Ra)\nMem[Reg[Ra] + SEXT(literal)] ← Reg[Rc]\nJMP(Ra,Rc)\nReg[Rc] ← PC + 4; PC ← Reg[Ra]\nBEQ/BF(Ra,label,Rc) Reg[Rc] ← PC + 4; if Reg[Ra] = 0 then PC ← PC + 4 + 4*SEXT(literal)\nBNE/BT(Ra,label,Rc) Reg[Rc] ← PC + 4; if Reg[Ra] = 0 then PC ← PC + 4 + 4*SEXT(literal)\nLDR(Ra,label,Rc)\nReg[Rc] ← Mem[PC + 4 + 4*SEXT(literal)]\n\nAlso included are some convenience macros:\n\nLD(label,Rc)\nexpands to LD(R31,label,Rc)\nST(Ra,label)\nexpands to ST(Ra,label,R31)\nBR(label)\nexpands to BEQ(R31,label,R31)\nCALL(label)\nexpands to BEQ(R31,label,LP)\nRTN()\nexpands to JMP(LP)\nDEALLOCATE(n)\nexpands to SUBC(SP,n*4,SP)\nMOVE(Ra,Rc)\nexpands to ADD(Ra,R31,Rc)\nCMOVE(literal,Rc)\nexpands to ADDC(R31,literal,Rc)\nPUSH(Ra)\nexpands to ADDC(SP,4,SP) ST(Ra,-4,SP)\nPOP(Rc)\nexpands to LD(SP,-4,Rc) ADDC(SP,-4,SP)\n\nHALT()\ncause the simulator to stop execution\n\n6.004 Computation Structures\n- 6 -\nBSim\n\nThe following is a complete example assembly language program:\n\n.include /mit/6.004/bsim/beta.uasm\n\n. = 0\n\n| start assembling at location 0\nLD(input,r0)\n\n| put argument in r0\nCALL(bitrev)\n\n| call the procedure (= BR(bitrev,r28))\nHALT()\n\n| reverse the bits in r0, leave result in r1\nbitrev:\n\nCMOVE(32,r2)\n\n| loop counter\n\nCMOVE(0,r1)\n\n| clear output register\nloop:\n\nANDC(r0,1,r3)\n\n| get low-order bit\nSHLC(r1,1,r1)\n\n| shift output word by 1\nOR(r3,r1,r1)\n\n| OR in new low-order bit\nSHRC(r0,1,r0)\n\n| done with this input bit\nSUBC(r2,1,r2)\n\n| decrement loop counter\nBNE(r2,loop)\n\n| repeat until done\nRTN()\n\n| return to caller (= JMP(r28))\n\ninput:\nLONG(0x12345)\n\n| 32-bit input (in HEX)\n\nThe BSim assembly language processor includes a few helpful directives:\n\n.include filename\nProcess the text found in the specified file at this point in the assembly.\n\n.align\n.align expression\nIncrement the value of \".\" until it is 0 modulo the specified value, e.g., \".align 4\" moves\nto the next word boundary in memory. A value of 4 is used if no expression is given.\n\n.ascii \"chars...\"\nAssemble the characters enclosed in quotes into successive bytes of memory. C-like\nescapes can be used for non-printing characters.\n\n.text \"chars...\"\nLike .ascii except an additional 0 byte is added to the end of the string in memory.\n\n.breakpoint\nStop the Beta simulator if it fetches an instruction from the current location (i.e., the\nvalue of \".\" at the point the .breakpoint directive occurred). You can define as many\nbreakpoints as you want.\n\n.protect\nThis directive indicates that subsequent bytes output by the assembler are \"protected,\"\ncausing the simulator to halt if a ST instruction tries to overwrite their value. This\ndirective is useful for protecting code (e.g., the checkoff program) from being overwritten\nby errant programs.\n\n6.004 Computation Structures\n- 7 -\nBSim\n\n.unprotect\nThe opposite of .protect - subsequent bytes output by the assembler are not protected and\ncan be overwritten by the program.\n\n.options ...\nUsed to configure the simulator. Available options:\n\nclk\nenable periodic clock interrupts to location 8\n\nnoclk\ndisable clock interrupts (default)\n\ndiv\nsimulate the DIV instruction (default)\nnodiv\nmake the DIV opcode an illegal instruction\n\nmul\nsimulate the MUL instruction (default)\nnomul\nmake the MUL opcode an illegal instruction\n\nkalways\ndon't let program enter user mode (ie, supervisor bit is always 1)\n\nnokalways allow program to enter user mode (default)\n\ntty\nenable RDCHAR(), WRCHAR(), CLICK() (see end of first section)\n\nnotty\nRDCHAR(), WRCHAR(), CLICK() are disabled (default)\n\nannotate\nif BP is non-zero, label stack frames in the programmer's panel\n\nnoannotate don't annotate stack frames (default)\n\n.pcheckoff ...\n.tcheckoff ...\n.verify ...\nSupply checkoff information to the simulator."
    },
    {
      "category": "Resource",
      "title": "Design Project",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/aa4f35432eb0ff79aa53822063af62e7_MIT6_004s09_lab_project.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.004 Computation Structures\nSpring 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nM A S S A C H U S E T T S I N S T I T U T E O F T E C H N O L O G Y\nDEPARTMENT OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE\n6.004 Computation Structures\nDesign Project\nInstructions\n1. MAKE SURE you have the latest beta.uasm and project files.\n2. optimize your Beta design as described below\n3. using the checkoff file verify that your program operates correctly and submit it\nto the on-line checkoff system (just as for a Lab). There is just one checkoff file\nfor this project. Completing the project will earn up to 10 points.\nProject Description\nFor this project we're asking you to optimize the price/performance of your Beta design. The\n\"price\" is determined from the area of your circuit as reported by JSim at the end of each\nsimulation run (it reports the size in square microns in the message area at the bottom of the\nnetlist window). The \"performance\" is determined by the time needed to complete a set of\nbenchmark programs. Benchmarking is the traditional, if somewhat unreliable, way of\ncharacterizing processor performance. The benchmark suite can be found in\n/mit/6.004/bsim/projcheckoff.uasm; in addition to the functional test run during Lab #6, it\nincludes four benchmarks:\n-\nBenchmark #1 makes two calls to a subroutine that performs an unsigned divide of its\narguments, and then stores the quotients and remainders in main memory.\n-\nBenchmark #2 makes two subroutine calls: one to do an in-place reverse an 11-element\nlist, the second to compare the reversed list with an \"answer\" list to see if all went well.\n-\nBenchmark #3 makes a copy of itself further up in memory and then jumps to the first\nlocation of the copy. The process is repeated 2 times.\n-\nBenchmark #4 just performs a lot register arithmetic and writes the result to memory -\nshould be a slam dunk for pipelined and superscalar machines.\nSee projcheckoff.uasm for details about what values are written by each benchmark. The suite\nrequires on the order of 1200 cycles to execute on a standard beta. Start by giving it plenty of\ntime to run the benchmark; you'll recognize its completion by the 1-instruction BEQ loop at\n0x44. You can then trim run time down to just get to this BEQ, to maximize your score.\nNote: Long simulations of big circuits can produce very large history files, so it\ncan take a while to prepare the waveform plots at the end of your simulation. Be\npatient and be modest in the number of signals you choose to plot.\nSince your optimized design may operate differently than the Beta described in the Lab handouts,\nthe checkoff file (/mit/6.004/jsim/projcheckoff.jsim) is a little different than the others we've\nseen so far this semester. It contains no test circuitry at all; you'll need to provide the main\nmemory, waveform generators for CLK and RESET, etc. - whatever your optimized design\nneeds to execute correctly. The only constraint on your netlist is that you must provide a 32-bit\n6.004 Computation Structures\n- 1 -\nDesign Project\n\nwide, 1024 location main memory called Xmem to store the benchmark program and hold the\nresults. The checkoff file checks certain locations of this memory to see that the benchmark has\ncompleted successfully. If your design passes verification, you can submit it to the on-line\ncheckoff system which will report the number of points your design has received. The points are\ndetermined by the following formula:\nBenmark(r) = 1e-10/(<ending simulation time in seconds>*<size of circuit in meter2>)\nPoints = (Benmark - 10)/2 [rounded; min = 0, max = 10]\nThe smaller your circuit and the faster it completes the benchmark, the better the Benmark. A\ngood Beta design completed as described in the Lab handouts has a Benmark of 15 or more\n(depending on various design choices) and would receive several points if submitted. So you'll\nneed to do some modest innovation to max out on points - see the Hints section for some\nsuggestions.\nYour netlist should have the form:\n// Design Project w/ beta implemention as described in lab handouts\n.include \"/mit/6.004/jsim/nominal.jsim\"\n.include \"/mit/6.004/jsim/stdcell.jsim\"\n.include \"/mit/6.004/jsim/projcheckoff.jsim\"\n// your optimized beta might have different terminals\n.subckt beta clk reset irq ia[31:0] id[31:0] ma[31:0] moe mrd[31:0] wr mwd[31:0]\n...\n.ends\n// matches .subckt above w/ IRQ tied to ground\nXbeta clk reset 0 ia[31:0] id[31:0] ma[31:0] moe mrd[31:0] wr mwd[31:0] beta\n// your memory must also be called Xmem (so the checkoff code can find it!) but\n// can have a different number of ports. It should however be initialized with\n// the contents shown below -- the benchmark suite. A file containing these\n// values can be found at /mit/6.004/jsim/projcheckoff.bin.\nXmem\n+ vdd 0 0 ia[11:2] id[31:0]\n+ moe 0 0 ma[11:2] mrd[31:0]\n+ 0 clk wr ma[11:2] mwd[31:0]\n+ $memory width=32 nlocations=1024\n+ file=\"/mit/6.004/jsim/projcheckoff.bin\"\n// 20ns cycle time, assert RESET for first cycle. Your cycle time may vary...\nVclk clk 0 pulse(3.3,0,9.99ns,.01ns,.01ns,9.99ns)\nVreset reset 0 pwl(0ns 3.3v, 30ns 3.3v, 30.1ns 0v)\n// Run the simulation for 1205 cycles. Your design might require more or less\n// cycles depending on how it executes instructions. Run only as long as necessary\n// to complete the benchmark since the ending simulation time is used to compute\n// your circuit's Benmark\n.tran 24100ns\nAs mentioned in the netlist comments, your memory should be initialized with the binary code\nfor the benchmark suite. A file with the appropriate values can be found at\n/mit/6.004/jsim/projcheckoff.bin - you can use the \"file=...\" argument to the $memory\ncomponent to load that file into your memory.\n6.004 Computation Structures\n- 2 -\nDesign Project\n\nYour first step should be to incorporate your current Beta design into the netlist template shown\nabove, and then adjust the timing of the CLK signal so that your circuit executes the checkoff\ncode correctly. As usual you can click on the green checkmark after running a simulation to\ncheck your results. When you submit your results, the server will tell you your Benmark and\nhow many points you earned. You can submit as often as you'd like and the server will keep\ntrack of your best design to date.\nGood luck! Using good design practices to implement the hints below can result in a Benmark\nover 30. If you get close to this, congratulations! But don't be discouraged if your design\ndoesn't make it on the first try - it takes some practice to get the knack of making circuits both\nfast and small. If you achieve a great Benmark, let us know and come join the 6.004 staff ☺.\nHints for size\n1. The benchmark DOESN'T USE the multiply instructions; so the first thing you should\ndo is to axe your multiplier, if you have one, and take credit for the size reduction. The\nbenchmark does require a 1024-word memory, however; don't bother trying to reduce its\nsize.\n2. The biggest single size reduction comes from eliminating one or more ports on the multi-\nport main memory. Ideally, you'd like to get it down to a single port. To do this you'll\nhave multiplex the memory between fetching instructions and doing data accesses. You\nmight, for example, fetch instructions during the first half of each cycle and do data\naccesses (if necessary) during the second half. You can use a MUX to select between\nthe instruction and data addresses, and use tristate drivers to send write data to the\nmemory's data pins (the memory has these built-in for sending data back to the processor\n- that's what the MOE signal has been controlling). See Appendix 2 of Lab #6 for\ndetails on how the memory operates.\n3. Use logic gates instead of a ROM to generate the necessary control signals. Even if you\ndo this for only some of the control signals, you'll see a noticeable reduction in circuit\nsize. Logic can also be faster than a ROM for control signals in your critical path.\n4. Design an incrementer circuit for adding 4 to the PC instead of using a regular 2-input\nadder. This is easy to do - just think about an adder where the second input is the\nconstant 4 (i.e., all but one of the input bits is zero!) and then eliminate/simplify the\nadder logic appropriately.\n5. See if you can eliminate the separate adder used to calculate the new PC for branch\ninstructions - can you figure out a way to use the ALU instead? Hint: you'll need a more\ncomplicated BSEL MUX circuit that can send either sxt(literal) or 4*sxt(literal) to the\nsecond input to the ALU. If you also pipeline your circuit remember that using the ALU\nfor branch-offset arithmetic might increase the number of branch delay slots.\nHints for speed\n1. minimize load-dependent delays. As you connect the output of a gate to additional\ninputs, its capacitance increases and so changes in the signal value take longer (see the tR\n6.004 Computation Structures\n- 3 -\nDesign Project\n\nand tF columns in the standard cell table on page 2 of Lab #3). Heavily loaded signals\nshould be buffered to reduce the total delay. There are several strengths of INVERTERs\nand BUFFERs available for driving different sized loads.\n2. use inverting logic. NAND, NOR, etc. gates are noticeably faster than their non-\ninverting counterparts (AND, OR, ...). Rearranging logic to use inverting gates can\nmake a big difference performance along the critical path of your circuit. Positive logic\ngates are included in the standard cell library because they are often smaller than their\ninverting logic equivalents and thus may be useful for implementing logic that is not on\nthe critical path.\n3. minimize delay along critical paths. Look at the device and cycle for which the\nminimum observed setup was reported - see Appendix 1 of Lab #6 to learn how to do\nthis. By tracing back through the circuitry that lead to this critical path you should get\nsome good ideas about where your circuit could use a speed boost. It's often possible to\nrearrange your logic to reduce the number of gate delays along the critical path,\nsometimes at the cost of additional gates elsewhere but that's a tradeoff which is often\nworthwhile. Using the \"complex\" functions provided by the AOI21 and OAI21 gates\ncould be quite helpful in this regard. In a ripple-carry adder, you want to minimize the\nCIN to COUT delay when trying to optimize overall circuit performance.\n4. use a different adder architecture. Consider using an architecture that avoids rippling\nthe carry through all 32 bits of the adder: carry-select and carry-lookahead are two\ntechniques that are described in more detail below.\n5. An unpipelined Beta makes two accesses to the relatively slow main memory (4ns access\ntime for the SRAM-sized memory we've been using). Even a two-stage pipeline\n(instruction fetch and everything else) can come close to cutting the cycle time in half.\nA pipelined design is fun but a lot of work, so try the other hints first - they should be\nsufficient to max out on points.\nAppendix 1: Carry-select adders\nIdea: do two additions, one assuming the carry-in is 0, the other assuming the carry-in is 1. Use a\nMUX to select the appropriate answers when the correct carry-in is known:\n6.004 Computation Structures\n- 4 -\nDesign Project\nDi\nag\nra m of\na c ar ry\n-se\nlec\nt a dd er\n.\nFigure by MIT OpenCourseWare.\n\nBlocks on the left can include more bits since there is more ripple time while waiting for the\nselect signal to arrive.\nWith one stage (i.e., using carry-select on the top sixteen bits of your adder driven by the carry-\nout from the bottom 16-bits) the circuit is 50% larger but almost twice as fast as ripple-carry\nadder. With multiple, variably-sized blocks, tPD approaches sqrt(N) where N is the number of\nbits in the operand. Carry-select can be combined with carry-lookahead (see below) for even\ngreater performance improvements.\nAppendix 2: Carry-lookahead adders\nThe basic building block used in the ripple-carry adder is the full adder, the 3-input, 2-output\ncombinational logic circuit show below:\nA\nB\nCIN\nCOUT = A B + A CIN + B CIN\nS = A ⊕ B ⊕ CIN\nSince the COUT signals of each full adder are in the critical path, let's see if we can't improve the\nadder's performance by generating the COUT signals more quickly. First let's rewrite the equation\nfor COUT:\nCOUT = A B + A CIN + B CIN = A B + (A+B) CIN = G + P CIN\nwhere G = A B is true if a carry is generated by the full adder and P = A + B is true if the carry\nis propagated by the full adder from CIN to COUT. Actually the propagate signal is sometimes\ndefined as P = A ⊕ B which won't change the computation of COUT but will allow us to express\nS as a simple function of P and CIN: S = P ⊕ CIN. Note that P and G depend only on A and B\nand not on CIN.\nWe can generalize the notion of P and G to blocks of several bits. For example, consider a two-\nbit adder:\nA0\nB0\nS0\nS1\nA1\nB1\nCIN\nCOUT\nDefine the block generate signal G0,1 as G0,1 = G1 + G0 P1, i.e., the 2-bit block will generate a\ncarry if a carry is generated in bit 1 (G1) or if a carry is generated in bit 0 and propagated by bit\n1 (G0P1). Similarly we can define the block propagate signal P0,1 as P0,1 = P0 P1, i.e., CIN will be\npropagated to COUT only if both bits are propagating their carry-ins.\nBuilding on this idea, we can choose the maximum fan-in we want for our logic gates and then\nbuild a hierarchical carry chain using these equations:\n6.004 Computation Structures\n- 5 -\nDesign Project\n\nWe can use this building block to construct P/G signals for any width operands. The following\nfigure shows how this works for 8-bit operands using 2-input logic gates:\nWe can then use the P, G and carry-in signals at each level to generate carry-in signals for\nprevious levels of the hierarchy, as shown in the following diagram for an 8-bit adder:\n6.004 Computation Structures\n- 6 -\nDesign Project\nDiagram of hierarc\nhical building block.\nFigure by MIT OpenCourseWare.\nai\nag\nram o\nf 8-b\nit op\nerand\ns usi\nng 2-\ninput\nlogi\nc gat\nes.\nFigure by MIT OpenCourseWare.\n\nWe can combine P/G generation and carry generation into a single building block, leading the\nfollowing diagram for a complete 8-bit carry-lookahead adder:\nIn order to use only inverting logic in the building blocks, it's common to have two versions:\none that takes inverted inputs and produces non-inverted outputs, and another that takes non-\ninverted inputs and produces inverted outputs. These blocks can be used on alternative levels of\nthe carry lookahead tree.\n6.004 Computation Structures\n- 7 -\nDesign Project\nDi\nag\nram o\nf an\n8-\nbit a\ndder.\nFigure by MIT OpenCourseWare.\nDi\nag\nram=of an 8-bit c\narry-looka\nhead adder.\nFigure by MIT OpenCourseWare."
    },
    {
      "category": "Resource",
      "title": "JSim",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/0c8eb21793c1fb46f083a6c556aba8a9_MIT6_004s09_lab_tool_jsim.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.004 Computation Structures\nSpring 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nM A S S A C H U S E T T S I N S T I T U T E O F T E C H N O L O G Y\nDEPARTMENT OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE\n6.004 Computation Structures\nJSim\nContents:\n1. Introduction to JSim\n2. Running JSim\n3. Netlist format\n4. Device statements\n5. User-defined subcircuits\n6. Built-in subcircuits\n7. Control statements\n8. Running a simulation\n9. Waveform browsing\n1. Introduction to JSim\nJSim is a computer-aided design (CAD) tool that provides:\n-\nA simple editor for entering a circuit description (called a \"netlist\")\n-\nA choice of programs to simulate your circuit\n-\nA waveform browser to view the results of a simulation\nJSim uses mathematical models of circuit elements to make predictions of how a circuit will\nbehave both statically (DC analysis) and dynamically (transient analysis). The model for each\ncircuit element is parameterized, with each parameter providing information about the design or\nphysical properties of the device. You'll specify the design parameters in your netlist, e.g., for a\nmosfet you would specify its length and width. The parameters specifying physical properties\nhave been derived from measurements taken at the integrated circuit fabrication facility. We'll\nprovide those parameters as part of the mosfet model.\n6.004 Computation Structures\n- 1 -\nJSim\n\n2. Running JSim\nRunning on Athena\nWe recommend using the computers in the 6.004 lab since JSim has been tested and is\nknown to run with satisfactory performance in that environment. Another benefit of using the\n6.004 lab is that there's plenty of help around, both from your fellow students and the course\nstaff. After signing onto the Athena station, add the 6.004 locker to gain access to the design\ntools and model files (you only have to do this once each session):\nathena% add 6.004\nStart JSim running in a separate window by typing\nathena% jsim &\nIt can take a few moments for the Java runtime system to start up, please be patient! JSim takes\nas input a netlist that describes the circuit to be simulated. The initial JSim window is a very\nsimple editor that lets you enter and modify your netlist. You may find the editor unsatisfactory\nfor large tasks--it's based on the JTextArea widget of the Java Swing toolkit that in some\nimplementations has only rudimentary editing capabilities. If you use a separate editor to create\nyour netlists, you can have JSim load your netlist files when it starts:\nathena% jsim filename ... filename &\nRunning standalone\nFirst, you'll need to install a Java system. The Sun Java Runtime Environment, Standard Edition\n(J2SE), for Linux, Solairs and Windows can be downloaded from http://java.sun.com. J2SE for\nMac OS X is available from http://www.apple.com/java. On Linux, you'll want to change your\nPATH environment variable so that the \"java\" command is on your search path. On Windows and\nOS X, double-clicking any of the 6.004 jar files will run the program (assuming you've installed\nthe Sun Java environment).\nOnce you've installed a Java environment and downloaded the 6.004 files (see \"Quick Clicks\"\nabove), you can run JSim using the following command:\njava -jar jsim.jar -Xms8m -Xmx32m -reporterrors file...\nYou may have to specify complete pathnames for \"java\" and \"jsim.jar\" depending on your current\nsearch path and working directory. Each of the arguments is explained below.\n-\n-jar jsim.jar adds the java archive jsim.jar to the list of files Java examines when\ntrying to find classes. jsim.jar contains the classes used by jsim for displaying/editing\nnetlists, running simulations and browsing the results.\nIf you get an error of the form \"Exception in thread \"main\"\njava.lang.NoClassDefFoundError: jsim/JSim\", the Java runtime didn't find the\njsim.jar file -- try giving its full pathname, e.g., C:\\6.004\\jsim.jar or whatever is\nappropriate for your installation.\n6.004 Computation Structures\n- 2 -\nJSim\n\n-\n-Xms8m -Xmx32m sets the minimum heap size to 8MB and the maximum heap size to\n32MB. Starting JSim with a generous heap allocation avoids a lot of garbage collection\noverhead the first time your circuit is processed. If you run out of memory, try specifying\nthe -no-local-names option when running JSim. This will greatly reduce the size of the\nnode name hashtable JSim constructs when processing the netlist. The downside of using\nthis option is that nodes can only be referred to by using their name in the (sub)circuit\nwhere they were first defined.\n-\n-reporterrors asks JSim to provide a backtrace whenever it encounters an internal error.\nIn the unlikely event an error occurs, it would be very helpful if you can email this\nbacktrace to 6004-labs.\n-\nfile... are optional arguments specifying one or more JSim netlist files.\nEditing netlists\nThe netlist editor built into JSim is based on the JTextArea class in Swing. Many people find the\nediting facilities provided by this class to be underwhelming and prefer to use an external editor.\nJsim.el (available from the Courseware webpage) defines a new major mode for EMACS useful\nfor editing JSim netlists. You can invoke the mode automatically when reading in a \".jsim\" file by\nadding the following to your .emacs file:\n;;; jsim support, assumes jsim.el lives in your home directory\n(autoload 'jsim-mode \"~/jsim\" nil t)\n(setq auto-mode-alist (cons '(\"\\.jsim$\" . jsim-mode) auto-mode-alist))\n(add-hook 'jsim-mode-hook 'turn-on-font-lock)\nJSim toolbar\nThere are various handy buttons on the JSim toolbar:\nExit. Asks if you want to save any modified file buffers and then exits JSim.\nNew file. Create a new edit buffer called \"untitled\". Any attempts to save this buffer\nwill prompt the user for a filename.\nOpen file. Prompts the user for a filename and then opens that file in its own edit buffer.\nIf the file has already been read into a buffer, the buffer will be reloaded from the file\n(after asking permission if the buffer has been modified).\nClose file. Closes the current edit buffer after asking permission if the buffer has been\nmodified.\nReload file. Reload the current buffer from its source file after asking permission if the\nbuffer has been modified. This button is useful if you are using an external editor to\nmodify the netlist and simply want to reload a new version for simulation.\n6.004 Computation Structures\n- 3 -\nJSim\n\nSave file. If any changes have been made, write the current buffer back to its source file\n(prompting for a file name if this is an untitled buffer created with the \"new file\"\ncommand). If the save was successful, the old version of the file is saved with a \".bak\"\nextension.\nSave file, specifying new file name. Like \"Save file\" but prompts for a new file name to\nuse.\nSave all files. Like \"save file\" but applied to all edit buffers.\n6.004 Computation Structures\n- 4 -\nJSim\n\n3. Netlist format\nInput to JSim is processed line-by-line. Fields within a line are separated by whitespace (blanks\nor tabs), a comma, an \"=\" (equal sign), or a left or right parenthesis; extra spaces are ignored.\nBlank lines are ignored during processing.\nThe circuit to be simulated is described to JSim by a set of element statements, which define the\ncircuit topology and element values, and a set of control cards, which define the model\nparameters and the simulation controls. The order of the statements is arbitrary (except, of course,\nthat continuation lines must immediately follow the line being continued).\nContinuation lines\nA line may be continued by entering a \"+\" (plus) as the first character of the following line.\nJSim will continue reading continuation lines starting with the second character. There's no\nlimit to the number of continuation lines allowed. Example:\n.MODEL NENH NMOS LEVEL=3 PHI=0.700000 TOX=9.4000E-09\n+ XJ=0.200000U TPG=1 VTO=0.6746 DELTA=1.1480E+00 LD=3.4510E-08\n+ KP=1.8217E-04 UO=495.9 THETA=1.7960E-01 RSH=3.2470E+01\nComments\nLines whose first character is \"*\" (asterisk) are treated as comments and ignored during\nprocessing. Note that comments can have continuation lines (see above). Examples:\n* Both of the following lines are treated as a comment\n*.MODEL NENH NMOS LEVEL=3 PHI=0.700000 TOX=9.4000E-09\n+ KP=1.8217E-04 UO=495.9 THETA=1.7960E-01 RSH=3.2470E+01\nYou can also add comments at the end of a line by preceding the comment with the characters\n\"//\" (C++- or Java-style comments). All characters starting with \"//\" to the end of the line\nare ignored. Any portion of a line or lines can be turned into a comment by enclosing the text\nin \"/*\" and \"*/\" (C-style comments). Examples:\nR1 A B 3k\n// 3K-ohm resistor between nodes A and B\n/* use when testing at high temps! But not now...\n.temp 125\n*/\nNames\nEach device and node in the circuit has a unique name. A name can be either a simple integer\n(0 is reserved as the name of the ground node) or an alphanumeric string. The string is\nsequence of characters consisting of letters, digits, \"_\", \":\", \"$\", \"[\", \"]\", and \".\". Strings\ncannot begin with a digit and names begining with \"$\" are reserved for naming built-in\ndevices. \".\" (period) is reserved for use as a separator in hierarchical names. Examples:\nR1\nstdcell:and\nThis_is_a_very_long_name\ncpu.alu.adder.bit31.carry_in\nWhen naming a collection of nodes that serve as a bus it is convenient to use iterators to save\non having to enter many names sequentially. Iterators have the form\n6.004 Computation Structures\n- 5 -\nJSim\n\nprefix[start:end]suffix\nwhere start and end are integers specifying the first and last indicies of the sequence\nrespectively. JSim expands the iterator into |start-end| + 1 names substituting a different\nvalue for the bracketed expression in each iteration. Examples:\ndata[7:0]\n=> data7 data6 data5 data4 data3 data2 data1 data0\nxxx[1:4]yyy => xxx1yyy xxx2yyy xxx3yyy xxx4yyy\nOne can also specify an increment between successive indicies using the form:\nprefix[start:end:inc]suffix\nExample:\ndata[7:0:2] => data7 data5 data3 data1\nMore than one iterator can be used in a name; they are expanded from left to right:\na[3:0][1:0] => a3[1:0] a2[1:0] a1[1:0] a0[1:0]\n=> a31 a30 a21 a20 a11 a10 a01 a00\nNote that using an iterator is exactly equivalent to specifying the expanded sequence:\n* The following two element statements are equivalent\nX1 out in[7:0] $xor\nX1 out in7 in6 in5 in4 in3 in2 in1 in0 $xor\nFinally, there's a shortcut for specifying multiple copies of the same node:\nfoo#4\n=>\nfoo foo foo foo\nx[1:0]#2 =>\nx[1:0] x[1:0]\n=>\nx1 x0 x1 x0\nNumbers\nA number may be an integer (12, -44), a floating point number (3.14159), an integer or\nfloating point number followed by an integer exponent (1E-14, 2.65E3), or an integer or a\nfloating point number followed by one of the following scale factors:\nScale Factor\nPronounced\nMultiplier\nT, t\ntera\n1E12\nG, g\ngiga\n1E9\nMEG, meg\nmega\n1E6\nK, k\nkilo\n1E3\nM, m\nmilli\n1E-3\nU, u\nmicro\n1E-6\nMIL, mil\n25.4E-6\nN,n\nnano\n1E-9\nP, p\npico\n1E-12\nF, f\nfemto\n1E-15\nLetters immediately following a number that are not scale factors are ignored and letters\nimmediately following a scale factor are ignored. Integers can be entered in binary, octal or\nhexadecimal notation by using the appropriate prefix:\n6.004 Computation Structures\n- 6 -\nJSim\n\n0b1011101110100 6004 in binary (\"0b\" prefix)\n6004 in octal (\"0\" prefix)\n0x1774\n6004 in hex (\"0x\" prefix)\nExamples:\n* The following all represent the same numeric value\n1000 1000.0 1000Hz 1E3 1.0E3 1kHz 1K 0x3E8 01750 0b1111101000\n6.004 Computation Structures\n- 7 -\nJSim\n\n4. Device statements\nEach device in the circuit is specified by a device statement that specifies the device name, the\ncircuit nodes to which the device is connected, and the values of the parameters that determine\nthe electrical characteristics of the element. The first letter of the device name specifies the\nelement type, the remainder of the name can be any legal name (see above). For example, a\nresistor name must begin with the letter \"R\" or \"r\". Hence \"R1\", \"rse\", and \"R3ac_2xy\" are valid\nresistor names. Device names must be unique at the level of circuit in which they appear.\nIn the following description, data field enclosed in braces (\"{\" and \"}\") are optional. All indicated\npunctuation (parenthesis, equal signs, etc.) are required.\nWith respect to branch voltages and currents, JSim uniformly uses the associated reference\nconvention: current flows in the direction of voltage drop. The circuit cannot contain a loop of\nvoltage sources and/or inductors and cannot contain a cutset of current sources and/or capacitors.\nEach node in the circuit must have a dc path to ground.\nRid n+ n- value\n// resistor, units in ohms, value > 0\nCid n+ n- value\n// capacitor, units in farads, value >= 0\nLid n+ n- value\n// inductor, units in henries, value >= 0\nLinear devices. \"n+\" and \"n-\" are the two terminal nodes and \"value\" is the specified\nresistance, capacitance, or inductance. Examples:\nR1 1 2 1k\n// 1k ohm resistor between nodes 1 and 2\nRbias base 0 33ohms\n// 33 ohm resistor between nodes base and ground\nCMILLER gate drain 1fF\n// 1 femtofarad capacitor\nLshunt a b 10u\n// 10 microhenry inductor\nMid nd ng ns nb model L=number W=number {params}\n// mosfet, units in meters\nMid nd ng ns nb model SL=number SW=number {params}\n// mosfet, scaled units\nMosfets.\nThe other new twist introduced in the example netlist is the use of symbolic dimensions for\nthe mosfets (\"SW=\" and \"SL=\") instead of physical dimensions (\"W=\" and \"L=\"). Symbolic\ndimensions specify multiples of a parameter called SCALE, which is also defined in\nnominal.JSim:\n.option SCALE=0.6u\nXid nodes.. subckt {params}\n// instance of user-defined subcircuit\nSubcircuit instance.\nVid n+ n- {{DC=}dcvalue} {tran} // voltage source, units in volts\nIid n+ n- {{DC=}dcvalue} {tran} // current source, units in amperes\nIndependent sources. n+ and n- are the positive and negative nodes respectively. Note that\nvoltage sources need not be grounded. Positive current is assumed to flow from the positive\nnode, through the source, to the negative node. A current source of positive value will force\ncurrent to flow out of the n+ node, through the source, and into the n- node. Voltage sources,\nin addition to be used for circuit excitation, are the \"ammeters\" for JSim, that is, zero-value\nvoltage sources may be inserted into the circuit for the purpose of measuring current. They\n6.004 Computation Structures\n- 8 -\nJSim\n\nwill, of course, have no effect on circuit operation since they represent short-circuits.\nThe dcvalue is used during DC and OP analyses and the initialization phase of TRAN\nanalysis. If no dc specification is provided, a value of 0 is assumed.\nAny source can be assigned a time-dependent value for transient analysis. If a source is\nassigned a time-dependent value, the t = 0 value is used in place of any specified dc value.\nThere are six source functions: pulse, exponential, sinusoidal, piece-wise linear, amplitude\nmodulation and single-frequency frequency modulation. Each source function takes several\nparameters that determine the shape of the waveform; the form of the specification can be any\nof the following:\nfname param1 param2 ... paramN\nfname param1, param2, ... paramN\nfname(param1 param2 ... paramN)\nfname(param1, param2, ... paramN)\ni.e., the parenthesis and commas are optional. The following paragraphs provide a detailed\ndescription for each of the source functions.\nmore here...\nEid n+ n- ctl+ ctl- gain\n// voltage-controlled voltage source (VCVS)\nFid n+ n- ctl+ ctl- gain\n// current-controlled current source (CCCS)\nGid n+ n- ctl+ ctl- gain\n// voltage-controlled current source (VCCS)\nHid n+ n- ctl+ ctl- gain\n// current-controlled voltage source (CCVS)\nDependent sources.\nmore here...\nWid nodes... nrz(vlow,vhigh,tperiod,tdelay,trise,tfall) data...\nThe \"W\" voltage source that generates digital waveforms for many nodes (e.g., a bus) at\nonce. If N nodes are specified, think of them as an N-bit value where the node names are\nlisted most-significant bit first. The \"W\" source will set those nodes to a sequence of data\nvalues using the data specified at the end of the \"W\" statement. At each step of the sequence,\nthe N low-order bits of each data value will be used to generate the appropriate voltage for\neach of the N nodes. The voltage and timing of the signals is given by the nrz parameters:\nvlow\nvoltage used for a logic low value (usually 0)\nvhigh\nvoltage used for a logic high value (usually 3.3V)\ntperiod\ninterval (in seconds) at which values will be changed\ntdelay\ninitial delay (in seconds) before periodic value changes start\ntrise\nrise time (in seconds) for low-to-high transitions\ntfall\nfall time (in seconds) for high-to-low transitions\nNote that the times are specified in seconds, so don't forget to specify the \"n\" scale factor\nwhen entering times! Example:\n.include \"/mit/6.004/jsim/nominal.jsim\"\nWtest a[2:0] nrz(0v,3.3v,10ns,0ns,.1ns,.1ns) 7 6 0x5\n+ 0 1 0b010 3\n.tran 80ns\n\n.plot a2\n\n.plot a1\n\n6.004 Computation Structures\n- 9 -\nJSim\n\n.plot a0\n\n.plot a[2:0]\n\nNote that you can specify data values in decimal, hex (\"0x\" prefix), octal (\"0\" prefix) or\nbinary (\"0b\" prefix). The last data value is repeated if necessary. This example produces the\nfollowing plot when run using the gate level simulator\nJSim knows how to plot a set of nodes as a single multi-bit data value, as shown in the\nbottom channel of the plot above. If you zoomed in on one of the transition times, you would\nsee that the values actually turn to \"X\" for 0.1ns while making the transition between valid\nlogic levels.\n6.004 Computation Structures\n- 10 -\nJSim\n\n5. User-defined subcircuits\n.subckt name terminals...\n* internal circuit elements are listed here\n.ends\nThe \".subckt\" statement introduces a new level of netlist. All lines following the \".subckt\" up to\nthe matching \".ends\" statement will be treated as a self-contained subcircuit. This includes model\ndefinitions, nested subcircuit definitions, electrical nodes and circuit elements. The only parts of\nthe subcircuit visible to the outside world are its terminal nodes which are listed following the\nname of the subcircuit in the \".subckt\" statement:\nOnce the definitions are complete, you can create an instance of a subcircuit using the \"X\" circuit\nelement:\nXid nodes... name\nwhere name is the name of the circuit definition to be used, id is a unique name for this instance\nof the subcircuit and nodes... are the names of electrical nodes that will be hooked up to the\nterminals of the subcircuit instance. There should be the same number of nodes listed in the \"X\"\nstatement as there were terminals in the \".subckt\" statement that defined name. For example,\nhere's a short netlist that instantiates 3 NAND gates (called \"g0\", \"g1\" and \"g2\"):\n.global vdd\nVdd vdd 0 3.3V\n* 2-input NAND: inputs are A and B, output is Z\n.subckt nand2 a b z\nMPD1 z a 1 0 NENH sw=8 sl=1\nMPD2 1 b 0 0 NENH sw=8 sl=1\nMPU1 z a vdd vdd PENH sw=8 sl=1\nMPU2 z b vdd vdd PENH sw=8 sl=1\n.ends\nXg0 d0 ctl z0 nand2\nXg1 d1 ctl z1 nand2\nXg2 d2 ctl z2 nand2\nThe node \"ctl\" connects to all three gates; all the other terminals are connected to different nodes.\nNote that any nodes that are private to the subcircuit definition (i.e., nodes used in the subcircuit\nthat don't appear on the terminal list) will be unique for each instantiation of the subcircuit. For\nexample, there is a private node named \"1\" used inside the nand2 definition. When JSim\nprocesses the three \"X\" statements above, it will make three independent nodes called \"xg0.1\",\n\"xg1.1\" and \"xg2.1\", one for each of the three instances of nand2. There is no sharing of internal\nelements or nodes between multiple instances of the same subcircuit.\nIt is sometimes convenient to define nodes that are shared by the entire circuit, including\nsubcircuits; for example, power supply nodes. The ground node \"0\" is such a node; all references\nto \"0\" anywhere in the netlist refer to the same electrical node. The example netlist defines\nanother shared node called \"vdd\" which is used whenever a connection to the power supply is\nrequired.\n6.004 Computation Structures\n- 11 -\nJSim\n\nJSim makes it easy to specify multiple gates with a single \"X\" statement. You can create\nmultiple instances of a device by supplying some multiple of the number of nodes it\nexpects, e.g., if a device has 3 terminals, supplying 9 nodes will create 3 instances of the\ndevice. To understand how nodes are matched up with terminals specified in the .subckt\ndefinition, imagine a device with P terminals. The sequence of nodes supplied as part of\nthe \"X\" statement that instantiates the device are divided into P equal-size contiguous\nsubsequences. The first node of each subsequence is used to wire up the first device, the\nsecond node of each subsequence is used for the second device, and so on until all the\nnodes have been used. For example, since xor2 has 3 terminals:\nXtest a[2:0] b[2:0] z[2:0] xor2\nis equivalent to\nXtest#0 a2 b2 z2 xor2\nXtest#1 a1 b1 z1 xor2\nXtest#2 a0 b0 z0 xor2\nThere is also a handy way of duplicating a signal: specifying \"foo#3\" is equivalent to specifying\n\"foo foo foo\". For example, xor'ing a 4-bit bus with a control signal could be written as\nXbusctl in[3:0] ctl#4 out[3:0] xor2\nwhich is equivalent to\nXbusctl#0 in3 ctl out3 xor2\nXbusctl#1 in2 ctl out2 xor2\nXbusctl#2 in1 ctl out1 xor2\nXbusctl#3 in0 ctl out0 xor2\nUsing iterators and the \"constant0\" device from the standard cell library, here's a better way of\nconnecting cmp[31:1] to ground:\nXgnd cmp[31:1] constant0\nSince the \"constant0\" has one terminal and we supply 31 nodes, 31 copies of the device will be\nmade.\n6.004 Computation Structures\n- 12 -\nJSim\n\n6. Built-in subcircuits\n$memory\n$xxx\nWe'll be using a new component this week: a multi-port memory. JSim has a built-in memory\ndevice that can be used to model memories with a specified width and number of locations, and\nwith one or more ports. Each port has 3 control signals and the specified number of address and\ndata wires. You can instantiate a memory device in your circuit with a statement of the form\nXid ports... $memory width=w nlocations=nloc options...\nThe width and nlocations properties must be supplied: w specifies the width of each memory\nlocation in bits and must be between 1 and 32. nloc specifies the number of memory locations\nand must be between 1 and 220. All the ports of a memory access the same internal storage, but\neach port operates independently. Each port specification is a list of nodes:\noe clk wen a\n... a d\n... d\nnaddr-1\nw-1\nwhere\noe is the output enable input for a read port. When 1, data is driven onto the data pins;\nwhen 0, the output pins are not driven by this memory port. If this port is only a write port,\nconnect this terminal to the ground node \"0\". If the port is only a read port and should\nalways be enabled, connect this terminal to the power supply node \"vdd\".\nclk is the clock input for write ports. When wen=1, data from the data terminals is written\ninto the memory on the rising edge of clk. If this port is only a read port, connect this\nterminal to the ground node \"0\".\nwen is the write enable input for write ports. See the description of \"clk\" for details about\nthe write operation. If this port is only a read port, connect this terminal to the ground node\n\"0\".\na\n... a are the address inputs, listed most significant bit first. The values of these\nnaddr-1\nterminals are used to compute the address of the memory location to be read or written. The\nnumber of address terminals is determined from the number of locations in the memory:\nnaddr = ceiling(log2(nloc)). When the number of locations in a memory isn't exactly a\npower of 2, reads that refer to non-existent locations return \"X\" and writes to non-existent\nlocations have no effect.\nd\n... d are the data inputs/tristate outputs, listed most significant bit first.\nw-1\nBy specifying one of the following options it is possible to specify the initial contents of a\nmemory (if not specified, the memory is initialized to all X's):\nfile=\"filename\"\nThe memory is initialized, location-by-location, from bytes in the file. Data is assumed\nto be in a binary little-endian format, using ceiling(w/8) bytes of file data per memory\nlocation. Bits 0 through 7 of the first file byte are used to initialize bits 0 through 7 of\nmemory location 0, bits 0 through 7 of the second file byte are used to initialize bits 8\n6.004 Computation Structures\n- 13 -\nJSim\n\nthrough 15 of memory location 0, and so on. When all the bits in a memory location\nhave been filled, any bits remaining in the current file byte are discarded and then the\nprocess continues with the next memory location. In particular, the \".bin\" files produced\nby BSim can be used to initialize JSim memories. For example, the following statement\nwould create a 1024-location 32-bit memory with three ports: 2 read ports and 1 one\nwrite port. The memory is initialized from the BSim output file \"lab6.bin\".\nXmem\n+ vdd 0 0 ia[11:2] id[31:0]\n// (read) instruction data\n+ vdd 0 0 ma[11:2] mrd[31:0]\n// (read) program data (LDs)\n+ 0 clk wr ma[11:2] mwd[31:0] // (write) program data (STs)\n+ $memory width=32 nlocations=1024\n+ file=\"/mit/6.004/bsim/lab6.bin\"\ncontents=( data... )\nThe memory is initialized, location-by-location, from the data values given in the list.\nThe least significant bit (bit 0) of a value is used to initialize bit 0 of a memory location,\nbit 1 of a value is used to initialize bit 1 of a memory location, etc. For example, to enter\nthe short test program ADDC(R31,1,R0); ADDC(R31,2,R1); ADD(R0,R1,R2) one might\nspecify:\nXmem\n+ vdd 0 0 ia[11:2] id[31:0]\n// (read) instruction data\n+ vdd 0 0 ma[11:2] mrd[31:0]\n// (read) program data (LDs)\n+ 0 clk wr ma[11:2] mwd[31:0] // (write) program data (STs)\n+ $memory width=32 nlocations=1024\n+ contents=(0xC01F0001 0xC03F0002 0x80400800)\nInitialized memories are useful for modeling ROMs (e.g., for control logic) or simply for loading\nprograms into the main memory of your Beta. One caveat: if the memory has a write port and\nsees a rising clock edge with its write enable not equal to 0 and with one or more of the address\nbits undefined (i.e., with a value of \"X\"), the entire contents of the memory will also become\nundefined. So you should make sure that the write enable for a write port is set to 0 by your\nreset logic before the first clock edge, or else your initialization will be for naught.\nThe following options can be used to specify the electrical and timing parameters for the memory.\nFor this lab, these should not be specified and the default values used.\ntcd=seconds\nthe contamination delay in seconds. Default value = 20ps.\ntpd=seconds\nthe propagation delay in seconds. This is how long it takes for changes in the address or\noutput enable terminals to be reflected in the values driven by the data terminals. Default\nvalue is determined from the number of locations:\nNumber of locations\ntPD\nInferred\ntype\nnlocations ≤ 128\n2ns\nRegister file\n128 < nlocations ≤ 1024\n4ns\nStatic ram\nnlocations > 1024\n40ns\nDynamic ram\n6.004 Computation Structures\n- 14 -\nJSim\n\ntr=seconds_per_farad\nthe output rise time in seconds per farad of output load. Default value is 1000, i.e., 1\nns/pf.\ntf=seconds_per_farad\nthe output fall time in seconds per farad of output load. Default value is 500, i.e., 0.5\nns/pf.\ncin=farads\ninput terminal capacitance in farads. Default value = 0.05pf.\ncout=farads\noutput terminal capacitance in farads. Default value = 0pf (additional tPD due to output\nterminal loading is already included in default tPD).\nThe size of a memory is determined by the sum of the sizes of the various memory building\nblocks shown in the following table:\nComponent\nSize (μ2)\nNotes\nStorage cells\nnbits * cellsize\nnbits = nlocs * width\ncellsize = nports (for ROMs and DRAMS)\ncellsize = nports + 5 (for SRAMS)\nAddress buffers\nnports * naddr * 20\nnports = total number of memory ports\nAddress decoders\nnports * (naddr+3)/4 * 4\nAssuming 4-input ANDs\nTristate drivers\nnreads * width * 30\nnreads = number of read ports\nWrite-data drivers\nnwrites * width * 20\nnwrites = number of write ports\n6.004 Computation Structures\n- 15 -\nJSim\n\n7. Control statements\n.checkoff\ninfo...\n.connect node1 node2 node3...\nThe .connect statement is useful for connecting two terminals of a subcircuit or for\nconnecting nodes directly to ground. For example, the following statement ties nodes cmp1,\ncmp2, ..., cmp31 directly to the ground node (node \"0\"):\n.connect 0 cmp[31:1]\nNote that the .connect control statement in JSim works differently than many people expect.\nFor example,\n.connect A[5:0] B[5:0]\nwill connect all twelve nodes (A5, A4, ..., A0, B5, B4, ..., B0) together - usually not what\nwas intended. To connect two busses together, one could have entered\n.connect A5 B5\n.connect A4 B4\n...\nwhich is tedious to type. Or one can define a two-terminal device that uses .connect\ninternally, and then use the usual iteration rules (see next section) to make many instances of\nthe device with one \"X\" statement:\n.subckt knex a b\n.connect a b\n.ends\nX1 A[5:0] B[5:0] knex\n.dc\ninfo...\n.end\ninfo...\n.global\ninfo...\n.include\ninfo...\n.model\ninfo...\n.mverify\ninfo...\n.op\ninfo...\n.options\ninfo...\n6.004 Computation Structures\n- 16 -\nJSim\n\n.plot\ninfo...\n.plotdef\ninfo...\n.subckt\n... circuit description...\n.ends\ninfo...\n.temp\ninfo...\n.tempdir\ninfo...\n.verify\ninfo...\n6.004 Computation Structures\n- 17 -\nJSim\n\n8. Running a simulation\nStop simulation. Clicking this control will stop a running simulation and display\nwhatever waveform information is available.\nDevice-level simulation. Use a Spice-like circuit analysis algorithm to predict the\nbehavior of the circuit described by the current netlist. After checking the netlist for errors, JSim\nwill create a simulation network and then perform the requested analysis (i.e., the analysis you\nasked for with a \".dc\" or \".tran\" control statement). When the simulation is complete the\nwaveform window is brought to the front so that the user can browse any results plotted by any\n\".plot\" control statements.\nFast transient analysis. This simulation algorithm uses more approximate device models\nand solution techniques than the device-level simulator but should be much faster for large\ndesigns. For digital logic, the estimated logic delays are usually within 10% of the predictions of\ndevice-level simulation. This simulator only performs transient analysis.\nGate-level simulation. This simulation algorithm only knows about gates and logic\nvalues (instead of devices and voltages). We'll use this feature later in the term when trying to\nsimulate designs that contain too many mosfets to be simulated at the device level.\nSwitch to waveform window. In the waveform window this button switches to the editor\nwindow. Of course, you can accomplish the same thing by clicking on the border of the window\nyou want in front, but sometimes using this button is less work.\nUsing information supplied in the checkoff file, check for specified node values at given\ntimes. If all the checks are successful, submit the circuit to the on-line assignment system.\n6.004 Computation Structures\n- 18 -\nJSim\n\n9. Waveform browsing\nThe waveform window shows various waveforms in one or more \"channels.\" Initially one\nchannel is displayed for each \".plot\" control statement in your netlist. If more than one waveform\nis assigned to a channel, the plots are overlaid on top of each using a different drawing color for\neach waveform. If you want to add a waveform to a channel simply type in the appropriate\nsignal name to the list appearing to the left of the waveform display (the name of each signal\nshould be on a separate line). You can also add the name of the signal you would like displayed\nto the appropriate \".plot\" statement in your netlist and rerun the simulation.\nIf you simply name a node in your circuit, its voltage is plotted. You can also ask for the current\nthrough a voltage source by entering \"I(Vid)\".\nInterpreting analog signal levels as logic values can be tedious. JSim will do it for you\nautomatically if you ask to plot \"L(a)\" instead of just \"a\". The logic-high and logic-thresholds\nare determined by the \"vih\" and \"vil\" options:\n.options vih=2.6 vil=0.6\nInitial values are specified in \"/mit/6.004/jsim/nominal.jsim\", but you can respecify them in your\nown netlist. Voltages between vil and vih are displayed as a filled-in rectangle to indicate that the\nlogic value cannot be determined. For example:\nA\nL(A)\nvih\nvil\nYou can also ask for the values of a set of signals to be displayed as a bus, e.g.,\n\"L(a3,a2,a1,a0)\". The signals should be listed most-significant bit first. A bus waveform is\ndisplayed as a filled-rectangle if any of the component signals has an invalid logic level or as a\nhexadecimal value otherwise. In the following plot the four signals a3, a2, a1 and a0 are\ninterpreted as a 4-bit integer where the high-order bit (a3) is making a 1→0 transition. The filled-\nin rectangle represents the period of time during which a3 transitions from VIH to VIL.\nL(a3,a2,a1,a0)\n0xF\n0x7\nThe default display radix for a bus is hexadecimal, but you can ask for other radicies as well:\nL(...) hexadecimal display, unsigned number\nd(...)\ndecimal display, unsigned number\nsd(...) decimal display, signed two's-complement number\nb(...)\nbinary display, unsigned number\no(...)\noctal display, unsigned number\nx(...)\nhexadecimal display, unsigned number (same as \"L\")\n6.004 Computation Structures\n- 19 -\nJSim\n\nIt's also possible to define your own \"symbolic\" display using plotdef, for example:\n.plotdef numbers zero one two three four five six seven eight nine ten\n+ eleven twelve thirteen fourteen fifteen\nWhen you plot a bus using numbers(...), the value of the bus is used as an index into the table of\nstrings and the resulting string is displayed in the plot:\nL(a3,a2,a1,a0)\nnumbers(a3,a2,a1,a0)\n0xF\n0x7\n0x0\nfifteen\nseven\nzero\nMaking Measurements\nEach waveform plot has an oscilloscope-like grid in the background. The scale for each division\nis shown at the bottom left-hand corner of the plot. You can change the scale by using the zoom\nbuttons at the bottom of the browser window (or you can use the \"X\" or \"x\" keyboard shortcuts).\nThe scrollbar at the bottom of the browser window can be used to pan along the horizontal\ndimension of the plot. Or you can use the \"c\" keyboard shortcut to recenter the plot at the current\nmouse position.\nAs you move the mouse over a particular waveform plot, a crosshair cursor follows the first\nwaveform found above the current mouse position. The readout in the upper left-hand corner of\nthe plot gives the current coordinates of the crosshair cursor.\nTo measure an interval on the plot, position the crosshair cursor at one end of the interval, click\nand drag the mouse to the other end of the interval. A second crosshair cursor will appear as you\ndrag the mouse, and a second readout line gives the current coordinates of the second cursor as\nwell as the delta between the two cursors' coordinates.\nThe Waveform Toolbar\nThe waveform window has several other buttons on its toolbar:\nSelect the number of displayed channels; choices range between 1 and 16.\nPrint. Prints the contents of the waveform window (in color if you have a color printer!).\nIf you are using Athena, you have to print to a file and then send the file to the printer: select\n\"file\" in the print dialog, supply the name you'd like to use for the plot file, then click \"print\".\nYou can send the file to one of the printers in the lab using \"lpr\", e.g., \"lpr -Pcs foo.plot\".\nYou can zoom and pan over the traces in the waveform window using the control found along the\nbottom edge of the waveform display:\n6.004 Computation Structures\n- 20 -\nJSim\n\nzoom in. Increases the magnification of the waveform display. You can zoom in around\na particular point in a waveform by placing the cursor at the point on the trace where you want to\nzoom in and typing upper-case \"X\".\nzoom out. Decreases the magnification of the waveform display. You can zoom out\naround a particular point in a waveform by placing the cursor at the point on the trace where you\nwant to zoom out and typing lower-case \"x\".\nsurround. Sets the magnification so that the entire waveform will be visible in the\nwaveform window.\nThe scrollbar at the bottom of the waveform window can be used to scroll through the\nwaveforms. The scrollbar will be disabled if the entire waveform is visible in the window. You\ncan recenter the waveform display about a particular point by placing the cursor at the point\nwhich you want to be at the center of the display and typing \"c\".\n6.004 Computation Structures\n- 21 -\nJSim"
    },
    {
      "category": "Resource",
      "title": "Summary of β Instruction Formats",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/477a612b77d3d3e439de0d4b7a6ee64c_MIT6_004s09_lab_beta_summary.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.004 Computation Structures\nSpring 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nSummary of β Instruction Formats\nOperate Class:\n10xxxx\nRc\nRa\nRb\nunused\nRegister Symbol\nUsage\nR31\nR31\nAlways zero\nR30\nXP\nException pointer\nR29\nSP\nStack pointer\nR28\nLP\nLinkage pointer\nR27\nBP\nBase of frame pointer\nOP(Ra,Rb,Rc):\nReg[Rc] ← Reg[Ra] op Reg[Rb]\nOpcodes: ADD (plus), SUB (minus), MUL (multiply), DIV (divided by)\nAND (bitwise and), OR (bitwise or), XOR (bitwise exclusive or)\nCMPEQ (equal), CMPLT (less than), CMPLE (less than or equal) [result = 1 if true, 0 if false]\nSHL (left shift), SHR (right shift w/o sign extension), SRA (right shift w/ sign extension)\n11xxxx\nRc\nRa\nliteral (two's complement)\nOPC(Ra,literal,Rc):\nReg[Rc] ← Reg[Ra] op SEXT(literal)\nOpcodes: ADDC (plus), SUBC (minus), MULC (multiply), DIVC (divided by)\nANDC (bitwise and), ORC (bitwise or), XORC (bitwise exclusive or)\nCMPEQC (equal), CMPLTC (less than), CMPLEC (less than or equal) [result = 1 if true, 0 if false]\nSHLC (left shift), SHRC (right shift w/o sign extension), SRAC (right shift w/ sign extension)\nOther:\n01xxxx\nRc\nRa\nliteral (two's complement)\nLD(Ra,literal,Rc):\nReg[Rc] ← Mem[Reg[Ra] + SEXT(literal)]\nST(Rc,literal,Ra):\nMem[Reg[Ra] + SEXT(literal)] ← Reg[Rc]\nJMP(Ra,Rc):\nReg[Rc] ← PC + 4; PC ← Reg[Ra]\nBEQ/BF(Ra,label,Rc):\nReg[Rc] ← PC + 4; if Reg[Ra] = 0 then PC ← PC + 4 + 4*SEXT(literal)\nBNE/BT(Ra,label,Rc):\nReg[Rc] ← PC + 4; if Reg[Ra] = 0 then PC ← PC + 4 + 4*SEXT(literal)\nLDR(label,Rc):\nReg[Rc] ← Mem[PC + 4 + 4*SEXT(literal)]\nOpcode Table: (*optional opcodes)\n2:0\n5:3\nLD\nST\nJMP\nBEQ\nBNE\nLDR\nADD\nSUB\nMUL*\nDIV*\nCMPEQ\nCMPLT\nCMPLE\nAND\nOR\nXOR\nSHL\nSHR\nSRA\nADDC\nSUBC\nMULC*\nDIVC*\nCMPEQC\nCMPLTC\nCMPLEC\nANDC\nORC\nXORC\nSHLC\nSHRC\nSRAC"
    },
    {
      "category": "Resource",
      "title": "Unpipelined Beta",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/33be804ab1b9f3694a79bb0a88e4c9ed_MIT6_004s09_lab_beta_diagram.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n6.004 Computation Structures\nSpring 2009\n\nControl logic:"
    },
    {
      "category": "Resource",
      "title": "β Documentation",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/c1b18db634e4052b737b87e2021e4b12_MIT6_004s09_lab_beta_doc.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.004 Computation Structures\nSpring 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nM A S S A C H U S E T T S I N S T I T U T E O F T E C H N O L O G Y\nDEPARTMENT OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE\n6.004 Computation Structures\nβ Documentation\n1. Introduction\nThis handout is a reference guide for the β, the RISC processor design for 6.004. This is\nintended to be a complete and thorough specification of the programmer-visible state and\ninstruction set.\n2. Machine Model\nThe β is a general-purpose 32-bit architecture: all registers are 32 bits wide and when loaded\nwith an address can point to any location in the byte-addressed memory. When read, register\n31 is always 0; when written, the new value is discarded.\nProgram Counter\nMain Memory\n232 bytes\n...\nPC\nalways a multiple of 4\nR0\nR1\nR30\nR31\nalways 0\n32 bits\nRegisters\nSUB(R3,R4,R5)\nST(R5,1000)\n32 bits\n32 bits\n0x00000000:\n0x00000004:\n0xFFFFFFF8:\n0xFFFFFFFC:\n...\n...\n3. Instruction Encoding\nEach β instruction is 32 bits long. All integer manipulation is between registers, with up to\ntwo source operands (one may be a sign-extended 16-bit literal), and one destination register.\nMemory is referenced through load and store instructions that perform no other computation.\nConditional branch instructions are separated from comparison instructions: branch\ninstructions test the value of a register that can be the result of a previous compare\ninstruction.\n6.004 Computation Structures\n- 1 -\nβ Documentation\n\nThere are only two types of instruction encoding: Without Literal and With Literal.\nInstructions without literals include arithmetic and logical operations between two registers\nwhose result is placed in a third register. Instructions with literals include all other\noperations.\nLike all signed quantities on the β, an instruction's literal is represented in two's complement.\n3.1 Without Literal\nOpcode\nRc\nRa\nRb\nunused\n3.2 With Literal\nOpcode\nRc\nRa\nliteral (two's complement)\n4. Instruction Summary\nBelow are listed the 32 β instructions and their 6-bit opcodes. For detailed instruction\noperations, see the following section.\nMnemonic\nOpcode\nADD\n0x20\nADDC\n0x30\nAND\n0x28\nANDC\n0x38\nBEQ\n0x1D\nBNE\n0x1E\nCMPEQ\n0x24\nCMPEQC\n0x34\nMnemonic\nOpcode\nCMPLE\n0x26\nCMPLEC\n0x36\nCMPLT\n0x25\nCMPLTC\n0x35\nDIV\n0x23\nDIVC\n0x33\nJMP\n0x1B\nLD\n0x18\nMnemonic\nOpcode\nLDR\n0x1F\nMUL\n0x22\nMULC\n0x32\nOR\n0x29\nORC\n0x39\nSHL\n0x2C\nSHLC\n0x3C\nSHR\n0x2D\nMnemonic\nOpcode\nSHRC\n0x3D\nSRA\n0x2E\nSRAC\n0x3E\nSUB\n0x21\nSUBC\n0x31\nST\n0x19\nXOR\n0x2A\nXORC\n0x3A\n5. Instruction Specifications\nThis section contains the specifications for the β instructions, listed alphabetically by\nmnemonic. No timing-dependent information is given: it is specifically assumed that there\nare no pathological timing interactions between instructions in this specification. Each\ninstruction is considered atomic and is presumed to complete before the next instruction is\nexecuted. No assumptions are made about branch prediction, instruction prefetch, or\nmemory caching.\n6.004 Computation Structures\n- 2 -\nβ Documentation\n\nOpcode:\nRc\nRa\nRb\nunused\n5.1 ADD\nUsage:\nADD(Ra,Rb,Rc)\nOperation:\nPC ← PC + 4\nReg[Rc] ← Reg[Ra] + Reg[Rb]\nThe contents of register Ra are added to the contents of register Rb and the 32-bit sum is\nwritten to Rc. This instruction computes no carry or overflow information. If desired, this\ncan be computed through explicit compare instructions.\n5.2 ADDC\nUsage:\nADDC(Ra,literal,Rc)\nOpcode:\nRc\nRa\nliteral\nOperation:\nPC ← PC + 4\nReg[Rc] ← Reg[Ra] + SEXT(literal)\nThe contents of register Ra are added to literal and the 32-bit sum is written to Rc. This\ninstruction computes no carry or overflow information. If desired, this can be computed\nthrough explicit compare instructions.\n5.3 AND\nUsage:\nAND(Ra,Rb,Rc)\nOpcode:\nRc\nRa\nRb\nunused\nOperation:\nPC ← PC + 4\nReg[Rc] ← Reg[Ra] & Reg[Rb]\nThis performs the bitwise boolean AND function between the contents of register Ra and the\ncontents of register Rb. The result is written to register Rc.\n5.4 ANDC\nUsage:\nANDC(Ra,literal,Rc)\nOpcode:\nRc\nRa\nliteral\nOperation:\nPC ← PC + 4\nReg[Rc] ← Reg[Ra] & SEXT(literal)\nThis performs the bitwise boolean AND function between the contents of register Ra and\nliteral. The result is written to register Rc.\n6.004 Computation Structures\n- 3 -\nβ Documentation\n\n5.5 BEQ/BF\nUsage:\nBEQ(Ra,label,Rc)\nBF(Ra,label,Rc)\nOpcode:\nRc\nRa\nliteral\nOperation:\nliteral = ((OFFSET(label) - OFFSET(current instruction)) / 4)-1\nPC ← PC + 4\nEA ← PC + 4*SEXT(literal)\nTEMP ← Reg[Ra]\nReg[Rc] ← PC\nif TEMP = 0 then PC ← EA\nThe PC of the instruction following the BEQ instruction (the updated PC) is written to\nregister Rc. If the contents of register Ra are zero, the PC is loaded with the target address;\notherwise, execution continues with the next sequential instruction.\nThe displacement literal is treated as a signed word offset. This means it is multiplied by 4\nto convert it to a byte offset, sign extended to 32 bits, and added to the updated PC to form\nthe target address.\n5.6 BNE/BT\nUsage:\nBNE(Ra,label,Rc)\nBT(Ra,label,Rc)\nOpcode:\nRc\nRa\nliteral\nOperation:\nliteral = ((OFFSET(label) - OFFSET(current instruction)) ÷ 4)-1\nPC ← PC + 4\nEA ← PC + 4*SEXT(literal)\nTEMP ← Reg[Ra]\nReg[Rc] ← PC\nif TEMP = 0 then PC ← EA\nThe PC of the instruction following the BNE instruction (the updated PC) is written to\nregister Rc. If the contents of register Ra are non-zero, the PC is loaded with the target\naddress; otherwise, execution continues with the next sequential instruction.\nThe displacement literal is treated as a signed word offset. This means it is multiplied by 4\nto convert it to a byte offset, sign extended to 32 bits, and added to the updated PC to form\nthe target address.\n6.004 Computation Structures\n- 4 -\nβ Documentation\n\n5.7 CMPEQ\nUsage:\nCMPEQ(Ra,Rb,Rc)\nOpcode:\nRc\nRa\nRb\nunused\nOperation:\nPC ← PC + 4\nif Reg[Ra] = Reg[Rb] then Reg[Rc] ← 1 else Reg[Rc] ← 0\nIf the contents of register Ra are equal to the contents of register Rb, the value one is written\nto register Rc; otherwise zero is written to Rc.\n5.8 CMPEQC\nUsage:\nCMPEQC(Ra,literal,Rc)\nOpcode:\nRc\nRa\nliteral\nOperation:\nPC ← PC + 4\nif Reg[Ra] = SEXT(literal) then Reg[Rc] ← 1 else Reg[Rc] ← 0\nIf the contents of register Ra are equal to literal, the value one is written to register Rc;\notherwise zero is written to Rc.\n5.9 CMPLE\nUsage:\nCMPLE(Ra,Rb,Rc)\nOperation:\nPC ← PC + 4\nif Reg[Ra] ≤ Reg[Rb] then Reg[Rc] ← 1 else Reg[Rc] ← 0\nIf the contents of register Ra are less than or equal to the contents of register Rb, the value\none is written to register Rc; otherwise zero is written to Rc.\n5.10 CMPLEC\nOpcode:\nRc\nRa\nRb\nunused\nUsage:\nCMPLEC(Ra,literal,Rc)\nOpcode:\nRc\nRa\nliteral\nOperation:\nPC ← PC + 4\nif Reg[Ra] ≤ SEXT(literal) then Reg[Rc] ← 1 else Reg[Rc] ← 0\nIf the contents of register Ra are less than or eqaul to literal, the value one is written to\nregister Rc; otherwise zero is written to Rc.\n6.004 Computation Structures\n- 5 -\nβ Documentation\n\nOpcode:\nRc\nRa\nRb\nunused\n5.11 CMPLT\nUsage:\nCMPLT(Ra,Rb,Rc)\nOperation:\nPC ← PC + 4\nif Reg[Ra] < Reg[Rb] then Reg[Rc] ← 1 else Reg[Rc] ← 0\nIf the contents of register Ra are less than the contents of register Rb, the value one is written\nto register Rc; otherwise zero is written to Rc.\n5.12 CMPLTC\nUsage:\nCMPLTC(Ra,literal,Rc)\nOpcode:\nRc\nRa\nliteral\nOperation:\nPC ← PC + 4\nif Reg[Ra] < SEXT(literal) then Reg[Rc] ← 1 else Reg[Rc] ← 0\nIf the contents of register Ra are less than literal, the value one is written to register Rc;\notherwise zero is written to Rc.\n5.13 DIV\nUsage:\nDIV(Ra,Rb,Rc)\nOperation:\nPC ← PC + 4\nReg[Rc] ← Reg[Ra] / Reg[Rb]\nThe contents of register Ra are divided by the contents of register Rb and the low-order 32\nbits of the quotient are written to Rc.\n5.14 DIVC\nOpcode:\nRc\nRa\nRb\nunused\nUsage:\nDIVC(Ra,literal,Rc)\nOpcode:\nRc\nRa\nliteral\nOperation:\nPC ← PC + 4\nReg[Rc] ← Reg[Ra] / SEXT(literal)\nThe contents of register Ra are divided by literal and the low-order 32 bits of the quotient is\nwritten to Rc.\n6.004 Computation Structures\n- 6 -\nβ Documentation\n\nOpcode:\nRc\nRa\n5.15 JMP\nUsage:\nJMP(Ra,Rc)\nOperation:\nPC ← PC+4\nEA ← Reg[Ra] & 0xFFFFFFFC\nReg[Rc] ← PC\nPC ← EA\nThe PC of the instruction following the JMP instruction (the updated PC) is written to\nregister Rc, then the PC is loaded with the contents of register Ra. The low two bits of Ra\nare masked to ensure that the target address is aligned on a 4-byte boundary. Ra and Rc may\nspecify the same register; the target calculation using the old value is done before the\nassignment of the new value. The unused literal field should be filled with zeroes. Note that\nJMP can clear the supervisor bit (bit 31 of the PC) but not set it - see section 6.3 for details.\n5.16 LD\nUsage:\nLD(Ra,literal,Rc)\nOpcode:\nRc\nRa\nliteral\nOperation:\nPC ← PC+4\nEA ← Reg[Ra] + SEXT(literal)\nReg[Rc] ← Mem[EA]\nThe effective address EA is computed by adding the contents of register Ra to the sign-\nextended 16-bit displacement literal. The location in memory specified by EA is read into\nregister Rc.\n5.17 LDR\nUsage:\nLDR(label,Rc)\nOpcode:\nRc\nliteral\nOperation:\nliteral = ((OFFSET(label) - OFFSET(current instruction)) / 4)-1\nPC ← PC + 4\nEA ← PC + 4*SEXT(literal)\nReg[Rc] ← Mem[EA]\nThe effective address EA is computed by multiplying the sign-extended literal by 4 (to\nconvert it to a byte offset) and adding it to the updated PC. The location in memory specified\nby EA is read into register Rc. The Ra field is ignored and should be 11111 (R31). The\nsupervisor bit (bit 31 of the PC) is ignored (i.e., treated as zero) when computing EA.\n6.004 Computation Structures\n- 7 -\nβ Documentation\n\nOpcode:\nRc\nRa\nRb\nunused\n5.18 MUL\nUsage:\nMUL(Ra,Rb,Rc)\nOperation:\nPC ← PC + 4\nReg[Rc] ← Reg[Ra] * Reg[Rb]\nThe contents of register Ra are multiplied by the contents of register Rb and the low-order 32\nbits of the product are written to Rc.\n5.19 MULC\nUsage:\nMULC(Ra,literal,Rc)\nOpcode:\nRc\nRa\nliteral\nOperation:\nPC ← PC + 4\nReg[Rc] ← Reg[Ra] * SEXT(literal)\nThe contents of register Ra are multiplied by literal and the low-order 32 bits of the product\nare written to Rc.\n5.20 OR\nUsage:\nOR(Ra,Rb,Rc)\nOperation:\nPC ← PC + 4\nReg[Rc] ← Reg[Ra] | Reg[Rb]\nThis performs the bitwise boolean OR function between the contents of register Ra and the\ncontents of register Rb. The result is written to register Rc.\n5.21 ORC\nOpcode:\nRc\nRa\nRb\nunused\nUsage:\nORC(Ra,literal,Rc)\nOpcode:\nRc\nRa\nliteral\nOperation:\nPC ← PC + 4\nReg[Rc] ← Reg[Ra] | SEXT(literal)\nThis performs the bitwise boolean OR function between the contents of register Ra and\nliteral. The result is written to register Rc.\n6.004 Computation Structures\n- 8 -\nβ Documentation\n\nOpcode:\nRc\nRa\nRb\nunused\n5.22 SHL\nUsage:\nSHL(Ra,Rb,Rc)\nOperation:\nPC ← PC + 4\nReg[Rc] ← Reg[Ra] << Reg[Rb]4:0\nThe contents of register Ra are shifted left 0 to 31 bits as specified by the five-bit count in\nregister Rb. The result is written to register Rc. Zeroes are propagated into the vacated bit\npositions.\n5.23 SHLC\nUsage:\nSHLC(Ra,literal,Rc)\nOpcode:\nRc\nRa\nliteral\nOperation:\nPC ← PC + 4\nReg[Rc] ← Reg[Ra] << literal4:0\nThe contents of register Ra are shifted left 0 to 31 bits as specified by the five-bit count in\nliteral. The result is written to register Rc. Zeroes are propagated into the vacated bit\npositions.\n5.24 SHR\nUsage:\nSHR(Ra,Rb,Rc)\nOpcode:\nRc\nRa\nRb\nunused\nOperation:\nPC ← PC + 4\nReg[Rc] ← Reg[Ra] >> Reg[Rb]4:0\nThe contents of register Ra are shifted right 0 to 31 bits as specified by the five-bit count in\nregister Rb. The result is written to register Rc. Zeroes are propagated into the vacated bit\npositions.\n5.25 SHRC\nUsage:\nSHRC(Ra,literal,Rc)\nOpcode:\nRc\nRa\nliteral\nOperation:\nPC ← PC + 4\nReg[Rc] ← Reg[Ra] >> literal4:0\nThe contents of register Ra are shifted right 0 to 31 bits as specified by the five-bit count in\nliteral. The result result is written to register Rc. Zeroes are propagated into the vacated bit\npositions.\n6.004 Computation Structures\n- 9 -\nβ Documentation\n\nOpcode:\nRc\nRa\nRb\nunused\n5.26 SRA\nUsage:\nSRA(Ra,Rb,Rc)\nOperation:\nPC ← PC + 4\nReg[Rc] ← Reg[Ra] >> Reg[Rb]4:0\nThe contents of register Ra are shifted arithmetically right 0 to 31 bits as specified by the\nfive-bit count in register Rb. The result is written to register Rc. The sign bit (Reg[Ra]31) is\npropagated into the vacated bit positions.\n5.25 SRAC\nUsage:\nSRAC(Ra,literal,Rc)\nOpcode:\nRc\nRa\nliteral\nOperation:\nPC ← PC + 4\nReg[Rc] ← Reg[Ra] >> literal4:0\nThe contents of register Ra are shifted arithmetically right 0 to 31 bits as specified by the\nfive-bit count in literal. The result is written to register Rc. The sign bit (Reg[Ra]31) is\npropagated into the vacated bit positions.\n5.28 ST\nUsage:\nST(Rc,literal,Ra)\nOpcode:\nRc\nRa\nliteral\nOperation:\nPC ← PC+4\nEA ← Reg[Ra] + SEXT(literal)\nMem[EA] ← Reg[Rc]\nThe effective address EA is computed by adding the contents of register Ra to the sign-\nextended 16-bit displacement literal. The contents of register Rc are then written to the\nlocation in memory specified by EA.\n5.29 SUB\nUsage:\nSUB(Ra,Rb,Rc)\nOpcode:\nRc\nRa\nRb\nunused\nOperation:\nPC ← PC + 4\nReg[Rc] ← Reg[Ra] - Reg[Rb]\nThe contents of register Rb are subtracted from the contents of register Ra and the 32-bit\ndifference is written to Rc. This instruction computes no borrow or overflow information. If\ndesired, this can be computed through explicit compare instructions.\n6.004 Computation Structures\n- 10 -\nβ Documentation\n\n5.30 SUBC\nUsage:\nSUBC(Ra,literal,Rc)\nOpcode:\nRc\nRa\nliteral\nOperation:\nPC ← PC + 4\nReg[Rc] ← Reg[Ra] - SEXT(literal)\nThe constant literal is subtracted from the contents of register Ra and the 32-bit difference is\nwritten to Rc. This instruction computes no borrow or overflow information. If desired, this\ncan be computed through explicit compare instructions.\n5.31 XOR\nUsage:\nXOR(Ra,Rb,Rc)\nOpcode:\nRc\nRa\nRb\nunused\nOperation:\nPC ← PC + 4\nReg[Rc] ← Reg[Ra] ^ Reg[Rb]\nThis performs the bitwise boolean XOR function between the contents of register Ra and the\ncontents of register Rb. The result is written to register Rc.\n5.32 XORC\nUsage:\nXORC(Ra,literal,Rc)\nOpcode:\nRc\nRa\nliteral\nOperation:\nPC ← PC + 4\nReg[Rc] ← Reg[Ra] ^ SEXT(literal)\nThis performs the bitwise boolean XOR function between the contents of register Ra and\nliteral. The result is written to register Rc.\n6. Extensions for Exception Handling\nThe standard β architecture described above is modified as follows to support exceptions and\nprivileged instructions.\n6.1 Exceptions\nβ exceptions come in three flavors: traps, faults, and interrupts.\nTraps and faults are both the direct outcome of an instruction (e.g., an attempt to execute an\nillegal opcode) and are distinguished by the programmer's intentions. Traps are intentional\nand are normally used to request service from the operating system. Faults are unintentional\nand often signify error conditions.\n6.004 Computation Structures\n- 11 -\nβ Documentation\n\nInterrupts are asynchronous with respect to the instruction stream, and are usually caused by\nexternal events (e.g., a character appearing on an input device).\n6.2 The XP Register\nRegister 30 is dedicated as the ``Exception Pointer'' (XP) register. When an exception\noccurs, the updated PC is written to the XP. For traps and faults, this will be the PC of the\ninstruction following the one which caused the fault; for interrupts, this will be the PC of the\ninstruction following the one which was about to be executed when the interrupt occurred.\nThe instruction pointed to by XP-4 has not been executed.\nSince the XP can be overwritten at unpredictable times as the result of an interrupt, it should\nnot be used by user-mode programs while interrupts are enabled.\n6.3 Supervisor Mode\nThe high bit of the PC is dedicated as the \"Supervisor\" bit. The instruction fetch and LDR\ninstruction ignore this bit, treating it as if it were zero. The JMP instruction is allowed to\nclear the Supervisor bit but not set it, and no other instructions may have any effect on it.\nOnly exceptions cause the Supervisor bit to become set.\nWhen the Supervisor bit is clear, the processor is said to be in \"user mode\". Interrupts are\nenabled while in user mode.\nWhen the Supervisor bit is set, the processor is said to be in \"supervisor mode\". While in\nsupervisor mode, interrupts are disabled and privileged instructions (see below) may be used.\nTraps and faults while in supervisor mode have implementation-defined (probably fatal)\neffects.\nSince the JMP instruction can clear the Supervisor bit, it is possible to load the PC with a\nnew value and enter user mode in a single atomic action. This provides a safe mechanism for\nreturning from a trap to the Operating System, even if an interrupt is pending at the time.\n6.4 Exception Handling\nWhen an exception occurs and the processor is in user mode, the updated PC is written to the\nXP, the Supervisor bit is set, the PC is loaded with an implementation-defined value, and the\nprocessor begins executing instructions from that point. This value is called\nthe \"exception vector\", and may depend on the kind of exception which occurred.\nThe only exception which must be supported by all implementations is the \"reset\" exception\n(also called the \"power up\" exception), which occurs immediately before any instructions are\nexecuted by the processor. The exception vector for power up is always 0. Thus, at power\nup time, the Supervisor bit is set, the XP is undefined, and execution begins at location 0 of\nmemory.\n6.004 Computation Structures\n- 12 -\nβ Documentation\n\n6.5 Privileged Instructions\nSome instructions may be available while in supervisor mode which are not available in user\nmode (e.g., instructions which interface directly with I/O devices). These are called\n\"privileged instructions\". These instructions always have an opcode of 0x00; otherwise, their\nform and semantics are implementation-defined. Attempts to use privileged instructions\nwhile in user mode will result in an illegal instruction exception.\n7. Software Conventions\nThis section describes our software programming conventions that supplement the basic architecture.\n7.1 Reserved Registers\nIt is convenient to reserve a number of registers for pervasive standard uses. The hardware\nitself reserves R31 and R30; in addition, our software conventions reserve R29, R28, and\nR27.\nThese are summarized in the following table and are described more fully below.\nRegister Symbol\nUsage\nR31\nR31\nAlways zero\nR30\nXP\nException pointer\nR29\nSP\nStack pointer\nR28\nLP\nLinkage pointer\nR27\nBP\nBase of frame pointer\n7.2 Convenience Macros\nWe augment the basic β instruction set with the following macros, making it easier to express\ncertain common operations:\nMacro\nDefinition\nBEQ(Ra, label)\nBEQ(Ra, label, R31)\nBF(Ra, label)\nBF(Ra, label, R31)\nBNE(Ra, label)\nBNE(Ra, label, R31)\nBT(Ra, label)\nBT(Ra, label, R31)\nBR(label, Rc)\nBEQ(R31, label, Rc)\nBR(label)\nBR(label, R31)\nJMP(Ra)\nJMP(Ra, R31)\nLD(label, Rc)\nLD(R31, label, Rc)\nST(Rc, label)\nST(Rc, label, R31)\nMOVE(Ra, Rc)\nADD(Ra, R31, Rc)\nCMOVE(c, Rc)\nADDC(R31, c, Rc)\nPUSH(Ra)\nADDC(SP, 4, SP)\nST(Ra, -4, SP)\n6.004 Computation Structures\n- 13 -\nβ Documentation\n\nPOP(Rc)\nLD(SP, -4, Rc)\nSUBC(SP, 4, SP)\nALLOCATE(k)\nADDC(SP, 4*k, SP)\nDEALLOCATE(k)\nSUBC(SP, 4*k, SP)\n7.3 Stack Implementation\nSP is a reserved register that points to the top of the stack. The stack is an arbitrary con\ntiguous region of memory. The contents of SP are always a multiple of 4 and each stack slot\nis 4 bytes. SP points to the location just beyond the topmost element on the stack. The stack\ngrows upward in memory (i.e., towards higher addresses). Four macros are defined for\nmanipulating the stack:\nPUSH(Ra) - Push the contents of register Ra onto the stack\nPOP(Rc) - Pop the top element of the stack into Rc\nALLOCATE(k) - Push k words of uninitialized data onto the stack\nDEALLOCATE(k) - Pop k words off of the stack and throw them away\n7.4 Procedure Linkage\nA procedure's arguments are passed on the stack. Specifically, when a procedure is entered,\nthe topmost element on the stack is the first argument to the procedure; the next element on\nthe stack is the second argument to the procedure, and so on. A procedure's return address is\npassed in LP, which is a register reserved for this purpose. A procedure returns its value (if\nany) in R0 and must leave all other registers, including the reserved registers, unaltered.\nThus, a typical call to a procedure named F looks like:\n\n(push argn-1)\n...\n\n(push arg1)\n\n(push arg0)\nBR (F, LP)\n\nDEALLOCATE (n)\n(use R0, which is now F(arg0, arg1, ... , argn-1))\n6.004 Computation Structures\n- 14 -\nβ Documentation\n\n7.5 Stack Frames\nThe preceeding section describes the rules which procedures must follow to interoperate\nproperly. This section describes our conventional means of writing a procedure which fol\nlows those rules.\nA procedure invocation requires storage for its arguments, its local variables, and any reg\nisters it needs to save and restore. All of this storage is allocated in a contiguous region of the\nstack called a \"stack frame\". Procedures \"activate\" stack frames on entry and \"deactivate\"\nthem on exit. BP is a reserved register which points to a fixed location within the currently\nactive stack frame. Procedures use a standard prologue and epilogue to activate and\ndeactivate the stack frame.\nThe standard prologue is:\n\nPUSH (LP)\n\nPUSH (BP)\n\nMOVE (SP, BP)\nALLOCATE (k)\n| allocate space for locals\n(push registers which are used by procedure)\nNote that either of the last two steps may be omitted if there are no local variables or if there\nare no registers to save.\nThe standard epilogue is:\n(pop registers which are used by procedure)\nMOVE (BP, SP)\n| deallocate space for locals\n\nPOP (BP)\n\nPOP (LP)\n\nJMP (LP)\n\nNote that the epilogue assumes that the body of the procedure has no net effect on SP. Also\nnote that either or both of the first two steps may be omitted if there are no registers to restore\nor if there are no local variables.\nThe standard prologue and epilogue together with the argument passing conventions imply\nthe following layout for a stack frame:\nBP⇒\nargn-1\n\n...\narg1\n\narg0\n\nsaved LP\n\nsaved BP\nlocal0\n\nlocal1\n\n...\nlocalK-1\nHigher\nAddresses\n6.004 Computation Structures\n- 15 -\nβ Documentation\n\n(saved regs)\nNote that BP always points to the first stack slot above the saved BP, which is the same as\nthe first local variable (if any). It also points to the third stack slot above the first argument (if\nany). So within the procedure's body, its arguments and locals may be accessed via constant\noffsets from BP.\n6.004 Computation Structures\n- 16 -\nβ Documentation\n\nSummary of β Instruction Formats\nOperate Class:\n10xxxx\nRc\nRa\nRb\nunused\nRegister Symbol\nUsage\nR31\nR31\nAlways zero\nR30\nXP\nException pointer\nR29\nSP\nStack pointer\nR28\nLP\nLinkage pointer\nR27\nBP\nBase of frame pointer\nOP(Ra,Rb,Rc):\nReg[Rc] ← Reg[Ra] op Reg[Rb]\nOpcodes: ADD (plus), SUB (minus), MUL (multiply), DIV (divided by)\nAND (bitwise and), OR (bitwise or), XOR (bitwise exclusive or)\nCMPEQ (equal), CMPLT (less than), CMPLE (less than or equal) [result = 1 if true, 0 if false]\nSHL (left shift), SHR (right shift w/o sign extension), SRA (right shift w/ sign extension)\n11xxxx\nRc\nRa\nliteral (two's complement)\nOPC(Ra,literal,Rc):\nReg[Rc] ← Reg[Ra] op SEXT(literal)\nOpcodes: ADDC (plus), SUBC (minus), MULC (multiply), DIVC (divided by)\nANDC (bitwise and), ORC (bitwise or), XORC (bitwise exclusive or)\nCMPEQC (equal), CMPLTC (less than), CMPLEC (less than or equal) [result = 1 if true, 0 if false]\nSHLC (left shift), SHRC (right shift w/o sign extension), SRAC (right shift w/ sign extension)\nOther:\n01xxxx\nRc\nRa\nliteral (two's complement)\nLD(Ra,literal,Rc):\nReg[Rc] ← Mem[Reg[Ra] + SEXT(literal)]\nST(Rc,literal,Ra):\nMem[Reg[Ra] + SEXT(literal)] ← Reg[Rc]\nJMP(Ra,Rc):\nReg[Rc] ← PC + 4; PC ← Reg[Ra]\nBEQ/BF(Ra,label,Rc):\nReg[Rc] ← PC + 4; if Reg[Ra] = 0 then PC ← PC + 4 + 4*SEXT(literal)\nBNE/BT(Ra,label,Rc):\nReg[Rc] ← PC + 4; if Reg[Ra] = 0 then PC ← PC + 4 + 4*SEXT(literal)\nLDR(Ra,label,Rc):\nReg[Rc] ← Mem[PC + 4 + 4*SEXT(literal)]\nOpcode Table: (*optional opcodes)\n2:0\n5:3\nLD\nST\nJMP\nBEQ\nBNE\nLDR\nADD\nSUB\nMUL*\nDIV*\nCMPEQ\nCMPLT\nCMPLE\nAND\nOR\nXOR\nSHL\nSHR\nSRA\n\nADDC\nSUBC\nMULC*\nDIVC*\nCMPEQC\nCMPLTC\nCMPLEC\n\nANDC\nORC\nXORC\nSHLC\nSHRC\nSRAC\n6.004 Computation Structures\n- 17 -\nβ Documentation"
    },
    {
      "category": "Resource",
      "title": "Lab #1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/bb98570fce23a00d0e8fb20201d350f9_MIT6_004s09_lab01.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.004 Computation Structures\nSpring 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nM A S S A C H U S E T T S I N S T I T U T E O F T E C H N O L O G Y\nDEPARTMENT OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE\n6.004 Computation Structures\nLab #1\nGeneral Information\nLab assignments are due on Thursdays; check the on-line course calendar for the actual due date\nfor each lab. Look at the on-line questions before you start to see what information you should\ncollect while working on each lab.\nYou can visit each on-line question page as many times as necessary to complete the assignment\n- you do not have to answer all the questions in a single session. Click on the \"Save\" button at\nthe bottom of the question page to save your answers. You can then come back to the page later\non and pick up where you left off. You can also check your answers at any time by clicking on\nthe \"Check\" button. When the system detects that all your answers are correct (either because of\na \"Check\" or \"Save\"), it will give you credit for completing the assignment.\nTo receive credit for a lab, you'll need to have a short lab checkoff meeting with a member of\nthe course staff and answer some questions about your work. Just come by the lab after you've\ncompleted your check-in and talk with one of the on-duty staff. The meeting can happen after the\ndue date of the lab but to receive full credit you must complete the meeting within one week of\nthe lab due date. To avoid long waits choose a time other than Wed or Thu evenings.\nThe lab gets crowded just before an assignment is due and some of the problems are probably too\nlong to be done the night before the due date, so plan accordingly. There will be course staff in\nthe lab during the late afternoon and evening; check the course website for this semester's\nschedule.\nThe 6.004 lab is open 24 hours-a-day, 7 days-a-week. An access code is\nrequired for entry; it was given out in the email that included your section assignment. The lab\nhas Linux-Athena workstations that can be used to complete the homework assignments. The lab\nsoftware is written in Java and should run on any Java Virtual Machine supporting JDK 1.3 or\nhigher. The courseware can be run on any Sun or Linux Athena workstation. You can also\ndownload the courseware for your Linux or Windows machine at home - see the Courseware\npage at the 6.004 website.\nIntroduction to JSim\nIn this lab, we'll be using a simulation program (JSim) to make some measurements of an N-\nchannel mosfet (or \"nfet\" for short). JSim uses mathematical models of circuit elements to make\npredictions of how a circuit will behave both statically (DC analysis) and dynamically (transient\nanalysis). The model for each circuit element is parameterized, e.g., the mosfet model includes\nparameters for the length and width of the mosfet as well as many parameters characterizing the\nphysical aspects of the manufacturing process. For the models we are using, the manufacturing\n6.004 Computation Structures\n- 1 -\nLab #1\n\nparameters have been derived from measurements taken at the integrated circuit fabrication\nfacility and so the resulting predictions are quite accurate.\nThe (increasingly) complete JSim documentation can be found at the course website. But we'll\ntry to include pertinent JSim info in each lab writeup.\nTo run JSim, login to an Athena console. We recommend using the computers in the 6.004 lab\nsince JSim has been tested and is known to run with satisfactory performance in that\nenvironment. Another benefit of using the 6.004 lab is that there's plenty of help around, both\nfrom your fellow students and the course staff. After signing onto the Athena station, add the\n6.004 locker to gain access to the design tools and model files (you only have to do this once each\nsession):\nathena% add 6.004\nStart JSim running in a separate window by typing\nathena% jsim &\nIt can take a few moments for the Java runtime system to start up, please be patient! JSim takes\nas input a netlist that describes the circuit to be simulated. The initial JSim window is a very\nsimple editor that lets you enter and modify your netlist. You may find the editor unsatisfactory\nfor large tasks--it's based on the JTextArea widget of the Java Swing toolkit that in some\nimplementations has only rudimentary editing capabilities. If you use a separate editor to create\nyour netlists, you can have JSim load your netlist files when it starts:\nathena% jsim filename ... filename &\nThere are various handy buttons on the JSim toolbar:\nExit. Asks if you want to save any modified file buffers and then exits JSim.\nNew file. Create a new edit buffer called \"untitled\". Any attempts to save this\nbuffer will prompt the user for a filename.\nOpen file. Prompts the user for a filename and then opens that file in its own edit\nbuffer. If the file has already been read into a buffer, the buffer will be reloaded\nfrom the file (after asking permission if the buffer has been modified).\nClose file. Closes the current edit buffer after asking permission if the buffer has\nbeen modified.\nReload file. Reload the current buffer from its source file after asking permission\nif the buffer has been modified. This button is useful if you are using an external\neditor to modify the netlist and simply want to reload a new version for\nsimulation.\n6.004 Computation Structures\n- 2 -\nLab #1\n\nSave file. If any changes have been made, write the current buffer back to its\nsource file (prompting for a file name if this is an untitled buffer created with the\n\"new file\" command). If the save was successful, the old version of the file is\nsaved with a \".bak\" extension.\nSave file, specifying new file name. Like \"Save file\" but prompts for a new file\nname to use.\nSave all files. Like \"save file\" but applied to all edit buffers.\nStop simulation. Clicking this control will stop a running simulation and display\nwhatever waveform information is available.\nDevice-level simulation. Use a Spice-like circuit analysis algorithm to predict\nthe behavior of the circuit described by the current netlist. After checking the\nnetlist for errors, JSim will create a simulation network and then perform the\nrequested analysis (i.e., the analysis you asked for with a \".dc\" or \".tran\" control\nstatement). When the simulation is complete the waveform window is brought to\nthe front so that the user can browse any results plotted by any \".plot\" control\nstatements.\nFast transient analysis. This simulation algorithm uses more approximate device\nmodels and solution techniques than the device-level simulator but should be\nmuch faster for large designs. For digital logic, the estimated logic delays are\nusually within 10% of the predictions of device-level simulation. This simulator\nonly performs transient analysis.\nGate-level simulation. This simulation algorithm only knows about gates and\nlogic values (instead of devices and voltages). We'll use this feature later in the\nterm when trying to simulate designs that contain too many mosfets to be\nsimulated at the device level.\nSwitch to waveform window. In the waveform window this button switches to\nthe editor window. Of course, you can accomplish the same thing by clicking on\nthe border of the window you want in front, but sometimes using this button is\nless work.\nUsing information supplied in the checkoff file, check for specified node values\nat given times. If all the checks are successful, submit the circuit to the on-line\nassignment system.\nThe waveform window shows various waveforms in one or more \"channels.\" Initially one\nchannel is displayed for each \".plot\" control statement in your netlist. If more than one waveform\n6.004 Computation Structures\n- 3 -\nLab #1\n\nis assigned to a channel, the plots are overlaid on top of each using a different drawing color for\neach waveform. If you want to add a waveform to a channel simply add the appropriate signal\nname to the list appearing to the left of the waveform display (the name of each signal should be\non a separate line). You can also add the name of the signal you would like displayed to the\nappropriate \".plot\" statement in your netlist and rerun the simulation. If you simply name a node\nin your circuit, its voltage is plotted. You can also ask for the current through a voltage source by\nentering \"I(Vid)\".\nThe waveform window has several other buttons on its toolbar:\nSelect the number of displayed channels; choices range between 1 and 16.\nPrint. Prints the contents of the waveform window (in color if you have a color\nprinter!). If you are using Athena, you have to print to a file and then send the\nfile to the printer: select \"file\" in the print dialog, supply the name you'd like to\nuse for the plot file, then click \"print\". You can send the file to one of the printers\nin the lab using \"lpr\", e.g., \"lpr -Pcs foo.plot\".\nYou can zoom and pan over the traces in the waveform window using the control found along the\nbottom edge of the waveform display:\nzoom in. Increases the magnification of the waveform display. You can zoom in\naround a particular point in a waveform by placing the cursor at the point on the\ntrace where you want to zoom in and typing upper-case \"X\".\nzoom out. Decreases the magnification of the waveform display. You can zoom\nout around a particular point in a waveform by placing the cursor at the point on\nthe trace where you want to zoom out and typing lower-case \"x\".\nsurround. Sets the magnification so that the entire waveform will be visible in\nthe waveform window.\nThe scrollbar at the bottom of the waveform window can be used to scroll through the\nwaveforms. The scrollbar will be disabled if the entire waveform is visible in the window. You\ncan recenter the waveform display about a particular point by placing the cursor at the point\nwhich you want to be at the center of the display and typing \"c\".\nThe JSim netlist format is quite similar to that used by Spice, a well-known circuit simulator.\nEach line of the netlist is one of the following:\nA comment line, indicated by an \"*\" (asterisk) as the first character. Comment lines (and also\nblank lines) are ignored when JSim processes your netlist. You can also add comments at the\nend of a line by preceding the comment with the characters \"//\" (C++- or Java-style\ncomments). All characters starting with \"//\" to the end of the line are ignored. Any portion\nof a line or lines can be turned into a comment by enclosing the text in \"/*\" and \"*/\" (C-style\ncomments).\n6.004 Computation Structures\n- 4 -\nLab #1\n\nA continuation line, indicated by a \"+\" (plus) as the first character. Continuation lines are\ntreated as if they had been typed at the end of the previous line (without the \"+\" of course).\nThere's no limitation on the length of an input line but sometimes it's easier to edit long lines\nif you use continuation lines. Note that \"+\" also continues \"*\" comment lines!\nA control statement, indicated by a \".\" (period) as the first character. Control statements\nprovide information about how the circuit is to be simulated. We'll describe the syntax of the\ndifferent control statements as we use them below.\nA circuit element, indicated by a letter as the first character. The first letter indicates the type\nof circuit element, e.g., \"r\" for resistor, \"c\" for capacitor, \"m\" for mosfet, \"v\" for voltage\nsource. The remainder of the line specifies which circuit nodes connect to which device\nterminals and any parameters needed by that circuit element. For example the following line\ndescribes a 1000Ω resistor called \"R1\" that connects to nodes A and B.\nR1 A B 1k\nNote that numbers can be entered using engineering suffixes for readability. Common\nsuffixes are \"k\"=1000, \"u\"=1E-6, \"n\"=1E-9 and \"p\"=1E-12.\nCharacterizing MOSFETs\nLet's make some measurements of an nfet by hooking it up to a couple of voltage sources to\ngenerate different values for VGS and VDS:\nWe've included an ammeter (built from a 0v voltage source) so we can measure IDS, the current\nflowing through the mosfet from its drain terminal to its source terminal. Here's the translation\nof the schematic into our netlist format:\n* plot Ids vs. Vds for 5 different Vgs values\n.include \"/mit/6.004/jsim/nominal.jsim\"\nVmeter vds drain 0v\nVds vds 0 0v\nVgs gate 0 0v\n* N-channel mosfet used for our test\nM1 drain gate 0 0 NENH W=1.2u L=600n\n.dc Vds 0 5 .1 Vgs 0 5 1\n.plot I(Vmeter)\n6.004 Computation Structures\n- 5 -\nLab #1\n\nThe first line is a comment. The second line is a control statement that directs JSim to include a\nnetlist file containing the mosfet model parameters for the manufacturing process we'll be\ntargeting this semester. The pathname that's shown will work when running on Athena; if you're\nrunning at home you'll need to specify the directory where you downloaded the 6.004 tools. The\nnext three lines specify three voltage sources; each voltage source specifies the two terminal\nnodes and the voltage we want between them. Note that the reference node for the circuit\n(marked with a ground symbol in the schematic) is always called \"0\". The \"v\" following the\nvoltage specification isn't a legal scale factor and will be ignored by JSim - it's included just\nremind ourselves that last number is the voltage of the voltage source. All three sources are\ninitially set to 0 volts but the voltage for the Vds and Vgs sources will be changed later when\nJSim processes the \".dc\" control statement.\nWe can ask JSim to plot the current through voltage sources which is how we'll see what IDS is\nfor different values of VGS and VDS. We could just ask for the current of the VDS voltage source,\nbut the sign would be wrong since JSim uses the convention that positive current flows from the\npositive to negative terminal of a voltage source. So we introduce a 0-volt source with its\nterminals oriented to produce the current sign we're looking for.\nThe sixth line is the mosfet itself, where we've specified (in order) the names of the drain, gate,\nsource and substrate nodes of the mosfet. The next item names the set of model parameters JSim\nshould use when simulating this device; specify \"NENH\" to create an nfet and \"PENH\" for a P-\nchannel mosfet (\"pfet\"). The final two entries specify the width and length of the mosfet. Note\nthat the dimensions are in microns (1E-6 meters) since we've specified the \"u\" scale factor as a\nsuffix. Don't forget the \"u\" or your mosfets will be meters long! You can always use\nscientific notation (e.g., 1.2E-6) if suffixes are confusing.\nThe seventh line is a control statement requesting a DC analysis of the circuit made with different\nsettings for the Vds and Vgs voltage sources: the voltage of Vds is swept from 0V to 5V in .1V\nsteps, and the voltage of Vgs is swept from 0V to 5V in 1V steps. Altogether 51 * 6 separate\nmeasurements will be made.\nThe eighth and final line requests that JSim plot the current through the voltage source named\n\"Vmeter\". JSim knows how to plot the results from the dual voltage sweep requested on the\nprevious line: it will plot I(Vmeter) vs. the voltage of source Vds for each value of voltage of the\nsource Vgs--there will be 6 plots in all, each consisting of 51 connected data points.\nAfter you enter the netlist above, you might want to save your efforts for later use by using the\n\"save file\" button. To run the simulation, click the \"device-level simulation\" button on the tool\nbar. After a pause, a waveform window will pop up where we can take some measurements. As\nyou move the mouse over the waveform window, a moving cursor will be displayed on the first\nwaveform above the mouse's position and a readout giving the cursor coordinates will appear in\nthe upper left hand corner of the window. To measure the delta between two points, position the\nmouse so the cursor is on top of the first point. Now click left and drag the mouse (i.e., move the\nmouse while holding its left button down) to bring up a second cursor that you can then position\nover the second point. The readout in the upper left corner will show the coordinates for both\ncursors and the delta between the two coordinates. You can return to one cursor by releasing the\nleft button.\nWe're now ready to make some measurements:\n6.004 Computation Structures\n- 6 -\nLab #1\n\n(A) To get a sense of how well the channel of a turned-on mosfet conducts, let's estimate the\neffective resistance of the channel while the mosfet is in the linear conduction region.\nWe'll use the Vgs = 5V curve (the upper-most plot in the window). The actual effective\nresistance is given by ∂VDS/∂IDS and clearly depends on which VDS we choose. Let's use\nVDS = 1.2V. We could determine the resistance graphically from the slope of a line\ntangent to the IDS curve at VDS = 1.2V. But we can get a rough idea of the channel\nresistance by determining the slope of a line passing through the origin and the point we\nchose on the IDS curve, i.e., 1.2V/ IDS.\nOf course, the channel resistance depends on the dimensions of the mosfet we used to\nmake the measurement. For mosfets, their IDS is proportional to W/L where W is the\nwidth of the mosfet (1.2 microns in this example) and L is the length (0.6 microns in this\nexample). When reporting the effective channel resistance, it's useful to report the sheet\nresistance, i.e., the resistance when W/L = 1. That way you can easily estimate the\neffective channel resistance for size device by scaling the sheet resistance appropriately.\nSince W/L = 2 for the device you measured, it conducted twice as much current and has\nhalf the channel resistance as a device with W/L = 1, so you need to double the channel\nresistance you computed above in order to estimate the effective channel sheet resistance.\nUse the on-line questions page for this lab to report the value for IDS that you measured\nand the effective channel sheet resistance you calculated from that measurement.\n(B) Now let's see how well the mosfet turns \"off.\" Take some measurements of IDS at various\npoints along the VGS=0V curve (the bottom-most plot in the window). Notice that they\naren't zero! Mosfets do conduct minute amounts of current even when officially \"off\", a\nphenomenon called \"subthreshold conduction.\" While negligible for most purposes, this\ncurrent is significant if we are trying to store charge on a capacitor for long periods of time\n(this is what DRAMs try to do). Make a measurement of IDS when VGS=0V and\nVDS=2.5V. Based on this measurement report how long it would take for a .05pF\ncapacitor to discharge from 5V to 2.5V, i.e., to change from a valid logic \"1\" to a voltage\nin the forbidden zone. Recall from 6.002 that Q = CV, so we can estimate the discharge\n. So if our mosfet switch controls access to the storage\ncapacitor, you can see we'll need to refresh the capacitor's charge at fairly frequent\nintervals.\nGate-level timing\nThe following JSim netlist shows how to define your own circuit elements using the \".subckt\"\nstatement:\n* circuit for Lab#1, parts C thru F\n.include \"/mit/6.004/jsim/nominal.jsim\"\n* 2-input NAND: inputs are A and B, output is Z\n.subckt nand2 a b z\nMPD1 z a 1 0 NENH sw=8 sl=1\nMPD2 1 b 0 0 NENH sw=8 sl=1\nMPU1 z a vdd vdd PENH sw=8 sl=1\nMPU2 z b vdd vdd PENH sw=8 sl=1\n.ends\ntime as\n6.004 Computation Structures\n- 7 -\nLab #1\n\n* INVERTER: input is A, output is Z\n.subckt inv a z\nMPD1 z a 0 0 NENH sw=16 sl=1\nMPU1 z a vdd vdd PENH sw=16 sl=1\n.ends\nThe \".subckt\" statement introduces a new level of netlist. All lines following the \".subckt\" up to\nthe matching \".ends\" statement will be treated as a self-contained subcircuit. This includes model\ndefinitions, nested subcircuit definitions, electrical nodes and circuit elements. The only parts of\nthe subcircuit visible to the outside world are its terminal nodes which are listed following the\nname of the subcircuit in the \".subckt\" statement:\n.subckt name terminals...\n* internal circuit elements are listed here\n.ends\nIn the example netlist, two subcircuits are defined: \"nand2\" which has 3 terminals (named \"a\",\n\"b\" and \"z\" inside the nand2 subcircuit) and \"inv\" which has 2 terminals (named \"a\" and \"z\").\nOnce the definitions are complete, you can create an instance of a subcircuit using the \"X\" circuit\nelement:\nXid nodes... name\nwhere name is the name of the circuit definition to be used, id is a unique name for this instance\nof the subcircuit and nodes... are the names of electrical nodes that will be hooked up to the\nterminals of the subcircuit instance. There should be the same number of nodes listed in the \"X\"\nstatement as there were terminals in the \".subckt\" statement that defined name. For example,\nhere's a short netlist that instantiates 3 NAND gates (called \"g0\", \"g1\" and \"g2\"):\nXg0 d0 ctl z0 nand2\nXg1 d1 ctl z1 nand2\nXg2 d2 ctl z2 nand2\nThe node \"ctl\" connects to all three gates; all the other terminals are connected to different nodes.\nNote that any nodes that are private to the subcircuit definition (i.e., nodes used in the subcircuit\nthat don't appear on the terminal list) will be unique for each instantiation of the subcircuit. For\nexample, there is a private node named \"1\" used inside the nand2 definition. When JSim\nprocesses the three \"X\" statements above, it will make three independent nodes called \"xg0.1\",\n\"xg1.1\" and \"xg2.1\", one for each of the three instances of nand2. There is no sharing of internal\nelements or nodes between multiple instances of the same subcircuit.\nIt is sometimes convenient to define nodes that are shared by the entire circuit, including\nsubcircuits; for example, power supply nodes. The ground node \"0\" is such a node; all references\nto \"0\" anywhere in the netlist refer to the same electrical node. The included netlist file\nnominal.jsim defines another shared node called \"vdd\" using the following statements:\n.global vdd\nVDD vdd 0 3.3v\nThe example netlist above uses \"vdd\" whenever a connection to the power supply is required.\n6.004 Computation Structures\n- 8 -\nLab #1\n\nThe other new twist introduced in the example netlist is the use of symbolic dimensions for the\nmosfets (\"SW=\" and \"SL=\") instead of physical dimensions (\"W=\" and \"L=\"). Symbolic\ndimensions specify multiples of a parameter called SCALE, which is also defined in\nnominal.jsim:\n.option SCALE=0.6u\nSo with this scale factor, specifying \"SW=8\" is equivalent to specifying \"W=4.8u.\" Using\nsymbolic dimensions is encouraged since it makes it easier to determine the W/L ratio for a\nmosfet (the current through a mosfet is proportional to W/L) and it makes it easy to move the\ndesign to a new manufacturing process that uses different dimensions for its mosfets. Note that in\nalmost all instances \"SL=1\" since increasing the channel length of a mosfet reduces its current\ncarrying capacity, not something we're usually looking to do.\nWe'll need to keep the PN junctions in the source and drain diffusions reverse biased to ensure\nthat the mosfets stay electrically isolated, so the substrate terminal of nfet (those specifying the\n\"NENH\" model) should always be hooked to ground (node \"0\"). Similarly the substrate terminal\nof pfet (those specifying the \"PENH\" model) should always be hooked to the power supply (node\n\"vdd\").\nWith the preliminaries out of the way, we can tackle some design issues:\n(C) To maximize noise margins we want to have the transition in the voltage transfer\ncharacteristic (VTC) of the nand2 gate centered halfway between ground and the power\nsupply voltage (3.3V). To determine the VTC for nand2, we'll perform a dc analysis to\nplot the gate's output voltage as a function of the input voltage using the following\nadditional netlist statements:\n* dc analysis to create VTC\nXtest vin vin vout nand2\nVin vin 0 0v\nVol vol 0 0.3v\n// make measurements easier!\nVoh voh 0 3v\n// see part (D)\n.dc Vin 0 3.3 .005\n.plot vin vout voh vol\nCombine this netlist fragment with the one given at the start of this section and run the\nsimulation. To center the VTC transition, keep the size of the nfet in the nand2 definition\nas \"SW=8 SL=1\" and adjust the width of both pfets until the plots for vin and vout\nintersect at about 1.65 volts. Just try different integral widths (i.e, 9, 10, 11, ...). Report\nthe integral width that comes closest to having the curves intersect at 1.65V.\n(D) The noise immunity of a gate is the smaller of the low noise margin (VIL - VOL) and the\nhigh noise margin (VOH - VIH). If we specify VOL = 0.3V and VOH = 3.0V, what is the\nlargest possible noise immunity we could specify and still have the \"improved\" NAND\ngate of part (C) be a legal member of the logic family?\nHint: to measure the low noise margin, use the VTC to determine what VIN has to be in\n6.004 Computation Structures\n- 9 -\nLab #1\n\norder for VOUT to be 3V, and then subtract VOL (0.3V) from that number. To measure the\nhigh noise margin, use the VTC to determine what VIN has to be in order for VOUT to be\n0.3V, and then subtract that number from VOH (3.0V). We've added some voltage sources\ncorresponding to VOL and VOH to make it easier to make the measurements on the VTC\nplot.\nNOTE: make these measurements using your \"improved\" nand2 gate that has the centered\nVTC, i.e., with the updated widths for the PFETS.\nNow that we have the mosfets ratioed properly to maximize noise immunity, let's measure the\ncontamination time (tC) and propagation time (tP) of the nand2 gate. The contamination delay,\ntCD, for the nand2 gate will be a lower bound for all the tC measurements we make. Similarly, the\npropagation delay, tPD, for the nand2 gate will be an upper bound for all the tP measurements.\nRecall that the contamination time is the period of output validity after the inputs have become\ninvalid. So for nand2:\ntC-FALL = time elapsed from when input > VIL to when output < VOH\ntC-RISE = time elapsed from when input < VIH to when output > VOL\ntC = min(tC-RISE, tC-FALL)\nSimilarly the propagation time is the period of output invalidity after the inputs have become\nvalid. So for nand2:\ntP-RISE = time elapsed from when input ≤ VIL to when output ≥ VOH\ntP-FALL = time elapsed from when input ≥ VIH to when output ≤ VOL\ntP = max(tP-RISE, tP-FALL)\nFollowing standard practice, we'll choose the logic thresholds as follows:\nVOL = 10% of power supply voltage = .3V\nVIL = 20% of power supply voltage = .6V\nVIH = 80% of power supply voltage = 2.6V\nVOH = 90% of power supply voltage = 3V\nYou can use a voltage source with either a pulse or piece-wise linear waveform to generate test\nsignals for your circuit. Here's how to enter them in your netlist:\nVid output 0 pulse(val1 val2 td tr tf pw per)\nThis statement produces a periodic waveform with the following shape:\n6.004 Computation Structures\n- 10 -\nLab #1\n\nDon't forget to specify your times in nanoseconds (use an \"n\" suffix)! Do not specify zero rise\nand fall times since the simulation will probably fail to converge. To construct a piece-wise\nlinear waveform, you need to supply a list of time,voltage pairs:\nVid output 0 pwl(t1 v1 t2 v2 ... tn vn)\nThe voltage will be v1 for times before t1, and vn for times after tn.\n(E) Replace the netlist fragment from (C) with the following test circuit that will let us\nmeasure various delays:\n* test jig for measuring tcd and tpd\nXdriver vin nin inv\nXtest vdd nin nout nand2\nCload nout 0 .02pf\nVin vin 0 pulse(3.3,0,5ns,.1ns,.1ns,4.8ns)\nVol vol 0 0.3v\n// make measurements easier!\nVil vil 0 0.6v\nVih vih 0 2.6v\nVoh voh 0 3.0v\n.tran 15ns\n.plot vin\n.plot nin nout vol vil vih voh\nNOTE: make these measurements using your \"improved\" nand2 gate that has the centered\nVTC, i.e., with the updated widths for the pfets.\nWe use an inverter to drive the nand2 input since we would normally expect the test gate\nto be driven by the output of another gate (there are some subtle timing effects that we'll\nmiss if we drive the input directly with a voltage source). Run the simulation and measure\nthe contamination and propagation delays for both the rising and falling output transitions.\n(You'll need to zoom in on the transitions in order to make an accurate measurement.)\nCombine as described above to produce estimates for tC and tP.\n(F) We mentioned several times in lecture the desire to have our circuits operate reliably over\na wide range of environmental conditions. We can have JSim simulate our test circuit at\ndifferent temperature by adding a \".temp\" control statement to the netlist. Normally JSim\nsimulates the circuit at room temperature (25°C), but we can simulate the circuit at, say,\n6.004 Computation Structures\n- 11 -\nLab #1\n\n100°C by adding the following to our netlist:\n.temp 100\nFor many consumer products, designs are tested in the range of 0°C to 100°C. Repeat\nyour measurements of part (E) at 100°C and report your findings. Recompute your\nestimates for tC and tP indicating which measurement(s) determined your final choice for\nthe two delays.\nBased on your experiment, if a Pentium 4 processor is rated to run correctly at 3Ghz at\n100°C, how fast can you clock it and still have it run correctly at room temperature\n(assuming tPD is the parameter that determines \"correct\" behavior)? This is why you can\nusually get away with overclocking your CPU--it's been rated for operation under much\nmore severe environmental conditions than you're probably running it at!\nCMOS logic-gate design\nAs the final part of this lab, your mission is to design and test a CMOS circuit that implements\nthe function F(A,B,C) = C + A·B using nfets and pfets. The truth table for F is shown below:\nA B C F(A,B,C)\nYour circuit must contain no more than 8 mosfets. Remember that only nfets should be used\nin pulldown circuits and only pfets should be in pullup circuits. Hint: using six mosfets,\nimplement the complement of F as one large CMOS gate and then use the remaining two mosfets\nto invert the output of your large gate.\nEnter the netlist for F as a subcircuit that can be tested by the test-jig built into lab1checkoff.jsim\n(a file that we supply and which can be found in the course locker). Note that the checkoff\ncircuitry expects your F subcircuit to have exactly the terminals shown below - the inside\ncircuitry is up to you, but the \".subckt F...\" line in your netlist should match exactly the one\nshown below.\n.include \"/mit/6.004/jsim/nominal.jsim\"\n.include \"/mit/6.004/jsim/lab1checkoff.jsim\"\n... you can define other subcircuits (eg, INV or NAND gates) here ...\n.subckt F A B C Z\n... your circuit netlist here\n.ends\n6.004 Computation Structures\n- 12 -\nLab #1\n\nlab1checkoff.jsim contains the necessary circuitry to generate the appropriate input waveforms to\ntest your circuit. It includes a .tran statement to run the simulation for the appropriate length of\ntime and a few .plot statements which will display the input and output waveforms for your\ncircuit.\nFor faster simulation, use the\n(fast transient analysis) button!\nCheckoff\nWhen you are satisfied your circuit works, you can start the checkoff process by making sure\nyour top-level netlist is visible in the edit window and that you've complete a successful\nsimulation run. Then click on the green checkmark in the toolbar. JSim proceeds with the\nfollowing steps:\n1. JSim verifies that a .checkoff statement was found when your netlist was read in.\n2. JSim processes each of the .verify statements in turn by retrieving the results of the most\nrecent simulation and comparing the computed node values against the supplied expected\nvalues. It will report any discrepancies it finds, listing the names of the nodes it was\nchecking, the simulated time at which it was checking the value, the expected value and\nthe actual value.\n3. When the verification process is successful, Jsim asks for your 6.004 user name and\npassword (the same ones you use to login to the on-line assignment system) so it can send\nthe results to the on-line assignment server.\n4. JSim sends your circuits to the on-line assignment server, which responds with a status\nmessage that will be displayed for you. If you've misentered your username or password\nyou can simply click on the green checkmark to try again. Note that the server will check\nif your circuit has at most 8 mosfets - if it contains more, you'll see a message to that\neffect and your check-in will not complete.\nIf you have any difficulties with checkoff, talk to a TA.\nRemember to schedule a lab checkoff meeting with a member of\nthe course staff after you complete the on-checkoff and on-line questions. This meeting can\nhappen after the due date of the lab but must be completed within one week of the lab's due\ndate in order to receive full credit.\n6.004 Computation Structures\n- 13 -\nLab #1"
    },
    {
      "category": "Resource",
      "title": "Lab #2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/1dcbb09e592e1dabbacea66b18ebb842_MIT6_004s09_lab02.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.004 Computation Structures\nSpring 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nM A S S A C H U S E T T S I N S T I T U T E O F T E C H N O L O G Y\nDEPARTMENT OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE\n6.004 Computation Structures\nLab #2\nYour mission this week is to design and test a CMOS circuit that performs addition on two\nunsigned 4-bit numbers, producing a 5-bit result:\nA0\nA1\nA2\nA3\nB0\nB1\nB2\nB3\nADD\nS0\nS1\nS2\nS3\nS4\nWhen you've completed and tested your design, you can ask JSim to send your circuit to the on\nline assignment system using the process described at the end of Lab #1. The checkoff file for\nLab #2 (lab2checkoff.jsim) checks that your circuit has the right functionality; the on-line system\nwill give you 5 points for checking off your lab using this file. (You'll receive your points after\ncompleting the on-line questions and a checkoff meeting with a TA.)\nNote: Our ability to provide automated checkoffs is predicated on trusting that you'll use\nthe checkoff and library files as given. Since these files are included in your submission, we\nwill be checking to see if these files have been used as intended. Submittals that include\nmodified checkoff or library files will be regarded as a serious breach of our trust and will\nbe dealt with accordingly.\nSome suggestions on how to proceed:\nA Let's start with a simple ripple-carry adder based on the full-adder module discussed in\nlecture. Later we'll discuss higher performance adder architectures you can use in the\nimplementation of the Beta (the computer central processing unit we'll be designing in later labs).\nThe full adder module has 3 inputs (A, B and Ci) and 2 outputs (S and Co). The logic equations\nand truth table for S and Co are shown below.\nS = A ⊕ B ⊕ Cin\nCo = A ⋅B + A ⋅Cin + B ⋅Cin\n6.004 Computation Structures\n- 1 -\nLab #2\n\nTypically S is implemented using two cascaded 2-input XOR gates. One can use three 2-input\nNANDs and one 3-input NAND to implement Co (remember that by Demorgan's Law two\ncascaded NANDs is logically equivalent to a cascade of AND/OR).\nThe module performs the addition of two one-bit inputs (A and B) incorporating the carry in from\nthe previous stage (Ci). The result appears on the S output and a carry (Co) is generated for the\nnext stage. A possible schematic for the 4-bit adder is shown below:\nA3\nB3\nA2\nB2\nA1\nB1\nA0\nB0\nFA\nFA\nFA\nFA\n\"0\"\nS4\nS3\nS2\nS1\nS0\nA Since we're using individual gates to implement the logic, a good place to start is to build\nyour own gate library (e.g., inverter, 2-input NAND, 2-input NOR, 2-input XOR), test them\nindividually, and then use them to implement your design. It's much easier to debug your circuit\nmodule-by-module rather than as one big lump. XOR/XNOR can be challenging gates to design;\nhere's one suggestion for how they might be implemented:\n6.004 Computation Structures\n- 2 -\nLab #2\n\nA You can use voltage sources with either a pulse or piece-wise linear waveforms to generate\ntest signals for your circuit (see Lab #1 for details). Another source of test waveforms is the file\n\"/mit/6.004/jsim/8clocks.jsim\" which can be included in your netlist. It provides eight different\nsquare waves (50% duty cycle) with different periods:\nclk1\nperiod = 10ns\nclk2\nperiod = 20ns\nclk3\nperiod = 40ns\nclk4\nperiod = 80ns\nclk5\nperiod = 160ns\nclk6\nperiod = 320ns\nclk7\nperiod = 640ns\nclk8\nperiod = 1280ns\nFor example, to completely test all possible input combinations for a 2-input gate, you could\nconnect clk1 and clk2 to the two inputs and simulate for 20ns.\nA Interpreting analog signal levels as logic values can be tedious. JSim will do it for you\nautomatically if you ask to plot \"L(a)\" instead of just \"a\". The logic-high and logic-thresholds\nare determined by the \"vih\" and \"vil\" options:\n.options vih=2.6 vil=0.6\nInitial values are specified in \"/mit/6.004/jsim/nominal.jsim\", but you can respecify them in your\nown netlist. Voltages between vil and vih are displayed as a filled-in rectangle to indicate that the\nlogic value cannot be determined. For example:\nA\nL(A)\nvih\nvil\n6.004 Computation Structures\n- 3 -\nLab #2\n\nYou can also ask for the values of a set of signals to be displayed as a bus, e.g.,\n\"L(a3,a2,a1,a0)\". The signals should be listed most-significant bit first. A bus waveform is\ndisplayed as a filled-rectangle if any of the component signals has an invalid logic level or as a\nhexadecimal value otherwise. In the following plot the four signals a3, a2, a1 and a0 are\ninterpreted as a 4-bit integer where the high-order bit (a3) is making a 1→0 transition. The filled-\nin rectangle represents the period of time during which a3 transitions from VIH to VIL.\nL(a3,a2,a1,a0)\n0xF\n0x7\nA Here's a list of design tasks you might use to organize your approach to the lab:\n1. Draw a gate-level schematic for the full-adder module. XOR gates can be used to\nimplement the S output; two levels of NAND gates are handy for implementing Co as a\nsum of products.\n2. Create a MOSFET circuit for each of the logic gates you used in step 1.\n3. Enter .subckt definitions in your netlist for each of the logic gates. Use Jsim to test each\nlogic gate with all possible combinations of inputs. Debugging your gate designs one-by\none will be much easier than trying to debug them as part of the adder circuit. Here's a\nsample netlist for testing a 2-input NAND gate called nand2:\n.include \"/mit/6.004/jsim/nominal.jsim\"\n\n.include \"/mit/6.004/jsim/8clocks.jsim\"\n\n.subckt nand2 a b z\n... internals of nand2 circuit here\n\n.ends\n\nXtest clk1 clk2 z nand2\n\n.tran 20ns\n\n.plot clk1\n\n.plot clk2\n\n.plot z\n4. Enter a .subckt definition for the full-adder, building it out of the gates you designed and\ntested above. Use Jsim to test your design with all 8 possible combinations of the three\ninputs. At this point you probably want to switch to using \"Fast Transient Analysis\" do to\nthe simulations as it is much faster than \"Device-level Simulation\".\n5. Enter the netlist for the 4-bit adder and test the circuit using input waveforms supplied by\nlab2checkoff.jsim. Note that the checkoff circuitry expects your 4-bit adder to have\nexactly the terminals shown below - the inside circuitry is up to you, but the \".subckt\nADDER4...\" line in your netlist should match exactly the one shown below.\n.include \"/mit/6.004/jsim/nominal.jsim\"\n\n.include \"/mit/6.004/jsim/lab2checkoff.jsim\"\n\n... subckt definitions of your logic gates\n.subckt FA a b ci s co\n... full-adder internals here\n6.004 Computation Structures\n- 4 -\nLab #2\n\n.ends\n.subckt ADDER4 a3 a2 a1 a0 b3 b2 b1 b0 s4 s3 s2 s1 s0\n* remember the node named \"0\" is the ground node\n* nodes c0 through c3 are internal to the ADDER module\nXbit0 a0 b0 0 s0 c0 FA\nXbit1 a1 b1 c0 s1 c1 FA\nXbit2 a2 b2 c1 s2 c2 FA\nXbit3 a3 b3 c2 s3 s4 FA\n.ends\n\nlab2checkoff.jsim contains the necessary circuitry to generate the appropriate input\nwaveforms to test your adder. It includes a .tran statement to run the simulation for the\nappropriate length of time and a few .plot statements showing the input and output\nwaveforms for your circuit.\nWhen debugging your circuits, you can plot additional waveforms by adding .plot\nstatements to the end of your netlist. For example, to plot the carry-out signal from the\nfirst full adder, you could say\n.plot Xtest.c0\nwhere Xtest is the name lab2checkoff.jsim gave to the ADDER4 device it created and c0\nis the name of the internal node that connects the carry-out of the low-order FA to the\ncarry-in of the next FA.\n6.004 Computation Structures\n- 5 -\nLab #2"
    },
    {
      "category": "Resource",
      "title": "Lab #3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/3c6bca5f331b2a89e120e8ca0b2c48d9_MIT6_004s09_lab03.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.004 Computation Structures\nSpring 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nM A S S A C H U S E T T S I N S T I T U T E O F T E C H N O L O G Y\nDEPARTMENT OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE\n6.004 Computation Structures\nLab #3\nIn this laboratory exercise, we'll build the arithmetic and logic unit (ALU) for the Beta processor.\nThe ALU has two 32-bit inputs (which we'll call \"A\" and \"B\") and produces one 32-bit output.\nWe'll start by designing each piece of the ALU as a separate circuit, each producing its own 32\nbit output. Then we'll combine these outputs into a single ALU result.\nWhen designing circuitry there are three separate factors that can be optimized:\n(1) design for maximum performance (minimum latency)\n(2) design for minimum cost (minimum area)\n(3) design for the best cost/performance ratio (minimize area*latency)\nHappily it's often possible to do all three at once but in some portions of the circuit some sort of\ndesign tradeoff will need to be made. When designing your circuitry you should choose which of\nthese three factors is most important to you and optimize your design accordingly.\nA functional ALU design will earn six points. Four additional points can be earned if you\nimplement the optional multiplier unit - see the section labeled \"Optional Design Problem:\nImplementing Multiply\" for details.\nStandard Cell Library\nThe building blocks for our design will be a family of logic gates that are part of a standard cell\nlibrary. The available combinational gates are listed in the table below along with information\nabout their timing, loading and size. You can access the library by starting your netlist with the\nfollowing include statements:\n.include \"/mit/6.004/jsim/nominal.jsim\"\n.include \"/mit/6.004/jsim/stdcell.jsim\"\nEveryone should use the provided cells in creating their design. The timings have been taken\nfrom a 0.18 micron CMOS process measured at room temperature.\n6.004 Computation Structures\n- 1 -\nLab #3\n\nNetlist\nFunction\ntCD\n(ns)\ntPD\n(ns)\ntR\n(ns/pf)\ntF\n(ns/pf)\nload\n(pf)\nsize\n(μ2)\nXid z constant0\n= 0\nZ\n--\n--\n--\n--\n--\nXid z constant1\n= 1\nZ\n--\n--\n--\n--\n--\nXid a z inverter\n.005\n.02\n2.3\n1.2\n.007 10\nXid a z inverter_2\n.009\n.02\n1.1\n.6\n.013 13\nXid a z inverter_4\nA\nZ =\n.009\n.02\n.56\n.3\n.027 20\nXid a z inverter_8\n.02\n.11\n.28\n.15\n.009 56\nXid a z buffer\n.02\n.08\n2.2\n1.2\n.003 13\nXid a z buffer_2\nA\nZ =\n.02\n.07\n1.1\n.6\n.005 17\nXid a z buffer_4\n.02\n.07\n.56\n.3\n.01 30\nXid a z buffer-8\n.02\n.07\n.28\n.15\n.02 43\nXid e a z tristate\n.03\n.15\n2.3\n1.3\n.004 23\nXid e a z tristate_2\nA\nZ =\nwhen e=1\n.03\n.13\n1.1\n.6\n.006 30\nXid e a z tristate_4\nelse Z not driven\n.02\n.12\n.6\n.3\n.011 40\nXid e a z tristate_8\n.02\n.11\n.3\n.17\n.02 56\nXid a b z and2\nB\nA\nZ\n⋅\n=\n.03\n.12\n4.5\n2.3\n.002 13\nXid a b c z and3\nB C\nA\nZ\n⋅\n⋅\n=\n.03\n.15\n4.5\n2.6\n.002 17\nXid a b c d z and4\nD\nB C\nA\nZ\n⋅\n⋅\n⋅\n=\n.03\n.16\n4.5\n2.5\n.002 20\nXid a b z nand2\nB\nA\nZ\n⋅\n=\n.01\n.03\n4.5\n2.8\n.004 10\nXid a b c z nand3\nB C\nA\nZ\n⋅\n⋅\n=\n.01\n.05\n4.2\n3.0\n.005 13\nXid a b c d z nand4\nD\nB C\nA\nZ\n⋅\n⋅\n⋅\n=\n.01\n.07\n4.4\n3.5\n.005 17\nXid a b z or2\nB\nA\nZ\n+\n=\n.03\n.15\n4.5\n2.5\n.002 13\nXid a b c z or3\nC\nB\nA\nZ\n+\n+\n=\n.04\n.21\n4.5\n2.5\n.003 17\nXid a b c d z or4\nD\nC\nB\nA\nZ\n+\n+\n+\n=\n.06\n.29\n4.5\n2.6\n.003 20\nXid a b z nor2\nB\nA\nZ\n+\n=\n.01\n.05\n6.7\n2.4\n.004 10\nXid a b c z nor3\nC\nB\nA\nZ\n+\n+\n=\n.02\n.08\n8.5\n2.4\n.005 13\nXid a b c d z nor4\nD\nC\nB\nA\nZ\n+\n+\n+\n=\n.02\n.12\n9.5\n2.4\n.005 20\nXid a b z xor2\nB\nA\nZ\n⊕\n=\n.03\n.14\n4.5\n2.5\n.006 27\nXid a b z xnor2\nB\nA\nZ\n⊕\n=\n.03\n.14\n4.5\n2.5\n.006 27\nXid a1 a2 b z aoi21\nB\nA\nA\nZ\n+\n⋅\n=\n2)\n( 1\n.02\n.07\n6.8\n2.7\n.005 13\nXid a1 a2 b z oai21\nB\nA\nA\nZ\n⋅\n+\n=\n2)\n( 1\n.02\n.07\n6.7\n2.7\n.005 17\nXid s d0 d1 z mux2\nZ = D0 when S = 0\nZ = D1 when S = 1\n.02\n.12\n4.5\n2.5\n.005 27\nXid s0 s1 d0 d1 d2 d3 z mux4\n(Note order of s0 and s1!)\nZ=D0 when S0=0, S1=0\nZ=D1 when S0=1, S1=0\nZ=D2 when S0=0, S1=1\nZ=D3 when S0=1, S1=1\n.04\n.19\n4.5\n2.5\n.006 66\nXid d clk q dreg\nt\n= .15, thold = 0\nD→Q on CLK↑\n.03\n.19\n4.3\n2.5\n.002 56\nsetup\n6.004 Computation Structures\n- 2 -\nLab #3\n\nGate-level Simulation\nSince we're designing at the gate level we can use a faster simulator that only knows about gates\nand logic values (instead of transistors and voltages). You can run JSim's gate-level simulator by\nin the toolbar. Note that your design can't contain any mosfets, resistors, capacitors,\netc.; this simulator only supports the gate primitives in the standard cell library.\nclicking\nInputs are still specified in terms of voltages (to maintain netlist compatability with the other\nsimulators) but the gate-level simulator converts voltages into one of three possible logic values\nusing the VIL and VIH thresholds specified in nominal.jsim:\nlogic low (voltages less than or equal to VIL threshold)\nlogic high (voltages greater than or equal to VIH threshold)\nX\nunknown or undefined (voltages between the thresholds, or unknown voltages)\nA fourth value \"Z\" is used to represent the value of nodes that aren't being driven by any gate\noutput (e.g., the outputs of tristate drivers that aren't enabled). The following diagram shows\nhow these values appear on the waveform display:\nX\nZ\nConnecting electrical nodes together using .connect\nJSim has a control statement that lets you connect two or more nodes together so that they behave\nas a single electrical node:\n.connect node1 node2 node3...\nThe .connect statement is useful for connecting two terminals of a subcircuit or for connecting\nnodes directly to ground. For example, the following statement ties nodes cmp1, cmp2, ..., cmp31\ndirectly to the ground node (node \"0\"):\n.connect 0 cmp[31:1]\nNote that the .connect control statement in JSim works differently than many people expect. For\nexample,\n.connect A[5:0] B[5:0]\nwill connect all twelve nodes (A5, A4, ..., A0, B5, B4, ..., B0) together -- usually not what was\nintended. To connect two busses together, one could have entered\n.connect A5 B5\n.connect A4 B4\n...\nwhich is tedious to type. Or one can define a two-terminal device that uses .connect internally,\nand then use the usual iteration rules (see next section) to make many instances of the device with\none \"X\" statement:\n6.004 Computation Structures\n- 3 -\nLab #3\n\n.subckt knex a b\n.connect a b\n.ends\nX1 A[5:0] B[5:0] knex\nUsing iterators to create multiple gates with a single \"X\" statement\nJSim makes it easy to specify multiple gates with a single \"X\" statement. You can create multiple\ninstances of a device by supplying some multiple of the number of nodes it expects, e.g., if a\ndevice has 3 terminals, supplying 9 nodes will create 3 instances of the device. To understand\nhow nodes are matched up with terminals specified in the .subckt definition, imagine a device\nwith P terminals. The sequence of nodes supplied as part of the \"X\" statement that instantiates the\ndevice are divided into P equal-size contiguous subsequences. The first node of each subsequence\nis used to wire up the first device, the second node of each subsequence is used for the second\ndevice, and so on until all the nodes have been used. For example:\nXtest a[2:0] b[2:0] z[2:0] xor2\nis equivalent to\nXtest#0 a2 b2 z2 xor2\nXtest#1 a1 b1 z1 xor2\nXtest#2 a0 b0 z0 xor2\nsince xor2 has 3 terminals. There is also a handy way of duplicating a signal: specifying \"foo#3\"\nis equivalent to specifying \"foo foo foo\". For example, xor'ing a 4-bit bus with a control signal\ncould be written as\nXbusctl in[3:0] ctl#4 out[3:0] xor2\nwhich is equivalent to\nXbusctl#0 in3 ctl out3 xor2\nXbusctl#1 in2 ctl out2 xor2\nXbusctl#2 in1 ctl out1 xor2\nXbusctl#3 in0 ctl out0 xor2\nUsing iterators and the \"constant0\" device from the standard cell library, here's a better way of\nconnecting cmp[31:1] to ground:\nXgnd cmp[31:1] constant0\nSince the \"constant0\" has one terminal and we supply 31 nodes, 31 copies of the device will be\nmade.\nALU Design\nNOTE: the ALUFN signals used to control the operation of the ALU circuitry use an encoding\nchosen to make the design of the ALU circuitry as simple as possible. This encoding is not the\nsame as the one used to encode the 6-bit opcode field of Beta instructions. In Lab 6, you'll\nbuild some logic (actually a ROM) that will translate the opcode field of an instruction into the\nappropriate ALUFN control bits.\n6.004 Computation Structures\n- 4 -\nLab #3\n\n(A) Design an adder/subtractor unit that operates on 32-bit two's complement inputs and\ngenerates a 32-bit output. It will be useful to generate three other output signals to be used\nby the comparison logic in part (B): \"Z\" which is true when the S outputs are all zero, \"V\"\nwhich is true when the addition operation overflows (i.e., the result is too large to be\nrepresented in 32 bits), and \"N\" which is true when the S is negative (i.e., S31 = 1).\nOverflow can never occur when the two operands to the addition have different signs; if\nthe two operands have the same sign, then overflow can be detected if the sign of the\nresult differs from the sign of the operands:\nV = XA 31 ⋅ XB 31 ⋅ S31 + XA31 ⋅ XB31 ⋅ S31\nNote that this equation uses XB31, which is the high-order bit of the B operand to the\nadder itself (i.e., after the XOR gate - see the schematic below).\nALUFN0 will be set to 0 for an ADD (S = A + B) and 1 for a SUBTRACT (S = A - B);\nA[31:0] and B[31:0] are the 32-bit two's complement input operands; S[31:0] is the 32\nbit result; z/v/n are the three condition code bits described above. We'll be using the\n\"little-endian\" bit numbering convention where bit 31 is the most-significant bit and bit 0\nis the least-significant bit.\nThe following schematic is one suggestion for how to go about the design:\n32-bit add\ncarry in\nx32\nA31...A0\nALUFN0\nB31...B0\nS31...S0\nV\nN\nZ\nXB\nXA\nS\nThe ALUFN0 input signal selects whether the operation is an ADD or SUBTRACT. To\ndo a SUBTRACT, the circuit first computes the two's complement negation of the \"B\"\noperand by inverting \"B\" and then adding one (which can be done by forcing the carry-in\nof the 32-bit add to be 1). Start by implementing the 32-bit add using a ripple-carry\narchitecture (you'll get to improve on this later on the lab). You'll have to construct the\n32-input NOR gate required to compute Z using a tree of smaller fan-in gates (the parts\nlibrary only has gates with up to 4 inputs).\nWe've created a test jig to test your adder. Your netlist should incorporate the following\nthree .include statements\n.include \"/mit/6.004/jsim/nominal.jsim\"\n.include \"/mit/6.004/jsim/stdcell.jsim\"\n.include \"/mit/6.004/jsim/lab3adder.jsim\"\n6.004 Computation Structures\n- 5 -\nLab #3\n\nand the following subcircuit definition (you can of course define other subcircuits as\nwell)\n.subckt adder32 ALUFN[0] A[31:0] B[31:0] s[31:0] z v n\n... your adder/subtractor circuit here ...\n.ends\nTo use the test jig, make sure your design file contains a definition for an \"adder32\"\nsubcircuit as shown above. Then do a gate-level simulation; a waveform window\nshowing the adder32 inputs and outputs should appear. Next click the checkoff button\n(the green checkmark) in the toolbar. JSim will check your circuit's results against a list\nof expected values and report any discrepancies it finds. Using this test jig file, nothing\nwill be sent to the on-line server - it's provided to help test your design as you go.\nThe Beta instruction set includes three compare instructions (CMPEQ, CMPLT, CMPLE) that\ncompare the \"A\" and \"B\" operands. We can use the adder unit designed above to compute\n\"A-B\" and then look at the result (actually just the Z, V and N condition codes) to determine\nif A=B, A<B or A≤B. The compare instructions generate a 32-bit Boolean result, using \"0\" to\nrepresent false and \"1\" to represent true.\n(B) Design a 32-bit compare unit that generates one of two constants (\"0\" or \"1\") depending\non the ALUFN control signals (used to select the comparison to be performed) and the Z,\nV, and N outputs of the adder/subtractor unit. Clearly the high order 31 bits of the output\nare always zero. The least significant bit of the output is determined by the comparison\nbeing performed and the results of the subtraction carried out by the adder/subtractor:\nComparison\nEquation for LSB\nALUFN2\nALUFN1\nA = B\nLSB = Z\nA < B\nLSB = N ⊕ V\nA <= B\nLSB = Z + (N ⊕ V)\nALUFN[2:1] are used to control the compare unit since we also need to control the\nadder/subtractor unit (i.e., ALUFN0 = 1 to force a subtract).\nPerformance note: the Z, V and N inputs to this circuit can only be calculated by the\nadder/subtractor unit after the 32-bit add is complete. This means they arrive quite late\nand then require further processing in this module, which in turn makes cmp0 show up\nvery late in the game. You can speed things up considerably by thinking about the\nrelative timing of Z, V and N and then designing your logic to minimize delay paths\ninvolving late-arriving signals.\nWe've created a test jig to test your compare unit. Your netlist should incorporate the\nfollowing three .include statements\n.include \"/mit/6.004/jsim/nominal.jsim\"\n.include \"/mit/6.004/jsim/stdcell.jsim\"\n.include \"/mit/6.004/jsim/lab3compare.jsim\"\nand the following subcircuit definition\n6.004 Computation Structures\n- 6 -\nLab #3\n\n.subckt compare32 ALUFN[2:1] z v n cmp[31:0]\n... your compare circuit here ...\n.ends\n(C) Design a 32-bit Boolean unit for the Beta's logic operations. One implementation of a\n32-bit boolean unit uses a 32 copies of a 4-to-1 multiplexer where ALUFN0, ALUFN1,\nALUFN2, and ALUFN3 encode the operation to be performed, and Ai and Bi are hooked\nto the select inputs. This implementation can produce any of the 16 2-input Boolean\nfunctions; we'll only be using 4 of the possibilities.\nThe following table shows the encodings for the ALUFN[3:0] control signals used by the\ntest jig. If you choose a different implementation you should also include logic to\nconvert the supplied control signals into signals appropriate for your design.\nOperation\nALUFN[3:0]\n\nAND\n\nOR\n\nXOR\n\n\"A\"\n\nWe've created a test jig to test your boolean unit. Your netlist should incorporate the\nfollowing three .include statements\n.include \"/mit/6.004/jsim/nominal.jsim\"\n.include \"/mit/6.004/jsim/stdcell.jsim\"\n.include \"/mit/6.004/jsim/lab3boolean.jsim\"\nand the following subcircuit definition\n.subckt boole32 ALUFN[3:0] A[31:0] B[31:0] boole[31:0]\n... your boolean unit circuit here ...\n.ends\n(D) Design a 32-bit shifter that implements SRA, SHR and SHL instructions. The \"A\"\noperand supplies the data to be shifted and the low-order 5 bits of the \"B\" operand are\nused as the shift count (i.e., from 0 to 31 bits of shift). The desired operation will be\nencoded on ALUFN[1:0] as follows:\nOperation\nALUFN[1:0]\nSHL (shift left)\n6.004 Computation Structures\n- 7 -\nLab #3\n\nSHR (shift right)\nSRA (shift right with sign extension)\nWith this encoding, ALUFN0 is 0 for a left shift and 1 for a right shift and ALUFN1\ncontrols the sign extension logic on right shift. For SHL and SHR, 0's are shifted into\nthe vacated bit positions. For SRA (\"shift right arithmetic\"), the vacated bit positions are\nall filled with A31, the sign bit of the original data so that the result will be the same as\ndividing the original data by the appropriate power of 2.\nThe simplest implementation is to build two shifters--one for shifting left and one for\nshifting right--and then use a 2-way 32-bit multiplexer to select the appropriate answer\nas the unit's output. It's easy to build a shifter after noticing that a multi-bit shift can be\naccomplished by cascading shifts by various powers of 2. For example, a 13-bit shift can\nbe implemented by a shift of 8, followed by a shift of 4, followed by a shift of 1. So the\nshifter is just a cascade of multiplexers each controlled by one bit of the shift count. The\nschematic below shows a possible implementation of the left shift logic; the right shift\nlogic is similar with the slight added complication of having to shift in either \"0\" or\n\"A31.\" Another approach that adds latency but saves gates is to use the left shift logic\nfor both left and right shifts, but for right shifts, reverse the bits of the \"A\" operand on the\nway in and reverse the bits of the output on the way out.\nA[31:0]\nx32\nW[31:0]\nW[31:0]\nx32\nX[31:0]\nA[15:0],GND#16\nW[23:0],GND#8\nB4\n\nB3\nX[31:0]\nx32\nY[31:0]\nY[31:0]\nx32\nZ[31:0]\nX[27:0],GND#4\nY[29:0],GND#2\nB2\nB1\nZ[31:0]\nZ[30:0],GND\nx32\nSL[31:0]\nB0\nWe've created a test jig to test your shift unit. Your netlist should incorporate the\nfollowing three .include statements\n.include \"/mit/6.004/jsim/nominal.jsim\"\n.include \"/mit/6.004/jsim/stdcell.jsim\"\n.include \"/mit/6.004/jsim/lab3shifter.jsim\"\nand the following subcircuit definition\n.subckt shift32 ALUFN[1:0] A[31:0] B[4:0] shift[31:0]\n... your shifter circuit here ...\n.ends\n6.004 Computation Structures\n- 8 -\nLab #3\n\n(E) Combine the outputs of the adder, compare, boolean and shift units to produce a single\nALU output. The simplest approach is to use a 4-way 32-bit multiplexer as shown in the\nschematic below:\nA[31:0]\nB[31:0]\nALUFN0\nA[31:0]\nB[31:0]\nALUFN[3:0]\nA[31:0]\nB[4:0]\nALUFN[1:0]\nZ\nV\nN\nALUFN[2:1]\nALUFN[5:4]\nTwo additional control signals (ALUFN[5:4]) have been introduced to select which unit\nwill supply the value for the ALU output. The encodings for ALUFN[5:0] used by the test\njig are shown in the following table:\nZ\nV\nN\nadd\nboole\nshift\ncmp\nALU[31:0]\nOperation\nALUFN[5:0]\nhex\nADD\n0x00\nSUB\n0x01\nAND\n0x18\nOR\n0x1E\nXOR\n0x16\n\"A\" (LDR)\n0x1A\nSHL\n0x20\nSHR\n0x21\nSRA\n0x23\nCMPEQ\n0x33\nCMPLT\n0x35\nCMPLE\n0x37\n6.004 Computation Structures\n- 9 -\nLab #3\n\n(F) When you've completed your design, you can use lab3checkoff_6.jsim to test your\ncircuit. Your netlist should incorporate the following three .include statements\n.include \"/mit/6.004/jsim/nominal.jsim\"\n.include \"/mit/6.004/jsim/stdcell.jsim\"\n.include \"/mit/6.004/jsim/lab3checkoff_6.jsim\"\nand the following subcircuit definition (you can of course define other subcircuits as\nwell)\n.subckt alu ALUFN[5:0] A[31:0] B[31:0] alu[31:0] z v n\n... your ALU circuit here ...\n.ends\nNote that the z, v, and n signals from the adder/subtractor unit are included in the\nterminal list for the alu subcircuit. While these signals are not needed when using the\nALU as part of the Beta, they are included here to make it easier for the test jig to\npinpoint problems with your circuit.\nBefore using lab3checkoff_6.jsim remember to comment out any test circuitry and .tran\nstatements you may have added to your netlist while debugging your circuit. Also\nremember to use JSim's gate-level simulator (\nin the toolbar) to simulate your circuit.\nIf this test jig runs okay, it will offer to check-in your lab with the on-line server.\nOptional Design Problem: Implementing Multiply\nThe goal of this design project is build a combinational multiplier that accepts 32-bit operands\nand produces a 32-bit result. Multiplying two 32-bit numbers produces a 64-bit product; the\nresult we're looking for is the low-order 32-bits of the 64-bit product.\nYour multiplier circuitry should be integrated into the ALU design you completed in the first part\nof this lab. We'll use the following encoding for ALUFN[5:0] when requesting a multiply\noperation by the ALU.\nOperation\nALUFN[5:0]\nhex\nMUL\n0x02\nHere's a detailed bit-level description of how a 4-bit by 4-bit unsigned multiplication works.\nThis diagram assumes we only want the low-order 4 bits of the 8-bit product.\nA3\nA2\nA1\nA0\n(multiplicand)\n*\nB3\nB2\nB1\nB0\n(multiplier)\n---------------------------\nA3*B0 A2*B0 A1*B0 A0*B0\n(partial product)\n+ A2*B1 A1*B1 A0*B1\n+ A1*B2 A0*B2\n+ A0*B3\n--------------------------\nP3\nP2\nP1\nP0\n6.004 Computation Structures\n- 10 -\nLab #3\n\nThis diagram can be extended in a straightforward way to 32-bit by 32-bit multiplication. Note\nthat since we only want the low-order 32-bits of the result, you don't need to include the circuitry\nthat generates the rest of the 64-bit product.\nAs you can see from the diagram above, forming the partial products is easy! Multiplication of\ntwo bits can be implemented using an AND gate. The hard part is adding up all the partial\nproducts (there will be 32 partial products in your circuit). One can use full adders (FAs) hooked\nup in a ripple-carry configuration to add each partial product to the accumulated sum of the\nprevious partial products (see the diagram below). The circuit closely follows the diagram above\nbut omits an FA module if two of its inputs are \"0\".\nThe circuit above works with both unsigned operands and signed two's complement\noperands. This may seem strange - don't we have to worry about the most significant bit\n(MSB) of the operands? With unsigned operands the MSB has a weight of 2MSB\n(assuming the bits are numbered 0 to MSB) but with signed operands the MSB has a\nweight of -2MSB. Doesn't our circuitry need to take that into account?\n6.004 Computation Structures\n- 11 -\nLab #3\n\nIt does, but when we're only saving the lower half of the product, the differences don't\nappear. The multiplicand (A in the figure above) can be either unsigned or two's\ncomplement, the FA circuits will perform correctly in either case. When the multiplier\n(B in the figure above) is signed, we should subtract the final partial product instead of\nadding it. But subtraction is the same as adding the negative, and the negative of a two's\ncomplement number can be computed by taking its complement and adding 1. When we\nwork this through we see that the low-order bit of the partial product is the same whether\npositive or negated. And the low-order bit is all that we need when saving only the lower\nhalf of the product! If we were building a multiplier that computed the full product, we'd\nsee many differences between a multiplier that handles unsigned operands and one that\nhandles two's complement operands, but these differences only affect how the high half\nof the product is computed.\nWe've created a test jig to help debug your multiplier. Your netlist should incorporate the\nfollowing three .include statements\n.include \"/mit/6.004/jsim/nominal.jsim\"\n.include \"/mit/6.004/jsim/stdcell.jsim\"\n.include \"/mit/6.004/jsim/lab3multiply.jsim\"\nand the standard alu subcircuit definition\n.subckt alu ALUFN[5:0] A[31:0] B[31:0] alu[31:0] z v n\n... your ALU with multiplier circuit here ...\n.ends\nThis test jig includes test cases for\nall combinations of (0, 1, -1)*(0,1,-1),\n2i*1 for i = 0, 1, ..., 31\n-1*2i for i = 0, 1, ..., 31\n(3 << i) * 3 for i = 0, 1, ..., 31\nWhen you've completed your design, you can use lab3checkoff_10.jsim to test your improved\nALU implementation. This checkoff file contains all the tests from lab3checkoff_6.jsim plus\nadditional tests to verify that your multiplier circuitry is working correctly. There's also a handy\nset of debugging tests in lab3multiply.jsim which can help track down problems in your design.\nDesign Note: Combinational multipliers implemented as described above are pretty slow! There\nare many design tricks we can use to speed things up - see the appendix on \"Computer\nArithmetic\" in any of the editions of Computer Architecture A Quantitative Approach by John\nHennessy and David Patterson (Morgan Kauffmann publishers).\n6.004 Computation Structures\n- 12 -\nLab #3"
    },
    {
      "category": "Resource",
      "title": "Lab #4",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/d15e04be4136f33e26e35f5884eeb35a_MIT6_004s09_lab04.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.004 Computation Structures\nSpring 2009\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nM A S S A C H U S E T T S I N S T I T U T E O F T E C H N O L O G Y\nDEPARTMENT OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE\n6.004 Computation Structures\nLab #4\nPreparation: the description of Turing Machines in Lecure #10 \"Models of Computation\" will be\nuseful when working on this lab.\nTuring Machine Simulation: TMSim\nThe goal of this lab is write the FSM controller for a Turing Machine (TM) which checks to see if\nthe string of left and right parentheses it finds on its input tape \"balance\".\nThe TM has a doubly-infinite tape with discrete symbol positions (cells) each of which contains\none of a finite set of symbols. The control FSM has one input: the symbol found in the current\ncell . The FSM produces several outputs: the symbol to be written into the current cell and a\nmotion control that determines how the head should move. In our simulation, the tape is\ndisplayed horizontally, so the tape can move left, right, or stay where it is.\nThe operation of the TM is specified by a file containing one or more of the following statements:\n// comment\nC++-style comment: ignore characters starting with the '//' and continuing to the end of\nthe current line.\n/* .... */\nC-style comment: ignore characters between \"/*\" and \"*/\". Note that the ignored\ncharacters may include newlines; this type of comment can be used to comment-out\nmultiple lines of your file.\nsymbols symbol...\nDeclare one or more tape symbols. The symbol \"-\" (dash) is predefined and is used to\nindicate that a tape cell is blank. You have to declare symbols you use in an action\nstatement (see below). A symbol can be any sequence of non-whitespace characters not\nincluding \"/\" or the quote character. If you want to declare a symbol containing\nwhitespace, \"/\" or quote, you must enclose the symbol in quotes. You can have more\nthan one symbols statement in your file.\nstates state...\nDeclare one or more states. There are two predefined states: \"*halt*\" and \"*error*\". The\nTM simulation will stop if either of these states is reached. The \"*error*\" state is useful\nfor indicating that the TM has halted due to an unexpected condition. You can have more\nthan one states statement in your file. The first state specified by the first states\nstatement is the starting state for the TM.\naction state symbol newstate writesymbol motion\nSpecify the action performed by the TM when the current state is state and the current\nsymbol is symbol. First the TM will write writesymbol into the current cell of the tape.\nThen the tape is moved left if \"l\" is specified for the motion, right if \"r\" is specified and\n6.004 Computation Structures\n- 1 -\nLab #4\n\nremain where it is if \"-\" is specified. Finally the current state of the control FSM is\nchanged to newstate and the TM searches for the next applicable action. If newstate is\n\"*halt*\" or \"*error*\", the TM simulation stops. If there is no action specified for the\ncurrent state and current symbol, the TM enters the \"*error*\" state. Note that you have to\ndeclare any symbols or states you use in an action statement - this requirement is helpful\nin catching typos.\ntape name symbol...\nSpecifies the initial configuration of a TM tape, each tape has a name. The various\nnames are displayed as a set of radio buttons at the top of the TM animation - you can\nselect which tape is loaded at reset by clicking on one of the buttons. You can specify\nwhich cell of the tape is to be current cell after reset by enclosing the appropriate symbol\nin square brackets. For example, an initial tape configuration called \"test\" consisting of\nthree non-blank cells with the head positioned over the middle cell is specified by\ntape test 1 [2] 3\nIf no initial head position is specified, the head is positioned over the leftmost symbol on\nthe tape.\nresult name symbol...\nSpecifies the expected head position and contents of the tape after the TM has finished\nprocessing the initial tape configuration called name. This statement is used by the\ncheckoff system to verify that your TM has correctly processed each of the test tapes.\nWhenever the TM enters the \"*halt*\" state, the final tape configuration is checked\nagainst the appropriate result statement if one has been specified and any discrepancies\nwill be reported in the status display at the bottom of the TMSim window.\nresult1 name symbol\nLike result except that only the current symbol is checked against the specified value.\ncheckoff server assignment checksum\nThis information is used by TMSim to contact the on-line checkoff server when you\ninvoke the checkoff tool (click the green checkmark in the toolbar). In order to complete\nthe checkoff, you need to have run your TM on all the supplied test tapes and have each\nof the final configurations match the specified results.\nHere's an example file that defines a control FSM with three states (s1, s2 and s3) and two\npossible tape symbols: \"1\" and \"-\" (recall that the \"-\" symbol is predefined). There is a single\ntape configuration defined called \"test\" which consists of a blank tape. The final tape\nconfiguration is expected to be a tape containing six consecutive \"1\" symbols with the head\npositioned over the second \"1\". Note that there is an action declared for each possible\ncombination of the three states and two tape symbols.\n// 3-state busy beaver Turing Machine example\n// See how many 1's we can write on a blank tape using\n// only a three-state Turing Machine\nstates s1 s2 s3 // list of state names, first is starting state\n6.004 Computation Structures\n- 2 -\nLab #4\n\nsymbols 1\n// list of symbols (- is blank cell)\ntape test -\n// initial tape contents, blank in this case\nresult test 1 [1] 1 1 1 1 // expected result\n// specify transistions: action state symbol state' write move\n//\nstate = the current state of the FSM\n//\nsymbol = the symbol read from the current cell\n//\nstate' = state on the next cycle\n//\nwrite = symbol to be written into the current cell\n//\nmove = tape movement (\"l\"=left, \"r\"=right, \"-\"=stay put)\naction s1 - s2\n1 r\naction s1 1 s3\n1 l\naction s2 - s1\n1 l\naction s2 1 s2\n1 r\naction s3 - s2\n1 l\naction s3 1 *halt* 1 r\nTo run TMSim, login to your Athena account, add the 6.004 locker and type\nathena% tmsim [filename]\nYou can supply an optional filename argument which will be loaded into the FSM editing buffer\n(you can load and save FSM files from within TMSim too). If no argument is supplied, the\nbuffer is initialized with a sample FSM. After TMSim starts, you'll see a window with the FSM\ndisplayed at the bottom and a state/tape animation at the top.\nThe TM display consists of the following parts:\n6.004 Computation Structures\n- 3 -\nLab #4\n\nTape select radio buttons. Select which of the named test tapes to use when initializing the\nTM after reset.\nTape display: Shows the current state and symbol.\nSpeed control. This slider controls the speed of the animation when you press the \"Run\"\nbutton. At the fastest speed, no status updates are performed which leads to a much faster\nsimulation.\n\"Reset\" button. Reset the TM to its initial state and selected tape configuration.\n\"Step\" button. Let the TM progress one state of the FSM.\n\"Run\" button. Like \"Step\" except the TM will continue running the FSM until it reaches\nthe \"*halt*\" or \"*error*\" state, the \"Stop\" button is pressed, or an error is detected.\n\"Stop\" button. Stop the TM. You can proceed by pressing the \"Step\" or \"Run\" button.\n\"All tests\" button. Automates the task of selecting each test tape in turn and clicking the\n\"Run\" button. The automated process will stop if an error is detected.\nAt the bottom of the screen is a state display showing the current cycle count and any\ndiscrepancies detected in the final tape configuration when the TM enters the \"*halt*\" state.\nWell-formed parenthesis string checker\nYour task is to write the control FSM for a TM that determines if the parenthesis string on its\ninput tape is balanced. Your TM should halt with a current symbol of \"1\" if the parens balance,\nor a current symbol of \"0\" if the parens don't balance. The head should be positioned over the\n\"0\" or \"1\" on the tape. Note that there are no constraints on what the rest of the tape contains.\nHere are the test tapes and the expected results. These statements can be found in\n/mit/6.004/lab4header and should be copied into the front of your TM file. You'll need to add\nstatements declaring your states and actions (and possibly more symbols) in order to complete the\nTM definition.\n// Parenthesis matcher Turing Machine\n// test tapes and checkoff information\ncheckoff \"6004.csail.mit.edu/currentsemester/6004assignment.doit\"\n\"Lab #4\" 1103641864 // this should be at the end of the line above\nsymbols ( ) 0 1\ntape\ntest1 (\nresult1 test1 0\ntape\ntest2 )\nresult1 test2 0\ntape\ntest3 ( )\n6.004 Computation Structures\n- 4 -\nLab #4\n\nresult1 test3 1\ntape\ntest4 ) (\nresult1 test4 0\ntape\ntest5 ( ) ( ) ( ( ( ) ) ( ) ) ) ( )\nresult1 test5 0\ntape\ntest6 ( ) ( ( ( ) ( ( ( ) ) ( ) ) )\nresult1 test6 0\ntape\ntest7 ( ) ( ( ) ( ( ( ) ) ( ) ) )\nresult1 test7 1\n// define additional symbols, your states and actions here...\nNote that you're welcome to add your own test tapes while debugging your implementation, but\nyou'll need to comment them out before running the checkoff tests (otherwise the checksum\nmechanism will get confused).\nScoring: The number of points you'll receive is determined by the number of states in your TM\ndefinition:\n4 points: 2 states\n3 points: 3 states\n2 points: 4 states\n1 point: 5 or more states\n6.004 Computation Structures\n- 5 -\nLab #4"
    },
    {
      "category": "Resource",
      "title": "Building the Beta",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/1b95818bb8a4cd883622cb827cf30967_MIT6_004s09_lec14.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n6.004 Computation Structures\nSpring 2009\n\nL14 - Building a Beta 1\n6.004 - Spring 2009\n3/31/09\nBuilding the Beta\nLab #5 due Thursday\nL14 - Building a Beta 2\n6.004 - Spring 2009\n3/31/09\nCPU Design Tradeoffs\nMinimum Cost : measured by the size\nof the circuit.\nBest Performance/Price: measured by the\nratio of MIPS to size. In power-sensitive\napplications MIPS/Watt is important too.\nMaximum Performance: measured by the\nnumbers of instructions executed per\nsecond\nL14 - Building a Beta 3\n6.004 - Spring 2009\n3/31/09\nPerformance Measure\nMIPS =\nClock Frequency (MHz)\nC.P.I.\nMillions of Instructions per Second\nClocks per instruction\nPUSHING PERFORMANCE ...\nTODAY: 1 cycle/inst.\nLATER: more MHz via pipelining\nL14 - Building a Beta 4\n6.004 - Spring 2009\n3/31/09\nThe Beta ISA\nInstruction classes\ndistinguished by\nOPCODE:\nOP\nOPC\nMEM\nTransfer of Control\nOpCode\nOperate class: Reg[Rc] Reg[Ra] op Reg[Rb]\nRa\nRc\nRb\n(UNUSED)\nX\nX\nX\nX\nOperate class: Reg[Rc] Reg[Ra] op SXT(C)\nRa\nRc\nLiteral C (signed)\nX\nX\nX\nX\nOpcodes, both formats:\nADD\nSUB\nMUL*\nDIV*\n*optional\nCMPEQ CMPLE CMPLT\nAND\nOR\nXOR\nSHL\nSHR\nSRA\nLD:\nReg[Rc] Mem[Reg[Ra]+SXT(C)]\nST:\nMem[Reg[Ra]+SXT(C)] Reg[Rc]\nJMP:\nReg[Rc] PC+4; PC Reg[Ra]\nBEQ: Reg[Rc] PC+4; if Reg[Ra]=0 then PC PC+4+4*SXT(C)\nBNE:\nLDR:\nReg[Rc] Mem[PC + 4 + 4*SXT(C)]\nReg[Rc] PC+4; if Reg[Ra]0 then PC PC+4+4*SXT(C)\nRa\nRc\nLiteral C (signed)\nX\nX\nX\nXAdjust stick figure appearance.\nFigure by MIT OpenCourseWare.\n\nL14 - Building a Beta 5\n6.004 - Spring 2009\n3/31/09\nApproach: Incremental Featurism\nEach instruction class can be implemented using a simple component\nrepertoire. We'll try implementing data paths for each class individually,\nand merge them (using MUXes, etc).\nSteps:\n1. Operate instructions\n2. Load & Store Instructions\n3. Jump & Branch instructions\n4. Exceptions\n5. Merge data paths\nOur Bag of Components:\nRegisters\nMuxes\nALU\nA\nB\n\"Black box\" ALU\nData\nMemory\nWD\nA\nRD\nR/W\nRegister\nFile\n(3-port)\nRA1\nRA2\nWA\nWE\nWD\nRD1\nRD2\nInstruction\nMemory\nA\nD\nMemories\nL14 - Building a Beta 6\n6.004 - Spring 2009\n3/31/09\nD\nQ\n1 0\ns\nQ\nD\nEN\nclk\nMulti-Port Register Files\nRegister\nFile\n(3-port)\nRA1\nRA2\nWA\nWE\nWD\nRD1\nRD2\nCLK\nWrite Enable\nWrite Address\nWrite Data\n(independent Read addresses)\n(Independent Read Data)\n2 combinational READ ports*,\n1 clocked WRITE port\n*internal logic ensures Reg[31] reads as 0\n...\ndest\nasel\nbsel\nEN\nEN\nEN\nEN\nclk\nRead\nPort A\nRead\nPort B\nWrite\nPort\nL14 - Building a Beta 7\n6.004 - Spring 2009\n3/31/09\nRegister File Timing\nCLK\nWE\nWA\nWD\nRA\nRD\nA\nReg[A]\nA\nnew Reg[A]\n2 combinational READ ports, 1 clocked WRITE port\nWhat if (say) WA=RA1???\nRD1 reads \"old\" value of Reg[RA1] until next clock edge!\nnew Reg[A]\ntS th\ntPD\ntPD\nL14 - Building a Beta 8\n6.004 - Spring 2009\n3/31/09\nStarting point: ALU Ops\nMeans, to BETA, Reg[R4] Reg[R2] + Reg[R3]\nOpCode\nRb\nRa\n(unused)\nRc\n32-bit (4-byte) ADD instruction:\nFirst, we'll need hardware to:\n- Read next 32-bit instruction\n- DECODE instruction: ADD, SUB, XOR, etc\n- READ operands (Ra, Rb) from Register File;\n- PERFORM indicated operation;\n- WRITE result back into Register File (Rc).\n\nL14 - Building a Beta 9\n6.004 - Spring 2009\n3/31/09\nInstruction Fetch/Decode\nINSTRUCTION\nWORD\nFIELDS\nPC\n+4\nInstruction\nMemory\nA\nD\nControl Logic\nCONTROL SIGNALS\nOPCODE <31:26>\n- use PC as memory address\n- add 4 to PC, load new value at\nend of cycle\n- fetch instruction from memory\no use some instruction fields\ndirectly (register numbers,\n16-bit constant)\no use bits <31:26> to\ngenerate controls\n- Use a counter to FETCH the next instruction:\nPROGRAM COUNTER (PC)\nL14 - Building a Beta 10\n6.004 - Spring 2009\n3/31/09\nALU Op Data Path\nRegister\nFile\nRA1\nRA2\nRD1\nRD2\nWA\nWD\nWE\nRc: <25:21>\nPC\n+4\nInstruction\nMemory\nA\nD\nRb: <15:11>\nRa: <20:16>\nALU\nA\nB\nALUFN\nControl Logic\nWERF\nALUFN\nWERF\nWERF!\nOperate class: Reg[Rc] Reg[Ra] op Reg[Rb]\nRa\nRc\nRb\n(UNUSED)\nX\nX\nX\nX\nL14 - Building a Beta 11\n6.004 - Spring 2009\n3/31/09\nALU Operations (w/constant)\nWA\nRc: <25:21>\nPC\n+4\nInstruction\nMemory\nA\nD\nRb: <15:11>\nRa: <20:16>\nRegister\nFile\nRA1\nRA2\nRD1\nRD2\nALU\nA\nB\nWA\nWD\nWE\nALUFN\nControl Logic\nALUFN\nBSEL\nC: SXT(<15:0>)\nBSEL\nWERF\nWERF\nOperate class: Reg[Rc] Reg[Ra] op SXT(C)\nRa\nRc\nLiteral C (signed)\nX\nX\nX\nX\nL14 - Building a Beta 12\n6.004 - Spring 2009\n3/31/09\nLoad Instruction\nWA\nRc: <25:21>\nPC\n+4\nInstruction\nMemory\nA\nD\nRb: <15:11>\nRa: <20:16>\nRegister\nFile\nRA1\nRA2\nRD1\nRD2\nALU\nA\nB\nWA\nWD\nWE\nALUFN\nControl Logic\nBSEL\nC: SXT(<15:0>)\nData Memory\nRD\nWD\nR/W\nAdr\nWr\nWDSEL\n0 1 2\nBSEL\nWDSEL\nALUFN\nWr\nWERF\nWERF\nLD:\nReg[Rc] Mem[Reg[Ra]+SXT(C)]\nRa\nRc\nLiteral C (signed)\n\nL14 - Building a Beta 13\n6.004 - Spring 2009\n3/31/09\nStore Instruction\nWA\nRc: <25:21>\nPC\n+4\nInstruction\nMemory\nA\nD\nRa: <20:16>\nRegister\nFile\nRA1\nRA2\nRD1\nRD2\nALU\nA\nB\nWA\nWD\nWE\nALUFN\nControl Logic\nBSEL\nC: SXT(<15:0>)\nData Memory\nRD\nWD\nR/W\nAdr\nWr\nWDSEL\n0 1 2\nBSEL\nWDSEL\nALUFN\nWr\nRb: <15:11>\nRA2SEL\nRc: <25:21>\nRA2SEL\nWERF\nWERF\nNo WERF!\nST:\nMem[Reg[Ra]+SXT(C)] Reg[Rc]\nRa\nRc\nLiteral C (signed)\nL14 - Building a Beta 14\n6.004 - Spring 2009\n3/31/09\nJMP Instruction\nWA\nRc: <25:21>\nPC\n+4\nInstruction\nMemory\nA\nD\nRa: <20:16>\nRegister\nFile\nRA1\nRA2\nRD1\nRD2\nALU\nA\nB\nWA\nWD\nWE\nALUFN\nControl Logic\nBSEL\nC: SXT(<15:0>)\nData Memory\nRD\nWD\nR/W\nAdr\nWr\nWDSEL\n0 1 2\nBSEL\nWDSEL\nALUFN\nWr\nRb: <15:11>\nRA2SEL\nRc: <25:21>\nRA2SEL\nJT\nPCSEL\nJT\nPCSEL\nWERF\nWERF\nPC+4\nJMP:\nReg[Rc] PC+4; PC Reg[Ra]\nRa\nRc\nLiteral C (signed)\nL14 - Building a Beta 15\n6.004 - Spring 2009\n3/31/09\nBEQ/BNE Instructions\nData Memory\nRD\nWD\nR/W\nAdr\nWr\nWDSEL\n0 1 2\nWA\nPCSEL\nJT\nPC\n+4\nInstruction\nMemory\nA\nD\nRb: <15:11>\nRA2SEL\nRc: <25:21>\nRa: <20:16>\n+\nRegister\nFile\nRA1\nRA2\nRD1\nRD2\nBSEL\nC: SXT(<15:0>)\nZ\nALU\nA\nB\nPC+4+4*SXT(C)\nWA\nWD\nWE\n4*SXT(<15:0>)\nALUFN\nControl Logic\nZ\nPCSEL\nRA2SEL\nBSEL\nWDSEL\nALUFN\nWr\nRc: <25:21>\nJT\nWERF\nWERF\nPC+4\nBEQ:\nReg[Rc] PC+4; if Reg[Ra]=0 then PC PC+4+4*SXT(C)\nBNE:\nReg[Rc] PC+4; if Reg[Ra]0 then PC PC+4+4*SXT(C)\nRa\nRc\nLiteral C (signed)\nRa\nRc\nLiteral C (signed)\nL14 - Building a Beta 16\n6.004 - Spring 2009\n3/31/09\nLoad Relative Instruction\nHey, WAIT A MINUTE. What's Load Relative good for anyway??? I\nthought\n- Code is \"PURE\", i.e. READ-ONLY; and stored in a \"PROGRAM\" region of\nmemory;\n- Data is READ-WRITE, and stored either\n- On the STACK (local); or\n- In some GLOBAL VARIABLE region; or\n- In a global storage HEAP.\nSo why an instruction designed to load data\nthat's \"near\" the instruction???\nC: X = X * 123456;\nBETA:\nLD(X, r0)\nLDR(c1, r1)\nMUL(r0, r1, r0)\nST(r0, X)\n...\nc1: LONG(123456)\nAddresses & other large constants\nLDR:\nReg[Rc] Mem[PC + 4 + 4*SXT(C)]\nRa\nRc\nLiteral C (signed)\n\nL14 - Building a Beta 17\n6.004 - Spring 2009\n3/31/09\nLDR Instruction\nData Memory\nRD\nWD\nR/W\nAdr\nWr\nWDSEL\n0 1 2\nWA\nPCSEL\nJT\nPC\nIF\n+4\nInstruction\nMemory\nA\nD\nRb: <15:11>\nRA2SEL\nRc: <25:21>\nRa: <20:16>\n+\nRegister\nFile\nRA1\nRA2\nRD1\nRD2\nBSEL\nC:SXT( <15:0>)\nZ\nALU\nA\nB\nWA\nWD\nWE\nALUFN\nControl Logic\nZ\nPCSEL\nRA2SEL\nBSEL\nWDSEL\nALUFN\nWr\nPC+4\nRc: <25:21>\nPC+4+4*SXT(C)\nASEL\nJT\nASEL\nWERF\nWERF\nLDR:\nReg[Rc] Mem[PC + 4 + 4*SXT(C)]\nRa\nRc\nLiteral C (signed)\nL14 - Building a Beta 18\n6.004 - Spring 2009\n3/31/09\nExceptions\nWhat if something BAD happens?\n- Execution of an illegal op-code\n- Reference to non-existent memory\n- Divide by zero\nOr, maybe, just something unanticipated...\n- User hits a key\n- A packet comes in via the network\nGOAL: handle all these cases (and more) in SOFTWARE:\n- Treat each such case as an (implicit) procedure call...\n- Procedure handles problem, returns to interrupted program.\n- TRANSPARENT to interrupted program!\n- Important added capability: handlers for certain errors (illegal op-\ncodes) can extend instruction set using software (Lab 7!).\nL14 - Building a Beta 19\n6.004 - Spring 2009\n3/31/09\nException Processing\nPlan:\n- Interrupt running program\n- Invoke exception handler (like a procedure call)\n- Return to continue execution.\nWe'd like RECOVERABLE INTERRUPTS for\n- Synchronous events, generated by CPU or system\nFAULTS (eg, Illegal Instruction, divide-by-0, illegal mem address)\nTRAPS & system calls (eg, read-a-character)\n- Asynchronous events, generated by I/O\n(eg, key struck, packet received, disk transfer complete)\nKEY: TRANSPARENCY to interrupted program.\n- Most difficult for asynchronous interrupts\nL14 - Building a Beta 20\n6.004 - Spring 2009\n3/31/09\nImplementation...\nHow exceptions work:\n- Don't execute current instruction\n- Instead fake a \"forced\" procedure call\n- save current PC (actually current PC + 4)\n- load PC with exception vector\n- 0x4 for synch. exception, 0x8 for asynch. exceptions\nQuestion: where to save current PC + 4?\n- Our approach: reserve a register (R30, aka XP)\n- Prohibit user programs from using XP. Why?\nLD(R31,A,R0)\nLD(R31,B,R1)\nDIV(R0,R1,R2)\nST(R2,C,R31)\nIllOp:\nPUSH(XP)\nFetch inst. at Mem[Reg[XP]-4]\ncheck for DIV opcode, get reg numbers\nperform operation in SW, fill result reg\nPOP(XP)\nJMP(XP)\nForced by\nhardware\nExample: DIV unimplemented\n\nL14 - Building a Beta 21\n6.004 - Spring 2009\n3/31/09\nExceptions\nPC+4+4*SXT(C)\nASEL\nData Memory\nRD\nWD\nAdr\nR/W\nWDSEL\nWA\nRc: <25:21> 0\nXP\nPC\nJT\n+4\nInstruction\nMemory\nA\nD\nRb: <15:11>\nRa: <20:16>\nRA2SEL\nRc: <25:21>\n+\nRegister\nFile\nRA1\nRA2\nRD1\nRD2\nBSEL\nC: SXT(<15:0>)\nZ\nALU\nA\nB\nJT\nWA\nWD\nWE\nALUFN\nControl Logic\nZ\nASEL\nBSEL\nPCSEL\nRA2SEL\nWDSEL\nALUFN\nWr\nPC+4\nWr\nXAdr\nILL\nOP\nWASEL\nWASEL\nIRQ\nWERF\nWERF\nOther: Reg[XP] PC+4; PC \"Xadr\"\nBad Opcode: Reg[XP] PC+4; PC \"IllOp\"\nPCSEL\nL14 - Building a Beta 22\n6.004 - Spring 2009\n3/31/09\nControl Logic\nImplementation choices:\n- ROM indexed by opcode, external branch & trap logic\n- PLA\n- \"random\" logic (eg, standard cell gates)\nOP\nOPC\nLD\nST\nJMP\nBEQ\nBNE\nLDR\nIllop\nIRQ\nALUFN\nF(op)\nF(op)\n\"+\" \"+\"\n--\n--\n--\n\"A\"\n--\n--\nWERF\nBSEL\n--\n--\n--\n--\n--\n--\nWDSEL\n--\nWR\nRA2SEL\n--\n--\n--\n--\n--\n--\n--\n--\nPCSEL\nZ ? 1 : 0\nZ ? 0 : 1\nASEL\n--\n--\n--\n--\n--\nWASEL\n--\nL14 - Building a Beta 23\n6.004 - Spring 2009\n3/31/09\nBeta: Our \"Final Answer\"\nPC+4+4*SXT(C)\nASEL\nData Memory\nRD\nWD\nAdr\nR/W\nWDSEL\nWA\nRc: <25:21> 0\nXP\nPC\nJT\n+4\nInstruction\nMemory\nA\nD\nRb: <15:11>\nRa: <20:16>\nRA2SEL\nRc: <25:21>\n+\nRegister\nFile\nRA1\nRA2\nRD1\nRD2\nBSEL\nC: SXT(<15:0>)\nZ\nALU\nA\nB\nJT\nWA\nWD\nWE\nALUFN\nControl Logic\nZ\nASEL\nBSEL\nPCSEL\nRA2SEL\nWDSEL\nALUFN\nWr\nPC+4\nWr\nXAdr\nILL\nOP\nWASEL\nWASEL\nIRQ\nWERF\nWERF\nPCSEL\nL14 - Building a Beta 24\n6.004 - Spring 2009\n3/31/09\nNext Time: Tackling the Memory Bottleneck\nIs that all\nthere is to\nbuilding a\nprocessor???\nNo.\nYou've gotta print\nup all those little\n\"Beta Inside\"\nstickers."
    },
    {
      "category": "Resource",
      "title": "Cache Issues",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/3117d0b5d39f621be788dfe814e905d9_MIT6_004s09_lec16.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n6.004 Computation Structures\nSpring 2009\n\nL16 - Cache Issues 1\n6.004 - Spring 2009\n4/7/09\nCache Issues\nCache miss\nWhy the book? Lecture\nnotes are faster!\nmodified 4/6/09 14:12\nQuiz #3 Friday!\nL16 - Cache Issues 2\n6.004 - Spring 2009\n4/7/09\nBasic Cache Algorithm\nON REFERENCE TO Mem[X]:\nLook for X among cache tags...\nHIT: X = TAG(i) , for some cache line i\n-\nREAD:\nreturn DATA(i)\n-\nWRITE:\nchange DATA(i); Start Write to Mem(X)\nMISS: X not found in TAG of any cache line\n-\nREPLACEMENT SELECTION:\nSelect some line k to hold Mem[X] (Allocation)\n-\nREAD:\nRead Mem[X]\nSet TAG(k)=X, DATA(K)=Mem[X]\n-\nWRITE:\nStart Write to Mem(X)\nSet TAG(k)=X, DATA(K)= new Mem[X]\nMAIN\nMEMORY\nCPU\n(1!)\nTag\nData\nA\nB\nMem[A]\nMem[B]\nL16 - Cache Issues 3\n6.004 - Spring 2009\n4/7/09\nCache Design Issues\nAssociativity - a basic tradeoff between\n- Parallel Searching (expensive) vs\n- Constraints on which addresses can be stored where\nReplacement Strategy:\n- OK, we've missed. Gotta add this new address/value pair to the\ncache. What do we kick out?\nLeast Recently Used: discard the one we haven't used the longest.\nPlausible alternatives, (e.g. random replacement.\nBlock Size:\n- Amortizing cost of tag over multiple words of data\nWrite Strategy:\n- When do we write cache contents to main memory?\nL16 - Cache Issues 4\n6.004 - Spring 2009\n4/7/09\nAssociativity\n...\nFully Associative\n- expensive!\n- flexible: any\naddress can be\ncached in any line\nLots...\n... or NONE!\nDirect Mapped\n- cheap (ordinary SRAM)\n- contention: addresses\ncompete for cache lines\n\nL16 - Cache Issues 5\n6.004 - Spring 2009\n4/7/09\nLoop B:\nPgm at\n1024,\ndata\nat\n2048:\n... but not here!\nLoop A:\nPgm at\n1024,\ndata\nat 37:\nWorks\nGREAT\nhere...\nDirect-Mapped Cache Contention\nAssume 1024-line direct-\nmapped cache, 1 word/line.\nConsider tight loop, at\nsteady state:\n(assume WORD, not BYTE,\naddressing)\nMemory\nAddress\n...\n...\nCache\nLine\nHit/\nMiss\nHIT\nHIT\nHIT\nHIT\nHIT\nHIT\nHIT\nMISS\nMISS\nMISS\nMISS\nMISS\nMISS\nMISS\nWe need some associativity,\nBut not full associativity...\nL16 - Cache Issues 6\n6.004 - Spring 2009\n4/7/09\nFully-assoc. vs. Direct-mapped\nFully-associative N-line cache:\n- N tag comparators, registers used\nfor tag/data storage ($$$)\n- Location A might be cached in any one\nof the N cache lines; no restrictions!\n- Replacement strategy (e.g., LRU)\nused to pick which line to use when\nloading new word(s) into cache\n-PROBLEM: Cost!\nDirect-mapped N-line cache:\n- 1 tag comparator, SRAM used for\ntag/data storage ($)\n- Location A is cached in a specific line\nof the cache determined by its\naddress; address \"collisions\" possible\n- Replacement strategy not needed:\neach word can only be cached in one\nspecific cache line\n-PROBLEM: Contention!\nL16 - Cache Issues 7\n6.004 - Spring 2009\n4/7/09\nCost vs Contention\ntwo observations...\n1. Probability of collision diminishes with cache size...\n... so lets build HUGE direct-mapped caches, using cheap SRAM!\n2. Contention mostly occurs between\nindependent \"hot spots\" --\n- Instruction fetches vs stack frame vs data\nstructures, etc\n- Ability to simultaneously cache a few (2? 4? 8?)\nhot spots eliminates most collisions\n... so lets build caches that allow each location to be\nstored in some restricted set of cache lines,\nrather than in exactly one (direct mapped) or\nevery line (fully associative).\nInsight: an N-way set-associative cache affords modest parallelism\n- parallel lookup (associativity): restricted to small set of N lines\n- modest parallelism deals with most contention at modest cost\n- can implement using N direct-mapped caches, running in parallel\nL16 - Cache Issues 8\n6.004 - Spring 2009\n4/7/09\nN-way Set-Associative Cache\nk\nHIT\nDATA TO CPU\nINCOMING ADDRESS\n=?\n=?\n=?\nt\nMEM DATA\nN direct-mapped caches, each with 2t lines\nSimultaneously addressed line in each subcache constitutes a set\n\nL16 - Cache Issues 9\n6.004 - Spring 2009\n4/7/09\nAssociativity\nthe birds-eye view\nCan place caches in 2D space:\n- Total lines = # Sets * Set Size\n- # Sets = 1: Fully Associative\n- Set Size = 1: Direct Mapped\n- Set Size = N: N-way Set Associative\n# Sets\nSet Size\n\n\"set\"\nL16 - Cache Issues 10\n6.004 - Spring 2009\n4/7/09\nISSUE: Replacement Strategy\nAssociativity implies choices...\naddress\nFully associative\n- compare addr with each\ntag simultaneously\n- location A can be\nstored in any cache line\naddress\nDirect-mapped\n- compare addr with\nonly one tag\n- location A can be\nstored in exactly one\ncache line\nN\naddress\nN-way set associative\n- compare addr with N tags\nsimultaneously\n- location A can be stored in\nexactly one set, but in any\nof the N cache lines\nbelonging to that set\nL16 - Cache Issues 11\n6.004 - Spring 2009\n4/7/09\nReplacement Strategy\nLRU (Least-recently used)\n- keeps most-recently used locations in cache\n- need to keep ordered list of N items N! orderings\nO(log2N!) = O(N log2N) \"LRU bits\" + complex logic\n(0,1,2,3) Hit 2 -> (2,0,1,3)\n(2,0,1,3) Hit 1 -> (1,2,0,3)\n(1,2,0,3) Miss -> (3,1,2,0)\n(3,1,2,0) Hit 3 -> (3,1,2,0)\nOverhead is\nO(N log2N)\nbits/set\nOverhead is\nO(log2N)\nbits/set\nOverhead is\nO(log2N)\nbits/cache!\nFIFO/LRR (first-in, first-out/least-recently replaced)\n- cheap alternative: replace oldest item (dated by access time)\n- within each set: keep one counter that points to victim line\nRandom (select replacement line using random, uniform distribution)\n- no \"pathological\" reference streams causing wost-case results\n- use pseudo-random generator to get reproducible behavior;\n- use real randomness to prevent reverse engineering!\nL16 - Cache Issues 12\n6.004 - Spring 2009\n4/7/09\nCache Benchmarking\nSuppose this loop is entered with R3=4000:\nADR: Instruction\nI\nD\n400: LD(R3,0,R0) 400 4000+...\n404: ADDC(R3,4,R3) 404\n408: BNE(R0,400) 408\nGOAL: Given some cache design, simulate (by hand or machine) execution\nwell enough to determine hit ratio.\n1. Observe that the sequence of memory locations referenced is\n400, 4000, 404, 408, 400, 4004, ...\nWe can use this simpler reference string, rather than the program, to\nsimulate cache behavior.\n2. We can make our life easier in many cases by converting to word\naddresses: 100, 1000, 101, 102, 100, 1001, ...\n(Word Addr = (Byte Addr)/4)\n\nL16 - Cache Issues 13\n6.004 - Spring 2009\n4/7/09\nCache Simulation\nAddr Line# Miss?\n100 0 M\n1000 1 M\n101 2 M\n102 3 M\n100 0\n1001 1 M\n101 2\n102 3\n100 0\n1002 1 M\n101 2\n102 3\n100 0\n1003 1 M\n101 2\n102 3\n4-line Fully-associative/LRU\n1/4 miss\nAddr Line# Miss?\n100 0 M\n1000 0 M\n101 1 M\n102 2 M\n100 0 M\n1001 1 M\n101 1 M\n102 2\n100 0\n1002 2 M\n101 1\n102 2 M\n100 0\n1003 3 M\n101 1\n102 2\n4-line Direct-mapped\n7/16 miss\nCompulsory\nMisses\nCollision\nCollision\nmiss\nL16 - Cache Issues 14\n6.004 - Spring 2009\n4/7/09\nAssociativity: Full vs 2-way\nAddr Line# Miss?\n100 0 M\n1000 1 M\n101 2 M\n102 3 M\n100 0\n1001 4 M\n101 2\n102 3\n100 0\n1002 5 M\n101 2\n102 3\n100 0\n1003 6 M\n101 2\n102 3\n8-line Fully-associative, LRU\n1/4 miss\nAddr Set#/N Miss?\n100 0,0 M\n1000 0,1 M\n101 1,0 M\n102 2,0 M\n100 0,0\n1001 1,1 M\n101 1,0\n102 2,0\n100 0,0\n1002 2,1 M\n101 1,0\n102 2,0\n100 0,0\n1003 3,1 M\n101 1,0\n102 2,0\n2-way, 8-line total, LRU\n1/4 miss\nL16 - Cache Issues 15\n6.004 - Spring 2009\n4/7/09\n1k\n2k\n4k\n8k\n16k\n32k\n64k\n128k\n1-way\n2-way\n4-way\n8-way\nfully assoc.\nAssociativity vs. miss rate\nMiss rate\n(%)\nCache size (bytes)\nAssociativity\nH&P: Figure 5.9\n- 8-way is (almost) as effective as fully-associative\n- rule of thumb: N-line direct-mapped == N/2-line 2-way set assoc.\n(direct-mapped)\nL16 - Cache Issues 16\n6.004 - Spring 2009\n4/7/09\nDevil's Advocacy Games\nYour company uses the\ncheaper FIFO cache,\nthe competition uses\nLRU. Can you devise\na benchmark to make\nyour cache look\nbetter?\nAssume 0x100 sets,\n2-way...\n2-way, LRU\n2-way, FIFO\nSet 0 tags:\nAdr\nSet, #\nH/M\nSet, #\nH/M\n0, 0\nM\n0, 0\nM\n0, 1\nM\n0, 1\nM\n0, 0\nH\n0, 0\nH\n0, 1\nM\n0, 0\nM\n0, 0\nM\n0, 0\nH\n#0\n#1\n#0\n#1\nA carefully-designed benchmark can make either look better...\nPessimal case: next adr referenced is the one just replaced!\nRandom replacement makes this game harder...\nBINGO!\n\nL16 - Cache Issues 17\n6.004 - Spring 2009\n4/7/09\nIncreasing Block Size\nMore Data/Tag\nA31:4\nMem[A]\nMem[A+4]\nMem[A+8] Mem[A+12]\n= ?\n[3:2]\n[31:4]\nADDR\nDATA\nHIT\n-blocks of 2B words, on 2B word boundaries\n- always read/write 2B word block from/to memory\n- locality: access on word in block, others likely\n- cost: some fetches of unaccessed words\nBIG WIN if there is a wide path to memory\nEnlarge each line in cache:\nTAG\nD0\nD1\nD2\nD3\n4 x 32 = 128 bits\n28 bits\nOverhead < 1⁄4 bit of\nTag per bit of data\nL16 - Cache Issues 18\n6.004 - Spring 2009\n4/7/09\n4-word block, DM Cache\n= ?\n[3:2]\n[31:8]\nADDR\nDATA\nHIT\nTAG\nD0\nD1\nD2\nD3\n[7:4]\nUse ordinary\n(fast) static\nRAM for tag and\ndata storage\nOnly one comparator for entire cache!\n16 cache lines\n4 bit index\n0x12\nM[0x1230]\nM[0x1234]\nM[0x1238]\nM[0x123C]\n0x12\nM[0x1240]\nM[0x1244]\nM[0x1248]\nM[0x124C]\n24-bit Tag!\nL16 - Cache Issues 19\n6.004 - Spring 2009\n4/7/09\nValid bits\nV\nMAIN\nMEMORY\nCPU\nA\nMem[A]\nB\nMem[B]\nTAG\nDATA\nProblem:\n- Ignoring cache lines that don't contain anything of value... e.g., on\nstart-up\n\"Back door\" changes to memory (eg loading program from disk)\nSolution:\n- Extend each TAG with VALID bit.\n- Valid bit must be set for cache line to HIT.\n- At power-up / reset : clear all valid bits\n- Set valid bit when cache line is first replaced.\n- Cache Control Feature: Flush cache by clearing all valid bits, Under\nprogram/external control.\nL16 - Cache Issues 20\n6.004 - Spring 2009\n4/7/09\nHandling of WRITES\nObservation: Most (90+%) of memory accesses are READs. How should we\nhandle writes? Issues:\nWrite-through: CPU writes are cached, but also written to main memory\n(stalling the CPU until write is completed). Memory always holds \"the\ntruth\".\nWrite-behind: CPU writes are cached; writes to main memory may be\nbuffered, perhaps pipelined. CPU keeps executing while writes are\ncompleted (in order) in the background.\nWrite-back: CPU writes are cached, but not immediately written to main\nmemory. Memory contents can be \"stale\".\nOur cache thus far uses write-through.\nCan we improve write performance?\n\nL16 - Cache Issues 21\n6.004 - Spring 2009\n4/7/09\nWrite-through\nON REFERENCE TO Mem[X]: Look for X among tags...\nHIT: X == TAG(i) , for some cache line i\n-READ: return DATA[I]\n-WRITE: change DATA[I]; Start Write to Mem[X]\nMISS: X not found in TAG of any cache line\n-REPLACEMENT SELECTION:\nSelect some line k to hold Mem[X]\n-READ: Read Mem[X]\nSet TAG[k] = X, DATA[k] = Mem[X]\n-WRITE: Start Write to Mem[X]\nSet TAG[k] = X, DATA[k] = new Mem[X]\nL16 - Cache Issues 22\n6.004 - Spring 2009\n4/7/09\nWrite-back\nON REFERENCE TO Mem[X]: Look for X among tags...\nHIT: X = TAG(i) , for some cache line I\n-READ: return DATA(i)\n-WRITE: change DATA(i); Start Write to Mem[X]\nMISS: X not found in TAG of any cache line\n-REPLACEMENT SELECTION:\nSelect some line k to hold Mem[X]\nWrite Back: Write Data(k) to Mem[Tag[k]]\n-READ: Read Mem[X]\nSet TAG[k] = X, DATA[k] = Mem[X]\n-WRITE: Start Write to Mem[X]\nSet TAG[k] = X, DATA[k] = new Mem[X]\nIs write-back worth the trouble? Depends on (1) cost of write; (2) consistency issues.\nL16 - Cache Issues 23\n6.004 - Spring 2009\n4/7/09\nWrite-back w/ \"Dirty\" bits\nON REFERENCE TO Mem[X]: Look for X among tags...\nHIT: X = TAG(i) , for some cache line I\n-READ: return DATA(i)\n-WRITE: change DATA(i); Start Write to Mem[X] D[i]=1\nMISS: X not found in TAG of any cache line\n-REPLACEMENT SELECTION:\nSelect some line k to hold Mem[X]\nIf D[k] == 1 (Write Back) Write Data(k) to Mem[Tag[k]]\n-READ: Read Mem[X]; Set TAG[k] = X, DATA[k] = Mem[X], D[k]=0\n-WRITE: Start Write to Mem[X] D[k]=1\nSet TAG[k] = X, DATA[k] = new Mem[X]\nMAIN\nMEMORY\nCPU\nA\nMem[A]\nB\nMem[B]\nTAG\nDATA\nV\nD\nL16 - Cache Issues 24\n6.004 - Spring 2009\n4/7/09\nCaches: Summary\nAssociativity:\n- Less important as size increases\n- 2-way or 4-way usually plenty for typical program clustering; BUT additional\nassociativity\n- Smooths performance curve\n- Reduces number of select bits (we'll see shortly how this helps)\n- TREND: Invest in RAM, not comparators.\nReplacement Strategy:\n- BIG caches: any sane approach works well\n- REAL randomness assuages paranoia!\nPerformance analysis:\n- Tedious hand synthesis may build intuition from simple examples, BUT\n- Computer simulation of cache behavior on REAL programs (or using REAL trace\ndata) is the basis for most real-world cache design decisions."
    },
    {
      "category": "Resource",
      "title": "CMOS Technology",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/e47305d1da75074fee16dd6501499f3e_MIT6_004s09_lec03.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n6.004 Computation Structures\nSpring 2009\n\nL03 - CMOS Technology 1\n6.004 - Spring 2009\n2/10/09\nCMOS Technology\n1. Qualitative MOSFET model\n2. CMOS logic gates\n3. CMOS design issues\npoly\nmetal\npdiff\nndiff\nmodified 2/9/09 15:07\nNEXT WEEK:\n- TUE: no lecture\n- THU: Lab 1 due!\n- FRI: QUIZ 1!!!\nL03 - CMOS Technology 2\n6.004 - Spring 2009\n2/10/09\nCombinational Device Wish List\nDesign our system to tolerate\nsome amount of error\n\nAdd positive noise margins\n\nVTC: gain>1 & nonlinearity\nLots of gain\nbig noise margin\nCheap, small\nChanging voltages will require\nus to dissipate power, but if no\nvoltages are changing, we'd like\nzero power dissipation\nWant to build devices with\nuseful functionality (what sort\nof operations do we want to\nperform?)\nVOL\nVIL\nVIH\nVOH\nVin\nVout\nVin\nVout\nL03 - CMOS Technology 3\n6.004 - Spring 2009\n2/10/09\nW\nL\nMOSFETS: Gain & non-linearity\ngate\ndrain\nsource\nbulk\nInter-layer SiO2 insulation\nPolysilicon wire\nDoped (p-type or n-type) silicon substrate\nVery thin (<20A) high-quality SiO2\ninsulating layer isolates gate from channel\nregion.\nHeavily doped (n-type or p-type) diffusions\nChannel region: electric field from charges on\ngate locally \"inverts\" type of substrate to create\na conducting channel between source and drain.\nMOSFETs (metal-oxide-semiconductor field-effect transistors) are four-\nterminal voltage-controlled switches. Current flows between the\ndiffusion terminals if the voltage on the gate terminal is large enough to\ncreate a conducting \"channel\", otherwise the mosfet is off and the\ndiffusion terminals are not connected.\nIDS W/L\nL03 - CMOS Technology 4\n6.004 - Spring 2009\n2/10/09\nFETs as switches\nCONDUCTION:\nIf a channel exists, a\nhorizontal field will\ncause a drift current\nfrom the drain to the\nsource.\nEh\ngate\nINVERSION:\nA sufficiently strong vertical field will attract enough\nelectrons to the surface to create a conducting n-\ntype channel between the source and drain. The gate\nvoltage when the channel first forms is called the\nthreshold voltage -- the mosfet switch goes from \"off\"\nto \"on\".\nEv\ninversion\nhappens here\nThe four terminals of a Field Effect Transistor (gate, source, drain and bulk)\nconnect to conductors that generate a complicated set of electric fields in the\nchannel region which depend on the relative voltages of each terminal.\np\nn\nn\nsource\ndrain\nbulk\nDepletion region\n(no carriers)\nforms at PN\njunction. Self\ninsulating!\n\nL03 - CMOS Technology 5\n6.004 - Spring 2009\n2/10/09\nFETs come in two flavors\nThe use of both NFETs and PFETs - complimentary transistor types - is a key\nto CMOS (complementary MOS) logic families.\np\np\nn\nG D\nS\nB\nG\nS\nD\nB\nG\nS\nD\nB\nConnect B to\nGND to keep PN\nreverse-biased\n(Vp < Vn); keeps D\nand S insulated\nfrom B\nConnect B to\nVDD to keep\nPN reverse-\nbiased\nn\nn\np\nD\nS\nG\nB\nNFET: n-type source/drain\ndiffusions in a p-type substrate.\nPositive threshold voltage;\ninversion forms n-type channel\nPFET: p-type source/drain\ndiffusions in a n-type substrate.\nNegative threshold voltage;\ninversion forms p-type channel.\nL03 - CMOS Technology 6\n6.004 - Spring 2009\n2/10/09\nCMOS Recipe\nD\nG\nS\nS\nD\nS\nD\n\"\n\"\nNFET Operating regions:\n\"off\":\nVG < VTH,NFET\n\"on\":\nVG > VTH,NFET\nS\nG\nD\nPFET Operating regions:\n\"off\":\nVG > VDD + VTH,PFET\n\"on\":\nVG < VDD + VTH,PFET\nS\nD\nS\nD\n\"\n\"\nIf we follow two rules when constructing CMOS circuits then we can model the\nbehavior of the mosfets as simple switches:\nRule #1: only use NFETs in pulldown circuits (paths from output node to GND)\nRule #2: only use PFETs in pullup circuits (paths from output node to VDD)\n~VDD/5\n~ -VTH,NFET\nL03 - CMOS Technology 7\n6.004 - Spring 2009\n2/10/09\nVOL\nVIL\nVIH\nVOH\nCMOS Inverter VTC\nVin\nVout\nIpu\nIpd\nSteady state reached\nwhen Vout reaches value\nwhere Ipu = Ipd.\nWhen VIN is low, the\nnfet is off and the pfet\nis on, so current flows\ninto the output node\nand VOUT eventually\nreaches VDD (= VOH) at\nwhich point no more\ncurrent will flow.\npfet \"on\"\nnfet \"off\"\nWhen VIN is high, the\npfet is off and the nfet\nis on, so current flows\nout of the output node\nand VOUT eventually\nreaches GND (= VOL) at\nwhich point no more\ncurrent will flow.\npfet \"off\"\nnfet \"on\"\nWhen VIN is in the middle, both the pfet and nfet are \"on\" and the shape of the VTC\ndepends on the details of the devices' characteristics. CMOS gates have very high gain\nin this region (small changes in VIN produce large changes in VOUT) and the VTC is almost a\nstep function.\nL03 - CMOS Technology 8\n6.004 - Spring 2009\n2/10/09\nBeyond Inverters:\nComplementary pullups and pulldowns\nWe want complementary pullup and pulldown logic, i.e., the\npulldown should be \"on\" when the pullup is \"off\" and vice\nversa.\npullup\npulldown\nF(A1,...,An)\non\noff\ndriven \"1\"\noff\non\ndriven \"0\"\non\non\ndriven \"X\"\noff\noff\nno connection\nNow you know what the \"C\"\nin CMOS stands for!\nSince there's plenty of capacitance on the output node, when the output\nbecomes disconnected it \"remembers\" its previous voltage -- at least\nfor a while. The \"memory\" is the load capacitor's charge. Leakage\ncurrents will cause eventual decay of the charge (that's why DRAMs\nneed to be refreshed!).\n\nL03 - CMOS Technology 9\n6.004 - Spring 2009\n2/10/09\nCMOS complements\nWhat a nice\nVOH you have...\nThanks. It runs\nin the family...\nconducts when VGS is high\nconducts when VGS is low\nconducts when A is high\nand B is high: A.B\nA\nB\nA\nB\nconducts when A is low\nor B is low: A+B = A.B\nconducts when A is high\nor B is high: A+B\nA\nB\nA\nB\nconducts when A is low\nand B is low: A.B = A+B\nL03 - CMOS Technology 10\n6.004 - Spring 2009\n2/10/09\nA pop quiz!\nA\nB\nWhat function does\nthis gate compute?\nA B C\n0 0\n0 1\n1 0\n1 1\n1 NAND\nCurrent technology: = 45nm\nCOST:\n- $3500 per 300mm wafer\n- 300mm round wafer = (150e-3)2= .07m2\n-NAND gate = (82)(16)(45e-9)2=2.66e-12m2\n-2.6e10 NAND gates/wafer (= 100 billion FETS!)\n- marginal cost of NAND gate: 132n$\nL03 - CMOS Technology 11\n6.004 - Spring 2009\n2/10/09\nHere's another...\nWhat function does\nthis gate compute?\nA B C\n0 0\n0 1\n1 0\n1 1\nA\nB\n0 NOR\nL03 - CMOS Technology 12\n6.004 - Spring 2009\n2/10/09\nGeneral CMOS gate recipe\nStep 1. Figure out pulldown network that\ndoes what you want, e.g.,\n(What combination of inputs\ngenerates a low output)\nA\nB\nC\nStep 2. Walk the hierarchy replacing nfets\nwith pfets, series subnets with parallel\nsubnets, and parallel subnets with series\nsubnets\nA\nB\nC\nSo, whats the big\ndeal?\nStep 3. Combine pfet pullup network\nfrom Step 2 with nfet pulldown\nnetwork from Step 1 to form fully-\ncomplementary CMOS gate.\nA\nB\nC\nA\nB\nC\nF = A-(B + C)\n\nL03 - CMOS Technology 13\n6.004 - Spring 2009\n2/10/09\nA Quick Review\n-\nA combinational device is a circuit element that has\n- one or more digital inputs\n- one or more digital outputs\n- a functional specification that details the value of each output for\nevery possible combination of valid input values\n- a timing specification consisting (at minimum) of an upper bound\ntPD on the required time for the device to compute the specified\noutput values from an arbitrary set of stable, valid input values\nStatic\ndiscipline\nIf C is 1 then copy A to Y,\notherwise copy B to Y\nI will generate a valid\noutput in no more than\n2 weeks after\nseeing valid inputs\ninput A\ninput B\ninput C\noutput Y\nL03 - CMOS Technology 14\n6.004 - Spring 2009\n2/10/09\nBig Issue 1: Wires\nToday (i.e., 100nm):\nRC ≈ 50ps/mm\nImplies > 1 ns to traverse a 20mm x 20mm chip\nThis is a long time in a 2GHz processor\nVIN\nR\nVout\nVIN\nC\nL03 - CMOS Technology 15\n6.004 - Spring 2009\n2/10/09\nDue to unavoidable delays...\nPropagation delay (tPD):\nAn UPPER BOUND on the delay from valid inputs\nto valid outputs.\nGOAL:\nminimize\npropagation\ndelay!\nISSUE:\nkeep\nCapacitances\nlow and\ntransistors\nfast\nVOUT\n< tPD\n< tPD\nVIN\nVOL\nVOH\nVIL\nVIH\ntime constant\n= RPD-CL\ntime constant\n= RPU-CL\nL03 - CMOS Technology 16\n6.004 - Spring 2009\n2/10/09\nContamination Delay\nan optional, additional timing spec\nINVALID inputs take time to propagate, too...\nCONTAMINATION DELAY, tCD\nA LOWER BOUND on the delay from any invalid input to an invalid output\nVOUT\n> tCD\n> tCD\nVIN\nVOL\nVOH\nVIL\nVIH\nDo we really need tCD?\nUsually not... it'll be\nimportant when we\ndesign circuits with\nregisters (coming\nsoon!)\nIf tCD is not specified,\nsafe to assume it's 0.\n\nL03 - CMOS Technology 17\n6.004 - Spring 2009\n2/10/09\nThe Combinational Contract\nA\nB\nA B\n0 1\n1 0\ntPD propagation delay\ntCDcontamination delay\nA\nB\nMust be ___________\nMust be ___________\nNote:\n1. No Promises during\n2. Default (conservative) spec: tCD = 0\n< tPD\n> tCD\nL03 - CMOS Technology 18\n6.004 - Spring 2009\n2/10/09\nAcyclic Combinational Circuits\nIf NAND gates have a tPD = 4nS and tCD = 1nS\nB\nC\nA\nY\ntPD = _______ nS\ntCD = _______ nS\ntPD is the maximum cumulative\npropagation delay over all paths\nfrom inputs to outputs\ntCD is the minimum cumulative\ncontamination delay over all\npaths from inputs to outputs\nL03 - CMOS Technology 19\n6.004 - Spring 2009\n2/10/09\nOh yeah... one last issue\nRecall the rules for combinational devices:\nOutput guaranteed to be valid when all inputs have been\nvalid for at least tPD, and, outputs may become invalid no\nearlier than tCD after an input changes!\nA\nB\nZ\ntPD\ntCD\nA\nZ\nB\nA\nB\nZ\nNOR:\nA\nB\nZ\ntPD\ntCD\nMany gate implementations--e.g., CMOS--\nadhere to even tighter restrictions.\nL03 - CMOS Technology 20\n6.004 - Spring 2009\n2/10/09\nWhat happens in this case?\nA\nB\nZ\ntPD\ntCD\nInput A alone is\nsufficient to\ndetermine the\noutput\nA\nB\nZ\nA\nB\nZ\nX\nX\nA B\nZ\nA\nB\nZ\nNOR:\nLenient\nNOR:\nLENIENT Combinational Device:\nOutput guaranteed to be valid when any combination of inputs\nsufficient to determine output value has been valid for at least tPD.\nTolerates transitions -- and invalid levels -- on irrelevant inputs!\nCMOS NOR:\n\nL03 - CMOS Technology 21\n6.004 - Spring 2009\n2/10/09\nBig Issue 2: Power\nEnergy dissipated = C VDD\n2 per cycle\nPower consumed = f n C VDD\n2 per chip\n\nwhere\nf = frequency of charge/discharge\n\nn = number of gates /chip\nVIN\nVDD\nC\nVOUT\nVIN moves from\nL to H to L\nVOUT moves from\nH to L to H\nC discharges and\nthen recharges\nL03 - CMOS Technology 22\n6.004 - Spring 2009\n2/10/09\nUnfortunately...\nModern chips (UltraSparc III, Power4, Itanium 2)\ndissipate from 80W to 150W with a Vdd ≈ 1.2V\n(Power supply current is ≈ 100 Amps)\nHey: could we\nSomehow recycle\nthe charge?\nWorse yet...\n-Little room left to reduce Vdd\n-nC and f continue to grow\nCooling challenge is like making the filament of a\n100W incandescent lamp cool to the touch!\nL03 - CMOS Technology 23\n6.004 - Spring 2009\n2/10/09\nMUST computation consume energy?\n(a tiny digression...)\nA B C\n0 0\n0 1\n1 0\n1 1\nHow energy-efficient can we make a gate? It\nseems that switching the input to a NAND gate\nwill always dissipate some energy...\nLandauer's Principle (1961): discarding\ninformation is what costs energy!\nBennett (1973): Use reversible logic gates, not NAND, and there's no lower\nbound to energy use!\nNAND GATE:\n2 bits 1 bit\n(information\nLoss!)\nA B\n0 0\n0 1\n1 0\n1 1\nP Q\nFEYNMAN\nGATE:\n2 bits 2 bits\n(information\nPreserving!)\nBennett, Fredkin, Feynman, others: Computer\nsystems constructed from info-\npreserving elements.\nTheory: NO lower bound on energy use!\nPractice: Research frontier (qubits, etc.)\nhttp://www.research.ibm.com/journal/rd/441/landauerii.pdf\nThe fundamental physical limits of computation, Bennett & Landauer, Scientific American. Vol. 253, pp. 48-56. July 1985\nhttp://www.research.ibm.com/journal/rd/176/ibmrd1706G.pdf\nL03 - CMOS Technology 24\n6.004 - Spring 2009\n2/10/09\nSummary\n- CMOS\n- Only use NFETs in pulldowns, PFETs in pullups mosfets behave as\nvoltage-controlled switches\n- Series/parallel Pullup and pulldown switch circuits are\ncomplementary\n- CMOS gates are naturally inverting (rising input transition can only\ncause falling output transition, and vice versa).\n- \"Perfect\" VTC (high gain, VOH = VDD, VOL = GND) means large noise\nmargins and no static power dissipation.\n- Timing specs\n- tPD: upper bound on time from valid inputs to valid outputs\n- tCD: lower bound on time from invalid inputs to invalid outputs\n- If not specified, assume tCD = 0\n- Lenient gates: output unaffected by some input transitions\n- Next time: logic simplification, other canonical forms"
    },
    {
      "category": "Resource",
      "title": "Computer Architecture: Exciting Times Ahead!",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/e8f4a903be991b1149a142b62a4611ac_MIT6_004s09_lec25.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n6.004 Computation Structures\nSpring 2009\n\nL25 - Wrapup Lecture 1\n6.004 - Spring 2009\n5/12/09\nComputer Architecture:\nExciting Times Ahead!\nPrediction is very\ndifficult, especially\nabout the future.\n-- Neils Bohr\nThe best way to\npredict the future is\nto invent it.\n-- Alan Kay\nmodified 5/4/09 10:17\nL25 - Wrapup Lecture 2\n6.004 - Spring 2009\n5/12/09\nYou've mastered a lot...\nFets & voltages\nLogic gates\nCombinational\nlogic circuits\nCombinational contract:\ndiscrete-valued inputs\ncomplete in/out spec.\nstatic discipline\nAcyclic connections\nSummary specification\nDesign:\nsum-of-products\nsimplification\nmuxes, ROMs, PLAs\nStorage & state\nDynamic discipline\nFinite-state machines\nMetastability\nThroughput & latency\nPipelining\nSequential logic\nL25 - Wrapup Lecture 3\n6.004 - Spring 2009\n5/12/09\n... a WHOLE lot ...\nSequential logic\n<PC>+4+C*4\nASEL\nData Memory\nRD\nWD\nAdr\nR/W\nW\nD S E\nL\nWA\nRc <25:21>\nXP\nPCSEL\nPC\nJT\n+4\nInstruction\nMemory\nA\nD\nRb: <15:11>\nRa <20:16>\nRA2SEL\nRc <25:21>\n+\nRegister\nFile\nRA1\nRA2\nRD1\nRD2\nBSEL\nC: <15:0>\nC: <15:0>\nsign-extended\nZ\nALU\nA\nB\nJT\nWA\nWD\nWE\nC: <15:0> << 2\nsign-extended\nALUFN\nControl Logic\nZ\nASEL\nBSEL\nPCSEL\nRA2SEL\nWDSEL\nALUFN\nWr\n<PC>+4\nWr\nXAdr\nILL\nOP\nWASEL\nWASEL\nIRQ\nW E R F\nWERF\nCPU Architecture\nComputing Theory\nInstruction Set Architectures\nBeta implementation\nPipelined Beta\nSoftware conventions\nMemory architectures\n?\nInterconnect\nVirtual machines\nInterprocess communication\nOperating Systems\nReal time, Interrupts\nParallel Processing\nComputer Systems\nMEM\nMEM\nCPU\nDISK\nI/O\nI/\nO\nL2 $\nGraphics\nI/O\n\"AGP\" bus\nL25 - Wrapup Lecture 4\n6.004 - Spring 2009\n5/12/09\n6.035 (U, )\nComputer\nLanguage\nEngineering\nWhat's next?\nSome follow-on options...\nSoftware\nHardware\n6.374 (G, )\nAnalysis and Design\nof Digital\nIntegrated Circuits\n6.033 (U, )\nComputer\nSystem\nEngineering\n6.111 (U, )\nIntroductory\nDigital Systems\nLaboratory\nLA for 6.004\nUROP\nSpecial\nTopics\n6.823 (G, )\nComputer\nSystem\nArchitecture\n6.115 (U, )\nMicrocomputer\nProject\nLaboratory\n6.375 (U, )\nComplex\nDigital System\nDesign\n\nL25 - Wrapup Lecture 5\n6.004 - Spring 2009\n5/12/09\nThings to look forward to...\n6.004 is only an appetizer!\nAlgorithms\nArithmetic\nSignal Processing\nLanguage implementation\nProcessors\nSuperscalars\nDeep pipelines\nMulticores\nSystems Software\nStorage\nVirtual Machines\nNetworking\nLanguages & Models\nPython/Java/Ruby/...\nObjects/Streams/Aspects\nNetworking\nTools\nDesign Languages\nFPGA prototyping\nTiming Analyzers\nL25 - Wrapup Lecture 6\n6.004 - Spring 2009\n5/12/09\nVerilog example: Beta Register File\n// 2-read, 1-write 32-location register file\nmodule regfile(ra1,rd1,ra2,rd2,clk,werf,wa,wd);\ninput [4:0] ra1;\n// address for read port 1 (Reg[RA])\noutput [31:0] rd1;\n// read data for port 1\ninput [4:0] ra2;\n// address for read port 2 (Reg[RB], Reg[RC] for ST)\noutput [31:0] rd2;\n// read data for port 2\ninput clk;\ninput werf;\n// write enable, active high\ninput [4:0] wa;\n// address for write port (Reg[RC])\ninput [31:0] wd;\n// write data\nreg [31:0] registers[31:0];\n// the register file itself (local)\n// read paths are combinational, check for reads from R31\nassign rd1 = (ra1 == 31) ? 0 : registers[ra1];\nassign rd2 = (ra2 == 31) ? 0 : registers[ra2];\n// write port is active only when WERF is asserted\nalways @(posedge clk)\nif (werf) registers[wa] <= wd;\nendmodule\nL25 - Wrapup Lecture 7\n6.004 - Spring 2009\n5/12/09\nPC\nmodule pc(clk,reset,pcsel,offset,jump_addr,\nbranch_addr,pc,pc_plus_4);\ninput clk;\ninput reset;\n// forces PC to 0x80000000\ninput [2:0] pcsel;\n// selects source of next PC\ninput [15:0] offset;\n// inst[15:0]\ninput [31:0] jump_addr;\n// from Reg[RA], used in JMP instruction\noutput [31:0] branch_addr;\n// send to datapath for LDR instruction\noutput [31:0] pc;\n// used as address for instruction fetch\noutput [31:0] pc_plus_4;\n// saved in regfile during branches, JMP, traps\nreg [31:0] pc;\nwire [30:0] pcinc;\nwire [31:0] npc;\n// the Beta PC increments by 4, but won't change supervisor bit\nassign pcinc = pc + 4;\nassign pc_plus_4 = {pc[31],pcinc};\n// branch address = PC + 4 + 4*sxt(offset)\nassign branch_addr = {0,pcinc + {{13{offset[15]}},offset[15:0],2'b00}};\nassign npc = reset ? 32'h80000000 :\n(pcsel == 0) ? {pc[31],pcinc} :\n// normal\n(pcsel == 1) ? {pc[31],branch_addr[30:0]} :\n// branch\n(pcsel == 2) ? {pc[31] & jump_addr[31],jump_addr[30:0]} :\n// jump\n(pcsel == 3) ? 32'h80000004 : 32'h80000008;\n// illop, trap\n// pc register, pc[31] is supervisor bit and gets special treatment\nalways @(posedge clk) pc <= npc;\nendmodule\nL25 - Wrapup Lecture 12\n6.004 - Spring 2009\n5/12/09\nThe Crystal Ball\nsome trends in computer evolution\n- Technology shrinks\n-\n30% linear shrink/generation\n-\nCheaper, faster, lower power\n- Multicores (SMP, Tiled NUMA, ...)\n- Superscalar/SMT pipelines\n- Power management\n- Reconfigurable processing/interconnect\n- VLIW, SIMD influences\n\nL25 - Wrapup Lecture 13\n6.004 - Spring 2009\n5/12/09\n2010 Architecture?\nI/O\nEmbedded DRAM\nEmbedded DRAM\nOff-chip\nDRAM\nTiles\nTiled (VLIW/reconfigurable/vector) machines\nbecome popular in systems with resource\nconstraints (hardware cost, low power, hard\nreal time)\n- 10 GHz processor clock\n- 5 GHz network clock\n- 128 processing tiles\n- >5 TFLOPS peak (32b FLOPS)\n- >40 TOPS peak (8b OPS)\n- 1GB on-chip DRAM\n- 100 GB/s off-chip DRAM interface\n- 100 GB/s I/O\n- 25x25mm2 in 0.045μm CMOS\nGiant uniprocessors (maybe with SMT) remain popular in markets\nwhere software is the main expense.\nL25 - Wrapup Lecture 14\n6.004 - Spring 2009\n5/12/09\nThinking Outside the Box\nWill computers always look\nand operate the way\ncomputers do today?\nSome things to question:\n- Well-defined system \"state\"\n- Programming\n- Silicon-based logic\n- Logic at all\nSi\nBoolean\nLogic\nMOSFET\ntransistors\nSynchronous\nClocked\nSystems\nVon Neumann\nArchitectures\nL25 - Wrapup Lecture 15\n6.004 - Spring 2009\nOur programming hangup\nOur machines slavishly execute\nsequences of instructions. Does a\ncerebellum? A society? A beehive?\nAn MIT student?\nIs there an engineering\ndiscipline for building goal-\noriented systems from\ngoal-oriented\ncomponents?\nIs learning an alternative to\nprogramming?\nAdaptive\nMemory\nPUNISH\nL25 - Wrapup Lecture 16\n6.004 - Spring 2009\n5/12/09\nWet Computers\n1) The most reliable, sustainable, efficient,\nand smartest??? machines that we\nknow of are biological\n2) Fined tuned through millions of years of\nevolution\n3) The assembly, repair, and operation\n\"instructions\" for multi-billion element\nmachines are \"digitally\" encoded in a\nsingle molecule\n4) We are just beginning to understand the\n\"gates\" and the \"machine language\"\nI wonder if\n2289384-1\nis prime?\n\nL25 - Wrapup Lecture 17\n6.004 - Spring 2009\n5/12/09\nDNA Chips\n(DNA probes or microarrays)\nLeverages VLSI fabrication techniques\n(photolithography)\nUse PCRs (polymerase chain reactions) to\nmake an exponential number of DNA\ncopies\nMechanically bind specific \"tagged\" gene\nsequences onto a patterned substrate\nExpose to bath of denatured nucleotides\n(separated and diced up pieces of DNA)\nLook for phosphorescent markers\nMedical applications are obvious, but what\ndoes it have to do with computation?\nMicro \"vials\"\nof a gene\nsequence\nQuestions:\nWhat inputs satisfy\nf(x1,x2,...xN) =1.\nWe can reliably reslice and\nrecombine (state machines?)\nL25 - Wrapup Lecture 18\n6.004 - Spring 2009\n5/12/09\nCan we Program Microbes?\nDNA = program\nProtein synthesis = gates?\nCan we \"engineer\" organisms\nto perform computations\nfor us?\nCan we make a \"standard cell library\" offering digital\nbuilding blocks from DNA sequences?\nThis is alien thinking for biologist, but standard fare\nfor systems designers\nF(n) = n * F(n-1);\nF(0) = 1\nL25 - Wrapup Lecture 19\n6.004 - Spring 2009\n5/12/09\nComputing at the limit\nAt the particle level nature behaves very strangely...\nFar separated particles can be entangled\n- electron spins\n- photon polarizations\n- magnetic fields\nThey can be simultaneously in either state\n(so long as you don't look).\nThe act of looking at them (measuring, or observing them)\nforces the entangled particle into one of its states.\nStrangely enough, it is believed that we can use such\nentangled particles in computations w/o disturbing them.\nL25 - Wrapup Lecture 20\n6.004 - Spring 2009\n5/12/09\nQuantum Computing?\nClassic computers perform operations on strings of bits (0s\nand 1s).\nA quantum computer would be able to compute on bits\n(qubits) that can be simultaneously in either state.\nF(0< x < 220) = x * 371\nF(?) = 197001\nClassic computer:\n(with a dumb algorithm)\nSearch through all 220\npermutations\nQuantum computer:\nInsert 20 qubits, select\nthe desired answer, then\nlook back and see what\nthe qubits resolved to...\n\nL25 - Wrapup Lecture 21\n6.004 - Spring 2009\n5/12/09\nThe Dilemma\n- We have no clue how to build a practical quantum computer\n- Currently, quantum computing is merely a fantasy of\ntheoreticians\n- What other problems can a quantum computer solve more\nefficiently than a classic computer?\nA SUBTLE Reminder:\nTuring, Church, Post,\nKleene, and Markov\nreally \"invented\" most\nof modern day computer\nscience long before a\n\"practical\" implementation.\nL25 - Wrapup Lecture 22\n6.004 - Spring 2009\n5/12/09\n6004: The Big Lesson\nEngineering Abstractions:\n- Understanding of their technical\nunderpinnings\n- Respect for their value\n- Techniques for using them\nBut, most importantly:\n- The self assurance to discard them, in\nfavor of new abstractions!\nGood engineers use abstractions;\nGREAT engineers create them!\nYou've built, debugged, understood a complex\ncomputer from FETs to OS... what have you\nlearned?\nL25 - Wrapup Lecture 23\n6.004 - Spring 2009\n5/12/09\nTHE END!\nPens, pencils, paper\nthey attempt to solve problems\nthat teachers set forth.\nThe only problem\nwith Haiku is that you just\nget started and then"
    },
    {
      "category": "Resource",
      "title": "Cost/Performance Tradeoffs",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/fddc12b3adcd482a847b944bf1a50f48_MIT6_004s09_lec09.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n6.004 Computation Structures\nSpring 2009\n\nL09 - Multipliers 1\n6.004 - Spring 2009\n3/5/09\nCost/Performance Tradeoffs:\na case study\nDigital Systems Architecture 1.01\nmodified 2/23/09 10:44\nLab #3 due tonight!\nL09 - Multipliers 2\n6.004 - Spring 2009\n3/5/09\nBinary Multiplication\nEngineering Principle:\nExploit STRUCTURE in problem.\na b\na\nb\nx\nn bits\nn bits\n2n bits\nsince (2n-1)2 < 22n\nEASY PROBLEM: design\ncombinational circuit to multiply\ntiny (1-, 2-, 3-bit) operands...\nHARD PROBLEM: design circuit to\nmultiply BIG (32-bit, 64-bit)\nnumbers\nWe can make big\nmultipliers out of\nlittle ones!\nL09 - Multipliers 3\n6.004 - Spring 2009\n3/5/09\nMaking a 2n-bit multiplier\nusing n-bit multipliers\nGiven n-bit multipliers:\nSynthesize 2n-bit multipliers:\nx\nab\naH aL\nbH bL\naLbL\naLbH\naHbL\naHbH\na\nx\nb =\nab\nn bits\nn bits\n2n bits\nx\na\nb\n2n bits\n2n bits\nab\n4n bits\nL09 - Multipliers 4\n6.004 - Spring 2009\n3/5/09\nOur Basis:\nn=1: minimalist starting point\nMultiplying two 1-bit numbers is pretty simple:\na\nx\nb =\nab\nOf course, we could start with optimized combinational\nmultipliers for larger operands; e.g.\na1a0\nb1b0\nc3c2c1c0\n2-bit\nMultiplier\nthe logic gets\nmore complex,\nbut some\noptimizations\nare possible...\n\nOur induction step:\n2n-bit by 2n-bit multiplication:\n1. Divide multiplicands into n-bit pieces\n2. Form 2n-bit partial products, using n-bit by n-bit\nmultipliers.\n3. Align appropriately\n4. Add.\nInduction: we can use the same structuring\nprinciple to build a 4n-bit multiplier from our\nnewly-constructed 2n-bit ones...\n6.004 - Spring 2009\n3/5/09\nL09 - Multipliers 5\nx\na - b\naH aL\nbH bL\naLbL\naLbH\naHbL\nREGROUP partial\nproducts -\n2 additions\nrather than 3!\naHbH\nBrick Wall view\nof partial products\nMaking 4n-bit multipliers from n-bit\nones: 2 \"induction steps\"\n6.004 - Spring 2009\n3/5/09\nL09 - Multipliers 6\nb 3\nx\nb\nb\nb\na\na\na\na\na0b2\na0b3\na1b2\na1b3\na0b0\na0b1\na1b0\na1b1\na2b2\na2b3\na3b2\na3b3\na2b0\na2b1\na3b0\na3b1\nMultiplier Cookbook: Chapter 1\nStep 1: Form (& arrange)\nGiven problem:\nPartial Products:\nSubassemblies:\n- Partial Products\n- Adders\nStep 2: Sum\n6.004 - Spring 2009\n3/5/09\nL09 - Multipliers 7\n1 a 0\na\na\na\nb 3\nx\nb\nb\nb\nMULT\nADD\na0b2\na0b3\na1b2\na1b3\na0b0\na0b1\na1b0\na1b1\na2b2\na2b3\na3b2\na3b3\na2b0\na2b1\na3b0\na3b1\nPerformance/Cost Analysis\nPartial Products:\nn\n=\n(n )\nThings to Add:\n2n-2\n=\n(n)\nAdder Width:\n2n\n=\n(n)\nHardware Cost:\n?\n=\n(n )\nLatency:\n(n2) ??\n6.004 - Spring 2009\n3/5/09\nL09 - Multipliers 8\nExample:\nn +2n+3 =\n(n )\nsince\nn\n\n(n +2n+3)\n2n\n\"almost always\"\n(...) implies both\ninequalities; O(...)\nimplies only the\nsecond.\n\"Order Of\" notation:\nsuch that for all but finitely many\ng(n) = O(f(n))\n\"g(n) is of order f(n)\"\ng(n) = (f(n))\ng(n) = (f(n)) if there exist C 2 C 1> 0,\nintegral n 0\nc1-f(n) g(n) c2-f(n)\n\nL09 - Multipliers 9\n6.004 - Spring 2009\n3/5/09\nObservations:\nHmmm.\n(n\n2) partial products.\n(n\n2) full adders.\nMULT\nADD\na0b2\na0b3\na1b2\na1b3\na0b0\na0b1\na1b0\na1b1\na2b2\na2b3\na3b2\na3b3\na2b0\na2b1\na3b0\na3b1\nL09 - Multipliers 10\n6.004 - Spring 2009\n3/5/09\nRepackaging Function\nEngineering Principle #2:\nPut the Solution where the\nProblem is.\nHow about n2 blocks, each doing a\nlittle multiplication and a little\naddition?\n2 1\na b\na b\n1 1\na b\na b\na b\n2 0\na b\n3 0\na b\n3 1\na b\n0 2\na b\n1 2\na b\n0 3\na b\n1 3\na b\n2 2\na b\n3 2\na b\n2 3\na b\n3 3\na b\nMULT\nADD\n(n\n2) partial products.\n(n\n2) full adders.\nL09 - Multipliers 11\n6.004 - Spring 2009\n3/5/09\nGoal:\nArray of Identical Multiplier Cells\nSingle \"brick\" of brick-wall\narray...\n- Forms partial product\n- Adds to accumulating sum\nalong with carry\nS'k+1\nS'k\nSk+1\nSk\nCk+2\nCk\nbi\naj\nAi Bi\n(A+B)i\nCi\nCi+1\nFA\nNecessary Component: Full Adder\nTakes 2 addend bits plus carry bit. Produces sum\nand carry output bits.\nCASCADE to form an n-bit adder.\n2 1\na b\na b\n1 1\na b\na b\na b\n2 0\na b\n3 0\na b\n3 1\na b\n0 2\na b\n1 2\na b\n0 3\na b\n1 3\na b\n2 2\na b\n3 2\na b\n2 3\na b\n3 3\na b\nb3\nb2\nb1\nb0\na3\na2\na1\na0\nL09 - Multipliers 12\n6.004 - Spring 2009\n3/5/09\nDesign of 1-bit multiplier \"Brick\":\n2 1\na b\na b\n1 1\na b\na b\na b\n2 0\na b\n3 0\na b\n3 1\na b\n0 2\na b\n1 2\na b\n0 3\na b\n1 3\na b\n2 2\na b\n3 2\na b\n2 3\na b\n3 3\na b\nb3\nb2\nb1\nb0\na3\na2\na1\na0\nArray Layout:\n- operand bits bused diagonally\n- Carry bits propagate right-to-left\n- Sum bits propagate down\nBrick design:\n- AND gate forms 1x1 product\n- 2-bit sum propagates from top to\nbottom\n- Carry propagates to left\nS'k+1\nS'k\nSk+1\nSk\nCk+2\nCk\nbi\naj\nFA\nFA\nWastes some gates... but consider\n(say) optimized 4x4-bit brick!\n\nL09 - Multipliers 13\n6.004 - Spring 2009\n3/5/09\nLatency revisited\nHere's our combinational multiplier:\nb3\na0\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na1\na2\na3\nb2\nb1\nb0\nWhat's its propagation delay?\nNaive (but valid) bound:\n- O(n) additions\n- O(n) time for each addition\n- Hence O(n2) time required\nOn closer inspection:\n- Propagation only toward\nleft, bottom\n- Hence longest path bounded\nby length + width of array:\nO(n+n) = O(n)!\nL09 - Multipliers 14\n6.004 - Spring 2009\n3/5/09\nMultiplier Cookbook:\nChapter 2\n(n2)\nHardware for\nn by n bits:\nLatency:\nThroughput:\n(n)\n(1/n)\nCombinational Multiplier:\nb3\na0\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na1\na2\na3\nb2\nb1\nb0\n+\nNote: lots of tricks are\navailable to make a faster\ncombinational multiplier...\nL09 - Multipliers 15\n6.004 - Spring 2009\n3/5/09\nCombinational Multiplier:\nbest bang for the buck?\nSuppose we have LOTS of\nmultiplications.\nCan we do better from a\ncost/performance\nstandpoint?\nPIPELINING\nL09 - Multipliers 16\n6.004 - Spring 2009\n3/5/09\nThe Pipelining Bandwagon...\nwhere do I get on?\nWE HAVE:\n- Pipeline rules - \"well\nformed pipelines\"\n- Plenty of registers\n- Demand for higher\nthroughput.\nWhat do we do? Where do we\ndefine stages?\nb3\na0\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na1\na2\na3\nb2\nb1\nb0\n\nL09 - Multipliers 17\n6.004 - Spring 2009\n3/5/09\nStupid Pipeline Tricks\n(n)\nStages:\nClock Period:\n(n\n2)\nHardware cost for n by n bits:\n(n)\nLatency:\n(n\n2)\nThroughput:\n( 1/n )\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\ngotta break\nthat long\ncarry chain!\nL09 - Multipliers 18\n6.004 - Spring 2009\n3/5/09\nEven Stupider Pipeline Tricks\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\nBack to basics:\nwhat's the point of pipelining, anyhow?\nWORSE idea:\n- Doesn't break long\ncombinational paths\n- NOT a well-formed pipeline...\n... different register\ncounts on alternative\npaths\n... data crosses stage\nboundaries in both\ndirections!\nL09 - Multipliers 19\n6.004 - Spring 2009\n3/5/09\nBreaking O(n) combinational paths\nGOAL: (n) stages; (1) clock period!\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na0\na1\na2\na3\nb3\nb2\nb1\nb0\nLONG PATHS go down, to left:\n- Break array into diagonal\nslices\n- Segment every long\ncombinational path\nL09 - Multipliers 20\n6.004 - Spring 2009\n3/5/09\nMultiplier Cookbook: Chapter 3\n- Well-formed pipeline\n(careful!)\n- Constant (high!)\nthroughput,\nindependently of\noperand size.\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na0\na1\na2\na3\nb3\nb2\nb1\nb0\nStages:\nClock Period:\n(n2)\nHardware cost for n by n bits:\n(n)\nLatency:\nThroughput:\n(1)\n(1)\n(n)\n... but suppose we don't need\nthe throughput?\n\nL09 - Multipliers 21\n6.004 - Spring 2009\n3/5/09\nMoving down the cost curve...\nSuppose we have INFREQUENT\nmultiplications... pipelining\ndoesn't help us.\nCan we do better from a cost/\nperformance standpoint?\nHmmm, do I\nreally need\nall these\nextras?\na0\na1\na2\na3\nb3\nb2\nb1\nb0\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na b\na0\na1\na2\na3\nb3\nb2\nb1\nb0\na b\na b\na b\na b\na0\na1\na2\na3\nb3\nb2\nb1\nb0\na b\na b\na b\na b\na0\na1\na2\na3\nb3\nb2\nb1\nb0\na b\na b\na b\na b\na0\na1\na2\na3\nb3\nb2\nb1\nb0\na b\na b\na b\na b\nL09 - Multipliers 22\n6.004 - Spring 2009\n3/5/09\nMultiplier Cookbook: Chapter 4\nStages:\n(1) (constant!)\nClock Period:\nHardware cost for n by n bits:\n(n )\nLatency:\nThroughput:\n(n )\n(1/n)\nai\nb3\nb2\nb1\nb0\ni\na b\ni\na b\ni\na b\ni\na b\nSequential Multiplier:\n- Re-uses a single n-bit \"slice\" to\nemulate each pipeline stage\n- a operand entered serially\n- Lots of details to be filled in...\nL09 - Multipliers 23\n6.004 - Spring 2009\n3/5/09\n(Ridiculous?)\nExtremes Dept...\nCost minimization: how far can we go?\nai\nb3\nb2\nb1\nb0\ni\na b\ni\na b\ni\na b\ni\na b\ni\na b\ni\na b\ni\na b\ni\na b\nSuppose we want to minimize\nhardware (at any cost)...\n- Consider bit-serial!\n- Form and add 1-bit\npartial product per clock\n- Reuse single \"brick\" for\neach bit bj of slice;\n- Re-use slice for each bit\nof a operand\nL09 - Multipliers 24\n6.004 - Spring 2009\n3/5/09\nai\nb3\nb2\nb1\nb0\nMultiplier Cookbook: Chapter 5\nLatency:\nThroughput:\nStages:\n(1)\nClock Period:\nHardware cost for n by n bits:\n(1 n )\n(1) + ?\n(n2)\n(1/n2)\ni\na b\ni\na b\ni\na b\ni\na b\nBit Serial multiplier:\n- Re-uses a single brick to emulate\nan n-bit slice\n- both operands entered serially\n- O(n2) clock cycles required\n- Needs additional storage\n(typically from existing\nregisters)\n(constant)\n\nL09 - Multipliers 25\n6.004 - Spring 2009\n3/5/09\nSummary:\nLatency\n(n)\n(n)\n(n)\n(n2)\nThruput\n(1/n)\n(1)\n(1/n)\n(1/n2)\n$\n(n2)\n(n2)\n(n)\n(1)*\nScheme:\nCombinational\nN-pipe\nSlice-serial\nBit-serial\nLots more multiplier technology: fast adders, Booth Encoding, column\ncompression, ..."
    },
    {
      "category": "Resource",
      "title": "Designing an Instruction Set",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/78c15985bad66daac3ef9d03261e3644_MIT6_004s09_lec10.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n6.004 Computation Structures\nSpring 2009\n\nInstruction Sets 1\n6.004 - Spring 2009\n3/10/09\nDesigning an Instruction Set\nNerd Chef at work.\nmove flour,bowl\nadd milk,bowl\nadd egg,bowl\nmove bowl,mixer\nrotate mixer\n...\nQuiz 2 FRIDAY\nInstruction Sets 2\n6.004 - Spring 2009\n3/10/09\nLet's Build a Simple Computer\nData path for computing N*(N-1)\n*\nL.E.\nA\nL.E.\nB\nN\n-1\n0 1\n0 1\nASEL\nBSEL\nBLE\nALE\nANSWER\nL.E. = load enable. Register only loads\nnew value when LE=1\nInstruction Sets 3\n6.004 - Spring 2009\n3/10/09\nA Programmable Control System\nComputing N*(N-1) with this data path is a multi-step process. We can control\nthe processing at each\nstep with a FSM. If we\nallow different control\nsequences to be loaded\ninto the control FSM, then\nwe allow the machine to\nbe programmed.\nControl\nFSM\nALE\nBLE\nASEL\nBSEL\nA 1\nA A * B\nB B - 1\nB N\nA A * B\nInstruction Sets 4\n6.004 - Spring 2009\n3/10/09\nA First Program\nA 1\nA A * B\nB B - 1\nB N\nA A * B\nS0\nS1\nS2\nS3\nS4\nOnce more, writing a control program is\nnothing more than filling in a table:\nSN\nSN+1\nAsel\nALE\nBsel\nBLE\nSN\nSN+1\nAsel\nALE\nBsel\nBLE\n\nInstruction Sets 5\n6.004 - Spring 2009\n3/10/09\nAn Optimized Program\nA 1\nA A * B\nB B - 1\nB N\nA A * B\nS0\nS1\nS2\nS3\nSome parts of the program\ncan be computed simultaneously:\nSN\nSN+1\nAsel\nALE\nBsel\nBLE\nInstruction Sets 6\n6.004 - Spring 2009\n3/10/09\nComputing Factorial\nL.E.\nL.E.\n*\nA\nB\nN\n-1\n0 1\n0 1\nASEL\nBSEL\nBLE\nALE\nANSWER\n=0?\nControl\nFSM\nAsel\nBsel\nAle\nBle\nThe advantage of a programmable\ncontrol system is that we can\nreconfigure it to compute new\nfunctions.\nIn order to compute N! we will need\nto add some new logic and an input\nto our control FSM:\nInstruction Sets 7\n6.004 - Spring 2009\n3/10/09\nControl Structure for Factorial\nA A * B\nB B - 1\nB N\nA 1\nZ=0\nZ=1\nDONE\nS0\nS1\nS2\nProgrammability allows us to reuse data\npaths to solve new problems. What we need\nis a general purpose data path, which can be\nused to efficiently solve most problems as\nwell as an easier way to control it.\nZ\nSN\nSN+1\nAsel\nALE\nBsel\nBLE\n-\n-\nZ\nSN\nSN+1\nAsel\nALE\nBsel\nBLE\n-\n-\nInstruction Sets 8\n6.004 - Spring 2009\n3/10/09\nA Programmable Engine\nWe've used the same data paths for computing\nN*(N-1) and Factorial; there are a variety\nof other computations we might\nimplement simply by re-programming the\ncontrol FSM.\nAlthough our little machine is programmable, it\nfalls short of a practical general-purpose\ncomputer - and fails the Turing\nUniversality test - for three primary\nreasons:\n1.\nIt has very limited storage: it lacks\nthe \"expandable\" memory resource of\na Turing Machine.\n2.\nIt has a tiny repertoire of operations.\n3.\nThe \"program\" is fixed. It lacks the\npower, e.g., to generate a new\nprogram and then execute it.\nControl\nFSM\nALE\nBLE\nASEL\nBSEL\nL.E.\nL.E.\n*\nA\nB\nN\n-1\n0 1\nASEL\nBSEL\nBLE\nALE\nANSWER\n=0?\n0 1\n\nInstruction Sets 9\n6.004 - Spring 2009\n3/10/09\nA General-Purpose Computer\nThe von Neumann Model\nMany architectural approaches to the general purpose computer have been explored.\nThe one on which nearly all modern, practical computers is based was proposed by\nJohn von Neumann in the late 1940s. Its major components are:\nCentral\nProcessing\nUnit\nCentral Processing Unit (CPU): containing several\nregisters, as well as logic for performing a\nspecified set of operations on their contents.\nMain\nMemory\nMemory: storage of N words of W bits each, where W\nis a fixed architectural parameter, and N\ncan be expanded to meet needs.\nInput/\nOutput\nI/O: Devices for communicating with the outside world.\nAh, an FSM!\nLike an\nInfinite Tape?\nHmm, guess J.\nvon N was an\nengineer after\nall!\nInstruction Sets 10\n6.004 - Spring 2009\n3/10/09\nThe Stored Program Computer\nThe von Neumann architecture easily addresses the first two limitations of our\nsimple programmable machine example:\n- A richer repertoire of operations, and\n- An expandable memory.\nBut how does it achieve programmability?\nCPU fetches and executes - interprets - successive\ninstructions of the program ...\n- Program is simply data for the interpreter, specifying\nwhat computation to perform\n- Single expandable resource pool - main memory -\nconstrains both data and program size.\nKey idea: Memory holds not only data,\nbut coded instructions that make up\na program.\nCentral\nProcessing\nUnit\nInstruction Sets 11\n6.004 - Spring 2009\n3/10/09\nregisters\noperations\nAnatomy of a von Neumann Computer\nControl\nUnit\nData\nPaths\nInternal storage\nMEMORY\ncontrol\nstatus\ninstructions\ndata\n...\ndest\nasel\nfn\nbsel\nCc's\nALU\nPC\n- INSTRUCTIONS coded as binary data\n- PROGRAM COUNTER or PC: Address of\nnext instruction to be executed\n- logic to translate instructions into control\nsignals for data path\n+1\nR1 R2+R3\naddress\naddress\nInstruction Sets 12\n6.004 - Spring 2009\n3/10/09\nInstruction Set Architecture\nCoding of instructions raises some interesting choices...\n- Tradeoffs: performance, compactness, programmability\n- Uniformity. Should different instructions\n- Be the same size?\n- Take the same amount of time to execute?\nTrend: Uniformity. Affords simplicity, speed, pipelining.\n- Complexity. How many different instructions? What level operations?\n- Level of support for particular software operations: array indexing,\nprocedure calls, \"polynomial evaluate\", etc\n\"Reduced Instruction Set Computer\" (RISC) philosophy: simple\ninstructions, optimized for speed\nMix of engineering & Art...\nTrial (by simulation) is our best technique for making choices!\nOur representative example: the architecture!\nMain Memory\ndata\ndata\ndata\ninstruction\ninstruction\ninstruction\n\nInstruction Sets 13\n6.004 - Spring 2009\n3/10/09\nProgramming Model\na representative, simple, contemporary RISC\nFetch/Execute loop:\n- fetch Mem[PC]\n- PC = PC + 4+\n- execute fetched instruction\n(may change PC!)\n- repeat!\nPC\n+Even though each memory word is 32-bits\nwide, for historical reasons the uses byte\nmemory addresses. Since each word\ncontains four 8-bit bytes, addresses of\nconsecutive words differ by 4.\nMain Memory\n(4 bytes)\n32-bit \"words\"\nnext instruction\nProcessor State\nr0\nr1\nr2\n...\nr31\n000000....0\n32-bit \"words\"\nGeneral Registers\nInstruction Sets 14\n6.004 - Spring 2009\n3/10/09\nInstruction Formats\nAll Beta instructions fit in a single 32-bit word, whose fields\nencode combinations of\n- a 6-bit OPCODE (specifying one of < 64 operations)\n- several 5-bit OPERAND locations, each one of the 32\nregisters\n- an embedded 16-bit constant (\"literal\")\nThere are two instruction formats:\n- Opcode, 3 register operands\n(2 sources, destination)\n- Opcode, 2 register operands,\n16-bit literal constant\nOPCODE\nrc\nra\nrb\nunused\nOPCODE\nrc\nra\n16-bit signed constant\nInstruction Sets 15\n6.004 - Spring 2009\n3/10/09\nALU Operations\nSample coded operation: ADD instruction\n32-bit hex: 0x80611000\nWhat we prefer to write: ADD(r1,r2,r3)\nADD(ra,rb,rc):\n\"Add the contents of ra to\nthe contents of rb; store\nthe result in rc\"\nOPCODE = 100000,\nencoding ADD\nrc=3,\nencoding R3 as\ndestination\nra=1,rb=2\nencoding R1 and R2 as\nsource locations\nReg[rc] = Reg[ra] + Reg[rb]\narithmetic: ADD, SUB, MUL, DIV\ncompare: CMPEQ, CMPLT, CMPLE\nboolean: AND, OR, XOR\nshift: SHL, SHR, SAR\nSimilar instructions for other\nALU operations:\n1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\nunused\n\"assembly language\"\nInstruction Sets 16\n6.004 - Spring 2009\n3/10/09\nALU Operations with Constant\nADDC instruction: adds constant, register contents:\nSymbolic version: ADDC(r1,-3,r3)\n\"Add the contents of ra to\nconst; store the result in\nrc\"\nOPCODE = 110000,\nencoding ADDC\nrc=3,\nencoding R3 as\ndestination\nra=1,\nencoding R1 as\nfirst operand\nReg[rc] = Reg[ra] + sxt(const)\narithmetic: ADDC, SUBC, MULC, DIVC\ncompare: CMPEQC, CMPLTC, CMPLEC\nboolean: ANDC, ORC, XORC\nshift: SHLC, SHRC, SARC\nSimilar instructions for other\nALU operations:\nconstant field,\nencoding -3 as\nsecond operand\n(sign-extended!)\nADDC(ra,const,rc):\n1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n\nInstruction Sets 17\n6.004 - Spring 2009\n3/10/09\nDo We Need Built-in Constants?\nPercentage of the operations that use a constant operand\nOne way to answer architectural questions is to evaluate the\nconsequences of different choices using carefully chosen representative\nbenchmarks (programs and/or code sequences). Make choices that are\n\"best\" according to some metric (cost, performance, ...).\nInstruction Sets 18\n6.004 - Spring 2009\n3/10/09\nBaby's First Beta Program\n(fragment)\nSUBC(r1,1,r2)\n| put N-1 into r2\nMUL(r2,r1,r2)\n| leave N*(N-1) in r2\nSuppose we have N in r1, and want to compute N*(N-1),\nleaving the result in r2:\nThese two instructions do what our little ad-hoc machine did. Of course,\nlimiting ourselves to registers for storage falls short of our ambitions....\nit amounts to the finite storage limitations of an FSM!\nNeeded: instruction-set support for reading and writing\nlocations in main memory...\nInstruction Sets 19\n6.004 - Spring 2009\n3/10/09\nLoads & Stores\nLD(ra,const,rc)\nReg[rc] = Mem[Reg[ra] + sxt(const)]\nST(rc,const,ra)\nMem[Reg[ra] + sxt(const)] = Reg[rc]\n\"Fetch into rc the contents of the memory location whose\naddress is C plus the contents of ra\"\nAbbreviation: LD(C,rc) for LD(R31,C,rc)\n\"Store the contents of rc into the memory location whose\naddress is C plus the contents of ra\"\nAbbreviation: ST(rc,C) for ST(rc,C,R31)\nBYTE ADDRESSES, but only 32-bit word accesses to word-aligned\naddresses are supported. Low two address bits are ignored!\nOPCODE\nrc\nra\n16-bit signed constant\naddress\nInstruction Sets 20\n6.004 - Spring 2009\n3/10/09\nStorage Conventions\n- Variables live in memory\n- Operations done on registers\n- Registers hold Temporary\nvalues\n1000:\n1004:\n1008:\n1010:\n100C:\nn\nr\nx\ny\nAddr assigned at compile time\nint x, y;\ny = x * 37;\nLD(r31, 0x1008, r0)\nMULC(r0, 37, r0)\nST(r0, 0x100C, r31)\nx=0x1008\ny=0x100C\nLD(x, r0)\nMULC(r0, 37, r0)\nST(r0, y)\ntranslates\nto\nor, more\nhumanely,\nto\nRa defaults to R31 (0)\nCompilation approach:\nLOAD, COMPUTE, STORE\nBar\ngrap\nh o\nf p\nercen\ntag\ne o\nf ope\nrat\nions\nthat use\na constant op\ner\nand\n, f\nor\nloa\nds,\nco\nmpa\nres\n, a\nnd A\nLU\nope\nrat\nion\ns.\nFigure by MIT OpenCourseWare.\n\nInstruction Sets 21\n6.004 - Spring 2009\n3/10/09\ncan do these with appropriate\nchoices for Ra and const\nCommon \"Addressing Modes\"\n-\nAbsolute: \"constant\"\n- Value = Mem[constant]\n- Use: accessing static data\n-\nIndirect (aka Register deferred): \"(Rx)\"\n- Value = Mem[Reg[x]]\n- Use: pointer accesses\n-\nDisplacement: \"constant(Rx)\"\n- Value = Mem[Reg[x] + constant]\n- Use: access to local variables\n-\nIndexed: \"(Rx + Ry)\"\n- Value = Mem[Reg[x] + Reg[y]]\n- Use: array accesses (base+index)\n-\nMemory indirect: \"@(Rx)\"\n- Value = Mem[Mem[Reg[x]]]\n- Use: access thru pointer in mem\n-\nAutoincrement: \"(Rx)+\"\n- Value = Mem[Reg[x]]; Reg[x]++\n- Use: sequential pointer accesses\n-\nAutodecrement: \"-(Rx)\"\n- Value = Reg[X]--; Mem[Reg[x]]\n- Use: stack operations\n-\nScaled: \"constant(Rx)[Ry]\"\n- Value = Mem[Reg[x] + c + d*Reg[y]]\n- Use: array accesses (base+index)\nArgh! Is the complexity worth the cost?\nNeed a cost/benefit analysis!\nInstruction Sets 22\n6.004 - Spring 2009\n3/10/09\nMemory Operands: Usage\nUsage of different memory operand modes\nInstruction Sets 23\n6.004 - Spring 2009\n3/10/09\nCapability so far: Expression Evaluation\nTranslation of an Expression:\nint x, y;\ny = (x-3)*(y+123456)\nx:\nlong(0)\ny:\nlong(0)\nc:\nlong(123456)\n...\nLD(x, r1)\nSUBC(r1,3,r1)\nLD(y, r2)\nLD(c, r3)\nADD(r2,r3,r2)\nMUL(r2,r1,r1)\nST(r1,y)\n- VARIABLES are allocated\nstorage in main memory\n- VARIABLE references translate\nto LD or ST\n- OPERATORS translate to ALU\ninstructions\n- SMALL CONSTANTS translate\nto ALU instructions w/ built-in\nconstant\n- \"LARGE\" CONSTANTS translate\nto initialized variables\nNB: Here we assume that\nvariable addresses fit into 16-\nbit constants!\nInstruction Sets 24\n6.004 - Spring 2009\n3/10/09\nCan We Run Every Algorithm?\nNeeded:\nability to\nchange the\nPC.\nModel thus far:\n- Executes instructions sequentially -\n- Number of operations executed =\nnumber of instructions in our program!\nGood news: programs can't \"loop forever\"!\n- Halting problem* is solvable for our\ncurrent Beta subset!\nBad news: can't compute Factorial:\n- Only supports bounded-time\ncomputations;\n- Can't do a loop, e.g. for Factorial!\nNOT\nUniversal*\n*more next week!\nBar\ngrap\nh o\nf u\nsage\nof\ndif\nferen\nt m\nemo\nry op\nera\nnd\nnodes\n, f\nor autoincrem\nent, displac\nement deferr\ned, scal\ned, regi\nster d\neferred,\na\nnd\ndis\npla\ncem\nent\n.\nFigure by MIT OpenCourseWare.\n\nInstruction Sets 25\n6.004 - Spring 2009\n3/10/09\nBeta Branch Instructions\nPC = PC + 4;\nReg[rc] = PC;\nif (REG[ra] != 0)\nPC = PC + 4*offset;\nBNE(ra,label,rc): Branch if not equal\nPC = PC + 4;\nReg[rc] = PC;\nif (REG[ra] == 0)\nPC = PC + 4*offset;\nBEQ(ra,label,rc): Branch if equal\nNB: \"offset\" is a SIGNED\nCONSTANT encoded as part of\nthe instruction!\nOPCODE\nrc\nra\n16-bit signed constant\nThe Beta's branch instructions provide a way of conditionally changing the PC to\npoint to some nearby location...\n... and, optionally, remembering (in Rc) where we came from (useful for procedure\ncalls).\noffset = (label - <addr of BNE/BEQ>)/4 - 1\n= up to 32767 instructions before/after BNE/BEQ\nInstruction Sets 26\n6.004 - Spring 2009\n3/10/09\nNow we can do Factorial...\nint n, ans;\nr1 = 1;\nr2 = n;\nwhile (r2 != 0) {\nr1 = r1 * r2;\nr2 = r2 - 1\n}\nans = r1;\nSynopsis (in C):\n- Input in n, output in ans\n- r1, r2 used for temporaries\n- follows algorithm of our earlier\ndata paths.\nn:\nlong(123)\nans:\nlong(0)\n...\nADDC(r31, 1, r1)\n| r1 = 1\nLD(n, r2)\n| r2 = n\nloop:\nBEQ(r2, done, r31)\n| while (r2 != 0)\nMUL(r1, r2, r1)\n| r1 = r1 * r2\nSUBC(r2, 1, r2)\n| r2 = r2 - 1\nBEQ(r31, loop, r31)\n| Always branches!\ndone:\nST(r1, ans, r31)\n| ans = r1\nBeta code, in assembly language:\nInstruction Sets 27\n6.004 - Spring 2009\n3/10/09\nSummary\n- Programmable data paths provide some algorithmic flexibility, just\nby changing control structure.\n- Interesting control structure optimization questions - e.g., what\noperations can be done simultaneously?\n- von Neumann model for general-purpose computation: need\n- support for sufficiently powerful operation repertoire\n- Expandable Memory\n- Interpreter for program stored in memory\n- ISA design requires tradeoffs, usually based on benchmark results:\nart, engineering, evaluation & incremental optimizations\n- Compilation strategy\n- runtime \"discipline\" for software implementation of a general\nclass of computations\n- Typically enforced by compiler, run-time library, operating\nsystem. We'll see more of these!"
    },
    {
      "category": "Resource",
      "title": "Devices & Interrupts",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/eeba6308073e39f0d7dc1b74fb353a78_MIT6_004s09_lec19.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n6.004 Computation Structures\nSpring 2009\n\nL19 - Devices & Interrupts 1\n6.004 - Spring 2009\n4/16/09\nDevices & Interrupts\nLab #6 due tonight!\nLoop:\nLD(R3,0,R0)\nADDC(R3,4,R3)\nSUBC(R2,1,R2)\nBNE(R2,Loop)\n...\n(Cough) Excuse me, sir...\nmodified 4/13/09 10:14\nL19 - Devices & Interrupts 2\n6.004 - Spring 2009\n4/16/09\nWhy an OS?\nWhat we've got:\n- A Single Sequence Machine, capable of doing ONE thing at a time - one\ninstruction, one I/O operation, one program.\n- A universe of gadgets - e.g. I/O devices - that do similar things\nslightly differently.\nWhat we'd like:\n- To listen to MP3s while reading email.\n- To access disk, network, and screen \"simultaneously\".\n- To write a single program that does I/O with anybody's disk.\nPlausible approaches:\n- An infinite supply of identical computers with\nuniform, high-level peripherals for every\nconceivable purpose... or\n- An illusion: Make one real computer look like\nmany \"virtual\" ones.\nL19 - Devices & Interrupts 3\n6.004 - Spring 2009\n4/16/09\nOperating Systems\nAn OS is the Glue that holds a\ncomputer together.\n- Mediates between\ncompeting requests\n- Resolves\nnames/bindings\n- Maintains\norder/fairness\nKERNEL - a RESIDENT portion\nof the O/S that handles the\nmost common and\nfundamental service\nrequests.\nvir.tu.al \\'v*rch-(*-)w*l, 'v*r-ch*l\\ \\.v*r-ch*-'wal-*t-e-\\ \\'v*rch-(*-)w*-le-, 'v*rch-(*-)le-\n\\ aj [ME, possessed of certain physical virtues, fr. ML virtualis, fr. L virtus strength,\nvirtue : being in essence or effect but not in fact - vir.tu.al.i.ty n\nHardware\nRegisters\nALUs\nPCs\nCaches\nKernel\nOperating System\nApplications\nDevice Drivers\nScheduler\nProcess Control\nBlocks (PCBs)\nShared\nLibraries\nWord Processors\nGraphical User\nInterface (GUI)\nGames\nSpread Sheets\nWeb\nBrowser\nPage Tables\nDevice\nQueues\nNetwork Interfaces\nSecurity\nFile system\nI/O Devices\nL19 - Devices & Interrupts 4\n6.004 - Spring 2009\n4/16/09\nOS organization\n\"Applications\" are quasi-parallel\n\"PROCESSES\"\non\n\"VIRTUAL MACHINES\",\neach with:\n- CONTEXT (virtual address space)\n- Virtual I/O devices\nO.S. KERNEL has:\n- Interrupt handlers\n- SVC (trap) handlers\n- Scheduler\n- PCB structures containing the\nstate of inactive processes\nScheduler\nKERNEL\nP1\nP2\nSVC 1 handler\nSVC 0 handler\nI/O Handler\nDevice\nAlarm Clock\nI/O Handler\nDevice\n...\nDPYNum=0\n...\nDPYNum=1\nPCBs:\nP1:\nP2:\nloop:SVC(0)\n...\nSVC(1)\n...\nBR(loop)\nloop:SVC(0)\n...\nSVC(1)\n...\nBR(loop)\n\nL19 - Devices & Interrupts 5\n6.004 - Spring 2009\n4/16/09\nAsynchronous I/O Handling\nSVC call from application\nApplication:\n...\nReadKey() | read key into R0\n...\n. . .\n\"A\"\nDevice Buffer\n(in OS Kernel)\nKEYhit_h() {\n(read ASCII code, put in buffer)\n}\nINTERRUPT from Keyboard n\nINTERRUPT\nto OS\nIN\nOUT\nReadKEY_h() {\n(remove next char from buffer,\nreturn in R0)\n...}\nTRAP to OS\nIN\nOUT\nL19 - Devices & Interrupts 6\n6.004 - Spring 2009\n4/16/09\nInterrupt-based Asynchronous I/O\nstruct Device {\nchar Flag, Data;\n} Keyboard;\nKEYhit_h() {\nBuffer[inptr] = Keyboard.Data;\ninptr = (inptr + 1) % BUFSIZE;\n}\nOPERATION: NO attention to Keyboard during normal operation\n- on key strike: hardware asserts IRQ to request interrupt\n- USER program interrupted, PC+4 of interrupted inst. saved in XP\n- state of USER program saved on KERNEL stack;\n- KeyboardHandler invoked, runs to completion;\n- state of USER program restored; program resumes.\nTRANSPARENT to USER program.\nKeyboard Interrupt Handler (in O.S. KERNEL):\nAssume each\nkeyboard has\nan associated\nbuffer\nL19 - Devices & Interrupts 7\n6.004 - Spring 2009\n4/16/09\nReadKey SVC: Attempt #1\nA supervisor call (SVC) is an instruction that transfers control to the kernel so\nit can satisfy some user request. Kernel returns to user program when\nrequest is complete.\nFirst draft of a ReadKey SVC handler (supporting a Virtual Keyboard): returns\nnext keystroke on a user's keyboard to that user's requesting application:\nProblem: Can't interrupt code running in the supervisor mode...\nso the buffer never gets filled.\nReadKEY_h()\n{\nint kbdnum = ProcTbl[Cur].DPYNum;\nwhile (BufferEmpty(kbdnum)) {\n/* busy wait loop */\n}\nUser.Regs[0] = ReadInputBuffer(kbdnum);\n}\nReadKEY_h()\n{\nint kbdnum = ProcTbl[Cur].DPYNum;\nwhile (BufferEmpty(kbdnum)) {\n/* busy wait loop */\n}\nUser.Regs[0] = ReadInputBuffer(kbdnum);\n}\nL19 - Devices & Interrupts 8\n6.004 - Spring 2009\n4/16/09\nReadKey SVC: Attempt #2\nA BETTER keyboard SVC handler:\nReadKEY_h()\n{\nint kbdnum = ProcTbl[Cur].DPYNum;\nif (BufferEmpty(kbdnum)) {\n/* busy wait loop */\nUser.Regs[XP] = User.Regs[XP]-4;\n} else\nUser.Regs[0] = ReadInputBuffer(kbdnum);\n}\nThat's a\nfunny way\nto write\na loop\nProblem: The process just wastes its time-slice waiting for\nsomeone to hit a key...\nThis one actually works!\n\nL19 - Devices & Interrupts 9\n6.004 - Spring 2009\n4/16/09\nReadKey SVC: Attempt #3\nEVEN BETTER: On I/O wait, YIELD remainder of quantum:\nReadKEY_h()\n{\nint kbdnum = ProcTbl[Cur].DPYNum;\nif (BufferEmpty(kbdnum)) {\nUser.Regs[XP] = User.Regs[XP]-4;\nScheduler( );\n} else\nUser.Regs[0] = ReadInputBuffer(kbdnum);\n}\nRESULT: Better CPU utilization!!\nDoes timesharing cause CPU use to be less efficient?\n- COST: Scheduling, context-switching overhead; but\n- GAIN: Productive use of idle time of one process by running another.\nL19 - Devices & Interrupts 10\n6.004 - Spring 2009\n4/16/09\nSophisticated Scheduling\nTo improve efficiency further, we can avoid scheduling processes in\nprolonged I/O wait:\n- Processes can be in ACTIVE or WAITING (\"sleeping\") states;\n- Scheduler cycles among ACTIVE PROCESSES only;\n- Active process moves to WAITING status when it tries to read\na character and buffer is empty;\n- Waiting processes each contain a code (eg, in PCB) designating\nwhat they are waiting for (eg, keyboard N);\n- Device interrupts (eg, on keyboard N) move any processes\nwaiting on that device to ACTIVE state.\nUNIX kernel utilities:\n- sleep(reason) - Puts CurProc to sleep. \"Reason\" is an arbitrary\nbinary value giving a condition for reactivation.\n- wakeup(reason) - Makes active any process in sleep(reason).\nL19 - Devices & Interrupts 11\n6.004 - Spring 2009\n4/16/09\nReadKey SVC: Attempt #4\nReadKEY_h() {\n...\nif (BufferEmpty(kbdnum)) {\nUser.Regs[XP] = User.Regs[XP] - 4;\nsleep(kbdnum);\n...\n}\nsleep(status s) {\nProcTbl[Cur].status = s;\nScheduler()\n}\nScheduler() {\n...\nwhile (ProcTbl[i].status != 0) {\ni = (i+1)%N;\n}\n...\n}\nwakeup(status s) {\nfor (i = 0; i < N; i += 1) {\nif (ProcTbl[i].status == s)\nPCB[i].status = 0;\n}\n}\nSVC call from application\nKEYhit_h() {\n...\nWriteBuffer(kbdnum, key)\nwakeup(kbdnum);\n...\n}\nINTERRUPT from Keyboard n\nL19 - Devices & Interrupts 12\n6.004 - Spring 2009\n4/16/09\nThe Need for \"Real Time\"\nSide-effects of CPU virtualization\n+ abstraction of machine resources\n(memory, I/O, registers, etc. )\n+ multiple \"processes\" executing concurrently\n+ better CPU utilization\n- Processing throughput is more variable\nOur approach to dealing with the asynchronous world\n- I/O - separate \"event handling\"\nfrom \"event processing\"\nDifficult to meet \"hard deadlines\"\n- control applications\n- playing videos/MP3s\nReal-time as an alternative to time-sliced\nor fixed-priority preemptive scheduling\n\nL19 - Devices & Interrupts 13\n6.004 - Spring 2009\n4/16/09\nInterrupt Latency\nOne way to measure the real-time performance of a system is INTERRUPT\nLATENCY:\n- HOW MUCH TIME can elapse between an interrupt request and the\nSTART of its handler?\ntime\nRequest\nLatency\nService\nTime\nDeadline?\nOFTEN bad things happen when service is delayed beyond\nsome deadline - \"real time\" considerations:\nMissed characters\nSystem crashes\nNuclear meltdowns\n\"HARD\"\nReal time\nconstraints\nL19 - Devices & Interrupts 14\n6.004 - Spring 2009\n4/16/09\nSources of Interrupt Latency\nWhat causes interrupt latency:\n- State save, context switch.\n- Periods of uninterruptability:\nLong, uninterruptable instructions -- eg block moves, multi-level indirection.\nExplicitly disabled periods (eg for atomicity, during service of other\ninterrupts).\nGOAL: BOUND (and minimize) interrupt latency!\n- Optimize interrupt sequence context switch\n- Make unbounded-time instructions INTERRUPTABLE (state in registers, etc).\n- Avoid/minimize disable time\n- Allow handlers to be interrupted, in certain cases (while still avoiding reentrant\nhandlers!).\ntime\nRequest\nLatency\nService\nTime\nDeadline?\nWe can consider this\nwhen we write our O/S\nWe can address\nthis in our ISA\nBut, this is\napplication\ndependent!\nL19 - Devices & Interrupts 15\n6.004 - Spring 2009\n4/16/09\nInterrupt Disable/Enable\nE\nInterrupt\nEnable/\nDisable bit\n(Misc. other stuff:\nContext, K/U mode,\netc.)\nINTERRUPT DISABLE\nBIT (part of processor\nstatus)... in PC:\nOften in separate\nProcessor Status\nWord ...\nE=1: DISABLED\nE=0: ENABLED\ne.g.\n- BETA K-mode bit\n(disables interrupts,\nother functions)\n- Often separate bit/\nmechanism\nPC\nTYPICAL OPERATION: (as with Beta K-mode bit):\n- ONLY take interrupts if E=0; else defer.\n- SAVE OLD E on interrupt, install new E from interrupt vector (along\nwith PC, etc). New E=1 (to disable interrupts during handler).\n- Run handler, with interrupts disabled.\n- On JMP (at return from handler), saved state restored to processor,\nresuming interrupted program (with E=0 again).\nL19 - Devices & Interrupts 16\n6.004 - Spring 2009\n4/16/09\nScheduling of Multiple Devices\nDEVICE\nKeyboard\nDisk\nPrinter\nService\nTime\nActual w/c\nLatency\n________\n________\n________\n\"TOY\" System scenario:\nWhat is the WORST CASE latency seen by each device?\nReq: K,P,D\nP\nK\nD\nReq: D,P,K\nD\nP\nK\nReq: K,D,P\nK\nD\nP\nAssumptions:\n- Infrequent interrupt requests (each happens only once/scenario)\n- Simultaneous requests might be served in ANY order.... Whence\n- Service of EACH device might be delayed by ALL others!\n... can we improve this?\n500 + 400 = 900\n800 + 400 = 1200\n800+ 500 = 1300\n\nL19 - Devices & Interrupts 17\n6.004 - Spring 2009\n4/16/09\nWeak (non-preemptive) Priorities\nISSUE: Processor becomes\ninterruptable (at fetch of next\ninstruction), several interrupt\nrequests are pending. Which is\nserved first?\nLATENCIES with WEAK PRIORITIES:\nService of each device might be\ndelayed by:\n- Service of 1 other (arbitrary)\ndevice, whose interrupt request\nwas just honored; PLUS\n- Service of ALL higher-priority\ndevices.\nD\nP\nK\nReq:\nP,D,K\nReq:\nK,P,D\nK\nP\nD\nDEVICE\nKeyboard\nDisk\nPrinter\nService\nTime\nActual w/c\nLatency\n________\n________\n________\nWEAK PRIORITY ORDERING: Check in prescribed sequence, eg:\nDISK > PRINTER > KEYBOARD.\nvs 1200 -\nNow delayed by only 1 service!\nL19 - Devices & Interrupts 18\n6.004 - Spring 2009\n4/16/09\nThe Need for Preemption\nWithout preemption, ANY interrupt service can delay ANY other service\nrequest... the slowest service time constrains response to fastest\ndevices. Often, tight deadlines can't be met using this scheme alone.\nEXAMPLE: 800 uSec deadline (hence 300 uSec maximum interrupt\nlatency) on disk service, to avoid missing next sector...\nDEVICE\nKeybrd\nDisk\nPrinter\nServ.\nTime\nActual\nLatency\nMax.\nDelay\nneed PREEMPTION: Allow handlers for LOWER PRIORITY interrupts to be\ninterrupted by HIGHER priority requests!\nLatency\nw/preemption\nPriority\n~0\n[D] 500\nD,P 900\nCAN'T SATISFY the disk requirement in this system using weak priorities!\nL19 - Devices & Interrupts 19\n6.004 - Spring 2009\n4/16/09\nStrong Priority Implementation\nSCHEME:\n- Expand E bit in PC to be a PRIORITY integer PRI\n(eg, 3 bits for 8 levels)\n- ASSIGN a priority to each device.\n- Prior to each instruction execution:\nFind priority Pi of highest requesting device, say Di\nTake interrupt if and only if Pi > PRI, set PRI = Pi.\nStrong priorities:\nKEY: Priority in Processor state\nAllows interruption of (certain) handlers\nAllows preemption, but not reentrance\nBENEFIT: Latency seen at high priorities UNAFFECTED by service times\nat low priorities.\nPC:\nProgram Counter\nPRI\nL19 - Devices & Interrupts 20\n6.004 - Spring 2009\n4/16/09\nRecurring Interrupts\nDEVICE\nKeybrd\nDisk\nPrinter\nServ.\nTime\nActual\nLatency\nMax.\nDelay\nP\nMax.\nFreq\n100/s\n500/s\n1000/s\nConsider interrupts which recur at bounded rates:\nNote that interrupt LATENCIES don't tell\nthe whole story--consider COMPLETION\nTIMES, eg for Keyboard in example to the\nright.\nKeyboard service not complete until 3ms\nafter request. Often deadlines used\nrather than max. delays.\nD\nP\nK\nP\nP\nD\nP\nP\nD\nD\nP\nK\nD\nP\nD\nP\nP\n\nL19 - Devices & Interrupts 21\n6.004 - Spring 2009\n4/16/09\nInterrupt Load\nDEVICE\nKeybrd\nDisk\nPrinter\nServ.\nTime\nActual\nLatency\nMax.\nDelay\nP\nMax.\nFreq\n100/s\n500/s\n1000/\n%\nLoad\n______\n______\n______\nHow much CPU time is consumed by interrupt service?\nD\nP\nK\nP\nP\nD\nP\nP\nD\nP\nP\nD\nD\nP\nK\nP\nP\nD\nP\n10 ms. cycle\n800u*100/s = 8%\n500u*500/s = 25%\n400u*1000/s = 40%\nRemaining fraction (27%) is left over for application; trouble if its <0!\nL19 - Devices & Interrupts 22\n6.004 - Spring 2009\n4/16/09\n< 10 mS\nTask\nPeriod\nService time Deadline\nSupply ship guidance\n30ms\n5ms\n25ms\nGyroscopes\nCabin pressure\n?\nExample: Ben visits ISS\nInternational Space Station's on-board computer performs 3 tasks:\n- guiding incoming supply ships to a safe docking\n- monitoring gyros to keep solar panels properly oriented\n- controlling air pressure in the crew cabin\nAssuming a weak priority system:\n1. What is the maximum service time for \"cabin pressure\" that\nstill allows all constraints to be met?\n2. Give a weak priority ordering that meets the constraints\n3. What fraction of the time will the processor spend idle?\n4. What is the worst-case delay for each type of interrupt until\ncompletion of the corresponding service routine?\n16.6 %\n25%\n10%\nG > SSG > CP\n48.33%\nC,G = 10 + 10 + (5) = 25\nC = 10 + (10) = 20\nS,G = 5 + 10 + (10) = 25\nL19 - Devices & Interrupts 23\n6.004 - Spring 2009\n4/16/09\nExample: Ben visits ISS (cont'd)\nOur Russian collaborators don't like the sound of a \"weak\" priority\ninterrupt system and lobby heavily to use a \"strong\" priority interrupt\nsystem instead.\nTask\nPeriod\nService time Deadline\nSupply ship guidance\n30ms\n5ms\n25ms\nGyroscopes\nCabin pressure\n?\nAssuming a strong priority system, G > SSG > CP:\n1. What is the maximum service time for \"cabin pressure\" that\nstill allows all constraints to be met?\n2. What fraction of the time will the processor spend idle?\n3. What is the worst-case delay for each type of interrupt until\ncompletion of the corresponding service routine?\n100 - (3*10) - (4*5) = 50\n16.6%\n25%\n50%\n8.33%\n[G] 10 + 5\nL19 - Devices & Interrupts 24\n6.004 - Spring 2009\n4/16/09\nSummary\nDevice interface - two parts:\n- Device side: handle interrupts from device (transparent to apps)\n- Application side: handle interrupts (SVCs) from application\nScheduler interaction:\n- \"Sleeping\" (*inactive) processes waiting for device I/O\n- Handler coding issues, looping thru User mode\nReal Time constraints, scheduling, guarantees\"\n- Complex, hard scheduling problems - a black art!\n- Weak (non-preemptive) vs Strong (preemptive) priorities help...\n- Common real-world interrupt systems:\n- Fixed number (eg, 8 or 16) of strong priority levels\n- Each strong priority level can support many devices, arranged\nin a weak priority chain"
    },
    {
      "category": "Resource",
      "title": "Finite State Machines",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/13f0393a7c56840b8d15efb89f84fe65_MIT6_004s09_lec06.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n6.004 Computation Structures\nSpring 2009\n\nL06 - FSMs 1\n6.004 - Spring 2009\n2/24/09\n(Synchronous)\nFinite State Machines\nLab 2 is due Thursday\nGreat - Theory!\nFinally!\nSome ENGINEERING!\nmodified 2/23/09 09:27\nL06 - FSMs 2\n6.004 - Spring 2009\n2/24/09\nOur New Machine\nCombinational\nLogic\nCurrent\nState\nNew\nState\nInput\nOutput\nClock\nState\nRegisters\nk\nk\nm\nn\n- Acyclic graph\n- Obeys static discipline\n- Can be exhaustively enumerated by a\ntruth table of 2k+mrows and k+n output\ncolumns\n-Engineered cycles\n-Works only if dynamic\ndiscipline obeyed\n-Remembers k bits for a total\nof 2k unique combinations\nL06 - FSMs 3\n6.004 - Spring 2009\n2/24/09\nMust Respect Timing Assumptions!\nQuestions:\n- Constraints on TCD for the logic?\n- Minimum clock period?\n- Setup, Hold times for Inputs?\nCombinational\nLogic\nCurrent\nState\nNew\nState\nInput\nOutput\nClock\ntCD,L = ?\ntPD,L = 5ns\ntCD,R = 1ns\ntPD,R = 3ns\ntS,R = 2ns\ntH,R = 2ns\ntCD,L > 1 ns\ntS = tPD,L + tS,R = 7 nS\ntH = tH,R- tCD,L= 1 nS\nWe know how fast it goes... But what can it do?\ntCD,R (1 ns) + tCD,L(?) > tH,R(2 ns)\ntCLK > tPD,R+tPD,L+ tS,R > 10nS\nL06 - FSMs 4\n6.004 - Spring 2009\n2/24/09\nA simple sequential circuit...\nLets make a digital binary Combination Lock:\nSpecification:\n- One input ( \"0\" or \"1\")\n- One output (\"Unlock\" signal)\n- UNLOCK is 1 if and only if:\nLast 4 inputs were the\n\"combination\": 0110\nHow many\nregisters do\nI need?\nLock\nIN\nU\nCLK\n\nL06 - FSMs 5\n6.004 - Spring 2009\n2/24/09\nAbstraction du jour:\nFinite State Machines\n-A FINITE STATE MACHINE has\nClocked\nFSM\nm\nn\n- k STATES: S1 ... Sk (one is \"initial\" state)\n- m INPUTS: I1 ... Im\n- n OUTPUTS: O1 ... On\n- Transition Rules s'(s, I) for each state s and input I\n- Output Rules Out(s) for each state s\nL06 - FSMs 6\n6.004 - Spring 2009\n2/24/09\nState Transition Diagram\nSX\nU=0\nS0\nU=0\nS01\nU=0\nS011\nU=0\nS0110\nU=1\nXXX\nU=0\nNAME\nof state\nOUTPUT\nwhen in this\nstate\nINPUT\ncausing\ntransition\nHeavy circle\nMeans\nINITIAL state\nDesigning our lock ...\n- Need an initial state; call it SX.\n- Must have a separate state for each step\nof the proper entry sequence\n- Must handle other (erroneous) entries\nWhy do these\ngo to S0 and S01?\nL06 - FSMs 7\n6.004 - Spring 2009\n2/24/09\nYet Another Specification\nSX\nU=0\nS0\nU=0\nS01\nU=0\nS011\nU=0\nS0110\nU=1\nIN Current State Next State Unlock\nSX\nS0\nSX\nSX\nS0\nS0\nS0\nS01\nS01\nS0\nS01\nS011\nS011\nS0110 0\nS011\nSX\nS0110\nS0\nS0110\nS01\nAll state transition\ndiagrams can be\ndescribed by truth\ntables...\nBinary encodings are\nassigned to each state\n(a bit of an art)\nThe truth table can then\nbe simplified using the\nreduction techniques we\nlearned for combinational\nlogic\n00 1\n00 1\n0 1 1\n0 1 1\n0 10\n0 10\n00 1\n00 1\n0 1 1\n00 1\n0 10\n1 00\n00 1\n0 1 1\nThe assignment of codes to\nstates can be arbitrary, however,\nif you choose them carefully you\ncan greatly reduce your logic\nrequirements.\nL06 - FSMs 8\n6.004 - Spring 2009\n2/24/09\nValid State Diagrams\n-\nArcs leaving a state must be:\n-\n(1) mutually exclusive\n-\ncan't have two choices for a given input value\n-\n(2) collectively exhaustive\n-\nevery state must specify what happens for each possible input\ncombination. \"Nothing happens\" means arc back to itself.\nS1\nS3\nS2\nMOORE Machine:\nOutputs on States\nS1\nS3\n0/0\n0/1\nS2\n1/0\n1/1\n1/0\nMEALY Machine:\nOutputs on Transitions\n0/0\n\nL06 - FSMs 9\n6.004 - Spring 2009\n2/24/09\nNow put it in Hardware!\nROM\n16x4\nunlock\nNext state\nCurrent state\nIN\nTrigger update periodically (\"clock\")\n4 inputs\n24 locations\neach location supplies 4 bits\nWe assume\ninputs are\nsynchronized\nwith clock...\nL06 - FSMs 10\n6.004 - Spring 2009\n2/24/09\nDiscrete State, Time\nClock\nSTATE\nNEXT\nClock\nPeriod\nClock\nPeriod\nClock\nPeriod\nClock\nPeriod\nClock\nPeriod\nROM\nNEXT\nSTATE\ninputs\noutputs\ns\ns\ns state bits\n2s possible states\nTwo design choices:\n(1) outputs only depend on state\n(2) outputs depend on inputs + state\n(Moore)\n(Mealy)\nL06 - FSMs 11\n6.004 - Spring 2009\n2/24/09\nAsynchronous Inputs - I\nOur example assumed a single clock transition per input. What if the\n\"button pusher\" is unaware of, or not synchronized with, the clock?\nSX\nU=0\nS0\nU=0\nS01\nU=0\nS011\nU=0\nU=0\nB0\nB0\nU=0\nB1\nB1\nU=0\nB1\nB1\nU=0\nB1\nB1\nWhat if each button input is an\nasynchronous 0/1 level? How\ndo we prevent a single button\npress, e.g., from making\nseveral transitions?\nLock\nB1\nU\nB0\nUse intervening states to synchronize button presses!\nBut what\nAbout the\nDynamic\nDiscipline?\nL06 - FSMs 12\n6.004 - Spring 2009\n2/24/09\nFSM Party Games\nROM\nk\nk\n1. What can you say about the\nnumber of states?\n2. Same question:\nm\nStates\nn\nStates\nx\ny\nz\n3. Here's an FSM. Can you\ndiscover its rules?\nA finite\n\nstate machine with two states, 0 and 1.\nFigure by MIT OpenCourseWare.\n\nL06 - FSMs 13\n6.004 - Spring 2009\n2/24/09\nWhat's My Transition Diagram?\nvs.\n0=\n\"1\nThe same\n\nfinite state machine, with a person jumping from 0 to 1.\nOFF,\n1=ON?\n111\" =\nSurprise!\n- If you know NOTHING about the FSM, you're never sure!\n- If you have a BOUND on the number of states, you can discover its\nbehavior:\nK-state FSM: Every (reachable) state can be\nreached in < k steps.\nBUT ... states may be equivalent!\nL06 - FSMs 14\n6.004 - Spring 2009\n2/24/09\nFSM Equivalence\nvs.\nARE THEY DIFFERENT?\nNOT in any practical sense! They are EXTERNALLY\nINDISTINGUISHABLE, hence interchangeable.\nFSMs EQUIVALENT iff every input sequence\nyields identical output sequences.\nENGINEERING GOAL:\n- HAVE an FSM which works...\n- WANT simplest (ergo cheapest) equivalent FSM.\nL06 - FSMs 15\n6.004 - Spring 2009\n2/24/09\nLets build an Ant\n- SENSORS: antennae L and R, each 1 if in\ncontact with something.\n- ACTUATORS: Forward Step F, ten-degree\nturns TL and TR (left, right).\n8 legs?\nGOAL: Make our ant smart enough to get out of a maze like:\nSTRATEGY: \"Right antenna to the wall\"\nL06 - FSMs 16\n6.004 - Spring 2009\n2/24/09\nLost in space\n?\nLOST\nF\nL+R\n_ _\nL R\n\"lost\" is the\ninitial state\nAction: Go forward until we hit something. The same finite state machine, with a person jumping from 0 to 1.\nFigure by MIT OpenCourseWare.\n\nL06 - FSMs 17\n6.004 - Spring 2009\n2/24/09\nBonk!\nLOST\nF\nL+R\n_ _\nL R\nRCCW\nL+R\nTL\n_ _\nL R\nAction: Turn left (CCW) until we don't touch anymore\nL06 - FSMs 18\n6.004 - Spring 2009\n2/24/09\nA little to the right...\nLOST\nF\nRCCW\nL+R\nL+R\nWall1\nTR,F\nR\n_\nR\nTL\n_ _\nL R\n_ _\nL R\nAction: Step and turn right a little, look for wall\nL06 - FSMs 19\n6.004 - Spring 2009\n2/24/09\nThen a little to the left\nLOST\nF\nWall1\nTR,F\nRCCW\nR\nL+R\nL+R\n_\nR\nTL\n_ _\nL R\n_ _\nL R\n_ _\nL R\nWall2\nTL,F\nL\n_\nL R\nAction: Step and turn left a little, till not touching (again)\nL06 - FSMs 20\n6.004 - Spring 2009\n2/24/09\nDealing with corners\nLOST\nF\nWall2\nTL,F\nWall1\nTR,F\nRCCW\nR\nL+R\nL+R\n_\nR\nL\nTL\n_ _\nL R\n_ _\nL R\n_ _\nL R\n_\nL R\nTR,F\nCorner\nR\n_\nR\nAction: Step and turn right until we hit perpendicular wall\n\nL06 - FSMs 21\n6.004 - Spring 2009\n2/24/09\nEquivalent State Reduction\nObservation: Si Sjif\n1. States have identical outputs; AND\n2. Every input equivalent states.\nReduction Strategy:\nFind pairs of equivalent states, MERGE them.\nLOST\nF\nWall2\nTL,F\nWall1\nTR,F\nRCCW\nR\nL+R\nL+R\n_\nR\nL\nTL\n_ _\nL R\n_ _\nL R\n_ _\nL R\n_\nL R\nTR,F\nCorner\nR\n_\nR\nL06 - FSMs 22\n6.004 - Spring 2009\n2/24/09\nAn Evolutionary Step\nBehaves exactly as previous (5-state) FSM, but requires half the\nROM in its implementation!\nMerge equivalent states Wall1 and Corner into a single new,\ncombined state.\nLOST\nF\nWall2\nTL,F\nWall1\nTR,F\nRCCW\nR\nL+R\nL+R\n_\nR\nL\nTL\n_ _\nL R\n_ _\nL R\n_ _\nL R\n_\nL R\nL06 - FSMs 23\n6.004 - Spring 2009\n2/24/09\n00 1 - 01 0 0 1\n00 0 1 01 0 0 1\nL+R\nRCCW\nL+R\n01 1 - | 01 0 1 0\n01 0 1 | 01 0 1 0\nTL\nBuilding the Transition Table\nS L R | S' TR TL F\n-------+-----------\n00 0 0 | 00 0 0 1\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\nLOST\nF\n_ _\nL R\nL06 - FSMs 24\n6.004 - Spring 2009\n2/24/09\nImplementation Details\nS L R | S' TR TL F\n-------+-----------\n00 0 0 | 00 0 0 1\n00 1 - | 01 0 0 1\n00 0 1 | 01 0 0 1\n01 1 - | 01 0 1 0\n01 0 1 | 01 0 1 0\n01 0 0 | 10 0 1 0\n10 - 0 | 10 1 0 1\n10 - 1 | 11 1 0 1\n11 1 - | 01 0 1 1\n11 0 0 | 10 0 1 1\n11 0 1 | 11 0 1 1\nLOST\nRCCW\nWALL1\nWALL2\nS1' S1S0\n00 01 11 10\n00 0 1 1 1\nLR 01 0 0 1 1\n11 0 0 0 1\n10 0 0 0 1\nS0' S1S0\n00 01 11 10\n00 0 0 0 0\nLR 01 1 1 1 1\n11 1 1 1 1\n10 1 1 1 0\nS\nR\nL\nS\nL\nS\nS\nS\n+\n+\n=\n\nComplete Transition table\nLS\nS\nL\nR\nS\n+\n+\n=\n\nL06 - FSMs 25\n6.004 - Spring 2009\n2/24/09\nAnt Schematic\nL06 - FSMs 26\n6.004 - Spring 2009\n2/24/09\nRoboant(r)\nMaze\nselection\nFSM state\ntable\nStatus\ndisplay\nPlan view\nof maze\nSimulation\ncontrols\nFeaturing the new Mark-II ant: can add (M),\nerase (E), and sense (S) marks along its path.\nL06 - FSMs 27\n6.004 - Spring 2009\n2/24/09\nHousekeeping issues...\nROM\nor\ngates\nNEXT\nSTATE\ninputs\noutputs\ns\ns\n1. Initialization? Clear the memory?\n2. Unused state encodings?\n- waste ROM (use PLA or gates)\n- what does it mean?\n- can the FSM recover?\n3. Choosing encoding for state?\n4. Synchronizing input changes with\nstate update?\nIN\nCLK\nU\nNow, that's a funny\nlooking state machine\nL06 - FSMs 28\n6.004 - Spring 2009\n2/24/09\nTwisting you Further...\n- MORE THAN ANTS:\nSwarming, flocking, and schooling can result\nfrom collections of very simple FSMs\n- PERHAPS MOST PHYSICS:\nCellular automata, arrays of simple FSMs,\ncan more accurately model fluilds than\nnumerical solutions to PDEs\n- WHAT IF:\nWe replaced the ROM with a RAM and have\noutputs that modify the RAM?\n... You'll see FSMs for the rest of your life!\nI prefer to think we\nascended...\nDid we all descend from FSMs???"
    },
    {
      "category": "Resource",
      "title": "Interconnect & Communication",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/726c2f408a9e086ecfcd14b65211676f_MIT6_004s09_lec20.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n6.004 Computation Structures\nSpring 2009\n\nL20 - Communication 1\n6.004 - Spring 2009\n4/23/09\nInterconnect & Communication\nSpace, Time, & stuff...\nQuiz #4 tomorrow!\nmodified 4/22/09 11:14\nL20 - Communication 2\n6.004 - Spring 2009\n4/23/09\nComputer System Technologies\nWhat's the most important part of this picture?\nMother boards\nSDRAM\nLAN technology\nLinux\nWindows\nXP\nHard Disk\nDrives\nDRAM\nFlash Memory\nGraphics\nAcceleration\nActiveX\nControls\nApp\nServers\nL20 - Communication 3\n6.004 - Spring 2009\n4/23/09\nTechnology comes & goes;\ninterfaces last forever\nInterfaces typically deserve more engineering attention than the\ntechnologies they interface...\n- Abstraction: should outlast many technology generations\n- Often \"virtualized\" to extend beyond original function (e.g. memory, I/O,\nservices, machines)\n- Represent more potential value to their proprietors than the technologies\nthey connect.\nInterface sob stories:\n- Interface \"warts\": Windows \"aux.c\" bug, Big/little Endian wars\n- IBM PC debacle\n... and many success stories:\n- IBM 360 Instruction set architecture; Postscript; Compact Flash; ...\n- Backplane buses\nL20 - Communication 4\n6.004 - Spring 2009\n4/23/09\nCPU\nMEM\nI/O\nDISK\nI/O\nMEM\nAncient Times (Ad hoc connections)\nSystem Interfaces & Modularity\nLate 60s (Processor-dependent Bus)\nCPU\nMEM\nI/O\nDISK\nI/O\nMEM\n?\n80s (Processor-independent Bus)\nCPU\nCPU\nI/O\nDISK\nI/O\nMEM\nToday\nBuses Galore\nMEM\nMEM\nCPU\nDISK\nI/O\nI/O\nL2/L3 $\nGraphics\nI/O\n\"AGP\" bus\nFront-side bus\nBack-side bus\nBridge\n\nL20 - Communication 5\n6.004 - Spring 2009\n4/23/09\nInterface Standard: Backplane Bus\nModular cards that plug into\na common backplane:\nCPUs\nMemories\nBulk storage\nI/O devices\nS/W?\nThe backplane provides:\nPower\nCommon system clock\nWires for communication\nL20 - Communication 6\n6.004 - Spring 2009\n4/23/09\nThe Dumb Bus: ISA & EISA\nOriginal primitive approach --\nJust take the control\nsignals and data bus from\nthe CPU module, buffer it, and call\nit a bus.\nISA bus (Original IBM PC bus) -\nPin out and timing is nearly identical\nto the 8088 spec.\nAh, you forget,\nUnibus, S-100,\nSWTP SS-50,\nSTB, MultiBus,\nApple 2E, ...\nL20 - Communication 7\n6.004 - Spring 2009\n4/23/09\nSmarter \"Processor Independent\" Buses\nNuBus, PCI...\nIsolate basic communication\nprimitives from processor\narchitecture:\n- Simple read/write protocols\n- Symmetric: any module can\nbecome \"Master\" (smart I/O,\nmultiple processors, etc)\n- Support for \"plug & play\"\nexpansion\nGoal: vendor-independent\ninterface standard\nTERMINOLOGY -\nBUS MASTER - a module that\ninitiates a bus transaction.\n(CPU, disk controller, etc.)\nBUS SLAVE - a module that\nresponds to a bus request.\n(Memory, I/O device, etc.)\nBUS CYCLE - The period from\nwhen a transaction is\nrequested until it is served.\nPCI: initiator\nPCI: target\nhttp://www.techfest.com/hardware/bus/pci.htm\nL20 - Communication 8\n6.004 - Spring 2009\n4/23/09\nBuses, Interconnect...\nwhat's the big deal?\nAren't buses simply logic circuits with long wires?\nWires: interconnect engineer's view:\nTransmission lines.\nFinite signal propagation\nvelocity.\nSpace matters.\nTime matters.\nReality matters.\nWires: circuit theorist's view:\nEquipotential \"nodes\" of a\ncircuit.\nInstant propagation of v, i\nover entire node.\n\"space\" abstracted out of\ndesign model.\nTime issues dictated by RLC\nelements; wires are\ntimeless.\n\nL20 - Communication 9\n6.004 - Spring 2009\n4/23/09\nBus Lines as Transmission Lines\n?\n?\n?\n?\nANALOG ISSUES:\n- Propagation times\nLight travels about 1 ft / ns\n(about 7\"/ns in a wire)\n- Skew\nDifferent points along the bus see\nthe signals at different times\n- Reflections & standing waves\nAt each interface (places where\nthe propagation medium changes)\nthe signal may reflect if the\nimpedances are not matched.\nMake a transition on a long line -\nmay have to wait many transition\ntimes for echos to subside.\nL20 - Communication 10\n6.004 - Spring 2009\n4/23/09\nCoping with Analog Issues...\nWe'd like our bus to be technology independent...\n- Self-timed protocols allow bus transactions to accommodate\nvarying response times;\n- Asynchronous protocols avoid the need to pick a (technology-\ndependent) clock frequency.\ni\ni\nWIRED-OR GLITCH: what happens\nwhen a switch is opened???\nCOMMON COMPROMISE: Synchronous, Self-Timed protocols\n- Broadcast bus clock\n- Signals sampled at \"safe\" times\n* DEAL WITH: noise, clock skew (wrt signals)\nBUT... asynchronous protocols are vulnerable to analog-domain\nproblems, like the infamous\nL20 - Communication 11\n6.004 - Spring 2009\n4/23/09\nSynchronous Bus Clock Timing\nCLK\nSignal\nat source\nassertion edge\nsample edge\nSignal\nat destination\nAllow for several \"round-trip\" bus delays so that ringing can die down.\n\"Settling\nTime\"\n\"de-skew time\"\nL20 - Communication 12\n6.004 - Spring 2009\n4/23/09\nA Simple Bus Transaction\nCLK\nassertion edge\nsample edge\nstart\nfinish\noperation\naddress\ndata\nWRITE (Master)\n(Master)\n(Master)\n(Master)\n(Slave)\nMASTER:\n1) Chooses bus operation\n2) Asserts an address\n3) Waits for a slave to\nanswer.\nSLAVE:\n1) Monitors start\n2) Check address\n3) If meant for me\na) look at bus operation\nb) do operation\nc) signal finish of cycle\nBUS:\n1) Monitors start\n2) Start count down\n3) If no one answers before\ncounter reaches 0 then\n\"time out\"\n\nL20 - Communication 13\n6.004 - Spring 2009\n4/23/09\nMultiplexed Bus: Write Transaction\nCLK\noperation\nstart\nfinish\naddress\n/data\nWRITE (Master)\n(Master)\n(Slave)\nadr (Master)\ndata (Master)\nOK (Slave)\nWe let the address and data\nbuses share the same\nwires.\nSlave sends a status\nmessage by driving the\noperation control signals\nwhen it finishes. Possible\nindications:\n- request succeeded\n- request failed\n- try again\nA slave can stall the write by\nwaiting several cycles before\nasserting the finish signal.\nassertion edge\nsample edge\nMore efficient use of shared wires\nL20 - Communication 14\n6.004 - Spring 2009\n4/23/09\nMultiplexed Bus: Read Transaction\nThroughput: 3+ Clocks/word\nCLK\noperation\nstart\nfinish\naddress\n/data\nREAD (Master)\n(Master)\n(Slave)\nadr (Master)\ndata (Slave)\nOK (Slave)\nOn reads, we allot one cycle for\nthe bus to \"turn around\" (stop\ndriving and begin receiving). It\ngenerally takes some time to\nread data anyway.\nA slave can stall the read (for\ninstance if the device is slow\ncompared to the bus clock) by\nwaiting several clocks before\nasserting the finish signal.\nThese delays are sometimes\ncalled \"WAIT-STATES\"\nassertion edge\nsample edge\nTurn around time\nTurn around time\nL20 - Communication 15\n6.004 - Spring 2009\n4/23/09\nBus Arbitration: Multiple Bus Masters\nISSUES:\n- Fairness - Given uniform requests, bus cycles should be divided evenly\namong modules (to each, according to their needs...)\n- Bounded Wait - An upper bound on how long a module has to wait between\nrequesting and receiving a grant\n- Utilization - Arbitration scheme should allow for maximum bus\nperformance\n- Scalability - Fixed-cost per module (both in terms of arbitration H/W and\narbitration time.\nSTATE OF THE ART ARBITRATION: N masters, log N time, log N wires.\nRequest\nGrant In\nGrant Out\nRequest\nGrant In\nGrant Out\nRequest\nGrant In\nGrant Out\nRequest\nGrant In\nGrant Out\nModule 1\nModule 2\nModule 3\nModule 4\nRequest\n\"Daisy-Chain Arbitration\"\nL20 - Communication 16\n6.004 - Spring 2009\n4/23/09\nMeanwhile, outside the box...\nThe Network as an interface standard\nApplication\nSession\nTCP\nUDP\nIP\nEthernet\nToken Ring\nPhysical\nNetwork\nTransport\nEMERGING IDEA: Protocol\n\"layers\" that isolate\napplication-level interface\nfrom low-level physical\ndevices:\nETHERNET: In the mid-70's Bob\nMetcalf (at Xerox PARC, an MIT\nalum) devised a bus for networking\ncomputers together.\n-Inspired by Aloha net (radio)\n-COAX replaced \"ether\"\n-Bit-serial (optimized for long wires)\n-Variable-length \"packets\":\n- self-clocked data (no clock, skew!)\n- header (dest), data bits\n-Issues: sharing, contention, arbitration,\n\"backoff\"\n\nL20 - Communication 17\n6.004 - Spring 2009\n4/23/09\nSerial, point-to-point communications....\nBecoming standard at all levels?\nETHERNET: Broadcast technology\n-Sharing (contention) issues\n-Multiple-drop-point issues...\nEvolution: Point-to-point\n-10BaseT, separate R & T wires\n-Each link shared by only 2 hosts\n-Network riddled with switches,\nrouters\nSerial point-to-point bus replacements\n-Multi Gbit/sec serial links!\n-PCIe, Infiniband, SATA, ...\n-Packets, headers\n-Switches, routing\n-Trend: localized, superfast, serial\nnetworks!\nL20 - Communication 18\n6.004 - Spring 2009\n4/23/09\nBuses & Bridges in Today's Computers\nDiagr\nam of\nbuses\nand b\nrid\nges\nin a comp\nuter.\nSerial ATA\nFirewire\nUSB 1.1/2.0\n...\nPCI Express x16...\n(point-to-point)\nL20 - Communication 19\n6.004 - Spring 2009\n4/23/09\nGeneralizing Buses...\nCommunication Topologies\nRING\n(n) steps for random message delivery\nBUS\nONE step for random message delivery (but\nonly one message at a time)\n1-dimensional approaches:\n\"Low cost networks\" - constant cost/node\nL20 - Communication 20\n6.004 - Spring 2009\n4/23/09\nQuadratic-cost Topologies\nCOMPLETE GRAPH:\nDedicated lines connecting each pair of\ncommunicating nodes. (n) simultaneous\ncommunications.\nCROSSBAR SWITCH:\n- Switch dedicated between each pair of\nnodes\n- Each Ai can be connected to one Bj at any\ntime\n- Special cases:\n- A = processors, B = memories\n- A, B same type of node\n- A, B same nodes (complete graph)\nFigure by MIT OpenCourseWare.Four computers.\nFigure by MIT OpenCourseWare.\nCo\nm\np\nu\nters, switches, and routers.\nFigure by MIT OpenCourseWare.\n\nL20 - Communication 21\n6.004 - Spring 2009\n4/23/09\nMesh Topologies\n2-Dimensional Meshes\n4-Neighbor\n8-Neighbor\n3-D, 6-Neighbor Mesh\nNearest-neighbor connectivity:\nPoint-to-point interconnect\n- minimizes delays\n- minimizes \"analog\" effects\nStore-and-forward\n(some overhead associated\nwith communication routing)\nL20 - Communication 22\n6.004 - Spring 2009\n4/23/09\nLogarithmic Latency Networks\nBINARY TREE:\nMaximum path length is (log n) steps;\nCost/node constant.\nHYPERCUBE (n-cube):\nCost = (n log n)\nWorst-case path length = (log n)\nL20 - Communication 23\n6.004 - Spring 2009\n4/23/09\nCommunication Topologies: Latency\nTheorist's view:\n- Each point-to-point link requires one hardware unit.\n- Each point-to-point communication requires one time unit.\nIS IT REAL?\n- Speed of Light: ~ 1 ns/foot (typical bus propagation: 5 ns/foot)\n- Density limits: can a node shrink forever? How about Power, Heat, etc ... ?\nOBSERVATION: Links on Tree, N-cube must grow with n; hence time/link must grow.\nTopology\n$\nComplete Graph\nCrossbar\n1D Bus\n2D Mesh\n3D Mesh\nTree\nN-cube\n( n 2 )\n( n )\n( n 2 )\n( n )\n( n )\n( n )\n(\n)\nn log n\nTheoretical\nLatency\n(\nn\n)\n(\nn )\n( 1 )\n( 1 )\n( 1 )\n( log\nn )\n( log\nn )\n\n(\nn\n)\n\n(\nn\n)\n\n(\nn\n)\n( n )\n( n )\nActual\nLatency\nL20 - Communication 24\n6.004 - Spring 2009\n4/23/09\nCommunications Futures\nThe Old Standbys:\n- In box: Backplane buses: parallel, shared data paths\n- Arbitration, skew problems\n- Local area: shared, single \"ether\" cable\n- Contention, collisions\nNew \"switched fabric\" tech (in & out of box):\n- Shared wires replaced by point-to-point serial\n- Parallel data paths replaced by serial \"packets\"\n- Communication network extended via active switches\nTopological Invariants:\n- Asymptotic performance/cost tradeoffs...\n- Log-latency topologies: a useful fiction\n- Best-case scaling with 3D mesh\nWatch this space!\n- Technologies: optical, proximity, ....\n- 3D packaging, interconnect\n- ???\nSATA\nEISA\nPCIe\nPCI\nInfiniband\nFireWire\nRDRAM\nSDRAM\nUSB"
    },
    {
      "category": "Resource",
      "title": "Machine Language, Assemblers, and Compilers",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-004-computation-structures-spring-2009/e4ee1fcae3d3281fe783974257f788cc_MIT6_004s09_lec11.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n6.004 Computation Structures\nSpring 2009\n\nL11 - Machine Language 1\n6.004 - Spring 2009\n3/12/09\nMachine Language, Assemblers,\nand Compilers\nWhen I find my code in tons of trouble,\nFriends and colleagues come to me,\nSpeaking words of wisdom:\n\"Write in C.\"\nLong, long, time ago, I can still remember\nhow mnemonics used to make me smile...\nAnd I knew that with just the opcode names\nthat I could play those BSim games\nand maybe hack some macros for a while.\nBut 6.004 gave me shivers\nwith every lecture they delivered.\nBad news at the door step,\nI couldn't read one more spec.\nI can't remember if I tried\nto get Factorial optimized,\nBut something touched my nerdish pride\nthe day my Beta died.\nAnd I was singing...\nReferences (on web site):\nDocumentation\nBSIM reference\nNotes on C Language\nQuiz 2 TOMORROW!\nmodified February 23, 09\nL11 - Machine Language 2\n6.004 - Spring 2009\n3/12/09\nMachine Language: 32-bit instructions\nRa and Rb are the operands,\nRc is the destination.\nR31 reads as 0, unchanged by writes\narithmetic: ADD, SUB, MUL, DIV\ncompare: CMPEQ, CMPLT, CMPLE\nboolean: AND, OR, XOR\nshift: SHL, SHR, SRA\narithmetic: ADDC, SUBC, MULC, DIVC\ncompare: CMPEQC, CMPLTC, CMPLEC\nboolean: ANDC, ORC, XORC\nshift: SHLC, SHRC, SRAC\nbranch: BNE/BT, BEQ/BF (const = word displacement from PCNEXT)\njump: JMP (const not used)\nmemory access: LD, ST (const = byte offset from Reg[ra])\nTwo's complement 16-bit constant for\nnumbers from -32768 to 32767;\nsign-extended to 32 bits before use.\nOPCODE\nrc\nra\nrb\nunused\nOPCODE\nrc\nra\n16-bit signed constant\nHow can we improve the programmability of the Beta?\nL11 - Machine Language 3\n6.004 - Spring 2009\n3/12/09\nEncoding Binary Instructions\nMeans, to BETA, Reg[4] = Reg[2] + Reg[3]\nOpCode\nRb\nRa\n(unused)\nRc\n32-bit (4-byte) ADD instruction:\nBut, most of us would prefer to write\nADD(R2, R3, R4)\na = b+c;\nor, better yet,\n(ASSEMBLER)\n(High Level Language)\nSoftware Approaches: INTERPRETATION, COMPILATION\nL11 - Machine Language 4\n6.004 - Spring 2009\n3/12/09\nStructure\nLanguage\nApplication\nApplic Lang\n(())()\n(()())\n(())()\nAPPLICATION\nM2\n- Result: a \"virtual\" M2\nInterpretation\nTuring's model of Interpretation:\nM1\n- Start with some hard-to-program\nuniversal machine, say M1\nPgm\n- Write a single program for M1which mimics\nthe behavior of some easier machine, say\nM2\n\"Layers\" of interpretation:\n- Often we use several layers of\ninterpretation to achieve\ndesired behavior, eg:\nDATA\nScheme Interp\nScheme\nSCHEME\nHardware\nX86 Instrs\nX86\n- X86 (Pentium), running\n- Scheme, running\n- Application, interpreting\n- Data.\n\nL11 - Machine Language 5\n6.004 - Spring 2009\n3/12/09\nCompilation\nModel of Compilation:\nM1\n- Given some hard-to-program\nmachine, say M1...\nP2\n- Find some easier-to-program language L2\n(perhaps for a more complicated machine,\nM2); write programs in that language\nP1\nC2-1\n- Build a translator (compiler) that translates programs from M2's language to\nM1's language. May run on M1, M2, or some other machine.\nInterpretation & Compilation: two tools for improving programmability ...\n- Both allow changes in the programming model\n- Both afford programming applications in platform (e.g., processor) independent\nlanguages\n- Both are widely used in modern computer systems!\nL11 - Machine Language 6\n6.004 - Spring 2009\n3/12/09\nInterpretation vs Compilation\nInterpretation\nCompilation\nHow it treats input \"x+2\"\ncomputes x+2\ngenerates a program that\ncomputes x+2\nWhen it happens\nDuring execution\nBefore execution\nWhat it complicates/slows\nProgram Execution\nProgram Development\nDecisions made at\nRun Time\nCompile Time\nMajor design choice we'll see repeatedly:\ndo it at Compile time or at Run time?\nThere are some characteristic differences between these two\npowerful tools...\nL11 - Machine Language 7\n6.004 - Spring 2009\n3/12/09\nSoftware: Abstraction Strategy\nInitial steps: compilation tools\nAssembler (UASM):\nsymbolic representation\nof machine language\nHides: bit-level representations,\nhex locations, binary values\nCompiler (C): symbolic\nrepresentation of\nalgorithm\nHides: Machine instructions,\nregisters, machine\narchitecture\nSubsequent steps: interpretive tools\nOperating system\nHides: Resource (memory, CPU,\nI/O) limitiations and details\nApps (e.g., Browser)\nHides: Network; location; local\nparameters\nL11 - Machine Language 8\n6.004 - Spring 2009\n3/12/09\nAbstraction step 1:\nA Program for Writing Programs\nUASM - the 6.004 (Micro) Assembly Language\nUASM\nPGM\n.....\nSymbolic\nSOURCE\ntext file\nBinary\nMachine\nLanguage\nUASM\nTranslator\nprogram\n(built into BSIM)\nUASM:\n1. A Symbolic LANGUAGE for representing strings of bits\n2. A PROGRAM (\"assembler\" = primitive compiler) for translating\nUASM source to binary.\nSTREAM of Bytes\nto be loaded\ninto memory\n\nL11 - Machine Language 9\n6.004 - Spring 2009\n3/12/09\nUASM Source Language\nA UASM SOURCE FILE contains, in symbolic text, values of\nsuccessive bytes to be loaded into memory... e.g. in\n37 -3 255\n0x25\n0b100101\ndecimal (default);\nbinary (note the \"0b\" prefix);\nhexadecimal (note the \"0x\" prefix);\nValues can also be expressions; eg, the source file\n37+0b10-0x10 24-0x1 4*0b110-1 0xF7&0x1F\ngenerates 4 bytes of binary output, each with the value 23!\nL11 - Machine Language 10\n6.004 - Spring 2009\n3/12/09\nSymbolic Gestures\nWe can also define SYMBOLS for use in source programs:\nx = 0x1000\n| A variable location\ny = 0x1004\n| Another variable\n| Symbolic names for registers:\nR0 = 0\nR1 = 1\n...\nR31 = 31\n. = 0x100\n| Assemble into 100\n1 2 3 4\nfive = .\n| Symbol \"five\" is 0x104\n5 6 7 8\n. = .+16\n| Skip 16 bytes\n9 10 11 12\nSpecial variable \".\" (period) means next byte address to be filled:\nA \"bar\" denotes\nthe beginning of\na comment...\nThe remainder\nof the line is\nignored\nL11 - Machine Language 11\n6.004 - Spring 2009\n3/12/09\nLabels (Symbols for Addresses)\nLABELS are symbols that represent memory addresses.\nThey can be set with the following special syntax:\nx: is an abbreviation for \"x = .\"\n. = 0x1000\nsqrs:\n0 1 4 9\n16 25 36 49\n64 81 100 121\n144 169 196 225\nslen:\n.-sqrs\nAn Example--\n---- MAIN MEMORY ----\n1000: 09 04 01 00\n1004: 31 24 19 10\n1008: 79 64 51 40\n100c: E1 C4 A9 90\n1010: 10 ... ... ... ...\nL11 - Machine Language 12\n6.004 - Spring 2009\n3/12/09\nMighty Macroinstructions\n| Macro to generate 4 consecutive bytes:\n.macro consec(n) n n+1 n+2 n+3\n| Invocation of above macro:\nconsec(37)\nMacros are parameterized abbreviations, or shorthand\nHas same effect as:\n| Assemble into bytes, little-endian (least-sig byte 1st)\n.macro WORD(x) x%256 (x/256)%256\n.macro LONG(x) WORD(x) WORD(x >> 16)\nHere are macros for breaking multi-byte data types into byte-sized chunks\nHas same effect as:\n0xef\n0xbe\n0xad\n0xde\nBoy, that's hard to read.\nMaybe, those big-endian\ntypes do have a point.\nMem: 0x100 0x101 0x102 0x103\n. = 0x100\nLONG(0xdeadbeef)\n\nL11 - Machine Language 13\n6.004 - Spring 2009\n3/12/09\nAssembly of Instructions\n| Assemble Beta op instructions\n.macro betaop(OP,RA,RB,RC) {\n.align 4\nLONG((OP<<26)+((RC%32)<<21)+((RA%32)<<16)+((RB%32)<<11))\n}\n| Assemble Beta opc instructions\n.macro betaopc(OP,RA,CC,RC) {\n.align 4\nLONG((OP<<26)+((RC%32)<<21)+((RA%32)<<16)+(CC % 0x10000))\n}\n| Assemble Beta branch instructions\n.macro betabr(OP,RA,RC,LABEL)\nbetaopc(OP,RA,((LABEL-(.+4))>>2),RC)\nOPCODE\nRC\nRA\nRB\nUNUSED\nOPCODE\nRC\nRA\n16-BIT SIGNED CONSTANT\n\".align 4\" ensures instructions will begin on\nword boundary (i.e., address = 0 mod 4)\nArrgh!\nFor Example:\nADDC(R15, -32768, R0) --> betaopc(0x30,15,-32768,0)\nL11 - Machine Language 14\n6.004 - Spring 2009\n3/12/09\nFinally, Beta Instructions\n| BETA Instructions:\n.macro ADD(RA,RB,RC)\nbetaop(0x20,RA,RB,RC)\n.macro ADDC(RA,C,RC)\nbetaopc(0x30,RA,C,RC)\n.macro AND(RA,RB,RC)\nbetaop(0x28,RA,RB,RC)\n.macro ANDC(RA,C,RC)\nbetaopc(0x38,RA,C,RC)\n.macro MUL(RA,RB,RC)\nbetaop(0x22,RA,RB,RC)\n.macro MULC(RA,C,RC)\nbetaopc(0x32,RA,C,RC)\n-\n-\n-\n.macro LD(RA,CC,RC)\nbetaopc(0x18,RA,CC,RC)\n.macro LD(CC,RC)\nbetaopc(0x18,R31,CC,RC)\n.macro ST(RC,CC,RA)\nbetaopc(0x19,RA,CC,RC)\n.macro ST(RC,CC)\nbetaopc(0x19,R31,CC,RC)\n-\n-\n-\n.macro BEQ(RA,LABEL,RC) betabr(0x1D,RA,RC,LABEL)\n.macro BEQ(RA,LABEL)\nbetabr(0x1D,RA,r31,LABEL)\n.macro BNE(RA,LABEL,RC) betabr(0x1E,RA,RC,LABEL)\n.macro BNE(RA,LABEL)\nbetabr(0x1E,RA,r31,LABEL)\nConvenience macros\nso we don't have to\nspecify R31...\n(from beta.uasm)\nL11 - Machine Language 15\n6.004 - Spring 2009\n3/12/09\nExample Assembly\nADDC(R3,1234,R17)\nbetaopc(0x30,R3,1234,R17)\nexpand ADDC macro with RA=R3, C=1234, RC=R17\n.align 4\nLONG((0x30<<26)+((R17%32)<<21)+((R3%32)<<16)+(1234 % 0x10000))\nexpand betaopc macro with OP=0x30, RA=R3, CC=1234, RC=R17\nWORD(0xC22304D2) WORD(0xC22304D2 >> 16)\nexpand LONG macro with X=0xC22304D2\n0xC22304D2%256 (0xC22304D2/256)%256 WORD(0xC223)\nexpand first WORD macro with X=0xC22304D2\n0xD2 0x04 0xC223%256 (0xC223/256)%256\nevaluate expressions, expand second WORD macro with X=0xC223\n0xD2 0x04 0x23 0xC2\nevaluate expressions\nL11 - Machine Language 16\n6.004 - Spring 2009\n3/12/09\nDon't have it? Fake it!\n.macro MOVE(RA,RC)\nADD(RA,R31,RC)\n| Reg[RC] <- Reg[RA]\n.macro CMOVE(CC,RC)\nADDC(R31,C,RC)\n| Reg[RC] <- C\n.macro COM(RA,RC)\nXORC(RA,-1,RC)\n| Reg[RC] <-\n~Reg[RA]\n.macro NEG(RB,RC)\nSUB(R31,RB,RC)\n| Reg[RC] <-\n-Reg[RB]\n.macro NOP()\nADD(R31,R31,R31) | do nothing\n.macro BR(LABEL)\nBEQ(R31,LABEL)\n| always branch\n.macro BR(LABEL,RC)\nBEQ(R31,LABEL,RC)\n| always\nbranch\n.macro CALL(LABEL)\nBEQ(R31,LABEL,LP)\n| call\nsubroutine\n.macro BF(RA,LABEL,RC)\nBEQ(RA,LABEL,RC) | 0 is false\n.macro BF(RA,LABEL)\nBEQ(RA,LABEL)\n.macro BT(RA,LABEL,RC)\nBNE(RA,LABEL,RC) | 1 is true\n.macro BT(RA,LABEL)\nBNE(RA,LABEL)\n| Multi-instruction sequences\n.macro PUSH(RA)\nADDC(SP,4,SP) ST(RA,-4,SP)\n.macro POP(RA)\nLD(SP,-4,RA) ADDC(SP,-4,SP)\nConvenience macros can be used to extend our assembly language:\n(from beta.uasm)\n\nL11 - Machine Language 17\n6.004 - Spring 2009\n3/12/09\nAbstraction step 2:\nHigh-level Languages\nMost algorithms are naturally\nexpressed at a high level. Consider the\nfollowing algorithm:\nWe've used (and will continue to use\nthroughout 6.004) C, a \"mature\" and\ncommon systems programming lanugage.\nModern popular alternatives include C++,\nJava, Python, and many others.\nWhy use these, not assembler?\n-\nreadable\n-\nconcise\n-\nunambiguous\n-\nportable\n(algorithms frequently outlast\ntheir HW platforms)\n-\nReliable (type checking, etc)\nReference: C handout (6.004 web site)\nstruct Employee\n{ char *Name;\n/* Employee's name. */\nlong Salary; /* Employee's salary. */\nlong Points;}/* Brownie points. */\n/* Annual raise program. */\nRaise(struct Employee P[100])\n{ int i = 0;\nwhile (i < 100)\n{ struct Employee *e = &P[i];\ne->Salary =\ne->Salary + 100 + e->Points;\ne->Points = 0; /* Start over! */\ni = i+1;\n}\n}\nL11 - Machine Language 18\n6.004 - Spring 2009\n3/12/09\nHow Compilers Work\nContemporary compilers go far\nbeyond the macro-expansion\ntechnology of UASM. They\n- Perform sophisticated analyses\nof the source code\n- Invoke arbitrary algorithms to\ngenerate efficient object code for\nthe target machine\n- Apply \"optimizations\" at both\nsource and object-code levels to\nimprove run-time efficiency.\nCompilation to unoptimized code is\npretty straightforward... following is\na brief glimpse.\nWhat\ncompilers do\nis\nnot\nall that\ncomplicated.\n(at least, in principle)\nL11 - Machine Language 19\n6.004 - Spring 2009\n3/12/09\nCompiling Expressions\nC code:\nint x, y;\ny = (x-3)*(y+123456)\nBeta assembly code:\nx:\nLONG(0)\ny:\nLONG(0)\nc:\nLONG(123456)\n...\nLD(x, r1)\nSUBC(r1,3,r1)\nLD(y, r2)\nLD(C, r3)\nADD(r2,r3,r2)\nMUL(r2,r1,r1)\nST(r1,y)\nc:\nx:\ny:\n- VARIABLES are assigned\nmemory locations and\naccessed via LD or ST\n- OPERATORS translate to ALU\ninstructions\n- SMALL CONSTANTS translate\nto \"literal-mode\" ALU\ninstructions\n- LARGE CONSTANTS translate\nto initialized variables\nL11 - Machine Language 20\n6.004 - Spring 2009\n3/12/09\nData Structures: Arrays\nint Hist[100];\n...\nHist[score] += 1;\nhist:\n.=.+4*100 | Leave room for 100 ints\n...\n<score in r1>\nMULC(r1,4,r2) | index -> byte offset\nLD(r2,hist,r0) | hist[score]\nADDC(r0,1,r0) | increment\nST(r0,hist,r2) | hist[score]\nMemory:\nhist:\nscore\nHist[score]\nThe C source code\nmight translate to:\nAddress:\nCONSTANT base address +\nVARIABLE offset computed from index\n\nL11 - Machine Language 21\n6.004 - Spring 2009\n3/12/09\nData Structures: Structs\nstruct Point\n{ int x, y;\n} P1, P2, *p;\n...\nP1.x = 157;\n...\np = &P1;\np->y = 157;\nMemory:\nP1:\nP1.x\nP1.y\nP2:\nP2.x\nP2.y\nP1: .=.+8\nP2: .=.+8\nx=0 | Offset for x component\ny=4 | Offset for y component\n...\nCMOVE(157,r0) | r0 <- 157\nST(r0,P1+x) | P1.x = 157\n...\n<p in r3>\nST(r0,y,r3) | p->y = 157;\nmight translate to:\nAddress:\nVARIABLE base address +\nCONSTANT component offset\nL11 - Machine Language 22\n6.004 - Spring 2009\n3/12/09\nConditionals\nC code:\nif (expr)\n{\nSTUFF1\n}\nelse\n{\nSTUFF2\n}\nBeta assembly:\n(compile expr into rx)\nBF(rx, Lelse)\n(compile STUFF1)\nBR(Lendif)\nLelse:\n(compile STUFF2)\nLendif:\nC code:\nif (expr)\n{\nSTUFF\n}\nBeta assembly:\n(compile expr into rx)\nBF(rx, Lendif)\n(compile STUFF)\nLendif:\nThere are little tricks\nthat come into play when\ncompiling conditional\ncode blocks. For\ninstance, the\nstatement:\nif (y > 32)\n{\nx = x + 1;\n}\ncompiles to:\nLD(y,R1)\nCMPLEC(R1,32,R1)\nBT(R1,Lendif)\nADDC(R2,1,R2)\nLendif:\nthere's no\n>32\ninstruction!\nL11 - Machine Language 23\n6.004 - Spring 2009\n3/12/09\nLoops\nBeta assembly:\nLwhile:\n(compile expr into rx)\nBF(rx,Lendwhile)\n(compile STUFF)\nBR(Lwhile)\nLendwhile:\nC code:\nwhile (expr)\n{\nSTUFF\n}\nAlternate Beta\nassembly:\nBR(Ltest)\nLwhile:\n(compile STUFF)\nLtest:\n(compile expr into rx)\nBT(rx,Lwhile)\nLendwhile:\nCompilers spend a lot of time optimizing in and around loops.\n- moving all possible computations outside of loops\n- \"unrolling\" loops to reduce branching overhead\n- simplifying expressions that depend on \"loop variables\"\nMove the test\nto the end of the\nloop and branch\nthere the first\ntime thru...\nsaves a branch\nL11 - Machine Language 24\n6.004 - Spring 2009\n3/12/09\nOur Favorite Program\nn: LONG(20)\nr: LONG(0)\nstart:\nADDC(r31, 1, r0)\nST(r0, r)\nloop:\nLD(n, r1)\nCMPLT(r31, r1, r2)\nBF(r2, done)\nLD(r, r3)\nLD(n,r1)\nMUL(r1, r3, r3)\nST(r3, r)\nLD(n,r1)\nSUBC(r1, 1, r1)\nST(r1, n)\nBR(loop)\ndone:\nCleverness:\nNone...\nstraightforward\ncompilation\n(11 instructions in loop...)\nOptimizations\nare what make\ncompilers\ncomplicated\ninteresting!\nint n = 20, r;\nr = 1;\nwhile (n > 0)\n{\nr = r*n;\nn = n-1;\n}\n\nL11 - Machine Language 25\n6.004 - Spring 2009\n3/12/09\nOptimizations\nn: LONG(20)\nr: LONG(0)\nstart:\nADDC(r31, 1, r0)\nST(r0, r)\nLD(n,r1)\n| keep n in r1\nLD(r,r3)\n| keep r in r3\nloop:\nCMPLT(r31, r1, r2)\nBF(r2, done)\nMUL(r1, r3, r3)\nSUBC(r1, 1, r1)\nBR(loop)\ndone:\nST(r1,n)\n| save final n\nST(r3,r)\n| save final r\nCleverness:\nWe move LDs/STs\nout of loop!\n(Still, 5 instructions in loop...)\nint n = 20, r;\nr = 1;\nwhile (n > 0)\n{\nr = r*n;\nn = n-1;\n}\nL11 - Machine Language 26\n6.004 - Spring 2009\n3/12/09\nReally Optimizing...\nn: LONG(20)\nr: LONG(0)\nstart:\nLD(n,r1) | keep n in r1\nADDC(r31,1,r3) | keep r in r3\nBEQ(r1, done) | why?\nloop:\nMUL(r1, r3, r3)\nSUBC(r1, 1, r1)\nBNE(r1,loop)\ndone:\nST(r1,n)\n| save final n\nST(r3,r)\n| save final r\nCleverness:\nWe avoid overhead\nof conditional!\n(Now 3 instructions in loop...)\nUNFORTUNATELY,\n20! = 2,432,902,008,176,640,000 > 261 (overflows!)\nbut 12! = 479,001,600 = 0x1c8cfc00\nint n = 20, r;\nr = 1;\nwhile (n > 0)\n{ r = r*n;\nn = n-1;\n}\nL11 - Machine Language 27\n6.004 - Spring 2009\n3/12/09\nComing Attractions:\nProcedures & Stacks"
    }
  ]
}