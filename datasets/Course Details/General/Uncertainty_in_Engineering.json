{
  "course_name": "Uncertainty in Engineering",
  "course_description": "This course gives an introduction to probability and statistics, with emphasis on engineering applications. Course topics include events and their probability, the total probability and Bayes’ theorems, discrete and continuous random variables and vectors, uncertainty propagation and conditional analysis. Second-moment representation of uncertainty, random sampling, estimation of distribution parameters (method of moments, maximum likelihood, Bayesian estimation), and simple and multiple linear regression. Concepts illustrated with examples from various areas of engineering and everyday life.",
  "topics": [
    "Engineering",
    "Civil Engineering",
    "Mathematics",
    "Probability and Statistics",
    "Engineering",
    "Civil Engineering",
    "Mathematics",
    "Probability and Statistics"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nRecitations: 1 session / week, 2 hours / session\n\nCourse Description\n\nThis course gives an introduction to probability and statistics, introducing students to quantitative uncertainty analysis and risk assessment with emphasis on engineering applications. The course focuses on probability theory and its applications, with a smaller module at the end covering basic topics in statistics (parameter estimation, hypothesis testing and regression analysis). The probability modules cover events and their probability, the total probability and Bayes' theorems, discrete and continuous random variables and vectors, the Bernoulli trial sequence and Poisson process models, conditional distributions, functions of random variables and vectors, statistical moments, second-moment uncertainty propagation and second-moment conditional analysis, and various probability models such as the exponential, gamma, normal, lognormal, uniform, beta and extreme-type distributions. Throughout the subjects, concepts are illustrated with examples from various areas of engineering and everyday life.\n\nPrerequisites\n\nNone in probability and statistics, but familiarity with elementary linear algebra (vectors, matrices) and calculus (derivatives, integrals) is assumed.\n\nTextbook\n\nAng, Alfredo H-S., and Wilson H. Tang.\nProbability Concepts in Engineering: Emphasis on Applications to Civil and Environmental Engineering\n. 2nd ed. New York, NY: John Wiley & Sons, 2006. ISBN: 9780471720645.\n\nGrading\n\nACTIVITIES\n\nWEIGHTS\n\nHomework\n\n1/3\n\nMini quizzes (open books and notes)\n\n1/3\n\nFinal exam (open books and notes)\n\n1/3\n\nGroup Work\n\nStudents are encouraged to discuss all course material with one another except for homework assignments, which should be individually done. Interaction to solve suggested problems is appropriate.\n\nUse of Old Solutions\n\nStudents should not seek to obtain previous years' solutions to homeworks. If they have knowledge of such solutions, they should so indicate for each homework problem. Problems for which students had previous knowledge of the solutions will not be used for grading.",
  "files": [
    {
      "category": "Resource",
      "title": "Homework Set #1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/1-010-uncertainty-in-engineering-fall-2008/43f9e019b165e42a9ad17ffb8a37c95e_homework_01.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n1.010 Uncertainty in Engineering\nFall 2008\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n1.010 Fall 2008\nHomework Set #1\n1. If the occurrences of earthquakes and high winds are unrelated, and if, at a particular\nlocation, the probability of a \"high\" wind occurring throughout any single minute is 10-5\nand the probability of a \"moderate\" earthquake during any single minute is 10-8:\n(a) Find the probability of the joint occurrence of the two events during any minute.\nBuilding codes do not require the engineer to design the building for the\ncombined effects of these loads. Is this reasonable?\n(b) Find the probability of the occurrence of one or the other or both during any\nminute. For rare events, i.e., events with small probabilities of occurrence, the\nengineer frequently assumes: P(A\"B) ~ P(A) + P(B). Give a comment.\n(c) If the events in successive minutes are mutually independent, what is the\nprobability that there will be no moderate earthquakes in a year near this location?\nIn 10 years?\n2. When tossing a fair dice, the probability of each outcome 1,2,3,4,5,6 is the same, and\ntherefore, is 1/6. Let A, B, and C be the events\nA: outcome is 1 or 4\nB: outcome is 1,2, or 3\nC: outcome is an even number\nCalculate\n(a) P[A], P[B], P[C]\n(b) P[A|B], P[A|C], P[C|B]\n(c) P[A|(B\"C)]\nAre the following statements true or false?\n(d) A is independent of B.\n(e) A is independent of C.\n(f) B is independent of C.\n(g) A is independent of (B\"C).\n(h) (A, B, C) are mutually independent.\n3. Read Application Example 1 - Section 1, and do Problem 1.1.\n4. Read Application Example 2, and do Problem 2.2."
    },
    {
      "category": "Resource",
      "title": "Homework Set #2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/1-010-uncertainty-in-engineering-fall-2008/b6f76c02e751f811cd5f44d64d9d134b_homework_02.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n1.010 Uncertainty in Engineering\nFall 2008\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n1.010 Fall 2008\nHomework Set #2\nDue September 25, 2008 (in class)\n1. There are three modes of transporting materials from Boston to Chicago: by plane, by\nhighway, and by rail. About half of the materials are transported by highway, 35% by\nrail, and 15% by plane. The percentages of damaged cargo are respectively 12% by\nhighway, 6% by rail, and 2% by plane.\na) What percentage of all cargoes may be expected to be damaged?\nb) If a damaged cargo is received, what is the probability that it was shipped by\nhighway?\n2. You play \"Monopoly\" with a less experienced friend. The probability you win each\ngame is p=0.6 and the outcomes of different games are independent.\na) Evaluate the probability of winning 2 games out of 5.\nb) If you play until you win twice and then stop, what is the probability that you play\nexactly 3 times?\n3. The Caspian Sea is a large lake with no outlet. Hence its water level fluctuates widely\nas a result of anomalous yearly water inflow, precipitation and evaporation. During wet\nyears, the Caspian Sea level (CSL) increases by 0.3m and in dry years it decreases by\n0.2m. Given that different years are independently wet or dry with probability Pwet=0.4\nand Pdry=0.6, find the probability mass function of the change in CSL over a period of 4\nyears.\n4. Read Application Examples 3, 4, and 5. (No problem assignment, but you may try on\nyour own.)"
    },
    {
      "category": "Resource",
      "title": "Homework Set #3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/1-010-uncertainty-in-engineering-fall-2008/b72e316b363219f54fb0270c86984343_homework_03.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n1.010 Uncertainty in Engineering\nFall 2008\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n1.010 Fall 2008\nHomework Set #3\nDue October 2, 2008 (in class)\n1. The service stations along a highway are located according to a Poisson process with\nan average of 1 service station in 10 miles. Because of a gas shortage, there is a\nprobability of 0.3 that a service station would have no gasoline available. Assume that the\navailabilities of gasoline at different service stations are statistically independent.\n(a) What is the probability that there is at most 1 service station in the next 15 miles\nof highway?\n(b) What is the probability that none of the next 3 stations has gasoline for sale?\n(c) A driver on this highway notices that the fuel gauge in his car reads empty; from\nexperience he knows that he can go another 15 miles. What is the probability that\nhe will be stranded on the highway without gasoline?1\n2. A random variable X has probability density function:\na\nf\nx\n, for 1 \" x \" 2\nX ( ) = x\n= 0, otherwise\nwhere α is a parameter.\na) Calculate α and plot fX(x)\nb) Find and plot the CDF of X.\n1 An important result for Poisson processes is that, if a Poisson process with rate λ is \"thinned\" randomly\nwith probability p (meaning that each point of the process is eliminated with probability p independently of\nthe other points), then the remaining points still form a Poisson process with a reduced mean rate of (1-p)λ.\nApply this result to answer question 1(c).\n\n3. A set of earthquake occurrence times (in years since the beginning of recording t =0) is\ngiven below. The mean recurrence rate λ may be estimated as the total number of events\ndivided by the observation period, in this case λ=50/101.74≈0.5 events/yr.\nEarthquake occurences\nno.\nt\nno.\nt\nno.\nt\nno.\nt\nno.\nt\n3.61\n19.01\n36.87\n54.57\n76.83\n5.22\n19.44\n40.53\n54.70\n84.62\n6.74\n21.81\n45.66\n55.32\n85.90\n6.83\n23.44\n47.98\n57.30\n86.03\n7.23\n23.71\n48.30\n57.63\n87.85\n11.04\n27.84\n48.75\n58.88\n90.41\n13.20\n28.41\n48.81\n61.96\n91.10\n15.90\n31.01\n49.22\n67.86\n91.34\n16.14\n32.23\n49.27\n72.35\n95.66\n17.21\n33.30\n50.28\n74.17\n101.74\na) Construct a histogram of the earthquake interarrival time and compare with the\nexponential PDF with parameter λ.\nb) Find the empirical distribution of N= number of earthquakes in T=4 years and\ncompare it with the Poisson distribution with λΤ=4λ=2\nc) Would you reasonably conclude that the occurrence of earthquakes follows a\nPoisson point process?"
    },
    {
      "category": "Resource",
      "title": "Homework Set #4",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/1-010-uncertainty-in-engineering-fall-2008/31a460822996eca6c0884bb71a4dd31c_homework_04.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n1.010 Uncertainty in Engineering\nFall 2008\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n1.010 Fall 2008\nHomework Set #4\nDue October 9, 2008 (in class)\n1. Calculate and plot the hazard function for the lifetime distribution shown below.\n2. Read Application Example 7 and do the following:\nFor a given suburb of Boston and a certain route, the commuting time D depends on traffic\nconditions T and weather W. Specifically, the random variable (D|T,W) has exponential distribution\nwith parameter λ that depends on T and W. The probability of various combinations of T and W and\nthe associated values of λ (in min-1) are:\nt\nfT(t)\nW\nT\nP[T∩W]\nλ (min-1)\ngood\ngood\ngood\nlight\nnormal\nheavy\n0.25\n0.40\n0.15\n1/30\n1/35\n1/55\nbad\nbad\nbad\nlight\nnormal\nheavy\n0.03\n0.07\n0.10\n1/35\n1/42\n1/70\n∑ = 1.00\nFind the probability density function of D, fD(d), using a relation analogous to Eq. 2 of Application\nExample 7. Plot this density function. Is it an exponential density? Calculate the unconditional\nprobability that D>40 minutes?\n3. The joint probability mass function of precipitation depth X (mm) at a raingauge station and flow\nY (m3/s) of a nearby river is as follows:\nX=25\nX=50\nX=75\nY=2\n0.05\n0.12\nY=4\n0.11\n0.30\n0.10\nY=6\n0.12\n0.20\na) Find the marginal PMFs of X and Y.\nb) If the raingauge indicates a precipitation of 50mm, what is the probability that the flow exceeds 4\nm3/s?"
    },
    {
      "category": "Resource",
      "title": "Homework Set #5",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/1-010-uncertainty-in-engineering-fall-2008/18f538aed65b43e5eb6732ca2624519c_homework_05.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n1.010 Uncertainty in Engineering\nFall 2008\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n%\n%\n1.010 Fall 2008\nHomework Set #5\nDue October 16, 2008 (in class)\n1. Two continuous variables X and Y have joint probability density function:\n#1\nfX ,Y (x, y) = $8 (x + y)\n0 \" x \" 2and0 \" y \" 2\n&\nelsewhere\na) Find and plot the marginal probability density function (PDF) of X.\nb) Find and plot the marginal cumulative distribution function (CDF) of X.\nc) Find and plot the conditional PDF of (Y|X=1).\nd) Are X and Y independent? Comment.\n2. According to schedule, Train A arrives at station S at 10:55 am and Train B departs\nfrom the same station at 11:05 am. Due to delays, the arrival time of Train A is uniformly\ndistributed between 10:55 and 11:10 and the departure time of Train B is uniformly\ndistributed between 11:05 and 11:15. If the arrival and departure times of the two trains\nare statistically independent, what is the probability that a passenger on Train A misses\nthe connection with Train B?[Hint: Let TA and TB be the times when train A arrives and\ntrain B departs, respectively. Plot the joint range of (TA,TB) on the (TA,TB)-plane and find\nthe region that corresponds to missing the connection.]\n3. Show that the function below is the PDF of R, the distance between the epicenter of an\nearthquake and the site of a dam, when the epicenter is equally likely to be at any location\nalong a neighboring fault (see figure below). You may restrict your attention to a length\nof fault l that is within a distance ro of the site because earthquakes at greater distances\nwill have negligible effect at the site.\nfR(r) = 2r (r2-d2)-1/2 , d ≤ r ≤ ro\nl\nSketch the function.\nSite\nro\nro\nd\nFault\nA\nB\nl/2\nl/2"
    },
    {
      "category": "Resource",
      "title": "Homework Set #6",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/1-010-uncertainty-in-engineering-fall-2008/cf7b98f236a7e6dbaef547da9b20434e_homework_06.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n1.010 Uncertainty in Engineering\nFall 2008\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n1.010 Fall 2008\nHomework Set #6\nDue October 28, 2008 (in class)\n1. A chain has 10 links. The strengths of the links, X1, ..., X10 are independent and\nidentically distributed random variables with exponential PDF\nfX(x) = e-x, x ≥ 0\nFind and plot the PDF of the strength of the chain S = min{X1, X2, ..., X10}. Is the\ndistribution of S also exponential?\n2. At a certain location storms arrive as a Poisson point process with rate λ=70\nstorms/year. The rainfall intensity I of each storm has CDF\nP[I ≤ i] = FI(i) = 1-e -0.067i , i ≥0\nwhere i is in mm/hour. Assuming independence among the storm intensities, find the\nprobability that the yearly maximum rainfall intensity exceeds 100mm/hour. [Read\napplication example 11.]\n3. U1 and U2 are independent random variables with uniform distribution in [0,1]. Find\nthe CDF of Y = U1+ U2. Then differentiate FY(y) to find the PDF, fY(y). [Hint: To find\nFY(y), sketch the region \" y on the (U1, U2) plane where Y \" y.]"
    },
    {
      "category": "Resource",
      "title": "Homework Set #7",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/1-010-uncertainty-in-engineering-fall-2008/29475fa98b390451d3c763383cc53666_homework_07.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n1.010 Uncertainty in Engineering\nFall 2008\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n!\n!\"\n#\n&\n$\n$ %\n'\n(\n)\n*\n+\n, -\n-\n1.010 Fall 2008\nHomework Set #7\nDue November 6, 2008 (in class)\n1. X has uniform distribution between 0 and 1. Let Y = X 2.\na) Find the exact mean value and variance of X\nb) Find the exact mean value and variance of Y.\nc) Approximate the mean value and variance of Y using first-order second-moment\n(FOSM) analysis. Compare with the results in (b).\n2. The natural frequency ω at which a one-story building vibrates is given by\nK\nω = M\nwhere K is the lateral stiffness of the building and M is its mass. If K is random with\nmean value mK and standard deviation σK and M is known, use FOSM analysis to find in\napproximation the mean value and variance of ω.\n3. In planning a building, the number of elevators is chosen on the basis of balancing\ninitial costs versus the expected delay times of the users. These delays are closely related\nto the number of stops the elevator makes on a trip. If an elevator runs full (n people) and\nthere are k floors, we want to find the expected number of stops R the elevator makes on\nany trip. Assuming that the passengers act independently and that any passenger chooses\na floor with equal probability 1 . Show that:\nk\nn\nk 1\nk\nHint: It is often useful to define \"indicator random variables\" as follows:\nLet Xi = 1 if the elevator stops at floor i, and Xi = 0 if it does not.\nk\nThen observe that R = !Xi .\ni=1\nFind the expected value of Xi after finding the probability that Xi = 0.\n4. In the very preliminary planning of some harbor island developments, there was\ndiscussion regarding the cost estimates of building five bridges. There had as yet not been\na preliminary soil survey in the harbor area, and it was recognized that the bridge costs\nare highly dependent on soil conditions. Therefore, there was great uncertainty in the\npreliminary cost estimates. A spokesman, however, made this statement; \"I recognize the\nE[R]\n=\n\nuncertainty on the cost estimate of any bridge. But I am much more confident of our\nestimate for the total cost of all five bridges because the likelihood that a high estimate on\none will be balanced by a low estimate on another.\"\nDiscuss this statement. Use your knowledge, simply, of the means and variances\nof sums of random variables to support your comments. If your initial intuition lies with\nthe spokesman, be sure you resolve in your own mind why it is inconsistent. Compare\nboth the variance and the coefficients of variation of the total cost versus those of an\nindividual cost.\nIf the five bridge sites are relatively close to one another, soil conditions, although\nunknown, are probably similar. What implications does this observation have for your\nanalysis? Would your answer change in the case of 10 bridges?"
    },
    {
      "category": "Resource",
      "title": "Homework Set #8",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/1-010-uncertainty-in-engineering-fall-2008/5c7a2224f30420195c5db45afa6c6994_homework_08.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n1.010 Uncertainty in Engineering\nFall 2008\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n1.010 Fall 2008\nHomework Set #8\nDue November 13, 2008 (in class)\n1. A concrete column has square cross-section with side length 0.5m. The column is\nsubject to a random axial stress S contributed by two random loads W1 and W2, i.e.\nS = W\nW\n2 where A is the cross-sectional area of the column. The mean value and\n+\nA\nA\ncoefficient of variation of W1 are mW1 = 2500kN and VW1 = 0.2, whereas for W2, mW2 =\n3000kN and VW2 = 0.15. W1 and W2 have correlation coefficient \" = 0.5. The\ncompressive strength of concrete, fc, is also random and independent of W1 and W2, with\nmean value mfc = 30000 kN/m2 and coefficient of variation Vfc = 0.1. Find in\napproximation the mean value and variance of the safety factor SF = fc /S.\n2. Daily temperature and humidity are measured each day of a week in August, and the\nresulting sample is:\nT (oF)\nH(%)\n(a) Plot the sample (Plot H against T).\n(b) Estimate the mean values and variances of T and H. (Replace the expectation in\nthe definition of m and !2 with the sample average)\n(c) Find the covariance and correlation coefficient between T and H. (Again replace\nthe expectation in the definition of m and !2 with the sample average)\n(d) Suppose that one day temperature is reported to be 80oF. What would you expect\nthe humidity to be? How much uncertainty would you have on H?"
    },
    {
      "category": "Resource",
      "title": "Homework Set #9",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/1-010-uncertainty-in-engineering-fall-2008/78783a0d45e6ef9a1b43b466a97b5600_homework_09.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n1.010 Uncertainty in Engineering\nFall 2008\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n\"\n$#\n%\n'&\n\"\n$#\n\"\n$\n#\n%\n'\n&\n(\n* * )\n+\n- - ,\n$#\n%\n'&\n\"\n$# '&\n\"\n$\n#\n%\n'\n&\n)\n+ + *\n,\n. . -\n\"\n# $\n$\n$\n%\n& '\n'\n'\n\"\n# $\n$\n$\n%\n'\n'\n& '\n\"\n$\n$\n$ #\n%\n&\n'\n'\n'\n(\n)\n*\n*\n*\n+\n,\n-\n-\n-\n1.010 Fall 2008\nHomework Set #9\nDue November 20, 2008 (in class)\n1. Let X1 and X2 be the gains, in millions of dollars, from investing $1million in two\ndifferent stocks. The objective of investing is of course to maximize gains with a\nminimum of uncertainty. Suppose that X1 and X2 have the following second moment\ncharacteristics:\nd /10\n5.9\n5.9\n3.5\nd /10\n2.6\n0.8\n2.6\n2.5\n~\n~\nX\nX\nA\nB\nresult.\n(a) Which stock type would you prefer for investment?\n(b) As an alternative, consider investing $500k in stock 1 and $500k in stock 2. Would\nyou consider this strategy more attractive? Give an intuitive explanation of your\n(c) How should you invest your $1 million to minimize uncertainty on the returns?\nH\nH\nSpecifically, the thicknesses\nand\nat locations A and B have second moment\nA\nB\n%\n\"\n2. The thickness H of a coal layer varies randomly with geographical location.\ncharacteristics:\nH\nm ,( 2\nH\nm\n0.8\nwhere m = 5m, σ = 2m and d is the distance in meters between points A and B. A boring\nis made at location A, giving HA =4m. Find and plot the BLUE estimate of (HB|HA=4m)\nand the standard deviation of the estimation error as a function of distance d, for 0 < d <\n20m. Comment on the behavior of these functions.\n3. Rivergauges A, B and C are located at three distant points on the same river stream.\nThe daily fluxes QA, QB and QC (in m3/s) at locations A, B and C, respectively, have\nsecond moment characteristics:\nFor some technical reason gauge A failed to report a measurement of today's flux. What\nis the best linear estimate of QA if QB = 3.5m3/s and QC = 6.8m3/s? What is the variance\nof the estimation error?\n1.52\n0.32\n0.2\n%\n0.1\n',\n0.1&\n~\n5 ,\nQA\nQB\nQC"
    },
    {
      "category": "Resource",
      "title": "Homework Set #10",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/1-010-uncertainty-in-engineering-fall-2008/9bf5626d7a0d332184e197c360ea3063_homework_10.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n1.010 Uncertainty in Engineering\nFall 2008\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n\"\n#\n$\n$\n$\n$\n%\n&\n'\n'\n'\n'\n\"\n#\n$\n$\n$\n$ &\n\"\n#\n$\n$\n$\n$\n%\n&\n'\n'\n'\n'\n(\n)\n*\n*\n*\n*\n+\n,\n-\n-\n-\n-\n1.010 Fall 2008\nHomework Set #10\nDue December 4, 2008 (in class)\n1. Random variables X1, ..., X50 are independent with the following distributions:\nX1, ..., X10 ∼ U[0, 1]\nX11, ..., X20 ∼ N(1, 0.1)\nX21, ..., X30 ∼ LN(3, 0.2)\nX31, ..., X40 ∼ Ex[0.4]\nX41, ..., X50 ∼ Ga(2, 0.2)\nwhere U[0, 1] is the uniform distribution in [0,1], N(m, σ2) and LN(m, σ2) are the normal\nand lognormal distributions with mean value m and variance σ2, Ex[m] is the exponential\ndistribution with mean value m, and Ga(n, m) is the gamma distribution that results from\nadding n iid exponential variables with distribution Ex[m]. Find in approximation the\nprobability that Y = \"Xi exceeds 56.\ni=1\n2. The daily SO2 concentration at a given location is normally distributed with mean 0.03\nppm (parts per million) and coefficient of variation 40%. Clean air standards require\nthat: a) the daily SO2 concentration does not exceed 0.06 ppm, and b) the weekly average\nSO2 concentration does not exceed 0.045 ppm. Assuming that SO2 concentrations in\ndifferent days are statistically independent, determine which of the above criteria is more\nlikely to be violated. Would correlation between daily concentrations lead you to a\ndifferent conclusion? Explain using qualitative arguments.\n3. Water flow into a reservoir is contributed by two rivers. During the spring season, the\nflows from the rivers, Q1 and Q2, the water demand Y, and the stored water volume in the\nreservoir at the beginning of the season S have multivariate normal distribution\n2.5\n∼ N\nQ1\nS\nY\nwhere Q1, Q2, Y and S are in million cubic feet. At the beginning of one spring season\nS=8. Find the probability of not meeting demand at the end of the season, i.e. find\nP[(Q1+Q2-Y|S=8)<-8].\n2.5\n%\n'\n'\n'\n'\n8 ,\nQ2"
    },
    {
      "category": "Exam",
      "title": "Final Exam",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/1-010-uncertainty-in-engineering-fall-2008/6317b853229962e88034e6eae0030c7b_final_exam.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n1.010 Uncertainty in Engineering\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\nFall 2008\n\n1.010 - Final Exam\n(3 hours - open books and notes)\n\nProblem 1 (10 Points)\n\nChristmas lights sold at Wall-Mart come in sets of 4 bulbs of different colors connected\nin two alternative configurations as shown below:\n\nConfiguration 1\n\nConfiguration 2\n\nIf both configurations cost the same, which configuration would you buy for a lower\nprobability of failure (dark Christmas tree). Assume that each light bulb fails with\nprobability 0.03 and bulbs fail independently.\n\nProblem 2 (10 Points)\n\nA computer can generate random variables U with uniform distribution in the interval\n[0, 1]. Suppose that you generate 10 such variables,\n,\n, ... ,\n. Let N be the\nnumber of variables\nwith value in the interval [0.4, 0.5]. Find:\nU\nU\nU\ni\nU\n\n(a) The probability that N = 0.\n(b) The probability mass function of N.\n(c) The mean value and variance of N.\n\nProblem 3 (10 Points)\n\nSuppose that the interarrival times T of busses has triangular distribution as shown\nbelow:\n\n)\n(t\nfT\nT (minutes)\n\nIf it has been 10 minutes since the last bus left, find the probability that a bus will arrive:\n\n(a) Within the next 5 minutes.\n(b) Within the next 10 minutes.\n\nProblem 4 (15 Points)\n\nDays may be sunny (\n) or cloudy (\n1 =\nI\n1 =\nI\n), and either dry (\n) or humid\n(\n0 ). Suppose that 80% of the days are sunny and of these, 80% are dry. Of the\ncloudy days, 50% are dry. Find:\n2 =\nI\n2 =\nI\n\n(a) The joint PMF of\nand\n.\n1I\n2I\n(b) The marginal PMF of\n.\n2I\n(c) The mean value and variance of\nand\n.\n1I\n2I\n(d) The covariance between\nand\n.\n1I\n2I\n\nProblem 5 (10 Points)\n\nThe deflection of a column, δ (in mm) depends on the applied load L (in tons) as\n. This function is plotted in the figure below.\n1.\nL\n=\nδ\n\n0.5\n1.5\n2.5\n3.5\n4.5\nLoad, L (tons)\nDeflection, δ (mm)\n\nL\nδ\n\nSuppose that L has exponential distribution with probability density function:\n\n⎩\n⎨\n⎧\n<\n≥\n=\n-\n\n,0\n\n,\n)\n(\nA\nA\nA\nA\ne\nf L\n\nFind:\n\n(a) The probability density function of δ.\n(b) The probability that δ exceeds 3 mm.\n\nProblem 6 (10 Points)\n\nAt a given location, the mean daily temperature in December (in degrees Celsius) is\n,\nwith a standard deviation of\n. The correlation coefficient between temperatures in day i\nand i + j is\no\no\nj\nj\ni\ni\n8.0\n,\n=\n+\nρ\n. Find the mean value and variance of the average temperature in\na three day period.\n\nProblem 7 (10 Points)\n\nA coin you found on a beach looks old. A friend, who is an antique dealer, says that it is\nprobably 150 years old\n40 years (meaning that the mean value of the age is 150 years\nand the standard deviation is 40 years). You decide to consult a coin expert who says that\nthe coin may actually be 120 years old\n±\n± 20 years (consider this as a 'noisy'\nobservation). How would you combine the two estimates to find the most likely age an its\nuncertainty?\n\n[Hint: You could view 150\n40 years as your own uncertainty prior to consulting the\ncoin expert, and the expert assessment of 120 years as a 'noisy measurement' of the true\nage, with an error standard deviation of 20 years. Notice that in this case, the coin expert\nacts like an imperfect measuring device.]\n±\n\nProblem 8 (10 Points)\n\nThe condition for failure of a column is given by\nR\nL\nD\n>\n+\n, where D is the dead load, L\nis the live load, and R is the resistance, all expressed in the same units.\n\nSuppose that D, L and R are independent normally distributed variables with the\nfollowing distributions:\n\n,\nand\n\n)\n,\n(\n~\nN\nD\n)\n,\n(\n~\nN\nL\n)\n,\n(\n~\nN\nR\n\nFind the probability of failure of the column.\n\nProblem 9 (15 Points)\n\nHurricanes occur according to a Poisson Point Process with unknown parameterλ . Given\nthat 5 hurricanes occurred during a two-month period, estimateλ by:\n\n(a) The Method of Moments\n(b) The Method of Maximum Likelihood"
    },
    {
      "category": "Exam",
      "title": "Mini-Quiz #1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/1-010-uncertainty-in-engineering-fall-2008/52495e9dbc66d317948e166d1e234a83_mini_quiz_1.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n1.010 Uncertainty in Engineering\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\nFall 2008\n\n1.010 -Mini-Quiz #1\n(30 min - open books and notes)\n\nProblem 1 (30 points)\n\nEvents A and B have probability P[A] = P[B] = 0.25.\n\nGiven that A and B are mutually exclusive:\n\na) Can you find\n?\nB]\nP[A ∪\nb) Can you find\n?\nB]\nP[A ∩\n\nGiven that A and B are independent:\n\na) Can you find\n?\nB]\nP[A ∪\nb) Can you find\n?\nB]\n|\nP[Ac\n\nIf your answer to any of the above questions is 'yes', then give its numerical value. If it is\n'no', then briefly explain why.\n\nProblem 2 (30 points)\n\nThe aqueduct from a reservoir to a city has the configuration shown below. If a large\nearthquake occurs, each of the five links of the aqueduct has a probability 0.3 of failing.\n\nAssuming that failures of different links are independent events, find the probability that\nthe water supply to the city is not interrupted by the earthquake.\n\nReservoir\nCity\n\nProblem 3 (40 points)\n\nYou left your cell phone either in the computer lab or in the cafeteria. You think that with\nprobability 0.6 it happened in the computer lab, because you always take it out of your\npocket there. With probability 0.4 it happened in the cafeteria. If you left the phone in the\ncomputer lab, there is a probability 0.3 that someone stole it. In the cafeteria, that\nprobability is 0.6.\n\n(a) What is the probability that you will find your cell phone? (either in the lab or in the\ncafeteria)\n\n(b) Given that you found the cell phone, what is the probability that you left it in the\ncafeteria?"
    },
    {
      "category": "Exam",
      "title": "Mini-Quiz #1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/1-010-uncertainty-in-engineering-fall-2008/9dd4e623693c3372120073136d8a6964_mini_quiz_5.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n1.010 Uncertainty in Engineering\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n1.010 - Mini-Quiz #5\n(45 min - open books and notes)\n\nProblem 1 (60 Points)\n\nTwo concrete beams have strengths\nand\nwith joint normal distribution:\nX\nX\n\n⎟⎟\n⎠\n⎞\n⎜⎜\n⎝\n⎛\n⎥⎦\n⎤\n⎢⎣\n⎡\n⎥⎦\n⎤\n⎢⎣\n⎡\n⎥⎦\n⎤\n⎢⎣\n⎡\n\n0.75\n0.75\n\n,\nN\n~\nX\nX\n\n(in some appropriate units)\n\nYou intend to use beam 1 for construction. For structural competency, its strength must\nexceed 2 with probability at least 0.99, i.e.\n.\n.0\n]\nX\n[\nP\n≥\n>\n\na. Find\n. If this value is less than 0.99, beam 1 cannot be used, unless\nmore information is obtained on its strength.\n]\nX\n[\nP\n>\nb. To get more information on\n, you test the second beam.\nX\nc. What is the probability that, after testing the second beam, the first beam is\nfound to be acceptable? Hint: First find\nsuch that\n.\nThen calculate\n.\n*2\nx\n.0\n]\n*2\nx\nX\n|\nX\n[\nP\n=\n=\n>\n]\n*2\nx\nX\n[\nP\n=\n\nProblem 2 (40 Points)\n\nThe strength X of a concrete batch has normal distribution\nwhere\nis\nknown (\n=1000 psi (pounds per square inch)) but m is uncertain with normal\ndistribution\n. To better constrain m, you test 4 concrete\ncylinders in the lab from which you obtain the sample average\n)\n,\nm\n(\nN\n~\nX\nσ\nσ\nσ\n)\n)\npsi\n(,\npsi\n(\nN\n~\nm\n∑\n=\ni\ni\nX\nX\n. Notice that\nX has distribution\n)\n,\nm\n(\nN\n~\nX\nσ\nand that it can be thought of as a noisy measurement\nof m,\nε\n+\n= m\nX\n, where\n)\n,0\n(\nN\n~\nσ\nε\n.\n\nSuppose that you measure\npsi\nX =\n. Find the mean value and variance of\n)\npsi\nX\n|\nm\n(\n=\n."
    },
    {
      "category": "Exam",
      "title": "Mini-Quiz #2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/1-010-uncertainty-in-engineering-fall-2008/99a2e59447f6f4926eaecceb84cfc38c_mini_quiz_2.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n1.010 Uncertainty in Engineering\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\nFall 2008\n\n1.010 - Mini-Quiz #2\n(45 min - open books and notes)\n\nProblem 1 (40 Points)\nA community of bacteria initially includes 1000 individuals. Given favorable conditions\n(light, temperature, nutrients), the community doubles in size during a unit time period,\nfor example one day. If conditions are unfavorable, the community downsizes by a factor\nof 2.\n\nSuppose that favorable conditions occur with probability 0.6, and unfavorable conditions\nwith probability 0.4, and that favorable/unfavorable conditions are independent in\ndifferent time periods (days). Find the probability mass function of the number of\nindividuals after 3 time periods.\n\nProblem 2 (20 Points)\nAt a given site, flood-producing storms occur infrequently. Considering the three\nconditions under which a point process is Poisson, state reasons for or against modeling\nthe storm arrival times as a Poisson point process.\n\nProblem 3 (40 Points)\nThe lifetime T of electric bulbs (e.g. the number of hours in operation before they fail)\nhas an exponential distribution with cumulative distribution function:\n\nt\nT\ne\n)t(\nF\n⎟\n⎠\n⎞\n⎜\n⎝\n⎛\n-\n-\n=\nfor\n, with t in hours.\nt ≥\n\nSuppose you have used a bulb for 500 hours without failure. Find the probability that the\nbulb will last at least 500 more hours.\n\nHint: Use P[A|B] = P[A∩B]/P[B], with appropriately defined events A and B."
    },
    {
      "category": "Exam",
      "title": "Mini-Quiz #3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/1-010-uncertainty-in-engineering-fall-2008/94cc85975403dc084a146af4c7c5d59d_mini_quiz_3.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n1.010 Uncertainty in Engineering\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\nFall 2008\n\n1.010 - Mini-Quiz #3\n(40 min - open books and notes)\n\nProblem 1 (33 points)\nA random variable X has uniform distribution between -1 and 1. This means that the\nPDF of X is\n\nfX(x) =\n0.5,\nfor -1 < x < 1\n0,\notherwise\n⎧\n⎨\n⎩\n\nFind the CDF of Y = |X|.\n\nProblem 2 (33 points)\nA chain is made of n links with independent and identically distributed strengths\n. If the distribution of the X's is uniform between 0 and 1, find and plot the\nCDF of the strength of the chain, Y, for n = 2 and n = 10.\nX1,X2,...,Xn\n\nProblem 3 (33 points)\nYour PC can simulate random variables U with uniform distribution between 0 and 1.\nHow can you simulate a random variable X with CDF given by\n\nFX(x) =\n0,\nx ≤0\nx2,\n0 < x < 1\n1,\nx ≥1\n⎧\n⎨\n⎪ ⎪\n⎩\n⎪ ⎪"
    },
    {
      "category": "Exam",
      "title": "Mini-Quiz #4",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/1-010-uncertainty-in-engineering-fall-2008/da2151843b9df9c1972deab0499662f6_mini_quiz_4.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n1.010 Uncertainty in Engineering\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\nFall 2008\n\n1.010 - Mini-Quiz #4\n(40 min - open books and notes)\nProblem 1 (30 Points)\nConsider two variables X1 and X2, each with possible values -1 and 1. The joint\nprobability mass function of X1 and X2 is shown in the figure below. (Notice that the\ndistribution is concentrated at four points, with equal probability 0.25 at each point)\nX2\n-1\nX1\n0.25\n0.25\n0.25\n0.25\n-1\n(a) Are X1 and X2 independent?\n(b) Are X1 and X2 uncorrelated?\nJustify your answers\nProblem 2 (30 Points)\nThe time T ( in hours) for a car to travel 50 miles of a highway is given by T = V ,\nwhere V is the car speed in miles/hour. Suppose that V has mean value\nmV =\nmiles\n/ hour and standard deviation σV =\nmiles\n/ hour . Find in approximation\nthe mean value and standard deviation of T.\n\nProblem 3 (40 Points)\nConsider the two columns shown in the figure below, where X1 and X2 are random\nload variables.\n2X1\nX1\nX2\nY1\nY2\nThe loads Y1 and Y2 at the base of the columns are given by:\nY1 = X\n2 1\nY2 = X1 + X2\nSuppose that X1 and X2 have common mean value m and common standard deviation\nσ and denote by ρ their correlation coefficient.\n(a) Find the variances of Y1 and Y2 .\n(b) What effect does ρ have on the variance of Y2 ?\n(c) Explain in intuitive ways the results for ρ = -1 and ρ =1. Also compare with the\nvariance of Y1. (Notice that for X1=X2 the load on the two columns is the\nsame)."
    },
    {
      "category": "Resource",
      "title": "Brief Notes #1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/1-010-uncertainty-in-engineering-fall-2008/e9911c0d241f63991438ed891063b508_notes_01.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n1.010 Uncertainty in Engineering\nFall 2008\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nBrief Notes #1\nEvents and Their Probability\n- Definitions\n\nExperiment: a set of conditions under which some variable is observed\n\nOutcome of an experiment: the result of the observation (a sample point)\nSample Space, S: collection of all possible outcomes (sample points) of an experiment\nEvent: a collection of sample points\n- Operations with events\n\n1. Complementation\nAc\nAc\nA\n2. Intersection\nA\nB\nA∩B\n3. Union\nA∪B\nB\nA\n- Properties of events\n1. Mutual Exclusiveness - intersection of events is the null set (Ai∩Aj = ∅, for all i = j)\n2. Collective Exhaustiveness (C.E.) - union of events is sample space (A1∪A2∪...∪An = S)\n3. If the events {A1, A2, ... , An} are both mutually exclusive and collectively exhaustive, they\nform a partition of the sample space, S.\n- Probability of events\n- Relative frequency fE and limit of relative frequency FE of an event E\nfE = nE\nn\nFE = limf E = lim n E\nn→inf\nn→inf n\n- Properties of relative frequency (the same is true for the limit of relative frequency\n\n1. 0 ≤ fE ≤ 1\n2. fS = 1\n3. f(A∪B) = fA + fB if A and B are mutually exclusive\n- Properties/axioms of probability\n1. 0 ≤ P(A) ≤ 1\n2. P(S) = 1\n3. P(A∪B) = P(A) + P(B) if A and B are mutually exclusive\n- Two consequences of the axioms of probability theory\n1. P(Ac) = 1 - P(A)\n2. P(A∪B) = P(A) + P(B) - P(A∩B), for any two events A and B,\n⇒ P(A∩B) = P(A) + P(B) - P(A∪B)\n- Conditional Probability\nDefinition:\nP(A | B) = P(A ∩ B)\nP(B)\nTherefore, P(A∩B) can also be obtained as P(A∩B) = P(B)P(A|B) = P(A) P(B|A)\n- Total Probability Theorem\nLet {B1, B2,..., Bn} be a set of mutually exclusive and collectively exhaustive events and let A\nbe any other event. Then the marginal probability of A can be obtained as:\nP(A) = ∑P(A ∩ Bi ) = ∑P(Bi )P(A | Bi )\ni\ni\n- Independent events\nA and B are independent if:\nP(A|B) = P(A), or equivalently if\nP(B|A) = P(B), or if\nP(A∩B) = P(A) P(B)\n- Bayes' Theorem\n\nP(A | B) = P(A) P(B | A)\nP(B)\nUsing Total Probability Theorem, P(B) can be expressed in terms of P(A), P(Ac) = 1 - P(A),\nand the conditional probabilities P(B|A) and P(B|AC):\nP(B) = P(A)P(B | A) + P(A C)P(B | A C)\nSo Bayes' Theorem can be rewritten as:\nP(A| B) = P(A)\nP(B| A)\nP(A)P(B| A)+ P(AC )P(B| AC )"
    },
    {
      "category": "Resource",
      "title": "Brief Notes #2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/1-010-uncertainty-in-engineering-fall-2008/8d0b1f53b504f9e460fa149b053f5566_notes_02.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n1.010 Uncertainty in Engineering\nFall 2008\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu.terms.\n\n-\nP\nP\n-\nP\n1.010 - Brief Notes # 2\nRandom Variables: Discrete Distributions\nDiscrete Distributions\n-\nProbability Mass Function (PMF)\nPX (x) = P (X = x) =\nP (O)\nall O: X(O)=x\n- Properties of PMFs\n1. 0 ≤ PX (x) ≤ 1\n2.\nPX (x) = 1\nall x\nCumulative Distribution Function (CDF)\nFX (x) = P (X ≤ x) =\nPX (u)\nu≤x\n- Properties of CDFs\n1. 0 ≤ FX (x) ≤ 1\n2. FX (-inf) = 0\n3. FX (inf) = 1\n4. if x1 > x2, then FX (x1) ≥ FX (x2)\nDiscrete distributions\n(a) Probability Mass Function PMF\n(b) Cumulative Distribution Function CDF\n\nP\nP\n- Examples of discrete probability distributions\nBernoulli distribution\n-\n⎧\n⎨1, if an event of interest occurs (success)\nY = ⎩0, if the event does not occur (failure)\nY is called a Bernoulli or indicator variable\n⎧\n⎨p,\ny = 1\nPY (y) = ⎩q = 1 - p,\ny = 0\nGeometric distribution\n-\nSequence of Bernoulli trials\nN = number of trials at which first success occurs\nN = 1, 2, 3, . . .\nPN (n) = P (N = n) = (1 - p)n-1p\nn\nn\nFN (n) =\nPN (i) =\n(1 - p)i-1p = 1 - (1 - p)n\ni=1\ni=1\nBinomial distribution\n-\nConsider a sequence of Bernoulli trials\nLet M = number of successes in n trials\nM = 1, 2, 3, . . . , n\nn!\nm\nPM (m) =\np qn-m ,\nm!(n - m)!\nn!\n\nwhere\n=\nn = binomial coefficient\nm\nm!(n - m)!\nwhere p and q = 1 - p are the probabilities of success and failure in individual Bernoulli trials\nIn particular, the probability of no success is:\nn\nPM (0) = q\n= (1 - p)n\nPM (0) = 1 - pn, if pn << 1\nand the probability of all successes is:\nn\nPM (n) = p\n\nPoisson distribution\n-\nAssumptions:\n1. In a time interval of short duration Δ, the probability of one occurrence is λΔ, where\nλ = occurrence rate (expected number of occurrences per unit time).\n2. The probability of two or more occurrences in Δ is negligible.\n3. The occurrences in non-overlapping intervals are independent.\nUnder these conditions, the number of occurrences in each interval of duration Δ is either 0\nor 1, with probability p = λΔ of being 1. Let Y = no. of occurrences in [0, t], where t = nλ.\nThen Y has binomial distribution with probability mass function\nPY (y) = n\ny py qn-y, where p = λΔ = λ n\nt\nAs n →inf,\nPY (y) = (λt)y\ny\ne\n!\n-λt\n(Poisson PMF)"
    },
    {
      "category": "Resource",
      "title": "Brief Notes #3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/1-010-uncertainty-in-engineering-fall-2008/49d6ce49494cb524bc894cf276b6a35b_notes_03.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n1.010 Uncertainty in Engineering\nFall 2008\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu.terms.\n\nR\nR\nR\n1.010 - Brief Notes # 3\nRandom Variables: Continuous Distributions\nContinuous Distributions\n-\n- Cumulative distribution function (CDF)\nFX (x) = P [X ≤ x]\nP [x1 < X ≤ x2] = FX (x2) - FX (x1)\n- Average probability density in an interval [x1, x2]\nP [x1 < X ≤ x2]\nFX (x2) - FX (x1)\n=\nx2 - x1\nx2 - x1\n- Probability density function (PDF)\nfX (x1) = lim P [x1 < X ≤ x2] = dFX\nx2→x1\nx2 - x1\ndX x1\nx2 fX (x)dx = P [x1 < X ≤ x2] = FX (x2) - FX (x1)\nx1\n- Properties of the PDF\n1. fX (x) ≥ 0\n2.\ninf fX (x)dx = 1\n-inf\nu\n3.\nfX (x)dx = FX (u)\n-inf\n\nExample of PDF and corresponding CDF of a continuous random variable: steel-yield-stress.\n(a) Probability density function; (b) cumulative distribution function.\n- Examples of continuous probability distributions\nUniform distribution\n-\n⎧\n⎨\n⎩\nc,\nwhere c is constant, a < x < b\nfX (x) =\n0,\notherwise.\nThen c = b-a\n⎧\n⎪\n⎨\n⎪\n⎩\n0,\nx < a\nx-a ,\nb-a\na ≤ x ≤ b\n1,\nx > b\nFX (x) =\nExample of uniform distribution.\n\n- Exponential Distribution\nLet T = time to first arrival in a Poisson point process\nFT (t) = P [T ≤ t] = 1 - P [T > t]\n= 1 - P [no occurrence in [0, t]]\n= 1 - e-λt\nfT (t) = λe-λt ,\nt ≥ 0\nPDF and CDF of the exponential distribution."
    },
    {
      "category": "Resource",
      "title": "Brief Notes #4",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/1-010-uncertainty-in-engineering-fall-2008/200b54ec87d7b1f16129d91dc84a2775_notes_04.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n1.010 Uncertainty in Engineering\nFall 2008\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n1.010 - Brief Notes #4\nRandom Vectors\nA set of 2 or more random variables constitutes a random vector. For example, a random\nvector with two components, X =\n\nX1\n, is a function from the sample space of an\nX 2\nexperiment to the (x1, x2) plane.\n- Discrete Random Vectors\n- Characterization\n- Joint PMF of X1 and X2:\nPX (x) = PX1 ,X 2 (x 1, x 2 ) = P[(X 1 = x 1) ∩(X 2 = x 2 )]\n- Joint CDF of X1 and X2:\nFX (x) = FX1,X2 (x1,x 2) = P[(X1 ≤x1) ∩(X 2 ≤x 2)]\n= ∑∑PX1,X2 (u1,u 2)\nu1 ≤x1 u2 ≤x2\n- Marginal Distribution\n- Marginal PMF of X1:\nPX1 (x1) = P[X1 = x1] = ∑P[(X1 = x1) ∩(X 2 = x 2)] = ∑PX1,X2 (x1,x 2)\nall x2\nall x2\n- Marginal CDF of X1:\nFX (x1) = P[X1 ≤x1] = P[(X1 ≤x1) ∩(X 2 < inf)] = FX ,X (x1,inf) = ∑∑PX ,X (u,x 2)\nall x2 u≤x1\n\n- Continuous Random Vectors\n- Characterization\n- Joint CDF FX1,X2 (x1,x 2 ) : same as for discrete vectors.\nX1\n- Joint Probability Density Function (JPDF) of X =\n, f X ,X (x1,x 2) :\nX 2\nThis function is defined such that:\nf X ,X (x 1, x 2 )dx 1dx 2 = P[(x 1 ≤X 1 < x 1 + dx 1) ∩(x 2 ≤X 2 < x 2 + dx 2 )]\nRelationships between f X1,X2 and FX1,X2 :\n∂2FX ,X (x 1, x 2 )\nf X1,X 2 (x 1, x 2 ) =\n∂x 1∂x 2\nx1\nx 2\nF\n,X 2 (x 1, x 2 ) = ∫∫\nf X ,X 2 (u 1,u 2 )du 1 du 2\nX1\n-inf -inf\n- Marginal distribution of X1\n- CDF:\nFX1 (x1) = FX1,X2 (x1,inf)\ndFX (x1)\n∂FX ,X (x1, inf)\n- PDF:\nf X (x1) =\n=\ndx 1\n∂x1\n= ∂\n∂\nx 1\n∫-\nx\ninf du 1 ∫-\ninf\ninf f X1,X (u 1,u 2 )du 2\ninf\n= ∫-inf f X1,X 2 (x 1,u 2 )du 2\n- Conditional PDF of (X1 | X2 = x2)\nf (X1|X 2 =x 2 ) (x 1) = f X1\nf\n,X 2 (\n(\nx\nx\n1,\n)\nx 2 )\nX 2\n∝ f X1,X2 (x1,x 2) , for f X2 (x 2) =0\n\n- Conditional Distribution\n- Conditional PMF of (X1 | X2 = x2):\nP(X |X = x ) (x 1) = P[X 1 = x 1 | X 2 = x 2 ] = PX1,X 2 (x 1, x 2 )\nPX 2 (x 2 )\n∝ PX1,X2 (x1,x 2)\nExample of discrete joint distribution: joint PMF of traffic at remote location\n(X in cars/30 sec. interval) and traffic recorded by some imperfect traffic counter (Y)\n(note: X and Y are the random variables X1 and X2 in our notation).\nExample of discrete joint distribution: marginal distributions.\n(a) Marginal PMF of actual traffic X, and (b) marginal counter response Y.\n\n- Independent Random Variables\nX1 and X2 are independent variables if:\nFX1,X2 (x1,x 2) = FX1 (x1) . FX2 (x 2)\nEquivalent conditions for continuous random vectors are:\nf X1,X2 (x1,x 2) = f X1 (x1) . f X2 (x 2)\nor:\nf(X1|X2 =x2 )(x1) = f X1 (x1)\nand for discrete random vectors:\nPX1,X2 (x1,x 2) = PX1 (x1) . PX2 (x 2)\nor:\nP(X1|X2 =x2 ) (x1) = PX1 (x1)\nExample of continuous joint distribution:\njoint and marginal PMF of random variables X and Y.\n(Note: X and Y are the random variables X1 and X2 in our notation)"
    },
    {
      "category": "Resource",
      "title": "Brief Notes #5",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/1-010-uncertainty-in-engineering-fall-2008/b4ffd1b0d7fb56037fbdaf251d7fac68_notes_05.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n1.010 Uncertainty in Engineering\nFall 2008\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu.terms.\n\nR\n\n1.010 - Brief Notes # 5\nFunctions of Random Variables and Vectors\n(a) Functions of One Random Variable\nProblem\n-\nGiven the CDF of the random variable X, FX (x), and a deterministic function Y = g(x), find the\n(derived) distribution of the random variable Y .\nGeneral Solution\n-\nLet ΩY = {x : g(x) ≤ y}. Then:\nFY (y) = P [Y ≤ y] = P [x ∈ ΩY ] =\nfX (x)dx\nΩY\n- Special Cases\n- Linear Functions\nY = g(x) = a + bx\nIf b > 0:\nX(y) = y - a\nb ;\nΩY = {x : a + bx ≤ y} =\n\n-inf, y - a\nb\n\nFY (y) = P [x ∈ ΩY ] = FX\ny - a\nb\nfY (y) = d FY (y) = d FX\ny - a\n= 1 fX\ny - a\ndy\ndy\nb\nb\nb\n\nIf b < 0:\nΩY = y -\nb\na, inf\nFY (y) = 1 - FX\ny - a\nb\nfY (y) = - 1 fX\ny - a\n= 1 fX\ny - a\nb\nb\n|b|\nb\nFor any b = 0:\nfY (y) = 1 fX\ny - a\n|b|\nb\n\n- General monotonic (one-to-one) functions\n- Monotonically increasing functions\nFY (y) = FX [x(y)]\ndFY (y)\ndx(y)\nfY (y) =\n=\nfX [x(y)]\ndy\ndy\n·\n- Monotonically decreasing functions\nFY (y) = 1 - FX [x(y)]\nfY (y) = dx(y)\ndy\nfX [x(y)]\n·\n\n- Examples of Monotonic Transformations\nConsider an exponential variable X\n∼\nEX(λ) with cumulative distribution function\nFX (x) = 1 - e-λx , x ≥ 0.\nExponential, Power and Log Functions\n- Exponential Functions\nSuppose Y = eX , ⇒ x = ln(y), y ≥ 0.\nThis is a monotonic increasing function, and\nFY (y) = FX (x(y)) = 1 - e-λln(y) = 1 - y-λ . This distribution is known as the (strict)\nPareto Distribution.\nPower Functions\n-\nSuppose Y = X1/α, α > 0,\nx\n⇒\nFY (y) = FX (x(y)) = 1 - e-λyα .\nDistribution.\n- Log Functions\n= ln(y), y ≥ 0. This is a monotonic increasing function, and\nThis distribution is known as the Weibull (Extreme Type III)\nSuppose Y = -ln(X), ⇒ x = e-y , -inf ≤ y ≤inf. This is a monotonic decreasing function, and\nFY (y) = 1 - FX (x(y)) = e-λe-y . This distribution is known as the Gumbell (Extreme Type I)\nDistribution.\n\n\" #\nRR\nZ\nZ\n(b) Functions of Two or More Random Variables\nProblem\n-\nX\nGiven the JCDF of the random vector\n, FX,Y (x, y), and a deterministic function Z = g(x, y),\nY\nfind the (derived) distribution of the random variable Z.\nGeneral Solution\n-\nLet ΩZ = {x, y : g(x, y) ≤ z}. Then:\nFZ (z) = P [Z ≤ z] = P [(x, y) ∈ ΩZ ] =\nfX,Y (x, y)dxdy\nΩZ\n- Special Cases\n- Minimum/maximum functions\ni.e. Z = Min[X1, X2, . . . , Xn] (eg. minimum strength)\nor Z = Max[X1, X2, . . . , Xn] (eg. maximum load)\n- Z = Min[X1, X2, . . . , Xn]. For n = 2,\nZZ\nFZ (z) = P [Z ≤ z] =\nfX1,X2 (x1, x2)dx1dx2,\nwith ΩZ shown in figure\nΩZ\ninf\ninf\n= 1 -\ndx1\nfX1,X2 (x1, x2)dx2\nz\nz\n\nR\nR\n\" \\\n#\nY\nP\nIf X1 and X2 are independent:\ninf\ninf fX1,X2 (x1, x2)dx2 = [1 - FX1 (z)][1 - FX2 (z)]\ndx1\nz\nz\nTherefore,\nFZ (z) = 1 - [1 - FX1 (z)][1 - FX2 (z)]\nFor n iid variables:\nFZ (z) = P [Z ≤ z] = 1 - P [(X1 > z) ∩ . . . ∩ (Xn > z)]\n= 1 - [1 - FX (z)]n\nor, with GX (x) = 1 - FX (x),\nGZ (z) = P [Z > z] = [GX (z)]n\nfZ (z) = d FZ (z) = - d GZ (z) = n[GX (z)]n-1fX (z)\ndz\ndz\n- Z = Max[X1, X2, . . . , Xn]\n⎤\n⎡\nFZ (z) = P\n(Xi ≤ z)\n= FX\n⎢⎢⎣\nz\n. . .\n⎥⎥⎦\ni\nz\n=\nFXi (z) (if Xi's are independent)\ni\n= [FX (z)]n\nand\nfZ (z) = n[Fx(z)]n-1fX (z) (if Xi's are iid)\nLinear transformations\n-\nY =\naixi\ni\n\nZ\nZ\nR\n- Simplest case: Y = X1 + X2\nZZ\nFY (y) = P [Y ≤ y] = P [x1 + x2 ≤ y] =\nfX1,X2 (x1, x2)dx1dx2\nx1+x2≤y\ninf\ny-x2\n=\ndx2\nfX1,X2 (x1, x2)dx1\nZ\n-inf\n-inf\ninf\nfY (y) =\nfX1,X2 (y - x2, x2)dx2\n-inf\nIf X1 and X2 are independent, then:\nfY (y) =\ninf fX1 (y - x2)fX2 (x2)dx2\n(convolution)\n-inf\n- Example: Derivation of Gamma distribution\nConsider Y = X1 + X2, where X1 and X2 are iid exponential , with density:\n⎧\n⎨λe-λx ,\nx ≥ 0\nfXi = ⎩0,\nx < 0\nThen,\n\nZ\ninf\nfY (y) =\nfX (y - x1)fX (x1)dx1\n= λ2 ye-λy\n(Rayleigh or Gamma (2) distribution)\nIn general, for any n, the probability density of Y = X1 + X2 + . . . + Xn, where Xi are\niid exponential, is:\nλ(λy)n-1e-λy\nfY (y) =\nΓ(n)\n,\ny ≥ 0, where Γ(n) = (n - 1)!\n(Gamma(n) distribution)\nNote: For n = 1, the Gamma distribution reduces to the exponential distribution."
    },
    {
      "category": "Resource",
      "title": "Brief Notes #6",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/1-010-uncertainty-in-engineering-fall-2008/c984d7981d383c8d7d8d9f62917cb61c_notes_06.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n1.010 Uncertainty in Engineering\nFall 2008\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu.terms.\n\nX\nZ\nX\nZ\nP\n1.010 - Brief Notes # 6\nSecond-Moment Characterization of Random Variables and Vectors.\nSecond-Moment(SM) and First-Order Second-Moment(FOSM)\nPropagation of Uncertainty\n(a) Random Variables\n.\nSecond-Moment Characterization\n-\n- Mean (expected value) of a random variable\nE[X] = mX =\nxiPX (xi) (discrete case)\nall xi\ninf\n=\nxfX (x)dx (continuous case)\n-inf\n- Variance (second central moment) of a random variable\nσ2 = V ar[X] = E[(X - mX )2] =\n(xi - mX )2PX (xi) (discrete case)\nX\nall xi\nσ2 =\ninf\n(x - mX )2fX (x)dx (continuous case)\nX\n-inf\n- Examples\nPoisson distribution\n-\n(λt)y e-λt\nPY (y) =\n,\ny = 0, 1, 2, . . .\ny!\nmY = λt\nσ2 =\ninf\n(y - λt)2PY (y) = λt = mY\nY\ny=0\n\np\n- Exponential distribution\nfX (x) = λe-λx ,\nx ≥ 0\nmX = λ\n\nR\nσ2 = 0\ninf x - λ\nfX (x)dx = λ = m2\nX\nX\nNotation\n-\nX ∼ (m, σ2) indicates that X is a random variable with mean value m and variance σ2 .\nOther measures of location\n-\n- Mode x = value that maximizes PX or fX\n- Median x50 = value such that FX (x50) = 0.5\n- Other measures of dispersion\nStandard deviation\n-\nσX =\nσ2\n(same dimension as X)\nX\nCoefficient of variation\n-\nVX = σX\n(dimensionless quantity)\nmX\n- Expectation of a Function of a Random Variable. Initial and Central Moments.\n- Expected value of a function of a random variable\nLet Y = g(X) be a function of a random variable X. Then the mean value of Y is:\n\nR\nR\nR\nR\nE[Y ] = E[g(X)] =\ninf yfY (y)dy\n-inf\nImportantly, it can be shown that E[Y ] can also be found directly from fX , as:\nE[Y ] =\ninf g(x)fX (x)dx\n-inf\n- Linearity of expectation\nIt follows directly from the above and from linearity of integration that, for any constants a1\nand a2 and for any functions g1(X) and g2(X):\nE[a1g1(X) + a2g2(X)] = a1E[g1(X)] + a2E[g2(X)]\n- Expectation of some important functions\n1. E[Xn] =\ninf xnfX (x)dx\n-inf\n(called initial moments; the mean mX is also the first initial moment)\n2. E[(X - mX )n] =\ninf (x - mX )nfX (x)dx\n-inf\n(called central moments; the variance σ2 is also called the second central moment)\nX\n- Consequences of Linearity of Expectation.\nSecond-Moment(SM) Propagation of\nUncertainty for Linear Functions.\n1. σX\n2 = V ar[X] = E[(X - mX )2] = E[X2] - 2mX E[X] + mX\n2 = E[X2] - m2\nX\nE[X2] = σ2 + m2\n⇒\nX\nX\n2. Let Y = a + bX, where a and b are constants. Using linearity of expectation, one obtains the\nfollowing expressions for the mean value and variance of Y :\nmY = a + bE[X] = a + bmX\nσ2 = E[(Y - mY )2] = b2σ2\nY\nX\n\n- First-Order Second-Moment(FOSM) Propagation of Uncertainty for Nonlinear\nFunctions\nUsually, with knowledge of only the mean value and variance of X, it is impossible to calculate mY\nand σY\n2 . However, a so-called first-order second-moment(FOSM) approximation can be obtained as\nfollows.\nGiven X ∼ (mX , σ2 ) and Y = g(X), a generic nonlinear function of X, find the mean value and\nX\nvariance of Y .\nReplace g(X) by a linear function of X, usually by linear Taylor expansion around mX . This\n→\ngives the following approximation to g(X):\n(X - mX )\n= g(X) ≈ g(mX ) + dg(X)\nY\ndX\nmX\nThen approximate values for mY and σY\n2 are:\n!2\nσ2\nX\ndg(X)\nσ2 =\nY\ndX\nmY = g(mX ),\nmX\n(b) Random Vectors\n.\nSecond-Moment Characterization. Initial and Central Moments.\n-\nConsider a random vector X with components X1, X2, . . . , Xn.\n- Expected value\n⎤\n⎡\n⎤\n⎡\n⎤\n⎡\nE[X1]\nX1\nm1\nE[X] = E\n⎢⎢⎣\n⎥⎥⎦ =\n⎢⎢⎣\n⎥⎥⎦ =\n⎢⎢⎣\n⎥⎥⎦ = m (mean value vector)\n. . .\n. . .\n. . .\nXn\nE[Xn]\nmn\n- Expected value of a scalar function of X\nLet Y = g(X) be a function of X. Then, extending a result given previously for functions of\nsingle variables, one finds that E[Y ] may be calculated as:\n\nR\nR\nR\nZ\nZ\nE[Y ] =\ng(x)fX (x)dx\nRn\nAgain, it is clear that linearity applies, in the sense that, for any given constants a1 and a2 and\nany given functions g1(X) and g2(X):\nE[a1g1(X) + a2g2(X)] = a1E[g1(X)] + a2E[g2(X)]\n- Expectation of some special functions\nInitial moments\n-\n1. Order 1:\nE[Xi] = mi ⇔ E[X] = m,\ni = 1, 2, . . . , n\n2. Order 2:\nE[XiXj ] =\ninf\ninf xixj fXi ,Xj (xi, xj )dxidxj ,\ni, j = 1, 2, . . . , n\n-inf -inf\n3. Order 3:\nE[XiXj Xk] = . . . ,\ni, j, k = 1, 2, . . . , n\nCentral moments\n-\n1. Order 1:\nE[Xi - mi] = 0,\ni = 1, 2, . . . , n\n2. Order 2 (covariance between two variables):\nCov[Xi, Xj ] = E[(Xi - mi)(Xj - mj )],\ni, j = 1, 2, . . . , n\ninf\ninf\n=\n(xi - mi)(xj - mj )fXi,Xj (xi, xj )dxidxj\n-inf\n-inf\nCovariance in terms of first and second initial moments\n-\nUsing linearity of expectation,\nCov[Xi, Xj ] = E[(Xi - mi)(Xj - mj )] = E[XiXj - Ximj - miXj + mimj ]\n= E[XiXj ] - mimj\nE[XiXj ] = Cov[Xi, Xj ] + mimj\n⇒\n\n\"\n#\nCovariance Matrix and Correlation Coefficients\n-\nCovariance matrix\n-\nCov[Xi, Xj ]\n⎤\n⎡\n⎢⎢⎣\n⎥⎥⎦\nΣX =\n.. .\n(i, j = 1, 2, . . . , n)\n)T ]\nx\n= E[(X - m )(X - m\nx\n- For n = 2:\nσ2\nCov[X1, X2]\nΣX =\nCov[X2, X1]\nσ2\n- ΣX is the matrix equivalent of σ2\nX\n- ΣX is symmetrical:\nΣX = ΣT\nX\nCorrelation coefficient between two variables\n-\nCov[Xi, Xj ]\nρij =\nσiσj\n,\ni, j = 1, 2, . . . , n,\n-1 ≤ ρij ≤ 1\n- ρij is a measure of linear dependence between two random variables;\n- ρij has values between -1 and 1, and is dimensionless.\n\nP\n\nP\nP\nP\nP P\n- SM Propagation of Uncertainty for Linear Functions of Several Variables\nn\nLet Y = a0 +\naiXi = a0 + a1X1 + a2X2 +\n+ anXn be a linear function of the vector X. Using\ni=1\n· · ·\nlinearity of expectation, one finds the following important results:\nn\nn\nE[Y ] = E a0 +\naiXi = a0 +\naimi\ni=1\ni=1\nn\nn\nn\nV ar[Y ] =\na2\ni V ar[Xi] + 2\naiaj Cov[Xi, Xj ]\ni=1\ni=1 j=i+1\n\nP\n\n- For n = 2:\nY = a0 + a1X1 + a2X2\nE[Y ] = a0 + a1E[X1] + a2E[X2]\nV ar[Y ] = a2\n1V ar[X1] + a2\n2V ar[X2] + 2a1a2Cov[X1, X2]\n- For uncorrelated random variables:\nV ar[Y ] =\nn\na\nP\ni=1\ni V ar[Xi]\nExtension to several linear functions of several variables\n-\nLet Y be a vector whose components Yi are linear functions of a random vector X. Then, one\ncan write Y = a + B X, where a is a given vector and B is a given matrix. One can show that:\nmY = a + B mX\nΣY = B ΣX BT\n- FOSM Propagation of Uncertainty for Nonlinear Functions of Several Variables\nLet X ∼ (mX , ΣX ) be a random vector with mean value vector mX and covariance matrix ΣX .\nConsider a nonlinear function of X, say Y = g(X). In general, mY and σY\n2 depend on the entire\njoint distribution of the vector X. However, simple approximations to mY and σY\n2 are obtained by\nlinearizing g(X) and then using the exact SM results for linear functions. If linearization is obtained\nthrough linear Taylor expansion about mX , then the function that replaces g(X) is:\nn\ni=1 ∂Xi\n∂g(X)\ng(X) ≈ g(mX ) +\n(Xi - mi)\nX=mX\nwhere mi is the mean value of Xi. The approximate mean and variance of Y are then:\nmY = g(mX ),\n\nP\n\nσ2\nY =\nbibj Cov[Xi, Xj ]\nP\nn\nn\ni=1 j=1\n∂g(X)\nwhere bi = ∂Xi\nX=mX\nThis way of propagating uncertainty is called FOSM analysis."
    },
    {
      "category": "Resource",
      "title": "Brief Notes #7",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/1-010-uncertainty-in-engineering-fall-2008/7d700f8750ca621ee14e370eeedaf000_notes_07.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n1.010 Uncertainty in Engineering\nFall 2008\nFor information about citing these materials or our Terms of Use, visit:http://ocw.mit.edu/terms.\n\n1.010 - Brief Notes #7\nConditional Second-Moment Analysis\n- Important result for jointly normally distributed variables X1 and X2\nIf X1 and X2 are jointly normally distributed with mean values m1 and m2, variances\nσ1\n2 and σ2\n2, and correlation coefficient ρ, then (X1 | X2 = x2) is also normally\ndistributed with mean and variance:\n\nσ1\nm1|2(x2) = m1 + ρ\n(x2 - m2)\n\nσ2\n\n(1)\n\nσ1\n|2(x2) = σ2(1-ρ2)\n\nNotice that the conditional variance does not depend on x2.\nThe results in Eq. 1 hold strictly when X1 and X2 are jointly normal, but may be used\nin approximation for other distributions or when one knows only the first two\nX1\nmoments of the vector X =\n.\nX 2\n- Extension to many observations and many predictions\nLet X =\nX1\n, where X1 and X2 are sub-vectors of X. Suppose X has multivariate\nX 2\nnormal distribution with mean value vector and covariance matrix:\nm =\nm1\n,\nand\nΣ =\nΣ11\nΣ12\n\n(Σ12 = Σ21\nT).\nm 2\nΣ21\nΣ 22\nThen, given X2 = x2, the conditional vector (X1 | X2 = x2) has jointly normal\ndistributions with parameters:\nm1|2(x 2) = m1 + Σ12 Σ\n-\n1(x 2 - m 2)\n\n(2)\n\n-1\nT\n\nΣ1|2(x 2) = Σ11 -Σ12 Σ22 Σ12\nNotice again that Σ1|2 does not depend on x2.\nAs for the scalar case, Eq. 2 may be used in approximation when X does not have\nmultivariate normal distribution or when the distribution of X is not known, except\nfor the mean vector m and covariance matrix Σ."
    },
    {
      "category": "Resource",
      "title": "Brief Notes #8",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/1-010-uncertainty-in-engineering-fall-2008/a53f4d787c758a4af86c14bd78cf1f2a_notes_08.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n1.010 Uncertainty in Engineering\nFall 2008\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu.terms.\n\nP\nP\n1.010 - Brief Notes # 8\nSelected Distribution Models\n- The Normal (Gaussian) Distribution:\nLet X1, . . . , Xn be independent random variables with common distribution FX (x). The so called\ncentral limit theorem establishes that, under mild conditions on FX , the sum Y = X1 + . . . + Xn\napproaches as n →inf, a limiting distributional form that does not depend on FX . Such a limiting\ndistribution is called the Normal or Gaussian distribution. It has the probability density function:\nfY (y) =\ne- 2 (y-m)2/σ2\n√\n2πσ\nwhere\nm = mean value of Y\nσ = standard deviation of Y\nNotice: m = nmX , σ2 = nσX 2, where mX and σX 2 are the mean value and variance of X.\n- Properties of the Normal (Gaussian) Distribution:\n1. For most distributions FX , convergence to the normal distribution is obtained already for n as\nsmall as 10.\n2. Under mild conditions, the distribution of\nXi approaches the normal distribution also for\ni\ndependent and differently distributed Xi.\n3. If X1, . . . , Xn are independent normal variables, then any linear function Y = a0 +\naiXi is\ni\nalso normally distributed.\n\nP\n- The Lognormal Distribution:\nLet Y\nWn, where the Wi are iid , positive random variables. Consider:\n= W1W2 · · ·\nX = ln Y =\nln Wi\nall i\nFor n large, X ∼ N(mln Y , σ2\n)\nln Y\nY = eX has a lognormal distribution with PDF:\ndx\n2 (ln Y -mln Y )2/σ2\nln Y ,\nfY (y) = dy fX [x(y)] = y √\n2πσln Y\ne-\ny ≥ 0\n\nIf X ∼ N(mX , σX 2), then Y = eX ∼ LN(mY , σY 2) with mean value and variance given by:\n⎧\n⎨mY\n= emX + 2\n1 σX\nσY 2\n= e2mX +σX\n2 eσX\n2 - 1\nConversely, mX and σX 2 are found from mY and σY 2 as follows:\n⎩\n⎧\n⎨\nmX\n= 2 ln(mY ) - 2 ln(σY 2 + m2\nY )\nσX 2\n= -2 ln(mY ) + ln(σY 2 + mY\n2 )\nProperty: products and ratios of independent lognormal variables are also lognormally distributed.\n⎩\n\nP\nThe Beta Distribution:\n-\nThe Beta distribution is commonly used to describe random variables with values in a finite interval.\nThe interval may be normalized to be [0, 1]. The Beta density can take on a wide variety of shapes.\nIt has the form:\nfY (y) ∝ ya(1 - y)b\nwhere a and b are parameters. For a = b = 0, the Beta distribution becomes the uniform distribution.\nMultivariate Normal Distribution:\n-\nn\nConsider Y =\nXi, where the Xi are iid random vectors.\ni=1\nAs n becomes large, the joint probability density of Y approaches a form of the type:\n(det Σ)-\nn\n2 (y-m)T Σ-1(y-m)\nfY (y) =\ne-\n(2π) 2\nwhere m and Σ are the mean vector and covariance matrix of Y .\n\n\"\n#\n\"\n#\nProperties\n1. Contours of fY are ellipsoids centered at m.\n2. If the components of Y are uncorrelated, then they are independent.\n3. The vector Z = a + B Y , where a is a given vector and B is a given matrix, has jointly normal\ndistribution N(a + B m, B Σ BT ).\nLet Y 1\nbe a partition of Y , with associated partitioned mean vector m1\nand covariance\nY 2\"\n#\nm2\nΣ11\nΣ12\nmatrix\n. Then:\nΣ21\nΣ22\n4. Y i has jointly normal distribution: N(mi, Σii).\n5. (Y 1 Y 2 = y ) has normal distribution N(m1 + Σ12Σ-1(y\nΣ-1ΣT ).\n|\n2 - m2), Σ11 - Σ12\n\nRelationships between Mean and Variance of Normal and Lognormal Distributions\nIf X ∼ N(mX , σX 2), then Y = eX ∼ LN(mY , σY 2) with mean value and variance given by:\n⎧\n⎨mY\n= emX + 2\n1 σX\nσY 2\n= e2mX +σX\n2 eσX\n2 - 1\nConversely, mX and σX 2 are found from mY and σY 2 as follows:\n⎩\n⎧\n⎨\nmX\n= 2 ln(mY ) - 2 ln(σY 2 + m2\nY )\nσX 2\n= -2 ln(mY ) + ln(σY 2 + m2 )\nY\n⎩"
    },
    {
      "category": "Resource",
      "title": "Brief Notes #9",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/1-010-uncertainty-in-engineering-fall-2008/1e2812e44a7977b4ff24a244d7f0dd70_notes_09.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n1.010 Uncertainty in Engineering\nFall 2008\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu.terms.\n\nP\nP\n1.010 - Brief Notes # 9\nPoint and Interval Estimation of Distribution Parameters\n(a) Some Common Distributions in Statistics\n.\n- Chi-square distribution\nLet Z1, Z2, . . . , Zn be iid standard normal variables. The distribution of\nn\nχ2\nZ2\n=\nn\ni\ni=1\nis called the Chi-square distribution with n degrees of freedom.\nE[χ2\nn] = n\nV ar[χ2\nn] = 2n\nProbability density function of χ2\nn for n = 2, 5, 10.\nt distribution\n-\nLet Z, Z1, Z2, . . . , Zn be iid standard normal variables. The distribution of\nZ\ntn =\n1/2\nn\nZ2\nn\ni\ni=1\nis called the Student's t distribution with n degrees of freedom.\n\nP\nE[tn] = 0\n⎧\n⎨ n\n,\nn > 2\nV ar[tn] = ⎩\nn - 2\ninf,\nn ≤ 2\nProbability density function of tn for n = 1, 5, inf.\nNote: t\n= N(0, 1).\ninf\nF distribution\n-\nLet W1, W2, . . . , Wm, Z1, Z2, . . . , Zn be iid standard normal variables. The distribution of\nm\nW 2\nm\ni\n1 χ2\nFm,n =\ni=1\n= m\nm\nn\nP\nχ2\nZ2\nn\nn\nn\ni\ni=1\nis called the F distribution with m and n degrees of freedom.\nAs n →inf,\nmFm,n → χm\n\nb\nb\nb\n(b) Point Estimation of Distribution Parameters: Objective and Criteria\n.\n- Definition of (point) estimator\nLet θ be an unknown parameter of the distribution FX of a random variable X, for example the\nmean m of the variance σ2 . Consider a random sample of size n from the statistical population\nof X, {X1, X2, . . . , Xn}. An estimator Θ of θ is a function Θ(X1, X2, . . . , Xn) that produces a\nnumerical estimate of θ for each realization x1, x2, . . . , xn of X1, X2, . . . , Xn.\nΘ is a random\nNotice: b\nvariable whose distribution depends on θ.\n- Desirable properties of estimators\n1. Unbiasedness:\nΘ is said to be an unbiased estimator of θ if, for any given θ, Esample[Θb|θ] = θ. The bias bb(θ)\nΘ\nof bΘ is defined as:\nbb(θ) = Esample[Θb θ] - θ\nΘ\n|\n2. Mean Squared Error (MSE):\nThe mean squared error of Θ is the second initial moment of the estimation error e\nb\n= Θb - θ,\ni.e.,\nMSEb(θ) = E[(Θb - θ)2] = b2 (θ) + V ar[Θb θ]\nΘ\nb\n|\nΘ\nOne would like the mean squared error of an estimator to be as small as possible.\n(c) Point Estimation of Distribution Parameters: Methods\n.\n1. Method of moments\nSuppose that FX has unknown parameters θ1, θ2, . . . , θr. The idea behind the method of moments is to\nestimate θ1, θ2, . . . , θr so that r selected characteristics of the distribution match their sample values. The\ncharacteristics are often taken to be the initial moments:\nμi = E[Xi],\ni = 1, . . . , r\nThe method is described below for the case r = 2.\n\nR\nR\nb\nb\nThe first and second initial moments of X are, in general, functions of the unknown parameters, θ1 and\nθ2:\nμ1(θ1, θ2) = E[X|θ1, θ2] =\nxfX|θ1 ,θ2 (x)dx\nμ2(θ1, θ2) = E[X2|θ1, θ2] =\nx2fX|θ1,θ2 (x)dx\nThe sample values of these moments are:\nn\n1 P\nμb1 =\nXi = X\nn i=1\nn\n1 P\nμb2 =\nXi\nn i=1\nEstimators of θ1 and θ2 are obtained by solving the equations for Θb 1 and Θb 2:\nμ1(Θb 1, Θb 2) = μb1\nμ2(Θb 1, Θb 2) = μb2\nThis method is often simple to apply, but may produce estimators that have higher MSE than other\nmethods, e.g. maximum likelihood.\nExample:\nIf θ1 = m and θ2 = σ2, then:\nμ1 = m and μ2 = m2 + σ2\n1 P\nP\nn\nn\nμb1 =\nXi = X and μb2 =\nXi\nn i=1\nn i=1\nThe estimators mb and σb2 are obtained by solving:\nm = X\nn\n1 P\nmb 2 + σb2 =\nXi\nn i=1\nwhich gives:\nm = X\n\nQ\nn\n1 P\nσb2 =\nXi\n- X\nn i=1\nn\n1 P\n=\n(X1 - X)2\nn i=1\nNotice that σb2 is a biased estimator since its expected value is n - 1 σ2 . For this reason, one typically\nn\nuses the modified estimator:\nn\nP\nS2 = n - 1 i=1\n(Xi - X)2\nwhich is unbiased.\n2. Method of maximum likelihood:\nConsider again the case r = 2. The likelihood function of θ1 and θ2 given a sample, L(θ1, θ2| sample), is\ndefined as:\nL(θ1, θ2| sample) ∝ P [sample |θ1, θ2]\nWhere P is either probability or probability density and is regarded for a given sample as a function of\nθ1 and θ2. In the case when X is a continuous variable:\nn\nP [sample θ1, θ2] =\nfX (xi θ1, θ2)\n|\ni=1\n|\nThe maximum likelihood estimators (Θb 1)ML and (Θb 2)ML are the values of θ1 and θ2 that maximize the\nlikelihood, i.e.,\nL(θ1, θ2| sample) is maximum for θ1 = (Θb 1)ML and θ2 = (Θb 2)ML\nIn many cases, (Θb 1)ML and ( Θb 2)ML can be found by imposing the stationarity conditions:\n∂L[(Θb 1, Θb 2)| sample] = 0\nand\n∂L[(Θb 1, Θb 2)| sample] = 0\n∂Θb 1\n∂Θb 2\nor, more frequently, the equivalent conditions in terms of the log-likelihood:\n\n∂(ln L[(Θb 1, Θb 2)| sample]) = 0\nand\n∂(ln L[(Θb 1, Θb 2)| sample]) = 0\n∂Θb 1\n∂Θb 2\n- Properties of maximum likelihood estimators:\nAs the sample size n →inf, maximum likelihood estimators:\n1. are unbiased;\n2. have the smallest possible value of MSE.\n- Example:\nFor X ∼ N(m, σ2) with unknown parameters m and σ2, the maximum likelihood estimators of the\nparameters are:\nn\n1 P\nσ2\nmb ML =\nXi = X ∼ N m,\nn i=1\nn\nn\n1 P\nσb2\n=\nmML)2\nML\n(Xi - b\nn i=1\nn\n1 P\nσ2\n= n i=1\n(Xi - X)2 ∼ n χ2\n(n-1)\nNotice that in this case the ML estimators m and σ2 are the same as the estimators produced by\nthe method of moments. This is not true in general.\n3. Bayesian estimation\nThe previous two methods of point estimation are based on the classical statistical approach which\nassumes that the distribution parameters θ1, θ2, . . . , θr are constants but unknown. In Bayesian\nestimation, θ1, θ2, . . . , θr are viewed as uncertain (random variables) and their uncertainty is quantified\nthrough probability distributions. There are 3 steps in Bayesian estimation:\nStep 1: Quantify initial\nStep 2: Use sample\nStep 3: Choose a single\nuncertainty on θ in the\ninformation to update\nvalue estimate of θ\nform of a prior distribution,\nuncertainty → posterior\nf 0\nθ\ndistribution, f 00\nθ\n\n-\nR\nThe various steps are described below in the order 2, 3, 1.\nStep 2: How to update prior uncertainty given a sample\nRecall that for random variables,\nfθ|X ∝ fθ(θ) fX|θ(x)\n·\nHere, fθ\n0 = fθ and fθ\n00 = fθ|X . Further, using `(θ|X) ∝ fX|θ(x), one obtains:\nf 00(θ) ∝ f 0(θ)`(θ|X)\nθ\nθ\nStep 3: How to choose bθ\nTwo main methods:\n1. Use some characteristic of fθ\n00, such as the mean or the mode. The choice is rather arbitrary. Note\nthat the mode corresponds in a sense to the maximum likelihood, applied to the posterior distribution\nrather than the likelihood.\n2. Decision theoretic approach: (more objective and preferable)\nθ by θb.\n- Define a loss function $(bθ|θ) which is the loss if the estimate is bθ and the true value is θ.\nCalculate the expected posterior loss or \"Risk\" of bθ as:\nR(bθ) = E00[$(bθ θ)] =\ninf $(bθ θ)fθ\n00(θ)dθ\n|\n-inf\n|\nChoose bθ such that R(bθ) is minimum.\n-\n- If $(bθ|θ) is a quadratic function of (θbi - θi), then R(bθ) is minimum for bθ = E00[θ]\n⎧\n⎨0,\nif bθ = θ\n- If $(bθ|θ) = ⎩c > 0,\nif bθ = θ\n,\nthen bθ is the mode of fθ\n00.\nStep 1: How to select fθ\n1. Judgementally. This approach is especially useful in engineering design, where subjective judgement\nis often neccessary. This is how subjective judgement is formally incorporated in the decision process.\n\n2. Based on prior data e.g. a \"sample\" of θ's from other data sets.\n3. To reflect ignorance, \"non-informative prior\".\nFor example, if θ is a scalar parameter that can attain values from -inf to +inf,\nthen f 0(θ)dθ ∝ dθ (\"flat\") and f 00(θ) ∝ `(θ| sample) i.e. the posterior reflects only the likelihood.\nθ\nθ\nIf θ > 0, then one typically takes f 0\n(ln θ)d ln θ ∝ d ln θ. In this case, f 0\n.\nln θ\nθ(θ) ∝ θ\n4. Conjugate prior. There are distribution types such that if f 0(θ) is of that type, then f 00(θ) ∝ f 0(θ)`(θ)\nθ\nθ\nθ\nis also of the same type. Such distributions are called conjugate distributions.\nExample:\nLet:\nX ∼ N(m, σ2) with σ2 known. θ = m unknown.\nSuppose: f 0 ∼ N(m0, σ02)\nm\nIt can be shown that `(m|X1, . . . , Xn) ∝ density of N(X, σ2/n)\nFrom f 00\nm`(m| sample), one obtains\nm ∝ f 0\nm0(σ2/n) + Xσ02\nn\nm\nf 00 ∼ N\nm00 =\n(σ2/n) + σ02\n, σ002 = σ02 + σ2\nIn this case, f 0\n∼ N(m0, σ02) is an example of a conjugate prior, since f 00 is also normal, of the type\nm\nm\nN(m00, σ002).\nσ2\nIf one writes σ02 =\n, then n0 has the meaning of equivalent prior sample size and m0 has the meaning\nn0\nof equivalent prior sample average.\n(d) Approximate Confidence Intervals for Distribution Parameters\n.\n1. Classical Approach\nProblem: θ is an unknown distribution parameter. Define two sample statistics Θb 1(X1, . . . , Xn) and\nΘb 2(X1, . . . , Xn) such that:\nP [Θb 1(X1, . . . , Xn) < θ < Θb 2(X1, . . . , Xn)] = P ∗\nwhere P ∗ is a given probability.\n\nAn interval [Θb 1(X1, . . . , Xn), Θb 2(X1, . . . , Xn)] with the above property is called a confidence interval\nof θ at confidence level P ∗.\nA simple method to obtain confidence intervals is as follows. Consider a point estimation bΘ such that,\nexactly or in approximation, Θb ∼ N(θ, σ2(θ)). If the variance σ2(θ) depends on θ, one replaces σ2(θ)\nwith σ2(bΘ). Then:\nΘb - θ\nΘ)\n∼ N(0, 1)\nσ(b\nP [Θb - σ(b\nΘ + σ(b\nΘ)ZP ∗/2 < θ < b\nΘ)ZP ∗/2] = P ∗\n⇒\nwhere Zα is the value exceeded with probability α by a standard normal variable.\nExample:\nθ = m = mean of an exponential distribution.\nIn this case, Θb = X ∼\nGamma(m, n), where Gamma(m, n) is the distribution of the sum of n iid\nn\nexponential variables, each with mean value m. The mean and variance of Gamma(m, n) are nm\nand nm2, respectively. Moreover, for large n, Gamma(m, n) is close to N(nm, nm2). Therefore, in\napproximation,\nm\nm,\nX ∼ N\nn\nUsing the previous method, an approximate confidence interval for m at confidence level P ∗ is\nX\nX\nX -√n · ZP ∗/2,\nX + √n · ZP ∗/2\n2. Bayesian Approach\nIn Bayesian analysis, intervals [θb1, θb2] that contain θ with a given probability P ∗ are simply obtained\nfrom the condition that:\nFθ\n00(θb2) - Fθ\n00(θb1) = P ∗\nwhere Fθ\n00 is the posterior CDF of θ."
    }
  ]
}