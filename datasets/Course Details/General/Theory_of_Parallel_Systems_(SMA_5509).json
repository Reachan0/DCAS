{
  "course_name": "Theory of Parallel Systems (SMA 5509)",
  "course_description": "6.895 covers theoretical foundations of general-purpose parallel computing systems, from languages to architecture. The focus is on the algorithmic underpinnings of parallel systems. The topics for the class will vary depending on student interest, but will likely include multithreading, synchronization, race detection, load balancing, memory consistency, routing networks, message-routing algorithms, and VLSI layout theory. The class will emphasize randomized algorithms and probabilistic analysis, including high-probability arguments.\nThis course was also taught as part of the Singapore-MIT Alliance (SMA) programme as course number SMA 5509 (Theory of Parallel Systems).",
  "topics": [
    "Engineering",
    "Computer Science",
    "Algorithms and Data Structures",
    "Theory of Computation",
    "Engineering",
    "Computer Science",
    "Algorithms and Data Structures",
    "Theory of Computation"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures:\n\nTwo sessions / week\n\nLec 1: 2 hours / session\n\nLec 2: 1.5 hours / session\n\nBoth lectures will be attended by students from Singapore. Lectures will be videotaped and made available through the\nSingapore-MIT Alliance\nprogram.\n\nIntroduction\n\nThis graduate research course is aimed at Ph.D. students in Area II. M.Eng. students who plan to write their Master's thesis on the topic of parallel systems may also wish to participate. This class cannot be used to satisfy the TQE requirements for the Ph.D. program.\n\nThis class covers theoretical foundations of general-purpose parallel computing systems, from languages to architecture. We will focus on the algorithmic underpinnings of parallel systems. The topics for the class will vary depending on student interest, but will likely include multithreading, synchronization, race detection, load balancing, memory consistency, routing networks, message-routing algorithms, and VLSI layout theory. The class will emphasize randomized algorithms and probabilistic analysis, including high-probability arguments.\n\nThe focus of the class is the term project, which can be done individually or in groups of two and will account for 70 percent of the grade. Students are encouraged to advance their own research interests in their project. The hope is that many of these term projects will lead to theses or publishable papers. In addition, there will be several problem sets and programming labs.\n\nA detailed schedule of topic coverage will appear in the\ncourse calendar\n.\n\nPrerequisites\n\nThe official prerequisites for the course are\n6.046J\n(Introduction to Algorithms) and\n6.170\n(Laboratory in Software Engineering) (or courses covering equivalent material), but since this class is intended for graduate students, a general undergraduate education in computer science is assumed.\n\nCourse Requirements\n\nThe course consists of three primary components:\n\nLectures\n: Each student creates scribe notes for approximately two lectures (more or less depending on the number of students in the course). Two students are assigned to each lecture, and they work together to generate high quality notes for the lecture. They then meet with one of the staff of the course to go over the notes generated, before being published to the web. Scribe notes should be produced as soon after the lecture as possible. Optimally, a first draft should be completed within a few days, as scheduled with the lecturer. The goal is to produce the final version of the scribe notes within a week of the lecture being given. Scribe notes will account for 20 percent of the grade.\n\nProblems\n: Each week new problems will be posted to the course web site. These will range in difficulty from easy exercises to open problems (noted by ***). Problems will be due two weeks from when they are posted on the website. When you submit the problem set, please complete the information for collaboration: Collaborators: Names (or \"None\") at the top of the cover page. You should also cite references appropriately. Problem sets will account for 10 percent of the grade.\n\nFinal Project\n: A major requirement of the course is a final project. The final project will account for 70 percent of the grade. Students will work either alone or in groups of two to complete a significant research project. As the semester goes on, we will post project ideas and open problems to the website. As people show interest in projects, we will add their name to the website, to encourage collaboration.\n\nCourse Books\n\nOne goal of this course is to produce a good set of scribe notes, describing the material presented in this course. To this end, it is recommended that you obtain a copy of the following book or some equivalent latex manual:\n\nLamport, Leslie.\nLaTex: A Document Preparation System User's Guide and Reference Manual\n. Reading, MA: Addison Wesley Professional, 1994. ISBN: 0201529831.\n\nCollaboration\n\nThroughout the class, collaboration is encouraged. The goal of homeworks is to give you practice in mastering the course material. Consequently, you are encouraged to collaborate on problem sets. You must write up each problem solution by yourself, however, even if you collaborate with others to solve the problem. If you do collaborate on homework, you must cite in a scholarly manner all of your collaborators.\n\nSimilarly, collaboration is expected on the final projects. Groups of two will probably be the norm. We will try to help group together students with similar interests to work on exciting open problems.",
  "files": [
    {
      "category": "Resource",
      "title": "lab1.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/862ed10b9ca719e6e2755c6bfd156908_lab1.pdf",
      "content": "!\n\n\"\n\n#\n$\n\n%\n&\n'\n(\n\n)\n#**+\n\n)\n\n,\n,\n\n-\n'\n\n'\n\n'\n\n.\n\n'\n\n')\n\n,\n,\n\n/\n'\n\n'\n\n)\n\n,\n,\n\n.-\n'\n\n'\n\n(\n\n,\n' &\n\n'\n\n/\n\n,\n/)\n\n' )\n\n'\n\n'\n,\n,\n'&\n\n(\n\n(\n\n'\n\n'\n\n+\n\n/\n\n,\n\n'\n\n.\n\n'\n\n'\n.-\n'\n\n')\n\n)\n\n,\n(\n'\n\n'\n\n'\n\n,\n\n,\n(\n'\n\n'\n\n'\n\n'\n)\n\n)\n\n:&\n\n'\n\n)\n'\n)\n\n,\n/\n\n;\n,)\n\n'\n\n(\n\n)\n\n'\n\n,\n\n'\n\n.\n\n&\n\n,\n<\n:\n\n'\n\n,\n\n,\n\n(\n\n/\n)\n=9*\n\n)\n,\n\n.\n\n& >\n\n,\n\n' <\n\n)\n\n+#-'\n\n?\n2)\n\n'\n\n:\n\n(\n\n/\n\n'\n\n'\n\n'\n\n.-\n'\n\n)\n(\n\n/\n\n,\n\n<\n\n,\n/\n\n'\n\n'\n\n'\n\n.\n\n,\n\n#\n\n/\n\n'\n\n)\n)\n(\n\n'\n-\n\n#\n#\n.\n\n'\n\n'\n\n#\n\n!\"#\n\n$%%&\n%\n'\n\n(\n)\n\n%\n\n%\n'\n)*\n\n+\n\n'\n\n+\n\n%\n\n%\n'\n*\n\n+\n,\n'\n\n%\n\n%\n'\n*\n\n+\n,\n'\n\n- .\n\n)/0*12\n\n.\n\n)/034*12\n\n5&\n\n- .\n*\n\n895 5):\n*\n\n%\n\n7- .\n\n%\n\n+\n/:2/;2<\n=\n\n)7&\n;\n+\n\n+\n+\n\n895 5):\n\n>\n\n*\n\n*\n\n,*\n+\n*\n+\n+*\n+\n\n?\n%\n)0*1\n/0312\n%\n1*#\n+/13#,2\n%\n0*#\n/03#,2\n\n*.*<\n%\n(<\n@<\n?\n%\n(<\n@,<\n?\n*\n(\n<\nA\nA\n%\n(<\n@<\n?\n%\n(<\n@,<\n?\n%\n.(<\n.@ <\n.33\n?\n*\n3(\n)*.\n\n.*<\nA\nA\nA\nA\n\n+\n+\n\n.'\n\n(\n\n=(\n\n/>\n\n)\n\n'\n\nÆ\n\n)\n\n9#\n(\n\n)\n,\n\n( -'\n\n@-'\n\n(\n\nA\nB\n\n'\n\n/\n\n(\n\n'\n\n(\n\n'\n\n,\n\n'\n\n. (\nC\n(\n\n.'\n\n'\n\n'\n\n'\n\n!\n\n'\n\n'\n\n,\n.-\n'\n\n'\n\n)\n,\n\n'\n\n.-\n'\n\n'\n\n,\n\n'\n\nC &\n\n-(8\n\n,\n\n<\n\n)\n\n)\n\n(\n\n'\n'\n(\n\n'\n\n'\n\n(\n\nD\n\n)\n\n.'\n\n.\n\n)\n\n,\n\n-(8\n\n(\n\n'\n\n()\n\n,\n\n'\n\n.'\n\n'\n\n.\n\n&\n?'\n\n'\n\n'\n\n?\n')\n:\n:)\n)\n\n.\n\n'\n\n-5\n\n)\n,\n\n'\n\n'\n\n/\n\n/\n'\n\n/\n'\n\n)\n(\n\n'\n\n'\n\n'\n\n8 --E\n.-\n'\n\n(\n\nC\n\n'\n\n(\n\nF\n\nG\n\nG\n\nG\n\nG\n\n/\n\n'\n\n,\n\n' &\n\n'\n\n.-\n'\n\n,\nH67\n,\n/\n\n.\n\n'\n\nA\n\n'\n\n.-\n'\n\n,\nH67\n,\n/\n\nH6\n-\n\n')\n\n'\n\n'\n\n'\n\n,\n\n'\n\n'\n\n,\n\n/\n(\n''\n\n'\n\nI\nJ8\n\n(\n\n,\n\n)\n\n'\n\n,\n\n#\nC\n\n'\n\n(\n\nF\n\n)\n,\n\nF\n\n:\n\n,\n\n'\n\n'\n)\n\n/\n(\n\n(\n\n.'\n,\n\n8 --E\n\n,\n\nF\n\nF\n\n,\n\nF\n\n,\n\nF\n\nG\n\nF\n\nG\n\nG\n\nG\n\nG\n\nF\n\nG\n\nG\n\nF\n\nG\n\nG\n\nG\n\nF\n\nG\n\n'''\n\n)\n\n'\n\n&\n\n(\n\nK\n\n'\n\n&\n#)\n'\n.\n\n'\n\n(\n\n,\n\n)\n\n'\n\nG\n\nG\n\nG\n\nG\n\nG\n\nG\n\nG\n\nK\n\n'\n\n(\n\n'\n\n(\n\n,\n/\n(\n\nF\nK\n\n6#7\nG\nH6\n\n,\n\nF\nH6\n\nF\n\n(\n\nL9)\n'\n+9M\n\n$\n(\n\n'\n\n,\n\n'\n-'\n\n,\n\n,\n\n'\n\n(\n\n' )\n\n&\n\n'\n\n'\n\n:&\n\n'\nE\n(\n\n'\n\n(\n\n'\n\n,\n\n'\n\n'\n\n'\n/\n\n'\n\n'\n\n(\n\n' )\nN\n\n$\n\n'\n\n' )\n\n' )\n\n')\n,\n\n/\n\n'&\n\n'\n.'\n\n,\n\n,\n\n.'\n\n,\n(\n\n,\n\n'\n\n'\n\n'\n\n''\n\n'\n\n6,\n\n7)\n\n(\n\n(\n\n)\n\n.'\n\n'\nC\n\n'\n)\n(\n\n(\n\n(\n\n'7\n\n'\n\n'\nE\n\n')\n&\n\n/\n\n'\n\n,\n' 8\n'\n\n)\n\n(\n\n(\n\n'\n#\n\n,\n\nF\n9)\n\n,\n\n(\n\n,\n\n'\n\n$\n.'\n\n.'\n\n.'\n\n,\n/)\n\n&\n,\n\n'\n\n,\n\n'\n\n(\n\n$\n\n'\n\n,\n\n.'\n\n<\n+\n\n-\n\n'\n\n,\n\nE\n\n.\n'\n\n(\n\n'\n\n8 ,)\n\n(\n\n,-\nN\n)\n\n'\n,\n\n)\n,\n\nE\n\n'\n\n'\n\nA\n: 8\n\n)\n\n'\n\n.'\n\n,\n\n'\n/\n'\n\n)\n\n,\n\n'&\n')\n\n,\n\n'\n\n'\n\n'\n\n(\n\n'\n\nÆ\n\n.'\n,\n\n(\n\n'\n\nO\n\n'\n,\n,\n\n'& P\n&\n\n,\n\n=\n\n'\n,\n\n'\n(\n>)\n=\n\n'\n(\n/\n'>)\n\n=\n\n>)\n=\n\n>)\n=\n\n>)\n=\n\n-(\n\n'>\n\n!\"\n!\n\n'\n\n!\"#$%\n\n)\n\n)\n/\n\n&'&(#&$))\n\n.'\n\n/\n\n6 !\")\n\n.'\n\n(\n\n/\n\n&'&(#&$\n\n'\n\n*+,\n\n(\n\n(\n\n.\n\n)\n\n,\n\n(\n\nK\n\n(\n\n&#))\n\n'\n\n,\n/\n\n'\n\n,\n:\n)\n\n-#)'\n,\n\n'\n\n'\n/\n\n'\n\n'\n\n'\n\n./\n\n&\n\n.\n\n(\n\n)\n\n.\n\n#\n#\n\n'\n\n.\n\n'\n\n)\n\n)\n\n,\n/\n\n'\n\n'\n\n)\n,\n\n'\n\n'\n\n'\n\n,\n\n)\n\n,\n\n)\n\n/\n'\n'\n\n' 5\n\n/\n'\n\n.'\n\n/\n\n(\n\n,\n\n'\n&\n./)\n\n(\n\n'\n\n'\n\n.'\n&#\n&\n\n%1\n!\n\n\"\n\n#\n$\n\n\"%\"\n\n\"\n\n#\n\n$\n\n.\n\n)\n\n(\n\n/\n\n'\n\n2 3 &'&\n\n)\n,\n\n'\n-'\n\n,-\n\n'\n\n.\n\n,\n\n'\n\n'\n\n/\n\n')\n\n'\n\n$&#4\n\n,\n\n- (\n\n/\n\n- (\n/\n\n%\n\n&\n\n'\n\n'\n\n&#)\n'\n:\n'\n8 )\n\n'\n\nE\n\n,\n\n)\n\n&#5\n7+8\n9+6\n\n#6\n7+8\n9:6\n\n$6\n7+8\n\n;\n\n'\n\nF\n\n)\n,\n)\n\n)\n\n'\n\n,\n\n,\n\n'\n\n7+8\n\n' 5\n\n$\n\n,\n,\n\n.\n\n)\n+7\n\n6#7\n+\n'\n\n.)\n\n#\n\n(\n\n,\n\n,\n\n,\n\n)\n\n+<\n#\n=\n>?\n\n8-5+6\n#6\n>;\n\n' 5\n\n(\n\n+<\n#\n=\n>?)\n\n'\n\n(\n\n-\n\n)\n'\n)\n'\n\n,\n\n)\n\n'\n\n,\n)\n\n!\n\n'\n\nE\n\n# &)\n\n'\n\n'\n\n,\n\n/\n\n(\n\n,)\n(\n\n/\n\n'\n\n'\n\n+#-'\n\n(\n\n'\n'\n\n(\n\nÆ\n\n'\n\n(\n\n,\n\n(\n%3\"\n1\"\n#6\n@3@6\n\n&6\n#\n#&#\n\nA&\n\n'&\n\nB*A\n**A\n(2#\n$&#\nC\n$&#\n%3\"(\n#\nC@\n#\n'#&\n# %#@3%\n%\n\n*)\n\n'\n\n'\n\n'\n\n)\n(\n\n,\n\n.' )\n\n'\n\n+#\n\n@\n'\n\n'\n)\n(\n\n,\n\n)\n\n,\n\n'\n\n)\n\n)\n\n,\n\n)\n(\n\n'\n\nL9M\n\n)\n\n)\nC\n\nC8\n\n)\n\n&\n\n$\n\"\n\n'\n#\n\n?\n,\n)\n#\n)\n#**9"
    },
    {
      "category": "Resource",
      "title": "ps1_soln.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/27e8e724162b60f2483bde77f8679052_ps1_soln.pdf",
      "content": "!\n\"\n\n#\n\n$!\n%\n\n\"\n\n&\n\n#\n\n'\n\n(\n\n)!\n\n*!\n\n&+\n,\n!\n\n-!\n\n,\n&+\n,\n*!!\n.!\n\n/!\n\n,\n&+\n,\n$!!\n0!\n\n1!\n\n%\n\n\"\n\n,\n\n.*\n\n!\n\n$1\n\n3!\n\n.*\n%\n\n!\n\"7\n\n$!\n%\n\n*\n\n)!\n\n*\n\n*!\n*\n\n)\n*\n\n-!\n*\n\n)\n*\n\n.!\n)$3\n\n)3\n/!\n$13\n0!\n4,\n\n1!\n\n&+\n,\n0!\n\n$13\n\n$1\n$3!\n'\n\n:\n\n+\n\n,\n&+\n$!\n3!\n\n1!\n\n;\n\n.*\n\n,\n\n<\n!\n&\n\n$\n\n)\n\n,\n,\n\n,\n\n,\n(\n\n,\n\n;\n,\n\n!\n\n;\n,\n\n!\n:\n\n%\n,\n\n$\n%\n\n,\n\n,\n\n;\n\n7,\n\n;\n\n,\n\n;\n\n,\n\n,\n\n;\n\n;\n\n!\n&\n\n;\n\n,\n\n&\n\n&\n\n,(\n\n;\n,\n\n;\n\n%\n\n,\n\n,#\n\n=\n\n,\n&\n\n%\n\n;\n\n;\n\n,\n\n$ !\n\n$$!\n\n;\n,\n\n,\n\n;\n,\n\n>\n\n>\n\n,\n\n,\n;\n\n,\n\n+\n\n,\n\n,\n\n,\n;\n\n&\n\n,\n\n?\n\n*\n\n:\n\n,\n\n\"\n\n;\n\n!\n\n%\n\n@\n;\n\n,\n\n%\n\n@\n,\n\n,\n\n;\n\n%\n\n,\n\n,\n\n,\n\n!\n\n$)!\n\n&+\n,\n$$!\n\n$*!\n\n!\n\n$-!\n\n$.!\n\n%\n\n>\n\n,\n\n+;\n\n>\n\n;\n\n&\n\n,\n\n%\n\n;\n\n,\n\n;\n\n):\n\n;\n\n&\n\n;\n\n-\n\n:\n\n$\n\n;\n#\n\nA\n\nA\n\n(\n*3\n\n%\n\n*3*\n\n%\n\n$\n\n(\n%\n\nB\n\n(\n*3\n\"7\n\n(\n\n$\n\n(\n*3\n\n(\n\n!\n\nB\n\n)3\n\n(\n*3\n\n)\n\n,\n\n;\n\n(\n\n(\n*3!\n\n)\n\n%\n\n)3*\n\n\"\n\n,\n\n,\n\n+\n\n,\n\n%\n\nB\n,\n\nX 100\n\n."
    },
    {
      "category": "Resource",
      "title": "ps1.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/30312bef5e50180a211e9f888e15ab8e_ps1.pdf",
      "content": "!\n\n!\n\n\"\n\n#\n\n$%\n\n&\n\n%\n\n'\n\n(\n)*\n\n(\n*\n+\n,\n\n-\n\n*\n\n.\n\n/\n\n%\n\n%\n\n/\n\n+\n\n%\n\n2+\n\n+\n\n+\n\n+\n\n(\n\n+\n\n+\n\n%\n\n+\n\n+\n\n!\n\n+\n\n%\n\n\"\n\n%\n\n&\n\n%\n\n.\n,\n\n%\n\n."
    },
    {
      "category": "Resource",
      "title": "ps2.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/dc4edaea6d330ede8da916ac6e86c55d_ps2.pdf",
      "content": "!\"\n\n#\n\n$\n\n%\n\n#\n\n$\n\n#\n\n\"\n\n&\n\n#\n\n#\n\n'\n(\n\n\"\n#"
    },
    {
      "category": "Lecture Notes",
      "title": "lecture1.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/4cbfff15cec4326bb2333e81aa5e6732_lecture1.pdf",
      "content": "Æ\n\n!\n\n\"\n\n!\n\n#\n\n$\n\n%\n\n&\n\n'#\n\n()\n\n()\n\n#\n()\n#\n\n()\n\n*\n(\n\n$\n\n$#\n\n)\n\n+\n\n$\n\n()\n\n()\n\n$\n(\n,-\n\n%!\n\n$\n)\n\n./\n\n()\n\n()\n\n()\n\n()\n\n()\n\n(\n\n$\n\n./\"\n\n)\n#\n\n&\n\n#\n\n(\n())\n\n3!\n\n!\n,\n\n+\n\n!\n\n-\n\n!\n\n!\n\n-\n\n$\n\n/\n!\n\n:\n\n$\n\n/\n!\n\n/\n\n\"\n%\n\n.\n\n(!\n\n!\n\n)\n\n&\n\n-&\n\n;&\n\n3&\n\n&\n\n!\n\n<\n\n!\n\n!\n\n!\n\n=\n-\n>\n\n#\n\nA\nB\nC\nA\nB\nC\nFib(3)\nA\nB\nC\nFib(2)\nA\nFib(1)\nA\nB\nC\nFib(2)\nA\nA\nA\nA\ncontinuation\nFib(4)\nfinal thread\nreturn\nspawn\nFib(1)\nFib(1)\nFib(1)\nFib(1)\ninitial thread\n\n#\n\n/\n\n!\n\n!\n\n?\n@\n/\n!\n\n?\nA\n\n&\n\n()\n\n-\n\n/\n\n!\n\n$\n\n;\n\n!\n\n!\n\n(#\n\n)\n\n!\n\n()\n\n:\n!\n\n/\n\n!\n\n!\n\n?\n\n()\n\n!\n\n(,\n\n&\n\n)\n-\n\"\n\n&\n\n&\n\n?\nB(\n)\n\n&\n\n?\n\n(\n)\n(\n\n!\n\n)\n\n&\n\n?\n(\n)\n\n!\n\n$\n\n-\n\n\"\n%\n\n/\n\n!\n\n$\n\n$\n\n!\n!\n\n$\n\n/\n!\n\n!\n\n!\n\nC\n\n!\n\n!\n\n$\n\n/\n\n!\n\n&\n\n&\n\n&\n\n:\n\n!\n\n#\n\nX\nX\n\n!\n\n\"\n\n!\n\n#\n\n!\n\nD\n\n()\n\n/\n\n!\n\n&\n!\n\nE\n\n!\n\n$\n\n!\n\n!\n\n#\n\n!\n\n$\n\n#\n\n!\n\n#\n\n/\n\n!\n\n!\n\nD\n\n!\n\"\n\n$\n$\n%\n$\n!\n\n>\n\n!\n\n&\n\n(\n\n)\n()\n/\n!\n\n&\n\n(\n\n)\n(F)\n\n(\n\n)\n(@)\n#\n\n>\n\n!\n\nD\n\n(A)\n\n(\n\n)\nD\n\n(\n\n)\n(7)\n\n(\n\n)\n(8)\n\nG\n\n$\n(\n\n)!\n\n(\n\n)\n-\n\n!\n#\n&\n\n!\n\n!\n\n?\n\n(\n\n)\n\nD\n\n()\n?\n\nD\n\n(\n\n)\n()\n?\nB(\n\n)\n()\n\n>\n\n!\n\n<\n:\n\n!\n\n\"\n\n*\n\n-\n\n!\n\nÆ\n\n!\n\n!\n\n!\n\n*\n\n$!\n\n#\n&\n\n?\nF!\n\n?\n8A!\n\n?\n\n?\n8!\n\n?\n8!\n\n?\nA\n:\n\n!\n\n!\n\n*\n\n&\n\n?\n\nD\n\n?\n\n?\n\nD\n\n?\n\n&\n\n$\n\n#\n\nF"
    },
    {
      "category": "Lecture Notes",
      "title": "lecture2.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/08fd01c65bec5dd4805a5088973f6e20_lecture2.pdf",
      "content": "6.895 Theory of Parallel Systems\nLecture 2\nCilk, Matrix Multiplication, and Sorting\nLecturer: Charles Leiserson\nLecture Summary\n1. Parallel Processing With Cilk\nThis section provides a brief introduction to the Cilk language and how Cilk schedules and executes\nparallel processes.\n2. Parallel Matrix Multiplication\nThis section shows how to multiply matrices efficiently in parallel.\n3. Parallel Sorting\nThis section describes a parallel implementation of the merge sort algorithm.\nParallel Processing With Cilk\nWe need a systems background to implement and test our parallel systems theories. This section gives\nan introduction to the Cilk parallel-programming language. It then gives some background on how Cilk\nschedules parallel processes and examines the role of race conditions in Cilk.\n1.1\nCilk\nThis section introduces Cilk. Cilk is a version of C that runs in a parallel-processing environment. It uses\nthe same syntax as C with the addition of the keywords spawn, sync, and cilk. For example, the Fibonacci\nfunction written in Cilk looks like this:\ncilk int fib(int n)\n{\nif(n < 2) return n;\nelse\n{\nint x, y;\nx = spawn fib(n-1);\ny = spawn fib(n-2);\nsync;\nreturn(x + y);\n}\n}\nCilk is a faithful extension of C, in that if the Cilk keywords are elided from a Cilk program, the result is\na C program which implements the Cilk semantics.\nA function preceded by cilk is defined as a Cilk function. For example,\ncilk int fib(int n);\ndefines a Cilk function called fib. Functions defined without the cilk keyword are typical C functions.\nA function call preceded by the spawn keyword tells the Cilk compiler that the function call can be made\n2-1\n\nsteal\npush\nprocedure\nstack\nstack\nframe\npop\nstack\nframe\npush\npop\nprocedure\nstack\n(a)\n(b)\nFigure 1: Call stack of an executing process. Boxes represent stack frames.\nasynchronously in a concurrent thread. The sync keyword forces the current thread to wait for asynchronous\nfunction calls made from the current context to complete.\nCilk keywords introduce several idiosyncracies into the C syntax. A Cilk function cannot be called with\nnormal C calling conventions - it must be called with spawn and waited for with sync. The spawn keyword\ncan only be applied to a Cilk function. The spawn keyword cannot occur within the context of a C function.\nRefer to the Cilk manual for more details.\n1.2\nParallel-Execution Model\nThis section examines how Cilk runs processes in parallel. It introduces the concepts of work-sharing and\nwork-stealing and then outlines Cilk's implementation of the work-stealing algorithm.\nCilk processes are scheduled using an online greedy scheduler. The performance bounds of the online\nscheduler are close to the optimal offline scheduler. We will look at provable bounds on the performance of\nthe online scheduler later in the term.\nCilk schedules processes using the principle of work-stealing rather than work-sharing. Work-sharing is\nwhere a thread is scheduled to run in parallel whenever the runtime makes an asynchronous function call.\nWork-stealing, in contrast, is where a processor looks around for work whenever it becomes idle.\nTo better explain how Cilk implements work-stealing, let us first examine the call stack of a vanilla C\nprogram running on a single processor. In figure 1, the stack grows downward. Each stack frame contains\nlocal variables for a function. When a function makes a function call, a new stack frame is pushed onto the\nstack (added to the bottom) and when a function returns, it's stack frame is popped from the stack. The\ncall stack maintains synchronization between procedures and functions that are called.\nIn the work-sharing scheme, when a function is spawned, the scheduler runs the spawned thread in\nparallel with the current thread. This has the benefit of maximizing parallelism. Unfortunately, the cost of\nsetting up new threads is high and should be avoided.\nWork-stealing, on the other hand, only branches execution into parallel threads when a processor is\nidle. This has the benefit of executing with precisely the amount of parallelism that the hardware can take\nadvantage of. It minimizes the number of new threads that must be setup. Work-stealing is the lazy way to\nput off work for parallel execution until parallelism actually occurs. It has the benefit of running with the\nsame efficiency as a serial program in a uniprocessor environment.\nAnother way to view the distinction between work-stealing and work-sharing is in terms of how the\nscheduler walks the computation graph. Work-sharing branches as soon and as often as possible, walking\nthe computation graph with a breadth-first search. Work-stealing only branches when necessary, walking\nthe graph with a depth-first search.\nCilk's implementation of work-stealing avoids running threads that are likely to share variables by schedul\ning threads to run from the other end of the call stack. When a processor is idle, it chooses a random processor\n2-2\n\nCall tree:\nViews\nA\nA\nB\nC\nD\nE\nF\nof stack:\nA\nB\nC\nE\nF\nA\nA\nA\nA\nB\nB\nB\nA\nC\nC\nD\nD\nE\nF\nFigure 2: Example of a call stack shown as a cactus stack, and the views of the stack as seen by each procedure.\nBoxes represent stack frames.\nand finds the sleeping stack frame that is closest to the base of that processor's stack and executes it. This\nway, Cilk always parallelizes code execution at the oldest possible code branch.\n1.3\nCactus Stack\nCilk uses a cactus stack to implement C's rule for sharing of function-local variables. A cactus stack is\na parallel stack implemented as a tree. A push maps to following a branch to a child and a pop maps to\nreturning to the parent in a tree. For example, the cactus tree in figure 2 represents the call stack constructed\nby a call to A in the following code:\nvoid A(void)\n{\nB();\nC();\n}\nvoid B(void)\n{\nD();\nE();\n}\nvoid C(void)\n{\nF();\n}\nvoid D(void) {}\nvoid E(void) {}\nvoid F(void) {}\nCilk has the same rules for pointers as C. Pointers to local variables can be passed downwards in the\ncall stack. Pointers can be passed upward only if they reference data stored on the heap (allocated with\nmalloc). In other words, a stack frame can only see data stored in the current and in previous stack frames.\nFunctions cannot return references to local variables.\nThe complete call tree is shown in figure 2. Each procedure sees a different view of the call stack based\non how it is called. For example, B sees a call stack of A followed by B, D sees a call stack of A followed by\nB followed by D and so on. When procedures are run in parallel by Cilk, the running threads operate on\n2-3\n\ntheir view of the call stack. The stack maintained by each process is a reference to the actual call stack, not\na copy of it. Cilk maintains coherence among call stacks that contain the same frames using methods that\nwe will discuss later.\n1.4\nRace Conditions\nThe single most prominent reason that parallel computing is not widely deployed today is because of race\nconditions. Identifying and debugging race conditions in parallel code is hard. Once a race condition has been\nfound, no methodology currents exists to write a regression test to ensure that the bug is not reintroduced\nduring future development. For these reasons, people do not write and deploy parallel code unless they\nabsolutely must. This section examines an example race condition in Cilk.\nConsider the following code:\ncilk int foo(void)\n{\nint x = 0;\nspawn bar(&x);\nspawn bar(&x);\nsync;\nreturn x;\n}\ncilk void bar(int *p)\n{\n*p += 1;\n}\nIf this were a serial code, we would expect that foo returns 2. What value is returned by foo in the parallel\ncase? Assume the increment performed by bar is implemented with assembly that looks like this:\nread x\nadd\nwrite x\nThen, the parallel execution looks like the following:\nbar 1:\nread x (1)\nadd\nwrite x (2)\nbar 2:\nread x (3)\nadd\nwrite x (4)\nwhere bar 1 and bar 2 run concurrently. On a single processor, the steps are executed (1) (2) (3) (4) and\nfoo returns 2 as expected. In the parallel case, however, the execution could occur in the order (1) (3) (2)\n(4), in which case foo would return 1. The simple code exhibits a race condition. Cilk has a tool called the\nNondeterminator which can be used to help check for race conditions.\n2-4\n\nMatrix Multiplication and Merge Sort\nIn this section we explore multithreaded algorithms for matrix multiplication and array sorting. We also\nanalyze the work and critical path-lengths. From these measures, we can compute the parallelism of the\nalgorithms.\n2.1\nMatrix Multiplication\nTo multiply two n × n matrices in parallel, we use a recursive algorithm. This algorithm uses the following\nformulation, where matrix A multiplies matrix B to produce a matrix C:\n\nC11\nC12\nA11\nA12\nB11\nB12\n=\n·\nC21\nC22\nA21\nA22\nB21\nB22\n\nA11B11 + A12B21\nA11B12 + A12B22\n=\n.\nA21B11 + A22B21\nA21B12 + A22B22\nThis formulation expresses an n× n matrix multiplication as 8 multiplications and 4 additions of (n/2) ×\n(n/2) submatrices. The multithreaded algorithm Mult performs the above computation when n is a power\nof 2. Mult uses the subroutine Add to add two n × n matrices.\nMult(C, A, B, n)\nif n = 1\nthen C[1, 1] ← A[1, 1] · B[1, 1]\nelse\nallocate a temporary matrix T [1 . .n, 1 . . n]\npartition A, B, C and T into (n/2) × (n/2) submatrices\nspawn Mult(C11, A11, B11, n/2)\nspawn Mult(C12, A11, B12, n/2)\nspawn Mult(C21, A21, B11, n/2)\nspawn Mult(C22, A21, B12, n/2)\nspawn Mult(T11, A12, B21, n/2)\nspawn Mult(T12, A12, B22, n/2)\nspawn Mult(T21, A22, B21, n/2)\nspawn Mult(T22, A22, B22, n/2)\nsync\nspawn Add(C, T, n)\nsync\nAdd(C, T, n)\nif n = 1\nthen C[1, 1] ← C[1, 1] + T [1, 1]\nelse\npartition C and T into (n/2) × (n/2) submatrices\nspawn Add(C11, T11, n/2)\nspawn Add(C12, T12, n/2)\nspawn Add(C21, T21, n/2)\nspawn Add(C22, T22, n/2)\nsync\nThe analysis of the algorithms in this section requires the use of the Master Theorem. We state the\nMaster Theorem here for convenience.\n2-5\n\nspawn\ncomp\nstart\n15 us\n10 us\n40 us\n30 us\nFigure 3: Critical path. The squiggles represent two different code paths. The circle is another code path.\nTheorem 1 (Master Theorem) Let a ≥ 1 and b > 1 be constants, let f (n) be a function, and let T (n) be\ndefined on the nonnegative integers by the recurrence\nT (n) = aT (n/b) + f (n),\nwhere we interpret n/b to mean either ⌊n/b⌋ or ⌈n/b⌉. Then T (n) can be bounded asymptotically as follows.\n1. If f (n) = O(nlogb a-ε) for some constant ε > 0, then T (n) = Θ(nlogba).\n2. If f (n) = Θ(nlogbalgkn) for some constant k ≥ 0, then T (n) = Θ(nlogbalgk+1n).\n3. If f (n) = Ω(nlogba+ε) for some constant ε > 0, and if af (n/b) ≤ cf (n) for some constant c < 1 and\nall sufficiently large n, then T (n) = Θ(f (n)).\nWe begin by analyzing the work for Mult. The work is the running time of the algorithm on one\nprocessor, which we compute by solving the recurrence relation for the serial equivalent of the algorithm.\nWe note that the matrix partitioning in Mult and Add takes O(1) time, as it requires only a constant\nnumber of indexing operations. For the subroutine Add, the work at the top level (denoted A1(n)) then\nconsists of the work of 4 problems of size n/2 plus a constant factor, which is expressed by the recurrence\nA1(n) = 4A1(n/2) + Θ(1)\n(1)\n= Θ(n 2).\n(2)\nWe solve this recurrence by invoking case 1 of the Master Theorem. Similarly, the recurrence for the work\nof Mult (denoted M1(n)):\nM1(n) = 8M1(n/2) + Θ(n 2)\n(3)\n= Θ(n 3).\n(4)\nWe also solve this recurrence with case 1 of the Master Theorem. The work is the same as for the traditional\ntriply-nested-loop serial algorithm.\nThe critical-path length is the maximum path length through a computation, as illustrated by figure\n3. For Add, all subproblems have the same critical-path length, and all are executed in parallel. The\ncritical-path length (denoted Ainf(n)) is a constant plus the critical-path length of one subproblem, and is\nrepresented by the recurrence (sovled by case 2 of the Master Theorem):\nAinf = Ainf(n/2) + Θ(1)\n(5)\n= Θ(lg n).\n(6)\nUsing this result, the critical-path length for Mult (denoted Minf(n)) is\nMinf = Minf(n/2) + Θ(lg n)\n(7)\n= Θ(lg2 n),\n(8)\n2-6\n\nby case 2 of the Master Theorem. From the work and critical-path length, we compute the parallelism:\nM1(n)/Minf(n) = Θ(n / lg2 n).\n(9)\nAs an example, if n = 1000, the parallelism ≈ 107 . In practice, multiprocessor systems don't have more that\n≈ 64,000 processors, so the algorithm has more than adequate parallelism.\nIn fact, it is possible to trade parallelism for an algorithm that runs faster in practice. Mult may\nrun slower than an in-place algorithm because of the hierarchical structure of memory. We introduce a\nnew algorithm, Mult-Add, that trades parallelism in exchange for eliminating the need for the temporary\nmatrix T .\nMult-Add(C, A, B, n)\nif n = 1\nthen C[1, 1] ← C[1, 1] + A[1, 1] · B[1, 1]\nelse partition A, B, and C into (n/2) × (n/2) submatrices\nspawn Mult(C11, A11, B11, n/2)\nspawn Mult(C12, A11, B12, n/2)\nspawn Mult(C21, A21, B11, n/2)\nspawn Mult(C22, A21, B12, n/2)\nsync\nspawn Mult(C11, A12, B21, n/2)\nspawn Mult(C12, A12, B22n/2)\nspawn Mult(C21, A22, B21, n/2)\nspawn Mult(C22, A22, B22, n/2)\nsync\nThe work for Mult-Add (denoted M1\n′ (n)) is the same as the work for Mult, M1\n′ (n) = Θ(n3). Since\nthe algorithm now executes four recursive calls in parallel followed in series by another four recursive calls\nin parallel, the critical-path length (denoted M ′\ninf(n)) is\nM ′\ninf(n) = 2M ′\ninf(n/2) + Θ(1)\n(10)\n= Θ(n)\n(11)\nby case 1 of the Master Theorem. The parallelism is now\nM ′\n1(n)/M ′\ninf = Θ(n 2).\n(12)\nWhen n = 1000, the parallelism ≈ 106, which is still quite high.\nThe naive algorithm (M ′′) that computes n2 dot-products in parallel yields the following theoretical\nresults:\nM ′′\n1 (n) = Θ(n 3)\n(13)\nM ′′\ninf = Θ(lg n)\n(14)\n=> Parallelism = Θ(n / lg n).\n(15)\nAlthough it does not use temporary storage, it is slower in practice due to less memory locality.\n2.2\nSorting\nIn this section, we consider a parallel algorithm for sorting an array. We start by parallelizing the code for\nMerge-Sort while using the traditional linear time algorithm Merge to merge the two sorted subarrays.\n2-7\n\nMerge-Sort(A, p, r)\nif p < r\nthen q ←⌊(p + r)/2⌋\nspawn Merge-Sort(A, p, q)\nspawn Merge-Sort(A, q + 1, r)\nsync\nMerge(A, p, q, r)\nSince the running time of Merge is Θ(n), the work (denoted T1(n)) for Merge-Sort is\nT1(n) = 2T1(n/2) + Θ(n)\n(16)\n= Θ(n lg n)\n(17)\nby case 2 of the Master Theorem. The critical-path length (denoted Tinf(n)) is the critical-path length of\none of the two recursive spawns plus that of Merge:\nTinf(n) = Tinf(n/2) + Θ(n)\n(18)\n= Θ(n)\n(19)\nby case 3 of the Master Theorem. The parallelism, T1(n)/Tinf = Θ(lg n), is not scalable. The bottleneck is\nthe linear-time Merge. We achieve better parallelism by designing a parallel version of Merge.\nP-Merge(A[1..l], B[1..m], C[1..n])\nif m > l\nthen spawn P-Merge(B[1 . . m], A[1 . . l], C[1 . . n])\nelseif n = 1\nthen C[1] ←A[1]\nelseif l = 1\nthen if A[1] ≤B[1]\nthen C[1] ←A[1]; C[2] ←B[1]\nelse\nC[1] ←B[1]; C[2] ←A[1]\nelse find j such that B[j] ≤A[l/2] ≤B[j + 1] using binary search\nspawn P-Merge(A[1..(l/2)], B[1..j], C[1..(l/2 + j)])\nspawn P-Merge(A[(l/2 + 1)..l], B[(j + 1)..m], C[(l/2 + j + 1)..n])\nsync\nP-Merge puts the elements of arrays A and B into array C in sequential order, where n = l + m. The\nalgorithm finds the median of the larger array and uses it to partition the smaller array. Then, it recursively\nmerges the lower portions and the upper portions of the arrays. The operation of the algorithm is illustrated\nin figure 4.\nWe begin by analyzing the critical-path length of P-Merge. The critical-path length is equal to the\nmaximum critical-path length of the two spawned subproblems plus the work of the binary search. The\nbinary search completes in Θ(lg m) time, which is Θ(lg n) in the worst case. For the subproblems, half of A\nis merged with all of B in the worst case. Since l ≥n/2, at least n/4 elements are merged in the smaller\nsubproblem. That leaves at most 3n/4 elements to be merged in the larger subproblem. Therefore, the\ncritical-path is\nTinf(n) ≤T (3/4n) + O(lg n)\n= O(lg2 n)\n2-8\n\nl/2\nl\nB\nm\nA\nj\n< A[\n< A[\nj + 1\nl/2]\nl/2]\nl\nl\n/2]\n> A[\n/2]\n> A[\nFigure 4: Find where middle element of A goes into B. The boxes represent arrays.\nby case 2 of the Master Theorem. To analyze the work of P-Merge, we set up a recurrence by using the\nobservation that each subproblem operates on αn elements, where 1/4 ≤ α ≤ 3/4. Thus, the work satisfies\nthe recurrence\nT1(n)\n=\n\nT (αn) + T ((1 - α)n) + O(lg n).\nWe shall show that T1(n) = Θ(n) by using the substitution method. We take T (n) ≤ an - b lg n as our\ninductive assumption, for constants a, b > 0. We have\nT(n) ≤ aαn - b lg(αn) + a(1 - α)n - b lg((1 - α)n) + Θ(lg n)\n= an - b(lg(αn) + lg((1 - α)n)) + Θ(lg n)\n= an - b(lg α + lg n + lg(1 - α) + lg n) + Θ(lg n)\n= an - b lg n - (b(lg n + lg(α(1 - α))) - Θ(lg n))\n≤ an - b lg n,\nsince we can choose b large enough so that b(lg n +lg(α(1 - α))) dominates Θ(lg n). We can also pick a large\nenough to satisfy the base conditions. Thus, T(n) = Θ(n), which is the same as the work for the ordinary\nMerge. Reanalyzing the Merge-Sort algorithm, with P-Merge replacing Merge, we find that the work\nremains the same, but the critical-path length is now\nTinf(n) = Tinf(n/2) + Θ(lg2 n)\n(20)\n= Θ(lg3 n)\n(21)\nby case 2 of the Master Theorem. The parallelism is now Θ(n lg n)/Θ(lg3 n) = Θ(n/ lg2 n). By using a more\nclever algorithm, a parallelism of Ω(n/ lg n) can be achieved.\nWhile it is important to analyze the theoritical bounds of algorithms, it is also necessary that the\nalgorithms perform well in practice. One short-coming of Merge-Sort is that it is not in-place. An\nin-place parallel version of Quick-Sort exists, which performs better than Merge-Sort in practice.\nAdditionally, while we desire a large parallelism, it is good practice to design algorithms that scale down\nas well as up. We want the performance of our parallel algorithms when running on one processor to compare\nwell with the performance of the serial version of the algorithm. The best sorting algorithm to date requires\nonly 20% more work than the serial equivalent. Coming up with a dynamic multithreaded algorithm which\nworks well in practice is a good research project."
    },
    {
      "category": "Lecture Notes",
      "title": "lecture3.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/20beab408a5b02847594dfcae2fca9c1_lecture3.pdf",
      "content": "!\n\n\"\n\n#\n\n$\n\n!\n\n%\n\n&\n\n'\n\n(\n)*\n+\n\n,\n\n(\n-\n\n+\n\n\"\n\n!\n\"\n\n#$%\n$%\n\n!\n\"\n!\n\n&'\n\n!\n\"\n(\n\n%\n%\n\n)\n!*\n+\n!\n+\n!\n\n!\n\"\n(\n\n!\n\"\n\n%\n$%\n\n!\n\"\n!\n\n&'\n\n!\n\"\n(\n\n%\n%\n\n)\n!*\n\n+\n!\n\n!\n\"\n(\n#$\n$\n!\n\n&'\n\n,\n!!\n&\n\n.\n\n\"\n\n(\n+\n\n\"\n\nCache\n128 byte cache line\n64 bit word\nCPU\nData\nAddress (tag)\nRegister file\n4096-4223\n3840-3967\n42,37,...\n17,19,...\n3840:\n3841:\n4096:\n4097:\nR0\nR1\nR2\nR31\nMain memory\n\n\"!\n\"\n-\n\n(\n\n+\n\"\n\n#\n\n,\n\n/\n\n,\n\n$\n\n(2 +\n*\n\n-\n\n3*\n\n-2\n\n&\n\n\"\n\n54\"3\n\n!\n!\n\n(\n\n+\n\"\n\n\"\n.:\n\n!\n\n;\n\n!\n\n<\n=\n\n\"\n\n;\n\n(\n\n!#\n\n0>+\n\"\n\n(\n\n!\n\n(+\n+\n'\n\n&\n\n#\n\n?\n\n!\n\n%\n\n@\n\n#\n\nA\n\ngas\ngcc -S\nHigh-level\n(e.g. C++)\nlanguage\nlanguage\nmachine\nlanguage\nassembly\n\n(!\n\n,\n\n,4'?\n.\n\nB\n:\n5,4'3\n\"\n\n,4')*\n\nC\n\nB\n\n!\n\"\n\nD\n\n!\n\n$\n\n.\n\n&\n\n!\n\n?\n\n.\n\n@\n\n@\n\n(\n.\n+\n\n\"\n\n<\n\n!\n\n!\n\n?\n!\n\n(+\n\n<\n\n(+\n\n!\n\n\"\nD\n\n(\n\n+\n\"\n\n#\n\n!\n\n!\n!\"\n\n#\n\n\"\n\n#\n\n*\n\n#\n\n#\n\n$$%&\n'\n%!\n&\n\n\"\n<\n\nE\nF\n(+\n\n(+\n\nE\nF\n(+\n\n(+\nE\n(\nE\n+\n\n(\nE\n+\n.\n\n$\n\n\"\n\"\n\nC\n\nC\n\n.\n\n4;.?\n\n#\n()\n*\n*+\n*\n,*\n**+\n\n!\"#\nD\n\nB(\n+\n\n'\n\n!-\n\n/\n>\n\"\n!\n\n\"\n\n\"\n\n!\n\n(;\n\n+\n\n1GG\n1-3\n--\nH1\n\n*\n\n,\n\n$\n\n,\n\n!\n\n<\n\n%\n\nF\n\n\"\n\n!\n\nD,4\n\n!\n\n4(\n\n+\n\nF\n\n\"\n\nF\n\n\"\n\nF\n\n\"\n\n!\n\n%\n\n&\n\n&\n\n,\n\n!\n\nÆ\n\n@\n\n$\n\nCPU\nRegisters\nL1 Cache\nL2 Cache\nDisk\n. . .\n. . .\n\n(*\n(\n!\n\"\n-\n.!\n\"\n(*\n(\n\"!\n!\n/\n\n1(\n\n!\n\n!\n\"\n(-\n4\"\n\n!\n!3\n\n(*\n(\n/-\n\n'\n()\n\n&\n\n\"\n\n@\n\n$\n*\nCPU\nM\nA\nI\nN\nM\nE\nM\nO\nR\nY\nB\nM / B\nCache\n\n+*\n(\n!\n\"\n\"\n1(\n\n!\n\"\n\n!\n\n(\n!\n\"-\n*\n\"\n\n&;=\n\n\"\n\n\"\n\n!\n\"\n\n!\n\n/\n\n!\n\n!\n\n)\n\n+,\n-\n\n/\n\n,\n\n$\n\n'\n&\n(\n)&\n\n*\nB\n\n!\n\n.\"\n/\n\n\"\n\n+\n$\n\n*\n,\n\n5$;DGG\n\n!\n\n!\n\n5 6\n/\n\n*Æ\n\n/\n)\n\n,\n\n$$\n\nB\n\n!\n\n!\n\nÆ\n\n!\"#$#\n%\n&\n\n'%\n&\n\n(\n$\n\n5$;DGG\n*Æ\n\n)\"\n%\n\n\"\n\n!\n\n(\n\n+\nD\n\n&\n\n!\n\n(\n\n+\n&\n\n!\n\n(\n\n+\n,\n\n.\n\n)# **\n+\n\n,\n-\n\n.\n\n!\"#$#\n%\n\n,\n\n-\n\n.\n\n--\n\n(+\n/-\n\n$\n\n5$;DGG\n\n&\n+\n\n&\n,-\n.\n\n/\n\n<\n\n#\n/\n(+,\n%\n,\n\n\"\n\n!\n\n&\n\n-\n\n-\n\n\"\n\n,\n\n/\n(%'\n\n%\n,\n\nH\n\n$\n+%&\n+%\n\"\n\n\"\n\n.\n\nD,4\n\n*2\n/\n/\n\n!\n\n&;=\n\n+\n%&\n&\n\n\"\n\n!\n\n'\n\n./\n/\n/\n\n#\n&\n\nE\n\n,\n\n$\nN\nB\nB\nB\nB\nB\n\n)!\n!\n!\n1(\n\n2-\n4\"\n(!\n!(\n!\n1(\n\n!\n\"-\n4\"\n\"!\n\n\"\n(\n!\n\"\n!!\n\n!\n\"-\n%\n\n,\n\n' %\n* &\n\n&\n\n/\n\"\n\nF\n\n(+\nF\n\n(+\n%\n\n\"\n\n\"\n\n/\n\n(\n\n+\n-\n\n)&\n\n&\n&\n\nF\n\n\"\n\n!\n\n#\n\nF\n\nF\n\nE\n\nE\n\nE\n\nE\n\n)\n\n/\n\n$\n\nI\n\nI\n\n$\n\n$\n)\n=\nA11 A12 A21 A22\nA\n\n!\n!5\n\n-\n\"\n\n-\n\n,0\n\n/\n*\n\n$\n/\n\n#\n\n(\n+\nF\n-\n\nE\n\n\"\n\n,\n\n/\n/\n\n\"\n\n(\n\n+\nF\n\n(+\n\n!\n\"\n\n(\n\n+\nF\n\nÆ\n\n\"\n\n(\n\n+\n\n,\n\nG\n\nh\n\n,2\n\n\"-\n\n(\n\n$\n/\n\n\"\n,\n!\n\n$\nH\n\"\n\n/\n\n(\n\n+\n\"\n\n#\n\nF\n\n(\n\n+\n\n\"\n\n#\n\nF\n-\n\n(+\nF\n\n(*+\n\"\n\nF\nJ(\n\n+\n\n#\n\nF\nJ\n\n(1+\nF\nJ\n\n()+\n(H+\n\"\n\nF\nJ\n\n(-+\n,\n\n\"\n#\n\"\n\nF\nJ\n\n%\n\n\"\n\n5,4'36\n,4'\n.\n\n,4'\n,\n)*\n\n,4'\nB\n\n\"\n\n,4'\n5$;DGG6\n$\n\n&\n%\n\n;!\n\n.\nD\n\n&\n\n-\n\n#\n\n3-\n\nB\n\nGGG\n54\"36\n&\n\n4\"\n)-G1#"
    },
    {
      "category": "Lecture Notes",
      "title": "lecture4.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/4a50951d57336f8ffb8af8fbfbb22b4d_lecture4.pdf",
      "content": "!\n\n\"# $\n\n\"# $\n\n%\n\n%\n\n&\n\n'\n\n&\n\n(\n\n%\n\n)\n\n*\n\n+\n\n+\n\n%\n\n%\n\n,\n\n-\n\n.\n\n/\n\n%\n\n&\n\n!\n\n\"\n#\n\n!\n\n\"\n\n#$\n)\n\n.\n\nread\nwrite\nread\nwrite\nread/write race\nwrite/write race\n\n!\n\n!\n\n%\n\n%\n\n\"\n\n\"\n\n!\n\n!\n\n!\n\n&\n\n&\n(\n\n*\n\n!\n\n\"\n\n!\n\n$\n\n!\n\n\"\n\n!\n\n!\n\n$\n\n!!\n\n\"\n\n%\n\n&\n\nÆ\n%\n\n((\n\n!\n\"#$%\n%&&\n%\n\n&\n\n\"\n\n'\n\n#\n'\n\n$\nInstructions: Place four queens on the board\nso that no two queens attack each other\n?\n?\n?\n?\na)\nb)\n\n\"#$\n\n%\n\n!\n%\n\n&\n%\n\n'\n\n'\n.\n!\n\n((\n\n&\n\n((\n\n&\n\n+\n\n&\n\n.\n\n!\n\n!\n+\n!\n\n\"\n\n#\n$%#\n$!\n\n&&\n\n'\n\n%\n(\"\n\n&&\n!\n)\n*\"\n\n&&\n\n!\n#\n%\n\n&&\n\n+\"\n#\n$,!\n\n-\"\n...\n/\"\n,!\n\n&&\n\n#\n!\n1\"\n,!\n\n!\n\n&&\n\n'\n!\n\n!\n2\"\n\n&&\n\n%\n5\"\n...\n\"\n,!\n6 7\n\n&&\n'\n\n%\n\n3#\n\n\"\n\n%,!\n\n&&\n\n#\n\n(\"\n...\n*\"\n\n+\"\n\n-\"\n...\n/\"\n\n&\n\n+\n.\n\n&\n\n=\nboard\n(child)\nnew_board\n(parent)\n\n(\n\n%\n\n!\n%\n\n)\n\n%\n\n!\n\n'\n\n!\n\n:\n\n;+\n\n%\n\n+\n\n\"\n!\n\n\"\n#\n\n$\nCilk program + input\nInformation localizing\ndeterminacy race\nEvery schedule\nproduces same result\nFail\nPass\nNondeterminator\n\n)#\n\n\"\n.\n<\n\n=\n\n%\n\n&\n\n*\n\n'\n\"\n\n%\n\n!\n\n(\n\n++\n\n+\n!\n\n(\n#\n\n)\n\n*\n\"\n\n+\n\"\n\n*\n\"$\n\n+\n\n+\n\n.\n\n+\n\n)\n\n%\n\n&%'\n\n%\n\n)\n\n(\n\n.\n>\n\n,\n\n!\n*\n\n!\n\n*\n?\n\n+\n\n+\n\n!\n\n*\n?\n\n+\n\n+\n\n<\n\na)\nb)\nc)\nG1\nG2\nG1\nG2\ns\nt\ns\nt\ns1\nt1, s2\nt2\nŒ\nŒ\ns\nt\ns1, s2\nŒ\nt1, t2\nŒ\n\n*#+\n\n,\n\n\"#\n\n.\n\n&\n\n\"\n*\n8\"\n\n\"\n\n8 \"\n\n\"\n&&\n\n8(\"\n\n(\"\n&&\n\n*\"\n\n.\n:\n\n\"#\n\n%/-\n\n.\n\n\"#\n\nÆ\n\n*\n\n()\n*\n\n!\n\n#\n\n\"\n\n-\n\n$\n\n!\n\n@\nA\n\n&\n\n\"#\n\n'\n\n,\n\n+\n$\n\nB\n\n>\n\nF1\nF2\ne0\ne1\ne2\ne3\nKey:\nstart/end node\nspawn node\nsync node\nF:\n\n*+#\n\n.\n:\n\n+\n\n.\n;\n\n%\n\n&%'\n\n(\n\n%\n\n\"#\n\n%\n\n\"#\n\n+\n\n.\n\n.\n\n.\nC\n\n\"#\n\n\"\n\n.\n\n@\nA\n\n*\n\n=\n+\n,\n\n=\n\n=\n+\n,\n\n=\n\n=\n\n=\n\n\"#\n\n&\n\n\"\n\n+\n\n@A\n\n\"#\n\n+\n\n.\nD\n\n\"#\n\n.\n\n\"#\n\n&\n\n+\n\n\"#\n\n.\n\n.\nD\n\n*\n-. /\n\n. /\n\"\n#\n\n#\n\n!\n\n$\n\n:\n\nS\nS\nP\nS\nP\ne2\nF2\ne1\nF1\ne0\ne3\nF:\n\n*+\n\n\"\n\n+\n\n\"\n\"\n\n$\n\n\"\n\n+\n\n$\n,\n\n\"#\n\n\"#\n\n!\"#\n%\n\n)*\n*\n\n-\n\n%\n\n(\n\n)\n\n)\n\n\"\n!\n\n\"#\n\nE-\n\nE\n\n\"\n!!\n\n)\n\n&\n\n'\n\nC\n\nS\nS\nS\ne9\nS\nP\nS\nP\ne2\nF2\ne1\nF1\ne0\nS\nP\nS\nP\ne5\nF4\ne4\nF3\ne3\nS\nP\nS\nP\ne8\nF6\ne7\nF5\ne6\nthread\nprocedure\nsync block\n\n*+\n\n-\n\n!\n.\n\n#\n\n/\n\n*+\n\n$\n\n#\n\n(\n\n*\n\n/.\"\n\n-\n\"#\n\n-\n\n&\n\n/.\"\n\n*\n\nFG\n\n*\n\n$\n\nH\n\n%/\n\n-\n\n+\n\n$\n\nH\n\n%/\n\n-\n\n\"\n\n!\n%\n\n#0*\n\n#0*\n%\n\n(\n\n.\n;\n\n\"#\n\n)\n\n\"\n\n+\n\nD\n\n)\n\n\"\n\n+\n\nF1\nF2\nF3\n........\nF4\nF5\nF6\n........\n....\n....\nsync block\nFi = parse tree of any spawned procedure\ne1\ne2\n\n*+#\n\n+\n%\n\n%\n\nE\n\n#\n\n%%\n3\"-\"#4\"\n\nI\n\"\n)\n$\n\n\"\n#\n\n(\n\nI\n\n$\n\nI\n!\n\n#\n\n$\n\nI\n)\n\n\"\n#\n!\n\n+\nI\n\nI\n\n$\n\n!\n\n#\n\nI$\n\n+\nI\n\nI\n\n+\n\n+\n\n\"\n\nI\n\n!\n\n#\n\n\"\n\n$\n\n+\n\nI\n\n$\n\nI\n\n!\n\n(\n\n.\n+\n\n3$\n\nE\n\n*\n\n-+\n\n#\n\n%%\n\n'\n\n\"\n\n!\n\n!\n\"\n\n++\n$\n,\n%-\n\n\"# $\n\n%\n\nE\n\n\"\n!!\n\n\"# $\n\n&\n\n\"\n!\n\n/.\"\n\n-\n\"#\n\n=\n\n\"# *\n;\n\n*\n\n+=\n\n*\n\n+=\n\n*\n\n+\n\n&\n\n\" #\n\n*\n\n@A*\n\n%/\n\n@A*\n\n%/\n\n+\n\n&*\n\n*\n\n@A+\n\n#\n\n@A+\n\n# +\n\n@A\n\n*\n\n@A\n\n# +\n\n@A\n\n\" +\n\n@A\n\n$\n\n%\n%\n\n\" #\n\n)\n\nE\n\n\"\n!\n\n,. \"\n$\n%\n/\n\n%-\n\n)\n\n\"\n!\n\n&\n\n\"#\n\n+\n\n\"\n!\n\n*\n\n+=\n\n\"\n\n&\n\n*\n\n+=\n\n+\n\n*\n\n+\n\n$\n\n$\n\n+\n,\n%\n\n\"#\n\n8\"\n!+\n\n&\n\n)\n\n=\n\n+\n\n\"\n\n=\n\n+\n\n(\n\n(\n\n$\n\"\n\n(\n\n$\n\n\"\n\n%\n\nJ\n$\n\n*\nP\nS\ne2\ne1\ne3\n%\n\n% /\n\n(\n\n(\n\n$\n\"\n\n(\n\n$\n\n\"\n\n&\n\n*\ne3\ne1\ne2\na2\na1\na2\ne3\na1\ne2\ne1\n\n%\n\n+\n\n+\n.\n\n\"\n\n+\n\n$\n\n&\n\nF\n&G\n\n@\nA\n.\n\n\"#\n\n\"\n!+\n\n@A\n\n&\n\n&\n\n\"\n\n\"#\n\n\"\n\n#\n\n\"#\n\n\"\n\n(\n\n+\n\n!\n\n$\n\n+\n\n\" 88++\n\n+\n\n+\n\n# 88++\n\n)\n\n)\n\n.\n\nC\n\n@\nA\n\n5*\n\n%\n\n-\n\n-\n\n)\n\n+\nJ\n.\n\n+\n8+\n\n+\n\n8+\n$\n\n+\n\n%/\n\n=\n\n+-\n%/\n\n%\n\n+\n8+\n\n-*\n\n%\n\n+\n\n,\n\n$\n$\n%-\n\n%\n\n\"\n\n\"#\n\n!\n\n\"\n\n\"\n\n4$\n)\n\n)\n\n.\n\n;\n\n@A\n\n%\n\n*\n!\n\nS\nS\nS\ne9\nS\nP\nS\nP\ne2\nF2\ne1\nF1\ne0\nS\nP\nS\nP\ne5\nF4\ne1\nF3\ne3\nS\nP\nS\nP\ne8\nF6\ne7\nF5\ne2\nsync block\nthread\na\nF\nprocedure\n\n*\n\n\"\n\n@A\n+\n\n%\n\n+\n\n# 88\n\n+++\n%\n\n*\n\nD\n\n*\n\n*\n\n!*\n\n\"\n\n+\n\n@A\n\n@A\n\n;+\n%\n\nS\nS\nS\ne9\nS\nP\nS\nP\ne2\nF2\ne1\nF1\ne0\nS\nP\nS\nP\ne5\nF4\ne1\nF3\ne3\nS\nP\nS\nP\ne8\nF6\ne7\nF5\ne2\nsync block\nthread\na\nF\nprocedure\n\n@A\n\n,\n\nK\n.\n\n,Æ\n\n%\n\n\"\n\n!\n\n.\n/\n\nH\n\nB\n\n%\nL\nDD:\n<"
    },
    {
      "category": "Lecture Notes",
      "title": "lecture5.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/45bc830c2e00e55e83db4cf3f1e067fe_lecture5.pdf",
      "content": "!\n\"\n\n!\n\n!\n\n\"#\n\n$\n\n%\n\n$\n\n&\n\n!\n\n'\n\n$\n(\n\n&\n\n)\n\n*\n\n#\n\n&\n\n'\n\n!\n'\n\n&\n+\n\n*\n\n&\n\n'\n\n,\n\n'\n\n!\n\n&\n\n!\n\n-\n\n&\n\n&\n\n\"\n!\n\n&\n\n'\n\n.\n\n&\n\n% /\"\n\n+\n\n!\n\"#\n$%\n&$'\n(\n$%$%%\n)#\n$%\n&$'\n*\n&\n!\n'\n\n&\n\n&\n\n+\n\n&\n\n*\n\n.\n\n&\n\n*\n\n!\n\n-\n\n!\n\n&\n'\n&\n\n'\n\n&\n\n!\n\n'\n\n&\n\n#\n\n$\n\n\"\n\n%\n\n$\n!\n%\n\n+!\n\n!!\n\n!'\n\n(\n\n&\n\n&\n\n!\n\n!\n\n%\n!\n\n!\n\n!!\n#\n(\n\n,\n&\n\n&\n\n!!\n\n!\n\n*\n\n!\n\n&\n\n+\n\n$\n!\n\n$\n\n&\n\n&\n\n&\n\n&\n\n#\n\n!\n\n\"\n\n!\n'\n\n$\n\n(\n\n\"\n\n!\n-\n\n&\n\n&\n\n'\n\n&\n!\n\n(\n\n'\n\n+\n\n)\n\n!\n\n-\n\n&\n\n&\n\n+\n\n,%\n\n--\n\n!\n!\n.!\n\n!\n!\n!!\n!\n\n--\n&\n\n!\n!\n\n%\n\n!\n!\n!\n/,\n\n,%0\n--\n\n!\n!\n!\n\n&\n!\n'\n\n&\n\n!\n\n'\n\n'\n\n'\n\n&\n\n$\n\n&\n\n!\n\n$\n\n!\n\n&\n\n$\n\n'\n&\n\n-\n\n&\n\n&\n\n&\n\n&\n\n$\n\n&\n\n#\n\n+\n\n'\n--\n\n!\n!\n!\n\n2,%0\n\n!\n!\n!\n!%\n\n--\n\n,\n3$4*\n!\n\n,,2\n\n,\n5(\")*\n!\n,,2\n\n--\n!\n\n,\n3$4*\n\n+!\n\n,2\n--\n!!!\n\n,\n5(\")*\n\n-\n\n!5\n\n&\n\n&\n\n&\n\n&\n\n!\n$\n\n&\n\n&\n\n!\n\n!\n\n!\n\n$\n\n!\n:\n\n&\n\n&\n-\n\n'\n\n&\n\n&\n\n!\n\n&\n\n,%'\n\n&,3$4*\n,\n\n&\n\n&\n\n&\n\n&\n\n\"\n\n&\n\n'\n\n&\n&\n\n'\n\n!\n:\n\n&\n&\n\n*\n\n&\n\n&\n\n&,5(\")*\n\n-\n\n&\n,&'\n\n&\n\n'\n\n&,3$4*\n-\n\n&\n\n&\n\n!\n$\n\n!\n\n!\n\n$\n\n&\n\n+\n*\n\n!\n\n$\n\n!\n\n\"\n\n!\n'\n\n/\n\n,\n\n!\n\n$\n\n&\n\n#\n\n-+\n\n.6\n--\n.\n\n!\n\n.!\n!\n.\n!\n\n7!\n!\n\n--\n&\n8,\n\n!\n%\n\n--\n\n!\n.!\n.\n,\n5(\")*\n\"9\n\n/,\n\n,\n3$4*\n\n.\n\n,\n\n\"\n\n!!\n\n,\n5(\")*\n\n2,&\n\n2/,\n::\n/2\n\n\"\n\n,\n3$4*\n.\n,\n3$4*\n\n&\n\n\"\n\n&\n\n$\n;<\n=\n;\n<\n=\n-\"7\n\n\"\n'\n&\n\n'\n\n&\n\n;<\n=\n-\"7\n$\n'\n\n;<\n=\n-\"7\n\n;\n<'\n\n$\n'\n\n'\n&\n\n$\n'\n\n$\n\n&\n\n&\n\n'\n\n!\n\n$\n\n;<\n=\n-\"7\n\n!\n\n#\n\n&\n\n1;\n<2:\n\n$\n\n#\n\n$\n&\n\n\"\n\n$\n\n;\n<\n=\n$>\n?7\n\n=\n'\n\n,'\n\n$\n\n;\n<\n=\n-\"7\n\n'\n\n@\n\n&'\n\n!\n\n'\n;<\n=\n-\"7'\n\n*\n\n'\n\n'\n&\n\n!\n\n=\n\n$\n&\n\n'\n&\n\n;\n<\n=\n$>\n?7\n\n'\n\n$\n'\n\n!\n\"\n\n,\n\n'\n&\n\n!\n\n&\n\n$\n\n\"\n\n+\n\n;\n,\n&\n<\n,\n&\n\n!\n&\n\n!\n%\n\n!\n&\n\n!\n%\n\n;\n,\n%\n!\n<\n,,\n&\n<\n,\n%\n\n;\n\n&\n\n'\n\n!\n\n$\n\n&\n\n'\n\n@\n\n&\n\n'\n\n&\n\n-\nA'\n&\n\n&\n\n!\n\n'\n\n-\nA\n\n!\n\n&\n\nB\n\n-\n\n'\n&\n\n)\n\n!\n'\n\n'\n\n'\n&\n\n&\n\n&\n\n!\n\n-\n\n&\n\n!\n\n-\n\n&\n\n!\n\n!\n\n-\n&\n\n&\n\n&\n\n&\n\n&\n\n*\n\n+\n\n$\n\n,\n\n(\n\n$\n\n\"\n\n\"\n\n)\n\n)\n\n,\n\n-\n\n(\n\n&\n\n-'\n\n(\n\n'\n\n&\n\n!\n\n&\n\n-\nA\n\n!\n\n&\n\n!\n\n\"#$\n\n$\n% /\"\n\n(\n\n&\n!\n'\n\nCB\n\n$\n\n#/?4\n\n$'\n!\n\n&\n\n'\n\n&\n\n(\n\n'\n\n.\n\n\"\n\n'\n\n%67,#7\n\n&\n\n%67,#7\n\n&\n\n%67,#7\n\n&\n!\n'\n\n%67,#7\n\n!\n\n&\n*\n\nA5\n/\n\n'\n\n%67,#7\n\n'\n\nA5\n-%B\n*\n\nC\n\n-\n&\n\n&'\n\n!\n\n&\n\n$\n'\n\n!\n\n*\n\n% %\n$\n\n&\n\n&\n\n+\n\n!+\n+!\n\n+ !\n,\n\n,\n\n!\n\n+ !\n\n-\n\n&'\n\n!\n\n&\n\n$\n\n'\n\n&\n\n!\n!+\n+!:\n\n+!\n\n,\n&\nD\n\n$\n\n&\n\n-\n\n!\n\n&\n\n-\n\n%\n-\n\n%\n\n'(\n$\n\n!'\n\n'\n\n\"A\n\"#\n\n&\n\n+\n\n!\n\"\"\n$%\n$'\n--\n$%\n,\n$'\n!\n)=\n$%\n$'\n--\n$'\n,\n$%\n.\n\n$'\n\n!\n!\n\"\"\n.7\n--\n!\n7!\n\n!\n\n!\n\n!\n-\n\n&'\n\"#\n\n'\n\n'\n\n&\n\n!\n\n@\n\n&\n\n'\n\"#\n\nE\n\n&\n\n\"#\n\n!\n\n&\n\n'\n\n&\n\n&\n\n,\n\n'\n&\n\n!\n\n-\n\n'\n\n'\n\n&\n\n\"#\n\n\"\"\n$%\n$'\n(\n$%\n$%\n%\n)=\n$%\n$'\n.7\n\n$\n\n'\n&\n\n&\n\n.\n\n$\n\n$\n\n% %\"$\n'(\n$\n\n#-\"1\n\n&\n\n=\n\n'\n\n*&'\n\n#-\"\n\n\"\n\n.\n\n$\n\n% %\"$\n'*(\n\"\n\n#-\"'\n#-\"1\n\n'\n\n'\n\n#-\"\n\n'\n\n*&'\n\n#-\"\n\n-\n\n$\n\n(\n\n(\n\n'\n\n$\n\n&\n\n&\n\nC"
    },
    {
      "category": "Lecture Notes",
      "title": "lecture6.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/92c757da34193afc7925267d96a5fca1_lecture6.pdf",
      "content": "!\n\n\"\n\n#\n\n$\n\n%\n\n&\n\n'\n\n(\n\n)&\n\n\"\n\n)\n\n*\n\n)\n\n\"\n\n+\n\n\"\n\n'\n\n\"\n\n)\n\n\"\n\n'\n\n'\n\n\"\n\n*\n\n\"\n\n\"))\n\n,\n\n-\n\n.\n\n'\n\n'\n\n'\n/\n\n*\n\n)\n\n)\n\nP1\nP2\nP3\nS1\n\nn\n\nn\nn\n.................\n\n\"\n\n\"))\n\n*\n\n\"\n\n+\n\n\"\n\n\"\n\n\"\n\n)\n\n*\n\n)\n\n\"\n\n\"))\n\n7\"\n\n\"\n\n\"\n\n*\n\n8%\n\n\"\n\n\"\n\n*\n\n\"\n\n*\n\n\"\n\n-01\n\n$\n\n\"\n\n\"\n\n-\n\n\"\n\n*\n\n\"\n\n\"\n\n.................\n.................\n.................\n.................\n...\n...\n...\n...\nP\nlog\nn\nlg\nSpace per level\nn\n\n8 n\n\n64 n\n...\n\nlog\nlog\nP\nn\nP\nn\n\n2n\n\n4n\n\n/\n1 1\n·\n(c)\n§\n\nP\nn\nP\n/\n1 1\n·\n(c)\n§\nu\n\nP\nn\nP\nconst\nP leaves\n\n!\n\n\"\n\n)\n\n,\n\n)\n\n\"\n\n(\n\n%\n\n$\n\n!\n\n\"\n\n#\n#\n\n$\n\n#\n\n\"\n\n%\n\n$\n!\n\n&\n\n&\n\n\"\n\n%\n'\n\n#\n\n%\n'\n:\n\n)%\n\n*\n\n\"\n\n\"\n\n\"\n\n\"\n%\n\n)%\n\n\"\n\n\"\n\n\"\n\n)!\n\nA\nB\nF\nM\nR\nS\nT\nC\nD\nE\nG\nH\nK\nL\nN\nO\nP\nQ\nI\nJ\n\n#\n\n#\n\n:\n\n!\n\n&\n\n$\n$(\n\n!\n\n-\n\n!\n\n(\n\n-\n\n!\n\"\n\n!\n\n:0\n\n%\n\n!\n$\n\n'\n;\n\n%\n\n*\n\n)\n\n\"\n\n'\n\n<=\n.................\n.................\n.................\nC1\nC2\nCm-1\nCm\n.........\n\n$#\n\n%\n):\n\n(\n\n&\n\n\"\n\n)\n\n*\n\n\"\n\n)%\n\n>\n\n\"\n\n\"\n\n\"\n\n$\n\n*\n\n?\n\n?6\n\n$\n\n.\n\n/\n\n\"\n\n@\n\n$\n\n\"\n\n\"\n\n@\n\n?\n\n.\n\n$\n\n*\n\n\"\n\n'\n\n#\n\n'\n\n\"\n\n+\n\n\"\n\n'\n\n'\n\n$\n\n\"\n\n$\n\n\"\n\n\"\n\n\"\n\n%\n\n?6\n\n*\n\n\"\n\n.)\n\n*\n\n'\n\n\"\n\n);\n\n(\n\n<\n=\n*\n\nA\n\n).\n\n.\n\n#\n\n.\n*\n\n/\n/\n\n#\n\n\"\n\n#\n\n?6\n\n\"\n\n#\n\n\"\n\n)\n\n:\n\n:\n\n\"\n\n$\n\n\"\n\n)\n\n*\n\n\"\n\n/\n!\n\"\n\n#\n\n%\n*\n\n$\n\"\n\n!\n\n\"\n\n*\n\n$\n\n\"\n\n$\n\n#\n\n\"\n\n$\n)\n\n&\n\n$!\n\n&\n\n$%\n'\n\n1%\n\n?6\n\n+\n\n\"\n\n\"\n\n?6\n*\n\n\"\n\nB\n\n+\n\n$\n\"\n\n!\n\n\"\n\n%\n\n\"\n\n$\n\n%\n\n\"\n\n*\n\n\"\n\n/\n)\n\n'\n\n,\n\n)\n\n*\n\n*\n\n#\n\n\"\n\n/\n\n(\n\n)&\n\n'\n\n\"\n\n#\n\n\"\n\n*\n\n*\n\nC\n\n<=\n,\nD\n(\nE\nAE\n\nF\nD\n(\n\nC\n\nF\n\n/\n+\n\n$\n\n\"\n\n*\n\n\"\n\n\"\n\n+\n\nGH-\n\nA+\n&\n\"\n\nBBB\n<=\nF\nD\n(\n\nA\n'\n\n'\nI\n\n,\n\nE\nF\n\n+\n\n)\n\n$\n\n\"\n\n,\n\n-\n.\nC\n\n$\n\nI\nJJ\nG"
    },
    {
      "category": "Lecture Notes",
      "title": "lecture7.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/1f5fa7ad8d9e595ae6698dd3d07dd38c_lecture7.pdf",
      "content": "!\n\n!\n\"#\n$\n\n#\n\n%\n\n&\n\n'\n&\n\n#\n\n#\n\n(\n)(\n\n&\n\n(\n\n#&\n\n&\n\n*\n\n&\n\n(\n\n(&\n\n\"\n\n(\n\n#\n\n&\n\n#\n\n&\n\n#\n)(\n\n(\n\n#\n\n+\n\n#\n\n(\n\n#,\n\n#\n\n&\n\n&\n\n#\n-\n\n&\n\n.\n\n/\n\n&\n\n1#\n\n#.\n\n%\n\n#\n\n2\"\n\n.\n&\n\n#&\n\n&\n\n%\n\n,\n\n&\n\n-\n\n'\n\n&\n\n&\n\n*\n\n*\n\n#\n\n+\n\n&\n\n*\n\n&\n\n(\n\n$\n\n*\n#\n\n&\n\n#\n\n$\n\n&\n\n8(\n\n&\n\n#\n\n'\n\n&\n\n:\n\n&\n\n&\n\n:\n\n'\n\n.\n\n:\n4&\n:\n;&\n\n:\n<\n\n&\n\n:\n&\n\n:\n&\n\n:\n=\n\n*\n\n,\n\n&\n\n#\n&\n\n$\n#\n\n:\n$\n\n$\n\n$\n\n$\n\n#\n\n+\n\n&\n\n=\n\n>\n\n.\n\n+\n,\n\n&\n\nReservoir\n\n/\n\n/\n\n)\n\nReservoir\n\nReservoir\n\nReservoir\n\nReservoir\n\nReservoir\n\nReservoir\n\n:\n<\n\n:\n\n!\n\n\"\n\n#\n\n!\n\n.\n\n1)/\n\n4;\n-\n\n-\n\n1)/\n\n-\n\n-\n\n&\n\n*\n\n'+'\n%,\n\n+\n\n&\n\n*\n\n&\n\nÆ\n\n&\n\n:\n\nÆ\n\n?\n\n#\n\n.\n\n8(\n\n(\n\n=\n\n-\n\n&\n\n*\n\n%\n\nÆ\n:\nÆ\n\n&\n\n=\n\n@\n\n&\n\nÆ\n:\n\n)\n\n*\n\n'+'\n%\n&\n\n&\n\n#\n\n&\n\n=\n\n+\n\n#\n\n21)/&\n\n*\n\n+)\nA\n\n,\n\n:\n\n&\n=\n\n&\n\n:\n\nÆ\n:\n\n\"\n\n-\n\n-\n\n5&\n:\n\n:\nB\n=\n\n:\n=5\n:\n\n:\n\n1(\n\n&\n\n:\n\n:\n:\n\n.;\n\n=,\n\n&\n\n@\n\n&\n\n=\n-\n\n&\n\n(\n\nC\n\n8(\n&\n\nÆ\n\n:\n\n\"\n\n&\n\n&\n\n2Æ\n\n&\n\n8(\n&\n\n:\n\nÆ\n\n;\n:\n\n+\n,\n\n*\n\n@\n\n+\n\n.&\n\n!\n\n&\n\n5&\n\n\"\n\n&\n\n&\n\nÆ\n:\n\n&\n\nÆ\n\n8(\n;\n\n)\n\nD\n\"&\n!\n\n8Æ\n7&\n\nD\n\n&\nD\n\n8.\n\n/&\n!\n\n+\n\n&\n/\n\n!\"\n#\n$\n\"\n\n%%&'\n.4\n\n!\n\n\"#\n$%%\n!\n\n&\n!'\n-\n\n#\n\n!\n\n&\n\n#\n\n#\n\n1)/\n\n-\n\n4&\n\n-\n\n)0\n-\n&\n\n(\n&\n\n&\n\n#\n\n&\n\n#\n\n&\n\n#\n\n+\n\n&\n\n(\n\n1)/\n\nÆ\n\n!\n\nE\n\n#\n\n&\n\n/&\n\n&\n\n.\n\n+\n\n#\n\n&\n\n&\n\n&\n\nB\n\n(\n\n+\n\n&\n\n&\n\n+\n\n#\n\n/\n\n(\n\n&\n\n&\nF\n\n+\n\nÆ\n\n&\n\n*\n\nG\n\n-\n\nÆ\n\n&\n\nG\n\n&\n\n#\n\n(9\n\n#\n&\n\nG\n\n(\n\n:\n\nH\n-\n\n&\n\n+\n)\nA\n\n+)\nA\n\n/\n\n&\n\n:\n\n-\n\n&\n=\n\n-\n\n#\n\n&\n*\n\n:\n\n:\nG\nG\n\n=\n\n:\n\n&\n.I\n\n:\n\n:\n\n:\n\n:\n\nB\n=\n\n:\n\n:\n\nB\n=\n\n:\n\n+\n\n&\n\n:\n\n+\n\n&\n#\n\n(,\n\n:\n\n4&\n\n#\n\n&\n\n/\n1)/\n/\n4;\n+\n&\n\n#\n\n+)\nA\n\n(@\n\n(\n\n\"\n\n!\n\n1)/&\n\n-\n\n&\n\n#\n\n.\n\n+\n\n&\n\n4;\n'\n\n#\n\n#\n\n(\n\n&\n\n(\n\n\"\n\n\"\n\n\"\n\n'\n\n,\n\n/\n\n\"\n\nÆ\n\n\"\n\n.\n\n-\n\nD\n\n\"\n\n*\n\nD\n)\nA\n\nJ\n\nA\n\n1;\n\nK\n\n\"\nD\n\n\"\n\n\"\nD\n\n!\n\"\n\n<"
    },
    {
      "category": "Lecture Notes",
      "title": "lecture8.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/e14282fa474d0062e6cc21bbec07ed68_lecture8.pdf",
      "content": "!\"\n!\n\n#\n\n$\n%\n\n\"\n\n!\"\n\n&\n\n!\"\n\n'\n\n(\n)\n\n*\n+\n\n,\n\n-\n\n\"\n\n.\n\n/0\n\n\"\n\n\"\n\n*\n\n\"\n\n./\n\n\"\n\n.\n/\n\n$\n\n\"\n./\n\n./\n\n\"\n\n.\n\n/\n\n$\n\n./\n\n\"\n\n#\n\n!\"0\n\n\"\n\n\"\n\nÆ\n\n\"\n\n:\n\n\"\n\n;\n\n.\n/\n.#/\n\n)\n\n./\n\n./\n\n!\"\n+\n<\n*\n\n=\n\n>00\n\n\"\n\n.675/\n\n:\n\n\"\n\n,\n.\n\n\"\n\n?\n\n/\n\n!\n\n\"\n#\n\n$\n\n,\n\n$0\n\n$\n\n$\n\n\"\n\n\"\n\nqueue end\nstack end\npop\npush\ndelete\n\n$\n\n(\n\n./\n\n\"\n\n!\"\n\n\"\n\n!\n\n$0\n\n\"\n\n$\n\n\"\n\n$\n\n)\n\n,\n\n\"\n\n#\n\n\"\n\n,\n\n\"\n-\n\n$\n\n\"\n\n!\n\n\"\n\n?\n\n7)\n,\n\n+\n<\n*\n7#\n\nα 1\nα 0\nα κ-1\nα κ\n\n!\n\n\"\n\n$\n\n:\n#\n\n$\n%\n\n@\n>\n\n\"\n\n@\n>\n\n:\n%\n\n)\n\n,\n\n.\n/\n\n:\n\n,\n\n)\n\n:\n&0\n\n$\n-\n\n$0\n\n!\n\n$0\n\n\"\n\n\"\n\n-\n\n,\n\n:\n'\n<\n\n?\n\n$\n\nA\n\n7%\n\nα κ\nα 0\nα κ-2\nα κ-1\n\nα κ-1\nα κ-2\nα 0\nα child\nα κ\nα κ\nα κ-1\nα κ-2\nα 0\n\n7&\n\nα κ-1\nα κ-2\nα 0\nα κ\nα 0\nα κ-2\nα κ-1\n\n-\n\n,\n\n$\n\n\"\n\n)\n:\n&\n(\n\n$0\n\n$0\n\n\"\n\n$\n=\n\n!\n\"\n\n\"\n\n!\"\n\n\"0\n\n\"\n\n$\n\n+\n<\n*0\n\n\"\n\nB\n\n\"\n\n,\n\n\"\n\n+\n<\n*7\n\nB\n\n\"\n\n+\n<\n*0\n\n\"\n\n\"0\n\n$\n\n+\n<\n*0\n\n$\n\n'"
    },
    {
      "category": "Lecture Notes",
      "title": "lecture9.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/533da8dd57712f1c0414ef6c84baf90a_lecture9.pdf",
      "content": "6.895 Theory of Parallel Systems\nLecture 9\nAnalysis of Cilk Scheduler\nLecturer: Michael A. Bender\nScribe: Alexandru Caraca s, C. Scott Ananian\nLecture Summary\n1. The Cilk Scheduler\nWe review the Cilk scheduler.\n2. Location of Shallowest Thread\nWe define the depth of a thread and the shallowest thread. Next, We prove that the shallowest thread\non a processor is either at the top of a deque or being executed.\n3. Critical Threads\nWe construct a computation graph G′ similar to the computation graph G, such that when a thread\nhas no incomplete predecessors in G′, then it is at the top of a deque.\n4. Execution Time Bounds For The Cilk Scheduler\nWe present the execution time bounds of the Cilk scheduler. We introduce an accounting argument to\nhelp in proving the bound.\n5. Performance Analysis of Scheduling Algorithm Using Delay Sequences\nWe analyze the performance of the Cilk work-stealing scheduling algorithm. We define delay sequences\nand use them to prove the execution time bounds of the Cilk scheduler.\nThe\nCilk Scheduler\nIn this section we review the work-stealing algorithm of the Cilk scheduler. Recall that in Cilk a processor\nworks on a procedure α until one of three events occur:\n1. Procedure α Spawns procedure β. In this case the processor pushes α on (the bottom of) the ready\ndeque, and starts work on procedure β.\n2. Procedure α Returns. There are three cases:\n(a) If the deque is nonempty, the processor pops a procedure (from the bottom) and begins working\non it.\n(b) If the deque is empty, the processor tries to execute α's parent (which is not in a deque).\n(c) If the deque is empty and the processor is unable to execute α's parent (because the parent is\nbusy), then the processor work steals.\n3. Procedure α Syncs. If there are no outstanding children, then continue: we're properly synced. Oth\nerwise (when there are no outstanding children), work steal. Note that deque is empty in this case.\nWhen a processor begins work stealing it operates as follows:\n1. Processor chooses a victim uniformly at random.\n2. If the victim's deque is empty, the processor repeats the first step.\n9-1\n\nTop of deque\nuk\nuk-1\n. . .\nu2\nu1\nBottom of deque\nu0\nCurrently executing\nFigure 1: Deque of a processor.\n3. Otherwise, the processor steals the top (oldest) thread from the victim's deque and begins to work on\nit.\nNote: A thread can be in exactly one of three places:\n1. in the deque,\n2. on a processor being executed, or\n3. waiting for predecessors to complete.\nIn the following we use the notion of procedure as composed of multiple threads.\nLocation of shallowest thread\nIn this section we define the depth of a thread and the shallowest thread. Next we prove that the shallowest\nthread on a processor is either at the top of a deque or being executed.\nDefinition 1 (Depth of thread) The depth d(ui) of a thread ui is the longest distance in the DAG from\nthe root thread to ui.\nDefinition 2 (Shallowest Thread) The shal lowest thread is the thread with minimal depth.\nWe prove some structural properties of the deque.\nLemma 3 (Structural lemma) Consider any processor p at time t. Let u0 be the current root thread (of\nprocedure α0) executing on processor p. Let u1, u2, . . . , uk be the threads (of procedures α1, α2, . . . , αk ) in p's\ndeque ordered from bottom to top. Let d(ui) be the depth of thread ui in DAG G. Then,\nd(u0) ≥ d(u1) > · · · > d(uk-1) > d(uk ).\nObservation 4 A procedure is made up of multiple threads, but at a given time t, there is only one thread\nin the procedure which is being executed.\nCorollary 5 The shallowest thread in a processor is either at the top of a deque or being executed.\nProof\nProof is by induction on actions. The base case (deque is empty; u0 executing) trivially satisfies\nthe inequality. Thereafter, only four possible actions can alter the currently-executing thread and/or the\ndeque. We prove that each of these actions maintains the desired depth-ordering property.\n9-2\n\n=⇒\nu1\nu2\nuk-1\nuk\n. . .\nu1\nu2\nuk-1\n. . .\nu0\nu0\nFigure 2: Change in the deque of a processor when a procedure is stolen.\nu1\nu2\nuk-1\nuk\n. . .\n=⇒\nu1\nu2\nuk-1\nuk\n. . .\nu0\nFigure 3: Change in the deque of a processor when the processor returns from a procedure.\n- Case 1: Steal. A steal removes the top entry from the deque (see Figure 2). The processor performing\nthe steal begins executing uk with an empty deque, trivially satisfying the inequality as in the base\ncase. The processor from whom the thread was stolen also maintains the invariant, because if before\nthe steal\nd(u0) ≥ d(u1) > · · · > d(uk-1) > d(uk ),\nafter the steal\nd(u0) ≥ d(u1) > · · · > d(uk-1).\nRemoving threads from anywhere in the deque cannot invalidate the ordering.\n- Case 2: Return. A return removes the bottom entry from the deque (see Figure 3). As in the case\nof the steal, removing terms from the inequality cannot invalidate it. If before the return\nd(u0) ≥ d(u1) > · · · > d(uk-1) > d(uk ),\nafterwards we have\nd(u1) > · · · > d(uk-1) > d(uk ).\nAs before, removing threads from anywhere in the deque cannot invalidate the ordering.\n- Case 3: Continue or sync.\nA sync or continue replaces the bottom entry of the deque (see Figure 4)\nwith a thread with depth increased by one. A sync with one or more children outstanding becomes a\nsteal, and is handled by Case 1. If all children are complete, then a sync and a continue are identical:\nwe replace the currently-executing thread u0 with its successor in the DAG, ua. Figure 5 shows the\nrelevant piece of the computation DAG.\nIf the longest path to u0 has length d(u0), then clearly there is a path to ua through u0 which has\nlength d(u0) + 1. Therefore,\nd(ua) ≥ d(u0) + 1,\n9-3\n\n=⇒\nu1\nu2\nuk-1\nuk\n. . .\nu1\nu2\nuk-1\nuk\n. . .\nu0\nua\nFigure 4: Change in the deque of a processor when the processor reaches a sync point or continues a procedure.\nu0\nua\n· · ·\nα0\n· · ·\nFigure 5: A piece of a computation DAG. Thread u0 is followed by thread ua in procedure α0.\nand since d(u0) ≥ d(u1), then\nd(ua) > d(u1) > · · · > d(uk-1) > d(uk).\n- Case 4: Spawn.\nA spawn replaces the bottom entry in the deque and also pushes a new procedure\n(see Figure 6). Figure 7 shows the portion of the computation DAG involved in a spawn. Note that,\nsince the only edges to ua and us come from u0, that ua and us have the same depth in the DAG,\nnamely d(u0) + 1. Therefore since d(u0) ≥ d(u1), then d(us) = d(ua) > d(u1) and, as required\nd(us) ≥ d(ua) > d(u1) > · · · > d(uk-1) > d(uk ).\nNote: This is the only case where the depth of the currently-executing thread may become equal to\nthe depth of the thread on the bottom of the deque; hence the only case where a shallowest thread\nmay be the currently-executing thread instead of the thread at the top of the deque.\nSince the base case satisfies the property, and every action maintains the property, then at any time t in the\nexecution for all deques\nd(u0) ≥ d(u1) > · · · > d(uk-1) > d(uk ).\nu1\nu2\nuk-1\nuk\n. . .\nu0\n=⇒\nu1\nu2\nuk-1\nuk\n. . .\nua\nus\nFigure 6: Change in the deque of a processor when the processor spwans a procedure.\n9-4\n\nua\nus\nu0\n· · ·\nα\nα0\n· · ·\nspawn\n· · ·\nFigure 7: A piece of a computation DAG. Thread u0 is followed by thread ua in procedure α0, and spawns thread\nus in procedure αspawn.\nv1\nv2\nu0\nu2\nu1\n· · ·\nα2\nα0\nα1\n· · ·\n· · ·\n· · ·\nFigure 8: An augmented DAG corresponding to the execution DAG of Figure 9. The dashed edges have been added\nfrom continuation threads to spawn threads.\nCritical threads\nIn this section we construct an augmented computation DAG G′ similar to the computation DAG G, such\nthat when a thread has no incomplete predecessors in G′, then it is at the top of a deque. Next, we introduce\ncritical threads.\nOur goal is to show that any thread on the top of a deque cannot stay there long because it will be stolen.\nDefinition 6 (Construction of G′) We create an augmented DAG G′ by making a (back) edge to every\nspawn thread from the parent's continuation thread. Note that if a sync is immediately followed the spawn,\nwe need to insert an extra continuation thread before the sync to prevent cycles in the DAG. Figure 8 shows\nthe execution graph of Figure 9 augmented with these edges.\nWe show that work-stealing makes progress on the critical path. Roughly speaking, Θ(P ) steals\ndecrease the critical path by a constant. We require a stronger property for the computation DAG.\nProposition 7 (Necessary property) Whenever a thread has no incomplete predecessors in the aug\nmented DAG G′, it is being executed or is on the top of some deque.\n9-5\n\nv1\nv2\nu0\nu2\nu1\n· · ·\nα2\nα0\nα1\n· · ·\n· · ·\n· · ·\nFigure 9: An execution DAG with threads v2 and u2 in procedure α2, threads v1 and u1 in procedure α1, and\nthread u0 in procedure α0.\nNote: Proposition 7 does not hold for the execution DAG G, that is why we create G′.\nExample 1 To see why, consider the execution DAG in Figure 9. After threads v2 and v1 have been\nexecuted, we begin to execute thread u0. We have the following deque:\nu1\nu2\nu0\nTop of deque\nBottom of deque\nCurrently executing\nThe thread u1 has no unexecuted precedessors left in the DAG, but it is neither being executed nor at the\ntop of the deque.\nObservation 8 (Depth of Augmented DAG G′) The critical path of the augmented DAS G′ is 2Tinf.\nWe can now get to a spawned thread via our back edge from the continuation edge, adding a distance of one\nto the longest path to the spawned thread. But we can only add one such extra edge to the path per spawn\nedge on the critical path of the original graph G. If we define the depth of a graph D(G) to be the depth\nof the deepest node, then,\nD(G) ≤ D(G′) ≤ 2D(G),\nwhere D(G) = Tinf.\nProposition 7 guarantees that whenever you execute a thread, you will be executing one of the shallowest\nthreads in the augmented DAG G′. Hence, when you steal work (i.e. not otherwise making progress) you\nmake progress on the critical path by executing the threads which are shallowest.\nDefinition 9 (Critical thread) A thread is critical if it has no un-executed predecessors in the augmented\nDAG G′.\nNote: The extra edges in the augmented DAG G′ are not execution edges, they simply induce extra ordering\nthat we require for our performance analysis of the Cilk scheduler.\nExecution Time Bounds For The Cilk Scheduler\nThe Cilk work-stealing scheduling algorithm has provably good execution time bounds. We introduce an\naccounting argument based on two buckets to help in proving the bound.\n9-6\n\nFigure 10: The work and steal buckets used in the accounting argument.\nTheorem 10 (Cilk Scheduler execution time bound) Consider the execution of any fully strict mul\ntithreaded computation with work T1 and critical path Tinf by Cilk's work-stealing algorithm on a parallel\ncomputer with P processors. For any ε > 0, with probability at least 1 - ε, the execution time on P proces\nsors is\n\nT1\nTP ≤\n+ O\nTinf + lg\n.\n(1)\nP\nε\nProof Idea\nThe proof is based on an accounting argument. We have 2 buckets, one for work and\none for steal. On each time-step, a processor puts one dollar into one of the two buckets, depending on the\naction that it performs in that step. The possible two actions and their respective results are:\n- execute instruction - one dollar into work bucket,\n- steal attempt - one dollar into steal bucket.\nWe assume that all threads are of equal size and that all processors have the same time-step. In the following\nWe overload the notation of steal attempt with steal.\nLemma 11 At the end of computation, the number of dollars in the work bucket is T1.\nProof\nThe total work is exactly T1 so at the end of the computation exactly T1 work has been executed.\nThus the number of dollars in the work bucket is precisely T1.\nWe know bound the steals with the following lemma.\nLemma 12 (Scheduler) At the end of a computation the number of dollars in the steal bucket is O(PTinf),\nwith high probability.\nWe give an exact definition of high probability during the course of the proof. In the rest of the lecture,\nwe prove the previous lemma. We introduce several concepts to prove the bound.\nObservation 13 (Dollars) Per time-step P dollars enter the buckets.\nThus, the running time on P processors is the amount of time it takes to fill out all the buckets.\nClaim 14 The running time on P processors is less than the total number of dollars (actions) divided by\nthe number of processors:\n#dollars in work bucket + #dollars in steal bucket\nTP\n≤\nP\nT1\n≤\n+ O(Tinf).\n(2)\nP\nWe know how many dollars will be in the work bucket. We need a bound for the number of dollars in\nthe steal bucket.\n9-7\n\nFigure 11: Dividing computation into steal rounds.\nPerformance Analysis of Scheduling Algorithm Using Delay Se-\nquences\nIn this section, we analyze the performance of the Cilk work-stealing scheduling algorithm. We define delay\nsequences and use them to prove the execution time bounds of the Cilk scheduler.\nWe prove equation (2) in the rest of the lecture.\nObservation 15 There is no constant in front of the term T1/P in the previous bound. All scheduling\noverhead cost is amortized against the critical path Tinf. Also, one important observation is that the term\nT1/P is much bigger than the term Tinf.\n5.1\nRounds\nAs long as no processor is stealing, the computation is progressing efficiently. What divide the computation\ninto rounds, which (more or less) contain the same number of steals (see Figure 11).\nDefinition 16 (Round) A round of work stealing attempts is a set of at least P and at most 2P - 1 steal\nattempts. We assume that steal attempts complete in one unit of time.\nRound 1 begins at time 0 and ends when there have been at least P steals. In general, round i begins when\nround i - 1 ends, and round i ends when there have been at least P steals.\nRecall by Definition 9 that a critical thread is at the top of some deque or is being executed.\nProof Idea\nThe main idea is that, if a thread is on the top of a deque for CP steal attempts, then the\nprobability that it would executed for C rounds is very small, namely:\n1 CP\n1 -\n≤ e -C .\nP\nThe idea is that a thread is unlikely to be critical for many rounds. Also, a thread may be ready for\nmany rounds but not critical.\n9-8\n\nFigure 12: A directed path U = (u1, u2, . . . , uL) in the augmented DAG G.\nTheorem 17 The probability that a critical thread is un-stolen for C rounds is:\n1 CP\n1 -\n≤ e -C .\n(3)\nP\nProof\nFor C rounds, there are at least CP steals. The steal process is done at random and there are\nP places that a processor could steal from. Hence, the probability that a critical thread would be stolen\nduring a round is 1/P, and the probability that the critical thread will not be stolen is 1 - 1/P. Hence, for\nC rounds we obtain the probability that a critical thread is not stolen:\n1 CP\n1 -\n.\nP\nFor a simplified bound of the above equation we use Euler's inequality:\n\nx\n1 +\n≤ e.\nx\nWe obtain\n1 CP\n1 -\n≤ e -C .\nP\nFor a small number of rounds a thread might not be executed. However, for a large number of rounds\na thread will be executed with very high probability, because the term e-C in the previous bound becomes\nexponentially small. If a thread is at the top of a deck then it has no un-executed predecessors in the\naugmented DAG G′. Intuitively we would expect that in every time-step we are able to peal some threads\nfrom the augmented DAG such that in time O(PTinf) there would be no augment DAG left (see Figure 12).\nThis idea is expressed in the following lemma and explanation.\n5.2\nDelay Sequence Argument\nLemma 18 (Structural Lemma) There exists a directed path U = (u1, u2, . . . , uL) in G′ in which, during\nevery time-step of the computation, some thread in U is critical.\n9-9\n\nProof\nThe proof is by induction backwards in time.\nBase case: During last time-step t, last thread in G′ is uL and it is critical and executed (see Figure 12).\nInduction step: Suppose during time-step t, thread uj is critical. During time-step t - 1, either uj is\nalso critical, or if not it is because it has un-executed predecessors in G′. Some threads ui, ui′, . . . ≺G′ uj are\nbeing executed during t-1. One of them is minimal and is critical.\nMain idea: We prove that since the probability that any thread remains critical for a long period of\ntime is small, there cannot be too many steal attempts.\nObservation 19 An executing thread is not necessarily critical. The very last thread in the computation uL\nwhen it is being executed it is critical. The extra edges in the augmented DAG introduce additional ordering\n(see Figure 9).\nIn order to complete the proof for the execution time bound of the Cilk scheduler, we introduce some\ndefinitions and lemmas.\nDefinition 20 (Delay Sequence) A delay sequence is an event described by the triple (U, R, Π) where:\n- U = (u1, u2, . . . , uL) is a directed path in G′ from source of DAG to sink,\n- R > 0 is an integer,\n- Π = (π1, π2, . . . , πL) is a partition of R, namely\nπi = R\ni\nA delay sequence occurs if ∀i at least πi rounds occur while ui is critical and un-executed.\nA delay sequence is a combinatorial object. A delay sequence occurs if there is a directed path in G′\nin which one of the threads in the path is critical at each time-step.\nLemma 21 If at least 2P(2Tinf + R) steal attempts occur, then some (U, R, Π) delay sequence occurs.\nProof\nConsider a path U = (u1, u2, . . . , uL) for which during each time-step t some thread is critical.\nThere are at most 2Tinf rounds during which a thread in U executes, since L ≤ 2Tinf. For all the other R\nrounds, some thread is critical and not executed.\nNote: The converse need not be true.\nThe idea is to sum up over all possible events the probability that the event occurs. Show that the sum is\nsmall. We would then be able to show that with high probability there at most 2P(2Tinf + R) steal attempts.\nA delay sequence is an event that explains why there are at least 2P(2Tinf + R) steal attempts. The\nprobability that at least 2P(2Tinf + R) steal attempts occur is at most the probability that there exists a\ndelay sequence (U, R, Π) that occurs.\nObservation 22 (Number of Steal Attempts) We make the following observation about the probability\nthat at least 2P(2Tinf + R) steal attempts occur.\nat least 2P(2Tinf + R)\nexists delay sequence\nPr\n≤ Pr\nsteal attempts occur\n(U, R, Π) that occurs\n≤\nPr [(U, R, Π) occurs]\ndelay sequences\n(U, R, Π)\n⎛\n⎞\n\nMaximum Probability\nNumber of\n⎠\n≤\nDelay Sequences\n⎝\nthat any delay\n.\nsequence occurs\n9-10\n\nLemma 23 (Probability of Delay Sequence) The probability that a particular delay sequence (U, R, Π)\noccurs is e-R, which is exponentially small in the number of rounds, R.\nProof\n\nPr [(U, R, Π) occurs] =\n\n1≤i≤L\nPr\nπi rounds occur\nwhile ui is critical\n\n≤\ne -πi\n1≤i≤L\n= e -R .\nLemma 24 (Number of Delay Sequences) The number of delay sequences is at most\n22Tinf\n2Tinf + R\n2Tinf\n.\nProof\nWe have\n#paths ≤ 22Tinf ,\nbecause no path is longer that 2Tinf and the out degree of any thread (vertex) is at most two. Thus, there\nare two choices (either a continuation edge or a spawn edge) starting from the root node. Also\n#partitions of a path\n2Tinf + R\nin augmented DAG G′ =\n2Tinf\n.\nWe are partitioning R into L pieces. However, for large R, this is smaller than partitioning R into 2Tinf pieces.\nWe simply count up all the possible ways of partitioning the path. Thus, by multiplying the number of paths\nwith the number of ways in which a path can be partitioned we obtain the number of delay sequences, which\nis at most\n\n22Tinf\n2Tinf + R\n2Tinf\n.\nOur goal is to show that the probability that any delay sequence occurs is very small. Since we know the\nprobability of a given delay sequence we use the Boole's inequality to compute the probability of any delay\nsequence. Recall that:\nPr [A ∨B ∨C ∨· · · ] ≤ Pr [A] + Pr [B] + Pr [C] + · · · .\nFollowing from Lemma 23 and Lemma 24 we have that:\n⎛\n⎞\n\nMaximum Probability\nNumber of\n⎠\nPr [any (U, R, Π) occurs] ≤\nDelay Sequences\n⎝\nthat any delay\nsequence occurs\n≤ 22Tinf\n2Tinf + R\n-R\n2Tinf\ne\nNext, we use the following \"death-bed\" formula to simplify the above inequality.\ny x\ny\ney x\n≤\n≤\n.\nx\nx\nx\n9-11\n\nWe have:\ne(2Tinf + R) 2Tinf\n22Tinf\n-R\nPr [any (U, R, Π) occurs] ≤\ne\n2Tinf\n2e(2Tinf + R) 2Tinf\n≤\ne -R\n2Tinf\nIn the right part of the inequality, the first term is considerably large, while the second term is exponentially\nsmall in R. We want to choose a value for R such that the product of the two terms is small. We let\nR = 2CTinf where C is a constant. Replacing in the previous equation we have:\n[2e(C + 1)]1/C R\nPr [any (U, R, Π) occurs] ≤\n.\n(4)\ne\nDefinition 25 (High Probability) An event E occurs with high probability if the probability that the\nevent occurs that is Pr [E] ≥ 1 - P (n), where P (n) is a polynomial. That is the probability that the event\ndoes not occur is polynomially small.\nObservation 26 When C ≥ 4, probability decreases exponentially in R.\nExample 2 If we take C = 4 then the probability is less or equal to 0.84R .\nLet:\n[2e(1 + C)]1/C R\nε =\n.\ne\n≥ 2P (2Tinf + R)\nThen Pr\nsteal attempts occur\n≤ ε, when R = Ω(lg ε ).\nWrap-Up\n\nWe have shown that with high probability the number of steal attempts is small. We have come up with\nan event which explains why there are steal attempts. This event is a delay sequence. Next we counted\nthe number of events. We computed the probability that such an event occurs. By summing up all the\npossible events we have shown that with high probability there are few steal attempts. With probability\nat least 1 - ε the number of steal attempts is O(P Tinf + lg ε\n).\nWe have started our analysis using an accounting argument, based on a work bucket and a steal\nbucket.We have shown that at the end of the computation there are T1 dollars in the work bucket and with\nprobability at least 1 - ε there are O(P Tinf + lg ε\n) dollars in the steal bucket. Thus, with probability\nat least 1 - ε:\nT1\n\nTP ≤\n+ O\nTinf + lg\n.\nP\nε\n6.1\nDeath-Bed Formulae\nFigure 13 shows a list of useful (\"death-bed\") formulae that were used in the analysis of the Cilk scheduler.\n\nEuler's inequality:\n\nx\n\n1 x+1\n1 +\n≤ e ≤ 1 +\n,\n(5)\nx\nx\n\nx\n1 x-1\n\n1 -\n≤\n≤ 1 -\n.\n(6)\nx\ne\nx\nBoole's inequality (Union bound):\nPr [A ∨ B ∨ C ∨ . . .] ≤ Pr [A] + Pr [B] + Pr [C] + . . . .\n(7)\nCombination of y choose x bound:\ny x\ny\ney x\n≤\n≤\n.\n(8)\nx\nx\nx\nFigure 13: Death-bed formulae."
    },
    {
      "category": "Lecture Notes",
      "title": "lecture10.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/a54de638968410376d5c27ea7842d20f_lecture10.pdf",
      "content": "!\n\"\n\n#\n$\n\n\"\n\n#\n\n$\n\n%\n\n!\n\n!\n\n#\n&\n\n#$\n\n#\n\n#\n\n$\n\n!\n\n!\n$\n\n#\n\n#\n\n$\n\n$\n\n$\n\n'\n\n#\n\n(\n\n)\n$\n\n*\n!+,\n-\n\n#\n\n.\n\n/\n\n&#\n\n$\n\n#\n\n&#\n\n-\n\n/\n\n#\n\n$\n\n#\n\n(\n#\n\n-\n\n\"\n\n$\n%\n\n(\n\n+\n\n$\n\n$\n\n.\n\n$\n\n!\n\ncilk2c\nC post-\nsource\nsource-to-source\ntranslator\nCilk\nsource\ngcc\nobject\ncode\nC compiler\nld\nCilk\nRTS\nbinary\nlinking\nloader\nCilk Run Time System\n\n#\n\n'\n\n#\n\n$\n\n#\n$\n\n!\n$\n\n#\n\n(\n\n(\n\n#\n\n$\n\n-\n\n#\n\n!\n\n\"\n\n#\n\n$\n(\n\n(\n\n$\n\n#\n\n%\n\n)\n\n(\n\n#$\n\n$\n\n#\n\n#\n\n$\n\n#\n\n)\n\n#\n\n(\n\n!\"\n\"\n\n#\n$\n\n%#\n\n#\n-\n\n!\n\nSLOW\nFAST\nFAST\nFAST\nFAST\nFAST\n\n!\n\"\n\n#\n\n$\n\n#\n\n%\n\n&\n'\n(\n\n)\n\n#\n$\n\n-\n\n%\n\n#\n\n$\n\n#\n\n*\n\n)\n\n(\n&\n\n#\n\n*\n\n#\n$\n\n*\n\n)\n#\n\n#\n\n(\n\n(\n#\n\n)\n\n!\n\n%\n\n#\n\n(\n\n%\n\n(\n\n$\n\n#\n\n+\n\n$\n\n%\n\n#\n\n(\n\n%\n\n#\n\n#\n\n!7\n\nsuspend\nparent\nrun child\nresume\nparent\nremotely\ncilk2c\nx = spawn fib(n-1);\nCilk\nsource\nframe->entry = 1;\nframe->n = n;\npush(frame);\nx = fib(n-1);\nif (pop() == FAILURE)\n{ frame->x = x;\nframe->join--;\n[clean up & return\nto scheduler ]\n}\nC post-\nsource\nentry\njoin\nn\nx\ny\nentry\njoin\nCilk\ndeque\nframe\n\nSLOW\nFAST\nFAST\nFAST\nFAST\nFAST\nCilk\nsource\nsync;\ncilk2c\n;\nC post-\nsource\n\n(\n\n#\n\n#\n\n$\n\n#\n\n#\n\n.\n\n$\n\n\"\n\n$\n\n#\n$\n(\n\n*\n\n(\n\n!\n%\n:\n\n(\n\n#\n\n$%\n\nÆ$\n\n)\n\n$\n\n#\n\n#\n$\n\n%#\n\n#\n\n#\n\n!\n\n%\n;\n\n!\n\n#\n\n+\n\n#\n\n#\n\n:\n\n#\n\n$\n\n$\n!9\n\nvoid fib_slow(fib_frame *frame)\n{\nint n,x,y;\nswitch (frame->entry) {\ncase 1: goto L1;\ncase 2: goto L2;\ncase 3: goto L3;\n}\n. . .\nframe->entry = 1;\nframe->n = n;\npush(frame);\nx = fib(n-1);\nif (pop() == FAILURE)\n{ frame->x = x;\nframe->join--;\n[clean up & return\nto scheduler ]\n}\nif (0) {\nL1:;\nn = frame->n;\n}\n. . .\n}\nentry\njoin\nn\nx\ny\nentry\njoin\nCilk\ndeque\nrestore\nprogram\ncounter\ncontinue\nsame\nas fast\nclone\nrestore local\nvariables\nif resuming\nframe\n\nSLOW\nFAST\nFAST\nFAST\nFAST\nFAST\nCilk\nsource\nsync;\ncilk2c\nif (frame->join > 0)\n{\n[clean up & return\nto scheduler]\n};\nC post-\nsource\n\n&\n\n<\n\n$\n\n$\n\n#\n\n.\n\n(\n\n$\n:\n\n$\n\n$\n\n$\n\n#\n\n$\n\n3#$!\n\n$\n\n#\n\n#\n\n$\n\n!8\n\n\"\n!\n\n!$\n\n$\n\n)\n$\n\n$\n\n#\n\nÆ$\n\n#\n\n\"\n\n+\n\n+\n\n$\n\n#\n\n$\n\n(\n\n#\n\n(\n\n#\n\n(\n\n)\n\n!#\n\n%\n=\n\n.\n\n#\n$\n\n#\n\n>\n\n#\n$\n\n#\n\n.\n\n$\n\n#\n$\n\n(\n\n+\n\n$\n\n+\n\n#\n$\n\n$\n\n$\n\n$\n\n#\n\n$\n\n#\n\n%\n\n%\n?\n\"\n\n$\n\n$\n\n#\n\n$\n\n#\n>\n\n$\n\n#\n\n$\n\n!:\n\n+\n\n,''\n\n\"\n,\n\n$\n\n-\n.\n,\n\n%\n,''\n&\n/\n(\n,\n\n)\n\n-\n.\n,\n\n,''\n\n/\n\n012/345\n\n\"\n/\n$\n\n%\n\n&\n&'\n\n/\n\n-''\n\n-\n.\n,\n\n\"\n-\n\n$\n/\n%\n\n012/345\n&\n\n(\n/\n)\n\n& '\n\n(a)\n(b)\n(c)\nH\nH\nT\nT\nH = T\n\n!;\n\n+\n\n,''\n\n\"\n,\n\n$\n\n.\n,\n\n%\n,''\n&\n/\n(\n,\n\n)\n\n-\n.\n,\n\n,''\n\n/\n\n012/345\n\n\"\n/\n$\n\n%\n\n&\n\n&'\n\n/\n\n5''\n\n-''\n\"\n\n-\n.\n,\n\n$\n\n%\n-''\n&\n/\n(\n\n012/345\n)\n\n/\n\n& '\n\n$\n\n#\n\n#\n\n$\n\n$\n\n$\n\n$\n\n$\n\n$\n\n#\n\n@\n\n$\n\n'$\n\n$\n:\n\nA\n\n$\n\n$\n\n3#4\n\n#\n\n#\n\n$\n$\n\n#\n\n#\n$\n\n#\n\n!\n\nB\n\n(\n\n(\n\n(\n\n$\n\n-\n\n#-\n\n$\n\n%\n\n#\n\n(\n\n%\n\n(\n\nC\n\n(\n\n(\n\n$\n\n(\n$\n\n>\n\n>\n\n(\n\n!=\n\n+\n\n(\n$\n\n+\n\n(\n\n#\n\n#\n\n#\n\n#!\n\n(\n\n(\n\n?"
    },
    {
      "category": "Resource",
      "title": "advait_sriram.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/1ec2c64d740de3aac8bf4809c8dcfc58_advait_sriram.pdf",
      "content": "Cache-efficient sorting for Burrows-Wheeler Transform\nAdvait D. Karande\nSriram Saroop\nDecember 2003\nAbstract\nThe expensive activity during compression in the Burrows-Wheeler Transform (BWT) is sorting of\nall the rotations of the block of data to compress. Most of the fast suffix sorting algorithms suggested\nto encounter this problem suffer from adverse memory effects. We present cache-efficient suffix sorting\nalgorithms that can overcome this overhead. We analyze the performance of Sadakane's algorithm that\nuses a cache-oblivious Distribution sort instead of Quick sort. We then present a fast cache-efficient\nsuffix sorting algorithm based on a Divide and Conquer strategy and evaluate its effectiveness. A recent\nbreakthrough in the field of suffix sorting has been the emergence of a linear time suffix sorting algorithm.\nWe present improvisations to the algorithm and compare its performance against contemporary suffix\nsorting implementations.\nIntroduction\nLossless data compression based on Burrows-Wheeler Transform (BWT) has received much attention\nsince its introduction [1]. The bzip2 software is an industrial strength compression tool based on this\ntransform. BWT involves sorting of all rotations of the block of data to be compressed. This problem is\nusually solved by transforming it into an easier problem of suffix sorting. Suffix array data structure is\nwidely used to perform this task, because of its lower memory requirements than other alternatives such\nas suffix trees. Yet, suffix array construction is a computationally intensive activity, which dominates the\ncompression time.\nSeveral successful attempts have been made to improve the asymptotic behavior of BWT. The original\nimplementation incurs the worst case complexity of O(N 2log(N)), if there is a high degree of repetitiveness\nin the input file. The number of characters, N, is typically quite large and such worst case behavior is\nunacceptable. An algorithm introduced by Larsson and Sadakane for suffix sorting, has a better worst case\nbound of O(N(logN)) [2]. Recently, linear time algorithms have been proposed [3][4], but little is known\nabout their practicability.\nThe practical aspects of the suffix array construction, however, have received little attention. The locality\nof reference and hence the cache performance is one such factor. On modern architectures with multiple\nlevels of memory hierarchy, the cache behavior and the memory access pattern of an algorithm play an\nimportant role in its overall performance.\nThe objective of this project is to devise ways of making suffix sorting for Burrows-Wheeler transform\ncache-efficient. In this paper, we present three approaches that can potentially lead to an improved cache\nbehavior for suffix sorting.\n- The use of cache-oblivious distribution sort in suffix sorting. It performs a factor of 1.76 to 3.76\nslower than the quick-sort based approach for relatively small block sizes. However, we expect it to\noutperform quick-sort and merge-sort based approaches when the block size is increased to extents\nthat accomodate paging. (Section 2)\nA divide and conquer algorithm to exploit the spatial locality of reference. It has O(N 2) running time\n-\nand takes 8N extra space. It displays better cache behavior than asymptotically faster algorithms such\nas Larsson-Sadakane. (Section 3)\n\n- Improvements to a linear time suffix array construction algorithm invented recently by Aluru et al [3].\nWe find that our implementation performs as well as contemporary fast suffix sorting implementations.\nIt displays the linear behavior as per expectations. We also analyze the trade-offs that need to be\nconsidered in order to come up with better implementations.(Section 4)\nWe conclude this paper in Section 5 by summarizing the main results and providing pointers for future\nwork based on this project.\nIncorporating Cache-oblivious Distribution sort\n2.1\nMotivation\nSadakane's algorithm for fast suffix-sorting involves passes that sort the suffix according to a given key\nvalue. Each of these passes can be modelled as an integer-sorting problem. Cache-oblivious sorting of\nstrings has not been dealt with in the past because of the implicit difficulty caused due to the low locality\nof references involved. However, cache-oblivious integer sorting algorithms have been developed in the past.\nThe modelling of Sadakane's suffix-sorting as passes of integer array sorting facilitates the usage of cache-\noblivious integer-sorting algorithms.\n2.2\nA Discussion of the Algorithm\nThe cache-oblivious distribution sort is described in [5]. The key aspects of the algorithm are described\nbelow. The distribution sorting algorithm uses O(n lg n) work to sort n elements, and it incurs (1 +\n(n/L)(1 + logZ n)) cache misses if the cache is tall. This algorithm uses a bucket - splitting technique to\nselect pivots incrementally during the distribution.\nGiven an array A of length n, the cache-oblivious distribution sort sorts A as follows:\nA\npn\npn.\nq pn\nB1\nq\n1. Partition\ninto\ncontiguous subarrays of size\n2. Distribute the sorted subarrays into\nbuckets\n, B , . . . , B\nRecursively sort each subarray.\nof size n1, n2, . . . , nq , respectively,\nsuch that for i = 1, 2, 3, . . . , q - 1, we have\n(a) max\nxBi} min{x\nni 2pn.\n{x\nxBi+1},\n|\n|\n(b)\n3. Recursively sort each bucket.\n4. Copy the sorted buckets back to array A.\nStrategy\nThe crux of the algorithm is the Distribution step, which is Step 2 of the above algorithm.\npn\nFirstly, each bucket holds at most 2\nelements at any time, and any\nThere are\ntwo invariants in the algorithm.\nelement in bucket Bi is smaller than any element in bucket Bi+1. Secondly, every bucket has a pivot\nassociated with it. The pivot is a value that is greater than all elements in the bucket. Initially, there\nexists only one empty bucket with pivot ∗. At the end of Step 2, all elements will be in buckets and both\nthe conditions stated in Step 2 will hold. State information for a subarray includes the index next of the\nelement which is to be copied next. The other component of the state information is the index bnum of the\nbucket into which the next element is to be copied. The basic strategy is to copy the element at position\nnext into bucket bnum. We increment bnum until we find a bucket for which the pivot is greater than the\nelement. However, this basic strategy has a poor caching behavior. Hence, a recursive approach is used\nto distribute the subarrays into buckets. The distribution step is accomplished by a recursive procedure,\nDISTRIBUTE(i, j, m) which distributes elements from the ith through (i+ m- 1)th subarrays into buckets\n\nstarting from Bj . Given the precondition that each subarray r = i, i + 1, . . . , i + m - 1 has its bnum[r] √ j,\nthe execution of DISTRIBUTE(i, j, m) enforces the postcondition that bnum[r] √ j + m. Step 2 invokes\nDISTRIBUTE(1, 1, pn). The DISTRIBUTE procedure is shown below:\nDISTRIBUTE(i, j, m)\nif m = 1\nthen COPYELEMS(i, j)\nelse DISTRIBUTE(i, j, m/2)\nDISTRIBUTE(i + m/2, j, m/2)\nDISTRIBUTE(i, j + m/2, m/2)\nDISTRIBUTE(i + m/2, j + m/2, m/2)\nThe base case occurs when only one bucket needs to be distributed. We then call the subroutine\nCOPYELEMS(i,j) that copies all elements from subarray i to bucket j. If bucket j has more than 2pn\nelements after the insertion, it is split into two buckets of size at least pn. The deterministic median-finding\nalgorithm is used for the partitioning of the bucket. The distribution sort described above is provably optimal\nin terms of number of cache-misses incurred = (1 + (n/L)(1 + logZ n)).\n2.3\nTesting Environment\nThe tests were performed on Sunfire (sf3.comp.nus.edu.sg). The technical specifications are shown in Table 1.\nTable 1: Testing Environment used\nP arameter\nV alue\nProcessor\nUltra Sparc III\nNumber of processors\nOperating System\nSolaris V8.0\nProcessor Speed\n750MHz\nCompiler\ngcc 3.2.2\nL1 Cache size\n64KB 4-way Data, 32KB 4-way Instruction\nL2 Cache size\n8MB\nRAM\n8GB\n2.4\nPerformance\nWe implemented Sadakane's algorithm for suffix sorting that used distribution sort for single character\ncomparisons, and compared it with implementations of Sadakane's algorithm that used Merge Sort and\nQuick Sort respectively. The last two columns in Table 2, RDistri/M erge and RDistri/Qsort are the ratios of\ntimes taken by Distribution sort versus Merge sort and Quick sort respectively. The ratios were calculated\nfor different files which varied from a random string like rand.txt that provides an example of a fairly good\ncase to worse cases exemplified by a large amount of repetition like the genome chromosome sequence -\ngene.txt. Table 2 shows that in the above cases, Distribution sort performs at worst a factor of 3.76 slower\nthan Quick sort and a factor of 3.452 slower than Merge sort. Considering the fact that Quick sort and\nMerge Sort do not suffer from as much memory management overhead as Distribution sort, this factor is\nacceptable. Moreover for the cases considered above, the cache misses may not be as dominating a factor\nas the extra housekeeping needed to store state information and splitting the buckets. These factors are\ndiscussed in the next subsection.\n\nTable 2: Performance of Distribution sort compared against Quick sort and Merge sort\nF ileN ame\nSize\ncia.txt\n52KB\nlinux.txt\n80KB\nbin.txt\n111KB\nbinop.txt\n550KB\nrome.txt\n1618KB\nrand.txt\n4MB\ngene.txt\n8MB\nTime(in sec.)\nQsort\nM ergeSort\nDistriSort\n0.081\n0.100\n0.170\n0.130\n0.140\n0.251\n0.210\n0.231\n0.370\n1.061\n1.102\n1.873\n2.594\n2.804\n9.634\n4.775\n5.201\n17.954\n32.166\n53.232\n100.224\nRDistri/M erge\nRDistri/Qsort\n1.700\n2.099\n1.793\n1.930\n1.602\n1.762\n1.699\n1.765\n3.436\n3.714\n3.452\n3.760\n1.883\n3.116\nOverheads incurred\nThe implementation of the distribution sort incurs a lot of memory management overhead unlike quick sort or\neven merge sort. The bucket splitting involves dynamic memory allocation using mallocs. This deteriorates\nthe performance of the implementation in terms of the observed timing. The cache is not ideal, and hence\nthe required cache-obliviousness may not be observed perfectly. Moreover, it is possible that the run-time is\nnot dominated by the cache-misses. The extra memory management in terms of maintaining correct state\ninformation for the buckets and the subarrays contribute adversely to the observed timings. We initially spilt\nthe original array into pn subarrays of size pn each. The splitting into square roots causes rounding errors,\nand we need to draw out special cases in order to produce correct results, which again results in an overhead.\nHence, the above mentioned factors contribute to the degradation of the observed performance. However,\nif we increase the file sizes to arbitrary lengths, page faults occur. In this paging scenario, distribution sort\ncan potentially perform better than merge sort or quick sort. This is an aspect which needs to be explored\nfurther.\nA divide and conquer algorithm for suffix sorting\nThis algorithm extends the basic divide and conquer strategy of the merge-sort algorithm used for sorting\nnumbers. Larger suffix arrays are formed by merging smaller suffix arrays formed recursively. In addition it\nhas two other important aspects.\n- Right-to-left processing: The relative order among the suffixes near the end of the block can be\ndetermined using a small number of character comparisons. The suffixes on their left (towards the\nbeginning of the block) can in turn use this order to determine the relative order among themselves.\nThe algorithm makes use of this intuition. The suffixes are sorted from right to left in a given block of\ndata.\n- Storing of match-lengths: The match length between of any two suffixes is the length of their common\nprefix. If two suffixes have a large common prefix, then determination of the relative order between\nthem requires a large number of comparisons. Comparisons of many other suffixes sharing a portion\nof this common prefix would also involve the same comparisons. We want to avoid this duplication of\nwork by storing the match lengths of the suffixes whose relative order has been determined.\nWe begin by defining the data structures used by the algorithm.\n3.1\nData structures and notations\n- T [0..N - 1]: The original block of characters to be transformed\n- SA[0..N - 1]: The suffix array which finally contains the sorted order of suffixes\n\n- M L[0..N - 1]: An array to store the match lengths between the sorted consecutive suffixes. M L[i] is\nthe match length between suffix SA[i] and SA[i - 1]. M L[0] = 0\n- last match len: The match length of the two leftmost suffixes in the sorted part.\n- P OS[0..1]: It stores the relative position of the two leftmost sorted suffixes\nSuffix Si denotes the suffix starting at T [i]. Similarly, suffix SA[i] denotes the suffix starting at position\nT[SA[i]]. The main structure of the algorithm is similar to that of merge-sort as shown below. We define the\nprecedence relation, ≤, on the suffixes as follows. Si ≤ Sj if and only if suffix Si is lexicographically smaller\nthan Sj .\nsort(unsigned char T[], int SA[], int ML[], int p, int q){\nif q - p 1\nthen compare suffixes Sp and Sp+1\nstore the order in POS[]\nupdate last match len\nupdate SA[p..q] and M L[p..q]\nreturn\nmid = (p+q+1)/2;\n//sort the right half\nsort(T, SA, ML, mid, q);\n//sort the left half\nsort(T, SA, ML, p, mid-1);\n//merge the two halves\nmerge(T, SA, ML, p, q);\n}\nThe algorithm recursively processes the two halves of the string - first the right half and then the left half.\nNow, at the lowest level of recursion, the length of the string to be processed is at most 2 (i.e. q - p 1).\nThe two suffixes Sp and Sp+1 are compared and their relative order is determined. The order is stored in\nthe POS array. If Sp ≤ Sp+1 then P OS =< p, p + 1 > and if Sp+1 ≤ Sp then P OS =< p + 1, p >.\nLemma 1 If q - p 1, the relative order of the two suffixes Sp and Sp+1 and their match length can be\ncompletely determined by string T [p..q + 1] and P OS.\nProof\nThe base case arises when q = N -1. Here T [p..q] is a rightmost substring of the original block. It is\nthe first short string to be processed and the POS array and last match len do not contain any information.\nHowever, by construction, T [N - 1] is the lowest character in the alphabet that does not appear in the string.\nHence T [N - 2] and T [N - 1] cannot be equal. Therefore their relative order is Sp+1 ≤ Sp; and the match\nlength equal to zero.\nIf q = N - 1 then suffix Sp can be represented as a concatenation of T [p..q] and T [q + 1..N - 1]. Sim-\n≥\nilarly, suffix Sp+1 is a concatenation of T [p + 1..q + 1] and T [q + 2..N - 1]. We first compare T [p..q] with\nT [p + 1..q + 1]. If they are not equal their relative order and the match length is evidently found. Otherwise,\nthe order is determined by the relative order between T [q + 1] and T [q + 2] However, this order is already\nknown, and has been stored in the POS array. Similarly, if T [p..q] with T [p + 1..q + 1] are equal, then the\nmatch length between Sp and Sp+1 is equal to (last match len + q - p + 1). Hence the result follows.\n\nThus, at any time, the POS array and the last match len variable store the information about the relative\norder and the match length of the two leftmost suffixes of the block, whose relative order has been determined.\nHence they propagate this information from right to left at the lowest level of recursion.\nThe relative orders and match lengths are also stored in SA and ML arrays, to be used at all the levels\nof recursion and they are updated in a bottom-up manner while merging of two suffix arrays.\nThe merging phase involves merging of the left and right halves of the suffix array formed recursively.\nmerge(unsigned char T[], int SA[], int ML[], int p, int q){\nmid = (p+q+1)/2;\ncreate arrays SA1[0..mid-1] and SA2[0..q-mid]\ncopy array SA[p..p+mid-1] into SA1[0..mid-1]\ncopy array SA[p+mid..q] into SA2[0..q-mid]\ncreate arrays ML1[0..mid-1] and ML2[0..q-mid]\ncopy array ML[p..p+mid-1] into ML1[0..mid-1]\ncopy array ML[p+mid..q] into ML2[0..q-mid]\nlast insert\n= 0\nprev\nlen\nmatch\n= LEFT\ni=j=k=r=0;\nwhile\nif last insert\n(i < mid AND j < q - mid)\n= LEFT\nthen min(prev\nmin(prev\nmatch len, ML1[i])\nr =\nmatch len, ML2[j])\nr =\nelse\ncompare suffixes T[SA1[i]+r] and T[SA2[j]+r]\nif SA1[i + r] ≤ SA2[j + r]\nthen\nSA[k] = SA1[i]\nif prev insert = LEFT\nthen ML[k] = ML1[i]\nelse ML[k] = prev\nlen\nmatch\ni=i+1\n= LEFT\nlast insert\nelse\nSA[k] = SA2[j]\nif prev insert = RIGHT\nthen ML[k] = ML2[j]\nelse ML[k] = prev\nlen\nmatch\nj=j+1\n= RIGHT\nlen\nupdate prev match\nlast insert\nk=k+1\nif i < mid\nthen copy SA1[i..mid] into SA[k..q]\nif j < q - mid\nthen copy SA2[j..q-mid] into SA[k..q]\n}\n\nThe routine merges two suffix arrays SA1 and SA2 into SA. Its structure is similar to the merge routine\nused in any merge sort. Figure 1 provides an illustration of the algorithm. The next two sections describe\nthe algorithm, especially the merging process in detail.\na\nb c\nd\n\\0\na\na\na\na\na\nb\nb\nb\nb\nb\nd\n0 1 2 3 4 5 6\n7 8 9 10 11\n0 0 0 0 5 4\n0 0 0 0 0 0\n0 0\n0 0\n2 0 3\ni\nT\nSA\nML\n12 1314 15\nPOS <5, 4>\nlast_match_len = 0\nFigure 1: A snapshot of the data structures as the divide and conquer algorithm builds a suffix array SA for block\nof data T . Suffixes T8 to T15 have been sorted and their order has been stored in SA[8..15]. Two suffix arrays of\nlength two (SA[4..5] and SA[6..7]) have been created in the sort phase. The next step would be two merge these two\nsuffix arrays into SA[4..7]. POS array stores the relative order of T4 and T5. The last match len variable stores the\nmatch length between the same.\n3.2\nComputation of match lengths\nThe routine computes the match lengths for the new suffix array. The variable prev match len stores the\nmatch lengths between the suffixes from SA1 and SA2 compared in the previous iteration of the loop. The\nvariable prev insert keeps track of which suffix array the suffix inserted in the SA in the previous iteration\ncame from. These two variables help us to compute minimum match length between the two suffixes SA1[i]\nand SA2[j] being compared in the current iteration. For example, let the previous insertion into SA be from\nthe left suffix array SA1. In other words, SA[k - 1] = SA1[i - 1] at the beginning of the current iteration.\nTherefore the prev match len variable stores the match length between SA1[i-1] and SA2[j]. Since, ML1[i]\nstores the match length between SA1[i - 1] and SA1[i], the match length between the suffixes SA1[i] and\nSA2[j] must be at least r = min(prev match len, ML[i]). Therefore substrings T [SA1[i]..SA[i] + r - 1] and\nT [SA2[j]..SA2[j] + r - 1] must be the same. Here, r is the length of minimun common prefix of the two\nsuffixes. Hence the order between suffixes SA1[i] and SA2[j] can be determined by comparing the suffixes\nstarting at T [SA1[i] + r] and T [SA2[j] + r] respectively. This mechanism avoids the necessity for repeated\ncomparisons.\nAfter determining the order between suffixes SA1[i] and SA2[j], we insert the smaller suffix into SA[k].\nWe also need to store the match length between SA[k - 1] and SA[k] in ML[k]. If SA[k - 1] and SA[k]\ncame from the same parent array then their match length is already known and can be copied from the\ncorresponding match length array. Otherwise, the match length between the two is equal to the value of the\nprev match len variable. Finally, the prev match len variable is updated to store the match length between\nSA1[i] and SA2[j]. This match length is found while comparing the two suffixes as explained in the next\nsection.\n\n3.3\nComparison of suffixes\nIn the merge routine the order between the suffixes SA1[i] and SA2[j] is determined by the order between\nthe suffixes starting at positions T[SA1[i] + r] and T[SA2[j] + r]. Let d = SA2[j] - SA1[i], the distance\nbetween the start points of the two suffixes. Then, in the worst case, this comparison would involve the\ncomparison between characters T[SA1[i]+r..N-d-1] and T[SA2[j]+r..N-1], which is computationally expensive.\nHowever, it turns out that this worst case behavior can be avoided based on the data structures that have\nbeen built so far. Recall that SA1[0..mid-1] contains the order of the suffixes Sp, Sp+1, . . . , Smid-1. Similarly,\nSA2[0..q-mid] stores the order of the suffixes Smid, Smid+1, . . . , Sq . Since the algorithm sorts suffixes from\nright to left, we know that the relative order of some suffixes starting from Sq+1 is already known. The\nfolowing observation formalizes this intuition based on the divide and conquer nature of the algorithm and\nright to left processing.\nObservation 2 While merging the suffix arrays for suffixes Sp, Sp+1, . . . , Smid-1 and Smid, Smid+1, . . . , Sq ,\nif q = N - 1, the relative order of at least M suffixes Sq+1, Sq+2, . . . , Sq+M is known and has been stored in\n≥\nSA[q + 1..q + M], where M = q - p + 1 and M N - q\nNote that if q = N - 1, the relative order of suffixes Sp, Sp+1, . . . , Sq can be completely determined by\nthe substring T[p..q], since the last character T[N - 1] is the lowest character in the alphabet, not appearing\nin the original block. For all other cases, if two substrings of equal length in T[p..q] are found to be equal\nthen the relative order of their corresponding suffixes can be determined using the order of already sorted\nsuffixes.\nWhile comparing the suffixes starting at positions T[SA1[i] + r] and T[SA2[j] + r], we need to handle 3\ncases.\nCase 1: SA1[i] + r q\nWe first compare T[SA1[i]+r..q] and T[SA2[j]+r..q+d]. Depending on their equality there are two subcases.\nCase 1a: T[SA1[i] + r..q] = T[SA2[j] + r..q + d]\n≥\nIn this case, the order has evidently been found. The match length between the two suffixes is the sum of r\nand the length of the common prefix of T[SA1[i] + r..q] and T[SA2[j] + r..q + d].\nCase 1b: T[SA1[i] + r..q] = T[SA2[j] + r..q + d]\nNow we must determine the order between Sq+1 and Sq+d+1. Start scanning the suffix array from SA[q + 1]\nand find positions a and b in the array such that SA[a] = q + 1 and SA[b] = q + d + 1. If a < b, then the\nSq+1 ≤ Sq+d+1, which implies SSA1[i] ≤ SSA2[j]. Similarly, if b < a, then SSA2[j] ≤ SSA1[i]. In this case,\nthe match length between SSA1[i] and SSA2[j] is the sum of q - SA[i] + 1 and the match length of Sq+1\nand Sq+d+1. The match length between Sq+1 and Sq+d+1 found by scanning the match length array ML as\nfollows.\n\nmin(ML[a + 1], ML[a + 2], . . . , ML[b]) if a < b\nmlab =\nmin(ML[b + 1], ML[b + 2], . . . , ML[a])\nif b < a\nCase 2: SA1[i] + r > q\nIn this case, the order between SA1[i] and SA2[j] is same as the order between Sq+1 and Sq+d+1. The order\nand match length is determined in the same manner as case 1b.\n3.4\nTiming analysis\nIn this analysis, for the sake of simplicity of explanation, we assume the block size, N, to be a power\nof 2, such that N = 2c . Also, we measure the running time of the algorithm in terms of the number of\ncharacter comparisons. A comparison of two suffixes would involve multiple character comparisons. For\nexample, consider the merging of two suffix arrays, SA[p..mid - 1] and SA[mid..q]. We define the character\ncomparisons on the portion of the block that is currently being merged as local comparisons. Therefore, the\n\ncharacter comparisons performed the substring T [p..q] during the merge process are the local comparisons.\nThey correspond to Case 1 described in Section 3.3. If the local portions of the two suffixes are equal, then\nthe sorted portion of the suffix array starting from SA[q + 1] is scanned to determine their relative order.\nThe comparisons involved in the scanning operation are termed as remote comparisons. We analyse the\nnumber of local and remote comparisons separately.\nLocal comparisons\nConsider the merging of two suffix arrays of length N/2 each at the topmost level of recursion. It involves\nat most N suffix comparisons. Further, each suffix comparison may involve at most N local comparisons.\nNote that this is a loose bound, because even for a block with high degree of repetitiveness, the number of\ncomparisons would be reduced by the match length heuristic. In total, N 2 local comparisons are needed.\nNow consider the next level of recursion, which involves two merges of two suffix arrays, each of length\nN/4. Here there are at most N suffix comparisons, N/2, in each merge. Now, number of local comparisons\nin each suffix comparison would be at most N/2, resulting in at most N 2/2 local comparisons.\nSimilarly, we can calculate the number of local comparisons at each level of recursion. At the lowest\nlevel, when 2c-2 merges result in as many suffix arrays of length 4, N 2/2c-2 local comparisons are required.\nSo, the total number of local comparisons is:\nN 2\nN 2\nTotal no. of local comparisons, L(N) = N 2 +\n+ . . . + 2c-2\n= 2(1 - (1/2)c-1)N 2\n= O(N 2)\n(1)\nRemote comparisons\nConsider the merging of two suffix arrays of length N/2 each at the topmost level of recursion. Here\nthere are no remote comparisons required, since the entire block of data is involved in merging, the order\nbetween any two suffixes can be determined by only local comparisons.\nNow consider the next level of recursion, which involves two merges of two suffix arrays, each of length\nN/4. Again, the merging of suffix array SA[N/2..(3N/4 - 1)] and SA[3N/4..N - 1] does not require any\nremote comparisons. The next merge between SA[0..(N/4-1)] and SA[N/4..(N/2-1)] may require scanning\nof SA[N/2..N], if the two suffixes turn out to be equal after the local comparisons. This follows from\nObservation 2. There are N/2 such suffix comparisons. Hence the total number of remote comparisons\nrequired is at most N/2 N/2.\nAt the next level, is one merge of length N/4 that requires scanning the sorted part of length N/2 and\ntwo merges of length N/4 that involve scanning sorted parts of length N/4 - again from Observation 2. The\ntotal number of remote comparisons is at most N/4 N. Proceeding in this manner, the total number of\nremote comparisons, R(N), can be found as follows.\n\nR(N)\n=\nN\nN\n+ 2 2\nN\n2N\n+ 4 2\nN\n3N\n+ 8 2\n+ . . .\nN\n((c - 2)N\n+2c-2\nN 2\n2N 2\n3N 2\n(c - 2)N 2\n=\n+\n+\n+ . . . +\n2c-1\nc - 2\n= N 2(1 +\n+\n+ . . . + 2c-1 )\n(2)\nc - 2\n2*R(N) = N 2(1 +\n+\n+ . . . + 2c-2 )\n(3)\nSubtracting Equation 2 from 3:\nc - 2\nR(N) = N 2(1 +\n+\n+ . . . + 2c-2 - 2c-1 )\n= N 2 (1 - (1/2)c-1) - c - 2\n2c-1\n= O(N 2)\n(4)\nFrom Equations 1 and 4, the running time of the divide and conquer algorithm is O(N 2).\n3.5\nMemory requirements\nThe memory efficiency of a suffix sorting algorithm is measured in terms of extra space required besides\nthe original character block and the suffix array. In the above discussion of the algorithm, we have assumed\nthe existence of two temporary arrays to hold the two sorted halves of the suffix array being merged into a\nsingle array. Hence we would need two temporary arrays of length N/2 each to merge suffixes. Similarly, we\nwould need two temporary arrays of length N/2 each for merging match lengths.\nHowever, this requirement can be reduced by a simple modification to the algorithm. At the beginning\nof the merge routine, only the left sorted half of SA needs to be stored in a temporary array. The right half\nof SA and the temporary array can then be merged into SA. Similarly, we would need only one temporary\narray of length N/2 for merging match lengths. Besides the temporary arrays, of course, we would need two\narrays of size N to store the merged suffixes and match lengths. All temporary arrays and the match length\narray, ML, are integer arrays. Therefore the total extra space required is 8N bytes, for a character data of\nlength N.\n3.6\nPerformance\nWe compared the performance of the divide and conquer algorithm with the qsufsort suffix sorting\nalgorithm developed by Larsson-Sadakane [2]. It uses the doubling technique first introduced by Manber-\nMayers in [6]. It has a worst case time complexity of O(Nlog(N)) and with a good implementation, it is\nconsidered as the fastest algorithm for generic suffix sorting.\n\n1.17\n2.52\n5.54\n21.96\n48.73 101.32\n0.1\nd&c\nqsufsort\nRatio\n0.41\n0.84\n1.82\n5.02\n18.01\n42.79\n2.854\n3.044\n4.374\n2.706\n2.368\n256K\n512K\n1M\n2M\n4M\n8M\nTime (sec.)\nBlocksize, N (=Datasize)\nFigure 2: Perfomance of divide and conquer algorithm as compared to the qsufsort algorithm on a Sun UltraSPARC\nIII cluster. This experiement has been perfomrmed on Human Genome dataset files.\n0.1\nTime (sec.)\nd&c\nqsufsort\nRatio\n0.25\n0.541 1.222 2.854 6.439 14.39\n0.241 0.531 1.232 2.724 6.009 13.17\n1.037 1.019 0.992 1.048 1.072 1.093\n256K\n512K\n1M\n2M\n4M\n8M\nBlocksize,N (=Datasize)\nFigure 3: Perfomance of divide and conquer algorithm as compared to the qsufsort algorithm on a Pentium4 2.4Ghz\nmachine. This experiment has been performed on Human Genome dataset files.\n\nFigure 2 shows the performance of the two algorithms for varying block size, N , on Human Genome\ndataset on a Sun UltraSPARC III cluster. Even though the qsufsort algorithm outperforms the divide and\nconquer algorithm, the time taken by the divide and conquer version exceeds the time taken by qsufsort by\na factor of 2-5 for sufficiently large blocksize.\nFigure 3 shows the result of a similar experiment on a Pentium4 2.4GHz machine. On this machine, the\ndivide and conquer algorithm performs almost as well as the qsufsort algorithm.\nBesides the running time, we also analyzed the cache performance of the two algorithms using a cache\nsimulator. The experiment was performed on two different data sets, reuters corpus and protein sequence\ndataset, using a fixed block size of N = 1, 048, 576. Tables 3 and 4, summarize the cache behavior of the\ntwo algorithms. An L1 data cache of size 16KB was used in this experiments. It can be seen that even\nthough the divide and conquer algorithm makes many more data references than the qsufsort algorithm, it\nstill incurs lesser cache misses.\na\nTable 3: Cache Performance: Reuters dataset\nqsufsort\n# data references\n525,305K\nL1 data cache misses\n14,614K\nCache miss ratio\n2.7%\nd&c\n1,910,425K\n13,707K\n0.7%\naInput file size: 1MB, Blocksize, N=1MB\na\nTable 4: Cache Performance: Protein sequence dataset\nqsufsort\n# data references\n531,201K\nL1 data cache misses\n16,323K\nCache miss ratio\n3.0%\naInput file size: 890KB, Blocksize, N=1MB\n3.7\nDiscussion\nd&c\n2,626,356K\n12,945K\n0.4%\nFrom the performance results it is clear that the qsufsort algorithm by Larsson-Sadakane is still faster\nfor generic suffix sorting. This is expected considering the asymptotic running times of the two algorithms.\nHowever, based on the results on different data sets, one can say that the divide and conquer algorithm has\na good average case running time. Most importantly, it shows a good cache behavior. Secondly, it does\nnot require any memory parameters to be set, besides the blocksize, N , which is usually determined by the\ncompression algorithm using the BWT. For these reasons, the algorithm is likely to be scalable and is likely\nto perform well across various memory hierarchies.\n\nLinear time construction of suffix arrays\n4.1\nMotivation\nA linear time algorithm to sort all suffixes of a string over a large alphabet of integers is presented in [3].\nThis could potentially be a major break-through in the field of suffix sorting which has witnessed several\nimprovisations to the O(n lg n) algorithm in terms of improving the constants, and coming up with better\nimplementations. The work done in [3] proves the linear bound on the proposed algorithm. With the\nknowledge gained from previous suffix-array implementations, we improvised on the linear time algorithm\nand came up with an efficient implementation. We examine the performance of our implementation, and\nanalyze the trade-offs involved.\n4.2\nDescription of the algorithm\nConsider a string T = t1t2 . . . tn over the alphabet = 1 . . . n. We denote the last character of T by $,\nassume it to be the lexicographically smallest character. Let Ti = titi+1 . . . tn denote the suffix of T starting\nwith ti. For strings and , we use ≤ to denote that is lexicographically smaller than . We classify\nthe suffixes into two types: Suffix Ti is of type S if Ti ≤ Ti+1, and is of type L if Ti+1 ≤ Ti. Since, the last\nsuffix does not have a successor, we can classify it as both type S and type L.\nLemma 3 All suffixes of T can be classified as either type S or type L in O(n) time.\nProof\nConsider a suffix Ti(i < n)\nCase 1 If ti = ti+1, we can determine if Ti is of type S or L by only comparing ti and ti+1.\n≥\nCase 2 If ti = ti+1, we find the smallest j > i such that tj = ti.\n≥\nif tj > ti, then suffixes Ti, Ti+1, . . . , Tj-1 are of type S.\nif tj < ti, then suffixes Ti, Ti+1, . . . , Tj-1 are of type L.\nTherefore, all suffixes can be classified using a left to right scan of T in O(n) time.\nAnother important property of type S and type L suffixes is, if a type S suffix and a type L suffix both begin\nwith the same character, the type S suffix is lexicographically greater than the type L suffix. Therefore,\namong all suffixes that start with the same character the type S suffixes appear after the type L suffixes.\nLet A be the suffix array, and B be the array of suffixes of type S, sorted in lexicographic order. The final\nsorted order is obtained from B as follows:\n1. Bucket all suffixes of T according to their first character in array A. Each bucket consists of all suffixes\nthat start with the same character. This step takes O(n) time.\n2. Scan B from right to left. For each suffix encountered in the scan, move the suffix to the current end\nof its bucket in A, and advance the current end of the bucket by one position to the left. After the\nscan of B is completed, all type S positions are in their correct positions in A. The time taken for this\nstep is O( B ), which is bounded by O(n).\n|\n|\n3. Scan A from left to right. For each entry A[i], if TA[i]-1 is a type L suffix, move it to the current front\nof its bucket in A, and advance the front of the bucket by one. This takes O(n) time. At the end of\nthis step, A contains all suffixes of T in sorted order.\nThe remaining task is to sort all type S suffixes in T. The substring ti . . . tj is called a type S substring\nif both i and j are type S positions, and every position between i and j is a type L position. For each suffix\nTi, define its S - distance to be the distance from its starting position i to the nearest type S position to its\nleft. We first sort all the type S substrings in O(n) time. This is done by a single traversal of S - distance\nlists. Sorting of the type S substrings generates buckets where all substrings are identical. A final recursive\nsorting of these buckets yields the final lexicographic ordering of the type S suffixes.\n\n9 10\n9 10\n4.3\nAn example\nFigure 4 illustrates obtaining the final sorted order of suffixes.\nT\nM\n\nI\nS\nS\nI\nS\nS\nI\nP\nP\nI\n$\nType\n\nL\nS\nL\nL\nS\nL\nL\nS\nL\nL\nL\nL\nPos\n\nSuffix array: A\n(sorted acc to\nfirst character)\nMove to front of\nBucket\naccording to B\nOrder of Type S\nsuffixes: B\nScan L to R- Move type L suffixes to front of bucket\n9 10\n9 10\nFinal Sorted Order\nFigure 4: An example of how to obtain the final sorted ordering of the suffixes\n4.4\nAn implementation strategy\nIn order to sort the type S suffixes in T, we have to sort all type S substrings. This sorting generates buckets\nwhere all the substrings in a bucket are identical. We generate a new string T 0 by scanning T from left to\nright and for each type S position in T writing the bucket number of the type S substring from that position\nto T 0 . Then we sort T 0 recursively. The ordering of T 0 corresponds to the sorted order of the type S suffixes.\nHowever, if a bucket contains only one type S substring, the position of the corresponding type S suffix in\nthe sorted order is already known. Let T 0 = b1b2 . . . bm. Consider the maximal substring bi . . . bj (j < m)\nsuch that each bk(i k j) contains only one type S substring. We can then shorten T 0 by replacing each\nsuch maximal substring bi . . . bj with its first character bi. Since j < m , the bucket corresponding to '$' is\nnever dropped. We can compute the shortened version of T 0 without having to compute T 0 first and then\nshorten it. This method has the effect of eliminating some of the suffixes of T 0 . It also modifies each suffix\nthat contains a substring that is shortened.\n\n4.5\nPerformance\nThe tests were performed on Sunfire (sf3.comp.nus.edu.sg).\n0.01\n0.1\nTime(in sec.)\nLinear Algo\nqsufsort\n0.168\n0.091\n64KB\n0.338\n0.192\n128KB\nBlock Size\n0.676\n1.387\n0.404\n0.876\n256KB\n512KB\n2.939\n1.914\n1MB\n6.766\n5.072\n2MB\nFigure 5: Performance of Linear time algorithm as compared to qsufsort\nIt can be seen from Figure 5 that as expected, the time taken by the linear algorithm is linearly pro\nportional to the block size. It performs as well as the fastest available suffix sorting algorithm available :\nqsufsort, for larger block sizes.Our implementation is still crude as compared to the refined implementations\nof the O(n log n) algorithms like qsufsort. Some of the trade-offs are discussed below.\nTrade-offs\nThe current implementation suffers from some memory-management overhead. We presently use 3 integer\narrays of size n, and 3 boolean arrays( 2 of size n, and 1 of size n/2). This gives rise to a total space of 12n\nbytes plus 2.5n bits. This is greater than the 8n bytes used by Manber and Myers' suffix sorting algorithm [6]\nwhich takes O(n log n) time. Hence, we find that a trade-off between space and time exists. However, with\nmore efficient implementations, the advantages in terms of linearity of time would outweigh problems caused\ndue to the extra space requirements. In fact we found that certain optimizations which included storing the\nreverse correspondences between buckets and suffixes led to a marked improvement in performance.\nConclusions and Future Work\nIncreasing the Block size in the Burrows-Wheeler transform provides better compression. But, the cost\nincurred for the sorting of the rotated strings proves to be the bottleneck in achieving better compression\nratios in a shorter time. We suggest techniques for fast suffix sorting that would ease this bottleneck.\nWe implemented a cache-oblivious Distribution sort based suffix sorting. We found that it incurs memory\nmanagement overheads, and performs a factor of 1.76 to 3.76 slower than the Quick sort based approach, and\na factor of 1.6 to 3.45 slower than the Merge sort based approach. However, we would expect our approach\nto be more effective when the Block sizes are increased to such an extent that the paging effects become\nmore significant. We proceeded to develop an O(N 2) divide and conquer algorithm that is cache-efficient\nand requires no memory parameters to be set. Even though it is found to be slower than the asymptotically\nsuperior algorithms such as qsufsort by a factor of upto 4.3 for sufficiently large block sizes, it shows a much\n\nbetter cache behaivor. We then present issues dealing with the linear time suffix sorting algorithm. We\nexamine the tradeoffs between space and time. The linear time algorithm performed as expected, and was\ncomparable to the fastest suffix sorting available- qsufsort.\nSeveral issues still remain to be investigated. The divide and conquer algorithm that we have introduced\nessentially processes the block of data from right to left. This sequential nature of the algorithm will not\nallow it to be easily parallelizable. A variant of the algorithm should be developed that offers parallelism, and\nyet maintains the good cache behavior of the original algorithm. We have tried to make our implementation\nof the linear time algorithm cache-efficient. Nevertheless the implementation is still crude and needs to be\nrefined. A more thorough analysis of the experimental data is needed in order to explain the experimental\nresults satisfactorily. Experiments should further be performed to analyze the effects of arbitrary block sizes.\nNew structures like the suffix cactus [7] offer an interesting point of view to the family of suffix structures.\nEmploying these structures to facilitate fast suffix sorting is another area which can be looked into.\nReferences\n[1] M. Burrows and D. J. Wheeler, \"A block-sorting lossless data compression algorithm.,\" Tech. Rep. 124,\nDigital Systems Research Center, Palo Alto, 1994.\n[2] N. J. Larsson and K. Sadakane, \"Faster suffix sorting,\" 1999.\n[3] P. Ko and S. Aluru, \"Space efficient linear time construction of suffix arrays,\" 2003.\n[4] J. K\nainen and P. Sanders, \"Simple linear work suffix array construction,\" in Proc. 13th International\narkk\nConference on Automata, Languages and Programming, Springer, 2003.\n[5] M. Frigo, C. E. Leiserson, H. Prokop, and S. Ramachandran, \"Cache-oblivious algorithms,\" in 40th\nAnnual Symposium on Foundations of Computer Science, FOCS '99, 1999.\n[6] U.Manber and G.Myers, \"Suffix arrays: a new method for on-line search,\" in SIAM Journal on Comput\ning, pp. 22:935-48, 1993.\n[7] J. Karkkainen, \"Suffix cactus: A cross between suffix tree and suffix array,\" in Combinatorial Pattern\nMatching, pp. 191-204, 1995."
    },
    {
      "category": "Resource",
      "title": "agrawal_sen.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/421b949717f1dd7cc3fe934855781904_agrawal_sen.pdf",
      "content": "6.895 Theory of Parallel Systems\nKunal Agrawal, Siddhartha Sen\nAdaptively Parallel Processor Allocation for Cilk Jobs\nFinal Report\nAbstract\nAn adaptively parallel job is one in which the number of processors that can be used without waste\nvaries during execution. This paper presents the design and implementation of a dynamic processor-\nallocation system for adaptively parallel jobs. We focus on the specific case of parallel jobs that are\nscheduled with the randomized work-stealing algorithm, as is used in the Cilk multithreaded language.\nThe jobs are run on a shared-memory symmetric multiprocessor (SMP) system.\nWe begin by presenting a simple heuristic for estimating the processor desire of a Cilk job, based\non the number of incomplete Cilk procedure frames queued on each of the job's processors. Then, we\nshow how these estimates can be used to allocate processors to multiple running Cilk jobs in a fair and\nefficient way.\nWe present empirical evidence verifying the correctness of our dynamic processor-allocation system\nand evaluating its overhead.\n\nIntroduction\nThe problem of allocating processor resources fairly and efficiently to parallel jobs has been studied exten\nsively in the past. Most of this work, however, assumes that the instantaneous parallelism of the jobs is\nknown and used by the job scheduler to make its decisions. In practice, this information is not easily dis\ncernible, which is why many systems resort to static algorithms when deciding how to allocate processors to\njobs. The current release of Cilk (Cilk-5.3.2), for example, expects the user to manually enter the parallelism\nof his job in the command line itself.\nIn this paper, we present a simple algorithm for estimating the instantaneous parallelism of a job, and\nshow how this information can be used to dynamically allocate processors to jobs in a fair and efficient\nmanner.\nWe define an adaptively parallel job to be a job for which the number of processors that can be used\nwithout waste--representing the instantaneous parallelism of the job--varies during execution. Our goal\nis to design and implement a dynamic processor-allocation system for adaptively parallel jobs. We call\nthe problem of allocating processors to adaptively parallel jobs the adaptively parallel processor-allocation\nproblem [6].\nThis paper investigates the adaptively parallel processor-allocation problem for the specific case of parallel\njobs that are scheduled with the randomized work-stealing algorithm, as is used in the Cilk multithreaded\nlanguage [5, 1, 4]. We present the design of a processor allocation system that allocates processors fairly\nand efficiently to multiple Cilk jobs running on a shared-memory symmetric multiprocessor (SMP) system.\nWe define the terms \"fair\" and \"efficient\", and other relevant terms, in Section 2 below. The design of our\nsystem can be divided into three primary tasks:\n1. Dynamically estimate the number of processors desired by each Cilk job on the system; this is our\nmeasure of the instantaneous parallelism of each job.\n2. Dynamically determine the number of processors that should be allocated to each job such that the\nresulting allocation is fair and efficient.\n3. Dynamically add or remove processors from a job if the number of processors granted in task 2 is less\nthan or greater than the current allotment, respectively.\nThe notion of \"dynamic\" here varies from task to task; an important part of our design is to determine\nwhen and how frequently the above tasks should be performed. We answer these questions in later sections.\nThe remainder of this paper is organized as follows. In Section 2, we define the relevant terms of the\nadaptively parallel processor-allocation problem and state our assumptions. Section 3 gives an overview of\nsome of the relevant previous work done on the problem, and Section 4 gives a brief overview of the Cilk\nruntime system. In Section 5, we present the design of our dynamic processor-allocation system, based on\nthe three tasks listed above. In Sections 6 and 7, we describe the prototype implementation of our system\nand summarize the experimental results that we obtained. We present possible extensions to the prototype\nin Section 8, and we conclude in Section 9.\nDefinitions and Assumptions\nIn this section, we define the relevant terms of the adaptively parallel processor-allocation problem and state\nour assumptions. Section 4 defines the terminology that is specific to Cilk jobs.\nFairness and Efficiency\nWe consider a shared-memory SMP system with P processors and J jobs. In general, we follow the termi\nnology and conventions used in [6], but for a shared-memory system instead of a distributed one. At any\ngiven time, each job has a desire dj , representing the maximum number of efficiently usable processors, and\n\nan allotment aj , representing the number of processors allocated to it. If aj < dj , we say that the job is\ndeprived, since it has a deprived allotment; if aj = dj , we say that the job is satisfied, since it's desire has\nbeen met. We introduce an additional term, pj , to represent the number of processors currently being used\nby the job; ideally, pj = aj , but this statement may not always be true, as explained in our assumptions\nlater on.\nOur problem, as stated in Section Section 1, is to find a fair and efficient allocation of processors among\nall J jobs in the system. We define the terms \"fair\" and \"efficient\" identically to [6]. An allocation is\nsaid to be fair if whenever a job receives fewer processors than it desires, then no other job receives more\nthan one more processor than this job received (the allowance of one processor is due to integer roundoff).\nMathematically, an allocation is fair if the following condition holds:\n1. 9j ∃{1, 2, . . . , J} such that aj < dj =) 8i ∃{1, 2, . . . , J}, mi ⇒aj + 1.\nAn allocation is efficient if no job receives more processors than it desires, and the maximum number of\nprocessors that can be used are being used. Mathematically, this definition translates to the following two\nconditions:\n1. 8j ∃{1, 2, . . . , J}, aj ⇒dj .\n2. 9j ∃{1, 2, . . . , J} such that aj < dj =) P\nj\nJ\n=1 aj = P.\nUnlike the definition of efficiency presented in [6], we do not allow the allotment of a job to exceed its\ndesire, even if there are unused processors in the system. To see why condition 2 is necessary, consider an\nallocation where each of the J jobs receives 0 processors. In this scenario, no job has more processors than\nit desires--so condition 1 is satisfied--but none of the jobs can make any progress and all P processors are\ngoing to waste. Condition 2 prevents situations like this by ensuring that we always use as many processors\nas possible.\nAn example of a fair and efficient allocation, according to the above definitions, is shown in Figure 1.\nFrom the example, we observe that the deprived job with the smallest allotment restricts the maximum\nnumber of processors any other job can have--since, by the fairness condition, no job can have more than\none more processor than this job.\nAssumptions\nWe make several assumptions in our solution to the adaptively parallel processor-allocation problem. These\nassumptions are summarized in the list below:\n1. All jobs on the system are Cilk jobs.\n2. Jobs can enter and leave the system and change their parallelism during execution.\n3. All jobs are mutually trusting, in that they will stay within the bounds of their allotments and com\nmunicate their desires honestly.\n4. Each job has at least one processor.\n5. Jobs have some amount of time to reach their allotments.\nAssumption 5 is a consequence of task 3 from Section 1; in particular, since there is necessarily some\noverhead associated with adding or removing processors from a job, it may take some time before a job's\nprocessor usage (pj ) actually reaches its granted allotment (aj ). This delay is illustrated in Figure 2; it is\nalso measured in one of the experiments performed in Section 7.\n\nJob 1\nJob 2\nJob 3\nJob 5\nJob 4\nJob 6\nFigure 1: An example of a fair and efficient allocation. The small circles represent the desires of each job (shaded\ncircles represent allocated processors; empty circles represent desired processors that have not been allocated). The\ndeprived job with the smallest allotment, Job 1, restricts the maximum number of processors any other job can have\n(6 processors). Since Job 6 has a deprived allotment that is less than Job 1's, it cannot be a part of this allocation\nwithout violating the fairness condition.\nTim e\nProcessors\nFigure 2: The processor desire (shown in blue), allotment (shown in black), and usage (shown in red) of an imaginary\nadaptively parallel job. The delayed response of the current usage (red line) to changes in the job's allotment (black\nline) are caused by the overhead of adding or removing processors during execution.\nPrevious Work\nThe literature on the adaptively parallel processor-allocation problem is quite extensive. We limit our\ndiscussion of previous work to the results that are pertinent to our definition of the problem, and which we\ndraw from in designing our system.\nIn [6], Song presents a randomized processor-allocation algorithm, called the SRLBA algorithm, for the\nadaptively parallel procesor-allocation problem in a distributed system of P processors and J jobs. The\nSRLBA algorithm is a variant of the Randomized Load-Balancing (RLB) algorithm that consists of rounds\nof load-balancing steps in which processor migration (from one running job to another) may occur. A\nround consists of P sequential load-balancing steps, in which each processor is given a chance to initiate\na load-balancing step. SRLBA differs from the RLB algorithm in two ways: 1) SRLBA operates in the\n\nsequential perfect-information model, which assumes that all load-balancing steps occur serially, and that\nall job allotments are updated promptly after each step; and 2) SRLBA introduces an adversary into the\nsystem who, at the beginning of each step i in a given round, having seen the previous i - 1 steps, chooses a\nprocessor not previously chosen in the round to initiate the ith load-balancing step [6]. The main result of\n[6] states that if all J jobs have a desire greater than the absolute average allotment P/J of processors, then\nwithin O(lg P ) rounds of the SRLBA algorithm, the system is in an almost fair and efficient allocation with\nhigh probability, meaning that every job has within 1 of P/J processors with high probability.\nBlumofe, Leiserson, and Song develop a processor-allocation heuristic in [3] that uses the \"steal rate\" of\nadaptively parallel jobs to estimate their desires during runtime. The jobs considered are those whose threads\nare scheduled with the randomized work-stealing algorithm, as is used in Cilk. Blumofe et. al. implement\ntheir processor-allocation heuristic in a job scheduler for Cilk, called the Cilk Macroscheduler; the scheduler\nuses the steal rates of multiple, concurrent Cilk jobs to achieve a fair and efficient allocation among the jobs.\nThe Macroscheduler is implemented in user space: it consists of a processor manager for each Cilk job as\nwell as a job registry (in shared memory) through which the processor managers can communicate. The job\nregistry contains the processor allotments and desires of every Cilk job; since each processor manager can\nview this information, it can easily calculate the number of processors its corresponding job should possess\nin a fair and efficient allocation, and adjust the processor allotment accordingly. The performance analysis\nof the Macroscheduler is incomplete; the only result provided in [3] is an empirical analysis of the scheduler's\noverhead, in which various Cilk programs are run on a multiprocessor machine two times, the first time with\nthe original Cilk runtime system and the second time with the Macroscheduler. Each program run occurs\nin isolation--that is, the program is the only job running on the machine.\nAlthough the work in this paper is largely influenced by [6] and [3] above, it is different in several key\nways. Unlike the SRLBA algorithm, we are solving the adaptively parallel processor-allocation problem for\na shared-memory multiprocessor system. Consequently, we eliminate the need for rounds of load-balancing\nsteps between pairs of processors, since the jobs in our system have complete knowledge of the processor\nallotments and desires of all other jobs (via shared memory). In this respect, our processor allocation system\nfor Cilk is similar to the Macroscheduler presented in [3]. Unlike the Macroscheduler, however, we use a\ncompletely different heuristic for estimating the processor desires of Cilk jobs. We argue that this heuristic\nis a more direct and timely measure of a job's parallelism, allowing the system to respond more quickly and\neffectively to changes in this parallelism.\nSections 5 and 6 explain the differences highlighted above in more detail.\nThe Cilk Runtime System\nWe provide a brief overview of the Cilk language and runtime system, and we define the terminology used\nin subsequent sections to describe Cilk jobs.\nCilk is a language for multithreaded parallel programming based on ANSI C that is very effective for\nexploiting highly asynchronous parallelism. Cilk differs from other multithreaded programming systems by\nbeing algorithmic in nature--that is, it employs a scheduler in its runtime system that guarantees provably\nefficient and predictable program execution performance [5]. The current release of Cilk is designed to run\nefficiently on shared-memory SMPs.\nWhen a Cilk program is run, the user has the option of specifying the maximum number of \"processors\"\nor workers that the Cilk job may attempt to use (if no number is specified, a default value is used). The\nruntime system then creates this many workers (implemented using operating system threads or processes)\nand schedules the user computation on the workers using a randomized, work-stealing algorithm [2, 4].\nThe work-stealing algorithm is simple: during program execution, if a worker runs out of work to do, it\nchooses another worker at random (called the victim) and tries to steal work from the victim. If the steal\nis successful, the worker begins working on the stolen piece of work; otherwise, it picks another worker\n(uniformly at random) to be the victim and continues work-stealing until the steal is successful. Cilk relies\non the operating system to schedule the workers onto the physical processors of the SMP.\n\nIn the remainder of this paper, the term \"processor\" is used analogously with \"worker\", and is distin\nguished from the term \"physical processor\", which refers to an actual processor on the SMP. As we mentioned\nabove, when a Cilk program is run, the user is expected to tell the runtime system how many processors to\nrun the job on. In other words, the user must provide a static (one-time) estimate of the job's parallelism,\nor else a default number of processors is used. As we discussed in Section 1, it is difficult to measure the\ninstantaneous parallelism of a job, let alone provide a one-time estimate for it, since a job's parallelism can\nvary significantly during its execution. Thus, while a static allocation scheme might be simpler to implement\nin the Cilk runtime system, it does not guarantee efficient use of the available processors. This inadequacy\nof Cilk is the primary motivation behind the processor allocator system presented in this paper.\nTerminology for Cilk Jobs\nA Cilk program execution consists of a collection of procedures, each of which is broken into a sequence of\nnonblocking threads [5]. Each processor of a Cilk job maintains a deque, or doubly-ended queue, of frames to\nkeep track of the current scheduling state. In Cilk terminology, a frame is a data structure that can hold the\nstate of a procedure--including, among other things, the thread of the procedure that is ready to execute\nnext. Within a given processor, execution always occurs at the bottom of the frame deque: procedure\nstate is saved by pushing a frame onto the bottom of the deque, and procedures are executed by popping\nthe bottommost frame on the deque. When a processor work-steals, it takes the topmost frame (the least\nrecently pushed frame) from the victim's deque and places it in its own deque [5].\nAs we show in Section 5, the size and contents of the deques of a Cilk job provide direct information\non the amount of pending work the job has accumulated so far; this information can help us estimate the\nnumber of processors that the job can most efficiently use.\nDesign Overview\nAt any instant, there may be many independent jobs in the system. Each of these jobs operates according to\nthe assumptions in Section 2. There is a global allocator which has all the information about the desires and\nallotments of each job and it is the responsibility of this allocator to keep the allocation fair and efficient.\nAs mentioned in Section 1, there are three major tasks in the processor allocation system, shown in more\ndetail steps in Figure 3. The job calculates its desire and communicates it to the allocator (steps 1 and 2).\nThis change might have made the previous allocation unfair or inefficient. If that is the case, the allocator\nrecalculates the allotments of jobs according to the change, so as to make the allocation fair and efficient\nagain (step 3). The job can then enquire about its current allotment (step 4) and adjust the number of\nprocessors it is using to be equal to its current allocation (step 5).\nThe main tasks of the system are described in detail in the following sections.\nEstimating the Job Desire\nAs we mentioned above, it is the responsibility of the jobs to calculate their desires and communicate them\naccurately to the allocator. In this prototype, we only consider Cilk jobs that employ the ramdomized\nwork-stealing algorithm. Thus, we can use some heuristics based on the properties of the jobs to calculate\ntheir desires. [3] uses a heuristic based on steal rate; the idea here is that if a job has too many processors\nthat are stealing (i.e. it has a high steal rate), then most of the processors dont have sufficient work to do,\nand so the desire of the job is low. On the other hand, if a job has a significantly low number of steals, then\nmost of the processors must have work to do, so the desire of the job is high. In [3], the processor manager\nof a job keeps track of the number of steals, and if this number is less than a certain value, the desire of the\njob is doubled; if the number of steals is greater, the job's desire is reduced.\nIn this paper, we use another heuristic based on the number of frames in the ready deques of a job's\nprocessors. The number of frames can be used as an estimate of the job desire because it represents the\nnumber of threads that are ready to execute, or the work that could be done now if the job had an infinite\n\n...\n...\nProcessor\nAllocator\nJob 1\nJob N\n1. Estimate desire\n5. Adjust allotment\n(add/remove\nprocessors)\n3. Recalculate allocation\n2. Report\ncurrent desire\n4. Get allotment\nFigure 3: System overview: the five steps of the processor allocation system.\nnumber of processors. We decided to use this approach instead of the steal rate idea because it is a more\nglobally-aware heuristic. If one processor of a job has a lot of work to do, other processors might be stealing\nand increasing the steal rate, when in fact the job actually has a lot of ready work. Also, if the desire of a job\nis low, and we wait until there are a lot of steals, we waste a lot of time during which we could have already\ndecreased the allocation. Since steal attempts and steals are expensive operations, and do not contribute at\nall to getting the work done, it is better to avoid them if possible. In our system, we designate a background\nprocessor to periodically add up the frames in all the processor deques and report the desire as a function\nof this sum. For the purpose of our prototype, we simply divide the total number of frames by a constant\ninteger, as shown in Figure 4. This constant is a tunable parameter; we are not yet sure if there is a single\nvalue of k that is best for all situations.\n+\n+\n+\nH\nH\nH\nH\nT\nT\nT\nT\nk\nFigure 4: Our algorithm for measuring a job's desire: the number of frames on the deques of the processors are\nadded together and divided by a tunable constant (k). There are 4 processors in this example.\nCalculating a Fair and Efficient Allocation\nThe allocator which calculates the fair and efficient allocation is a global allocator that knows the desires\nand allotments of all the jobs in the system; these values are stored by the allocator in a table. The jobs\nmay update their desires at any time, and the allocator changes the allotments so that the allocation is\n\nalways fair and efficient. We implement an equipartitioning algorithm, similar to the one described in [6].\nIn the beginning, one processor is allocated to each job. Any jobs that reach their desires drop out. The\nscheduler repeats this process with the remaining jobs until either there are no remaining jobs or all the\nprocessors have been allocated. An allocation produced by equipartitioning is fair and efficient. However, we\ndo not want to run the entire algorithm every time any job changes its desire. Thus, we use a subset of the\nalgorithm which has the same effect and correctness, but completes faster than running the entire algorithm\nevery time.\nTo explain our modified equipartitioning algorithm, we define a few terms:\n- The maximum allotment of any job in the system at the current time is maxAlloc; maxAlloc =\nmax(ai).\n- The minimum allotment of any deprived job in the system at the current time is the fairShare.\nf airShare = min(ai), where i is a deprived job.\nAs explained in Section 2, the deprived job with the smallest allotment restricts the maximum allotment\nto be at most one more than this allotment. Thus, f airShare ⇒ maxAlloc ⇒ f airShare + 1. For simplicity,\nwhen there are no deprived jobs we define f airShare = maxAlloc.\nWhenever a job j changes its desire, the allocator might have to change the allotments of j and some\nother jobs to maintain the fairness and efficiency of the allocation. The following code describes the actions\n0 .\nj\nA new job entering the system is a special\nof the allocator when job j changes its desire from dj to d\ncase of increasing an existing job's desire (from 0 to a positive integer). Similarly, when a job completes its\nexecution, it decreases its desire to 0.\n0 )\nj\nChangeDesire(dj , d\n0 )\nj\nif (dj = d\nthen return\nj ∀ dj )\nif (d\n0 )\nj\nthen aj = min(aj + f reeP rocs, d\nif (aj ∀ f airShare)\nthen\nreturn\n0 )\nj\nelse\nIncreaseAllocation(j, aj , d\nd )\n⇒ j\nj\nif (d\n0 )\nj\nthen if (aj ⇒ d\nthen\nreturn\nj\nelse f ree = aj - d\nj\naj = d\nReallocate(f ree)\nIncreaseAllocation(j, aj , dj )\n0 )and(\nf airShare))\na ⇒\nj\nj\nwhile ((aj ⇒ d\nfind a job i where (ai = maxAlloc)\nDecrement ai\nIncrement aj\nReallocate(f ree)\nwhile 9i such that ai ⇒ di\nfind a job i where (ai = f airShare)and(ai ⇒ di)\nDecrement f ree\nIncrement ai\nf reeP rocs = f reeP rocs + f ree\n\nThe above code shows how the allocator behaves when job j changes its desire from dj to d0 .\nj There are\nthree possible cases:\n1. Desire of the job j increases: If there are free processors available, then the allocator gives the\nfree processors to the job until either the job has as many processors as it desires or we run out\nof free processors. But if there aren't enough free processors, then we might have to reallocate the\nprocessors--i.e, reduce the allotment of other processors to increase this job's allotment. There are\ntwo cases:\n(a) Case 1: The allocation of the job j is more than or equal to the f airShare. In this case, the job\nalready has its fair share of processors and thus the allocation remains unchanged.\n(b) Case 2: The allotment of the job j is less than f airShare. In this case, the job doesn't have its\nfair share. Thus, we take processors from jobs which have more than their fair share and give\nthem to this job.\n2. Desire of job j decreases: There are again two cases:\n(a) Case 1: If the desire of j is still more than its current allocation, then there is no need to perform\na reallocation since j can still efficiently use the processors that have been allocated to it.\n(b) Case 2: If the desire of j is now less than its current allocation, then we have to reduce the\nallotment of j and increase the allotment of other deprived jobs. Thus, we find deprived jobs that\nhave an allotment equal to the f airShare and increase their allotment by 1. We go on until we\nhave run out of free processors or all jobs have allotments equal to their desires.\nTheorem 1 The allocation is always fair and efficient.\nProof\nThe proof is by induction.\nBase Case: There are no jobs in the system to begin with. Thus, f reeP rocs = total number of\nprocessors. When the first job, J ob1, enters the system, if d1 ⇒ f reeP rocs then a1 = d1; otherwise,\nd1 = totalnumberof processors. This allocation is obviously fair and efficient.\nInductive case: Suppose the allocation is fair and efficient at time t. At this time, job j changes its\ndesire from dj to d0 .\nj We consider what the above algorithm does in all cases and prove that the resulting\nallocation is also be fair and efficient. As stated earlier, the entrance of a new job into the system can just\nbe treated as an increase of the desire of that job from 0 to d0 .\nj Similarly, when a job finishes its execution,\nit can be considered to reduce its desire to 0.\n1. d0\nj ∀ dj : The following three cases are possible\n(a) According to the above algorithm, if there are any free processors, then these free processors are\ngiven to job j. We assumed that the allotment was fair and efficient at time t. Hence, if there\nwere free processors, none of the jobs were deprived at time t, due to the efficiency condition in\nSection 2. According to the code above, the free processors will be given to job j until\ni. aj = d0\nj : That is, when the allotment of j is equal to its desire. In this case, since we have\nestablished that all other jobs already have allotments equal to their desires, the allocation is\nfair and efficient.\nii. f reeP rocs = 0 and aj ⇒ d0 : We have allocated all the processors and j is still deprived. In\nj\nthis case, we fall into the cases below, when there were no free processors to begin with.\n(b) There are no free processors and aj ∀ f airShare: The allocation was efficient at time t. Thus,\nno job had more processors than it desired, and we haven't given any more processors to any\nother job. Also, for job j, the allotment is greater than or equal to the fair share. As a result, all\nother jobs have at most one more processor than j. At this point, the fairness criteria has been\nsatisfied, and the allocation is already fair and efficient (no reallocation is necessary).\n\n(c) There are no free processors and aj ⇒ f airShare: Now, job j doesn't have a fair share of processors\nand we need to reduce the allotment of other jobs to increase the allotment of j. This is done\nin the procedure IncreaseAllocation above. Here, we find a job i where ai = maxAlloc and\nreduce the allotment of i by 1. Now if maxAlloc = f airShare + 1, then new ai = f airShare.\nThus i still has its fair share. If maxAlloc = f airShare, then the fairShare will reduce by 1 and\ni will be the job with the smallest deprived allotment, so no other job can have more than a1 + 1\nprocessors. Thus, the allocation is still fair and efficient.\n2. d0\nd : When the desire of a job decreases, we might have to take processors from job j and allocate\n⇒ j\nj\nthem to other jobs.\n(a) aj ⇒ d0\nj : When the allotment of j is still less than the new desire, we do nothing. Since the old\nallocation was fair, the new one remains fair too. Also since job j can still use its processors\nefficiently, the allocation remains efficient too.\n(b) aj ∀ d0 : In this case, the allotment of j is now greater than its new desire, and we are violating the\nj\nefficiency rule. Thus, we have to reduce the allotment of this job. Since d0\nf airShare + 1,\n⇒ a ⇒\nj\nj\ndj ⇒ f airShare. So now aj = d0 , and we free the remaining processors. These processors have\nj\nto be distributed to other jobs which are deprived. Since none of the deprived jobs can have\nallotments less than f airShare, we give these processors one by one to the deprived jobs and\nremain fair and efficient.\nAdjusting a Job's Allotment\nWe discussed above that the job periodically calculates its desire and communicates it to the allocator,\nwhich adjusts the allotments accordingly. If the allotment of a job changes, it has to increase or decrease\nthe number of processors it is currently using to be the same as its allotment. That is, the job might have\nto increase or decrease its pi.\n1. Increase pi: If the allotment of a job increases, the job increases its pi by starting to use more\nprocessors. We already know that the jobs report their desires periodically to the allocator. The\nallotments change immediately after this. At this point, the job gets its new allotment and if the\nnumber of processors it is using is less than its allotment, that is pi ⇒ ai, then it starts using more\nprocessors and makes pi = ai\n2. Decrease pi: If the allotment increases, the job has to decrease its pi by reducing the number of\nprocessors in use. This can again be done when the desires are reported and the job gets new allotment.\nBut this is complicated since all the processors might have work to do and the processors would have\nto move work from one processor to another. Thus, for ease of implementation, we decided not to do\nthis in the prototype. Instead, every time a processor runs out of work and tries to steal, it checks the\nallotment. If the allotment is less than the number of processors being used, that is pi ∀ ai, then this\nprocessor stops working for this job. At this point, this processor did not have any unfinished work\n(since it was stealing). Thus, this strategy is easy to implement. The disadvantage of the strategy is\nthat pi might remain more than ai for considerable period of time depending on how often the job i\nsteals. But this is unlikely, since the allocation of i has reduced, it is possible that its desire had also\ndecreased before that, and thus it has more processors than it needs and there are likely to be more\nsteal attempts as a result.\n\nImplementation\nWe implemented a prototype of the dynamic processor allocation system as an extension to the Cilk runtime\nsystem. The implementation details of the three tasks described in Section 5 are given below.\nEstimating the Job Desire\nWhen a job starts up, it registers itself with the allocator and sets its initial desire as 1. After this, the job\nstarts executing and periodically a background thread counts up the number of frames in the deque and\nupdates the desire in the allocator. How often the desire is estimated is a tunable parameter and can be\nchanged according to the number of jobs in the system and how often they change their desires.\nAllocating the Processors\nThe processor allocator is implemented as a group of functions that share a global data-structure. This\ndata-structure is a memory mapped file which is basically a big array that holds the desires and allotments\nof all the jobs in the system. The parameter maxAlloc described in Section 5 is also stored in this file. Note\nthat the parameter f airShare can be easily inferred from the value of maxAlloc. When any job changes its\ndesire, it just calls a function that simply executes the algorithm for fair and efficient allocation described\nin Section 5 and changes the allotments. To find the jobs to take or give processors to, it randomly picks a\nplace in the array to start from and scans the array until it is done. A job can get the allotment from this\narray at any time by simply reading the value of its allotment from this file.\nAdjusting the allotment\nWhen a job allotment changes, the job has to increase or decrease the number of processors that are being\nused. In this prototype, the processors are implemented as virtual processors or to be more exact pthreads.\nWhen a job starts, it creates number of pthreads that are equal to the number of processors in the machine.\nOn getting its first allotment (which is 1), the job puts most of the pthreads to sleep. After this whenever\nthe allotment of a job increases, some of these pthreads are woken up. When the allotment decreases, the\npthreads are put to sleep. We mentioned above that the number of processors are decreases during steals.\nThis is when the pthreads are put to sleep. Thus we ensure that threads go to sleep only when they dont\nhave any work.\nExperimental Results\nWe have performed some preliminary tests on our processor allocation system to measure its correctness,\noverhead, and responsiveness to allocation changes. The system we are using is an SGI Origin 2000 SMP\nwith 1 Gb of memory and 32 195 MHz processors. The tests we have performed do not constitute a full\nperformance analysis of the system, but they serve as a good foundation for future experiments. We explain\neach of the tests, and the results we obtained, in the sections below.\nFor all tests, we estimate the processor desire every 10 milliseconds and use a value of 6 for k. These\nparameters are not optimal--they are simply the ones we settled on for the purpose of running the experi\nments.\nCorrectness\nTo verify the correctness of our processor allocation system, we run a few sample programs with different\ninputs and ensure that the outputs are correct in every case. The programs we use are the nfib program\nfor calculating the nth Fibonacci number (included in the latest distribution of Cilk), and an optimized\n\nInput (n)\n1 processor\n32 processors\nOur System\n(seconds)\n(seconds)\n(seconds)\n0.01\n0.09\n0.06\n0.02\n0.1\n0.1\n0.08\n0.12\n0.16\n0.78\n0.17\n0.66\n8.55\n0.7\n2.47\n97.3\n6.48\n11.39\n1048.8\n70.4\n154.7\nTable 1: The elapsed time measured for separate runs of the nfib program on different inputs (n represents the nth\nFibonacci number).\nInput (n)\n1.74\n0.6\n1.2\n11.91\n2.15\n3.58\n84.8\n11.29\n17.37\n591.2\n72.3\n117.7\n1 processor\n(seconds)\n32 processors\n(seconds)\nOur System\n(seconds)\nTable 2: The elapsed time measured for separate runs of the strassen program on different inputs (n is the size of\nthe matrices).\nstrassen program for multiplying two n × n matrices using Strassen's Algorithm. The optimization in the\nstrassen program uses a fast, serial algorithm to multiply the submatrices when n ⇒ 128.\nFor both the nfib and strassen programs, all of the outputs obtained using our system are correct:\nthe correct Fibonacci number is computed using nfib, and the correct matrix product is calculated using\nstrassen. As an additional check, we ran the same tests without our processor allocation system (i.e. using\nthe original Cilk runtime system), and we see that the corresponding outputs are identical.\nOverhead\nTo measure the overhead of our processor allocation system, we run the nfib and strassen programs on\ndifferent inputs, once using our system and again using the original Cilk runtime system. When using the\noriginal Cilk runtime system, we need to specify the number of processors to run the programs on: we\nperform each run using 1 processor (referred to as the 1 processor system) and again using 32 processors\n(referred to as the 32 processor system). During a given run, the program in question is the only job running\non the system. The results from our tests are summarized in the tables below.\nFrom Table 7, we see that the nfib program runs faster using our system than using the 32 processor\nsystem, for jobs that have insufficient parallelism to use all 32 processors efficiently. Our system performs\nbetter in this scenario because it is able to dynamically reduce the number of processors in use, so that no\nprocessors are going to waste. The performance of our system is still slower than the 1 processor system\nbecause of the overhead associated with creating and manipulating the worker threads (recall from Section 6\nthat P threads are created for each job at startup, even though most of them are put to sleep).\nFor higher values of n, the nfib program runs about twice as slow using our system than on the 32\nprocessor system. This slowdown is primarily due to the work overhead incurred by the background thread\nwhen measuring and reporting the job's current desire, as well as the overhead incurred by the processor\nallocator when recalculating the allocation. It is possible that our system would perform better if the value\nof k is decreased, since this change would increase the estimated desires of the jobs (and the results for the\n\n32 processor system seem to indicate that the higher runs of nfib benefit from having more processors).\nFor the strassen program, the results in Table 7 indicate that our system is always less than twice as\nslow as the 32 processor system. The reasons for this slowdown are identical to those for the higher runs\nof nfib explained above. We expect that a decreased value of k could also improve the performance of our\nsystem in this case.\nResponsiveness\nTo measure the responsiveness of the processor allocation system, we measure the time it takes for a job to\ndecrease its processor usage when the granted allotment is reduced by the allocator (recall that processors\nare reduced during work-stealing only). Since we are only running one job on the system at a time, however,\nthe granted allotment usually drops by 1 processor at a time, and so the measured response is almost\ninstantaneous. We do not measure the response time for increases in job allotments because this is handled\nsynchronously by the background thread.\nWe tried to artificially decrease the allotment of a job and measure the system's response time. For\nnfib run on n = 30, it took between 15 and 50 milliseconds for the job's processor usage to decrease from\n10 to 5 processors. The same experiment took between 50 millisecond and 2 seconds for strassen run on\nn = 1024. In general, it is not a good idea to interpret these results: since jobs only reduce their processor\nusage while work-stealing, even if we artificially reduce a job's allotment, the number of processors it uses\ndoes not immediately decrease if the job's desire has not really changed.\nAdditional Experiments\nThere are a number of additional tests that we would like to perform. Most of these tests involve running\nmultiple, concurrent Cilk jobs and evaluating the performance of our processor allocation system (we have\nnot had time to do this yet). In particular, we would like to perform the following tests:\n1. Vary the frequency with which the job desires are estimated and measure the system's performance\nwhen running multiple jobs (we already tried this test for single jobs but did not notice any significant\nimprovements/degradation in performance).\n2. Vary the value of k and measure the system's performance both when running single jobs and when\nrunning multiple.\n3. Run a pair of jobs together that have complementary parallelism profiles (e.g. one job starts out with\nhigh parallelism and ends with low parallelism and the other job starts out with low parallelism and\nends with high parallelism). Try to find a pair of jobs that performs better using our system than with\nthe original Cilk runtime system.\n4. Run a group of jobs with arbitrary parallelism profiles and measure the system's performance.\nFuture Work\nThere are a number of extensions to the processor allocation system that we would like to explore in the\nfuture. We divide these extensions according to the task they pertain to, and discuss them below.\nEstimating the Job Desire\nWe would like to consider ways of combining the steal rate idea used in [3] with the heuristic used in our\nsystem, to see if some combination of the two allows us to measure a job's parallelism more accurately.\n\nAllocating the Processors\nThere are many ways to improve both the design and implementation of the processor allocator. In an\nimplementation level, we can consider using a sorted data structure to store the job entries instead of the\nlinear array that we currently use; the jobs could be sorted using their desires or their allotments, or perhaps\nthe difference between the two numbers. Another alternative would be to divide the jobs into separate\ngroups: jobs whose desires are satisfied, deprived jobs that have the smallest allotment, and deprived jobs\nthat have the maximum allotment.\nOn a design level, we would also like to consider alternative definitions for fairness and efficiency. For\ninstance, we could define fairness in a way that incorporates the histories of processor usage for each job, or\nwe could design a mechanism for respecting different job (and user) priorities. As a longterm goal, we would\nlike to generalize our desire estimation and processor allocation algorithms to work for all types of parallel\njobs; this way, the processor allocation system can be generalized into a core operating system service, and\nmoved into the kernel itself.\nAdjusting the Allotment\nOne improvement to our method of adjusting allotments that is relatively easy to implement is to allow\nprocessors to be removed by the background thread (instead of during work steals). This optimization\nwould result in a faster response time to changes in a job's allotment, but would also require a mechanism\nfor putting processors with pending work to sleep (and waking them up when increasing the allocation).\nConclusion\nIn this paper, we introduced a simple heuristic for estimating the instantaneous parallelism of an adaptively\nparallel job, based on the amount of pending work queued on the job's processors. For the specific case of\nparallel jobs scheduled with the randomized work-stealing algorithm, we demonstrated how this heuristic\ncan be used to design a processor allocation system that allocates processors to jobs in a fair and efficient\nmanner. We tested our design by implementing a dynamic processor allocation system for Cilk jobs running\non a shared-memory symmetric multiprocessor (SMP) system. We provided empirical evidence evaluating\nthe correctness and overhead of our system, and outlined the experiments to be run in the future. Finally,\nwe discussed various extensions and improvements to our processor allocation system, which we plan to\ninvestigate in the future.\nReferences\n[1] R. D. Blumofe, C. F. Joerg, B. C. Kuszmaul, C. E. Leiserson, K. H. Randall, and Y. Zhou. Cilk:\nAn efficient multithreaded runtime system. In Proceedings of the Fifth ACM SIGPLAN Symposium on\nPrinciples and Practice of Parallel Programming (PPoPP), pages 207-216, Santa Barbara, California,\nJuly 1995.\n[2] R. D. Blumofe and C. E. Leiserson. Scheduling multithreaded computation by work stealing. In Pro\nceedings 35th Annual Symposium on Foundations of Computer Science, November 1994.\n[3] R. D. Blumofe, C. E. Leiserson, and B. Song. Automatic processor allocation for work-stealing jobs.\n[4] M. Frigo, C. E. Leiserson, and K. H. Randall. The implementation of the cilk-5 multithreaded language.\nIn ACM SIGPLAN Conference on Programming Language, 1998.\n[5] Supercomputing Technologies Group. Cilk 5.3.2 Reference Manual. MIT Lab for Computer Science,\nNovember 2001.\n\n[6] B. Song. Scheduling adaptively parallel jobs. Master's thesis, Massachusetts Institute of Technology,\nJanuary 1998."
    },
    {
      "category": "Resource",
      "title": "cananin_proj_pro.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/79b302b3b7eac92627fe9850cdaf9e84_cananin_proj_pro.pdf",
      "content": "!\"\n\n!!\n\n!#\n$\n%\n\n&\n\n'\n\n%&#(\n$\n\nÆ\n\n)\n*\n\n)++\n$\n,\n\n'\n\n'\n\n#-\n%)&.($\n/\n0'\n\n'\n\n/\n1.2\n$\n\n/\n#(\n$\n\n)!2\n\n4)!-\n)5!+\n\n#.\n$\n\n)!!\n\n6%4!!$\n\n&)\n\n8'\n9&):85;\n\n84#2\n\n%&\n#<\n$\n=\n\n>\n\n> ?\n\n>\n\n&\n\n&!@\n$\n\n'\n\n'\n\n'\n\n>\n\n=\n\n%\n\n/\n0'\n;\n\n>\n\n;\nA\n\nB\nC\n\n'\n\nD\n\n'\n\n>\n=:\n'\n\n=\n\n=\n\n'\n\n%\n'\n\n'\n\n?\n\n'\n\n=\n'\n\nÆ\n\n'\n\n=\n'\n\nÆ\n\n=\n\n'\n\n'\n\n)\n\n'\n\n=\n\n'\n\n>\n\n'\n:\n'\n\n%'\n\n'\n\nÆ\n\n'\n\n'\n\n'\n\n;\n\n>\n\n'\n\n'\n\n:\n\n:\n\n6%4!!$\n\n)\n&\n,\n\n%\n\n&\n=\n\n:EE\n\n-+F\"#\n\n@#!!\n%&\n#<$\n\n%\n\n&\n\n&\n\n&\n\n@\"9\";D@+@#F@+(\"\n@##<\n@\n\n%)&.($\n&\n%\n\nG\n)\n\n&\n&\n\n=\n=\n=\n\n'\n\n=\n\n!\n\n#2F@.@\n\n&\n,\n2..(\n%&#($\n&\n%\n\nH\n&\n\nD\n\n=\n\n\"\n\n2!#F(..\n\n&\n,\n@##(\n!#$\n&\n\n,\n\nD\n\n>\n\n/\n\n&=\n:):\n/<-<\n&=\n)\n\n@#!#\n!\"$\n\n=\n\n#$%&\n\n'\n\n'\n\n@.-F@@2\n\n&\n,\n@#!\"\n)++$\n)\n)\n\n'\n\n2.9@@;D!.\"F!@@\n\n@#++\n)5!+$\nH\n)\n\n,\n\n/\n\n=\n\n=\n\n##\n\n@@@F@22\n\n&\n,\n@#!+\n)!!$\n/\n\n)\n/\n\nD\n\n:EE\n=\n\n#\n\n!\n\n!\n\n@(F2@\n=333\n\n,\n@#!!\n)!2$\nH\n)\n\n/\n\nI\n\nD\n)\n\n=\n\n$\n\n()*(\n\n+F@#\n\n&\n,\n@#!2\n&!@$\nH\n&\nD\n\n/\n\n&=\n:):\n/2\".\n&=\n)\n\n@#!@\n84#2$\n\n&\n\n&\n,\n\nE\n\nJ\n\n=\n\n*\n\n+*,\n\n!(2F!<(\n\n@##2\n=333\n\n,\n\n&K#@@+(\n\n&\nK\n,\n,\n\n@##@\n/\n#($\n\n/\n\n&\n\n@-9(;D<#<F\n-(<\n@##(\n/\n1.2$\n/\n\n/\n0'\n\n/\n\n>\n\n=\n\n#\"\n\n*-\n\n-F@+\n\n&\n,\n2..2\n\n#-$\n\n'\n\n=\n\n#.\n\n!\n\n2.<F2@(\n\n&\n,\n@##-\n\n!!$\n,\n\n&\n\n,\n\n>\n\n'\n,\n\n=\n\n*\n\n#$%%\n\n'\n\n'\n\n2!F(#\n\n&\n,\n@#!!\n#.$\n\n)\n\n@292;D@+!F2.2\n@##.\n4)!-$\n\nH\n)\n\n=\n\n+92;D2<<F2\"#\n@#!-"
    },
    {
      "category": "Resource",
      "title": "caracas.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/ff36e49b354fc41ea1edc5ec2c9a0246_caracas.pdf",
      "content": "6.895 Theory of Parallel Systems\nAlexandru Caraca s\nFast Serial-Append. Parallel File I/O Support for Cilk\nProject Report\nAbstract\nWe present a method for parallelizing serial applications strictly dependent on sequential file output.\nWe give a scheme for maintaining serial-append for parallel programs. Subsequently, we describe the\nimplementation of serial-append for the Cilk multithreaded language. Empirically, we show that a single\nfile is a bottleneck in multithreaded computations that use files. We propose using multiple files to\nrepresent the same logical file and gain performance. Our results show that the application of concurrent\nskip lists, as a concurrent order maintenance data structure to support serial-append, scales well with\nthe number for processors.\nContents\n1 Serial-Append\n1.1 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.2 Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.3 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2 Algorithm\n2.1 General Algorithm Idea . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.2 Cilk Algorithm for serial-append . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3 Implementation\n3.1 Na ıve Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.2 Improved Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.3 Cheerio . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4 Performance\n4.1 Testing Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.2 Experimental results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.2.1\nPLIO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.2.2\nPthreads . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.3 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5 Additional Documentation\n6 Conclusion and Improvements\n6.1 Summary of Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6.2 Improvements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nWe start by defining the concept of serial-append as a method of doing parallel file I/O. Next, we\ngive a general idea for an algorithm to maintain serial-append. In the subsequent section, we discuss the\nimplementation of serial-append for the Cilk multithreaded language. Following, we present the performance,\nexperimental results of two implementation flavors.\nSerial-Append\nThis section describes serial-append as a parallel file I/O mode. On most operating systems, serial-append is\nequivalent to opening a file for writing in append mode. This means that written data always gets appended\nto the end of the file.\nWe revise serial-append when executing sequentially, on a single processor. On a single processor, serial-\nappend is equivalent with a depth first search traversal of the computation DAG. Consider we have a program,\nrepresented by the computation DAG from Figure 1, with threads numbered from 1 to 12. Consider that\neach thread in the computation DAG takes one unit time to execute. When the program gets executed we do\na depth first search traversal and append all the written data from the threads at the end of the file. Figure 1\nshows the serial-append file at different time steps during the single processor execution of the computation\nDAG. The already executed threads are shaded. A pointer represents the current position of the processor\nwithing the file.\n(a) time= 0\n(b) time= 3\n(c) time= 7\n(d) time= 12\nFigure 1: Different time-steps of the serial execution of a program that writes to a serial-append file.\nNext, we describe the parallel case for serial-append, when the program is executed on multiple processors.\nOn multiple processors, serial-append is equivalent to each processor having a pointer within the file and\nbeing able to insert data at that point. Consider the same program represented by computation DAG\nas in Figure 1, with the threads taking one unit of time to execute. Also, consider that we have three\nprocessors, with colors red, green, and blue, which execute the computation in parallel. Figure 2 shows the\nserial-append file at different time steps during the multiprocessor execution on the computation DAG. The\nalready executed threads are colored based on the processor who executed them. Each processor has its own\npointer that represents its position within the file.\n\n(a) time= 2\n(b) time= 3\n(c) time= 4\n(d) time= 5\nFigure 2: Different time-steps of the multiprocessor execution of a program that writes to a serial-append file.\nWe define serial-append as a file I/O mode that allows for parallel execution with the same final file\noutput as the sequential execution. From a conceptual stand point, serial-append from parallel applications\nrequires an insert into file primitive. With such a file primitive the implementation of serial-append would be\ngreatly simplified. Since file systems available today do not support such a primitive, the implementation of\nserial append for parallel applications is more challenging. The concept of serial-append is general and can\nbe applied to any multithreading environment. In the following, we will discuss the implementation issues\nof serial-append for Cilk, a multithreaded language with a provably good work stealing scheduler.\n1.1\nApplications\nA usual operation performed by many applications that use files is to append data to a file. Examples of\nsuch applications include compression utilities, logging programs, and databases. Since these applications\nare strictly dependent on the sequential output to the files they are using, parallelizing them is extremely\nchallenging.\nIdeally, we would want to execute these applications in parallel, and benefit from their eventual paral\nlelism, improving their performance. A good example is the compression tool bz2, which is based on block\ncompression, where each block compression is independent. Hence, we want to perform all write operations\nto a serial-append file in parallel, while still being able to have as output, conceptually, the same serial-append\nfile as the sequential execution. Another important goal is to be able to read and seek efficiently within the\nserial-append file that was written in parallel.\nNext we give a clear definition of the exact semantics of serial-append and the view of the programmer.\n1.2\nSemantics\nThe external API for serial-append, available to the Cilk programmer is similar to the file I/O API available\non most operating systems. This similarity enables serial applications requiring sequential output to be\nparallelized easier using serial-append. The serial-append API includes the following functions:\n\n- open (FILE, mode) / close (FILE),\n- write (FILE, DATA, size),\n- read (FILE, BUFFER, size),\n- seek (FILE, offset, whence).\nThe semantics of serial-append are intuitive. The write operation, executed on a file opened in serial-\nappend mode, maintains the correct ordering of the appends to the file, while executing in parallel. The\nread and seek operations can only occur after the file is closed, or on a newly opened file, otherwise we will\nhave a data race situation. The semantics of write, read, and seek for serial-append are summarized in\nthe following:\n- write operations maintain the correct serial-append of the file, with the final output the same as the\nsequential, single processor, execution;\n- read and seek operations can occur only after the serial-append file has been closed, or on a newly\nopened serial-append file.\nNote: The open and close operation maintain the familiar semantics.\n1.3\nRelated Work\nThis project leverages on Cheerio, previous work done by Matthew DeBergalis in his Master Thesis ([DeB00]).\nCheerio includes a more general Parallel I/O API and implementation in the Cilk multithreaded language,\nand is described in [DeB00]. Cheerio provides a more general frame work and describes several possible\nschemes of performing file I/O from parallel programs, using global, local, or serial-append mode.\nWe focus on serial-append as a parallel file I/O mode and have two major differences with respect to the\nalgorithm used in [DeB00] for performing serial-append. The first difference is that we use multiple files to\nallow each processor to write at will, unrestricted, whereas Cheerio uses a single file. As our experimental\nresults show, using a single file on multiple processors is a major bottleneck in parallel computations that\nuse that single file. The second difference is that we use a different data structure to keep the metadata\nabout the execution of the parallel computation, namely a concurrent skip list, whereas Cheerio uses a data\nstructure very similar to linked list.\nAlgorithm\n2.1\nGeneral Algorithm Idea\nWe present a general algorithm idea used for maintaining the serial-append of a file from parallel programs.\nWe reduce the problem of maintaining the serial-append of a file by braking the execution into parts executed\nby a processor and performing the appropriate bookkeeping and ordering of the parts.\nObservation 1 (serial-append) Serial-append is equivalent to a depth first search traversal of the\ncomputation DAG.\nMaintaining serial-append from a parallel program depends on the threading environment and the schedul\ning scheme used. Without file system support of a primitive to insert into a file, we need to do bookkeeping\nduring the parallel execution of the program for maintaining the serial-append. Our focus will be on Cilk\nand its work stealing scheduler as described in [BFJ+96].\n\n2.2\nCilk Algorithm for serial-append\nWe describe a serial-append algorithm for Cilk. Cilk is a multithreaded programming language with a\nprovably good work-stealing scheduler. In Cilk, a spawn of a procedure is a little more than a function call\nand the processor that spawned the procedure immediately executes the spawned procedure. However, on a\nsteal operation a processor goes and steals work, or procedures from a different processor. Hence, we have\nthat a spawn preserves the depth first search traversal of the computation DAG. The problem arises with\nthe steal operations which disrupt the normal depth first search traversal of the computation DAG.\nObservation 2 (steals) Steals affect the correct ordering for a serial-append file.\nWithout a file system primitive that can support the insert into a file operation we need to do bookkeeping\nduring the parallel execution of the program and account for the steal operations.\nWe simplify the problem of maintaining the serial-append of a file by partitioning the execution of the\ncomputation and properly maintaining the order of the partitions. A partition (or a PION) is represent by\nthe following definition.\nDefinition 3 (PION) A PION (ParalleL I/O Node) represents all the write operations that a processor\nperforms in-between two times it steals. A PION contains metadata, as well as a pointer to the actual data\nas we show in Figure 3.\nunion MetaData{\nint victimID;\n// the ID of the victim from which the PION was stolen\nunsigned long numbytes;\n// number of written data bytes\nint fileid;\n// the file where this node's data is written\n}\nFigure 3: Metadata fields contained by a PION.\nLeveraging on the above definition, we give the algorithm for maintaining serial-append in Cilk.\nAlgorithm:\n1. All PIONs are kept in an order maintenance data structure.\n2. Every active processor has a current PION.\n3. On each steal performed by a processor Pi from processor Pj , with current PION j , we perform the\nfollowing two steps:\n(a) we create a new, empty PION, i, and\n(b) attach i immediately after j in the order maintenance data structure.\nThe order maintenance structure, at a very basic, intuitive level, is represented by a linked list. The\noperations supported are: insert an element after a given element, search for an element with a certain rank,\ndelete a given element, update an element; where an element is given by a pointer. For our case, the order\nmaintenance data structure also needs support for concurrent operations.\nImplementation\nIn this subsection we present the two implementations flavors of serial-append for Cilk and we discuss the\nissues with porting Cheerio to the new version of Cilk. We will refer to serial-append for Cilk as PLIO,\nstanding for ParalleL File I/O.\n\nPLIO interacts closely with the Cilk scheduler and runtime system and implements hooks on the steal\noperations. On each steal, the runtime system makes a call to internal PLIO routines, part of the runtime\nsystem, to do the required bookkeeping about the parallel execution. The external PLIO API available to\nthe programmer also interacts with the Cilk runtime to perform the file I/O operations. The actual reading\nand writing from disk use the low level file I/O API provided by the operating system. The most significant\nwork for PLIO was implementing the PLIO runtime as part of the Cilk runtime system together with the\nimplementation of the external PLIO API.\nMultiple, separate files are used as buffers for each processor to allow them to write to disk unobstructed.\nThe metadata data structure is kept in memory, and written to disk when the file is closed.\n3.1\nNa ıve Implementation\nThis subsection describes the first prototype of serial-append for Cilk. The na ıve implementation uses a\nsimple concurrent linked-list maintain the ordering of the metadata about the parallel execution of a Cilk\nprogram.\nThe PLIO implementation using linked lists for maintaining the order of PIONs represents the first\nimplementation and it is simply a prototype. Because the data structure used to maintain the order is a\nconcurrent linked list, the seek operations are extremely inefficient, namely they take O(N ) time, where\nN is the number of PIONs (the size of the linked list). However, a simple concurrent linked list offers the\nadvantage of very efficient inserts and update operations, in O(1) time, together with much easier algorithms\nfor inserting and deleting PIONs.\n3.2\nImproved Implementation\nWe describe the improvements to the very simple prototype, which involved using skip lists as un underlying\nconcurrent data structure for order maintenance, as described in [Pug90]. Skip lists are a probabilistic variant\nto balanced trees and have logarithmic performance, namely all operations execute in O(log N ) time, where\nN is the size of the structure (in our case the number of PIONs).\nIn the improved implementation we use a variant of skip lists as un underlying data structure to maintain\nthe order of the PIONs. The data structure being used can be defined as a rank order statistics double linked\nskip list. Figure 4 visually represents the data structure. Every node has one or more levels. Each level has\nforward and backward pointers to the immediate nodes with the same level. In this way, nodes with a higher\nnumber of levels can skip over the nodes with a low number of levels. There are no absolute keys stored in\nthe nodes. Instead we use the metadata kept in the nodes, namely the number of data bytes written from\nthis node to the serial-append file, to compute the absolute offset within the file. The computed absolute\noffset, represents the rank of a PION. The existent concurrent skip list algorithms were adapted accordingly\nto support backward pointers.\nFigure 4: Visual representation of a double linked skip list.\nWhen inserting, we give a pointer to the PION after which we want to insert. Using this data structure,\nwe have logarithmic performance in the size of the structure, for all the insert, search, delete and update\noperations, namely O(log N ), where N is the size of the structure, in our case the number of PIONs.\n\n3.3\nCheerio\nIn this subsection we describe the issues with porting Cheerio, written for Cilk 5.2, to the new version of\nCilk 5.4. Porting Cheerio was a real challenge due to the fact that Cilk has been restructured substantially\nfrom the earlier versions. The global and local parallel file I/O modes from the Cheerio API, were ported\nsuccessfully and are working well for the simple cases and sanity checks that were tested.\nThe port of Cheerio is not a complete success due to deadlocks issues with the implementation of the\nserial-append append mode. For documentation purpose, we give the description of the deadlock problem.\nDuring the program execution, two pointers, which should logically be distinct, become the same. Because\nthere is a locking sequence involving acquiring locks for these particular pointers, the program deadlocks.\nMore detail about the problem is documented in the code (in function cio commit node, see HTML API).\nThe problem usually arises with the increase in the number of processors or execution time, and it manifests\nwhen committing nodes to disk. Due to time constraints, we were not able to remediate the problem in a\ntimely manner to allow for a performance comparison with PLIO.\nPerformance\n4.1\nTesting Environment\nThe tests were performed on a 32 processor Origin machine running Linux (see\nTable 1 for the technical specifications).\nOperating System\nC Compiler\nProcessor speed\nNumber of processors\nRegisters\nL1 Cache size\nL1 Cache access time (ns)\nL1 Cache line size\nL1 Associativity\nL1 Cache lines\nL2 Cache size\nL2 Cache access time (ns)\nL2 Cache line size\nL2 Associativity\nL2 Cache lines\nMemory access time (ns)\nOrigin 2000 w/R10000\nIRIX64 6.5 07121148 IP27 mips\ngcc - 3.2.2\n195 MHz\n32KB\n128B\n2-way\n4MB\n128B\n2-way\n32K\nTable 1: Technical specification of testing environment.\nAs benchmark for the tests, we used a modified version of the example program fib.cilk, which computes\nthe Fibonacci numbers. We chose this multithreaded program due to its high parallelism, approximately 300,\nmeasured with the Cilk profiling tools. The modified version of fib writes 10KB of data to a serial-append\nfile for each recursive function call. It turns out that, if we want to compute the 25th Fibonacci number, we\nend up writing approximately 2GB of data to disk.\nThe running time for the two programs were measured with the time command, and the \"real time\"\nmeasurement was considered. The results of the tests are summarized in the following section.\n\n4.2\nExperimental results\nIn this subsection we present the experimental results and benchmarks for the two versions of PLIO. We also\nshow an interesting experimental result related to scalability of parallel computations that want concurrent\naccess to a single file. Based on the experimental result we argue that speedup can only be achieved when\nusing multiple files.\n4.2.1\nPLIO\nIn this subsection we show the experimental results of the modified fib that writes to disk each time we do\na function call. The difference in performance between the two implementation flavors of PLIO is practically\nindistinguishable when compared with the normal fib program.\nFigure 5 compares the normal fib program with the modified fib program (represented by \"PLIO\" in Fig\nure 5) that writes 2GB of data to disk in parallel. The graph shows linear speedup (execution time(seconds)\ntimes the number of processors) versus the number of processors. The scale is logarithmic since the execution\ntime for the normal fib program is very small compared to the modified version which writes 2GB of data\nto disk. We notice that the normal fib, which performs no file I/O operations (represented by \"NOIO\" in\nFigure 5) achieves almost linear speedup. The same is true for the version of fib that performs disk I/O,\nwhich shows the good scalability of the serial-append scheme. On the same graph is plotted a simple C pro\ngram (represented by \"Pthread-sep\"), described in the following subsection, which performs write operations\nto disk as fast as possible. The C program uses Pthreads and multiple files, as the implementation of PLIO\nin the Cilk runtime system. The C program emulates the bare disk operations of PLIO. The difference in\nperformance between the Pthread bare emulation and PLIO is due to the Cilk overhead present in fib.\nFigure 5: Performace of PLIO with respect to linear speedup.\nThe two implementation flavors of serial-append have almost the same performance for writing to disk.\nFigure 6 shows a comparison of the two PLIO flavors. The graph shows the execution time in seconds with\nrespect to the number of processors. The linked list implementation is labeled as \"PLIO\" and the skip list\nimplementation is denoted by \"PLIO-SL\".\nImportant to note is that the skip list implementation gains in performance, with respect to the linked\nlist implementation, when reading and seeking (which involve search operations on the order maintenance\ndata structure) from a serial append file. Also, from Figure 6, we see that write operations (which involve\ninserts into the order maintenance data structure) for the skip list implementation of PLIO are in practice as\neffective as the linked list implementation. The experimental results assert that skip lists are an efficient and\nsimple alternative to more complicated concurrent order maintenance data structures, such as concurrent\nbalanced trees.\n\nFigure 6: Comparison of the two implementation flavors (linked-lists and skip-lists) for serial-append .\n4.2.2\nPthreads\nWe conducted a very simple experiment involving writing data to disk as fast as possible using a C program.\nWe had three versions of the C program. The first version (represented as \"FASTIO\" in Figure 7) is a\nserial program that simply writes data in chunks of size 10KB . The second version (labeled as \"Pthread\"\nin Figure 7) uses a single file and multiple Pthreads to write in parallel to the file. The Pthreads seek to\ndisjoint offsets within the file and start writing concurrently . The third version (labeled as \"Pthread-sep\"\nin Figure 7) also uses multiple Pthreads that write data in parallel. However, each Pthread has its own\nseparate file.\nThe best results were obtained when using multiple Pthreads that write to separate files. The multiple\nPthreads represent an emulation of what the PLIO runtime actually performs. When using a single file\nand multiple Pthreads that seek to separate offsets within the file and start writing in parallel, virtually, no\nspeedup is possible. The results are summarized in Figure 7.\nFigure 7: Performace of different versions of a C program that writes 2GB of data to disk as fast as possible.\n4.3\nInterpretation\nThe interpretation of the fact that, when using a single file, is impossible to achieve speedup, may come\nfrom the way in which the operating systems performs the file I/O operations. One possibility is that the\n\noperating system uses internal kernel locks for each access to the file, literally serializing the multiple threads\nthat are trying to write data to the same file.\nA solution to this problem, as the empirical results show, is to use, instead of a single file, multiple files\nwhich represent a logical single file (as it is being implemented in PLIO). Another solution would be to have\nspecific kernel and file system support for parallel access to files.\nAdditional Documentation\nThe implementations are available as follows:\n- Cheerio - https://bradley.csail.mit.edu/svn/repos/cilk/current_parallel_io\n- PLIO (with Linked List) - https://bradley.csail.mit.edu/svn/repos/cilk/plio_append\n- PLIO (with Skip List) - https://bradley.csail.mit.edu/svn/repos/cilk/plio_skip_list\nMore detailed documentation about the C implementation and the API, individual methods and struc\ntures used by serial-append is available in HTML format in the doc folder for the respective implementation,\nas follows:\n- Cheerio: doc/cheerio.\n- PLIO (both flavors): doc/plio.\nThe file index.html is the entry point for the HTML documentation.\nDifference files are available to ease the task of finding the pieces of code that were added to the current\nCilk runtime-system. They are located inside the doc folder (together with the HTML API documentation)\nfor the respective implementation (as described above), and have the extension .diff .\nAdditional documentation, available in the respective doc folders for the two implementations of PLIO,\nincludes the final pdf presentation as presented in the 6.895 - Theory of Parallel Systems class.\nThe programs used for the benchmarks and sanity checks are available in the examples folder for the\nrespective implementation as follows:\n- Cheerio: examples/cheerio.\n- PLIO (both flavors): examples/plio.\nConclusion and Improvements\nIn this subsection we conclude our results and suggest several improvements for future work.\n6.1\nSummary of Results\nWe briefly summarize our results:\n- Two flavors of Cilk runtime support for performing serial-append, with good scalability.\n- Port of Cheerio, a more general parallel file I/O API, to Cilk 5.4.\n- Experimental results concerning scalability constraints, related to the operating system internal file\nlocks, when accessing the same file from multiple threads.\n- Assertion of concurrent skip lists as an efficient concurrent data structure.\n\n6.2\nImprovements\nPossible improvements to the presented schemes for serial-append include:\n- Deleting PIONs that contain no data. This improvement decreases the number of PIONs kept in the\norder maintenance data structure that maintains the metadata about the parallel execution.\n- Develop a cache oblivious algorithm for the skip list used to maintain the order of PIONs.\n- Experiment with other concurrent order maintenance data structures such as B-trees.\n- Develop a new file system with an insert primitive, and support from the operating systems for con\ncurrent operations on a single file from multiple threads.\nReferences\n[BFJ+96] Robert D. Blumofe, Matteo Frigo, Chrisopher F. Joerg, Charles E. Leiserson, and Keith H. Ran\ndall. An analysis of dag-consistent distributed shared-memory algorithms. In Proceedings of the\nEighth Annual ACM Symposium on Parallel Algorithms and Architectures (SPAA), pages 297-308,\nPadua, Italy, June 1996.\n[DeB00] Matthew S. DeBergalis. A parallel file I/O API for Cilk. Master's thesis, Department of Electrical\nEngineering and Computer Science, Massachusetts Institute of Technology, June 2000.\n[Pug90] William Pugh. Concurrent maintenance of skip lists. Technical Report CS-TR-2222, 1990."
    },
    {
      "category": "Resource",
      "title": "fineman.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/508836356d5e876ccec130b11c4b305c_fineman.pdf",
      "content": "On-the-Fly Detection of Determinacy Races in Fork-Join Programs\nJeremy T. Fineman\nDecember 15, 2003\nAbstract\nQuick determination of series-parallel thread relationships is at the core of some determinacy-race-\ndetections schemes. In this paper, we present two provably good algorithms for maintaining the series-\nparallel relationships between threads on shared memory systems, on-the-fly. The first algorithm, called\nthe SP-order algorithm, is sequential, executing a fork-join program asymptotically optimally on one\nprocessor. For any two threads that have already executed, we can determine the relationship between\nthem in constant time. For any fork-join program that runs in T1 time on a single processor, an execution\nof the program with the SP-order algorithm run asymptotically optimally in O(T1) time. The SP-order\nalgorithm depends on an order-maintenance structure similar to that proposed by Dietz and Sleator.\nThe second algorithm, called the parallel-SP-order algorithm, runs on any number P of processors.\nAs with the serial algorithm, we can determine the relationship between two threads that have already\nexecuted in constant time. However, we rely on the work-first principle of the Cilk scheduler to get a\ngood bound. For a Cilk program that runs in TP = Θ(T1/P + Tinf) time on P processors, where T1 is\nthe work, and Tinfis the critical-path, the parallel-SP-order algorithm runs in O(T1/P + PTinf).\nIntroduction\nA multithreaded program in the fork-join model contains threads, which are maximal sequences of in-\nstructions not containing parallel control statements. Multiple threads concurrently access shared memory.\nGenerally, fork-join programs are meant to be deterministic. That is to say, the program should generate\nthe same output no matter how the threads are scheduled. A determinacy race can occur, however, when\na thread writes to a memory location while another thread is accessing the same location.\nDeterminacy races are difficult to detect through normal debugging techniques, as the races depend on\ntiming and scheduling of the threads. Inserting breakpoints in a debugger or diagnostic statements into the\ncode alters the timings.\nFigure 1 shows an example of a program containing a determinacy race in the Cilk [2] multithreaded\nlanguage.1 The main() procedure uses the spawn keyword to fork a procedure foo() and continue executing\nconcurrently with the spawned procedure. Then, the main() procedure spawns another instance of the foo()\nprocedure. The sync keyword suspends main() until both instances of foo() return. A determinacy race\noccurs in this program because both instances of foo() can execute and write to a location x concurrently.\nOn-the-fly techniques for detecting determinacy races involve augmenting the original program to re-\nport races as they occur.\nAt the core of these techniques is an efficient algorithm for determining the\nseries-parallel relationship between threads efficiently. Mellor-Crummey [5] reviews different techniques for\nreporting determinacy-races, but we only consider on-the-fly techniques in this paper.\nThe SP-order algorithm is an on-the-fly technique for maintaining the series-parallel relationships\nbetween threads.\nThis algorithm relies on efficient order-maintenance structures inspired by Dietz and\nSleator [3].\nThe remainder of this paper is organized as follows. In Section 2, we review the series-parallel DAG and\nparse tree representations of fork-join programs. We present a serial version of the SP-order algorithm in\n1This example is taken from [4].\n\nint x;\ncilk void foo()\n{\nx = x + 1;\n}\ncilk int main()\n{\nx=0;\n/* e0 */\nspawn foo();\n/* F1 */\n. . .\n/* e1 */\nspawn foo();\n/* F2 */\n. . .\n/* e2 */\nsync;\nprintf(\"x is %d\\n\", x);\n/* e3 */\nreturn\n}\nFigure 1: A simple Cilk program that contains a determinacy race. The thread labels are given in comments on the\nright. The \". . . \" represent arbitrary sequential code segments.\nSection 3. The serial version works efficiently for any serial execution of a fork-join program. In Section 4, we\nprove the correctness of the SP-order algorithm. In Section 5, we present a naive version of the parallel-SP-\norder algorithm, which has poor performance due to concurrent accesses in the order-maintenance structure.\nSection 6 contains a quick review of the Cilk scheduling properties necessary for a good bound on the\nimproved order-maintenance structure and algorithm presented in Section 7. Finally, in Section 8, we prove\nthat the Cilk parallel-SP-order algorithm executes in O(T1/P + PTinf) time on P processors.\nA model for fork-join program executions\nIn this section, we review the representations of fork-join programs as series-parallel DAGs and parse trees.\nWe can represent the parallel control flow of fork-join program as a series-parallel directed acyclic graph,\nor series-parallel DAG, G = (V, E). Each vertex represents a unique thread in the program. Each edge\nin the DAG represents a dependency between threads introduced by a parallel control construct. Before\nformalizing the construction of a series-parallel DAG, we review some notation. On a spawn statement,\nwe add a continuation edge to the thread representing the next sequence of instruction in the current\nprocedure, and we add a spawn edge to the first thread in the spawned procedure. On a sync, we add\nedges from the last thread of each spawned procedure to the thread following the sync. We say that a thread\ne1 ∈V precedes a thread e2 ∈V , denoted e1 ≺e2, if there is a path from e1 to e2 in the DAG. Conversely,\nwe say that threads e1 and e2 operate logically in parallel, denoted e1 ∥e2, if e1 ≺e2 and e2 ≺e1.\nThe DAG for Figure 1 is shown in Figure 2. We see that there is a path from e1 to e3, for example, so\ne1 ≺e3. There is no directed path between F1 and F2, so F1 ∥F2.\nAs a more formal definition, series-parallel DAGs can be constructed recursively as follows:\nBase The graph has a single edge e ∈E connecting s to t.\nSeries Composition The graph consists of two series-parallel DAGs, G1 and G2, with disjoint edges such\nthat s is the source of G1, t is the sink of G2, and the sink of G1 is the source of G2.\n\n!\"\n!#\n!$\n!%\n&#\n&$\nFigure 2: A DAG representing the parallel control flow of the program shown in Figure 1. Threads and procedures\nare represented by circles and rectangles respectively. Downward edges represent spawn dependencies, upward edges\nrepresent return dependencies, and horizontal edges represent the continuation of a procedure.\n!\n!\n\"#\n\"$\n%\n!\n&'\n\"'\n%\n\"(\n&(\nFigure 3: A parse tree representing the parallel control flow of the program shown in Figure 1. The S-nodes indicate\na series relationship, while the P-nodes represent a parallel relationship. Each of the leaves is a thread in the program.\n\nParallel Composition The graph consists of two series-parallel DAGs, G1 and G2, with disjoint edges\nsuch that s is the source of G1 and G2, and t is the sink of G1 and G2.\nFor most of this paper, we use the parse tree representation of the series-parallel DAG, as described by\nFeng and Leiserson [4]. Figure 3 shows the parse tree for Figure 1. Each of the leaves in the tree represent\nthreads in the computation. The S-nodes represent series composition, while the P-nodes indicate parallel\ncomposition.\nA nice property about the parse tree representation is that the least common ancestor of two threads\ndetermines their relationship. We revisit this concept in Section 4\nThe SP-order algorithm\nIn this section, we present a serial version of the SP-order algorithm. First, we discuss what it means to take\na total order of a DAG. Then, we review a data structure for maintaining a total order on the fly. Next,\nwe present a serial implementation of the SP-order algorithm. Finally, we prove that the serial SP-order\nalgorithm executes in O(T1) time, where T1 is the execution time of the original program.\nA total order of a control flow DAG is essentially a topological sort of the DAG, which represents a valid\nexecution of the program. That is to say, for any two threads e1 and e2, if e1 ≺e2 in the DAG, then e1\nprecedes e2 in the total order.\nThe SP-order algorithm maintains total orders on the fly using a data structure. At the very least, we\nneed the following two operations:\n1. Insert(X, Y ): Insert a new element Y immediately after the element X in the order.\n2. Precedes(X, Y ): Return true if X precedes Y in the order.\nA naive implementation of the order maintenance structure is a linked list. The Insert operation a normal\ninsert into the linked list. The Precedes query can be implemented simply by starting at X and walking\nforward in the list until hitting the end (return false) or Y (return true). While this implementation is\ncorrect, it will result in poor performance for our algorithm. An insert takes O(1) time, but a query takes\nO(n) time, where the size of the list is n.\nInstead of a simple linked list, the SP-order algorithm uses the Dietz and Sleator [3] order maintenance\nstructure with. This structure is essentially a linked list with labels. A query takes O(1) time by simply\ncomparing the labels of two nodes. Inserts, however, are no longer trivial--we may need to relabel elements\nin the list to make room for the new element. Even so, the amortized cost of an insert is still O(1).\nThe SP-order algorithm uses two complementary total-orders to determine whether threads are logically\nparallel. Recall that for any fork-join program, we have a corresponding parse tree. In both orders, the\nnodes in the left subtree of an S-node precede those in the right subtree of the S-node. In the left-to-right\norder, the nodes in the left subtree of a P-node precede those in the right subtree of the P-node. In a\nright-to-left order, the nodes in the right subtree of a P-node precede those in the left. For example, the\nleft-to-right order for the threads in the parse tree shown in Figure 3 would be e0, F1, e1, F2, e2, e3, while the\nright-to-left order is e0, e1, e2, F2, F2, e3.\nFor pedagogical purposes, suppose that the SP-order algorithm takes as input a parse tree. In practice,\none might implement the algorithm by augmenting the fork and join (or, in Cilk, spawn and sync) constructs.\nWe want to execute the program while maintaining relationships between threads. Thus, the algorithm\nperforms a valid walk of the tree, executing the threads of the original program. While walking the tree,\nthe SP-order algorithm code performs Insert operations into the complementary total-orders. The queries\nwould be found in the leaves which represent the computation of the input program. A determinacy-race\ndetector like the Nondeterminator would perform a query on each memory access of the original program.\nThe SP-order algorithm, shown in Figure 4, is a serial algorithm, taking advantage that a fork-join\nprogram can be executed on a single processor without changing the semantics of the program by executing\nthe leaves of the parse tree in a left-to-right order. This algorithm essentially performs a left-to-right preorder\n\nwalkTree(X)\n{\nif X is a leaf\nthen execute(X)\nreturn\ninsert left(X), right(X) after X in Order1\nif X is an S-node\nthen insert left(X), right(X) after X in Order2\nif X is a P-node\nthen insert right(X), left(X) after X in Order2\nwalkTree(left(X))\nwalkTree(right(X))\n}\nboolean inSeries(X, Y )\n{\nif X precedes Y in both orders\nthen return true\nelse return false\n{\nboolean inParallel(X, Y )\n{\nif inSeries(X, Y ) or\ninSeries(Y , X)\nthen return false\nelse return true\n}\nFigure 4: The SP-order algorithm. The walkTree procedure maintains the relationships between threads nodes in\na parse tree. A non-leaf node X has a left child, left(X), and a right child, right(X). If X is an S-node, we insert\nits children in the same order in both order structures. If X is a P-node, we insert its children in different orders in\neach order structure. The inSeries and inParallel procedures return the relationship between two nodes by querying\nthe order structures.\n!\n\"\n#\n!\n!\n$%&'%(\n$%&'%)\n!\n!\n$%&'%(\n$%&'%)\n\"\n#\n*+\n,+\n-+\n\"\n#\nFigure 5: An illustration of the SP-order algorithm at an S-node. (a) shows a simple parse tree with an S-node S\nand two children L and R. (b) shows the order structures before traversing to S. The clouds are the rest of the order\nstructure, which does not change when traversing to S. (c) shows the result of the inserts after traversing to S. The\nleft child L then the right child R are inserted after S in both lists.\n\n!\n\"\n#\n!\n!\n$%&'%(\n$%&'%)\n!\n!\n$%&'%(\n$%&'%)\n\"\n#\n*+\n,+\n-+\n\"\n#\nFigure 6: An illustration of the SP-order algorithm at a P-node. (a) shows a simple parse tree with an P-node P\nand two children L and R. (b) shows the order structures before traversing to P. The clouds are the rest of the order\nstructure, which does not change when traversing to P. (c) shows the result of the inserts after traversing to P. The\nleft child L then the right child R are inserted after P in Order1, and R then L are inserted after P in Order2.\ntraversal of the parse tree while maintaining two order structures, Order1 and Order2. We maintain the\nfollowing invariant about these order structures.\n- Order1 represents a left-to-right order of the parse tree.\n- Order2 represents a right-to-left order of the parse tree.\nWhen traversing to a node X, we perform one of three different actions depending on what type of node X\nis:\nX is a leaf X just represents a thread in the original program. Just execute it.\nX is an S-node Insert the left then right child of X after X in both order structures.\nX is a P-node Insert the left then right child of X after X in the Order1 (left-to-right) order structure.\nInsert the right then left child of X after X in the Order2 (right-to-left) order structure.\nFigures 5 and 6 illustrate the inserts for an S-node and P-node respectively.\nTo determine the relationship of two threads e1 and e2, we simply compare the relationship of both\nthreads in both orders. If Precedes(e1, e2) in both orders, then e1 ≺e2. We prove the correctness of this\nalgorithm in Section 4.\nFinally, we analyze the asymptotic running time of the serial SP-order algorithm.\nTheorem 1 Consider a fork-join program that executes in T1 time on a single processor. Then the SP-order\nalgorithm executes the same computation while maintaining thread relationships in O(T1) time.\nProof\nIf there are t threads in the original computation, the parse tree contains t -1 non-leaf nodes.\nThus, a walk of the tree takes O(t). Obviously, there can not be more than O(T1) threads, so the tree walk\ntakes O(T1) time.\n\nWhen traversing to each non-leaf node, we insert each child into two order maintenance structures. In\nother words, we insert each node, exactly once, into two structures. Thus, in total, we perform 2(2t -1) =\nO(T1) inserts. Using a Dietz and Sleator structure, the amortized cost of all the inserts is O(T1).\nA leaf node is just a thread in the original program. At a leaf node, we simply execute the thread. Since\nwe walk to each leaf exactly once, the computation time is O(T1). In general, one would augment the original\nprogram to include some number of queries to determine thread relationships. As long as the augmentation\ninserts no more than one query for each line of code, we execute O(T1) queries, each taking a constant time.\nCombining all three of these asymptotic bounds, we get a total of O(T1) for the SP-order algorithm.\nCorrectness of the SP-order algorithm\nIn this section, we prove the correctness of the SP-order algorithm. First, we show that the series-parallel\nrelationship between threads can be inferred from the parse tree. Then we show that our queries into both\norders do in fact determine the correct relationship between threads. Finally, we show that the SP-order\nalgorithm maintains the left-to-right and right-to-left orders correctly.\nWe have the following properties of series-parallel parse trees from Feng and Leiserson:\nLemma 2 Let e1 and e2 be distinct threads in a series-parallel DAG, and let LCA(e1, e2) be their least\ncommon ancestor in a parse tree for the DAG. Then, e1 ∥e2 if and only if LCA(e1, e2) is a P-node.\nCorollary 3 Let e1 and e2 be distinct threads in a series-parallel DAG, and let LCA(e1, e2) be their least\ncommon ancestor in a parse tree for the DAG. Then, e1 ≺e2 if and only if LCA(e1, e2) is an S-node, e1 is\nin the left subtree of LCA(e1, e2), and e2 is in the right subtree of LCA(e1, e2).\nNext, we show that given a left-to-right and right-to-left total order of the DAG, we can determine the\nrelationship between two threads.\nLemma 4 Consider a series-parallel DAG G and the corresponding parse tree. Consider also the left-to-\nright and right-to-left total orders as defined in Section 3. Then for any two threads e1 and e2 in the DAG,\ne1 ≺e2 if and only if e1 precedes e2 in the left-to-right and right-to-left orders.\nProof\n(=⇒) Suppose e1 ≺e2 in the DAG G. Then from Corollary 3, we know that LCA(e1, e2) is an\nS-node S in the parse tree, e1 is in the left subtree of S, and e2 is in the right subtree of S. By definition of\nboth orders, the nodes in the left subtree of an S-node precede those in the right subtree, so e1 precedes e2\nin both the left-to-right and right-to-left orders.\n(⇐=) Suppose e1 precedes e2 in both the left-to-right and right-to-left total orders of the DAG. Let X\nbe the LCA(e1, e2).\nSince e1 appears before e2 in the left-to-right order, e1 must appear in the left subtree of X, and e2 must\nappear in the right subtree of X.\nAssume for the sake of contradiction that X is not an S-node. Then X is a P-node. Since e1 pre-\ncedes e2 in the right-to-left order, e1 must appear in the right subtree of X and e2 must appear in the left\nsubtree of X, which generates a contradiction. Thus, we can apply to Corollary 3 to conclude that e1 ≺e2.\nCorollary 5 Consider a series-parallel DAG G and the corresponding parse tree. Consider also the left-\nto-right and right-to-left orders. then for any two threads e1 and e2 in the DAG, e1 ∥e2 if and only if e1\nprecedes e2 in one order, but e2 precedes e1 in the other.\n\nProof\nThis corollary follows from Lemma 4.\nFinally, we show that the SP-order algorithm shown in Figure 4 correctly maintains the series-parallel\nrelationships between threads. First, we have a lemma proving this algorithm maintains the left-to-right\nand right-to-left orders. We finish this section with a theorem stating that the SP-order-algorithm queries\nreturn the correct results.\nLemma 6 Consider an execution of the SP-order algorithm shown in Figure 4. Suppose x and y are two\nnodes in the parse tree that have already been inserted into the order-maintenance structures. Then x precedes\ny in Order1 if and only if x precedes y in the left-to-right order of the parse tree. Similarly, x precedes y in\nOrder2 if and only if x precedes y in the right-to-left order of the parse tree.\nProof\nWe use induction on the depth of the parse tree.\nFor a base case, consider the start of the computation (depth of 0 in the parse tree). Both structures are\ninitialized to contain only the root of the tree.\nFor the inductive step, we assume that the lemma holds for all nodes inserted before executing walkTree(z).\nWe need to consider the case of Order1 and Order2 separately. The Order2 case is more complicated, so we\nconsider that first. The proof for Order1 is similar.\nConsider Order2 at walkTree(z). Suppose that x is any node other than z that is already in Order2.\nThen x precedes z in Order2 if and only if x precedes z in the right-to-left order. Let a be the least common\nancestor of x and z in the parse tree.\nCase 1: Suppose that a is an S-node and that x precedes z in Order2. Then by definition of the right-to-left\norder, x is in the left subtree of a, and z is in the right subtree of a. Thus, left(z) and right(z) are also in\nthe right subtree of a, so left(z) and right(z) follow x in the right-to-left order. Inserting left(z) and right(z)\nafter z is consistent with the right-to-left order.\nCase 2: Suppose that a is an S-node and that z precedes x in Order2. Then z is in the left subtree of a\nand x is in the right subtree of a. Thus, left(z) and right(z) precede x in the right-to-left order. Inserting\nleft(z) and right(z) immediately after z in Order2 means that they are inserted somewhere before x, which\nis consistent with the right-to-left order.\nCase 3: Suppose that a is a P-node and that x precedes z in Order2. Then x is in the right subtree and a\nand z is in the left subtree of a. The proof for this case is similar to Case 2.\nCase 4: Suppose that a is a P-node and that z precedes x in Order2. Then z is in the right subtree of a and\nx is in the left subtree of a. The proof for this case is similar to Case 1.\nSince we do not move any nodes in the order structures, relationships between any two nodes x and y do\nnot change on walkTree(z).\nSo far we have shown that for any node x already in the order structures and either child of z, the\nrelationships between x and the child of z are the same in the left-to-right order and Order1 and for the\nright-to-left order and Order2. For completeness, we also need to show that the relationships between the\ntwo children of z also match. This fact is obvious given the algorithm. For Order1, left(z) is always inserted\nbefore right(z). For Order2, left(z) is inserted before right(z) if z is an S-node, but right(z) is inserted before\nleft(z) if z is a P-node.\nTheorem 7 Consider an execution of the SP-order algorithm. Consider any two threads e1 and e2 that\nhave already executed or are currently being executed. Then e1 ≺e2 if and only if e1 precedes e2 in Order1\nand Order2. Similarly, e1 ∥e2 if and only if e1 precedes e2 in exactly one of the orders.\nProof\nThe SP-order algorithm always inserts a node x into the order structures before executing walkTree(x).\nThus, any thread is in the order structures before it is executed. Therefore, we can apply Lemma 4, Corol-\nlary 5 and Lemma 6 to prove this theorem.\n\ncilk void walkTree(X)\n{\nif X is a leaf\nthen execute(X)\nreturn\ninsert left(X), right(X) after X in Order1\nif X is an S-node\nthen insert left(X), right(X) after X in Order2\nspawn walkTree(left(X))\nsync\nspawn walkTree(right(X))\nsync\nif X is a P-node\nthen insert right(X), left(X) after X in Order2\nspawn walkTree(left(X))\nspawn walkTree(right(X))\nsync\n}\nFigure 7: A naive implementation of the parallel-SP-order algorithm, based on the SP-order algorithm shown in\nFigure 4. At an S-node, the left child must execute before the right child. At a P-node, both children may execute\nin parallel.\nThe parallel-SP-order algorithm\nIn this section, we look at parallelizing the SP-order algorithm. First, we notice that the correctness of\nthe algorithm is not a difficulty to overcome. We propose a slight variation of the SP-order algorithm to\ncreate the parallel-SP-order algorithm. For efficient asymptotic running time, however, we need an efficient\nconcurrent-order-maintenance structure, which we will discuss in Section 7.\nIf we look at the proofs for Theorem 7 and Lemma 6, we see that they do not rely on the serial execution\nof the SP-order algorithm. Thus, we can perform any valid serial or parallel walk of the parse tree while\nmaintaining the same correctness properties. A valid walk of the parse tree is one that reflects a valid\nexecution of the program. That is to say, a thread is not executed before any of the threads it depends on.\nThus, at a P-node, both children can execute in parallel, but at an S-node, we need to execute the left child\nbefore the right child. Figure 7 shows the parallel-SP-order algorithm, with Cilk-like syntax, performing a\nvalid walk of the parse tree.\nThe parallel-SP-order algorithm depends on a concurrent-order-maintenance structure. A naive imple-\nmentation of this structure would be simply to lock the entire structure on every insert. While this approach\ndoes yield a correct implementation, it can perform quite poorly. If there are Θ(T1) threads in the program,\nthen an insert into one of the order structures may need to relabel Θ(T1) nodes. Thus, the critical path of\nthe computation can increase to T1, yielding a very poor performance of Ω(T1/P + T1) = Ω(T1) in the worst\ncase.\nThe biggest hit to performance with the naive implementation of the concurrent-order-maintenance struc-\nture is the size of the list--if we bound the size of the order structure to less than T1, we can significantly\nimprove the performance.\n\nCilk scheduler\nIn this section, we review the Cilk scheduler [1], which uses a provably good work-stealing algorithm. Then\nwe present a bound of O(PTinf) steal attempts with high probability.\nThe Cilk scheduler uses a ready deque for each processor to queue up threads that can be stolen. Threads\nare always inserted into the deque on the bottom of the deque, but they can be deleted from either end. A\nprocessor p works on a procedure α until α\n- spawns procedure β: p working pushes the next thread in α on the ready deque and starts working\non the procedure β.\n- returns:\n- if the deque is nonempty, p pops a thread from the bottom of its deque and begins working on it.\n- if the deque is empty and no processor is working on α's parent--the procedure that spawned α--\nthen p begins resumes working on α's parent.\n- if the deque is empty and α's parent is busy, then work steal.\n- syncs: If α has outstanding children, then work-steal. Note that the deque is empty in this case.\nWhen a processor p tries to steal, it operates as follows:\n- p chooses a victim uniformly at random.\n- if the victim's deque is empty, p tries to steal again.\n- if the victim's deque is non-empty, p steals the top thread of the victim's deque and begins working on\nit.\nOne important property of the Cilk scheduler is that we can bound the number of steals attempts to\nO(PTinf). We state the following lemma from Blumofe /citecilkscheduler without proof:\nLemma 8 Consider an execution of any fully strict multithreaded computation with critical path Tinfby the\nWork-Stealing Algorithm on a parallel computer with P processors. For any ε > 0, with probability at least\n1 -ε, at most O(P(Tinf+ lg(1/ε))) work-steal attempts occur. The expected number of steal attempts is\nO(PTinf).\nConcurrent order-maintenance of Cilk threads\nIn this section, we consider how to apply the Cilk work-stealing algorithm with a bounded number of steals\nto the parallel-SP-order algorithm. First, we look at what a steal means in terms of the parse tree. Then\nwe propose a modified order-maintenance structure to use for the parallel-SP-order algorithm. Finally, we\nrewrite the algorithm to more explicitly manage the data structures.\nFirst, let us consider a steal attempt in the parallel-SP-order algorithm shown in Figure 7. The procedure\nwalkTree(X) essentially only has one point at which a steal can occur: walkTree(right(X)).\nWe define a subcomputation to be a largest subtree of the original parse tree such that:\n- Every node in the subtree that has been traversed has been executed by the same processor.\n- Suppose X is a node in the subcomputation. Then the left child of X is in the subcomputation if and\nonly if the right child of X is in the subcomputation.\n\n!\n\"\n#\n$%\n!\n\"\n#\n$%\n$%\n$&\n'(\n)(\nFigure 8: An illustration of the subcomputations created through a steal in terms of the parse tree. (a) shows a\nsingle subcomputation begin working on by the processor p1. In (b), the processor p2 steals walkTree(right(X))\nfrom the processor p1. There are now three subcomputations.\n!\n\"\n#\n$%\n!\n\"\n#\n&%\nFigure 9: An illustration of the two-level order-maintenance structure representing the left-to-right order of the\nparse tree generated from the subcomputations shown in Figure 8. The ordering of subcomputations is shown in\nthe top level of each list, while the lower level shows the ordering of nodes within a subcomputation. The clouds\nindicate other nodes in the list we are not focusing on. a) shows the single subcomputation before the steal. In (b),\nwe see that the subcomputation splits into three subcomputations, and new nodes are inserted into the top level to\nrepresent the new subcomputations.\n\nFor example, at the start of the computation, only the root node has been traversed, so the entire tree is a\nsubcomputation. Once a steal occurs, however, we create more subcomputations. Figure 8 illustrates the\nsubcomputations created in a steal. At first, p1 owns the entire subcomputation rooted at the node X. Then,\np2 steals from p1. The procedure walkTree(right(X)) must be at the top of p1's deque at this point, so\np2 steals the right subtree of X. Thus, we now have three subcomputations.\nNow, let us revisit the concurrency problem of the order-maintenance structure. By definition, only one\nprocessor handles all the inserts related to a single subcomputation. Thus, we propose a two level oder-\nmaintenance structure. The top level is the concurrent order-maintenance structure that locks on every\ninsert, while the bottom level is the regular, serial, Dietz and Sleator order-maintenance structure ordering\nthe nodes within a subcomputation. On a steal, we split a subcomputation and perform an insert into the\ntop level structure. Figure 9 shows the left-to-right order structure generated by subcomputations shown in\nFigure 8.\nIn this new scheme, all the nodes in the parse tree occur within a subcomputation. That is to say, the\nnodes of the parse tree are on the bottom level in the order structure. To query the relationship between\ntwo nodes, we just need to query the relationship between the subcomputations in the top level. If both\nnodes belong to the same subcomputation, we query the relationship between the nodes in the lower level\norder structure.\nNext, we rewrite the parallel-SP-order algorithm in Figure 10 to deal with the new data structure. We\nnow allow inserts before or after an element in the order structures. Since we have a lot of different order\nstructures around, data types can get confusing. Thus, we are more explicit about typing the variables.\nAnalysis of the parallel-SP-order algorithm\nIn this section, we show that the parallel-SP-order algorithm executes in O(T1/P + PTinf) time on P pro-\ncessors, where T1 is the work and Tinfis the critical-path.\nTheorem 9 The parallel-SP-order algorithm shown in Figure 10 executes in O(T1/P + PTinf) time with\nhigh probability, where P is the number of processors, T1 is the work, and Tinfis the critical path.\nProof\nApplying Lemma 8, there are 2 · O(PTinf) = O(PTinf) subcomputations. Each subcomputation is\ninserted into the top-level, concurrent, order-maintenance structure. The entire top-level structure is locked\non each insert. Let us suppose that all other processors stop working entirely on an insert into the top-level\nstructure. Then we have O(PTinf) inserts occurring serially, taking a total of O(PTinf) time.\nAs for the inserts into the lower level, there can be O(T1) threads, so there can be O(T1) inserts. However,\nthese inserts always occur within a subcomputation, so there is no locking involved. Thus, we increase the\nwork by O(T1).\nAdding up these two asymptotic bounds, we get a total running time of O(T1/P + PTinf).\nConclusions\nWe have presented two efficient algorithms for determining series-parallel thread relationships. The serial\nversion of the SP-order algorithm is asymptotically optimal.\nWe do not prove the Cilk, parallel-SP-order algorithm to be asymptotically optimal, but it still quite\nefficient. It has several advantages which are not reflected in the upper bound shown in Section 8. First,\nthe upper bound is a worst case scenario. For normal programs, we would expect steals to be more evenly\ndistributed. If the steals occur more evenly across the parse tree, then we do not need to relabel the top-level\nlist as many times. Furthermore, given that we expect there to be quite a bit of computation in the average\nthread, we would not expect there to be P processors trying to perform concurrent inserts very frequently.\n\nOrderNode* newOrder(void *data);\nOrderNode* insertBefore(OrderNode* X, void *data);\nOrderNode* insertAfter(OrderNode* X, void *data);\ncilk void walkTree(TreeNode *X)\n{\nif (leaf(X)) {\nexecute(X);\nreturn;\n}\nTreeNode *L = left(X);\nTreeNode *R = right(X);\nL->Order1 = insertAfter(X->Order1, L);\nR->Order2 = insertAfter(L->Order1, R);\nif (SNode(X)) {\nL->Order2 = insertAfter(X->Order2, L);\nR->Order2 = insertAfter(L->Order2, R);\nspawn walkTree(left(X));\nsync;\nspawn walkTree(right(X));\nsync;\n}\nif (PNode(X)) {\nR->Order2 = insertAfter(X->Order2, R);\nL->Order2 = insertAfter(R->Order2, L);\nspawn walkTree(left(X));\nif (!SYNCHED) {\n/* a steal happened */\n/* split the subcomputation for Order1\nOrderNode *subc1 = newOrder(X);\nOrderNode *subc2 = newOrder(R);\nX->Order1 = insertBefore(parent(L->Order1), subc1);\nR->Order1 = insertAfter(parent(L->Order1), subc2);\n/* split the subcomputation for Order2\nsubc1 = newOrder(X);\nsubc2 = newOrder(R);\nX->Order2 = insertBefore(parent(L->Order1), subc1);\nR->Order2 = insertBefore(parent(L->Order1), subc2);\n}\nspawn walkTree(right(X));\nsync;\n}\nFigure 10: The efficient parallel-SP-order algorithm. This implementation is similar to that shown in Figure 7\nexcept that we add the \"if (!SYNCHED)\" block to execute on a steal. On a steal, we create two new subcomputations\nrepresenting the node X and the node R. In both orders, we insert the subcomputation for X before that for L. In\nOrder1, R comes after L. In Order2, it comes before.\n\nFinally, our the label length is constant. Thus, queries always take constant time. This fact is important\nsince a determinacy race detector may perform O(T1) queries.\nRelated Work\nNudler and Rudolph [6] present an English-Hebrew labeling for fork-join programs. Each thread is assigned\ntwo labels, similar to our left-to-right and right-to-left labeling schemes.\nThey do not, however, use a\ncentralized data structure. Instead, label size can grow proportionally to the maximum concurrency of the\nprogram.\nMellor-Crummey [5] proposes an \"offset-span labeling\" for fork-join programs that has shorter label\nlengths than the English-Hebrew scheme. However, the offset-span solution still has label lengths propor-\ntional to the maximum fork nesting.\nReferences\n[1] Robert D. Blumofe. Executing Multithreaded Programs Efficiently. PhD thesis, Department of Electrical\nEngineering and Computer Science, Massachusetts Institute of Technology, 1995.\n[2] Robert D. Blumofe, Christopher F. Joerg, Bradley C. Kuszmaul, Charles E. Leiserson, Keith H. Randall,\nand Yuli Zhou.\nCilk: An efficient multithreaded runtime system.\nIn Proceedings of the Fifth ACM\nSIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), pages 207-216,\nSanta Barbara, California, July 1995.\n[3] Paul F. Dietz and Daniel D. Sleator. Two algorithms for maintaining order in a list. In STOC, pages\n365-372, 1987.\n[4] Charles. E. Leiserson and Mingdong Feng. Efficient detection of determinacy races in cilk programs. In\nProceedinges of the Ninth Annual ACM Symposium on Parallel Algorithms and Architectures (SPAA),\npages 1-11, Newport, Rhode Island, June 1997.\n[5] John Mellor-Crummey. On-the-fly detection of data races for programs with nested fork-join parallelism.\nIn Proceedings of Supercomputing '91, pages 24-33, 1991.\n[6] Itzhak Nudler and Larry Rudolph. Tools for the efficient development of efficient parallel programs. In\nProceedings of the First Israeli Conference on Computer Systems Engineering, May 1986."
    },
    {
      "category": "Resource",
      "title": "fp_advait_sriram.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/f1d9226744f771f7cbdab75e3f34be91_fp_advait_sriram.pdf",
      "content": "Cache-efficient string sorting for\nBurrows-Wheeler Transform\nAdvait D. Karande\nSriram Saroop\n\nWhat is Burrows-Wheeler Transform?\nA pre-processing step for data\ncompression\nInvolves sorting of all rotations of the\nblock of data to be compressed\nRationale: Transformed string\ncompresses better\n\nBurrows-Wheeler Transform\ns : abraca\ns' : caraab, I=1\naabrac\nabraca\nacaabr\nbracaa\ncaabra\nracaab\n\nSuffix sorting for BWT\nSuffix array construction\nEffect of block size, N\nHigher N -> Better compression\n-> Slower SA construction\nSuffix array construction algorithms\nQuick sort based O(N2(logN)) : Used in bzip2\nLarsson-Sadakane algorithm : Good worst case\nbehavior [O(NlogN) ]\nPoor cache performance\n\nWhat we did\nImplemented cache-oblivious distribution sort [Frigo,\nLeiserson, et al] and used it in suffix sorting.\nFound to be a factor of 3 slower than using qsort based\nimplementation.\nDeveloped a cache-efficient divide and conquer suffix\nsorting algorithm.\nO(N2lgN) time and 8N extra space\nImplemented an O(n) algorithm for suffix\nsorting[Aluru and Ko 2003].\nFound to be a factor of 2-3 slower than the most efficient\nsuffix sorting algorithm available.\n\nIncorporating cache-oblivious\nDistribution Sort\nSadakane performs sorting based on 1\ncharacter comparisons\nIncorporate cache-oblivious distribution\nsorting of integers1.\nIncurs Ө(1+(n/L)(1+logZ n))cache misses\n1.Matteo Frigo, Charles E. Leiserson, Harald Prokop, Sridhar Ramachandran\nCache-Oblivious Algorithms FOCS 1999\n\nAlgorithm\n1. Partition A into √n contiguous subarrays of size √n.\nRecursively sort each subarray.\n2. Distribute sorted subarrays into q ≤ √n buckets B1, B2,...\nBq of size n1, n2,..., nq respectively, $ for i=[1,q-1]\na. max{x|x ε Bi} ≤ min{x|x ε Bi+1},\nb. ni ≤ 2 √n\n3. Recursively sort each bucket.\n4. Copy sorted buckets back to array A.\n\nBasic Strategy\nCopy the element at position next of a\nsubarray to bucket bnum.\nIncrement bnum until we find a bucket for\nwhich element is smaller than pivot .\nnext\n\nBucket found...\nnext\n\nPerformance\nBlock Size\nTime (in secs)\nQsort\n0.321\n1.252\n2.428\n4.679\n12.342\nMergeSort\n0.491\n1.171\n2.623\n5.808\n16.833\nDistriSort\n0.811\n1.993\n4.807\n14.13\n36.657\n128KB\n256KB\n512KB\n1MB\n2MB\n\nRestrictions\nmallocs caused by repeated bucket-splitting.\nNeed to keep track of state information for buckets\nand sub-arrays\nBuckets, subarrays, copying elements back and forth\nincur memory management overhead.\nSplitting into √n * √n subarrays, when n is not a\nperfect square causes rounding errors.\nRunning time may not be dominated by cache\nmisses.\n\nDivide and conquer algorithm\n\nSimilar to merge sort\n\nSuffix sorting : From right to left\n\nStores match lengths to avoid repeated comparisons in the\nmerge phase\nsort(char *str, int *sa, int *ml, int len){\nint mid = len/2;\nif(len <=2){\n...\n}\n...\nsort(&str[mid], &sa[mid], &ml[mid], len-mid);\nsort(str, sa, ml, mid);\nmerge(s, sa, ml, len);\n}\n\nDivide and conquer algorithm\nSuffix array\na\na\na\na\na\na\na\n\\0\ns\nSA\n\nDivide and conquer algorithm\nSort phase\na\na\na\n\\0\na\n\\0\na\na\na\na\na\na\na\n\\0\ns\nlast_match_length=0\nSuffix array\nMatch lengths\n\nDivide and conquer algorithm\nSort phase\na\na\na\n\\0\na\n\\0\na\na\na\na\na\na\na\n\\0\ns\nlast_match_length=0\nSuffix array\nMatch lengths\n\nDivide and conquer algorithm\nSort phase\na\na\na\n\\0\na\n\\0\na\na\na\na\na\na\na\n\\0\ns\nlast_match_length=0\na\na\nSuffix array\nMatch lengths\n\nDivide and conquer algorithm\nSort phase\na\na\na\n\\0\na\n\\0\na\na\na\na\na\na\na\n\\0\ns\nlast_match_length=0\na\na\nSuffix array\nMatch lengths\n\nDivide and conquer algorithm\nSort phase\na\na\na\n\\0\na\n\\0\na\na\na\na\na\na\na\n\\0\ns\nlast_match_length=0\na\na\nSuffix array\nMatch lengths\n\nDivide and conquer algorithm\nSort phase\na\na\na\n\\0\na\n\\0\na\na\na\na\na\na\na\n\\0\ns\nlast_match_length=2\na\na\nSuffix array\nMatch lengths\n\nDivide and conquer algorithm\nMerge phase\na\na\na\n\\0\nOld suffix array\nOld match lengths\nNew suffix array\nNew Match lengths\n\nDivide and conquer algorithm\nMerge phase\na\na\na\n\\0\nNew suffix array\nNew Match lengths\nOld suffix array\nOld match lengths\n\nDivide and conquer algorithm\nMerge phase\na\na\na\n\\0\nNew suffix array\nNew Match lengths\nOld suffix array\nOld match lengths\n\nDivide and conquer algorithm\nMerge phase\na\na\na\n\\0\nNew suffix array\nNew Match lengths\nOld suffix array\nOld match lengths\n\nDivide and conquer algorithm\nMerge phase\na\na\na\n\\0\nNew suffix array\nNew Match lengths\nOld suffix array\nOld match lengths\n\nDivide and conquer algorithm\nMerge phase\nAnother example - large match lengths\na\na\na\na\na\na\na\n\\0\nSA\n\nDivide and conquer algorithm\nMerge phase\nAnother example - large match lengths\na\na\na\na\na\na\na\n\\0\nSA\n\nDivide and conquer algorithm\nMerge phase\nAnother example - large match lengths\na\na\na\na\na\na\na\n\\0\nSA\n\nDivide and conquer algorithm\nMerge phase\nAnother example - large match lengths\na\na\na\na\na\na\na\n\\0\nSA\n\nDivide and conquer algorithm\nAnalysis\nTime complexity\nO(N2lgN)\nSpace complexity\n8N extra space\nO(NlgN) random I/Os\n\nDivide and conquer algorithm\nPerformance\nComparison with qsufsort1 algorithm for\nsuffix sorting\nTime: O(NlogN) time\nSpace: 2 integer arrays of size N\n1N. Larsson & K.Sadakane, Faster Suffix Sorting. Technical Report, LU_CS-TR-99-214,\nDept. of Computer Science, Lund University, Sweden, 1999\n\nDivide and conquer Vs qsufsort\nBlocksize, N (=datasize)\nTime (sec.)\nd&c\n0.25\n0.541\n1.222\n2.854\n6.439\n14.39\nqsufsort\n0.241\n0.531\n1.232\n2.724\n6.009 13.169\n256K\n512K\n1M\n2M\n4M\n8M\n- Based on Human Genome data set\n- Pentium4 2.4 GHz, 256MB RAM\n\nDivide and conquer algorithm\nCache performance\n\nBlocksize N = 1,048,576\n\nInput file: reut2-013.sgm (Reuters corpus) [1MB]\nqsufsort\nd&c\n# data references\n525,305K\n1,910,425K\nL1 data cache misses\n14,614K\n13,707K\ncache miss ratio\n2.7%\n0.7%\n- 16KB L1 set associative data cache\n\nDivide and conquer algorithm\nCache performance\n\nBlocksize N = 1,048,576\n\nInput file: nucall.seq (Protein sequence) [890KB]\nqsufsort\nd&c\n# data references\n531,201K\n2,626,356K\nL1 data cache misses\n16,323K\n12,945K\ncache miss ratio\n3.0%\n0.4%\n- 16KB L1 set associative data cache\n\nDivide and conquer algorithm\nRequires no memory parameters to be set\nGood cache behavior\nExtensive testing with different types of files\nis needed to evaluate its utility\n\nLinear time construction of Suffix\nArrays1\n1Pang Ko and Srinivas Aluru, Space Efficient Linear Time Construction of Suffix Arrays\n\nClassify as type S or L\nT\nM\nI\nS\nS\nI\nS\nS\nI\nP\nP\nI\n$\nType\nL\nS\nL\nL\nS\nL\nL\nS\nL\nL\nL\nS\nPos\n10 11 12\nS: Ti < Ti+1\nL: Ti+1 < Ti\n\nSorting Type S suffixes\nT\nM\nI\nS\nS\nI\nS\nS\nI\nP\nP\nI\n$\nType\nL\nS\nL\nL\nS\nL\nL\nS\nL\nL\nL\nS\nPos\n10 11 12\nOrder of Type S\nsuffixes:\n\nSorting Type S suffixes\nT\nM\nI\nS\nS\nI\nS\nS\nI\nP\nP\nI\n$\nType\nL\nS\nL\nL\nS\nL\nL\nS\nL\nL\nL\nS\nPos\n10 11 12\nDist\n3 6\n2 5 8\n10 4 7\n5 8 11\n12 8\n\nBucket acc. to first character\nT\nM\nI\nS\nS\nI\nS\nS\nI\nP\nP\nI\n$\nType\nL\nS\nL\nL\nS\nL\nL\nS\nL\nL\nL\nS\nPos\n10 11 12\n$\nI\nM\nP\nS\n12 2 5 8 11 1 9 10\n3 4 6 7\nSuffix Array\nOrder of Type S\nsuffixes\n\nObtaining the sorted order\n2 5 8 11\n9 10\n3 4 6 7\n11 8 5 2\n9 10\n3 4 6 7\nType S\nsuffixes : B\nMove to end\nof Bucket as\nper B\n12 11\nScan L to R- Move type L suffixes to front of bucket\n\nImplementation Results\nSize of the\nfile\nLinear Algo\nqsufsort\n8KB\n0.02s\n0.005s\n16KB\n0.047s\n0.01s\n100KB\n0.263s\n0.06s\n500KB\n1.352s\n0.551s\n1MB\n2.963s\n1.292s\n2MB\n6.963s\n2.894s\nFile used:\nGenome\nChromosome\nsequence\nBlock Size =\nSize of file\n\nPerformance\nFile Size\nTIme (in secs)\nLinear Algo\n0.02\n0.047\n0.263\n1.352\n2.963\n6.963\nqsufsort\n0.005\n0.01\n0.06\n0.551\n1.292\n2.894\n8KB\n16KB\n100KB\n500KB\n1MB\n2MB\n\nObservations\nUsing 3 integer arrays of size n, 3 boolean arrays of\n(2 of size n, 1 of size n/2)\nGives rise to 12n bytes plus 2.5 bits, in comparison to\n8n bytes used by Manber and Myers' O(n log n)\nalgorithm →Trade-off between time and space.\nImplementation still crude. Further optimizations\npossible.\nAn extra integer array to store the Reverse positions\nof the Suffix array in the string improves\nperformance.\n\nConclusions\nThe cache-oblivious Distribution Sort based suffix\nsorting incurs memory management overheads.\n\nFactor of 3 to 4 slower than qsort based approach.\nOur Divide and conquer algorithm is cache-efficient\nand requires no memory parameters to be set.\nO(N2lgN) time and 8N extra space\nLinear time suffix sorting algorithm's performance\ncan be improved further. Requires more space.\nFactor of 2 slower than qsufsort."
    },
    {
      "category": "Resource",
      "title": "fp_ananian.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/3e1136d12262c073888fb5ebc5e709a2_fp_ananian.pdf",
      "content": "Verifying Software Transactions\nC. Scott Ananian\nComputer Science and Artifical Intelligence Laboratory\nMassachusetts Institute of Technology\nAnanian, 6.895 - p. 1\n\nOutline\n-\n-\n-\n- Conclusions\nConcurrency control with non-blocking\ntransactions (review)\nIntroduction to the Spin Model Checker\nModelling a software transaction implementation\nAnanian, 6.895 - p. 2\n\nNon-blocking Transactions\nAnanian, 6.895 - p. 3\n\n-\ncommits or\n.\n-\natomically.\n-\n-\nTransactions (review)\nA transaction is a sequence of loads and stores\nthat either\naborts\nIf a transaction commits, all the loads and store\nappear to have executed\nIf a transaction aborts, none of its stores take\neffect.\nTransaction operations aren't visible until they\ncommit or abort.\nAnanian, 6.895 - p. 4\n\n-\nin\n-\n-\n-\n-\n-\nNon-blocking synchronization\nAlthough transactions can be implemented with\nmutual exclusion (locks), we are interested only\nnon-blocking implementations.\nIn a non-blocking implementation, the failure of\none process cannot prevent other processes from\nmaking progress. This leads to:\nScalable parallelism\nFault-tolerance\nSafety: freedom from some problems which\nrequire careful bookkeeping with locks,\nincluding priority inversion and deadlocks.\nLittle known requirement: limits on transaction\nsuicide.\nAnanian, 6.895 - p. 5\n\n-\n- One\n- One\n- One\n-\nNon-blocking algorithms are hard!\nIn published work on Synthesis, a non-blocking\noperating system implementation, three separate\nraces were found:\nABA problem in LIFO stack.\nlikely race in MP-SC FIFO queue.\ninteresting corner case in quaject\ncallback handling.\nIt's hard to get these right! Ad hoc reasoning\ndoesn't cut it.\nAnanian, 6.895 - p. 6\n\nThe Spin Model Checker\nAnanian, 6.895 - p. 7\n\n-\n-\n-\n-\n-\nfinite\ninfinite\n-\n-\nThe Spin Model Checker\nSpin is a model checker for communicating\nconcurrent processes. It checks:\nSafety/termination properties.\nLiveness/deadlock properties.\nPath assertions (requirements/never claims).\nIt works on\nmodels, written in the Promela\nlanguage, which describe\nexecutions.\nExplores the entire state space of the model,\nincluding all possible concurrent executions,\nverifying that Bad Things don't happen.\nNot an absolute proof -- but pretty useful in\npractice.\nAnanian, 6.895 - p. 8\n\nint turn;\nint wants[2];\n// i is the current\nis the other thread\nwants[i] = TRUE;\nwhile\nif\nwants[i] = FALSE;\nwhile\nwants[i] = TRUE;\n}\n}\ncritical_section();\nturn=j;\n// release\nwants[i] = FALSE;\nnoncrit();\n}\nDekker's mutex algorithm (C)\nthread, j=1-i\nwhile(1) {\n// trying\n(wants[j]) {\n(turn==j) {\n(turn==j) ; // empty loop\nAnanian, 6.895 - p. 9\n\nDekker's \"railroad\"\nRailroad visualization of Dekker's algorithm for mutual\nexclusion. The threads \"move\" in the direction shown\nby the arrows. [from lecture 5 scribe notes]\nAnanian, 6.895 - p. 10\n\nbool\nactive\n/* Dekker's 1965\n{\ni = _pid;\nj = 1 - _pid;\nagain:\nflag[i] = true;\ndo\n/* can be 'if' - says Doran&Thomas */\n:: flag[j] ->\nif\n:: turn == j ->\nflag[i] = false;\n!(turn == j);\nflag[i] = true\n:: else\nfi\n:: else -> break\nod;\n==\ncritical section */\nturn = j;\nflag[i] = false;\n}\nDekker's mutex algorithm (Promela)\nturn, flag[2]; byte cnt;\n[2] proctype mutex()\nalgorithm */\npid i, j;\ncnt++; assert(cnt\n1); cnt--; /*\ngoto again\nAnanian, 6.895 - p. 11\n\n$ spin -a mutex.pml\n$ cc -DSAFETY -o\n$ ./pan\n(Spin Version 4.1.0 - 6 December 2003)\n+ Partial Order Reduction\nFull statespace search for:\nnever claim\n-\nassertion violations\n+\ncycle checks\n-\ninvalid end states\n+\nState-vector 20\nreached 65, errors: 0\n190 states, stored\n173 states, matched\n363 transitions\n0 atomic steps\nhash conflicts: 0 (resolved)\n(max size 2ˆ18 states)\n$\nSpin verification\npan pan.c\n(none specified)\n(disabled by -DSAFETY)\nbyte, depth\n(= stored+matched)\nIf an error is found, will give you execution trail producing the error.\nAnanian, 6.895 - p. 12\n\n-\nspecification.\n-\n-\n-\n- If x\ny\ny\nx\n-\nSpin theory\nGenerates a Buchi Automaton from the Promela\nFinite-state machine w/ special acceptance conditions.\nTransitions correspond to executability of statements.\nDepth-first search of state space, with each state\nstored in a hashtable to detect cycles and prevent\nduplication of work.\nfollowed by\nleads to the same state as\nfollowed\nby , will not re-traverse the succeeding steps.\nIf memory is not sufficient to hold all states, may\nignore hashtable collisions: requires one bit per\nentry. # collisions provides approximate coverage\nmetric.\nAnanian, 6.895 - p. 13\n\nModeling software transactions\nAnanian, 6.895 - p. 14\n\n- Goals:\n-\n-\n-\n- Solution:\n-\nFLAG\n-\n-\nFLAG\nsoftware transaction implementation\nNon-transactional operations should be fast.\nReads should be faster than writes.\nMinimal amount of object bloat.\nUse special\nvalue to indicate \"location involved in\na transaction\".\nObject points to a linked list of versions, containing\nvalues written by (in-progress, committed, or aborted)\ntransactions.\nSemantic value of a\nged field is: \"value of the first\nversion owned by a committed transaction on the\nversion list.\"\nAnanian, 6.895 - p. 15\n\nfield1\nfield2\n3.14159\nFLAG\nfield1\nfield2\nFLAG\n2.71828\nObject #1\nObject #2\nVersion\nfield1\nfield2\nFLAG\nowner\nnext\nVersion\nfield1\nfield2\nFLAG\nowner\nnext\ntype\nreaders\nversions\nOtherClass\ntype\n{OID68}\nMyClass\nreaders\nversions\nstatus\nWAITING\nstatus\nCOMMITTED\nstatus\nCOMMITTED\nVersion\nfield1\nfield2\n'A'\nFLAG\nowner\nnext\nVersion\nfield1\nfield2\n'B'\nFLAG\nowner\nnext\n{OID25}\nTransaction ID #68\nTransaction ID #56\nTransaction ID #23\n. . .\n. . .\n. . .\n. . .\n. . .\n. . .\nTransactions using version lists\nAnanian, 6.895 - p. 16\n\n-\n-\nFLAG\ntime?\n-\nFLAG\n-\n-\n-\n-\nyggdrasil\nRaces, races, everywhere!\nLots of possible races:\nWhat if two threads try to\na field at the same\nWhat if two threads try to copy-back a\nged field at\nthe same time?\nWhat if two transactions perform conflicting updates?\nDo transactions commit atomically?\nFormulated model in Promela and used Spin to\nverify correctness.\nUsed the 16G on memory on\nto good\nadvantage.\nAnanian, 6.895 - p. 17\n\ninline\ndo\n:: v = object[o].field[f];\nif\n:: (v!=FLAG) -> break /* done! */\n:: else\nfi;\nif\n:: (_st==false_flag) ->\nv = FLAG;\nbreak\n:: else\nfi\nod\n}\nNon-transactional Read\nreadNT(o, f, v) {\ncopyBackField(o, f, kill_writers, _st);\nAnanian, 6.895 - p. 18\n\ninline\nif\n:: (nval != FLAG) ->\ndo\n:: atomic {\nif /* this is a\n:: (object[o].readerList == NIL) ->\nobject[o].fieldLock[f] = _thread_id;\nobject[o].field[f] = nval;\nbreak /* success! */\n:: else\nfi\n}\n/* unsuccessful SC */\nod\n:: else -> /* create false\nthis as a short *transactional* write.\n*/\n/* start a new transaction, write FLAG, commit the transaction,\n* repeat until successful.\nImplementation elided. */\nfi;\n}\nNon-transactional Write\nwriteNT(o, f, nval) {\nLL(readerList)/SC(field) */\ncopyBackField(o, f, kill_all, _st)\nflag */\n/* implement\nAnanian, 6.895 - p. 19\n\n_nonceV=NIL; _ver = NIL; _r = NIL; st = success;\nto abort each version.\nwhen abort\ngot a\n*\ndo\n:: _ver = object[o].version;\nif\n:: (_ver==NIL) ->\nst =\n/*\nthe\nus */\n:: else\nfi;\n/* move owner to local var to avoid races (owner set to NIL behind\n* our\nif\n::\n->\nbreak /* found a\n:: else\nfi;\n/* link out an\nod;\nCopy-back Field, part I\ninline copyBackField(o, f, mode, st) {\n/* try\nfails, we've\ncommitted version. */\nsaw_race; break\nsomeone's done\ncopyback for\nback) */\n_tmp_tid=version[_ver].owner;\ntryToAbort(_tmp_tid);\n(_tmp_tid==NIL || transid[_tmp_tid].status==committed)\ncommitted version */\naborted version */\nassert(transid[_tmp_tid].status==aborted);\nCAS_Version(object[o].version, _ver, version[_ver].next, _);\ncontinued. . .\nAnanian, 6.895 - p. 20\n\nlink in our nonce.\nthis will\n*\nif\n:: (st==success) ->\nassert (_ver!=NIL);\nif\n:: (!_cas_stat) ->\nst = saw_race_cleanup\n:: else\nfi\n:: else\nfi;\nCopy-back Field, part II\n/* okay,\nprevent others from doing the\ncopyback. */\nallocVersion(_retval, _nonceV, aborted_tid, _ver);\nCAS_Version(object[o].version, _ver, _nonceV, _cas_stat);\ncontinued. . .\nAnanian, 6.895 - p. 21\n\n/* check that no one's beaten us to the copy back */\nif\n:: (st==success) ->\nif\n::\n->\n_val =\nif\n:: (_val==FLAG) -> /* false\nst =\n...no copy back\n:: else -> /* not a false\nif\n:: (object[o].version == _nonceV) ->\n= _thread_id;\nobject[o].field[f] = _val;\n:: else\nfail.\nMust\nst =\nneed to clean up nonce */\nfi\n}\nfi\n:: else\ndoesn't set _val=FLAG*\nst =\nneed to\nnonce */\nfi\n:: else /*\nfi;\nCopy-back Field, part III\n(object[o].field[f]==FLAG)\nversion[_ver].field[f];\nflag... */\nfalse_flag /*\nneeded */\nflag */\nd_step { /* LL/SC */\nobject[o].fieldLock[f]\n/* hmm,\nretry. */\nsaw_race_cleanup /*\n/* may arrive here because of readT, which\nsaw_race_cleanup /*\nclean up\n!success */\ncontinued. . .\nAnanian, 6.895 - p. 22\n\nkill\nor not.\nThis ensures that we\n* make\ncalled from\na readNT sets readerList\n*\nto\nst will\n*\nthis\nif\n:: (mode == kill_all) ->\ndo /* kill all\n::\nif\n:: (_r==NIL) -> break\n:: else\nfi;\n/* link out this reader */\n_r,\nod;\n:: else /* no more\nfi;\n/* done */\n}\ndone!\nCopy-back Field, part IV\n/* always\nreaders, whether successful\nprogress if\nwriteNT after\nnon-null without changing FLAG\n_val (see immediately above;\nequal saw_race_cleanup in\nscenario). */\nreaders */\nmoveReaderList(_r, object[o].readerList);\ntryToAbort(readerlist[_r].transid);\nCAS_Reader(object[o].readerList,\nreaderlist[_r].next, _);\nkilling needed. */\nAnanian, 6.895 - p. 23\n\nConclusions\nAnanian, 6.895 - p. 24\n\n-\n-\n-\nConclusions\nNon-blocking transactions are a useful and\nintuitive means of concurrency control.\nSoftware implementations of non-blocking\ntransactions are possible and may be efficient,\nbut hard to get right!\nThe Spin model checking tool is an excellent way\nto nail down indeterminacies in parallel code and\nmore rigorously show correctness.\nAnanian, 6.895 - p. 25"
    },
    {
      "category": "Resource",
      "title": "fp_caracas.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/574a00352a8bae8b199acc766bffa7ab_fp_caracas.pdf",
      "content": "Problem\n● Parallelize (serial) applications that use files.\n- Examples: compression tools, logging utilities, databases.\n● In general\n- applications that use files depend on sequential output,\n- serial append is the usual file I/O operation.\n● Goal:\n- perform file I/O operations in parallel,\n- keep the sequential, serial append of the file.\n\nResults\n● Cilk runtime-support for serial append with good\nscalability.\n● Three serial append schemes and implementations for\nCilk:\n1. ported Cheerio, previous parallel file I/O API (M. Debergalis),\n2. simple prototype (with concurrent Linked Lists),\n3. extension, more efficient data structure (concurrent double-\nlinked Skip Lists).\n● Parallel bz2 using PLIO.\n\nSingle Processor Serial Append\nFILE (serial append)\ncomputation DAG\n\nSingle Processor Serial Append\nFILE (serial append)\ncomputation DAG\n\nSingle Processor Serial Append\nFILE (serial append)\ncomputation DAG\n\nSingle Processor Serial Append\nFILE (serial append)\n10 11 12\ncomputation DAG\n\nSingle Processor Serial Append\nFILE (serial append)\n10 11 12\ncomputation DAG\nWhy not in parallel?!\n\nFast Serial Append\nParalleL file I/O (PLIO) support\nfor Serial Append in\nCilk\nAlexandru Caracas\n\nOutline\n● Example\n- single processor & multiprocessor\n● Semantics\n- view of Cilk Programmer\n● Algorithm\n- modification of Cilk runtime system\n● Implementation\n- Previous work\n● Performance\n- Comparison\n\nMultiprocessor Serial Append\nFILE (serial append)\ncomputation DAG\n\nMultiprocessor Serial Append\nFILE (serial append)\ncomputation DAG\n\nMultiprocessor Serial Append\nFILE (serial append)\ncomputation DAG\n\nMultiprocessor Serial Append\nFILE (serial append)\ncomputation DAG\n\nMultiprocessor Serial Append\nFILE (serial append)\n10 11 12\ncomputation DAG\n\nFile Operations\n● open (FILE, mode) / close (FILE).\n● write (FILE, DATA, size)\n- processor writes to its PION.\n● read (FILE, BUFFER, size)\n- processor reads from PION.\n● Note: a seek operation may be required\n● seek (FILE, offset, whence)\n- processor searches for the right PION in the ordered data\nstructure\n\nSemantics\n● View of Cilk programmer:\n- Write operations\n● preserve the sequential, serial append.\n- Read and Seek operations\n● can occur only after the file has been closed,\n● or on a newly opened file.\n\nApproach (for Cilk)\n● Bookkeeping (to reconstruct serial append)\n- Divide execution of the computation,\n- Meta-Data (PIONs) about the execution of the computation.\n● Observation\n- In Cilk, steals need to be accounted for during execution.\n● Theorem\n- expected # of steals = O ( PTinf ).\n● Corollary (see algorithm)\n- expected # of PIONs = O ( PTinf ).\n\nPION (Parallel I/O Node)\n● Definition: a PION represents all the write operations to\na file performed by a processor in between 2 steals.\n● A PION contains:\n- # data bytes written,\n- victim processor ID,\n- pointer to written data.\nπ1\nπ1\nπ3\nπ3\nπ2\nπ2\nπ4\nπ4\nPION\n10 11 12\nFILE\n\nAlgorithm\n● All PIONSs are kept in an ordered data structure.\n- very simple Example: Linked List.\n● On each steal operation performed by processor Pi from\nprocessor Pj:\n- create a new PION πi,\n- attach πi immediately after πj, the PION of P in the order data\nj\nstructure.\nPIONs\nπ1\nπ1\nπk\nπk\nπj\nπj\n\nAlgorithm\n● All PIONSs are kept in an ordered data structure.\n- very simple Example: Linked List.\n● On each steal operation performed by processor Pi from\nprocessor Pj:\n- create a new PION πi,\n- attach πi immediately after πj, the PION of P in the order data\nj\nstructure.\nPIONs\nπ1\nπ1\nπk\nπk\nπj\nπj\nπi\nπi\n\nAlgorithm\n● All PIONSs are kept in an ordered data structure.\n- very simple Example: Linked List.\n● On each steal operation performed by processor Pi from\nprocessor Pj:\n- create a new PION πi,\n- attach πi immediately after πj, the PION of P in the order data\nj\nstructure.\nπ1\nπ1\nπj\nπj\nπk\nπk\nPIONs\nπi\nπi\n\nImplementation\n● Modified the Cilk runtime system to support desired\noperations.\n- implemented hooks on the steal operations.\n● Initial implementation:\n- concurrent Linked List (easier algorithms).\n● Final implementation:\n- concurrent double-linked Skip List.\n● Ported Cheerio to Cilk 5.4.\n\nDetails of Implementation\n● Each processor has a buffer for the data in its own\nPIONs\n- implemented as a file.\n● Data structure to maintain the order of PIONs:\n- Linked List, Skip List.\n● Meta-Data (order maintenance structure of PIONs)\n- kept in memory,\n- saved to a file when serial append file is closed.\n\nSkip List\n● Similar performance with search trees:\n- O ( log (SIZE) ).\nNIL\nNIL\nNIL\nNIL\nNIL\nNIL\nNIL\nNIL\n\nDouble-Linked Skip List\n● Based on Skip Lists (logarithmic performance).\n● Cilk runtime-support in advanced implementation of\nPLIO as rank order statistics.\nNIL\nNIL\nNIL\nNIL\nNIL\nNIL\nNIL\nNIL\n\nPLIO Performance\n● no I/O vs writing 100MB with PLIO (w/ linked list),\n● Tests were run on yggdrasil a 32 proc Origin machine.\n● Parallelism=32,\n● Legend:\n- black: no I/O,\n- red: PLIO.\nExecution Time (seconds)\nNumber of Processors\n\nImprovements & Conclusion\n● Possible Improvements:\n- Optimization of algorithm:\n● delete PIONs with no data,\n● cache oblivious Skip List,\n- File system support,\n- Experiment with other order maintenance data structures:\n● B-Trees.\n● Conclusion:\n- Cilk runtime-support for parallel I/O\n● allows serial applications dependent on sequential output to be\nparallelized.\n\nReferences\n- Robert D. Blumofe and Charles E. Leiserson. Scheduling\nmultithreaded computations by work stealing. In Proceedings\nof the 35th Annual Symposium on Foundations of Computer\nScience, pages 356-368, Santa Fe, New Mexico, November\n1994.\n- Matthew S. DeBergalis. A parallel file I/O API for Cilk.\nMaster's thesis, Department of Electrical Engineering and\nComputer Science, Massachusetts Institute of Technology,\nJune 2000.\n- William Pugh. Concurrent Maintenance of Skip Lists.\nDepartments of Computer Science, University of Maryland,\nCS-TR-2222.1, June, 1990.\n\nReferences\n- Thomas H. Cormen, Charles E. Leiserson, Donald L. Rivest\nand Clifford Stein. Introduction to Algorithms (2nd Edition).\nMIT Press. Cambridge, Massachusetts, 2001.\n- Supercomputing Technology Group MIT Laboratory for\nComputer Science. Cilk 5.3.2 Reference Manual, November\n2001. Available at http://supertech.lcs.mit.edu/cilk/manual-\n5.3.2.pdf.\n- bz2 source code. Available at http://sources.redhat.com/bzip2."
    },
    {
      "category": "Resource",
      "title": "fp_fineman.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/b8af8087e39496628e1b1933ce0fd0bb_fp_fineman.pdf",
      "content": "Maintaining SP Relationships\nEfficiently, on-the-fly\nJeremy Fineman\n\nThe Problem\n- Fork-Join (e.g. Cilk) programs have threads\nthat operate either in series or logically\nparallel.\n- Want to query relationship between two\nthreads as the program runs.\n- For example, Nondeterminator uses\nrelationship between two threads as basis\nfor determinacy race.\n\nParse Tree\nS\nP\nd\na\nS\nc\nb\na\nc\nb\nd\n- Represent SP-DAG as a parse tree\n- S nodes show series relationships\n- P nodes are parallel relationships\n\nLeast Common Ancestor\nS\nP\nd\na\nS\nc\nb\n- SP-Bags uses LCA lookup.\n- LCA of b and d is an S-node\n- So b and d are in series\n- Cost is α(v,v) per query (in Nondeterminator)\n\nTwo Complementary Walks\nS\nP\nd\na\nS\nc\nb\n- At S-node, always walk left then right\n- At P-node, can go left then right, or right then left\n\nTwo Complementary Walks\nS\nP\nd\na\nS\nc\nb\n- Produce two orders of threads:\n- a b c d\n- a c b d\n- Notice b || c, and orders differ between b and c.\n\nTwo Complementary Walks\nS\nP\nd\na\nS\nc\nb\n- Claim: If e1 precedes e2 in one walk, and e2\nprecedes e1 in the other, then e1 || e2.\n\nMaintaining both orders in a\nsingle tree walk\n- Walk of tree represents execution of\nprogram.\n- Can execute program twice, but execution\ncould be nondeterministic.\n- Instead, maintain both thread orderings on-the-\nfly, in a single pass.\n\nOrder Maintenance Problem\n- We need a data structure which supports the\nfollowing operations:\n- Insert(X,Y): Place Y after X in the list.\n- Precedes(X,Y): Does X come before Y?\n\nNaive Order Maintenance\nStructure\n- Naive Implementation is just a linked list\n\nNaive Order Maintenance Insert\nX\nY\n- Insert(X,Y) does standard linked list insert\n\nNaive Order Maintenance Insert\nX\nY\n- Insert(X,Y) does standard linked list insert\n\nNaive Order Maintenance Insert\nX\nY\n- Insert(X,Y) does standard linked list insert\n\nNaive Order Maintenance Query\nX\nZ\n- Precedes(X,Z) looks forward in list.\n\nNaive Order Maintenance Query\nX\nZ\n- Precedes(X,Z) looks forward in list.\n\nNaive Order Maintenance Query\nX\nZ\n- Precedes(X,Z) looks forward in list.\n\nNaive Order Maintenance Query\nZ\nX\n- Precedes(X,Z) looks forward in list.\n\nThe algorithm\n- Recall, we are thinking in terms of parse\ntree.\n- Maintain two order structures.\n- When executing node x:\n- Insert children of x after x in the lists.\n- Ordering of children depends on whether x is\nan S or P node.\n\nExample\nS1\nP\nd\na\nS2\nc\nb\nOrder 1:\nOrder 2:\n\nExample\nS1\nP\nd\na\nS2\nc\nb\nS1\nOrder 1:\nS1\nOrder 2:\n\nExample\nS1\nP\nd\na\nS2\nc\nb\nS1\nS2\nd\nOrder 1:\nS1\nS2\nd\nOrder 2:\n\nExample\nS1\nP\nd\na\nS2\nc\nb\nS2\nS1\nd\nOrder 1:\nS2\nS1\nd\nOrder 2:\n\nExample\nS1\nP\nd\na\nS2\nc\nb\nS2\nS1\nOrder 1:\nOrder 2:\nS1\nS2\nd\nd\na\nP\na\nP\n\nExample\nS1\nP\nd\na\nS2\nc\nb\nOrder 1:\nOrder 2:\nS1\nS1\nS2\nS2\nd\nd\na\nP\na\nP\n\nExample\nS1\nP\nd\na\nS2\nc\nb\nP\nS1\nS2\nOrder 1:\nOrder 2:\nS1\nS2\nd\nd\na\nP\na\n\nExample\nS1\nP\nd\na\nS2\nc\nb\nP\nS1\nS2\nOrder 1:\nOrder 2:\nS1\nS2\nd\nd\na\na\nP\nb\nc\nc\nb\n\nExample\nS1\nP\nd\na\nS2\nc\nb\nOrder 1:\nOrder 2:\nS1\nS1\nS2\nS2\nd\nd\na\na\nP\nP\nb\nc\nb\nc\n\nExample\nS1\nP\nd\na\nS2\nc\nb\nc\nS1\nS2\nOrder 1:\nOrder 2:\nS1\nS2\nd\nd\na\nP\na\nP\nb\nc\nb\n\nExample\nS1\nP\nd\na\nS2\nc\nb\nd\nS1\nS2\nOrder 1:\nOrder 2:\nS1\nS2\nd\na\nP\na\nP\nb\nc\nc\nb\n\nAnalysis\n- Correctness does not depend on execution\n- Any valid serial or parallel execution produces\ncorrect results.\n- Inserts after x in orders only happen when x\nexecutes.\n- Only one processor will ever insert after x.\n- Running time depends on implementation\nof order maintenance data structure.\n\nSerial Running Time\n- Current Nondeterminator does serial\nexecution.\n- Can have O(T1) queries and inserts.\n- Naive implementation is\n- O(n) time for query of n-element list.\n- O(1) time for insert.\n- Total time is very poor: O(T12)\n\nUse Dietz and Sleator Order\nMaintenance Structure\n- Essentially a linked list with labels.\n- Queries are O(1): just compare the labels.\n- Inserts are O(1) amortized cost.\n- On some inserts, need to perform relabel.\n- O(T1) operations only takes O(T1) time.\n- Gives us linear time Nondeterminator. Better\nthan SP-bags algorithm.\n\nParallel Problem\n- Dietz and Sleator relabels on inserts\n- Does not work concurrently.\n- Lock entire structure on insert?\n- Query is still O(1).\n- Single relabel can cost O(T1) operations.\n- Critical path increases to O(T1)\n- Running time is O(T1/p + T1).\n\nParallel Problem Solution\n- Leverage the Cilk scheduler:\n- There are only O(pTinf) steals\n- There is no contention on subcomputations\ndone by single processor between steals.\n- We do not need to lock every insert.\n\nParallel Problem Solution\nGlobal Structure\nSize: O(pTinf)\n- Top level is global ordering of subcomputations.\n- Bottom level is local ordering of subcomputation\nperformed on single processor.\n- On a steal, insert O(1) nodes into global structure.\n\nParallel Running Time\n- An insert into a local order structure is still O(1).\n- An insert into the global structure involves locking\nthe entire global structure.\n- May need to wait for p processors to insert serially.\n- Amortized cost is O(p) per insert.\n- Only O(pTinf) inserts into global structure.\n- Total work and waiting time is O(T1 + p2Tinf)\n- Running time is O(T1/p + pTinf)\n\nQuestions?"
    },
    {
      "category": "Resource",
      "title": "fp_gilbert.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/cf6de56c4724b3fe87a270d3fd98c762_fp_gilbert.pdf",
      "content": "Concurrent\nOrder Maintenance\nSeth Gilbert\n(Collaboration with\nJeremy Fineman and Michael Bender)\n6/8/2004\nMIT CSAIL - 6.895 Final Presentation\n\nOrder Maintenance\n- Problem:\n- Insert(Item, Predecessor)\n- Inserts Item after Predecessor\n- Returns pointer to item\n- Precedes(A, B)\n- Does item A precede item B?\n- Solutions:\n- Dietz, Sleator, Order Maintenance Problem, 1987\n- Bender, Cole, Demaine, Farach-Colton, Zito, 2002\n6/8/2004\nMIT CSAIL - 6.895 Final Presentation\n\nExample\nInsert(B, A)\nInsert(A, 0)\nInsert(C, B)\nInsert(D, B)\n6/8/2004\nMIT CSAIL - 6.895 Final Presentation\n(A, 50)\n(B, 75)\n(C, 76)\n(C, 87)\nInsert Cost: O(log n)\n\nExample\nPrecedes(A, C)?\nInsert Cost: O(log n)\nPrecedes Cost: O(1)\n(A, 50)\n(B, 75)\n(C, 87)\n6/8/2004\nMIT CSAIL - 6.895 Final Presentation\n\nOutline\n- Introduction\n- Indirection\n- Results - Total Work\n- O log p T1 + p·Tinf\n- O 1 T1 + p(1+ε)·Tinf , 0 < ε ≤ 0.5\nε\n- Non-blocking Implementations\n- Conclusion\n6/8/2004\nMIT CSAIL - 6.895 Final Presentation\n\nGetting Constant Time\n- Maintain n/log n lists of size log n\nInsert(A, 0)\nInsert(B, A)\nInsert(C, A)\nn\n(A, 3)\n(B, 5)\n(C, 4)\nlog n\nlog n\n6/8/2004\nMIT CSAIL - 6.895 Final Presentation\n\nGetting Constant Time\n-\nO(n·log n) to insert n items\n-\nMaintain n/log n lists of size log n\nn log n = O(n)\nlog n\n-\nMaintain lists of size log n\n-\nEasy!\n-\nNo reorganization => constant time ops\n-\nEach insert divides tag space in half...\n6/8/2004\nMIT CSAIL - 6.895 Final Presentation\n\nOutline\n- Introduction\n- Indirection\n- Results - Total Work\n- O log p T1 + p·Tinf\n- O 1 T1 + p(1+ε)·Tinf , 0 < ε ≤ 0.5\nε\n- Non-blocking Implementations\n- Conclusion\n6/8/2004\nMIT CSAIL - 6.895 Final Presentation\n\nParallel Problems\n- Lock during inserts?\n- Queries are still fast\n- Inserts may be slow\n- Focus on Cilk graph (Non-Determinator)\n≤ T1 Precedes queries\n≤ p·TinfInsert ops (steal attempts)\n- Desired goal: O(T1 + p·Tinf) work\n- Reality: slower...\n6/8/2004\nMIT CSAIL - 6.895 Final Presentation\n\nApplications\n- Non-Determinator\n- Cache-oblivious B-Trees\n- Distributed Search Data Structures\n6/8/2004\nMIT CSAIL - 6.895 Final Presentation\n\nSmall Number Processors\n- If p ≤ log n, use indirection\n- Waiting time per processor:\nTinf\nlog n = O(Tinf)\nlog n\n- Total waiting time: O(p·Tinf)\nTinf\nlog n\np\np\nlog n\nlog n\n6/8/2004\nMIT CSAIL - 6.895 Final Presentation\n\nOne Level Indirection\n- Assume p is not so small\n- How expensive is ordering p elements?\n- Waiting time per insert O(p)\n- Total waiting time: p2·Tinf\n- Total work: O(T1 + p2·Tinf)\nTinf\nlog n\np\np\nlog n\nlog n\n6/8/2004\nMIT CSAIL - 6.895 Final Presentation\n\nMore Indirection\n- If p > log n, but not too big:\n- Precedes: O(log p)\n- Total: O log p T1 + p·Tinf\nTinf\nlog n\nlog n\nlog n\nlog n\nlog n\nk =\nlog p + 1\nloglog n\nlogkn = p·log n\nBetter when:\nT1\np2\nTinf log p\n<\n6/8/2004\nMIT CSAIL - 6.895 Final Presentation\n\nVariable Indirection\n- Trade-off between queries and inserts:\n- 0 < ε ≤ 0.5\n- Precedes: 1/ε\n- Total: O 1 T1 + p(1+ε)·Tinf\nε\nTinf\nlog n\npε\npε\npε\nlog(n)\nk = 1\nε\npεk = p\n6/8/2004\nMIT CSAIL - 6.895 Final Presentation\n\nMore Indirection (Again)\n- ε = 1\nlog p\n- Precedes: log p\n- Total: O log p T1 + p·Tinf\nTinf\nlog n\npε\npε\npε\nlog(n)\nk = 1\nε\nlog(n)\nlog(n)\nlog(pε) + 1\nloglog(n)\n6/8/2004\nMIT CSAIL - 6.895 Final Presentation\n\nOutline\n- Introduction\n- Indirection\n- Results - Total Work\n- O log p T1 + p·Tinf\n- O 1 T1 + p(1+ε)·Tinf , 0 < ε ≤ 0.5\nε\n- Non-blocking Implementations\n- Conclusion\n6/8/2004\nMIT CSAIL - 6.895 Final Presentation\n\nNon-Blocking\n- Assume DCAS\n- Compares two addresses with old values\n- DCAS(A, B, old-A, old-B, new-A, new-B)\n- if ((*A == old-A) && (*B == old-B))\n*A = new-A\n*B = new-B\n- Lock-freedom / Obstruction-freedom\n- Some operation is always able to make\nprogress\n- Start with linked-list implementation\n6/8/2004\nMIT CSAIL - 6.895 Final Presentation\n\nConcurrent Reorganization\n- How to ensure that operations make\nprogress?\n- Precedes queries can always proceed\n- Always renumber monotonically increasing\n- How to ensure Insert does not interfere?\n- Increment \"owner\" field of predecessor\n- Only Insert or Renumber if you own the\npredecessor\n- Backoff\n6/8/2004\nMIT CSAIL - 6.895 Final Presentation\n\nConclusion\n- Concurrent Order Maintenance\n- T1 Precedes queries\n- pTinf Inserts (steals)\n- p Processors\n- Results - Total Work\n- O log p T1 + p·Tinf\n- O 1 T1 + p(1+ε)·Tinf , 0 < ε ≤ 0.5\nε\n6/8/2004\nMIT CSAIL - 6.895 Final Presentation\n\nConclusion\n- Concurrent Order Maintenance\n- T1 Precedes queries\n- pTinf Inserts (steals)\n- p Processors\n- Results - Total Work\n- O loglog p T1 + p·Tinf\n- O 1\nT1 + p(1+ε)·Tinf , 0 < ε ≤ 0.5\nlog ε\n6/8/2004\nMIT CSAIL - 6.895 Final Presentation\n\nBackup Slides\n6/8/2004\nMIT CSAIL - 6.895 Final Presentation\n\nBinary Tree\n85%\n100%\n73%\n63%\n(C, 0) (B, 1)\n6/8/2004\nMIT CSAIL - 6.895 Final Presentation\n(A, 5) (F, 6) (D, 7)\n(B, 2)\n(E, 1)\nN\n\nBad Example\np\np·log(p)\n6/8/2004\nMIT CSAIL - 6.895 Final Presentation\n\nWhy does it work?\n- Concurrent reorganization can only help\n- Successful insert implies some processor\nmade progress\n- No worse than starting after insert completes\n- At worst, same as locking:\n- Begin after operation completes\n6/8/2004\nMIT CSAIL - 6.895 Final Presentation"
    },
    {
      "category": "Resource",
      "title": "cach_oblivs_focs.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/7e5331aae765482e87251ba75d9aeabe_cach_oblivs_focs.pdf",
      "content": "Cache-Oblivious Algorithms\nEXTENDED ABSTRACT\nMatteo Frigo\nCharles E. Leiserson\nHarald Prokop\nSridhar Ramachandran\nMIT Laboratory for Computer Science, 545 Technology Square, Cambridge, MA 02139\nAbstract\nThis paper presents asymptotically optimal algo-\nrithms for rectangular matrix transpose, FFT, and sorting on\ncomputers with multiple levels of caching. Unlike previous\noptimal algorithms, these algorithms are cache oblivious: no\nvariables dependent on hardware parameters, such as cache\nsize and cache-line length, need to be tuned to achieve opti-\nmality. Nevertheless, these algorithms use an optimal amount\nof work and move data optimally among multiple levels of\ncache. For a cache with size Z and cache-line length L where\nZ\nΩ(L2\n) the number of cache misses for an m\nn ma-\ntrix transpose is Θ(1\n+ mnL). The number of cache misses\nfor either an n-point FFT or the sorting of n numbers is\nΘ(1\n+\n( nL)(1\n+ logZ n)). We also give an Θ(mnp)-work al-\ngorithm to multiply an m\nn matrix by an n\np matrix that\nincurs Θ(1\n+\n( mn\n+ np\n+ mp)L\n+ mnpL\np\nZ\n) cache faults.\nWe introduce an \"ideal-cache\" model to analyze our algo-\nrithms. We prove that an optimal cache-oblivious algorithm\ndesigned for two levels of memory is also optimal for multi-\nple levels and that the assumption of optimal replacement in\nthe ideal-cache model can be simulated efficiently by LRU re-\nplacement. We also provide preliminary empirical results on\nthe effectiveness of cache-oblivious algorithms in practice.\n1.\nIntroduction\nResource-oblivious algorithms that nevertheless use re-\nsources efficiently offer advantages of simplicity and\nportability over resource-aware algorithms whose re-\nsource usage must be programmed explicitly. In this\npaper, we study cache resources, specifically, the hier-\narchy of memories in modern computers. We exhibit\nseveral \"cache-oblivious\" algorithms that use cache as\neffectively as \"cache-aware\" algorithms.\nBefore discussing the notion of cache obliviousness,\nwe first introduce the\n(Z\nL) ideal-cache model to study\nthe cache complexity of algorithms. This model, which\nis illustrated in Figure 1, consists of a computer with a\ntwo-level memory hierarchy consisting of an ideal (data)\ncache of Z words and an arbitrarily large main mem-\nory. Because the actual size of words in a computer is\ntypically a small, fixed size (4 bytes, 8 bytes, etc.), we\nThis research was supported in part by the Defense Advanced\nResearch Projects Agency (DARPA) under Grant F30602-97-1-0270.\nMatteo Frigo was supported in part by a Digital Equipment Corpora-\ntion fellowship.\nMain\norganized by\nMemory\nQ\ncache\nmisses\nCache\nZ\nL\nLines\nL\nCPU\nW\noptimal replacement\nstrategy\nCache lines\nof length\nwork\nFigure 1: The ideal-cache model\nshall assume that word size is constant; the particular\nconstant does not affect our asymptotic analyses. The\ncache is partitioned into cache lines, each consisting of\nL consecutive words which are always moved together\nbetween cache and main memory. Cache designers typ-\nically use L\n1, banking on spatial locality to amortize\nthe overhead of moving the cache line. We shall gener-\nally assume in this paper that the cache is tall:\nZ\nΩ(L2\n)\n\n(1)\nwhich is usually true in practice.\nThe processor can only reference words that reside\nin the cache. If the referenced word belongs to a line\nalready in cache, a cache hit occurs, and the word is\ndelivered to the processor. Otherwise, a cache miss oc-\ncurs, and the line is fetched into the cache. The ideal\ncache is fully associative [20, Ch. 5]: cache lines can be\nstored anywhere in the cache. If the cache is full, a cache\nline must be evicted. The ideal cache uses the optimal\noff-line strategy of replacing the cache line whose next\naccess is furthest in the future [7], and thus it exploits\ntemporal locality perfectly.\nUnlike various other hierarchical-memory models\n[1, 2, 5, 8] in which algorithms are analyzed in terms of\na single measure, the ideal-cache model uses two mea-\nsures. An algorithm with an input of size n is measured\nby its work complexity W\n(n)--its conventional running\ntime in a RAM model [4]--and its cache complexity\nQ(n; Z\nL)--the number of cache misses it incurs as a\n\nfunction of the size Z and line length L of the ideal cache.\nWhen Z and L are clear from context, we denote the\ncache complexity simply as Q(n) to ease notation.\nWe define an algorithm to be cache aware if it con-\ntains parameters (set at either compile-time or runtime)\nthat can be tuned to optimize the cache complexity for\nthe particular cache size and line length. Otherwise, the\nalgorithm is cache oblivious. Historically, good perfor-\nmance has been obtained using cache-aware algorithms,\nbut we shall exhibit several optimal1 cache-oblivious al-\ngorithms.\nTo illustrate the notion of cache awareness, consider\nthe problem of multiplying two n\nn matrices A and\nB to produce their n\nn product C. We assume that\nthe three matrices are stored in row-major order, as\nshown in Figure 2(a). We further assume that n is\n\"big,\" i.e., n\nL, in order to simplify the analysis. The\nconventional way to multiply matrices on a computer\nwith caches is to use a blocked algorithm [19, p. 45].\nThe idea is to view each matrix M as consisting of\n(ns)\n\n(ns) submatrices Mij (the blocks), each of\nwhich has size s\ns, where s is a tuning parame-\nter. The following algorithm implements this strategy:\nBLOCK-MULT\n(A BC\nn)\n1 for i\n1 to ns\ndo for j\n1 to ns\ndo for k\n1 to ns\ndo ORD-MULT\n(Aik\nBk j\nCi j\ns)\nThe\nORD-MULT\n(A B C\ns)\nsubroutine\ncomputes\nC\nC\n+ AB on s\ns matrices using the ordinary O(s3\n)\nalgorithm. (This algorithm assumes for simplicity that\ns evenly divides n, but in practice s and n need have no\nspecial relationship, yielding more complicated code in\nthe same spirit.)\nDepending on the cache size of the machine on which\nBLOCK-MULT is run, the parameter s can be tuned to\nmake the algorithm run fast, and thus BLOCK-MULT is\na cache-aware algorithm. To minimize the cache com-\nplexity, we choose s to be the largest value such that\nthe three s\ns submatrices simultaneously fit in cache.\nAn s\ns submatrix is stored on Θ (s\n+ s2\nL) cache lines.\nFrom the tall-cache assumption (1), we can see that\ns\nΘ (\np\nZ\n). Thus, each of the calls to ORD-MULT runs\nwith at most Z\nL\nΘ (s2\nL) cache misses needed to\nbring the three matrices into the cache. Consequently,\nthe cache complexity of the entire algorithm is Θ (1\n+\nn2\nL\n+\n( n\np\nZ\n)3\n(Z\nL))\nΘ (1\n+ n2\nL\n+ n3\nL\np\nZ\n), since\nthe algorithm has to read n2 elements, which reside on\n\nn2\nL cache lines.\nThe same bound can be achieved using a simple\n1For simplicity in this paper, we use the term \"optimal\" as a syn-\nonym for \"asymptotically optimal,\" since all our analyses are asymp-\ntotic.\ncache-oblivious algorithm that requires no tuning pa-\nrameters such as the s in BLOCK-MULT. We present\nsuch an algorithm, which works on general rectangular\nmatrices, in Section 2. The problems of computing a\nmatrix transpose and of performing an FFT also suc-\ncumb to remarkably simple algorithms, which are de-\nscribed in Section 3. Cache-oblivious sorting poses a\nmore formidable challenge. In Sections 4 and 5, we\npresent two sorting algorithms, one based on mergesort\nand the other on distribution sort, both of which are op-\ntimal in both work and cache misses.\nThe\nideal-cache\nmodel\nmakes\nthe\nperhaps-\nquestionable assumptions that there are only two\nlevels in the memory hierarchy, that memory is man-\naged automatically by an optimal cache-replacement\nstrategy, and that the cache is fully associative. We\naddress these assumptions in Section 6, showing that\nto a certain extent, these assumptions entail no loss\nof generality. Section 7 discusses related work, and\nSection 8 offers some concluding remarks, including\nsome preliminary empirical results.\n2.\nMatrix multiplication\nThis section describes and analyzes a cache-oblivious al-\ngorithm for multiplying an m\nn matrix by an n\np ma-\ntrix cache-obliviously using Θ (mnp) work and incurring\nΘ (m\n+ n\n+ p\n+\n( mn\n+ np\n+ mp)L\n+ mnpL\np\nZ\n) cache\nmisses. These results require the tall-cache assumption\n(1) for matrices stored in row-major layout format, but\nthe assumption can be relaxed for certain other layouts.\nWe also show that Strassen's algorithm [31] for multi-\nplying n\nn matrices, which uses Θ (nlg 7\n) work2, incurs\nΘ (1\n+ n2\nL\n+ nlg7\nL\np\nZ\n) cache misses.\nIn [9] with others, two of the present authors analyzed\nan optimal divide-and-conquer algorithm for n\nn ma-\ntrix multiplication that contained no tuning parameters,\nbut we did not study cache-obliviousness per se. That\nalgorithm can be extended to multiply rectangular matri-\nces. To multiply a m\nn matrix A and a n\np matrix B,\nthe REC-MULT algorithm halves the largest of the three\ndimensions and recurs according to one of the following\nthree cases:\n\nA1\nB\n\nA1B\n\n(2)\nA2\nA2B\n\n;\n\nB1\nA1\nA2\nB2\n\nA1B1\n+ A2B2\n\n(3)\n;\n\n;\n\nA B1\nB2\n\nAB1\nAB2\n:\n(4)\nIn case (2), we have m\nmax\nfn pg. Matrix A is split\nhorizontally, and both halves are multiplied by matrix B.\nIn case (3), we have n\nmax\nfm pg. Both matrices are\n2We use the notation lg to denote log2.\n\nsplit, and the two halves are multiplied. In case (4), we\nhave p\nmax\nfm ng. Matrix B is split vertically, and\neach half is multiplied by A. For square matrices, these\nthree cases together are equivalent to the recursive mul-\ntiplication algorithm described in [9]. The base case oc-\ncurs when m\nn\np\n1, in which case the two ele-\nments are multiplied and added into the result matrix.\nAlthough this straightforward divide-and-conquer al-\ngorithm contains no tuning parameters, it uses cache op-\ntimally. To analyze the REC-MULT algorithm, we as-\nsume that the three matrices are stored in row-major or-\nder, as shown in Figure 2(a). Intuitively, REC-MULT\nuses the cache effectively, because once a subproblem\nfits into the cache, its smaller subproblems can be solved\nin cache with no further cache misses.\nTheorem 1 The REC-MULT algorithm uses Θ (mnp)\nwork and incurs Θ (m\n+ n\n+ p\n+\n( mn\n+ np\n+ mp)L\n+\nmnpL\np\nZ\n) cache misses when multiplying an m\nn ma\ntrix by an n\np matrix.\nProof.\nIt can be shown by induction that the work of\nREC-MULT is Θ (mnp). To analyze the cache misses, let\nα\n0 be the largest constant sufficiently small that three\nsubmatrices of sizes m\n\nn , n\n\np\n, and m\n\np\n, where\nmax\nfm\nn\np\ng\nα\np\nZ, all fit completely in the cache.\nWe distinguish four cases depending on the initial size\nof the matrices.\nCase I: m n p\nα\np\nZ. This case is the most intuitive.\nThe matrices do not fit in cache, since all dimensions are\n\"big enough.\" The cache complexity can be described\nby the recurrence\nQ(m n p)\n\n(5)\nΘ((mn\n+ np\n+ mp)L)\nif m n p\n\n[α\np\nZ\n2 α\np\nZ\n]\n\n2Q(m2 n p)\n+ O(1)\now. if m\nn and m\np\n\n2Q(m n2 p)\n+ O(1)\now. if n\nm and n\np\n\n: 2Q(m n p2)\n+ O(1)\notherwise :\nThe base case arises as soon as all three submatrices fit\nin cache. The total number of lines used by the three\nsubmatrices is Θ ((mn\n+ np\n+ mp)L). The only cache\nmisses that occur during the remainder of the recursion\nare the Θ ((mn\n+ np\n+ mp)L) cache misses required to\nbring the matrices into cache. In the recursive cases,\nwhen the matrices do not fit in cache, we pay for the\ncache misses of the recursive calls, which depend on the\ndimensions of the matrices, plus O(1) cache misses for\nthe overhead of manipulating submatrices. The solution\nto this recurrence is Q(m n p)\nΘ (mnpL\np\nZ\n).\nCase II: (m\nα\np\nZ and n p\nα\np\nZ) or (n\nα\np\nZ and\nm p\nα\np\nZ) or (p\nα\np\nZ and m n\nα\np\nZ). Here, we\nshall present the case where m\nα\np\nZ and n p\nα\np\nZ.\nThe proofs for the other cases are only small variations\nof this proof. The REC-MULT algorithm always divides\nn or p by 2 according to cases (3) and (4). At some point\n(a)\n\n(b)\n11 22 33 44 55 66\n88 99 10\n10 11\n11 12\n12 13\n13 14\n14 15\n16 17\n17 18\n18 19\n19 20\n20 21\n21 22\n22 23\n24 25\n25 26\n26 27\n27 28\n28 29\n29 30\n30 31\n32 33\n33 34\n34 35\n35 36\n36 37\n37 38\n38 39\n40 41\n41 42\n42 43\n43 44\n44 45\n45 46\n46 47\n48 49\n49 50\n50 51\n51 52\n52 53\n53 54\n54 55\n56 57\n57 58\n58 59\n59 60\n60 61\n61 62\n62 63\n(c)\n17 18\n(d)\n11 22 33\n44 55 66 77\n88 99 10\n10 11\n12 13\n13 14\n14 15\n20 21\n21 22\n22 23\n24 25\n25 26\n26 27\n28 29\n29 30\n30 31\n32 33\n33 34\n34 35\n36 37\n37 38\n38 39\n40 41\n41 42\n42 43\n44 45\n45 46\n46 47\n48 49\n49 50\n50 51\n52 53\n53 54\n54 55\n56 57\n57 58\n58 59\n60 61\n61 62\n62 63\n00 11\n22 33\n44 55\n66 77\n88 99\n10 11\n12 13\n14 15\n16 17\n18 19\n20 21\n22 23\n24 25\n26 27\n28 29\n30 31\n32 33\n34 35\n36 37\n38 39\n40 41\n42 43\n44 45\n46 47\n48 49\n50 51\n52 53\n54 55\n56 57\n58 59\n60 61\n62 63\nFigure 2: Layout of a 16\n16 matrix in (a) row ma-\njor, (b) column major, (c) 4\n4-blocked, and (d) bit-\ninterleaved layouts.\nin the recursion, both are small enough that the whole\nproblem fits into cache. The number of cache misses\ncan be described by the recurrence\nQ(m n p)\n\n(6)\nΘ(1\n+ n\n+ npL\n+ m)\nif n p\n\n[α\np\nZ\n2 α\np\nZ\n]\n\n2Q(m n2 p)\n+ O(1)\notherwise if n\np\n\n: 2Q(m n p2)\n+ O(1)\notherwise ;\nwhose solution is Q(m n p)\nΘ (npL\n+ mnpL\np\nZ\n).\nCase III: (n p\nα\np\nZ and m\nα\np\nZ) or (m p\nα\np\nZ\nand n\nα\np\nZ) or (m n\nα\np\nZ and p\nα\np\nZ). In each\nof these cases, one of the matrices fits into cache, and\nthe others do not. Here, we shall present the case where\nn p\nα\np\nZ and m\nα\np\nZ. The other cases can be\nproven similarly. The REC-MULT algorithm always di-\nvides m by 2 according to case (2). At some point in the\nrecursion, m falls into the range α\np\nZ\nm\nα\np\nZ,\nand the whole problem fits in cache. The number cache\nmisses can be described by the recurrence\nQ(m n)\n\n(7)\n\nΘ(1\n+ m)\nif m\n\n[α\np\nZ\n2 α\np\nZ\n]\n\n2Q(m2 n p)\n+ O(1)\notherwise ;\nwhose solution is Q(m n p)\nΘ (m\n+ mnpL\np\nZ\n).\nCase IV: m n p\nα\np\nZ. From the choice of α, all\nthree matrices fit into cache. The matrices are stored\non Θ (1\n+ mnL\n+ npL\n+ mpL) cache lines. Therefore,\nwe have Q(m n p)\nΘ (1\n+\n( mn\n+ np\n+ mp)L).\nWe require the tall-cache assumption (1) in these\nanalyses, because the matrices are stored in row-major\norder. Tall caches are also needed if matrices are stored\n\nin column-major order (Figure 2(b)), but the assumption\nthat Z\nΩ(L2\n) can be relaxed for certain other matrix\nlayouts. The s\ns-blocked layout (Figure 2(c)), for some\ntuning parameter s, can be used to achieve the same\nbounds with the weaker assumption that the cache holds\nat least some sufficiently large constant number of lines.\nThe cache-oblivious bit-interleaved layout (Figure 2(d))\nhas the same advantage as the blocked layout, but no\ntuning parameter need be set, since submatrices of size\nO(\np\nL\n)\nO(\np\nL) are cache-obliviously stored on O(1)\ncache lines. The advantages of bit-interleaved and re-\nlated layouts have been studied in [11, 12, 16]. One of\nthe practical disadvantages of bit-interleaved layouts is\nthat index calculations on conventional microprocessors\ncan be costly, a deficiency we hope that processor archi-\ntects will remedy.\nFor square matrices, the cache complexity Q(n)\n\nΘ (n\n+ n2\nL\n+ n3\nL\np\nZ\n) of the REC-MULT algorithm is\nthe same as the cache complexity of the cache-aware\nBLOCK-MULT algorithm and also matches the lower\nbound by Hong and Kung [21].\nThis lower bound\nholds for all algorithms that execute the Θ (n3\n) opera-\ntions given by the definition of matrix multiplication\nn\ncij\n∑aikbk j\n:\nk\nNo tight lower bounds for the general problem of matrix\nmultiplication are known.\nBy using an asymptotically faster algorithm, such as\nStrassen's algorithm [31] or one of its variants [37], both\nthe work and cache complexity can be reduced. When\nmultiplying n\nn matrices, Strassen's algorithm, which\nis cache oblivious, requires only 7 recursive multiplica-\ntions of n2\nn2 matrices and a constant number of\nmatrix additions, yielding the recurrence\n\nΘ (1\n+ n\n+ n2\nL)\nif n2\nαZ\n\n(8)\nQ(n)\n\n7Q(n2)\n+ O(n2\nL)\notherwise ;\nwhere α is a sufficiently small constant. The solution to\nthis recurrence is Θ (n\n+ n2\nL\n+ nlg7\nL\np\nZ\n).\n3.\nMatrix transposition and FFT\nThis section describes a recursive cache-oblivious al-\ngorithm for transposing an m\nn matrix which uses\nO(mn) work and incurs O(1\n+ mnL) cache misses,\nwhich is optimal. Using matrix transposition as a sub-\nroutine, we convert a variant [36] of the \"six-step\" fast\nFourier transform (FFT) algorithm [6] into an optimal\ncache-oblivious algorithm. This FFT algorithm uses\nO(nlg n) work and incurs O\n(1\n+\n( nL)\n( 1\n+ logZ n))\ncache misses.\nThe problem of matrix transposition is defined as fol-\nlows. Given an m\nn matrix stored in a row-major lay-\nout, compute and store AT into an n\nm matrix B also\nstored in a row-major layout. The straightforward algo-\nrithm for transposition that employs doubly nested loops\nincurs Θ (mn) cache misses on one of the matrices when\nm\nZ\nL and n\nZ\nL, which is suboptimal.\nOptimal work and cache complexities can be ob-\ntained with a divide-and-conquer strategy, however. If\nn\nm, the REC-TRANSPOSE algorithm partitions\n\nB1\nA\n\n( A1 A2\n)\n\nB\n\nB2\nand recursively executes REC-TRANSPOSE\n(A1\nB1\n) and\nREC-TRANSPOSE\n(A2\nB2\n). Otherwise, it divides matrix\nA horizontally and matrix B vertically and likewise per-\nforms two transpositions recursively. The next two lem-\nmas provide upper and lower bounds on the performance\nof this algorithm.\nLemma 2 The REC-TRANSPOSE algorithm involves\nO(mn) work and incurs O(1\n+ mnL) cache misses for\nan m\nn matrix.\nProof.\nThat the algorithm does O(mn) work is straight-\nforward. For the cache analysis, let Q(m n) be the cache\ncomplexity of transposing an m\nn matrix. We as-\nsume that the matrices are stored in row-major order, the\ncolumn-major layout having a similar analysis.\nLet α be a constant sufficiently small such that two\nsubmatrices of size m\nn and n\nm, where max\nfm ng\n\nαL, fit completely in the cache even if each row is stored\nin a different cache line. We distinguish the three cases.\nCase I: max\nfm ng\nαL. Both the matrices fit in\nO(1)\n+ 2mnL lines. From the choice of α, the number\nof lines required is at most Z\nL. Therefore Q(m n)\n\nO(1\n+ mnL).\nCase II: m\nαL\nn or n\nαL\nm. Suppose first that\nm\nαL\nn. The REC-TRANSPOSE algorithm divides\nthe greater dimension n by 2 and performs divide and\nconquer. At some point in the recursion, n falls into the\nrange αL2\nn\nαL, and the whole problem fits in\ncache. Because the layout is row-major, at this point the\ninput array has n rows and m columns, and it is laid out\nin contiguous locations, requiring at most O(1\n+ nmL)\ncache misses to be read. The output array consists of nm\nelements in m rows, where in the worst case every row\nlies on a different cache line. Consequently, we incur at\nmost O(m\n+ nmL) for writing the output array. Since\nn\nαL2, the total cache complexity for this base case\nis O(1\n+ m). These observations yield the recurrence\n\nO(1\n+ m)\nif n\n\n[αL2 αL]\nQ(m n)\n\n2Q(m n2)\n+ O(1)\notherwise ;\nwhose solution is Q(m n)\nO(1\n+ mnL).\nThe case n\nαL\nm is analogous.\n\nCase III: m n\nαL. As in Case II, at some point in the\nrecursion both n and m fall into the range\n[αL2 αL].\nThe whole problem fits into cache and can be solved\nwith at most O(m\n+ n\n+ mnL) cache misses. The cache\ncomplexity thus satisfies the recurrence\nQ(m n)\n\nO(m\n+ n\n+ mnL)\nif m n\n\n[αL2 αL\n]\n\n2Q(m2 n)\n+ O(1)\nif m\nn\n\n: 2Q(m n2)\n+ O(1)\notherwise;\nwhose solution is Q(m n)\nO(1\n+ mnL).\nTheorem 3 The REC-TRANSPOSE algorithm has opti\nmal cache complexity.\nProof.\nFor an m\nn matrix, the algorithm must write to\nmn distinct elements, which occupy at least\ndmnLe\n\nΩ(1\n+ mnL) cache lines.\nAs an example of an application of this cache-\noblivious transposition algorithm, in the rest of this sec-\ntion we describe and analyze a cache-oblivious algo-\nrithm for computing the discrete Fourier transform of a\ncomplex array of n elements, where n is an exact power\nof 2. The basic algorithm is the well-known \"six-step\"\nvariant [6, 36] of the Cooley-Tukey FFT algorithm [13].\nUsing the cache-oblivious transposition algorithm, how-\never, the FFT becomes cache-oblivious, and its perfor-\nmance matches the lower bound by Hong and Kung [21].\nRecall that the discrete Fourier transform (DFT) of\nan array X of n complex numbers is the array Y given by\nn1\n]ω\nij\nY\n[i]\n∑X\n[ j\n\n(9)\nn\nj\nwhere ωn\ne2π\np\n1n is a primitive nth root of unity,\nand 0\ni\nn. Many algorithms evaluate Equation (9) in\nO(nlg n) time for all integers n [15]. In this paper, how-\never, we assume that n is an exact power of 2, and we\ncompute Equation (9) according to the Cooley-Tukey al-\ngorithm, which works recursively as follows. In the base\ncase where n\nO(1), we compute Equation (9) directly.\nOtherwise, for any factorization n\nn1n2 of n, we have\nY\n[i1\n+ i2n1\n]\n\n(10)\n\"\n!\n#\nn2\nn1\n∑ X\n[ j1n2\n+ j2\n]ω\ni1 j1\nω\ni1 j2 ω\ni2 j2\n:\n∑\nn1\nn\nn2\nj2\nj1\nObserve that both the inner and outer summations in\nEquation (10) are DFT's. Operationally, the computa-\ntion specified by Equation (10) can be performed by\ncomputing n2 transforms of size n1 (the inner sum), mul-\ntiplying the result by the factors ω\ni1 j2 (called the twid-\nn\ndle factors [15]), and finally computing n1 transforms of\nsize n2 (the outer sum).\nWe choose n1 to be 2\ndlg n2e and n2 to be 2\nblg n2c. The\nrecursive step then operates as follows:\n1. Pretend that input is a row-major n1\nn2 matrix A.\nTranspose A in place, i.e., use the cache-oblivious\nREC-TRANSPOSE algorithm to transpose A onto an\nauxiliary array B, and copy B back onto A. Notice\nthat if n1\n2n2, we can consider the matrix to be\nmade up of records containing two elements.\n2. At this stage, the inner sum corresponds to a DFT\nof the n2 rows of the transposed matrix. Compute\nthese n2 DFT's of size n1 recursively. Observe that,\nbecause of the previous transposition, we are trans-\nforming a contiguous array of elements.\n3. Multiply A by the twiddle factors, which can be\ncomputed on the fly with no extra cache misses.\n4. Transpose A in place, so that the inputs to the next\nstage are arranged in contiguous locations.\n5. Compute n1 DFT's of the rows of the matrix recur-\nsively.\n6. Transpose A in place so as to produce the correct\noutput order.\nIt can be proven by induction that the work com-\nplexity of this FFT algorithm is O(nlg n). We now an-\nalyze its cache complexity. The algorithm always op-\nerates on contiguous data, by construction. Thus, by\nthe tall-cache assumption (1), the transposition oper-\nations and the twiddle-factor multiplication require at\nmost O(1\n+ nL) cache misses. Thus, the cache com-\nplexity satisfies the recurrence\nO(1\n+ nL)\nif n\nαZ\n\nQ(n)\n\nn1Q(n2\n)\n+ n2Q(n1\n)\notherwise ;\n(11)\n:\n+O(1\n+ nL)\nwhere α\n0 is a constant sufficiently small that a sub-\nproblem of size αZ fits in cache. This recurrence has\nsolution\nQ(n)\nO\n(1\n+\n( nL)\n( 1\n+ logZ n))\n\nwhich is optimal for a Cooley-Tukey algorithm, match-\ning the lower bound by Hong and Kung [21] when n is\nan exact power of 2. As with matrix multiplication, no\ntight lower bounds for cache complexity are known for\nthe general DFT problem.\n4.\nFunnelsort\nCache-oblivious algorithms, like the familiar two-way\nmerge sort, are not optimal with respect to cache misses.\nThe Z-way mergesort suggested by Aggarwal and Vit-\nter [3] has optimal cache complexity, but although it ap-\nparently works well in practice [23], it is cache aware.\nThis section describes a cache-oblivious sorting algo-\nrithm called \"funnelsort.\" This algorithm has optimal\n\nL1\nk\nR\nL\np\nk\n-merger\nbuffers\nFigure 3: Illustration of a k-merger. A k-merger is built\nrecursively out of\np\nk \"left\"\np\nk-mergers L1, L2,\n:\n:\n: , L\np\nk,\na series of buffers, and one \"right\"\np\nk-merger R.\nO(nlg n) work complexity, and optimal O(1\n+\n( nL)(1\n+\nlogZ n)) cache complexity.\nFunnelsort is similar to mergesort. In order to sort\na (contiguous) array of n elements, funnelsort performs\nthe following two steps:\n1. Split the input into n13 contiguous arrays of size\nn23, and sort these arrays recursively.\n2. Merge the n13 sorted sequences using a n13-\nmerger, which is described below.\nFunnelsort differs from mergesort in the way the\nmerge operation works. Merging is performed by a de-\nvice called a k-merger, which inputs k sorted sequences\nand merges them. A k-merger operates by recursively\nmerging sorted sequences which become progressively\nlonger as the algorithm proceeds. Unlike mergesort,\nhowever, a k-merger suspends work on a merging sub-\nproblem when the merged output sequence becomes\n\"long enough\" and resumes work on another merging\nsubproblem.\nThis complicated flow of control makes a k-merger\na bit tricky to describe. Figure 3 shows a representa-\ntion of a k-merger, which has k sorted sequences as in-\nputs. Throughout its execution, the k-merger maintains\nthe following invariant.\nInvariant Each invocation of a k-merger outputs the\nnext k3 elements of the sorted sequence obtained by\nmerging the k input sequences.\nA k-merger is built recursively out of\np\nk-mergers in\nthe following way. The k inputs are partitioned into\np\nk\nsets of\np\nk elements, which form the input to the\np\nk\np\nk-mergers L1\nL2\n:\n:\n:\nL\np\nk in the left part of the figure.\nThe outputs of these mergers are connected to the inputs\nof\np\nk buffers. Each buffer is a FIFO queue that can\nhold 2k32 elements. Finally, the outputs of the buffers\nare connected to the\np\nk inputs of the\np\nk-merger R in\nthe right part of the figure. The output of this final\np\nk-\nmerger becomes the output of the whole k-merger. The\nintermediate buffers are overdimensioned, since each\ncan hold 2k32 elements, which is twice the number k32\nof elements output by a\np\nk-merger. This additional\nbuffer space is necessary for the correct behavior of the\nalgorithm, as will be explained below. The base case of\nthe recursion is a k-merger with k\n2, which produces\nk3\n8 elements whenever invoked.\nA k-merger operates recursively in the following way.\nIn order to output k3 elements, the k-merger invokes\nR k32 times. Before each invocation, however, the k-\nmerger fills all buffers that are less than half full, i.e.,\nall buffers that contain less than k32 elements. In order\nto fill buffer i, the algorithm invokes the corresponding\nleft merger Li once. Since Li outputs k32 elements, the\nbuffer contains at least k32 elements after Li finishes.\nIt can be proven by induction that the work com-\nplexity of funnelsort is O(nlg n). We will now analyze\nthe cache complexity. The goal of the analysis is to\nshow that funnelsort on n elements requires at most Q(n)\ncache misses, where\nQ(n)\nO(1\n+\n( nL)(1\n+ logZ n))\n:\nIn order to prove this result, we need three auxiliary lem-\nmas. The first lemma bounds the space required by a\nk-merger.\nLemma 4 A k-merger can be laid out in O(k2\n) contigu\nous memory locations.\nProof.\nA k-merger requires O(k2\n) memory locations\nfor the buffers, plus the space required by the\np\nk-\nmergers. The space S\n(k\n) thus satisfies the recurrence\nS\n(k\n)\n\n(\np\nk\n+ 1)S\n(\np\nk\n)\n+ O(k2\n)\n\nwhose solution is S\n(k\n)\nO(k2\n).\nIn order to achieve the bound on Q(n), the buffers\nin a k-merger must be maintained as circular queues of\nsize k. This requirement guarantees that we can man-\nage the queue cache-efficiently, in the sense stated by\nthe next lemma.\nLemma 5 Performing r insert and remove operations\non a circular queue causes in O(1\n+ r\nL) cache misses\nas long as two cache lines are available for the buffer.\nProof.\nAssociate the two cache lines with the head and\ntail of the circular queue. If a new cache line is read\nduring a insert (delete) operation, the next L\n1 insert\n(delete) operations do not cause a cache miss.\nThe next lemma bounds the cache complexity of a\nk-merger.\n\nLemma 6 If Z\nΩ(L2\n), then a k-merger operates with\nat most\nQM\n(k\n)\nO(1\n+ k\n+ k3\nL\n+ k3logZ k\nL)\ncache misses.\nProof.\nThere are two cases: either k\nα\np\nZ or k\n\nα\np\nZ, where α is a sufficiently small constant.\nCase I: k\nα\np\nZ. By Lemma 4, the data structure\nassociated with the k-merger requires at most O(k2\n)\n\nO(Z\n) contiguous memory locations, and therefore it fits\ninto cache. The k-merger has k input queues from\nwhich it loads O(k3\n) elements. Let ri be the number\nof elements extracted from the ith input queue. Since\nk\nα\np\nZ and the tall-cache assumption (1) implies that\nL\nO(\np\nZ\n), there are at least Z\nL\nΩ(k\n) cache lines\navailable for the input buffers. Lemma 5 applies, whence\nthe total number of cache misses for accessing the input\nqueues is\nk\n∑O(1\n+ ri\nL)\nO(k\n+ k3\nL)\n:\ni1\nSimilarly, Lemma 4 implies that the cache complexity\nof writing the output queue is O(1\n+ k3\nL). Finally, the\nalgorithm incurs O(1\n+ k2\nL) cache misses for touching\nits internal data structures. The total cache complexity is\ntherefore QM\n(k\n)\nO(1\n+ k\n+ k3\nL).\nCase I: k\nα\np\nZ. We prove by induction on k that\nwhenever k\nα\np\nZ, we have\nQM\n(k\n)\nck3 logZ k\nL\nA(k\n)\n\n(12)\nwhere A(k\n)\nk\n(1\n+ 2clogZ k\nL)\no(k3\n). This particular\nvalue of A(k\n) will be justified at the end of the analysis.\nThe base case of the induction consists of values of\nk such that αZ14\nk\nα\np\nZ. (It is not sufficient only\nto consider k\nΘ (\np\nZ\n), since k can become as small as\nΘ (Z14\n) in the recursive calls.) The analysis of the first\ncase applies, yielding QM\n(k\n)\nO(1\n+ k\n+ k3\nL). Be-\ncause k2\nα\np\nZ\nΩ(L) and k\nΩ(1), the last term\ndominates, which implies QM\n(k\n)\nO(k3\nL). Conse-\nquently, a big enough value of c can be found that satis-\nfies Inequality (12).\nFor the inductive case, suppose that k\nα\np\nZ. The\nk-merger invokes the\np\nk-mergers recursively. Since\nαZ14\n\np\nk\nk, the inductive hypothesis can be used to\nbound the number QM\n(\np\nk\n) of cache misses incurred by\nthe submergers. The \"right\" merger R is invoked exactly\nk32 times. The total number l of invocations of \"left\"\nmergers is bounded by l\nk32\n+ 2\np\nk. To see why, con-\nsider that every invocation of a left merger puts k32 el-\nements into some buffer. Since k3 elements are output\nand the buffer space is 2k2, the bound l\nk32\n+ 2\np\nk\nfollows.\nBefore invoking R, the algorithm must check every\nbuffer to see whether it is empty. One such check re-\nquires at most\np\nk cache misses, since there are\np\nk\nbuffers. This check is repeated exactly k32 times, lead-\ning to at most k2 cache misses for all checks. These\nconsiderations lead to the recurrence\n\nQM\n(k\n)\n\n2k32\n+ 2\np\nk QM\n(\np\nk\n)\n+ k2\n:\nApplication of the inductive hypothesis and the choice\nA(k\n)\nk\n(1\n+ 2clogZ k\nL) yields Inequality (12) as fol-\nlows:\n\nQM\n(k\n)\n\n2k32\n+ 2\np\nk QM\n(\np\nk\n)\n+ k2\n\"\n#\n\nck32 logZ k\nA(\np\nk\n)\n+ k2\n2 k32\n+\np\nk\n2L\nck3 logZ k\nL\n+ k2\n(1\n+ clogZ k\nL)\n\n2k32\n+ 2\np\nk A(\np\nk\n)\nck3 logZ k\nL\nA(k\n)\n:\nTheorem 7 To sort n elements, funnelsort incurs O(1\n+\n(nL)(1\n+ logZ n)) cache misses.\nProof.\nIf n\nαZ for a small enough constant α, then\nthe algorithm fits into cache. To see why, observe that\nonly one k-merger is active at any time. The biggest\nk-merger is the top-level n13-merger, which requires\nO(n23\n)\nO(n) space. The algorithm thus can operate\nin O(1\n+ nL) cache misses.\nIf N\nαZ, we have the recurrence\nQM\n(n13\nQ(n)\nn13Q(n\n)\n+\n)\n:\nBy Lemma 6, we have QM\n(n13\n)\nO(1\n+ n13\n+ nL\n+\nnlogZ nL).\nBy the tall-cache assumption (1), we have nL\n\nΩ(n13\n). Moreover, we also have n13\nΩ(1) and lg n\n\nΩ(lg Z\n).\nConsequently, QM\n(n13\n)\nO(nlogZ nL)\nholds, and the recurrence simplifies to\nQ(n)\nn13Q(n23\n)\n+ O(nlogZ nL)\n:\nThe result follows by induction on n.\nThis upper bound matches the lower bound stated\nby the next theorem, proving that funnelsort is cache-\noptimal.\nTheorem 8 The cache complexity of any sorting algo\nrithm is Q(n)\nΩ(1\n+\n( nL)(1\n+ logZ n)).\nProof.\nAggarwal and Vitter [3] show that there is an\nΩ((nL) logZ\nL\n(nZ\n)) bound on the number of cache\nmisses made by any sorting algorithm on their \"out-of-\ncore\" memory model, a bound that extends to the ideal-\ncache model. The theorem can be proved by apply-\ning the tall-cache assumption Z\nΩ(L2\n) and the trivial\nlower bounds of Q(n)\nΩ(1) and Q(n)\nΩ(nL).\n\n5.\nDistribution sort\nIn this section, we describe another cache-oblivious op-\ntimal sorting algorithm based on distribution sort. Like\nthe funnelsort algorithm from Section 4, the distribution-\nsorting algorithm uses O(nlg n) work to sort n elements,\nand it incurs O\n(1\n+\n( nL)\n( 1\n+ logZ n)) cache misses.\nUnlike previous cache-efficient distribution-sorting al-\ngorithms [1, 3, 25, 34, 36], which use sampling or other\ntechniques to find the partitioning elements before the\ndistribution step, our algorithm uses a \"bucket splitting\"\ntechnique to select pivots incrementally during the dis-\ntribution step.\nGiven an array A (stored in contiguous locations) of\nlength n, the cache-oblivious distribution sort operates\nas follows:\n1. Partition A into\npn contiguous subarrays of size\npn. Recursively sort each subarray.\n2. Distribute the sorted subarrays into q buckets\nB1\n:\n:\n:\nBq of size n1\n:\n:\n:\nnq, respectively, such that\n1. max\nfx\nj x\nBi\ng\nmin\nfx\nj x\nBi+1\ng for i\n\n1 2:\n:\n:\nq\n1.\n2. ni\npn for i\n1 2:\n:\n:\nq.\n(See below for details.)\n3. Recursively sort each bucket.\n4. Copy the sorted buckets to array A.\nA stack-based memory allocator is used to exploit spatial\nlocality.\nThe goal of Step 2 is to distribute the sorted subarrays\nof A into q buckets B1\nB2\n:\n:\n:\nBq. The algorithm main-\ntains two invariants. First, at any time each bucket holds\nat most 2\npn elements, and any element in bucket Bi is\nsmaller than any element in bucket Bi+1. Second, every\nbucket has an associated pivot. Initially, only one empty\nbucket exists with pivot inf.\nThe idea is to copy all elements from the subarrays\ninto the buckets while maintaining the invariants. We\nkeep state information for each subarray and bucket. The\nstate of a subarray consists of the index next of the next\nelement to be read from the subarray and the bucket\nnumber bnum where this element should be copied. By\nconvention, bnum\ninfif all elements in a subarray have\nbeen copied. The state of a bucket consists of the pivot\nand the number of elements currently in the bucket.\nWe would like to copy the element at position next of\na subarray to bucket bnum. If this element is greater than\nthe pivot of bucket bnum, we would increment bnum un-\ntil we find a bucket for which the element is smaller than\nthe pivot. Unfortunately, this basic strategy has poor\ncaching behavior, which calls for a more complicated\nprocedure.\nThe distribution step is accomplished by the recur-\nsive procedure DISTRIBUTE\n(i j\nm) which distributes\nelements from the ith through\n(i\n+ m\n1)th subarrays\ninto buckets starting from B j. Given the precondition\nthat each subarray i i\n+ 1:\n:\n:\ni\n+ m\n1 has its bnum\nj,\nthe execution of DISTRIBUTE\n(i j\nm) enforces the post-\ncondition that subarrays i i\n+ 1:\n:\n:\ni\n+ m\n1 have their\nbnum\nj\n+ m. Step 2 of the distribution sort invokes\nDISTRIBUTE\n(1 1\npn\n). The following is a recursive im-\nplementation of DISTRIBUTE:\nDISTRIBUTE\n(i j\nm)\n1 if m\nthen COPYELEMS\n(i j\n)\nelse DISTRIBUTE\n(i j\nm2)\nDISTRIBUTE\n(i\n+ m2 j\nm2)\nDISTRIBUTE\n(i j\n+ m2 m2)\nDISTRIBUTE\n(i\n+ m2 j\n+ m2 m2)\nIn the base case, the procedure COPYELEMS\n(i j\n)\ncopies all elements from subarray i that belong to\nbucket j. If bucket j has more than 2\npn elements af-\nter the insertion, it can be split into two buckets of size\nat least\npn. For the splitting operation, we use the deter-\nministic median-finding algorithm [14, p. 189] followed\nby a partition.\nLemma 9 The median of n elements can be found\ncache-obliviously using O(n) work and incurring O(1\n+\nnL) cache misses.\nProof.\nSee [14, p. 189] for the linear-time median find-\ning algorithm and the work analysis. The cache com-\nplexity is given by the same recurrence as the work com-\nplexity with a different base case.\nO(1\n+ mL)\nif m\nαZ\n\nQ(m)\n\nQ(dm5e)\n+ Q(7m10\n+ 6\n)\notherwise ;\n:\n+ O(1\n+ mL)\nwhere α is a sufficiently small constant. The result fol-\nlows.\nIn our case, we have buckets of size 2\npn\n+ 1. In ad-\ndition, when a bucket splits, all subarrays whose bnum\nis greater than the bnum of the split bucket must have\ntheir bnum's incremented. The analysis of DISTRIBUTE\nis given by the following lemma.\nLemma 10 The distribution step involves O(n) work,\nincurs O(1\n+ nL) cache misses, and uses O(n) stack\nspace to distribute n elements.\nProof.\nIn order to simplify the analysis of the work\nused by DISTRIBUTE, assume that COPYELEMS uses\nO(1) work for procedural overhead. We will account for\nthe work due to copying elements and splitting of buck-\nets separately. The work of DISTRIBUTE is described by\n\nthe recurrence\nT\n(c)\n4T\n(c2)\n+ O(1)\n:\nIt follows that T\n(c)\nO(c2\n), where c\n\npn initially. The\nwork due to copying elements is also O(n).\nThe total number of bucket splits is at most\npn. To\nsee why, observe that there are at most\npn buckets at the\nend of the distribution step, since each bucket contains at\nleast\npn elements. Each split operation involves O(\npn)\nwork and so the net contribution to the work is O(n).\nThus, the total work used by DISTRIBUTE is W\n(n)\n\nO(T\n(\npn))\n+ O(n)\n+ O(n)\nO(n).\nFor the cache analysis, we distinguish two cases. Let\nα be a sufficiently small constant such that the stack\nspace used fits into cache.\nCase I, n\nαZ: The input and the auxiliary space of\nsize O(n) fit into cache using O(1\n+ nL) cache lines.\nConsequently, the cache complexity is O(1\n+ nL).\nCase II, n\nαZ: Let R(c m) denote the cache misses\nincurred by an invocation of DISTRIBUTE\n(a b c) that\ncopies m elements from subarrays to buckets. We first\nprove that R(c m)\nO(L\n+ c2\nL\n+ mL), ignoring the\ncost splitting of buckets, which we shall account for sep-\narately. We argue that R(c m) satisfies the recurrence\n8 O(L\n+ mL)\nif c\nαL\n\nR(c m)\n\n: ∑R(c2 mi\n)\notherwise ;\n(13)\ni1\nwhere ∑i\n1 mi\nm, whose solution is R(c m)\nO(L\n+\nc2\nL\n+ mL). The recursive case c\nαL follows im-\nmediately from the algorithm.\nThe base case c\n\nαL can be justified as follows.\nAn invocation of\nDISTRIBUTE\n(a b c) operates with c subarrays and c\nbuckets. Since there are Ω(L) cache lines, the cache can\nhold all the auxiliary storage involved and the currently\naccessed element in each subarray and bucket. In this\ncase there are O(L\n+ mL) cache misses. The initial ac-\ncess to each subarray and bucket causes O(c)\nO(L)\ncache misses. Copying the m elements to and from con-\ntiguous locations causes O(1\n+ mL) cache misses.\nWe still need to account for the cache misses caused\nby the splitting of buckets. Each split causes O(1\n+\npnL) cache misses due to median finding (Lemma 9)\nand partitioning of\npn contiguous elements. An addi-\ntional O(1\n+\npn\nL) misses are incurred by restoring the\ncache. As proven in the work analysis, there are at most\npn split operations. By adding R(\npn n) to the split\ncomplexity, we conclude that the total cache complexity\nof the distribution step is O(L\n+ nL\n+\npn(1\n+\npn\nL))\n\nO(nL).\nThe analysis of distribution sort is given in the next\ntheorem. The work and cache complexity match lower\nbounds specified in Theorem 8.\nTheorem 11 Distribution sort uses O(nlg n) work and\nincurs O(1\n+\n( nL)\n( 1\n+ logZ n)) cache misses to sort n\nelements.\nProof.\nThe work done by the algorithm is given by\nq\nW\n(n)\n\npnW\n(\npn)\n+ ∑W\n(ni\n)\n+ O(n)\n\ni1\nwhere each ni\npn and ∑ni\nn. The solution to this\nrecurrence is W\n(n)\nO(nlg n).\nThe space complexity of the algorithm is given by\nS\n(n)\nS\n(2\npn\n)\n+ O(n)\n\nwhere the O(n) term comes from Step 2. The solution to\nthis recurrence is S\n(n)\nO(n).\nThe cache complexity of distribution sort is described\nby the recurrence\nO(1\n+ nL)\nif n\nαZ\n\nQ(n)\n\npnQ(\npn)\n+ ∑q\ni1 Q(ni\n)\notherwise ;\n:\n+O(1\n+ nL)\nwhere α is a sufficiently small constant such that the\nstack space used by a sorting problem of size αZ, in-\ncluding the input array, fits completely in cache. The\nbase case n\nαZ arises when both the input array A\nand the contiguous stack space of size S\n(n)\nO(n) fit\nin O(1\n+ nL) cache lines of the cache. In this case,\nthe algorithm incurs O(1\n+ nL) cache misses to touch\nall involved memory locations once. In the case where\nn\nαZ, the recursive calls in Steps 1 and 3 cause\nQ(\npn)\n+ ∑i\nq\n1 Q(ni\n) cache misses and O(1\n+ nL) is\nthe cache complexity of Steps 2 and 4, as shown by\nLemma 10. The theorem follows by solving the recur-\nrence.\n6. Theoretical justifications for the ideal-\ncache model\nHow reasonable is the ideal-cache model for algorithm\ndesign? The model incorporates four major assumptions\nthat deserve scrutiny:\noptimal replacement,\nexactly two levels of memory,\nautomatic replacement,\nfull associativity.\nDesigning algorithms in the ideal-cache model is easier\nthan in models lacking these properties, but are these\nassumptions too strong? In this section we show that\ncache-oblivious algorithms designed in the ideal-cache\nmodel can be efficiently simulated by weaker models.\nThe first assumption that we shall eliminate is that\nof optimal replacement. Our strategy for the simula-\ntion is to use an LRU (least-recently used) replacement\nstrategy [20, p. 378] in place of the optimal and om-\nniscient replacement strategy. We start by proving a\n\nlemma that bounds the effectiveness of the LRU simu-\nlation. We then show that algorithms whose complex-\nity bounds satisfy a simple regularity condition (includ-\ning all algorithms heretofore presented) can be ported to\ncaches incorporating an LRU replacement policy.\nLemma 12 Consider\nan\nalgorithm\nthat\ncauses\nQ\n\n(n; Z\nL) cache misses on a problem of size n using\na\n(Z\nL) ideal cache. Then, the same algorithm incurs\nQ(n; Z\nL)\n2Q\n\n(n; Z\n2 L) cache misses on a\n(Z\nL)\ncache that uses LRU replacement.\nProof.\nSleator and Tarjan [30] have shown that the\ncache misses on a\n(Z\nL) cache using LRU replacement\nare\n(Z\nL)((Z\nZ\n\n)L\n+ 1)-competitive with optimal\nreplacement on a (Z\n\nL) ideal cache if both caches start\nempty. It follows that the number of misses on a\n(Z\nL)\nLRU-cache is at most twice the number of misses on a\n(Z\n2 L) ideal-cache.\nCorollary 13 For\nany\nalgorithm\nwhose\ncache-\ncomplexity bound Q(n; Z\nL) in the ideal-cache model\nsatisfies the regularity condition\nQ(n; Z\nL)\nO(Q(n;2Z\nL))\n\n(14)\nthe number of cache misses with LRU replacement is\nΘ (Q(n; Z\nL)).\nProof.\nFollows directly from (14) and Lemma 12.\nThe second assumption we shall eliminate is the as-\nsumption of only two levels of memory. Although mod-\nels incorporating multiple levels of caches may be nec-\nessary to analyze some algorithms, for cache-oblivious\nalgorithms, analysis in the two-level ideal-cache model\nsuffices.\nSpecifically, optimal cache-oblivious algo-\nrithms also perform optimally in computers with mul-\ntiple levels of LRU caches. We assume that the caches\nsatisfy the inclusion property [20, p. 723], which says\nthat the values stored in cache i are also stored in cache\ni\n+ 1 (where cache 1 is the cache closest to the proces-\nsor). We also assume that if two elements belong to\nthe same cache line at level i, then they belong to the\nsame line at level i\n+ 1. Moreover, we assume that cache\ni\n+ 1 has strictly more cache lines than cache i. These as-\nsumptions ensure that cache i\n+ 1 includes the contents\nof cache i plus at least one more cache line.\nThe multilevel LRU cache operates as follows. A hit\non an element in cache i is served by cache i and is not\nseen by higher-level caches. We consider a line in cache\ni\n+ 1 to be marked if any element stored on the line be-\nlongs to cache i. When cache i misses on an access, it\nrecursively fetches the needed line from cache i\n+ 1, re-\nplacing the least-recently accessed unmarked cache line.\nThe replaced cache line is then brought to the front of\ncache\n(i\n+ 1)'s LRU list. Because marked cache lines are\nnever replaced, the multilevel cache maintains the inclu-\nsion property. The next lemma, whose proof is omitted,\nasserts that even though a cache in a multilevel model\ndoes not see accesses that hit at lower levels, it neverthe-\nless behaves like the first-level cache of a simple two-\nlevel model, which sees all the memory accesses.\nLemma 14 A\n(Zi\nLi\n)-cache at a given level i of a mul\ntilevel LRU model always contains the same cache lines\nas a simple\n(Zi\nLi\n)-cache managed by LRU that serves\nthe same sequence of memory accesses.\nLemma 15 An\noptimal\ncache-oblivious\nalgorithm\nwhose cache complexity satisifies the regularity condi\ntion (14) incurs an optimal number of cache misses on\neach level3 of a multilevel cache with LRU replacement.\nProof.\nLet cache i in the multilevel LRU model be a\n(Zi\nLi\n) cache. Lemma 14 says that the cache holds ex-\nactly the same elements as a\n(Zi\nLi\n) cache in a two-level\nLRU model. From Corollary 13, the cache complex-\nity of a cache-oblivious algorithm working on a\n(Zi\nLi\n)\nLRU cache lower-bounds that of any cache-aware algo-\nrithm for a (Zi\nLi\n) ideal cache. A (Zi\nLi\n) level in a mul-\ntilevel cache incurs at least as many cache misses as a\n(Zi\nLi\n) ideal cache when the same algorithm is executed.\nFinally, we remove the two assumptions of automatic\nreplacement and full associativity. Specifically, we shall\nshow that a fully associative LRU cache can be main-\ntained in ordinary memory with no asymptotic loss in\nexpected performance.\nLemma 16 A\n(Z\nL) LRU-cache can be maintained us\ning O(Z\n) memory locations such that every access to a\ncache line in memory takes O(1) expected time.\nProof.\nGiven the address of the memory location to\nbe accessed, we use a 2-universal hash function [24,\np. 216] to maintain a hash table of cache lines present\nin the memory.\nThe Z\nL entries in the hash table\npoint to linked lists in a heap of memory that contains\nZ\nL records corresponding to the cache lines. The 2-\nuniversal hash function guarantees that the expected size\nof a chain is O(1). All records in the heap are organized\nas a doubly linked list in the LRU order. Thus, the LRU\npolicy can be implemented in O(1) expected time using\nO(Z\nL) records of O(L) words each.\n3Alpern, Carter and Feig [5] show that optimality on each level of\nmemory in the UMH model does not necessarily imply global optimal-\nity. The UMH model incorporates a single cost measure that combines\nthe costs of work and cache faults at each of the levels of memory. By\nanalyzing the levels independently, our multilevel ideal-cache model\nremains agnostic about the various schemes by which work and cache\nfaults might be combined.\n\nTheorem 17 An optimal cache-oblivious algorithm\nwhose cache-complexity bound satisfies the regularity\ncondition (14) can be implemented optimally in expec\ntation in multilevel models with explicit memory man\nagement.\nProof.\nCombine Lemma 15 and Lemma 16.\nCorollary 18 The recursive cache-oblivious algorithms\nfor matrix multiplication, matrix transpose, FFT, and\nsorting are optimal in multilevel models with explicit\nmemory management.\nProof.\nTheir complexity bounds satisfy the regularity\ncondition (14).\nIt can also be shown [26] that cache-oblivous algo-\nrithms satisfying (14) are also optimal (in expectation)\nin the previously studied SUMH [5, 34] and HMM [1]\nmodels. Thus, all the algorithmic results in this paper\napply to these models, matching the best bounds previ-\nously achieved.\nOther simulation results can be shown. For example,\nby using the copying technique of [22], cache-oblivious\nalgorithms for matrix multiplication and other problems\ncan be designed that are provably optimal on direct-\nmapped caches.\n7.\nRelated work\nIn this section, we discuss the origin of the notion of\ncache-obliviousness. We also give an overview of other\nhierarchical memory models.\nOur research group at MIT noticed as far back as\n1994 that divide-and-conquer matrix multiplication was\na cache-optimal algorithm that required no tuning, but\nwe did not adopt the term \"cache-oblivious\" until 1997.\nThis matrix-multiplication algorithm, as well as a cache-\noblivious algorithm for LU-decomposition without piv-\noting, eventually appeared in [9]. Shortly after leaving\nour research group, Toledo [32] independently proposed\na cache-oblivious algorithm for LU-decomposition with\npivoting. For n\nn matrices, Toledo's algorithm uses\nΘ (n\n) work and incurs Θ (1\n+ n2\nL\n+ n3\nL\np\nZ\n) cache\nmisses. More recently, our group has produced an FFT\nlibrary called FFTW [18], which in its most recent incar-\nnation [17], employs a register-allocation and schedul-\ning algorithm inspired by our cache-oblivious FFT al-\ngorithm. The general idea that divide-and-conquer en-\nhances memory locality has been known for a long\ntime [29].\nPrevious theoretical work on understanding hierar-\nchical memories and the I/O-complexity of algorithms\nhas been studied in cache-aware models lacking an auto-\nmatic replacement strategy, although [10, 28] are recent\nTime (microseconds)\n0.25\n0.2\n0.15\n0.1\n0.05\nN\niterative\nrecursive\nFigure 4: Average time to transpose an N\nN matrix,\ndivided by N2.\nexceptions. Hong and Kung [21] use the red-blue peb-\nble game to prove lower bounds on the I/O-complexity\nof matrix multiplication, FFT, and other problems. The\nred-blue pebble game models temporal locality using\ntwo levels of memory. The model was extended by\nSavage [27] for deeper memory hierarchies. Aggarwal\nand Vitter [3] introduced spatial locality and investigated\na two-level memory in which a block of P contiguous\nitems can be transferred in one step. They obtained tight\nbounds for matrix multiplication, FFT, sorting, and other\nproblems. The hierarchical memory model (HMM) by\nAggarwal et al. [1] treats memory as a linear array,\nwhere the cost of an access to element at location x is\ngiven by a cost function f\n(x). The BT model [2] extends\nHMM to support block transfers. The UMH model by\nAlpern et al. [5] is a multilevel model that allows I/O at\ndifferent levels to proceed in parallel. Vitter and Shriver\nintroduce parallelism, and they give algorithms for ma-\ntrix multiplication, FFT, sorting, and other problems in\nboth a two-level model [35] and several parallel hierar-\nchical memory models [36]. Vitter [33] provides a com-\nprehensive survey of external-memory algorithms.\n8.\nConclusion\nThe theoretical work presented in this paper opens two\nimportant avenues for future research. The first is to\ndetermine the range of practicality of cache-oblivious\nalgorithms, or indeed, of any algorithms developed in\nthe ideal-cache model. The second is to resolve, from a\ncomplexity-theoretic point of view, the relative strengths\nof cache-oblivious and cache-aware algorithms. To con-\nclude, we discuss each of these avenues in turn.\nFigure 4 compares per-element time to transpose a\nmatrix using the naive iterative algorithm employing a\ndoubly nested loop with the recursive cache-oblivious\nREC-TRANSPOSE algorithm from Section 3. The two\nalgorithms were evaluated on a 450 megahertz AMD\nK6III processor with a 32-kilobyte 2-way set-associative\nL1 cache, a 64-kilobyte 4-way set-associative L2 cache,\nand a 1-megabyte L3 cache of unknown associativ-\nity, all with 32-byte cache lines. The code for REC-\nTRANSPOSE was the same as presented in Section 3, ex-\n\nTime (microseconds)\n0.12\n0.1\n0.08\n0.06\n0.04\n0.02\nN\niterative\nrecursive\nFigure 5: Average time taken to multiply two N\nN\nmatrices, divided by N3.\ncept that the divide-and-conquer structure was modified\nto produce exact powers of 2 as submatrix sizes wher-\never possible. In addition, the base cases were \"coars-\nened\" by inlining the recursion near the leaves to in-\ncrease their size and overcome the overhead of proce-\ndure calls. (A good research problem is to determine\nan effective compiler strategy for coarsening base cases\nautomatically.)\nAlthough these results must be considered prelimi-\nnary, Figure 4 strongly indicates that the recursive al-\ngorithm outperforms the iterative algorithm throughout\nthe range of matrix sizes. Moreover, the iterative al-\ngorithm behaves erratically, apparently due to so-called\n\"conflict\" misses [20, p. 390], where limited cache asso-\nciativity interacts with the regular addressing of the ma-\ntrix to cause systematic interference. Blocking the itera-\ntive algorithm should help with conflict misses [22], but\nit would make the algorithm cache aware. For large ma-\ntrices, the recursive algorithm executes in less than 70%\nof the time used by the iterative algorithm, even though\nthe transpose problem exhibits no temporal locality.\nFigure 5 makes a similar comparison between the\nnaive iterative matrix-multiplication algorithm, which\nuses three nested loops, with the O(n3\n)-work recur-\nsive REC-MULT algorithm described in Section 2. This\nproblem exhibits a high degree of temporal locality,\nwhich REC-MULT exploits effectively. As the figure\nshows, the average time used per integer multiplication\nin the recursive algorithm is almost constant, which for\nlarge matrices, is less than 50% of the time used by the\niterative variant. A similar study for Jacobi multipass\nfilters can be found in [26].\nSeveral researchers [12, 16] have also observed that\nrecursive algorithms exhibit performance advantages\nover iterative algorithms for computers with caches. A\ncomprehensive empirical study has yet to be done, how-\never. Do cache-oblivious algorithms perform nearly as\nwell as cache-aware algorithms in practice, where con-\nstant factors matter? Does the ideal-cache model cap-\nture the substantial caching concerns for an algorithms\ndesigner?\nAn anecdotal affirmative answer to these questions is\nexhibited by the popular FFTW library [17, 18], which\nuses a recursive strategy to exploit caches in Fourier\ntransform calculations. FFTW's code generator pro-\nduces straight-line \"codelets,\" which are coarsened base\ncases for the FFT algorithm. Because these codelets are\ncache oblivious, a C compiler can perform its register\nallocation efficiently, and yet the codelets can be gen-\nerated without knowing the number of registers on the\ntarget architecture.\nTo close, we mention two theoretical avenues\nthat should be explored to determine the complexity-\ntheoretic relationship between cache-oblivious algo-\nrithms and cache-aware algorithms.\nSeparation:\nIs there a gap in asymptotic complexity\nbetween cache-aware and cache-oblivious algorithms?\nIt appears that cache-aware algorithms should be able to\nuse caches better than cache-oblivious algorithms, since\nthey have more knowledge about the system on which\nthey are running. Do there exist problems for which this\nadvantage is asymptotically significant, for example an\nΩ(lg Z\n) advantage? Bilardi and Peserico [8] have re-\ncently taken some steps in proving a separation.\nSimulation:\nIs there a limit as to how much better a\ncache-aware algorithm can be than a cache-oblivious\nalgorithm for the same problem? That is, given a class\nof optimal cache-aware algorithms to solve a single\nproblem, can we construct a good cache-oblivious al-\ngorithm that solves the same problem with only, for\nexample, O(lg Z\n) loss of efficiency? Perhaps simula-\ntion techniques can be used to convert a class of effi-\ncient cache-aware algorithms into a comparably efficient\ncache-oblivious algorithm.\nAcknowledgments\nThanks to Bobby Blumofe, now of the University of\nTexas at Austin, who sparked early discussions at MIT\nabout what we now call cache obliviousness. Thanks to\nGianfranco Bilardi of University of Padova, Sid Chat-\nterjee of University of North Carolina, Chris Joerg of\nCompaq CRL, Martin Rinard of MIT, Bin Song of MIT,\nSivan Toledo of Tel Aviv University, and David Wise of\nIndiana University for helpful discussions. Thanks also\nto our anonymous reviewers.\nReferences\n[1] A. Aggarwal, B. Alpern, A. K. Chandra, and M. Snir.\nA model for hierarchical memory. In Proceedings of the\n19th Annual ACM Symposium on Theory of Computing\n(STOC), pages 305-314, May 1987.\n[2] A. Aggarwal, A. K. Chandra, and M. Snir. Hierarchi-\ncal memory with block transfer. In 28th Annual Sym\nposium on Foundations of Computer Science (FOCS),\npages 204-216, Los Angeles, California, 12-14 Oct.\n1987. IEEE.\n\n[3] A. Aggarwal and J. S. Vitter. The input/output complex-\nity of sorting and related problems. Communications of\nthe ACM, 31(9):1116-1127, Sept. 1988.\n[4] A. V. Aho, J. E. Hopcroft, and J. D. Ullman. The Design\nand Analysis of Computer Algorithms. Addison-Wesley\nPublishing Company, 1974.\n[5] B. Alpern, L. Carter, and E. Feig. Uniform memory hier-\narchies. In Proceedings of the 31st Annual IEEE Sym\nposium on Foundations of Computer Science (FOCS),\npages 600-608, Oct. 1990.\n[6] D. H. Bailey. FFTs in external or hierarchical memory.\nJournal of Supercomputing, 4(1):23-35, May 1990.\n[7] L. A. Belady. A study of replacement algorithms for vir-\ntual storage computers. IBM Systems Journal, 5(2):78-\n101, 1966.\n[8] G. Bilardi and E. Peserico. Efficient portability across\nmemory hierarchies. Unpublished manuscript, 1999.\n[9] R. D. Blumofe, M. Frigo, C. F. Joerg, C. E. Leiserson,\nand K. H. Randall. An analysis of dag-consistent dis-\ntributed shared-memory algorithms. In Proceedings of\nthe Eighth Annual ACM Symposium on Parallel Algo\nrithms and Architectures (SPAA), pages 297-308, Padua,\nItaly, June 1996.\n[10] L. Carter and K. S. Gatlin. Towards an optimal bit-\nreversal permutation program. In Proceedings of the 39th\nAnnual Symposium on Foundations of Computer Science,\npages 544-555. IEEE Computer Society Press, 1998.\n[11] S. Chatterjee, V. V. Jain, A. R. Lebeck, and S. Mundhra.\nNonlinear array layouts for hierarchical memory sys-\ntems. In Proceedings of the ACM International Confer\nence on Supercomputing, Rhodes, Greece, June 1999.\n[12] S. Chatterjee, A. R. Lebeck, P. K. Patnala, and M. Thot-\ntethodi. Recursive array layouts and fast parallel matrix\nmultiplication. In Proceedings of the Eleventh ACM Sym\nposium on Parallel Algorithms and Architectures (SPAA),\nJune 1999.\n[13] J. W. Cooley and J. W. Tukey. An algorithm for the ma-\nchine computation of the complex Fourier series. Mathe\nmatics of Computation, 19:297-301, Apr. 1965.\n[14] T. H. Cormen, C. E. Leiserson, and R. L. Rivest. In\ntroduction to Algorithms. MIT Press and McGraw Hill,\n1990.\n[15] P. Duhamel and M. Vetterli. Fast Fourier transforms: a\ntutorial review and a state of the art. Signal Processing,\n19:259-299, Apr. 1990.\n[16] J. D. Frens and D. S. Wise.\nAuto-blocking matrix-\nmultiplication or tracking BLAS3 performance from\nsource code. In Proceedings of the Sixth ACM SIG\nPLAN Symposium on Principles and Practice of Parallel\nProgramming (PPoPP), pages 206-216, Las Vegas, NV,\nJune 1997.\n[17] M. Frigo. A fast Fourier transform compiler. In Proceed\nings of the ACM SIGPLAN'99 Conference on Program\nming Language Design and Implementation (PLDI), At-\nlanta, Georgia, May 1999.\n[18] M. Frigo and S. G. Johnson. FFTW: An adaptive soft-\nware architecture for the FFT. In Proceedings of the In\nternational Conference on Acoustics, Speech, and Signal\nProcessing, Seattle, Washington, May 1998.\n[19] G. H. Golub and C. F. van Loan. Matrix Computations.\nJohns Hopkins University Press, 1989.\n[20] J. L. Hennessy and D. A. Patterson. Computer Architec\nture: A Quantitative Approach. Morgan Kaufmann, 2nd\nedition, 1996.\n[21] J.-W. Hong and H. T. Kung. I/O complexity: the red-blue\npebbling game. In Proceedings of the 13th Annual ACM\nSymposium on Theory of Computing (STOC), pages 326-\n333, Milwaukee, 1981.\n[22] M. S. Lam, E. Rothberg, and M. E. Wolf. The cache per-\nformance and optimizations of blocked algortihms. In\nFourth International Conference on Architectural Sup\nport for Programming Languages and Operating Systems\n(ASPLOS), pages 63-74, Santa Clara, CA, Apr. 1991.\nACM SIGPLAN Notices 26:4.\n[23] A. LaMarca and R. E. Ladner. The influence of caches\non the performance of sorting. Proceedings of the Eighth\nAnnual ACM-SIAM Symposium on Discrete Algorithms\n(SODA), pages 370-377, 1997.\n[24] R. Motwani and P. Raghavan. Randomized Algorithms.\nCambridge University Press, 1995.\n[25] M. H. Nodine and J. S. Vitter. Deterministic distribution\nsort in shared and distributed memory multiprocessors.\nIn Proceedings of the Fifth ACM Symposium on Paral\nlel Algorithms and Architectures (SPAA), pages 120-129,\nVelen, Germany, 1993.\n[26] H. Prokop. Cache-oblivious algorithms. Master's thesis,\nMassachusetts Institute of Technology, June 1999.\n[27] J. E. Savage. Extending the Hong-Kung model to mem-\nory hierarchies. In D.-Z. Du and M. Li, editors, Com\nputing and Combinatorics, volume 959 of Lecture Notes\nin Computer Science, pages 270-281. Springer Verlag,\n1995.\n[28] S. Sen and S. Chatterjee. Towards a theory of cache-\nefficient algorithms. Unpublished manuscript, 1999.\n[29] R. C. Singleton. An algorithm for computing the mixed\nradix fast Fourier transform. IEEE Transactions on Audio\nand Electroacoustics, AU-17(2):93-103, June 1969.\n[30] D. D. Sleator and R. E. Tarjan. Amortized efficiency\nof list update and paging rules. Communications of the\nACM, 28(2):202-208, Feb. 1985.\n[31] V. Strassen. Gaussian elimination is not optimal. Nu\nmerische Mathematik, 13:354-356, 1969.\n[32] S. Toledo. Locality of reference in LU decomposition\nwith partial pivoting. SIAM Journal on Matrix Analysis\nand Applications, 18(4):1065-1081, Oct. 1997.\n[33] J. S. Vitter. External memory algorithms and data struc-\ntures. In J. Abello and J. S. Vitter, editors, External\nMemory Algorithms and Visualization, DIMACS Series\nin Discrete Mathematics and Theoretical Computer Sci-\nence. American Mathematical Society Press, Providence,\nRI, 1999.\n[34] J. S. Vitter and M. H. Nodine. Large-scale sorting in\nuniform memory hierarchies. Journal of Parallel and\nDistributed Computing, 17(1-2):107-114, January and\nFebruary 1993.\n[35] J. S. Vitter and E. A. M. Shriver. Algorithms for par-\nallel memory I: Two-level memories. Algorithmica,\n12(2/3):110-147, August and September 1994.\n[36] J. S. Vitter and E. A. M. Shriver. Algorithms for parallel\nmemory II: Hierarchical multilevel memories. Algorith\nmica, 12(2/3):148-169, August and September 1994.\n[37] S. Winograd. On the algebraic complexity of functions.\nActes du Congr`es International des Math ematiciens,\n3:283-288, 1970."
    },
    {
      "category": "Resource",
      "title": "cach_oblvs_thsis.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/6dc7de52dcf13b53cebf2fe10ae6752a_cach_oblvs_thsis.pdf",
      "content": "Cache-Oblivious Algorithms\nby\nHarald Prokop\nSubmitted to the\nDepartment of Electrical Engineering and Computer Science\nin partial fulfillment of the requirements for the degree of\nMaster of Science\nat the\nMASSACHUSETTS INSTITUTE OF TECHNOLOGY.\nJune 1999\nc\nMassachusetts Institute of Technology 1999.\nAll rights reserved.\nAuthor\nDepartment of Electrical Engineering and Computer Science\nMay 21, 1999\nCertified by\nCharles E. Leiserson\nProfessor of Computer Science and Engineering\nThesis Supervisor\nAccepted by\nArthur C. Smith\nChairman, Departmental Committee on Graduate Students\n\nCache-Oblivious Algorithms\nby\nHarald Prokop\nSubmitted to the\nDepartment of Electrical Engineering and Computer Science\non May 21, 1999 in partial fulfillment of the\nrequirements for the degree of Master of Science.\nAbstract\nThis thesis presents \"cache-oblivious\" algorithms that use asymptotically optimal\namounts of work, and move data asymptotically optimally among multiple levels\nof cache. An algorithm is cache oblivious if no program variables dependent on\nhardware configuration parameters, such as cache size and cache-line length need\nto be tuned to minimize the number of cache misses.\nWe show that the ordinary algorithms for matrix transposition, matrix multi\nplication, sorting, and Jacobi-style multipass filtering are not cache optimal. We\npresent algorithms for rectangular matrix transposition, FFT, sorting, and multi\npass filters, which are asymptotically optimal on computers with multiple levels\nof caches. For a cache with size Z and cache-line length L, where Z\nΩ(L2\n),\nthe number of cache misses for an m\nn matrix transpose is Θ (1\n+ mn\nL\n). The\nnumber of cache misses for either an n-point FFT or the sorting of n numbers is\nΘ (1\n+\n( n\nL\n)(1\n+ logZn\n)). The cache complexity of computing n time steps of a\nJacobi-style multipass filter on an array of size n is Θ (1\n+ n\nL\n+ n2\nZL\n). We also\ngive an Θ (mnp\n)-work algorithm to multiply an m\nn matrix by an n\np matrix\nthat incurs Θ (m\n+ n\n+ p\n+\n( mn\n+ np\n+ mp\n)\nL\n+ mnp\nL\np\nZ\n) cache misses.\nWe introduce an \"ideal-cache\" model to analyze our algorithms, and we prove\nthat an optimal cache-oblivious algorithm designed for two levels of memory is\nalso optimal for multiple levels. We further prove that any optimal cache-oblivious\nalgorithm is also optimal in the previously studied HMM and SUMH models. Al\ngorithms developed for these earlier models are perforce cache-aware: their be\nhavior varies as a function of hardware-dependent parameters which must be\ntuned to attain optimality. Our cache-oblivious algorithms achieve the same as\nymptotic optimality on all these models, but without any tuning.\nThesis Supervisor: Charles E. Leiserson\nTitle: Professor of Computer Science and Engineering\n\nAcknowledgments\nI am extremely grateful to my advisor Charles E. Leiserson. He has greatly helped\nme both in technical and nontechnical matters. Without his insight, suggestions,\nand excitement, this work would have never taken place. Charles also helped with\nthe write-up of the paper on which this thesis is based. It is amazing how patiently\nCharles can rewrite a section until it has the quality he expects.\nMost of the work presented in this thesis has been a team effort. I would like\nto thank those with whom I collaborated: Matteo Frigo, Charles E. Leiserson, and\nSridhar Ramachandran. Special thanks to Sridhar who patiently listened to all my\n(broken) attempts to prove that cache-oblivious sorting is impossible.\nI am privileged to be part of the stimulating and friendly environment of the\nSupercomputing Technologies research group of the MIT Laboratory of Computer\nScience. I would like to thank all the members of the group, both past and present,\nfor making it a great place to work. Many thanks to Don Dailey, Phil Lisiecki,\nDimitris Mitsouras, Alberto Medina, Bin Song, and Volker Strumpen.\nThe research in this thesis was supported in part by the Defense Advanced\nResearch Projects Agency (DARPA) under Grant F30602-97-1-0270 and by a fel\nlowship from the Cusanuswerk, Bonn, Germany.\nFinally, I want to thank my family for their love, encouragement, and help,\nwhich kept me going during the more difficult times.\nHARALD PROKOP\nCambridge, Massachusetts\nMay 21, 1999\n\nContents\nIntroduction\nMatrix multiplication\nMatrix transposition and FFT\nFunnelsort\nDistribution sort\nJacobi multipass filter\n6.1\nIterative algorithm\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6.2\nRecursive algorithm\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6.3\nLower bound\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6.4\nExperimental results\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nCache complexity of ordinary algorithms\n7.1\nMatrix multiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7.2\nMatrix transposition\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7.3\nMergesort\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nOther cache models\n8.1\nTwo-level models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8.2\nMultilevel ideal caches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8.3\nThe SUMH model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8.4\nThe HMM model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nRelated work\n10 Conclusion\n10.1 Engineering cache-oblivious algorithms . . . . . . . . . . . . . . . . . . . . .\n10.2 Cache-oblivious data structures . . . . . . . . . . . . . . . . . . . . . . . . . .\n10.3 Complexity of cache obliviousness . . . . . . . . . . . . . . . . . . . . . . . .\n10.4 Compiler support for divide-and-conquer . . . . . . . . . . . . . . . . . . . .\n10.5 The future of divide-and-conquer . . . . . . . . . . . . . . . . . . . . . . . . .\nA Bibliograpy\n\nSECTION 1\nIntroduction\nResource-oblivious algorithms that nevertheless use resources efficiently offer ad\nvantages of simplicity and portability over resource-aware algorithms whose re\nsource usage must be programmed explicitly. In this thesis, we study cache re\nsources, specifically, the hierarchy of memories in modern computers. We exhibit\nseveral \"cache-oblivious\" algorithms that use cache as effectively as \"cache-aware\"\nalgorithms.\nBefore discussing the notion of cache obliviousness, we introduce the\n(Z, L\n)\nideal-cache model to study the cache complexity of algorithms. This model, which\nis illustrated in Figure 1-1, consists of a computer with a two-level memory hier\narchy consisting of an ideal (data) cache of Z words and an arbitrarily large main\nmemory. Because the actual size of words in a computer is typically a small, fixed\nsize (4 bytes, 8 bytes, etc.), we shall assume that word size is constant; the par\nticular constant does not affect our asymptotic analyses. The cache is partitioned\ninto cache lines, each consisting of L consecutive words that are always moved\ntogether between cache and main memory. Cache designers typically use L\n1,\nbanking on spatial locality to amortize the overhead of moving the cache line. We\nshall generally assume in this thesis that the cache is tall:\nZ\nΩ(L2\n) ,\n(1.1)\nwhich is usually true in practice.\nThe processor can only reference words that reside in the cache. If the refer\nenced word belongs to a line already in cache, a cache hit occurs, and the word is\n\nMain\nQ\nstrategy\nMemory\n. . .\n. . .\nCache\nZ\nL\nLines\nL\nCPU\nwork W\ncache misses\norganized by\noptimal replacement\nCache lines\nof length\nFigure 1-1: The ideal-cache model\ndelivered to the processor. Otherwise, a cache miss occurs, and the line is fetched\ninto the cache. The ideal cache is fully associative [24, Ch. 5]: Cache lines can be\nstored anywhere in the cache. If the cache is full, a cache line must be evicted. The\nideal cache uses the optimal off-line strategy of replacing the cache line whose next\naccess is farthest in the future [7], and thus it exploits temporal locality perfectly.\nAn algorithm with an input of size n is measured in the ideal-cache model\nin terms of its work complexity W\n(n\n)--its conventional running time in a RAM\nmodel [4]--and its cache complexity Q\n(n\nZ, L\n)--the number of cache misses it\nincurs as a function of the size Z and line length L of the ideal cache. When Z and\nL are clear from context, we denote the cache complexity as simply Q\n(n\n) to ease\nnotation.\nWe define an algorithm to be cache aware if it contains parameters (set at ei\nther compile-time or runtime) that can be tuned to optimize the cache complexity\nfor the particular cache size and line length. Otherwise, the algorithm is cache\noblivious. Historically, good performance has been obtained using cache-aware\nalgorithms, but we shall exhibit several cache-oblivious algorithms for fundamen\ntal problems that are asymptotically as efficient as their cache-aware counterparts.\nTo illustrate the notion of cache awareness, consider the problem of multiply\ning two n\nn matrices A and B to produce their n\nn product C. We assume\nthat the three matrices are stored in row-major order, as shown in Figure 2-1(a).\nWe further assume that n is \"big,\" i.e., n\nL, in order to simplify the analysis.\nThe conventional way to multiply matrices on a computer with caches is to use\na blocked algorithm [22, p. 45]. The idea is to view each matrix M as consist\n\ning of\n(n\ns\n)\n\n(n\ns\n) submatrices Mij (the blocks), each of which has size s\ns,\nwhere s is a tuning parameter. The following algorithm implements this strategy:\nBLOCK-MULT\n(A, B, C, n\n)\n1 for i\n1 to n\ns\ndo for j\n1 to n\ns\ndo for k\n1 to n\ns\ndo ORD-MULT\n(Aik, Bkj, Cij, s\n)\nwhere ORD-MULT\n(A, B, C, s\n) is a subroutine that computes C\nC\n+ AB on s\ns\nmatrices using the ordinary O\n(s3\n) algorithm (see Section 7.1). (This algorithm as\nsumes for simplicity that s evenly divides n. In practice, s and n need have no\nspecial relationship, which yields more complicated code in the same spirit.)\nDepending on the cache size of the machine on which BLOCK-MULT is run,\nthe parameter s can be tuned to make the algorithm run fast, and thus BLOCK\nMULT is a cache-aware algorithm. To minimize the cache complexity, we choose\ns as large as possible such that the three s\ns submatrices simultaneously fit in\ncache. An s\ns submatrix is stored on Θ (s\n+ s2\nL\n) cache lines. From the tall-\ncache assumption (1.1), we can see that s\nΘ (\np\nZ\n). Thus, each of the calls to\nORD-MULT runs with at most Z\nL\nΘ (s\n+ s2\nL\n) cache misses needed to bring\nthe three matrices into the cache. Consequently, the cache complexity of the entire\nalgorithm is Θ (n\n+ n2\nL\n+\n( n\np\nZ\n)3\n(Z\nL\n))\nΘ (n\n+ n2\nL\n+ n3\nL\np\nZ\n), since the\nalgorithm must read n2 elements, which reside on\ndn2\nL\ne cache lines.\nThe same bound can be achieved using a simple cache-oblivious algorithm that\nrequires no tuning parameters such as the s in BLOCK-MULT. We present such an\nalgorithm, which works on general rectangular matrices, in Section 2. The prob\nlems of computing a matrix transpose and of performing an FFT also succumb to\nremarkably simple algorithms, which are described in Section 3. Cache-oblivious\nsorting poses a more formidable challenge. In Sections 4 and 5, we present two\nsorting algorithms, one based on mergesort and the other on distribution sort, both\nof which are optimal. Section 6 compares an optimal recursive algorithm with an\n\"ordinary\" iterative algorithm, both of which compute a multipass filter over one-\ndimensional data. It also provides some brief empirical results for this problem. In\nSection 7, we show that the ordinary algorithms for matrix transposition, matrix\nmultiplication, and sorting are not cache optimal.\nThe ideal-cache model makes the perhaps-questionable assumption that mem\nory is managed automatically by an optimal cache replacement strategy. Although\nthe current trend in architecture does favor automatic caching over programmer-\nspecified data movement, Section 8 addresses this concern theoretically. We show\n\nthat the assumptions of two hierarchical memory models in the literature, in which\nmemory movement is programmed explicitly, are actually no weaker than ours.\nSpecifically, we prove (with only minor assumptions) that optimal cache-oblivious\nalgorithms in the ideal-cache model are also optimal in the hierarchical memory\nmodel (HMM) [1] and in the serial uniform memory hierarchy (SUMH) model\n[5, 42]. Section 9 discusses related work, and Section 10 offers some concluding\nremarks.\nMany of the results in this thesis are based on a joint paper [21] coauthored by\nMatteo Frigo, Charles E. Leiserson, and Sridhar Ramachandran.\n\nSECTION 2\nMatrix multiplication\nThis section describes and analyzes an algorithm for multiplying an m\nn matrix\nby an n\np matrix cache-obliviously using Θ (mnp\n) work and incurring Θ (m\n+\nn\n+ p\n+\n( mn\n+ np\n+ mp\n)\nL\n+ mnp\nL\np\nZ\n) cache misses. These results require the\ntall-cache assumption (1.1) for matrices stored in row-major layout format, but the\nassumption can be relaxed for certain other layouts. We also show that Strassen's\nalgorithm [38] for multiplying n\nn matrices, which uses Θ (nlog2 7\n) work, incurs\nΘ (1\n+ n2\nL\n+ nlog2 7\nL\np\nZ\n) cache misses.\nThe following algorithm extends the optimal divide-and-conquer algorithm for\nsquare matrices described in [9] to rectangular matrices. To multiply an m\nn ma\ntrix A by an n\np matrix B, the algorithm halves the largest of the three dimensions\nand recurs according to one of the following three cases:\n\nA1\nA1B\nAB\n\nB\n\nA2B\n,\n(2.1)\nA2\n\nB1\nAB\n\nA1\nA2\nA1B1\n+ A2B2 ,\n(2.2)\nB2\n\nAB\n\nA B1\nB2\nAB1\nAB2\n.\n(2.3)\nIn case (2.1), we have m\nmax\nfn, p\ng. Matrix A is split horizontally, and both\nhalves are multiplied by matrix B. In case (2.2), we have n\nmax\nfm, p\ng. Both\nmatrices are split, and the two halves are multiplied. In case (2.3), we have p\n\nmax\nfm, n\ng. Matrix B is split vertically, and each half is multiplied by A. For square\nmatrices, these three cases together are equivalent to the recursive multiplication\n\n(a) 1 2 3 4 5 6 7 8\n(b)\n1 2\n3 4\n5 6\n7 8\n9 10\n11 12\n13 14\n15 16\n17 18\n19 20\n21 22\n23 24\n25 26\n27 28\n29 30\n31 32\n33 34\n35 36\n37 38\n39 40\n41 42\n43 44\n45 46\n47 48\n49 50\n51 52\n53 54\n55 56\n57 58\n59 60\n61 62\n63 64\n(c)\n(d)\nFigure 2-1: Layout of a 16\n16 matrix in (a) row major, (b) column major, (c) 4\n4-blocked,\nand (d) bit-interleaved layouts.\nalgorithm described in [9]. The base case occurs when m\nn\np\n1, in which\ncase the two elements are multiplied and added into the result matrix.\nAlthough this straightforward divide-and-conquer algorithm contains no tun\ning parameters, it uses cache optimally. To analyze the algorithm, we assume that\nthe three matrices are stored in row-major order, as shown in Figure 2-1(a). In\ntuitively, the cache-oblivious divide-and-conquer algorithm uses the cache effec\ntively, because once a subproblem fits into the cache, its smaller subproblems can\nbe solved in cache with no further cache misses.\nTheorem 1 The cache-oblivious matrix multiplication algorithm uses Θ (mnp\n) work and\nincurs Θ (m\n+ n\n+ p\n+\n( mn\n+ np\n+ mp\n)\nL\n+ mnp\nL\np\nZ\n) cache misses when multiplying\nan m\nn by an n\np matrix.\nProof.\nIt can be shown by induction that the work of this algorithm is Θ (mnp\n).\nTo analyze the cache misses, let α be a constant sufficiently small that three sub-\nmatrices of size m\n\nn\n, n\n\np\n, and m\n\np\n, where max\nfm\n, n\n, p\n\ng\nα\np\nZ, all fit\ncompletely in the cache. We distinguish the following four cases cases depending\non the initial size of the matrices.\n\nCase I: m, n, p\nα\np\nZ.\nThis case is the most intuitive. The matrices do not fit in cache, since all\ndimensions are \"big enough.\" The cache complexity of matrix multiplication\ncan be described by the recurrence\n\nΘ ((mn\n+ np\n+ mp\n)\nL\n)\nif\n(mn\n+ np\n+ mp\n)\nαZ ,\n\n2Q\n(m\n2, n, p\n)\n+ O\n(1\n)\notherwise and if m\nn and m\np ,\nQ\n(m, n, p\n)\n\n2Q\n(m, n\n2, p\n)\n+ O\n(1\n)\notherwise and if n\nm and n\np ,\n\n2Q\n(m, n, p\n)\n+ O\n(1\n)\notherwise .\n(2.4)\nThe base case arises as soon as all three submatrices fit in cache. The total\nnumber of lines used by the three submatrices is Θ ((mn\n+ np\n+ mp\n)\nL\n). The\nonly cache misses that occur during the remainder of the recursion are the\nΘ ((mn\n+ np\n+ mp\n)\nL\n) cache misses required to bring the matrices into cache.\nIn the recursive cases, when the matrices do not fit in cache, we pay for the\ncache misses of the recursive calls, which depend on the dimensions of the\nmatrices, plus O\n(1\n) cache misses for the overhead of manipulating submatri\nces. The solution to this recurrence is Q\n(m, n, p\n)\nΘ (mnp\nL\np\nZ\n).\nCase II: (m\nα\np\nZ and n, p\nα\np\nZ) OR (m\nα\np\nZ and n, p\nα\np\nZ) OR (p\n\nα\np\nZ and m, n\nα\np\nZ).\nHere, we shall present the case where m\nα\np\nZ and n, p\nα\np\nZ. The proofs\nfor the other cases are only small variations of this proof. The multiplication\nalgorithm always divides n or p by 2 according to cases (2.2) and (2.3). At\nsome point in the recursion, both are small enough that the whole problem\nfits into cache. The number of cache misses can be described by the recur\nrence\n\nΘ (1\n+ n\n+ np\nL\n+ m\n)\nif n, p\n\n[α\np\nZ\n2,α\np\nZ\n] ,\n\nQ\n(m, n, p\n)\n\n2Q\n(m, n\n2, p\n)\n+ O\n(1\n)\notherwise and if n\np ,\n\n2Q\n(m, n, p\n)\n+ O\n(1\n)\notherwise .\nThe solution to this recurrence is Θ (np\nL\n+ mnp\nL\np\nZ\n).\nCase III: (n, p\nα\np\nZ and m\nα\np\nZ) OR (m, p\nα\np\nZ and n\nα\np\nZ) OR\n(m, n\nα\np\nZ and p\nα\np\nZ).\nIn each of these cases, one of the matrices fits into cache, and the others do\nnot. Here, we shall present the case where n, p\nα\np\nZ and m\nα\np\nZ. The\nother cases can be proven similarly. The multiplication algorithm always\n\ndivides m by 2 according to case (2.1). At some point in the recursion, m is\nin the range α\np\nZ\nm\nα\np\nZ, and the whole problem fits in cache. The\nnumber cache misses can be described by the recurrence\n\nΘ (1\n+ m\n)\nif m\n\n[α\np\nZ\n2,α\np\nZ\n] ,\nQ\n(m, n\n)\n\n2Q\n(m\n2, n, p\n)\n+ O\n(1\n)\notherwise\n\nwhose solution is Q\n(m, n, p\n)\nΘ (m\n+ mnp\nL\np\nZ\n).\nCase IV: m, n, p\nα\np\nZ.\nFrom the choice of α, all three matrices fit into cache. The matrices are stored\non Θ (1\n+ mn\nL\n+ np\nL\n+ mp\nL\n) cache lines. Therefore, we have Q\n(m, n, p\n)\n\nΘ (1\n+\n( mn\n+ np\n+ mp\n)\nL\n).\nWe require the tall-cache assumption (1.1) in these analyses, because the matri\nces are stored in row-major order. Tall caches are also needed if matrices are stored\nin column-major order (Figure 2-1(b)), but the assumption that Z\nΩ(L2\n) can be\nrelaxed for certain other matrix layouts. The s\ns-blocked layout (Figure 2-1(c)),\nfor some tuning parameter s, can be used to achieve the same bounds with the\nweaker assumption that the cache holds at least some sufficiently large constant\nnumber of lines. The cache-oblivious bit-interleaved layout (Figure 2-1(d)) has the\nsame advantage as the blocked layout, but no tuning parameter need be set, since\nsubmatrices of size Θ (\np\nL\np\nL\n) are cache-obliviously stored on one cache line.\nThe advantages of bit-interleaved and related layouts have been studied in [18]\nand [12, 13]. One of the practical disadvantages of bit-interleaved layouts is that\nindex calculations on conventional microprocessors can be costly.\nFor square matrices, the cache complexity Q\n(n\n)\nΘ (n\n+ n2\nL\n+ n3\nL\np\nZ\n) of the\ncache-oblivious matrix multiplication algorithm is the same as the cache complex\nity of the cache-aware BLOCK-MULT algorithm and also matches the lower bound\nby Hong and Kung [25]. This lower bound holds for all algorithms that execute\nthe Θ (n3\n) operations given by the definition of matrix multiplication\nn\nX\ncij\n\naikbkj .\nk\nNo tight lower bounds for the general problem of matrix multiplication are known.\nBy using an asymptotically faster algorithm, such as Strassen's algorithm [38]\nor one of its variants [45], both the work and cache complexity can be reduced.\nWhen multiplying n\nn matrices, Strassen's algorithm, which is cache oblivious,\n\nrequires only 7 recursive multiplications of n\nn\n2 matrices and a constant\nnumber of matrix additions, yielding the recurrence\n\nQ\n(n\n)\n\nΘ (1\n+ n\n+ n2\nL\n)\nif n2\nαZ ,\n(2.5)\n7Q\n(n\n)\n+ O\n(n2\nL\n)\notherwise\n\nwhere α is a sufficiently small constant. The solution to this recurrence is Θ (n\n+\nL\n+ nlog2 7\nL\np\nZ\n).\nn\nSummary\nIn this section we have used the ideal-cache model to analyze two algorithms\nfor matrix multiplication. We have described an efficient cache-oblivious algo\nrithm for rectangular matrix multiplication and analyzed the cache complexity of\nStrassen's algorithm.\n\nSECTION 3\nMatrix transposition and FFT\nThis section describes an optimal cache-oblivious algorithm for transposing an\nm\nn matrix. The algorithm uses Θ (mn\n) work and incurs Θ (1\n+ mn\nL\n) cache\nmisses. Using matrix transposition as a subroutine, we convert a variant [44] of\nthe \"six-step\" fast Fourier transform (FFT) algorithm [6] into an optimal cache\n\noblivious algorithm. This FFT algorithm uses O\n(n lg n\n) work and incurs O 1\n+\n\n(n\nL\n) 1\n+ logZn\ncache misses.\nThe problem of matrix transposition is defined as follows. Given an m\nn ma\ntrix stored in a row-major layout, compute and store AT into an n\nm matrix B also\nstored in a row-major layout. The straightforward algorithm for transposition that\nemploys doubly nested loops incurs Θ (mn\n) cache misses on one of the matrices\nwhen mn\nZ, which is suboptimal.\nOptimal work and cache complexities can be obtained with a divide-and-con-\nquer strategy, however. If n\nm, we partition\n\nB1\nA\n( A1 A2\n) ,\nB\n\n.\nB2\nThen, we recursively execute TRANSPO SE\n(A1, B1\n) and TRANSP OSE\n(A2, B2\n). Alter\nnatively, if m\nn, we divide matrix A horizontally and matrix B vertically and\nlikewise perform two transpositions recursively. The next two theorems provide\nupper and lower bounds on the performance of this algorithm.\nTheorem 2 The cache-oblivious matrix-transpose algorithm involves Θ (mn\n) work and\nincurs Θ (1\n+ mn\nL\n) cache misses for an m\nn matrix.\n\nProof.\nThat the algorithm uses Θ (mn\n) work can be shown by induction. For the\ncache analysis, let Q\n(m, n\n) be the cache complexity of transposing an m\nn matrix.\nWe assume that the matrices are stored in row-major order, the column-major case\nhaving a similar analysis.\nn\nLet α be a constant sufficiently small that two submatrices of size m\n\nn\nand\n\nm\n, where max\nfm\n, n\n\ng\nαL, fit completely in the cache. We distinguish the\nfollowing three cases.\nCase I: max\nfm, n\ng\nαL.\nBoth the matrices fit in O\n(1\n)\n+ 2mn\nL lines. From the choice of α, the number\nof lines required is at most Z\nL, which implies Q\n(m, n\n)\nΘ (1\n+ mn\nL\n).\nCase II: m\nαL\nn OR n\nαL\nm.\nFor this case, we assume without loss of generality that m\nαL\nn. The\ncase n\nαL\nm is analogous. The transposition algorithm divides the\ngreater dimension n by 2 and performs divide-and-conquer. At some point\nin the recursion, n is in the range αL\nn\nαL, and the whole problem fits\nin cache. Because the layout is row-major, at this point the input array has\nn rows, m columns, and it is laid out in contiguous locations, thus requiring\nat most O\n(1\n+ nm\nL\n) cache misses to be read. The output array consists of\nnm elements in m rows, where in the worst case every row lies on a different\ncache line. Consequently, we incur at most O\n(m\n+ nm\nL\n) for writing the\noutput array. Since n\nαL\n2, the total cache complexity for this base case is\nO\n(1\n+ m\n).\nThese observations yield the recurrence\nΘ (1\n+ m\n)\nif n\n\n[αL\n2,αL\n] ,\nQ\n(m, n\n)\n\n2Q\n(m, n\n)\n+ O\n(1\n)\notherwise\n\nwhose solution is Q\n(m, n\n)\nΘ (1\n+ mn\nL\n).\nCase III: m, n\nαL.\nAs in Case II, at some point in the recursion, both n and m fall in the interval\n[αL\n2,αL\n]. The whole problem then fits into cache, and it can be solved with\nat most O\n(m\n+ n\n+ mn\nL\n) cache misses.\nThe cache complexity thus satisfies the recurrence\n\nΘ (m\n+ n\n+ mn\nL\n)\nif m, n\n\n[αL\n2,αL\n] ,\nQ\n(m, n\n)\n\n2Q\n(m\n2, n\n)\n+ O\n(1\n)\nif m\nn ,\n2Q\n(m, n\n)\n+ O\n(1\n)\notherwise\n\nwhose solution is Q\n(m, n\n)\nΘ (1\n+ mn\nL\n).\n\nTheorem 3 The cache-oblivious matrix-transpose algorithm is asymptotically optimal.\nProof.\nFor an m\nn matrix, the matrix-transposition algorithm must write to mn\ndistinct elements, which occupy at least\ndmn\nL\ne\nΩ(1\n+ mn\nL\n) cache lines.\nAs an example application of the cache-oblivious transposition algorithm, the\nrest of this section describes and analyzes a cache-oblivious algorithm for comput\ning the discrete Fourier transform of a complex array of n elements, where n is an\nexact power of 2. The basic algorithm is the well-known \"six-step\" variant [6, 44] of\nthe Cooley-Tukey FFT algorithm [15]. By using the cache-oblivious transposition\nalgorithm, however, we can make the FFT cache oblivious, and its performance\nmatches the lower bound by Hong and Kung [25].\nRecall that the discrete Fourier transform (DFT) of an array X of n complex\nnumbers is the array Y given by\nn\n-1\nX\n-ij\nY\n[i\n]\n\nX\n[ j\n]ωn\n,\n(3.1)\nj\nwhere ωn\ne2π\np\n-1\nn is a primitive nth root of unity, and 0\ni\nn.\nMany known algorithms evaluate Equation (3.1) in time O\n(n lg n\n) for all inte\ngers n [17]. In this thesis, however, we assume that n is an exact power of 2, and\ncompute Equation (3.1) according to the Cooley-Tukey algorithm, which works re\ncursively as follows. In the base case where n\nO\n(1\n), we compute Equation (3.1)\ndirectly. Otherwise, for any factorization n\nn1n2 of n, we have\n\nn2\n-1\nn1\n-1\nX\nX\n-i1 j2\n-i2 j2\nY\n[i1\n+ i2n1\n]\n\nX\n[ j1n2\n+ j2\n]ω\n-i1 j1\nA ωn\nωn2\n.\n(3.2)\nn1\nj2\nj1\nObserve that both the inner and outer summations in Equation (3.2) are DFT's.\nOperationally, the computation specified by Equation (3.2) can be performed by\ncomputing n2 transforms of size n1 (the inner sum), multiplying the result by the\nfactors ωn\n-i1 j2 (called the twiddle factors [17]), and finally computing n1 transforms\nof size n2 (the outer sum).\nWe choose n1 to be 2\nd\n(lg n\n)\ne and n2 to be 2\nb\n(lg n\n)\nc . The recursive step then\noperates as follows:\n1. Pretend that the input is a row-major n1\nn2 matrix A. Transpose A in place,\ni.e., use the cache-oblivious algorithm to transpose A onto an auxiliary array\nB, and copy B back onto A. (If n1\n2n2, consider the matrix to be made up\nof records containing two elements.)\n\n2. At this stage, the inner sum corresponds to a DFT of the n2 rows of the trans\nposed matrix. Compute these n2 DFT's of size n1 recursively. Observe that,\nbecause of the previous transposition, we are transforming a contiguous ar\nray of elements.\n3. Multiply A by the twiddle factors, which can be computed on the fly with no\nextra cache misses.\n4. Transpose A in-place, so that the inputs to the next stage are arranged in\ncontiguous locations.\n5. Compute n1 DFT's of the rows of the matrix, recursively.\n6. Transpose A in-place so as to produce the correct output order.\nIt can be proven by induction that the work complexity of this FFT algorithm\nis O\n(n lg n\n). We now analyze its cache complexity. The algorithm always operates\non contiguous data, by construction. In order to simplify the analysis of the cache\ncomplexity, we assume a tall cache, in which case each transposition operation and\nthe multiplication by the twiddle factors require at most O\n(1\n+ n\nL\n) cache misses.\nThus, the cache complexity satisfies the recurrence\nO\n(1\n+ n\nL\n),\nif\n\nn\nαZ ,\nQ\n(n\n)\n\n(3.3)\nn1Q\n(n2\n)\n+ n2Q\n(n1\n)\n+ O\n(1\n+ n\nL\n)\notherwise\n\nfor a sufficiently small constant α chosen such that a subproblem of size αZ fits in\ncache. This recurrence has solution\n\nQ\n(n\n)\nO 1\n+\n( n\nL\n) 1\n+ logZn\n,\nwhich is asymptotically optimal for a Cooley-Tukey algorithm, matching the lower\nbound by Hong and Kung [25] when n is an exact power of 2. As with matrix mul\ntiplication, no tight lower bounds for cache complexity are known for the general\nproblem of computing the DFT.\nSummary\nIn this section, we have described an optimal cache-oblivious algorithm for FFT.\nThe basic algorithm is the well-known \"six-step\" variant [6, 44] of the Cooley-\nTukey FFT algorithm [15]. By using an optimal cache-oblivious transposition al\ngorithm, however, we can make the FFT cache oblivious, and its performance\nmatches the lower bound by Hong and Kung [25].\n\nSECTION 4\nFunnelsort\nAlthough it is cache oblivious, algorithms like familiar two-way merge sort (see\nSection 7.3) are not asymptotically optimal with respect to cache misses. The Z-\nway mergesort mentioned by Aggarwal and Vitter [3] is optimal in terms of cache\ncomplexity, but it is cache aware. This section describes a cache-oblivious sorting\nalgorithm called \"funnelsort.\" This algorithm has an asymptotically optimal work\n\ncomplexity Θ (n lg n\n), as well as an optimal cache complexity Θ 1\n+\n( n\nL\n)(1\n+\n\nlogZn\n) if the cache is tall. In Section 5, we shall present another cache-oblivious\nsorting algorithm based on distribution sort.\nFunnelsort is similar to mergesort. In order to sort a (contiguous) array of n\nelements, funnelsort performs the following two steps:\n1. Split the input into n1\n3 contiguous arrays of size n2\n3, and sort these arrays\nrecursively.\n2. Merge the n1\n3 sorted sequences using a n1\n3-merger, which is described be\nlow.\nFunnelsort differs from mergesort in the way the merge operation works. Merg\ning is performed by a device called a k-merger, which inputs k sorted sequences\nand merges them. A k-merger operates by recursively merging sorted sequences\nthat become progressively longer as the algorithm proceeds. Unlike mergesort,\nhowever, a k-merger stops working on a merging subproblem when the merged\noutput sequence becomes \"long enough,\" and it resumes working on another\nmerging subproblem.\n\nL1\nL\np\nk\nR\nbuffer\nbuffer\nbuffer\nFigure 4-1: Illustration of a k-merger. A k-merger is built recursively out of\np\nk left\np\nk-\nmergers\nL1,\nL2, . . . , L\np\nk, a series of buffers, and one right\np\nk-merger\nR.\nSince this complicated flow of control makes a k-merger a bit tricky to describe,\nwe explain the operation of the k-merger pictorially. Figure 4-1 shows a repre\nsentation of a k-merger, which has k sorted sequences as inputs. Throughout its\nexecution, the k-merger maintains the following invariant.\nInvariant The invocation of a k-merger outputs the first k3 elements of the sorted sequence\nobtained by merging the k input sequences.\nA k-merger is built recursively out of\np\nk-mergers in the following way. The k\ninputs are partitioned into\np\nk sets of\np\nk elements, and these sets form the input\nto the\np\nk left\np\nk-mergers\nL1,\nL2, . . . , L\np\nk in the left part of the figure. The out\nputs of these mergers are connected to the inputs of\np\nk buffers. Each buffer is a\nFIFO queue that can hold 2k3\n2 elements. Finally, the outputs of the buffers are\nconnected to the\np\nk inputs of the right\np\nk-merger\nR in the right part of the figure.\nThe output of this final\np\nk-merger becomes the output of the whole k-merger. The\nreader should notice that the intermediate buffers are overdimensioned. In fact,\neach buffer can hold 2k3\n2 elements, which is twice the number k3\n2 of elements\noutput by a\np\nk-merger. This additional buffer space is necessary for the correct\nbehavior of the algorithm, as will be explained below. The base case of the recur\nsion is a k-merger with k\n2, which produces k3\n8 elements whenever invoked.\nA k-merger operates recursively in the following way. In order to output k3\nelements, the k-merger invokes\nR k3\n2 times. Before each invocation, however, the\nk-merger fills all buffers that are less than half full, i.e., all buffers that contain less\nthan k3\n2 elements. In order to fill buffer i, the algorithm invokes the corresponding\n\nleft merger\nLi once. Since\nLi outputs k3\n2 elements, the buffer contains at least k3\nelements after\nLi finishes.\nIn order to prove this result, we need three auxiliary lemmata. The first lemma\nbounds the space required by a k-merger.\nLemma 4 A k-merger can be laid out in O\n(k2\n) contiguous memory locations.\nProof.\nA k-merger requires O\n(k2\n) memory locations for the buffers, plus the space\nrequired by the\np\nk-mergers. The space S\n(k\n) thus satisfies the recurrence\nS\n(k\n)\n\n(\np\nk\n+ 1\n)S\n(\np\nk\n)\n+ O\n(k2\n) ,\nwhose solution is S\n(k\n)\nO\n(k2\n).\nIt follows from Lemma 4, that a problem of size α\np\nZ can be solved in cache\nwith no further cache misses, where α is a sufficiently small constant.\nIn order to achieve the bound on the number Q\n(n\n) of cache misses, it is im\nportant that the buffers in a k-merger be maintained as circular queues of size k.\nThis requirement guarantees that we can manage the queue cache-efficiently, in\nthe sense stated by the next lemma.\nLemma 5 Performing r insert and remove operations on a circular queue causes O\n(1\n+\nr\nL\n) cache misses if four cache lines are available for the buffer.\nProof.\nAssociate the two cache lines with the head and tail of the circular queue.\nThe head- and tail-pointers are kept on two seperate lines. Since the replacement\nstrategy is optimal, it will keep the frequently accessed pointers in cache. If a new\ncache line is read during an insert (delete) operation, the next L\n- 1 insert (delete)\noperations do not cause a cache miss. The result follows.\nDefine QM to be the number of cache misses incurred by a k-merger. The next\nlemma bounds the number of cache misses incurred by a k-merger.\nLemma 6 On a tall cache, one invocation of a k-merger incurs\n\nQM\n(k\n)\nO k\n+ k3\nL\n+ k3logZk\nL\ncache misses.\nProof.\nThere are two cases: either k\nα\np\nZ or k\nα\np\nZ.\nAssume first that k\nα\np\nZ. By Lemma 4, the data structure associated with\nthe k-merger requires at most O\n(k2\n)\nO\n(Z\n) contiguous memory locations. By the\nchoice of α the k-merger fits into cache. The k-merger has k input queues, from\n\nwhich it loads O\n(k3\n) elements. Let ri be the number of elements extracted from the\nith input queue. Since k\nα\np\nZ and L\nO\n(\np\nZ\n), there are at least Z\nL\nΩ(k\n)\ncache lines available for the input buffers. Lemma 5 applies, whence the total\nnumber of cache misses for accessing the input queues is\nk\nX\nO\n(1\n+ ri\nL\n)\nO\n(k\n+ k3\nL\n) .\ni\nSimilarly by Lemma 5, the cache complexity of writing the output queue is at most\nO\n(1\n+ k3\nL\n). Finally, for touching the O\n(k2\n) contiguous memory locations used by\nthe internal data structures, the algorithm incurs at most O\n(1\n+ k2\nL\n) cache misses.\nThe total cache complexity is therefore\n\nQM\n(k\n)\n\nO k\n+ k3\nL\n+ O 1\n+ k2\nL\n+ O 1\n+ k3\nL\n\nO k\n+ k3\nL\ncompleting the proof of the first case.\nAssume now that k\nα\np\nZ. In this second case, we prove by induction on k\nthat whenever k\nα\np\nZ, we have\nQM\n(k\n)\n\n(ck3logZk\n)\nL\n- A\n(k\n) ,\n(4.1)\nfor some constant c\n0, where A\n(k\n)\nk\n(1\n+\n(2clogZk\n)\nL\n)\no\n(k3\n). The lower-\norder term A\n(k\n) does not affect the asymptotic behavior, but it makes the induction\ngo through. This particular value of A\n(k\n) will be justified later in the analysis.\nThe base case of the induction consists of values of k such that\npαZ1\nk\n\nα\np\nZ. (It is not sufficient to just consider k\nΘ (\np\nZ\n), since k can become as small\nas Θ (Z1\n) in the recursive calls.) The analysis of the first case applies, yielding\n\nQM\n(k\n)\nO k\n+ k3\nL . Because k2\nα\np\nZ\nΩ(L\n) and k\nΩ(1\n), the last term\n\ndominates, and QM\n(k\n)\nO k3\nL\n\nholds. Consequently, a large enough value of c\ncan be found that satisfies Inequality (4.1).\nFor the inductive case, let k\nα\np\nZ. The k-merger invokes the\np\nk-mergers\nrecursively. Since\npαZ1\n\np\nk\nk, the inductive hypothesis can be used to\nbound the number QM\n(\np\nk\n) of cache misses incurred by the submergers. The right\nmerger\nR is invoked exactly k3\n2 times. The total number l of invocations of left\nmergers is bounded by l\nk3\n+ 2\np\nk. To see why, consider that every invocation\nof a left merger puts k3\n2 elements into some buffer. Since k3 elements are output\nand the buffer space is 2k2, the bound l\nk3\n+ 2\np\nk follows.\nBefore invoking\nR, the algorithm must check every buffer to see whether it\nis empty. One such check requires at most\np\nk cache misses, since there are\np\nk\n\nbuffers. This check is repeated exactly k3\n2 times, leading to at most k2 cache misses\nfor all checks.\nThese considerations lead to the recurrence\n\nQM\n(k\n)\n\n2k3\n+ 2\np\nk\nQM\n(\np\nk\n)\n+ k2 .\nApplication of the inductive hypothesis yields the desired bound Inequality (4.1),\nas follows.\n\nQM\n(k\n)\n\n2k3\n+ 2\np\nk\nQM\n(\np\nk\n)\n+ k2\n\n2 k3\n+\np\nk\nck3\n2logZk\n- A\n(\np\nk\n)\n+ k2\n2L\n\n(ck3logZk\n)\nL\n+ k2 1\n+\n( clogZk\n)\nL\n\n2k3\n+ 2\np\nk\nA\n(\np\nk\n) .\n-\nIf A\n(k\n)\nk\n(1\n+\n( 2clogZk\n)\nL\n) (for example), we get\n\nQM\n(k\n)\n\n(ck3logZk\n)\nL\n+ k2 1\n+\n( clogZk\n)\nL\n\n- 2k3\n+ 2\np\nk\np\nk 1\n+ 2clogZ\np\nk\nL\n\n(ck3logZk\n)\nL\n+ k2 1\n+\n( clogZk\n)\nL\n\n- 2k2\n+ 2k\n+ clogZk\nL\n\n(ck3logZk\n)\nL\n-\n( k2\n+ 2k\n) 1\n+\n( clogZk\n)\nL\n\n(ck3logZk\n)\nL\n- A\n(k\n)\nand Inequality (4.1) follows.\nIt can be proven by induction that the work complexity of funnelsort is O\n(n lg n\n).\nThe next theorem gives the cache complexity of funnelsort.\nTheorem 7 Funnelsort sorts n elements incurring at most Q\n(n\n) cache misses, where\n\nQ\n(n\n)\nO 1\n+\n( n\nL\n) 1\n+ logZn\n.\nProof.\nIf n\nαZ for a small enough constant α, then the funnelsort data structures\nfit into cache. To see why, observe that only one k-merger is active at any time.\nThe biggest k-merger is the top-level n1\n3-merger, which requires O\n(n2\n)\nO\n(n\n)\nspace. The algorithm thus can operate in O\n(1\n+ n\nL\n) cache misses.\nIf n\nαZ, we have the recurrence\nQM\n(n1\nQ\n(n\n)\nn1\n3Q\n(n\n)\n+\n) .\n\nBy Lemma 6, we have QM\n(n1\n)\nO n1\n+ n\nL\n+\n(nlogZn\n)\nL .\nWith the hypothesis Z\nΩ(L2\n), we have n\nL\nΩ(n1\n). Moreover, we also\n\nhave n1\nΩ(1\n) and lg n\nΩ(lg Z\n). Consequently, QM\n(n1\n)\nO\n(nlogZn\n)\nL\nholds, and the recurrence simplifies to\n\nQ\n(n\n)\nn1\n3Q\n(n\n)\n+ O\n(nlogZn\n)\nL\n.\nThe result follows by induction on n.\nThis upper bound matches the lower bound stated by the next theorem, prov\ning that funnelsort is cache-optimal.\nTheorem 8 The cache complexity of any sorting algorithm is\n\nQ\n(n\n)\nΩ1\n+\n(n\nL\n) 1\n+ logZn\n.\n\nProof.\nAggarwal and Vitter [3] show that there is an Ω\n(n\nL\n)logZ\nL\n(n\nZ\n) bound\non the number of cache misses made by any sorting algorithm on their \"out-of-\ncore\" memory model, a bound that extends to the ideal-cache model. By applying\nthe tall-cache assumption Z\nΩ(L2\n), we have\nQ\n(n\n)\n\na\n(n\nL\n)logZ\nL\n(n\nZ\n)\n\na\n(n\nL\n) lg\n(n\nZ\n)\n(lg Z\n- lg L\n)\n\na\n(n\nL\n) lg\n(n\nZ\n)\nlg Z\n\na\n(n\nL\n) lg n\nlg Z\n- a\n(n\nL\n) .\nIt follows that Q\n(n\n)\nΩ((n\nL\n)logZn\n). The theorem can be proven by combining\nthis result with the trivial lower bounds of Q\n(n\n)\nΩ(1\n) and Q\n(n\n)\nΩ(n\nL\n).\nCorollary 9 The cache-oblivious Funnelsort is asymptotically optimal.\nProof.\nFollows from Theorems 8 and 7.\nSummary\nIn this section we have presented an optimal cache-oblivious algorithm based on\nmergesort. Funnelsort uses a device called a k-merger, which inputs k sorted se\nquences and merges them in \"chunks\". It stops when the merged output becomes\n\"long enough\" to resume work on another subproblem. Further, we have shown\n\nthat any sorting algorithm incurs at least Ω1\n+\n(n\nL\n) 1\n+ logZn\ncache misses.\nThis lower bound is matched by both our algorithms.\n\nSECTION 5\nDistribution sort\nIn this section, we describe a cache-oblivious optimal sorting algorithm based on\ndistribution sort. Like the funnelsort algorithm from Section 4, the distribution-\nsorting algorithm uses O\n(n lg n\n) work to sort n elements, and it incurs\n\nΘ 1\n+\n( n\nL\n) 1\n+ logZn\ncache misses if the cache is tall. Unlike previous cache-efficient distribution-sorting\nalgorithms [1, 3, 30, 42, 44], which use sampling or other techniques to find the\npartitioning elements before the distribution step, our algorithm uses a \"bucket-\nsplitting\" technique to select pivots incrementally during the distribution.\nGiven an array A (stored in contiguous locations) of length n, the cache-oblivi-\nous distribution sort sorts A as follows:\n1. Partition A into\npn contiguous subarrays of size\npn. Recursively sort each\nsubarray.\n2. Distribute the sorted subarrays into q\n\npn buckets B1, B2, . . . , Bq of size n1,\nn2, . . . , nq, respectively, such that for i\n1, 2, . . . , q\n- 1, we have\n1. max\nfx\nj x\nBi\ng\nmin\nfx\nj x\nBi\n+1\ng ,\n2. ni\npn .\n(See below for details.)\n3. Recursively sort each bucket.\n4. Copy the sorted buckets back to array A.\n\nA stack-based memory allocator is used to exploit spatial locality. A nice prop\nerty of stack based allocation is that memory is not fragmented for problems of\nsmall size. So if the space complexity of a procedure is S, only O\n(1\n+ S\nL\n) cache\nmisses are made when S\nZ, provided the procedure accesses only its local vari\nables.\nDistribution step\nThe goal of Step 2 is to distribute the sorted subarrays of A into q buckets B1,\nB2, . . . , Bq. The algorithm maintains two invariants. First, each bucket holds at\nmost 2\npn elements at any time, and any element in bucket Bi is smaller than any\nelement in bucket Bi\n+1. Second, every bucket has an associated pivot, a value\nwhich is greater than all elements in the bucket. Initially, only one empty bucket\nexists with pivot\n. At the end of Step 2, all elements will be in the buckets and\nthe two conditions (a) and (b) stated in Step 2 will hold.\nThe idea is to copy all elements from the subarrays into the buckets cache effi\nciently while maintaining the invariants. We keep state information for each sub-\narray and for each bucket. The state of a subarray consists of an index next of the\nnext element to be read from the subarray and a bucket number bnum indicating\nwhere this element should be copied. By convention, bnum\n\nif all elements in\na subarray have been copied. The state of a bucket consists of the bucket's pivot\nand the number of elements currently in the bucket.\nWe would like to copy the element at position next of a subarray to bucket\nbnum. If this element is greater than the pivot of bucket bnum, we would incre\nment bnum until we find a bucket for which the element is smaller than the pivot.\nUnfortunately, this basic strategy has poor caching behavior, which calls for a more\ncomplicated procedure.\nThe distribution step is accomplished by the recursive procedure DISTRIBU TE.\nDISTRIB UTE\n(i, j, m\n) distributes elements from the ith through\n(i\n+ m\n- 1\n)th sub-\narrays into buckets starting from Bj. Given the precondition that each subarray\nr\ni, i\n+ 1, . . . , i\n+ m\n- 1 has its bnum\n[r\n]\nj, the execution of DISTRIBU TE\n(i, j, m\n)\nenforces the postcondition that bnum\n[r\n]\nj\n+ m. Step 2 of the distribution sort in\nvokes DISTRIB UTE\n(1, 1,\npn\n). The following is a recursive implementation of DIS\nTRIBUTE:\n\nDISTRIB UTE\n(i, j, m\n)\n1 if m\nthen COPYELEMS\n(i, j\n)\nelse DISTRIBUTE\n(i, j, m\n)\nDISTRI BUTE\n(i\n+ m\n2, j, m\n)\nDISTRI BUTE\n(i, j\n+ m\n2, m\n)\nDISTRI BUTE\n(i\n+ m\n2, j\n+ m\n2, m\n)\nIn the base case (line 1), the subroutine COPYELEMS\n(i, j\n) copies all elements from\nsubarray i that belong to bucket j. If bucket j has more than 2\npn elements after the\ninsertion, it can be split into two buckets of size at least\npn. For the splitting oper\nation, we use the deterministic median-finding algorithm [16, p. 189] followed by\na partition. The next lemma shows that the median-finding algorithm uses O\n(m\n)\nwork and incurs O\n(1\n+ m\nL\n) cache misses to find the median of an array of size m.\n(In our case, we have m\npn\n+ 1.) In addition, when a bucket splits, all sub-\narrays whose bnum is greater than the bnum of the split bucket must have their\nbnum's incremented. The analysis of DISTRIBU TE is given by the following two\nlemmata.\nLemma 10 The median of m elements can be found cache-obliviously using O\n(m\n) work\nand incurring O\n(1\n+ m\nL\n) cache misses.\nProof.\nSee [16, p. 189] for the linear-time median finding algorithm and the work\nanalysis. The cache complexity is given by the same recurrence as the work com\nplexity with a different base case.\nO\n(1\n+ m\nL\n)\nif m\nαZ ,\nQ\n(m\n)\n\nQ\n(\ndm\ne\n)\n+ Q\n(7m\n+ 6\n)\n+ O\n(1\n+ m\nL\n)\notherwise ,\nwhere α is a sufficiently small constant. The result follows.\nLemma 11 Step 2 uses O\n(n\n) work, incurs O\n(1\n+ n\nL\n) cache misses, and uses O\n(n\n) stack\nspace to distribute n elements.\nProof.\nIn order to simplify the analysis of the work used by DISTRIBU TE, assume\nthat COPYELEMS uses O\n(1\n) work. We account for the work due to copying ele\nments and splitting of buckets separately. The work of DISTRI BUTE on m subarrays\nis described by the recurrence\nT\n(m\n)\n4T\n(m\n)\n+ O\n(1\n) .\n\nIt follows that T\n(m\n)\nO\n(m2\n), where m\n\npn initially.\nWe now analyze the work used for copying and bucket splitting. The number\nof copied elements is O\n(n\n). Each element is copied exactly once and therefore the\nwork due to copying elements is also O\n(n\n). The total number of bucket splits is\nat most\npn. To see why, observe that there are at most\npn buckets at the end of\nthe distribution step, since each bucket contains at least\npn elements. Each split\noperation involves O\n(\npn\n) work and so the net contribution to the work is O\n(n\n).\nThus, the total work used by DISTRI BUTE is W\n(n\n)\nO\n(T\n(\npn\n))\n+ O\n(n\n)\n+ O\n(n\n)\n\nO\n(n\n).\nFor the cache analysis, we distinguish two cases. Let α be a sufficiently small\nconstant such that the stack space used by sorting a problem of size αZ, including\nthe input array, fits completely into cache.\nCase I: n\nαZ.\nThe input and the auxiliary space of size O\n(n\n) fit into cache using O\n(1\n+ n\nL\n)\ncache lines. Consequently, the cache complexity is O\n(1\n+ n\nL\n).\nCase II: n\nαZ.\nLet R\n(m, d\n) denote the cache misses incurred by an invocation of the subrou\ntine DISTRIBU TE\n(i, j, m\n) that copies d elements from m subarrays to m buck-\nets. We again account for the splitting of buckets separately. We first prove\nthat R satisfies the following recurrence:\nO\n(L\n+ d\nL\n)\nif m\nαL ,\nR\n(m, d\n)\n\nP\n(5.1)\ni\n4 R\n(m\n2, di\n)\notherwise ,\nP\nwhere\ni\n4 di\nd.\nFirst, consider the base case m\nαL. An invocation of DISTRIBU TE\n(i, j, m\n)\noperates with m subarrays and m buckets. Since there are Ω(L\n) cache lines,\nthe cache can hold all the auxiliary storage involved and the currently ac\ncessed element in each subarray and bucket. In this case there are O\n(L\n+\nd\nL\n) cache misses. The initial access to each subarray and bucket causes\nO\n(m\n)\nO\n(L\n) cache misses. The cache complexity for copying the d elements\nfrom one set of contiguous locations to another set of contiguous locations is\nO\n(1\n+ d\nL\n), which completes the proof of the base case. The recursive case,\nwhen m\nαL, follows immediately from the algorithm. The solution for\nRecurrence 5.1 is R\n(m, d\n)\nO\n(L\n+ m2\nL\n+ d\nL\n).\nWe still need to account for the cache misses caused by the splitting of buck\nets.\nEach split causes O\n(1\n+\npn\nL\n) cache misses due to median finding\n\n(Lemma 10) and partitioning of\npn contiguous elements.\nAn additional\nO\n(1\n+\npn\nL\n) misses are incurred by restoring the cache. As proven in the\nwork analysis, there are at most\npn split operations.\nBy adding R\n(\npn, n\n) to the complexity of splitting, we conclude that the total\ncache complexity of the distribution step is O\n(L\n+ n\nL\n+\npn\n(1\n+\npn\nL\n))\n\nO\n(n\nL\n).\n\nTheorem 12 Distribution sort uses O\n(n lg n\n) work and incurs O 1\n+\n(n\nL\n) 1\n+ logZn\ncache misses to sort n elements.\nProof.\nThe work done by the algorithm is given by\nq\np\nX\nW\n(n\n)\n\np\nnW\n(\nn\n)\n+\nW\n(ni\n)\n+ O\n(n\n) ,\ni\nPq\nwhere q\n\npn, each ni\npn, and\ni\n1 ni\nn. The solution to this recurrence is\nW\n(n\n)\nO\n(n lg n\n).\nThe space complexity of the algorithm is given by\nS\n(n\n)\nS\n(2\np\nn\n)\n+ O\n(n\n) .\np\nEach bucket has at most 2\npn elements, thus the recursive call uses at S\n(\nn\n) space\nand the O\n(n\n) term comes from Step 2. The solution to this recurrence is S\n(n\n)\n\nO\n(n\n).\nThe cache complexity of distribution sort is described by the recurrence\n\nO\n(1\n+ n\nL\n)\nif n\nαZ ,\n\np\nX\nQ\n(n\n)\n\np\nnQ\n(\nn\n)\n+\nq\nQ\n(ni\n)\n+ O\n(1\n+ n\nL\n)\notherwise ,\n\ni\nwhere α is a sufficiently small constant such that the stack space used by a sorting\nproblem of size αZ, including the input array, fits completely in cache. The base\ncase n\nαZ arises when both the input array A and the contiguous stack space\nof size S\n(n\n)\nO\n(n\n) fit in O\n(1\n+ n\nL\n) cache lines of the cache. In this case, the\nalgorithm incurs O\n(1\n+ n\nL\n) cache misses to touch all involved memory locations\nonce. In the case where n\nαZ, the recursive calls in Steps 1 and 3 cause Q\n(\npn\n)\n+\nPq\ni\n1 Q\n(ni\n) cache misses and O\n(1\n+ n\nL\n) is the cache complexity of Steps 2 and 4,\nas shown by Lemma 11. The theorem now follows by solving the recurrence.\nCorollary 13 The cache-oblivious distribution sort algorithm is asymptotically optimal.\nProof.\nFollows from Theorems 8 and 12.\n\nSummary\nIn this section, we have presented another optimal cache-oblivious sorting algo\nrithm, which is based on distribution sort. All previous cache-efficient distribution\nsort algorithms [1, 3, 30, 42, 44] are cache aware, since they are designed for caching\nmodels where the data is moved explicitly. They usually use a sampling processes\nto find the partitioning elements before the distribution step. Our algorithm finds\nthe pivots incrementally during the distribution.\n\nSECTION 6\nJacobi multipass filter\nThis section compares an optimal recursive algorithm with a more straightforward\niterative algorithm, both which compute a multipass filter over one-dimensional\ndata. When computing n generations on n elements, both algorithms use Θ (n2\n)\nwork. The iterative incurs Θ (n2\nL\n) cache misses, if the data does not fit into the\ncache, where the recursive algorithm incurs only Θ (1\n+ n\nL\n+ n2\nZL\n) cache misses\nwhich we prove to be cache optimal. We also provide some brief empirical results\nfor this problem. The recursive algorithm executes in less than 70% of the time of\nthe iterative algorithm for problem sizes that do not fit in L2-cache\nConsider the problem of a computing a multipass filter on an array A of size\nn, where a new value A\n(τ\n+1\n) at generation τ\n+ 1 is computed from values at the\ni\nprevious step τ according to some update rule. A typical update function is\n\nA\n(τ\n+1\n)\n\nA\n(τ\n)\ni\ni\n-1\n+ A\n(τ\n)\n+ A\n(τ\n)\n3 .\n(6.1)\ni\ni\n+1\nApplications of multipass filtering include the Jacobi iteration for solving heat-\ndiffusion equations [31, p. 673] and the simulation of lattice gases with cellular\nautomata. These applications usually deal with multidimensional data, but here,\nwe shall explore the one-dimensional case for simplicity, even though caching ef\nfects are often more pronounced with multidimensional data.\n\nJACOBI-ITER\n(A\n)\n1 n\nlength of A\n2 for i\n1 to n\ndo for j\n1 to n\nGeneration 2i\n\ndo tmp\n[ j\n]\n\nA\n[( j\n- 1\n) mod n\n]\n+ A\n[ j\n]\n+ A\n[( j\n+ 1\n) mod n\n]\nfor j\n1 to n\nGeneration 2i\n+ 1\n\ndo A\n[ j\n]\ntmp[( j\n- 1\n) mod n\n]\n+ tmp[ j\n]\n+ tmp\n[( j\n+ 1\n) mod n\n]\nFigure 6-1: Iterative implementation of n-pass Jacobi update on array A with n elements.\n6.1\nIterative algorithm\nWe first analyze the cache complexity of the straightforward implementation JA-\nCOBI-ITER of the update rule given in Equation (6.1). We show that this algorithm,\nshown in Figure 6-1, uses Θ (n\n) temporary storage and performs Θ (n2\n) memory\naccesses for an array of size n. If the array of size n does not fit into cache, the total\nnumber of cache misses is Θ (n2\nL\n).\nTo illustrate the order of updates of JACOBI-ITER on input A of size n, we view\nthe computation of n generations of the multipass as a two-dimensional trace ma\ntrix\nT of size n\nn. One dimension of\nT is the offset in the input array and the\nother dimension is the \"generation\" of the filtered result. The value of element\nT4,2 is the value of array element A\n[2\n] at the 4th generation of the iterative algo\nrithm. One row in the matrix represents the updates on one element in the array.\nThe trace matrix of the iterative algorithm on a data array of size 16 is shown in\nFigure 6-2. The height of a bar represents the ordering of the updates, where the\nhigher bars are updated later. The bigger the difference in the height of two bars,\nthe further apart in time are their updates. If the height of a bar is not much bigger\nthan the height of the bar directly in front of it, it is likely that the element is still in\ncache and a hit occurs. The height differences between two updates to the same el\nement in the iterative algorithm are all equal. Either the updates are close enough\ntogether that all updates are cache hits, or they are too far apart, and all updates\nare cache misses.\nTheorem 14 The JACOBI-ITER algorithm uses Θ (n2\n) work when computing n genera\ntions on an array of size n. JACOBI-ITER incurs Θ (1\n+ n\nL\n) cache misses if the data fits\ninto cache, and it incurs Θ (n2\nL\n) cache misses if the array does not fit into cache.\nProof.\nSince there are two nested loops, each of which performs n iterations, the\nwork is Θ (n2\n).\n\ndata array A\ndata array A\ngeneration\ngeneration\ntime\ntime\ndata array A\ndata array A\ngeneration\ngeneration\n(a)\n(b)\n241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256\n225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240\n209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224\n193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208\n177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192\n161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176\n145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160\n129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128\n99 100 101 102 103 104 105 106 107 108 109 110 111 112\nFigure 6-2: Ordering of updates of JACOBI-ITER on an array with 16 elements. (a) The\ntrace matrix with the order of updates from 1 to 256. (b) A bar-graph illustrating the\nupdates, where the height of a bar represents the ordering of updates and the smallest bar\nis updated first.\nWe now analyze the cache misses of JACOBI-ITER on a\n(Z, L\n) ideal cache. Let α\nbe a constant sufficiently small that αZ elements fit into a cache of size Z. As long\nas the array and the temporary storage fit into cache, e.g., n\nαZ, the algorithm\nperforms well. The cache complexity is only Θ (1\n+ n\nL\n) since the O\n(n\n) elements\nare read (in order) only once.\nIf the array has size n\nαZ, however, then it does not all fit in cache at one time.\nThe optimal replacement strategy can keep at most O\n(Z\n) elements in cache. Thus,\nper iteration we have Ω(n\nL\n- Z\n) updates which are cache misses. Consequently,\nthe total number of cache misses is Θ (n\n)Ω(n\nL\n- Z\n)\nΩ(n2\nL\n) for the n iterations.\nThe algorithm can be optimized to use only O\n(1\n) temporary storage and avoid\nthe modulo computation, but the number of cache misses remains at Θ (n2\nL\n).\n6.2\nRecursive algorithm\nIn this section, we present an optimal recursive algorithm to compute an n-pass\nJacobi filter. This cache-oblivious algorithm JACOBI-REC is sketched in Figure 6-3,\nwhere the input size is a power of 2.1 We prove that the work used by JACOBI-\n1The algorithm for the general case is slightly more complicated.\n\nJACOBI\n\n(A, n, s, w, τ\n)\n1 if w\nthen JACOBI\n\n(A, n, s, w\n2, τ\n)\nJACOBI\n\n(A, n,\n(s\n+ w\n), w\n2, τ\n)\nJACOBI\n\n(A, n,\n(s\n+ w\n)\n+ 1, w\n2, τ\n)\nJACOBI\n\n(A, n,\n(s\n+ w\n), w\n2, τ\n+ w\n)\nelse p\nτ mod 2\nq\n\n(τ\n+ 1\n) mod 2\nA\n[p\n][s mod n\n]\n\nA\n[q\n][(s\n- 1\n) mod n\n]\n+ A\n[q\n][s mod n\n]\n+ A\n[q\n][(s\n+ 1\n) mod n\n]\nA\n[p\n][(s\n+ 1\n) mod n\n]\n\nA\n[q\n][s mod n\n]\n+ A\n[q\n][(s\n+ 1\n) mod n\n]\n+ A\n[q\n][(s\n+ 2\n) mod n\n]\nJACOBI\n\n(A, n, s, w, τ\n)\n1 if w\nthen JACOBI\n\n(A, n, s\n+ w\n4, w\n2, τ\n)\nJACOBI\n\n(A, n, s\n+ w\n4, w\n2, τ\n+ w\n)\nJACOBI\n\n(A, n, s, w\n2, τ\n+ w\n)\nJACOBI\n\n(A, n, s\n+ w\n2, w\n2, τ\n+ w\n)\nJACOBI-REC\n(A\n)\n1 n\nlength of A\n2 JACOBI\n\n(A, n, 0, n, 0\n)\n3 JACOBI\n\n(A, n, n\n2, n, 0\n)\n4 JACOBI\n\n(A, n, n\n2, n, n\n)\n5 JACOBI\n\n(A, n, 0, n, n\n)\nFigure 6-3: The recursive implementation of the multipass filter on array A of size n, where\nn is a power of 2. The algorithm uses two auxiliary subroutines JACOBI\n( A, n, s, w, τ\n) and\nJACOBI\n\n( A, n, s, w, τ\n). The input is in A\n[0\n] and A\n[1\n] is the O\n(n\n) auxiliary space. The\nparameters s and w specify the position and size of the computed triangle and τ is the\ngeneration of the lowest level of the triangle. JACOBI-REC\n( A\n) is the initial call.\n\n(a)\n(b)\n(c)\nmod n\ngeneration\ndata\ngeneration\ndata\ngeneration\ndata\nFigure 6-4: (a) Decomposition of trace matrix in four triangles by JACOBI-REC. Triangles 2\nand 3 \"wrap around\" since array positions are computed modulo n. (b) shows the decom\nposition used by JACOBI\nand (c) shows the decomposition used by JACOBI\n.\nREC is Θ (n2\n) and the cache complexity is O\n(1\n+ n\nL\n+ n2\nZL\n), even if the problem\ndoes not fit into cache, which is a factor of Z fewer cache misses than the iterative\nmethod.\nIn order to simplify the description, we describe the recursive algorithm as if\nthe whole trace matrix would be computed. It turns out that in practice one auxil\niary array of size n suffices to compute the n steps on an array of size n.\nThe divide-and-conquer algorithm divides the trace matrix into 4 triangles,\nwhich are recursively divided into smaller triangles, as shown in Figure 6-4(a).\nTwo auxiliary functions JACOBI\nand JACOBI\nare used to implement the recur\nsion. JACOBI\n\n(A, n, s, w, τ\n) computes an \"upper triangle\" of the trace matrix A\nof size n, where the base of the triangle has size w and starts at s with generation\nτ. It recursively computes up to w\n2 generations ahead as shown in Figure 6-4(b).\nAnalogously, JACOBI\ncomputes a lower triangle recursively as shown in Fig\nure 6-4(c). The resulting trace matrix for an array of size 16 is shown in Figure 6-5.\nIt illustrates the locality of the recursive algorithms. The triangles of the decom\nposition are clearly visible. Depending on the cache size, triangles of different size\nfit entirely into cache, which are then computed without any further cache misses.\nAlthough JACOBI-REC computes the elements in different order than JACOBI-ITER,\nit computes exactly the same values as JACOBI-ITER.\n\ndata\n(a)\ndata array A\ndata array A\n200 241 242 239 240 243 244 231 232 253 254 251 252 255 256 199\n197 198 235 236 237 238 227 228 229 230 247 248 249 250 195 196\n194 191 192 233 234 217 218 225 226 223 224 245 246 185 186 193\n187 188 189 190 213 214 215 216 219 220 221 222 181 182 183 184\n176 179 180 167 168 209 210 207 208 211 212 147 148 177 178 175\n173 174 163 164 165 166 203 204 205 206 143 144 145 146 171 172\n170 153 154 161 162 159 160 201 202 133 134 141 142 139 140 169\n149 150 151 152 155 156 157 158 129 130 131 132 135 136 137 138\n104 125 126 123 124 127 128\n72 113 114 111 112 115 116 103\n101 102 119 120 121 122\n70 107 108 109 110\n99 100\n96 117 118\n64 105 106\narray A\ndata array A\ngeneration\ngeneration\ntime\ntime\ngeneration\ngeneration\n(b)\nFigure 6-5: Ordering of updates of JACOBI-REC on an array with 16 elements. (a) The\ntrace matrix with the order of updates from 1 to 256. (b) A bar-graph illustrating the\nupdates, where the height of a bar represents the ordering of updates and the smallest bar\nis updated first.\nTheorem 15 The recursive JACOBI-REC algorithm involves Θ (n2\n) work and incurs Θ (1\n+\nn\nL\n+ n2\nZL\n) cache misses when computing n generations on n elements.\nProof.\nTo simplify the analysis, we assume that n is an exact power of 2.2 The\nwork of JACOBI-REC can be described by three recurrences:\nW\n(n\n)\n\n2W\n\n(n\n)\n+ 2W\n\n(n\n)\n+ O\n(1\n) ,\nW\n\n(n\n)\n\n3W\n\n(n\n)\n+ W\n\n(n\n)\n+ O\n(1\n) ,\nW\n\n(n\n)\n\n3W\n\n(n\n)\n+ W\n\n(n\n)\n+ O\n(1\n)\n\nwhere W\nand W\nare the work used by the recursive procedures JACOBI\nand\nJACOBI\n. The solution for the total work is W\n(n\n)\nΘ (n2\n), which is the same as\nthe work of the iterative algorithm.\nThe number Q\n(n\n) of cache misses incurred by a subproblem of size n is de\nscribed by three recurrences:\nQ\n(n\n)\n\n2Q\n\n(n\n)\n+ 2Q\n\n(n\n)\n+ O\n(1\n)\n\nΘ (1\n+ n\nL\n)\nif n\nαZ ,\nQ\n\n(n\n)\n\n3Q\n\n(n\n)\n+ Q\n\n(n\n)\n+ O\n(1\n)\notherwise ;\n2The results can be extended, but the analysis is somewhat more complicated.\n\n(a)\n(b)\n(c)\nu\nv\nFigure 6-6: Computational dag of JACOBI-ITER. (a) complete subgraph of size 9\n9, (b) its\ndecomposition into lines, and (c) diamond-shaped subdag of width Ω\n(d\n) that is enclosed\nby two nodes u and v of distance d\n4.\nΘ (1\n+ n\nL\n)\nif n\nαZ ,\nQ\n\n(n\n)\n\n3Q\n\n(n\n)\n+ Q\n\n(n\n)\n+ O\n(1\n)\notherwise ;\nwhere Q\nand Q\nare the cache misses of the two recursive procedures JACOBI\n\nand JACOBI\n, and α is a sufficiently small constant. The base case occurs when\nthe two arrays fit into the cache. Solving these recurrences, we obtain Q\n(n\n)\n\nΘ (1\n+ n\nL\n+ n2\nZL\n) cache misses.\n6.3\nLower bound\nFinally, we prove that the number of cache misses for this problem is lower bounded\nby Ω(1\n+ n\nL\n+ n2\nZL\n), which implies that the recursive algorithm JACOBI-REC is\nindeed optimal.\nWe can use the red-blue pebble game technique described by Hong and Kung\n[25] to lower-bound the number of cache misses incurred by any algorithm com\nputing n generations of an Jacobi-multipass filter on n elements. Hong and Kung\nuse properties of the computation dag (directed acyclic graph) G given by a com\nputation to lower-bound the number of cache misses on a two-level memory. Nodes\nin a computation dag represent operations, and edges, the data-flow of the algo\nrithm. Nodes with no incoming edges are input and nodes with no outgoing edges\nare output. Figure 6.3(a) shows a subgraph of the computation dag given by JA-\nCOBI-ITER. A vertex-disjoint path from inputs to outputs will be called lines. The\ndecomposition of Figure 6.3(a) into lines is shown in in Figure 6.3(b). The number\nof cache misses can be lower-bounded by the information speed function FG\n(d\n) of\na dag G, which is defined as follows.\n\nFor any two vertices u and v on the same line that are at least d apart, there are\nFG\n(d\n) vertices in the dag G satisfying two properties:\n1. None of these vertices belongs to the same line.\n2. Each of these vertices belongs to a path connecting u and v.\nIn the dag given by JACOBI-ITER, for example, two nodes u and v enclose a\ndiamond-shaped subdag of width Ω(d\n), where d is the distance of u and v, as\nshown in Figure 6.3(c).\nWe can obtain lower bounds on the cache complexity Q using the following\nlemma which is proven in [25].\nLemma 16 Suppose G is a computation dag where all inputs can reach all outputs through\nvertex-disjoint paths, and its information speed function is Ω(FG\n(d\n)). If FG\n(d\n) is mono\ntonically increasing, and F\n-1\nG\n(d\n) exists, then the number of cache misses required to execute\nG is\nQ\nΩ(K\nF\n-1\nG\n(Z\n)) ,\nwhere K is the total number of vertices on the vertex-disjoint paths or lines.\nWe use Lemma 16 to prove a lower bound on the cache complexity of any algo\nrithm computing n generations of a Jacobi multipass filter on n elements by finding\nan upper bound on F\n-1\nG .\nTheorem 17 Any scheduling of the computation dag induced by the JACOBI-ITER algo\nrithm on an array of size n incurs Ω(1\n+ n\nL\n+ n2\nLZ\n) cache misses.\nProof.\nThis theorem can be proven by applying three lower bounds:\n1. Suppose that L\n1. We can lower-bound the cache complexity using Lem\nma 16. Consider the subnetwork of the dag of JACOBI-ITER that includes only\none third of the edges, as shown in Figure 6.3(b). The subnetwork has n lines\nwith K\nΘ (n2\n) vertices. The information speed function is F\n(d\n)\nΩ(d\n),\nsince a diamond-shaped subdag of width Ω(d\n- 2\n) is enclosed by two nodes\nas illustrated in Figure 6.3(c) for d\n4. Therefore F\n-1\n(d\n)\nO\n(d\n) and the\nresulting lower bound for Q is Q\n(n\n)\nΩ(n2\nZ\n).\nAt most L data items are moved into cache when a cache miss occurs. Thus,\na first lower bound for L\n1 is\nQ\n(n\n)\nΩ(n2\nZL\n) .\n\niterative\nrecursive\ntime per element update in nanoseconds\n65536 131072 262148\nproblem size N\nFigure 6-7: Plot of update time per element per generation for optimized iterative and\nrecursive implementations of a multipass filter on a 167-MHz UltraSparc with 16kB L1\ncache and 512kB L2-cache.\n2. The algorithm must read all Θ (n\n) inputs, which reside on Ω(n\nL\n) cache lines.\nThis yields the second lower bound of Ω(n\nL\n).\n3. The third lower bound is the trivial lower bound of Q\n(n\n)\nΩ(1\n).\nBy combining these lower bounds we get Q\n(n\n)\nΩ(1\n+ n\nL\n+ n2\nLZ\n).\n6.4\nExperimental results\nWe now compare optimized implementations of the iterative and the recursive\nalgorithms for the simple update rule given in Equation (6.1). (The iterative algo\nrithm uses only 2 temporary variables, and the recursive implementation uses a\n\"unfolded\" [18] base case.) Figure 6-7 shows a plot of the update time per element\nper generation for the two versions on a 167-MHz Sun UltraSparc with 16kB L1\ncache and 512kB L2-cache. The update time for the recursive algorithm is not only\nfaster than the iterative algorithm, it is also nearly constant, whereas the iterative\nimplementation slows down with every new level of the memory hierarchy. For\narrays that do not fit in L2-cache, the recursive implementation executes in less\nthan 70% of the time of the iterative version. The gain can be even higher for out-\nof-core algorithms, because disk bandwidth is considerably less than memory or\ncache bandwidths.\n\nSummary\nIn this section, we have presented an optimal recursive algorithm to compute a\nmultipass filter over one-dimensional data. We compared its cache complexity\nto a iterative algorithm and gave some brief empirical results for this problem.\nThe recursive algorithm executes in less than 70% of the time of the iterative al\ngorithm on problems that do not fit in L2-cache. The technique presented here\ncan be extended to multidimensional stencil-filters. I expect that the advantage of\nthe cache-oblivious algorithm on the multidimensional data will prove to be even\ngreater.\n\nSECTION 7\nCache complexity of ordinary\nalgorithms\nThis section analyzes the cache complexity of the \"ordinary\" algorithms for ma\ntrix transposition, matrix multiplication, and sorting. Although optimal in the\nrandom-access machine model [4] and cache oblivious, these algorithms are not\nasymptotically optimal with respect to cache misses. We first prove that the num\nber of cache misses of algorithms with a \"regular \" complexity bound (as defined\nlater) is asymptotically the same even if least-recently-used (LRU) is used instead\nof optimal replacement. We then show that the standard iterative algorithm to\ntranspose a matrix incurs Ω( n2\n) cache misses on a n\nn matrix matching the triv\nial upper bound of one cache miss per time step. The ordinary iterative algorithm\nto multiply two n\nn matrices incurs Ω( n3\n) cache misses, which is also the worst\npossible asymptotic behavior for an O\n( n3\n) -work algorithm. Many \"ordinary\" al\ngorithms for sorting exit. We pick mergesort and prove that its cache complexity is\nΩ\n(\n(\nn\nL\n) lg\n( n\nZ\n)\n) when sorting an array of n elements, which is a factor of Θ ( lg Z\n)\naway from optimal.\nThe ideal-cache model is well suited for algorithm design and upper-bound\nanalyses. This comes in part from the optimal replacement strategy employed by\nthe ideal-cache.\nLower-bounding the cache complexity of an algorithm with optimal replace\nment is somewhat hard, since it must be proven that the optimal replacement\nstrategy will do. For upper bounds, we can pick any replacement strategy we\n\nwant and the optimal replacement will perform as least as well as our arbitrary\nstrategy. However, for lower bounds we must be more careful. We usually do\nnot know which line the optimal replacement strategy would replace. The follow\ning analysis shows that the optimal and omniscient replacement strategy used by\nan ideal cache can be simulated efficiently by the LRU replacement strategy. The\nLRU strategy replaces the cache line whose most recent access was earliest among\nall lines in the associativity set. In fact, for algorithms with a \"regular \" complex\nity bound, LRU and optimal replacement yield the same asymptotic bounds. We\ndefine a cache complexity bound Q\n(n\nZ, L\n) to be regular if\nQ\n(n\nZ, L\n)\nO\n(Q\n(n\n2Z, L\n)) .\n(7.1)\nLemma 18 Consider an algorithm that causes Q\n\n(n\nZ, L\n) cache misses on a problem\nof size n using a\n(Z, L\n) ideal cache.\nThen, the same algorithm incurs Q\n(n\nZ, L\n)\n\n2Q\n\n(n\nZ\n2, L\n) cache misses on a\n(Z, L\n) cache that uses LRU replacement.\nProof.\nSleator and Tarjan [37] have shown that the cache misses on a\n(Z, L\n) cache\nusing LRU replacement is\n(Z\n(Z\n- Z\n\n+ 1\n))-competitive with optimal replacement\non a\n(Z\n, L\n) ideal if both caches start with an empty cache. It follows that the\nnumber of misses on a\n(Z, L\n) LRU-cache is at most twice the number of misses on\na\n(Z\n2, L\n) ideal-cache.\nCorollary 19 For algorithms with regular cache complexity bounds, the asymptotic num\nber of cache misses is the same for LRU and optimal replacement.\nProof.\nThis corollary follows directly from Lemma 18 and the regularity condi\ntion.\nThe same argument extends to a variety of other replacement strategies [11],\nincluding:\nflush when full: Whenever there is a cache miss and there is no space left in the\ncache, evict all lines currently in the cache (call this action a \"flush\").\nclock replacement: An approximation to LRU in which a single \"use bit\" replaces\nthe implicit (time of last access) timestamp of LRU.\nfirst-in, first-out: Replace the line that has been in the fast memory longest.\nrandom: Whenever a cache miss occurs, evict a page chosen randomly and uni\nformly among all fast memory pages.\nWe shall use Corollary 19 in the following lower-bound proofs and assume that\nthe cache is handled by LRU to simplify our analyses. If an algorithm analyzed\nwith LRU is regular, then the optimal strategy must also be regular. Therfore,\naccording to Corollary 19 the bound derived with the LRU analysis applies to the\nideal cache model as well.\n\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n7.1\nMatrix multiplication\nIn this section, we analyze the straightforward iterative algorithm for matrix mul\ntiplication. We prove that it causes Ω(n3\n) cache misses when the n\nn matrices\nare stored in row-major order and do not fit in cache. We further show that even if\nthe matrices are stored in the order in which they are used and do not fit in cache,\nthe number of cache misses is at least Ω(n3\nL\n), compared to Θ (n3\nL\np\nZ\n) for an the\ncache-optimal Θ (n3\n)-work algorithm presented in Section 2.\nThe simplest way to compute the product of two matrices is to evaluate the\nformula\nn\nX\ncij\n\naikbkj\nk\ndirectly, as in the following program:\nORD-MULT\n(A, B, C, n\n)\n1 for i\n1 to n\ndo for j\n1 to n\ndo cij\nfor k\n1 to n\ndo cij\ncij\n+ aikbkj\nTheorem 20 The ORD-MULT algorithm for matrix multiplication uses Θ (n3\n) work when\nmultiplying n\nn matrices that do not fit in cache. It incurs Ω(n3\n) cache misses, when\nthe matrices are stored in row-major order. Even if the matrices are stored in the order\nin which they are used, ORD-MULT incurs Ω(n3\nL\n) cache misses, which is a factor of\nΘ (\np\nZ\n) from optimal.\nProof.\nAnalyzing the work of ORD-MULT\n(A, B\n) is straightforward. Since there\nare three nested loops, each of which performs n iterations, the work is Θ (n3\n).\nSince the algorithm cannot access more than O\n(1\n) elements in constant time, O\n(n3\n)\nis also an upper bound on the number of cache misses for this algorithm.\nFirst, we assume that both matrices are stored in row-major order (Figure 2-\n1(a)):\nC\n\nA\n\nB\nThe number of cache misses can be lower-bounded by counting only the misses\ncaused by reading matrix B. In the ith iteration of the outer loop, the elements of\n\nthe ith row of C are computed. While i is fixed the inner two loops iterate over all\nn2 values of k and j, reading all elements of matrix B column by column.\nAssuming that the matrix does not fit in cache, e.g. n\nZ\nL, the LRU replace\nment strategy overwrites lines of matrix B before they can be reused. Therefore, the\nnumber of misses on matrix B to compute one element of matrix C is Ω(n\n). Since\nC contains n2 elements, the algorithm causes Ω(n3\n) cache misses. It follows from\nCorollary 19 that even with an optimal replacement strategy, ORD-MULT incurs\nΩ(n3\n) cache misses.\nORD-MULT\n(A, B\n) does not exhibit good cache behavior. Accesses to the same\nelement, or at least to the same cache line, are far apart. The spatial locality of the\nmemory accesses can be improved by changing the memory layout of the matrices:\nThe previous analysis showed that the accesses to matrix B alone cause Θ (n3\n) cache\nmisses. The problem is that matrix B is stored in row-major order, but accessed\ncolumnwise. Assume that the memory layout for B is changed from row-major to\ncolumn-major order (Figure 2-1(b)):\nC\n\nA\n\nB\n-\n-\n-\n-\n-\n-\n-\n-\n\nNow, both matrices are accessed in the order in which they are stored. As long as\nthe cache can provide a single line for each of A, B, and C, for each cache miss,\nthe following L\n- 1 accesses are cache hits. Hence, the number of cache misses\nis Θ (n3\nL\n), which is a factor of Θ (L\n) improvement over the previous algorithm.\nThis improvement comes with the disadvantage that the matrices are not stored\nuniformly and it is still a factor of Θ (\np\nZ\n) away from the cache-optimal algorithms\nshown in Sections 1 and 2.\n7.2\nMatrix transposition\nIn this section, we argue that the iterative algorithm for matrix transposition causes\nΩ(n2\n) cache misses on a n\nn matrix, when the matrix is stored in row- or column-\nmajor order (Figure 2-1(a,b)). This is a factor of Θ (L\n) more cache misses than the\ncache-optimal algorithm presented in Section 3.\nThe ordinary algorithm for matrix transposition walks through the matrix row\nby row and swaps elements:\n\nORD-TRANSPOSE\n(A, B, n\n)\n1 for i\n1 to n\ndo for j\n1 to n\ndo bij\naji\nTheorem 21 The ORD-TRANSPOSE algorithm for matrix multiplication uses Θ (n2\n) work\nand incurs Ω(n2\n) cache misses, when transposing a n\nn matrix that does not fit into\ncache.\nProof.\nTransposing a matrix is equivalent to changing the memory layout from\nrow- to column-major layout or vice versa. Here, we show that accessing in column-\nmajor order a matrix stored in row-major layout causes Ω(n2\n) cache misses. After\nZ\nL cache misses, the cache is filled and lines must be evicted. The LRU strat\negy replaces the lines in the same order in which they are read. Therefore, after\nn accesses, when a line could be reused, it has been evicted from cache by LRU\nreplacement. Thus, all n2 accesses are cache misses. Since Ω(n3\n) is regular, it fol\nlows from Corollary 19 that ORD-TRANSPOSE incurs Ω(n2\n) cache misses in the\nideal-cache model.\n7.3\nMergesort\nWe have just shown that divide-and-conquer algorithms presented in Sections 2\nand 3 for matrix multiplication and matrix transposition incur fewer cache misses\nthan their iterative counterparts. In this section, we show that divide-and-conquer\nalgorithms are not per se cache-optimal. Specifically, we show that Mergesort [16,\np. 13] incurs Ω((n\nL\n) lg\n(n\nZ\n)\n) cache misses for an input of size n, which is a fac\ntor of Θ (lg Z\n) more cache misses than the cache-optimal algorithms presented in\nSections 4 and 5.\nMergesort is a recursive sorting algorithm that divides the input sequence in\ntwo parts, sorts them recursively and then merges the two sorted subsequences\ninto one sorted sequence. The following pseudocode is the standard description\nof mergesort and can be found in a variety of textbooks [16, 34].\nMERGESOR T\n(A, p, r\n)\nif p\nr\nthen q\n\nb\n(p\n+ r\n)\nc\nMERGESO RT\n(A, p, q\n)\nMERGESO RT\n(A, q\n+ 1, r\n)\nMERGE\n(A, p, q, r\n)\n\nWe assume that the input array A of length n is stored in Θ (n\n) contiguous\nmemory locations. MERGESO RT uses an auxiliary procedure MERGE\n(A, p, q, r\n) that\nmerges two sorted subarrays A\n[p . . q\n] and A\n[q\n+ 1 . . r\n] into a single sorted subarray\nthat replaces the current subarray A\n[p . . r\n]. Merging two subarrays of length n\nuses Θ (n\n) work and causes O\n(n\nL\n) cache misses assuming that Z\nL\n3, since\nthe Θ (n\n) data items can be accessed in linear order and each cache miss brings L\nelements into cache.\nThe work of MERGESOR T is Θ (n lg n\n), which is optimal in the random-access\nmachine model [16, p. 172]. Although mergesort is a divide-and-conquer algo\nrithm, its cache complexity is not asymptotically optimal.\nLemma 22 MERGESORT incurs Ω((n\nL\n) lg\n(n\nZ\n)\n) cache misses for an input of size n.\nProof.\nThe cache complexity of MERGESORT can be described by the recurrence:\nΩ(n\nL\n)\nif n\nαZ ,\nQ\n(n\n)\n\n(7.2)\n2Q\n(n\n)\n+ Ω(n\nL\n)\notherwise ,\nwhere α is a sufficiently small constant. The base case arises when the Θ (n\n) ele\nments fit into the cache. Sorting Θ (n\n) elements requires Θ (n\n) auxiliary storage for\nthe merging procedure. In the recursive case where n\nαZ, two subproblems of\nhalf the size are solved and then merged together. After line 3 of MERGESORT is\nexecuted, LRU replacement will have evicted most of the n\n2 data items used in\nthe first recursive call. Thus, Ω(n\n) data elements must be read from contiguous\nmemory locations incurring Ω(n\nL\n) cache misses. The solution of Equation (7.2)\nis Ω((n\nL\n) lg\n(n\nZ\n)).\nSummary\nIn this section, we have shown that the cache complexity of the ordinary algo\nrithms for matrix transposition, matrix multiplication, and sorting are not asymp\ntotically optimal. We have proven in Corollary 19 that the optimal replacement\nstrategy can be efficiently simulated by the LRU replacement strategy. For algo\nrithms with regular complexity bounds LRU and optimal replacement yield the\nsame asymptotic bounds. Since it is easier to analyze the caching behavior of an\nalgorithm when we understand what the replacement strategy does, this Corollary\noften helps to simplify the analyses.\n\nSECTION 8\nOther cache models\nIn this section we show that cache-oblivious algorithms designed in the two-level\nideal-cache model can be efficiently ported to other cache models. We show that\nalgorithms with regular complexity bounds (Equation (7.1)) (including all algo\nrithms heretofore presented) can be ported to less-ideal caches incorporating least-\nrecently-used (LRU) or first-in, first-out (FIFO) replacement policies [24, p. 378].\nWe argue that optimal cache-oblivious algorithms are also optimal for multilevel\ncaches. Finally, we present simulation results proving that optimal cache-oblivious\nalgorithms satisfying the regularity condition are also optimal (in expectation) in\nthe previously studied SUMH [5, 42] and HMM [1] models. Thus, all the algorith\nmic results in this thesis apply to these models, matching the best bounds previ\nously achieved.\n8.1\nTwo-level models\nMany researchers, such as [3, 25, 43], employ two-level models similar to the ideal-\ncache model, but without an automatic replacement strategy. In these models,\ndata must be moved explicitly between the the primary and secondary levels \"by\nhand.\"\nWe now show that optimal algorithms in the ideal-cache model whose cache\ncomplexity bounds are regular (Equation (7.1)) can be ported to these models\nto run using optimal work and incurring an optimal expected number of cache\nmisses. Since previous two-level models do not support automatic replacement,\n\nto port a cache-oblivious algorithms to them, we implement an LRU (or FIFO) re\nplacement strategy in software.\nLemma 23 A\n(Z, L\n) LRU cache (or FIFO-cache) can be maintained using O\n(Z\n) primary\nmemory locations such that every access to a cache line in primary memory takes O\n(1\n)\nexpected time.\nProof.\nGiven the address of the memory location to be accessed, we use a 2\nuniversal hash function [29, p. 216] to maintain a hash table of cache lines present\nin the primary memory. The Z\nL entries in the hash table point to linked lists in\na heap of memory containing Z\nL records corresponding to the cache lines. The\n2-universal hash function guarantees that the expected size of a chain is O\n(1\n). All\nrecords in the heap are organized as a doubly linked list in the LRU order (or singly\nlinked for FIFO). Thus, the LRU (FIFO) replacement policy can be implemented in\nO\n(1\n) expected time using O\n(Z\nL\n) records of O\n(L\n) words each.\nTheorem 24 An optimal cache-oblivious algorithm with a regular cache-complexity bound\ncan be implemented optimally in expectation in two-level models with explicit memory\nmanagement.\nConsequently, our cache-oblivious algorithms for matrix multiplication, matrix\ntransposition, FFT, and sorting are optimal in two-level models with explicit mem\nory management.\n8.2\nMultilevel ideal caches\nWe now show that optimal cache-oblivious algorithms also perform optimally in\ncomputers with multiple levels of ideal caches. Moreover, Theorem 24 extends to\nmultilevel models with explicit memory management.\nThe\nh\n(Z1, L1\n),\n(Z2, L2\n), . . . ,\n(Zr, Lr\n)\ni ideal-cache model consists of an arbitrar\nily large main memory and a hierarchy of r caches, each of which is managed by\nan optimal replacement strategy. The model assumes that the caches satisfy the\ninclusion property [24, p. 723], which says that for i\n1, 2, . . . , r\n- 1, the values\nstored in cache i are also stored in cache i\n+ 1. The performance of an algorithm\nrunning on an input of size n is measured by its work complexity W\n(n\n) and its\ncache complexities Qi\n(n\nZi, Li\n) for each level i\n1, 2, . . . , r.\nTheorem 25 An optimal cache-oblivious algorithm in the ideal-cache model incurs an\nasymptotically optimal number of cache misses on each level of a multilevel cache with\noptimal replacement.\n\nProof.\nThe theorem follows directly from the definition of cache obliviousness\nand the optimality of the algorithm in the two-level ideal-cache model.\nTheorem 26 An optimal cache-oblivious algorithm with a regular cache-complexity bound\nincurs an asymptotically optimal number of cache misses on each level of a multilevel cache\nwith LRU or optimal replacement.\nProof.\nFollows from Corollary 19 and Theorem 25.\n8.3\nThe SUMH model\nIn 1990 Alpern et al. [5] presented the uniform memory hierarchy model (UMH), a\nparameterized model for a memory hierarchy. In the UMHα,ρ,b\n(l\n) model, for integer\nconstants α, ρ\n1, the size of the ith memory level is Zi\nαρ2i and the line\nlength is Li\nρi. A transfer of one ρl-length line between the caches on level l and\nl\n+ 1 takes ρl\nb\n(l\n) time. The bandwidth function b\n(l\n) must be nonincreasing. The\nprocessor can access the cache on level 1 in constant time per access. An algorithm\ngiven for the UMH model must include a schedule that, for a particular set of\ninput variables, tells exactly when each block is moved along which of the buses\nbetween caches. Work and cache misses are folded into one cost measure T\n(n\n).\nAlpern et al. prove that an algorithm that performs the optimal number of cache\nmisses at all levels of the hierarchy does not necessarily run in optimal time in\nthe UMH model, since scheduling bottlenecks can occur when all buses are active.\nIn the more restrictive SUMH model [42], however, only one bus is active at a\ntime. Consequently, we can prove that optimal cache-oblivious algorithms run in\noptimal expected time in the SUMH model.\nLemma 27 A cache-oblivious algorithm with W\n(n\n) work and Q\n(n\nZ, L\n) cache misses on\na\n(Z, L\n)-ideal cache can be executed in the SUMHα,ρ,b\n(l\n) model in expected time\nr\n-1\n\nX ρi\n\nT\n(n\n)\nO W\n(n\n)\n+\nQ\n(n\nΘ (Zi\n), Li\n)\n,\nb\n(i\n)\ni\nwhere Zi\nαρ2i, Li\nρi, and Zr is big enough to hold all elements used during the\nexecution of the algorithm.\nProof.\nUse the memory at the ith level as a cache of size Zi\nαρ2i with line length\nLi\nρi and manage it with software LRU described in Lemma 23. The rth level is\nthe main memory, which is direct mapped and not organized by the software LRU\n\nmechanism. An LRU cache of size Θ (Zi\n) can be simulated by the ith level, since\nit has size Zi. Thus, the number of cache misses at level i is 2Q\n(n\nΘ (Zi\n), Li\n), and\neach takes ρi\nb\n(i\n) time. Since only one memory movement happens at any point\nin time and there are O\n(W\n(n\n)) accesses to level 1, the lemma follows by summing\nthe individual costs.\nLemma 28 Consider a cache-optimal algorithm whose work on a problem of size n is\nlower-bounded by W\n\n(n\n) and whose cache complexity is lower-bounded by Q\n\n(n\nZ, L\n) on\nan\n(Z, L\n) ideal-cache. Then, no matter how data movement is implemented in SUMHα,ρ,b\n(l\n),\nthe time taken on a problem of size n is at least\nr\n\nX ρi\n\nT\n(n\n)\nΩ W\n\n(n\n)\n+\nQ\n\n(n, Θ (Zj\n), Li\n)\n,\nb\n(i\n)\ni\nwhere Zi\nαρ2i, Li\nρi and Zr is big enough to hold all elements used during the\nexecution of the algorithm.\nProof.\nThe optimal scheduling of the data movements does not need to obey the\ninclusion property, and thus the number of ith-level cache misses is at least as large\nas for an ideal cache of size\nPi\nj\n1 Zi\nO\n(Zi\n). Since Q\n\n(n, Z, L\n) lower-bounds the\ncache misses on a cache of size Z, at least Q\n\n(n, Θ (Zi\n), Li\n) data movements occur\nat level i, each of which takes ρi\nb\n(i\n) time. Since only one movement can occur at a\ntime, the total cost is the maximum of the work and the sum of the costs at all the\nlevels.\nTheorem 29 A cache-oblivious algorithm that is optimal in the ideal-cache model and\nwhose cache complexity is regular can be executed in the SUMHα,ρ,b\n(l\n) model in optimal\nexpected time.\nProof.\nThe theorem follows directly from regularity and Lemmata 27 and 28.\n8.4\nThe HMM model\nAggarwal, Alpern, Chandra, and Snir [1] proposed the hierarchical memory model\n(HMM) in which an access to location x takes f\n(x\n) time. The authors assume that\nα\nf is a monotonically nondecreasing function, usually of the form\ndlog x\ne or\ndx\ne.\nLemma 30 Consider a cache-oblivious algorithm with W\n(n\n) work and Q\n(n\nZ, L\n) cache\nmisses on a\n(Z, L\n) ideal cache. Let Z1\nZ2\n\nZr be positive integers such that a\n\ncache of size Zr can hold all of the data used during the execution of the algorithm. Then,\nthe algorithm can be executed in the HMM model with cost function f in expected time\nr\n\nX\n\nT\n(n\n)\nO W\n(n\n) f\n(s1\n)\n+\nf\n(si\n)Q\n(n\nΘ (Zi\n), 1\n)\n,\ni\nwhere s1\nO\n(Z1\n), s2\ns1\n+ O\n(Z2\n), . . ., sr\nsr\n-1\n+ O\n(Zr\n).\nProof.\nUsing Lemma 23 we can simulate a\nh\n(Z1, 1\n),\n(Z2, 1\n), . . . ,\n(Zr, 1\n)\ni LRU cache\nin the HMM model by using locations 1, 2, . . . , s1 to implement cache 1, locations\ns1\n+ 1, s1\n+ 2, . . . , s2 to implement cache 2, etc. The cost of each access to the ith\ncache is at most f\n(si\n). Cache 1 is accessed at most W\n(n\n) times, cache 2 is accessed\nat most Q\n(n\nΘ (Z2\n), 1\n) times, and so forth. The lemma follows.\nLemma 31 Consider a cache-optimal algorithm whose work on a problem of size n is\nlower-bounded by W\n\n(n\n) and whose cache complexity is lower-bounded by Q\n\n(n\nZ, L\n)\non an\n(Z, L\n) ideal cache. Then, no matter how data movement is implemented in an HMM\nmodel with cost function f, the time taken on a problem of size n is at least\n\nr\n\nX\n\nT\n(n\n)\nΩ W\n\n(n\n)\n+\nf\n(Zi\n-1\n- 1\n)\n- f\n(Zi\n-2\n- 1\n) Q\n\n(n\nZi, 1\n)\ni\nfor any Z0\nZ1\n\nZr such that a cache of size Zr can hold all of the data used\nduring the execution of the algorithm.\nProof.\nThe memory of the HMM model can be viewed as a cache hierarchy with\narbitrary parameters Z0\nZ1\n\nZr, where the memory elements are\nmapped to fixed locations in the caches. The processor works on elements in the\nlevel 0 cache with Θ (1\n) cost. The first Z1\n- 1 elements of the HMM memory are\nkept in the level 1 cache, the first Z2\n- 1 elements in the level 2 cache, etc. One el\nement in each cache is used as a \"dynamic entry\" which allows access to elements\non higher levels. Accessing a location at level i is then done by incorporating the\nmemory item in the dynamic element of each of the caches closer to the proces\nsor. This \"cache hierarchy\" obeys the inclusion principle, but it does not do any\nreplacement. Memory elements are exchanged--as in HMM--by moving them to\nthe processor and writing them back to their new location.\nIf we charge f\n(Zi\n-1\n- 1\n)\n- f\n(Zi\n-2\n- 1\n) to a cache miss on cache i, an access to\nelement at position x in cache at level k costs\nPk\ni\n1 f\n(Zi\n-1\n- 1\n)\n- f\n(Zi\n-2\n- 1\n)\n\nf\n(Zk\n-1\n- 1\n)\n- f\n(0\n), which is at most f\n(x\n). Thus, the access cost for accessing el\nement x is the same in the HMM as in this \"cached\" HMM model. The cost T\n(n\n)\n\nof an algorithm in the HMM model can be bounded by the cost of the algorithm in\nthe multilevel model, which is at least\n\nr\n\nX\n\nT\n(n\n)\nΩ W\n(n\n)\n+\nf\n(Zi\n- 1\n)\n- f\n(Zi\n-1\n- 1\n) Q\n(n\nZi, 1\n)\n.\ni\nSince W\n(n\n)\nW\n\n(n\n) and Q\n(n\nZi, 1\n)\nQ\n\n(n\nZi, 1\n), the lemma follows.\nTheorem 32 A cache-oblivious algorithm that is optimal in the ideal-cache model and\nwhose cache complexity is regular can be executed in optimal expected time in the HMM\nmodel, if the cost function is monotonically nondecreasing and satisfies f\n(2x\n)\nΘ ( f\n(x\n)).\nProof.\nAssume that the cache at level r is big enough to hold all elements used\nduring the execution of the algorithm. We choose Z1,\n\n, Zr such that 2 f\n(Zi\n-1\n\n)\nZi\n- 1\nO\n(Zi\n-1\n- 1\n) for all 1\ni\nr. Such a sequence can be computed\ngiven that f is monotonically nondecreasing and satisfies f\n(2x\n)\nΘ ( f\n(x\n)).\nWe execute the algorithm as described in Lemma 30 on the HMM model with\nP\n2Z1, 2Z2, . . . , 2Zr. The cost of warming up the caches is\nf\n(i\n)\nΘ (Zr f\n(Zr\n))\ni\nO\n(Zr\n)\nwhich is asymptotically no greater than the cost of the algorithm even if it accesses\neach input item just once. The result follows from Lemmata 18, 30 and 31.\nSummary\nOne strength of the ideal-cache model, compared to other models studied in the lit\nerature, is that designing and analyzing algorithms is easier. But this section shows\nthat the assumptions of the ideal-cache model are not stronger than the assump\ntions of two hierarchical memory models in the literature. Specifically, we have\nshown that optimal cache-oblivious algorithms in the ideal-cache model are also\noptimal in the hierarchical memory model (HMM) [1] and in the serial uniform\nmemory hierarchy (SUMH) model [5, 42].\nDue to its simplifications, the ideal-cache model falls short of modeling some of\nthe idiosyncrasies of a real-world memory hierarchy. It ignores issues such as con\nflict misses, and has only one level of caching. In developing recursive algorithms,\nhowever, we have found that these additional complications are comparatively\neasy to deal with once an algorithm has been designed in the ideal model.\n\nSECTION 9\nRelated work\nIn this section, we discuss the origin of the notion of cache obliviousness. We also\ngive an overview of other hierarchical memory models.\nOur research group at MIT noticed as far back as 1994 that divide-and-conquer\nmatrix multiplication was a cache-optimal algorithm that required no tuning, but\nwe did not adopt the term \"cache-oblivious\" until 1997. This matrix-multiplication\nalgorithm, as well as a cache-oblivious algorithm for LU-decomposition without\npivoting, eventually appeared in [9]. Shortly after leaving our research group,\nToledo [40] independently proposed a cache-oblivious algorithm for LU-decom-\nposition, but with pivoting. For n\nn matrices, Toledo's algorithm uses Θ (n3\n)\nwork and incurs Θ (1\n+ n2\nL\n+ n3\nL\np\nZ\n) cache misses. More recently, our group\nhas produced an FFT library called FFTW [20], which in its most recent incar\nnation [19], employs a register-allocation and scheduling algorithm inspired by\nour cache-oblivious FFT algorithm. The general idea that divide-and-conquer en\nhances memory locality has been known for a long time [36].\nPrevious theoretical work on understanding hierarchical memories and the\nI/O-complexity of algorithms has been studied in cache-aware models lacking an\nautomatic replacement strategy. Hong and Kung [25] use the red-blue pebble game\nto prove lower bounds on the I/O-complexity of matrix multiplication, FFT, and\nother problems. The red-blue pebble game models temporal locality using two\nlevels of memory. The model was extended by Savage [33] for deeper memory\nhierarchies. Aggarwal and Vitter [3] introduced spatial locality and investigated\na two-level memory in which a block of P contiguous items can be transferred in\n\none step. They obtained tight bounds for matrix multiplication, FFT, sorting, and\nother problems. The hierarchical memory model (HMM) by Aggarwal et al. [1]\ntreats memory as a linear array, where the cost of an access to element at location x\nis given by a cost function f\n(x\n). The BT model [2] extends HMM to support block\ntransfers. The UMH model by Alpern et al. [5] is a multilevel model that allows\nI/O at different levels to proceed in parallel. Vitter and Shriver introduce paral\nlelism, and they give algorithms for matrix multiplication, FFT, sorting, and other\nproblems in both a two-level model [43] and several parallel hierarchical mem\nory models [44]. Vitter [41] provides a comprehensive survey of external-memory\nalgorithms.\n\nWir\nstehen\nselbst\nentt\nauscht\nund\nsehn\nb\netroen\nDen\nV\no\nrhang\nzu\nund\nalle\nF\nragen\noen\n[...]\nV\nerehrtes\nPublikum\nlos\nsuch\ndir\nselbst\nden\nSchlu\nEs\nmu\nein\nguter\nda\nsein\nmu\nmu\nmu\nW\ne\nfeel\ndeated\nto\no\nW\ne\nt o\no\na\nre\nnettled\nT\no\nsee\nthe\ncurtain\ndo\nwn\nand\nnothing\nsettled\n[...]\nY\nou\nwrite\nthe\nhapp\ny\nending\nto\nthe\npla\ny\nThere\nmust\nthere\nmust\ntheres\ngot\nto\nb\ne\na\nw\na\ny\nBERTOLD BRECHT, Der gute Mensch von Sezuan, 1940\nSECTION 10\nConclusion\nThis thesis has introduced the notion of cache obliviousness and has presented\nasymptotically optimal cache-oblivious algorithms for fundamental problems. Fig\nure 10 gives an overview of the known efficient cache-oblivious algorithms, most\nof which are described in this thesis. Two that we have not discussed are matrix\naddition and LUP-decomposition. For matrix addition, a simple iterative algo\nrithm turns out to be cache-optimal if the matrix elements are read in the same\norder in which they are stored in memory. The algorithm for LUP-decomposition\nis due to Toledo [40], but it uses cache-aware algorithms as subprocedures. By ap\nplying the cache-oblivious algorithms presented here, however, his algorithm can\nbe converted into a cache-oblivious one.\nThe remainder of this section outlines research questions related to cache oblivi\nousness. Section 10.1 discusses the engineering task of implementing cache-oblivi-\nous algorithms. Section 10.2 discusses cache-oblivious data structures and briefly\npresents a cache-oblivious data structure for static binary search trees. Section 10.3\nraises two theoretical questions about the general power of cache-oblivious algo\nrithms. Section 10.4 discusses divide-and-conquer as a programming strategy and\nthe tools needed to help programmers to write recursive programs. In Section 10.5,\nI argue that because divide-and-conquer works well with cache hierarchies and\nalso with parallel computers, the coming revolution of shared-memory multipro\ncessors will make this design paradigm of paramount importance.\n\nAlgorithm\nCache complexity\nOptimal?\nMatrix Multiplication\nΘ (m\n+ n\n+ p\n+\n( mn\n+ np\n+ mp\n)\nL\n+mnp\nL\np\nZ\n)\ntight lower\nbound unknown\nStrassen's Algorithm\nΘ (n\n+ n2\nL\n+ nlog2 7\nL\np\nZ\n)\ntight lower\nbound unknown\nMatrix Transpose\nΘ (1\n+ n2\nL\n)\nyes\nMatrix Addition\ny\nΘ (1\n+ n2\nL\n)\nyes\nLUP-decomposition\nz [40]\nΘ (1\n+ n2\nL\n+ n3\nL\np\nZ\n)\ntight lower\nbound unknown\nDiscrete Fourier Transform\nΘ (1\n+\n( n\nL\n)(1\n+ logZn\n))\nyes\nDistribution sort\nΘ (1\n+\n( n\nL\n)(1\n+ logZn\n))\nyes\nFunnelsort\nΘ (1\n+\n( n\nL\n)(1\n+ logZn\n))\nyes\nJacobi multipass filter\nΘ (1\n+ n\nL\n+ n2\nZL\n)\nyes\nFigure 10-1: Overview of the known cache-oblivious algorithms. Except for matrix addi\ntion (\ny) and LUP-decomposition (\nz), all these algorithms are presented in this thesis.\n10.1\nEngineering cache-oblivious algorithms\nThe job is not done after an efficient algorithm has been designed in the ideal-\ncache model. The software-engineering task of programming the algorithm on a\nreal machine remains to be done. This task often involves coping with the less-\nthan-ideal behavior of real caches. Nevertheless, if the original algorithm in the\nideal-cache model exploits locality effectively, a program based on the algorithm\ncan usually be made to run efficiently in practice. If the algorithm fails to exploit\nlocality in the ideal-cache model, the algorithm will be slow no matter what the\nreal-world computer environment looks like.\nThe trend in architecture is towards bigger caches with steeper hierarchies and\ntowards new cache organizations which employ more \"intelligent\" algorithms to\nuse the cache memory more effectively. But even when caches get more intelli\ngent, the algorithm designer retains responsibility to ensure that frequently ac\ncessed data has the opportunity to reside in cache.\nBoth cache-aware and cache-oblivious strategies can be used to achieve good\ncaching behavior of an algorithm. This thesis has shown that optimal cache-ob-\nlivious algorithms exist which have the same cache complexity as optimal cache-\naware algorithms. But how do these two strategies compare in practice? How\nmuch faster is a cache-aware algorithm optimized for a given architecture than a\ncache-oblivious algorithm that solved the same problem?\n\nInitial measurements I have taken indicate that cache-oblivious algorithms can\nrival the performance of hand-tuned cache-aware code, but in general cache-aware\nprograms are faster. I hope to quantify this difference, as well as resolve other em\npirical questions. How do more levels of caching affect the difference between\ncache-aware and cache-oblivious algorithms? By how much do cache-aware algo\nrithms slow down when executed on hardware they are not optimized for? An\nswers to these questions will become increasingly important as cache hierarchies\nbecome more pronounced.\n10.2\nCache-oblivious data structures\nThis section discusses cache-oblivious data structures and briefly presents a cache-\noblivious data structure for static binary search trees.\nAs there are cache-oblivious algorithms, there are cache-oblivious data struc\ntures. The blocked layout (Figure 2-1(c)), for example, is cache aware. To optimize\nit for a certain cache, the line length must be known. The bit-interleaved layout\n(Figure 2-1(d)), however, is cache oblivious and has the same asymptotic behavior\nas the blocked layout for matrix multiplication. Other cache oblivious layouts for\nmatrices exist like the Morton or Hilbert layouts discussed in [12, 13, 18].\nDifferent data layouts can greatly affect the asymptotic behavior of an algo\nrithm. For cache-optimal matrix multiplication, as discussed in Section 2, the tall\ncache requirement can be relaxed if matrices are stored in blocked (Figure 2-1(c))\nor bit-interleaved order (Figure 2-1(d)). In Section 7.1 we have shown that the\nnumber of cache misses for the ordinary matrix multiplication algorithm can be\nreduced by a factor of Θ (L\n) by choosing a different data layout.\nCan the idea of cache obliviousness be extended to data structures? Do efficient\ncache-oblivious data structures exist for dynamic data structures, such as linked\nlists, heaps, or trees? Although I do not yet know the answer to this question, I\nhave been able to devise a cache-oblivious layout for static binary search trees that\nis O\n(1\n)-competitive with the performance of B-Trees [28], which are used in file\nsystems and other out-of-core applications because of their low cache complexity.\nFigure 10-2 shows the cache-oblivious layout for a complete binary search tree\nof height 4. Let T be a complete binary tree of height h\nΘ (lg n\n), where n is the\nnumber of elements in the tree. To find the layout, divide T at level\nbh\nc, which\nseparates T into subtree T0 (top\nbh\nc levels) and k\nbh\nc subtrees having height\nat most\ndh\ne. The cache-oblivious data layout\nL\n(T\n) of T is defined recursively as\nfollows.\nL\n(T\n)\n\nL\n(T0\n)\nk\nL (T1\n)\nk\n\nk\nL (Tk\n) ,\n\n2 1 3 6 5\nmemory\n4 12\n7 10\n11 14 13 15\nFigure 10-2: Cache-oblivious layout of a binary tree of height 4 with 15 elements 1,\n2, . . . , 15. The values stored in the nodes of the tree are shown in the order in which they\nare stored. Pointers to left children are shown in grey and pointers to right children in\nblack.\nwhere\nk is the concatenation operator. The base case, when the tree has only one\nnode, is trivial. The cache complexity of finding an element in this data structure\non a\n(Z, L\n) ideal-cache is O\n(lgL n\n), which is asymptotically equivalent to the per\nformance of a B-Tree. Making this layout strategy work for dynamic search trees\nis a high research priority.\n10.3\nComplexity of cache obliviousness\nIn this section, we discuss whether a separation theorem can be proven, showing\nthat certain problems can only be solved cache-optimally by a cache-aware algo\nrithm. We also discuss whether a simulation result can be proven that bounds the\nadvantage of cache-aware algorithms over cache-oblivious algorithms.\nWe know now that many optimal cache-oblivious algorithms exist. But, how\npowerful are cache-oblivious algorithms compared to cache-aware algorithms in\ngeneral?\nSeparation: Is there a separation in asymptotic complexity between cache-aware\nand cache-oblivious algorithms?\nIt appears that cache-aware algorithms should be able to use caches better than\ncache-oblivious algorithms since they have more knowledge about the system they\nare running on. But so far, I have not found a cache-aware algorithm that has better\nasymptotic behavior than a well-designed cache-oblivious algorithm. Neverthe\nless, I do believe a seperating problem exists. I conjecture that for such a seperat\ning problem, the best cache-oblivious algorithm has a factor of Ω(lg Z\n) more cache\nmisses than the best cache-aware algorithm.\nSimulation: Given a class of optimal cache-aware algorithms to solve a single\nproblem, can we construct a good cache-oblivious algorithm that solves the\nsame problem?\n\nI believe that the gap between cache-aware and cache-oblivious algorithms (if it\nexists) is not bigger than a factor of O\n(lg Z\n) difference. Perhaps this result can\nbe proven by using simulation techniques to convert a class of cache-aware algo\nrithms into a cache-oblivious algorithm. I have not yet had much success in this\nline of research, however.\n10.4\nCompiler support for divide-and-conquer\nThis section discusses how new compiler techniques can help to ease the program\nming of divide-and-conquer algorithms. Most algorithms given in this thesis are\ndivide-and-conquer algorithms. Conventional wisdom says that recursive proce\ndures should be converted into iterative loops in order to improve performance [8].\nWhile this strategy was effective ten years ago, many recursive programs now ac\ntually run faster than their iterative counterparts. So far most of the work by archi\ntects and compiler writers is concentrated on loop-based iterative programs. Their\ntools are often not appropriate for recursion and divide-and-conquer programs.\nFor a divide-and-conquer algorithm to be efficient, the base case must be ef\nficiently coded. Coding recursion with a simple \"unit\" base case is usually easy\nfor a programmer, but then the overhead of the recursive implementation can be\nsubstantial. To get full performance out of a recursive algorithm, it is necessary\nto coarsen the base case of recursion (a transformation called \"unfolding\" in [18]),\nwhich is analogous to loop unrolling. Coarsening of base cases is motivated by\nthe observation that for many recursive algorithms, the overhead of recursion is\noften in the lowest few levels, near the leaves. With a branching factor of 2, for\nexample, 97% of the recursive function calls are in the bottom 5 levels of recursion.\nThe proportion is even higher for branching factors greater than 2.\nTypically, a variety of coarsened base cases must be written, making it hard\nto code by hand. Can a compiler effectively generate coarsened base cases? This\nproblem is much like loop-unrolling, which is already done by compilers.\nMatteo Frigo, a member of our research group, and Steve Johnson, also at MIT,\nhave implemented a discrete Fourier transform library FFTW [20] that incorpo\nrates a cache-oblivious algorithm with a specialized compiler to generate coars\nened base cases. I believe that parts of their technique can be extended to general\ndivide-and-conquer algorithms.\nFFTW also employs an adaptive runtime execution, which chooses base cases\nduring an initialization phase of the program. This strategy is effective when the\nquestion of which of several coarsened base cases yields the fastest results on a\n\ngiven architecture cannot be determined at compile time. An adaptive execution\nstrategy allows the compiler to produce several distinct base-case implementa\ntions at different granularities and with different strategies, and then at runtime\ninitialization, choose the fastest for the particular machine by timing the various\nalternatives. Benchmarks performed on a variety of platforms show that FFTW's\nperformance is typically superior to that of other publicly available FFT software,\nand it rivals or is better than many hand-coded vendor libraries.\n10.5\nThe future of divide-and-conquer\nShared-memory multiprocessors are now available as deskside workstations and\nwill appear in desktop PC's in the next two years and in mobile laptops within five\nyears. Divide-and-conquer algorithms seem to be a perfect match for these parallel\nmachines in which the technologies of parallelism and caching are converging.\nIn a shared-memory multiprocessor machine, multiple processors, each having\nits own cache, work together to solve problems faster, communicating through a\nsingle shared memory.\nOur research group discovered, while working on the parallel programming\nlanguage Cilk [39, 9], that divide-and-conquer programs work well with shared-\nmemory multiprocessors. In Cilk a function can be \"spawned\", making it logically\nparallel to the spawning procedure. Since the Cilk scheduler decides at runtime\nwhether two logically parallel functions are actually executed in parallel, a Cilk\nprogram is processor oblivious. It can be effectively executed on many processors,\nas long as the problem has enough inherent parallelism. Rugina and Rinard [32]\nhave experimented with automatic parallelization from C to Cilk and achieved\ngood speedups on divide-and-conquer programs.\nRecursive calls can often be replaced by recursive spawns, which allow the chil\ndren to work in parallel. Once the division phase is complete, the subproblems are\nusually independent and can therefore be solved in parallel. Our experiments with\nCilk show that divide-and-conquer algorithms scale well and have good memory\nbehavior on a parallel machine. The number of cache misses of a Cilk program can\nbe upper-bounded using the cache complexity of its C elision (the Cilk program\nwithout the parallel keywords) as shown in [9, 10].\nCan we design algorithms which are optimal with respect to work, parallelism,\nand cache complexity but which are also cache oblivious and processor oblivious?\nI believe that resource-oblivious versions of the algorithms given in this thesis can\nbe proven to satisfy all three optimality requirements.\nSmall shared-memory multiprocessors are readily avaiable: A 4-processor ma\n\nchine costs less than $20,000 [26].\nMost of these machines are designed to be\nservers, but workstations intended to be used by a single user are starting to ap\npear [35]. These machines will become more common over the next few years, and\nit is expected that we will see a shared-memory multiprocessor-on-a-chip within a\nfew years [23, 27]. Writing efficient parallel programs is considered hard. Caching\nproblems are more pronounced in these machines than they are in single-processor\nmachines. Memory hierarchies will be bigger and steeper in the future, and cache\nmisses will be more expensive.\nThe new Alpha 21264 chip [14], for example,\ncan deliver 2 words from L1-cache in one cycle, but it takes around 100 cycles\nto fetch from main memory. Divide-and-conquer seems to provide a way to write\nprocessor- and cache-oblivious algorithms, which will help to ease programming\non the future machines.\n\nBibliography\n[1] AGGARWAL, A., ALPERN, B., CHANDR A, A. K., AND SNIR, M. A model\nfor hierarchical memory. In Proceedings of the 19th Annual ACM Symposium on\nTheory of Computing (May 1987), pp. 305-314.\n[2] AGGARWAL, A., CHANDR A, A. K., AND SNIR, M. Hierarchical memory with\nblock transfer. In 28th Annual Symposium on Foundations of Computer Science\n(Los Angeles, California, 12-14 Oct. 1987), IEEE, pp. 204-216.\n[3] AGGARWAL, A., AND VITTER, J. S. The input/output complexity of sorting\nand related problems. Communications of the ACM 31, 9 (Sept. 1988), 1116-1127.\n[4] AHO, A. V., HOPCROF T, J. E., AND ULLMAN, J. D. The Design and Analysis of\nComputer Algorithms. Addison-Wesley Publishing Company, 1974.\n[5] ALPERN, B., CARTER, L., AND FEIG, E. Uniform memory hierarchies. In Pro\nceedings of the 31st Annual IEEE Symposium on Foundations of Computer Science\n(Oct. 1990), pp. 600-608.\n[6] BAILEY, D. H. FFTs in external or hierarchical memory. Journal of Supercom\nputing 4, 1 (May 1990), 23-35.\n[7] BELADY, L. A. A study of replacement algorithms for virtual storage com\nputers. IBM Systems Journal 5, 2 (1966), 78-101.\n[8] BENTLEY, J. L. Writing Efficient Programs. Prentice-Hall, 1982.\n[9] BLUMOFE, R. D., FRIGO, M., JOERG, C. F., LEISERS ON, C. E., AND RAN\nDALL, K. H. An analysis of dag-consistent distributed shared-memory algo\nrithms. In Proceedings of the Eighth Annual ACM Symposium on Parallel Algo\nrithms and Architectures (SPAA) (Padua, Italy, June 1996), pp. 297-308.\n[10] BLUMOFE, R. D., FRIGO, M., JOERG, C. F., LEISERS ON, C. E., AND RAN\nDALL, K. H. Dag-Consistent distributed shared memory. In Proceedings of\n\nthe 10th International Parallel Processing Symposium (IPPS) (Honolulu, Hawaii,\nApr. 1996).\n[11] BORODIN, A., AND EL-YANIV, R. Online Computation and Competitive Analy\nsis. Cambridge University Press, 1998.\n[12] CHATTER JEE, S., JAIN, V. V., LEBECK, A. R., AND MUNDHR A, S. Nonlinear\narray layouts for hierarchical memory systems.\nIn Proceedings of the ACM\nInternational Conference on Supercomputing (Rhodes, Greece, June 1999).\n[13] CHATTER JEE, S., LEBECK, A. R., PATNALA, P. K., AND THOTTETH ODI, M.\nRecursive array layouts and fast parallel matrix multiplication. In Proceedings\nof the Eleventh Annual ACM Symposium on Parallel Algorithms and Architectures\n(SPAA) (Saint-Malo, France, June 1999).\n[14] COMPAQ.\nhttpftpdigitalcompu\nbDi\ngita\nlin\nfo\nsemi\ncond\nucto\nr ...\nliteraturedsclibraryhtm\nl.\n[15] COOLEY, J. W., AND TUKEY, J. W. An algorithm for the machine computation\nof the complex Fourier series. Mathematics of Computation 19 (Apr. 1965), 297-\n301.\n[16] CORMEN, T. H., LEISERSO N, C. E., AND RIVEST, R. L. Introduction to Algo\nrithms. MIT Press and McGraw Hill, 1990.\n[17] DUHAMEL, P., AND VETTERLI, M. Fast Fourier transforms: a tutorial review\nand a state of the art. Signal Processing 19 (Apr. 1990), 259-299.\n[18] FRENS, J. D., AND WISE, D. S. Auto-blocking matrix-multiplication or track\ning blas3 performance from source code. In Proceedings of the Sixth ACM SIG\nPLAN Symposium on Principles and Practice of Parallel Programming (Las Vegas,\nNV, June 1997), pp. 206-216.\n[19] FRIGO, M.\nA fast Fourier transform compiler.\nIn Proceedings of the ACM\nSIGPLAN'99 Conference on Programming Language Design and Implementation\n(PLDI) (Atlanta, Georgia, May 1999).\n[20] FRIGO, M., AND JOHNSON, S. G. FFTW: An adaptive software architecture\nfor the FFT. In Proceedings of the International Conference on Acoustics, Speech,\nand Signal Processing (Seattle, Washington, May 1998).\n\n[21] FRIGO, M., LEISERS ON, C. E., PROKOP, H., AND RAMACH ANDRAN, S.\nCache-oblivious algorithms.\nExtended abstract submitted for publication,\nMay 1999.\n[22] GOLUB, G. H., AND VAN LOAN, C. F. Matrix Computations. Johns Hopkins\nUniversity Press, 1989.\n[23] HENNESSY, J. L. Back to the future: Time to return to some long standing\nproblems in computer systems? Plenary talk at FCRC'99.\n[24] HENNESSY, J. L., AND PATTERS ON, D. A. Computer Architecture: A Quantita\ntive Approach, 2nd ed. Morgan Kaufmann Publishers, INC., 1996.\n[25] HONG, J.-W., AND KUNG, H. T. I/O complexity: the red-blue pebbling game.\nIn Proceedings of the Thirteenth Annual ACM Symposium on Theory of Computing\n(Milwaukee, 1981), pp. 326-333.\n[26] IBM.\nhttpwwwpcibmcomus\nnet\nfini\ntyi\nnde\nxht\nml.\n[27] KENNEDY, K. Future investment in information technology research: Report\nof the president's information technology advisory committee. Plenary talk\nat FCRC'99.\n[28] KNUTH, D. E. Sorting and Searching, second ed., vol. 3 of The Art of Computer\nProgramming. Addison-Wesley, 1997.\n[29] MOTWANI, R., AND RAGHAVAN, P. Randomized Algorithms. Cambridge Uni\nversity Press, 1995.\n[30] NODINE, M. H., AND VITTER, J. S. Deterministic distribution sort in shared\nand distributed memory multiprocessors. In Proceedings of the Fifth Symposium\non Parallel Algorithms and Architectures (Velen, Germany, 1993), pp. 120-129.\n[31] PRESS, W. H., FLANNERY, B. P., TEUKOLSK Y, S. A., AND VETTERLIN G, W. T.\nNumerical Recipies in C. Cambridge University Press, 1988.\n[32] RUGINA, R., AND RINARD, M. Automatic parallelization of divide and con\nquer algorithms. In Seventh ACM SIGPLAN Symposium on Principles and Prac\ntice of Parallel Programming (PPOPP) (Atlanta, Georgia, May 1999), pp. 72-83.\n[33] SAVAGE, J. E. Extending the Hong-Kung model to memory hierarchies. In\nComputing and Combinatorics, D.-Z. Du and M. Li, Eds., vol. 959 of Lecture Notes\nin Computer Science. Springer Verlag, 1995, pp. 270-281.\n\n[34] SEDGEWICK, R. Algorithms in C. Addison-Welsey Publishing Company, 1990.\n[35] SGI.\nhttpwwwsgicomprodu\ncts\nhw\nworkstationshtml.\n[36] SINGLETON, R. C. An algorithm for computing the mixed radix fast Fourier\ntransform. IEEE Transactions on Audio and Electroacoustics AU-17, 2 (June 1969),\n93-103.\n[37] SLEATOR, D. D., AND TARJAN, R. E. Amortized efficiency of list update and\npaging rules. Communications of the ACM 28, 2 (Feb. 1985), 202-208.\n[38] STRASSE N, V. Gaussian elimination is not optimal. Numerische Mathematik 13\n(1969), 354-356.\n[39] SUPERCOM PUTING TECHNOL OGIES GROUP, MIT LABORATORY FOR COM\nPUTER SCIENCE. Cilk-5.2 (Beta 1) Reference Manual. Cambridge, MA, 1998.\nAvailable on the Internet from\nhttpsupertechlcsmit\nedu\nci\nlk.\n[40] TOLEDO, S. Locality of reference in LU decomposition with partial pivoting.\nSIAM Journal on Matrix Analysis and Applications 18, 4 (Oct. 1997), 1065-1081.\n[41] VITTER, J. S. External memory algorithms and data structures. In External\nMemory Algorithms and Visualization, J. Abello and J. S. Vitter, Eds., DIMACS\nSeries in Discrete Mathematics and Theoretical Computer Science. American\nMathematical Society Press, Providence, RI, 1999.\n[42] VITTER, J. S., AND NODINE, M. H. Large-scale sorting in uniform memory\nhierarchies. Journal of Parallel and Distributed Computing 17, 1-2 (January and\nFebruary 1993), 107-114.\n[43] VITTER, J. S., AND SHRIVER, E. A. M. Algorithms for parallel memory I:\nTwo-level memories. Algorithmica 12, 2/3 (August and September 1994), 110-\n147.\n[44] VITTER, J. S., AND SHRIVER, E. A. M. Algorithms for parallel memory II: Hi\nerarchical multilevel memories. Algorithmica 12, 2/3 (August and September\n1994), 148-169.\n[45] WINOGRAD, S. On the algebraic complexity of functions. Actes du Congr`es\nInternational des Math ematiciens 3 (1970), 283-288."
    },
    {
      "category": "Resource",
      "title": "kai_thesis.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/a160822fd9fd64d3405872c26acdcea2_kai_thesis.pdf",
      "content": "Data-Race Detection in Transactions-\nEverywhere Parallel Programming\nby\nKai Huang\nB.S. Computer Science and Engineering, B.S. Mathematics\nMassachusetts Institute of Technology, June 2002\nSubmitted to the Department of Electrical Engineering and Computer\nScience in partial fulfillment of the requirements for the degree of\nMaster of Engineering\nin Electrical Engineering and Computer Science\nat the Massachusetts Institute of Technology\nJune 2003\nc⃝ 2003 Massachusetts Institute of Technology. All rights reserved.\nSignature of Author\nDepartment of Electrical Engineering and Computer Science\nMay 21, 2003\nCertified by\nCharles E. Leiserson\nProfessor of Computer Science and Engineering\nThesis Supervisor\nAccepted by\nArthur C. Smith\nChairman, Department Committee on Graduate Theses\n\nData-Race Detection in Transactions-\nEverywhere Parallel Programming\nby\nKai Huang\nSubmitted to the Department of Electrical Engineering\nand Computer Science on May 21, 2003 in partial fulfillment\nof the requirements for the degree of Master of Engineering\nin Electrical Engineering and Computer Science\nABSTRACT\nThis thesis studies how to perform dynamic data-race detection in programs using \"transactions\neverywhere\", a new methodology for shared-memory parallel programming. Since the conventional\ndefinition of a data race does not make sense in the transactions-everywhere methodology, this\nthesis develops a new definition based on a weak assumption about the correctness of the target\nprogram's parallel-control flow, which is made in the same spirit as the assumption underlying the\nconventional definition.\nThis thesis proves, via a reduction from the problem of 3cnf-formula satisfiability, that data-race\ndetection in the transactions-everywhere methodology is an NP-complete problem. In view of this\nresult, it presents an algorithm that approximately detects data races. The algorithm never reports\nfalse negatives. When a possible data race is detected, the algorithm outputs simple information\nthat allows the programmer to efficiently resolve the root of the problem. The algorithm requires\nrunning time that is worst-case quadratic in the size of a graph representing all the scheduling\nconstraints in the target program.\nThesis Supervisor: Charles E. Leiserson\nTitle: Professor of Computer Science and Engineering\n\nContents\nIntroduction\nConstraints in Transaction Scheduling\n2.1\nSerialization Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.2\nAccess Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.3\nAccess Interactions and Access Assignments . . . . . . . . . . . . . . . . . . . . . . .\nThe Definition of a Data Race\n3.1\nDiscussion and Definition\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.2\nRace Assignments\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nNP-Completeness of Data-Race Detection\n4.1\nProof Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.2\nReduction Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.3\nForward Direction\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.4\nBackward Direction\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nAn Algorithm for Approximately Detecting Data Races\n5.1\nPart A : Serialization Structure and Shared-Memory Accesses . . . . . . . . . . . . .\n5.2\nPart B : LCA and Access Interactions\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n5.3\nPart C : Searching for a Possible Data Race . . . . . . . . . . . . . . . . . . . . . . .\n5.4\nCorrectness and Analysis\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nConclusion\nRelated Work\nBibliography\n\nChapter 1\nIntroduction\nThis thesis considers the problem of data-race detection in parallel programs using \"transactions\neverywhere\", a new methodology for shared-memory parallel programming suggested by Charles E.\nLeiserson [26]. Transactions everywhere reduces the amount of thinking required of the programmer\nconcerning concurrent shared-memory accesses.\nThis type of thinking is often unintuitive and\nerror-prone, and represents a main obstacle to the widespread adoption of shared-memory parallel\nprogramming.\nThis introduction first describes Cilk, the parallel-programming language in which we conduct\nour study. Then, it presents transactions as an alternative system to conventional locks for creat-\ning atomic sections and eliminating data races. Finally, it describes the transactions-everywhere\nmethodology for parallel programming with transactions.\nThe Cilk Language.\nThe results in this thesis apply to parallel programming using transactions\neverywhere, irrespective of the implementation language. For concreteness, however, we conduct\nour study in the context of Cilk [2, 3, 22, 13, 39], a shared-memory parallel-programming language\ndeveloped by Charles E. Leiserson's research group. Cilk is faithful extension of C, which means that\nif all Cilk keywords are elided from a Cilk program, then a semantically correct serial C program,\ncalled the serial elision, is obtained. The three most basic Cilk keywords are cilk, spawn, and\nsync. This thesis considers programs that contain only these three extensions.\nWe illustrate the usage of the three Cilk keywords by an example, which we shall reuse through-\nout this introduction. Consider the common scenario of concurrent linked-list update, such as might\narise from insertions into a shared hash table that resolves collisions using linked lists. The fol-\nlowing is a Cilk function for inserting new data at the head of a shared singly linked list. The\nkeyword cilk is placed before a function declaration or definition (in this case the definition of\nlist_insert) to indicate a Cilk function. The shared variable head points to the head of the list,\nand each node of type Node has a data field and a next pointer.\ncilk void list_insert( double new_data )\n{\nNode *pnode = malloc( sizeof(Node) );\npnode->data = process(new_data);\npnode->next = head;\nhead = pnode;\n}\nCilk functions must be called by using the keyword spawn immediately before the function\n\nname.\nA Cilk function call spawns a new thread of computation to execute the new function\ninstance, while the parent function instance continues to execute in parallel. The keyword sync\nis used as a standalone statement to synchronize all the threads spawned by a Cilk function. All\nCilk function instances that have been previously spawned by the current function are guaranteed\nto have returned before execution continues on to the statement after the sync. The following\nsegment of code demonstrates spawn and sync. It inserts two pieces of data into the shared linked\nlist in parallel, and then prints the length of the list.\n. . .\nspawn list_insert(23.118);\nspawn list_insert(23.170);\nsync;\nprintf( \"The list has length %d.\", list_length() );\n. . .\nThe sync statement guarantees that the printf statement only executes after both spawn calls\nhave returned. Without the sync statement, the action of list_length would be unpredictably\nintertwined with the actions of the two calls to list_insert, thus causing an error. Note that the\nfunction list_length for counting the length of the list is a regular C function.\nCilk functions may call other Cilk functions or C functions, but C functions cannot call Cilk\nfunctions. Thus, the function main in a Cilk program must be a Cilk function. Also, all Cilk\nfunctions implicitly sync1 their spawned children before returning.\nIn an execution of a Cilk program, a Cilk thread is defined as a maximal sequence of instruc-\ntions without any parallel-control constructs, which in our case are spawn and sync. Thus, the\nsegment of code above is divided into four serially ordered Cilk threads by the three parallel con-\ntrol constructs (two spawns and a sync). Also, each spawned instance of list_insert comprises\na thread that executes in parallel with some of the threads in the parent function instance. Figure\n1.1 is a graphical view of the serial relationships among these threads.\nbefore\nbetween\nbetween 2nd\nafter\n1st spawn\nspawns\nspawn and sync\nsync\nlist_insert 2nd instance\nlist_insert 1st instance\nFigure 1.1: A graphical view of the threads in our code segment.\nWe use \"sync\" instead of \"synch\" because this is the spelling of the keyword in Cilk.\n\nChapter 2 gives formal terminology and notation for these serial relationships, so that we can\nwork with them mathematically. Henceforth in this thesis, when we talk about programs, functions,\nand threads, we shall mean Cilk programs, Cilk functions, and Cilk threads, respectively.\nNow, a note about the programs and graphs that we study in this thesis is in order. First,\nsince different inputs to a program can cause the program to behave differently in its execution, we\nconsider detecting data races in programs that are run on fixed given inputs. This assumption is\ncommon in dynamic data-race detection. Furthermore, we assume that the control flow of a program\nstays the same even though its threads and transactions may be scheduled nondeterministically,\nbecause it has been proven by Robert H. B. Netzer and Barton P. Miller [32] that it is otherwise\nNP-hard to determine all possible executions of a parallel program. This assumption means that\nalthough a data-race detector can only build a graph from a particular execution trace, we may\nassume that such a graph is representative of all possible executions with respect to its control flow.\nAlso, we are allowed to define and analyze graphs that represent programs in addition to graphs\nthat represent executions.\nTransactions versus Conventional Locks.\nThe function list_insert is not correct as defined\nabove. It contains a data race on the variable head. Between the two accesses to head made by a\ngiven instance of list_insert, the value of head could be changed by a concurrent thread (possibly\nanother instance of list_insert).\nThe conventional solution for eliminating data-race errors is to use locks. (Throughout this\nthesis, we shall use the word \"conventional\" only to describe items related to programming using\nlocks.) In the Cilk library, the functions for acquiring and releasing a lock are Cilk_lock and\nCilk_unlock, respectively. If we use a lock list_lock to protect the shared linked list, then the\nfollowing code is a correct version of list_insert using conventional locks.\ncilk void list_insert( double new_data )\n{\nNode *pnode = malloc( sizeof(Node) );\npnode->data = process(new_data);\nCilk_lock(list_lock);\npnode->next = head;\nhead = pnode;\nCilk_unlock(list_lock);\n}\nThe holding of list_lock during the two accesses to head guarantees that no other thread can\nconcurrently access head while holding list_lock. Thus, if all parallel threads follow a common\ncontract of only accessing head while holding list_lock, then the data race is eliminated.\nFor parallel programmers, keeping track of all the locks in use and deciding which locks should be\nheld at any one time often becomes confusing and error-prone as the size of the program increases.\nThis reasoning about concurrency may be simplified if a programmer uses a single global lock to\nprotect all shared data, but programmers typically cannot just use one lock because of efficiency\nconcerns. Locks are a preventive measure against data races, and as such, only one thread can hold\na lock at any one time, while other concurrent threads that need the lock wait idly.\nThe concept of transactional memory [19] enables parallel programming under the logic of\nusing a single global lock, but without a debilitating loss of efficiency. A transaction is a section of\ncode that must be executed atomically within a single thread of computation, meaning that there\ncan be no intervening shared-memory accesses by concurrent threads during the execution of the\n\ntransaction. Transactional-memory support guarantees that the result of running a program looks\nas if all transactions happened atomically. For example, if we assume that the keyword atomic is\nused to indicate an atomic block to be implemented by a transaction, then the following code is a\ncorrect version of list_insert using transactions.\ncilk void list_insert( double new_data )\n{\nNode *pnode = malloc( sizeof(Node) );\npnode->data = process(new_data);\natomic {\npnode->next = head;\nhead = pnode;\n}\n}\nTransactional-memory support can be provided in the form of new machine instructions for\ndefining transactions, which would minimally include transaction_begin and transaction_end\nfor defining the beginning and end of a transaction. This support can be provided in hardware\nor simulated by software. Hardware transactional memory [19, 20] can be implemented on top of\ncache-consistency mechanisms already used in shared-memory computers. The rough idea is that\neach processor uses its own cache to store changes from transactional writes, and those changes\nare propagated to main memory when the transaction ends and successfully commits. Multiple\nprocessors can be attempting different transactions at the same time, and if there are no memory\nconflicts, then all the transactions should successfully commit.\nIf two concurrent transactions\nexperience a memory conflict (they both access the same shared-memory location, and at least one\nof the accesses is a write), then one of the transactions is aborted and retried at some later time.\nSoftware transactional memory [36, 18] simulates this mechanism in software, but the overhead per\ntransaction is higher, making it less practical for high-performance applications.\nFrom the programmer's point of view, using transactions is logically equivalent to using a single\nglobal lock, because all shared-memory accesses within a transaction happen without interruption\nfrom other concurrent transactions.\nThis property eliminates many problems that arise when\nusing multiple locks, such as priority inversion and deadlock.\nAt the same time, transactional\nmemory avoids the debilitating loss of efficiency that comes with using a single global lock, because\nthe strategy of transactional memory is to greedily attempt to process multiple atomic sections\nconcurrently, only aborting a transaction when an actual memory conflict occurs.\nTransactions Everywhere.\nAs its name suggests, transactions everywhere [26] is a method-\nology for parallel programming in which every instruction becomes part of a transaction. A working\nassumption is that hardware transactional memory provides low enough overhead per transaction\nto make this strategy a viable option.\nLet us define some terminology for the transactions-everywhere methodology.\nThe division\npoints between transactions in the same thread are called cutpoints. Cutpoints can be manually\ninserted by the programmer or automatically inserted by the compiler. To atomize a program\nis to divide a program up into transactions everywhere by inserting cutpoints, resulting in an\natomized program, also known as an atomization of the program. An atomization strategy\nis a method that defines where to insert cutpoints into a program, such as that used by a compiler\nto automatically generate transactions everywhere.\n\nWe first go over the process when transactions everywhere are defined manually by the program-\nmer. In this case, the starting point is the original program with no cutpoints. This starting point\nis in fact a valid atomization of the program itself, with each thread consisting of one transaction\nthat spans the whole thread. Since each thread is an atomic section, we call this most conservative\natomization the atomic-threads atomization of the original program.\nFrom the atomic-threads starting point, the programmer looks to reduce potential inefficiency by\ncutting up large transactions, but only if doing so does not compromise correctness. For example,\nthe function list_insert remains correct as long as the two accesses to head are in the same\ntransaction. Since list_insert is a small function, the programmer may choose to leave it as one\ntransaction. On the other hand, if the call to process takes a long time, then potential inefficiency\nexists because one instance of list_insert may make both of its accesses to head while another\ninstance is calling process. This situation does not produce an error, but is nevertheless disallowed\nby the atomic-threads atomization. Thus, if the call to process takes a long time, the programmer\nmay choose to add a cutpoint after this call to reduce inefficiency. If we assume that the keyword\ncut is used to indicate a cutpoint, then the following code is a more efficient form of list_insert\nusing transactions everywhere.\ncilk void list_insert( double new_data )\n{\nNode *pnode = malloc( sizeof(Node) );\npnode->data = process(new_data);\ncut;\npnode->next = head;\nhead = pnode;\n}\nTo illustrate this atomization, we redraw Figure 1.1 with transactions instead of threads as\nvertices. This new graph is shown in Figure 1.2. We use dashed lines for the edges connecting\nconsecutive transactions within the same thread.\nbefore\nbetween\nbetween 2nd\nafter\n1st spawn\nspawns\nspawn and sync\nsync\nbefore cut\nafter cut\nbefore cut\nafter cut\nFigure 1.2: A revised version of Figure 1.1 with transactions as vertices.\nChapter 2 gives formal terminology and notation for these serial relationships, so that we can\nwork with them mathematically.\n\nWe now consider the process when transactions everywhere are generated automatically by the\ncompiler. In this case, the compiler uses an atomization strategy that consists of a set of heuristic\nrules describing where to insert cutpoints. For example, Cl ement M enier has experimented with one\nsuch set of heuristic rules [28], which we shall call the M enier atomization strategy. This strategy\nproduces cutpoints at the following places:\n- Parallel-programming constructs (spawn, sync).\n- Calling and returning from a C function.\n- The end of a loop iteration (for, while, do loops).\n- Some other language constructs (label, goto, break, continue, case, default).\nIf the cutpoints inserted by the compiler produce a correct atomization (see Chapter 3 for a\nrigorous definition), then the programmer is spared from much thinking about concurrency issues.\nThus, parallel programming becomes easier and more accessible from the average programmer's\npoint of view. For example, applying the M enier atomization strategy to list_insert produces\ncutpoints at the following places, marked by comments beginning with the symbol \"▷\".\ncilk void list_insert( double new_data )\n{\n▷ cutpoint: C function call\nNode *pnode = malloc( sizeof(Node) );\n▷ cutpoint: C function return\n▷ cutpoint: C function call\npnode->data = process(new_data);\n▷ cutpoint: C function return\npnode->next = head;\nhead = pnode;\n}\nThis atomization is correct because the two accesses to head appear in the same transaction.\nIn general, we expect that good heuristic rules should produce correct atomizations for most\nfunctions. In some cases, however, the granularity of the automatically generated transactions is too\nfine to preclude all data races. For example, if list_insert were rewritten so that the statement\nthat sets pnode->next precedes the statement that sets pnode->data, then applying the M enier\natomization strategy produces cutpoints at the following places, once again marked by comments\nbeginning with the symbol \"▷\".\ncilk void list_insert( double new_data )\n{\n▷ cutpoint: C function call\nNode *pnode = malloc( sizeof(Node) );\n▷ cutpoint: C function return\npnode->next = head;\n▷ cutpoint: C function call\npnode->data = process(new_data);\n▷ cutpoint: C function return\nhead = pnode;\n}\n\nThis atomization is incorrect because the call to process causes the two accesses to head to be\nplaced in different transactions, thus resulting in a data-race error.\nThe above example shows that when the compiler automatically generates transactions every-\nwhere, the atomization strategy used may be inaccurate for the particular situation. Likewise, when\nthe programmer manually defines transactions everywhere, human error can lead to an incorrect\natomization. In both cases, the programmer needs to use a data-race detector to find possible error\nlocations, and then to adjust the transaction cutpoints if necessary. This thesis studies how to\nperform this task of detecting data races.\n\nChapter 2\nConstraints in Transaction Scheduling\nThis chapter introduces some terminology for discussing the constraints that define the \"schedules\"\n(i.e. legal executions) of an atomized program. It also proves some preparatory lemmas about these\nconstraints. This background material allows us in Chapters 3-4 to develop a definition for a data\nrace in the transactions-everywhere setting, and to prove properties about data-race detection.\nSection 2.1 lists the \"thread constraints\" and \"transaction constraints\" on program scheduling\nimposed by the serial control flow of the program, and formally defines a schedule of an atomized\nprogram. Section 2.2 gives a notion of equivalence for schedules, and defines \"access constraints\",\nwhich determine whether two schedules are equivalent. Section 2.3 then defines \"access interactions\"\nfor atomized programs, which are the counterpart to access constraints for schedules, and \"access\nassignments\", which are the links between access constraints and access interactions.\nThroughout our analysis of data-race detection, we shall use P to denote a Cilk program that\nhas not yet been atomized, Q to denote an atomized program, and R to denote a schedule of an\natomized program. Also, we shall use e to denote a thread and t to denote a transaction.\n2.1\nSerialization Constraints\nThis section formally defines the \"thread constraints\" and \"transaction constraints\" on the legal\nschedules of an atomized program, which are imposed by the serial control flow of the program. It\nalso defines a \"serialization graph\" for visualizing these constraints. Finally, the formal definition\nof a schedule is derived in terms of these constraints.\nConsider an atomized program Q. Since Q has a fixed control flow, we can view it as being\ncomposed of a set of n transactions, with certain serialization constraints between them. These\nconstraints are determined by the serial order of transactions in the same thread, the serial order\nof threads in the same function instance, and the spawn and sync parallelization structure of the\ncontrol flow. We now give names to these constraints.\nA \"thread constraint\" of an atomized program Q is a constraint on schedules R of Q imposed\nby a serial relationship between two threads in the control flow of Q.\nDefinition 1. In an atomized program Q, a thread constraint exists from transaction t1 to\nE\ntransaction t2, denoted t1 → t2, if\n1. t1 is the last transaction of a thread e1 and t2 is the first transaction of a thread e2, and\n2. the relationship between e1 and e2 is one of the following:\na. e1 immediately precedes e2 in the same function instance, or\nb. e1 directly precedes the spawn point of the function instance whose first thread is e2, or\n\nc. e2 directly follows the sync point of the function instance whose final thread is e1.\nA \"transaction constraint\" of an atomized program Q is a constraint on schedules R of Q\nimposed by a serial relationship between two transactions in the control flow of Q.\nDefinition 2. In an atomized program Q, a transaction constraint exists from transaction t1\nT\nto transaction t2, denoted t1 → t2, if t1 immediately precedes t2 in the same thread.\nWe can view these serialization constraints as edges in a graph with transactions as vertices.\nDefinition 3. The serialization graph of an atomized program Q is the graph G = (V, EE, ET),\nE = {(t1, t2) | t1\nE →\nhere V is the set of transactions, and the two types of ed\ne thread edges E\nw\nges ar\nT = {(t1, t2) | t1\nT\n}\nt\n→\n2 .\nt2} and transaction edges E\nExample. The following example program will be used throughout Chapters 2-3 to illustrate our\ndefinitions. The symbol \"▷\" denotes a comment.\nint x1,\n▷ location l1\ncilk int main()\nx2;\n▷ location l2\n{\nint a;\ncilk void fun1()\n▷ t0\n{\n...\nint a;\nspawn fun1();\n▷ t1\n▷ t4\n...\nx1 = a;\n▷ write l1\n...\n=\n...\na\nx1;\n▷ read l1\n...\n▷ cutpoint\n▷ cutpoint\n▷ t2\n▷ t5\n...\n=\n...\na ...\nx2;\n▷ read l2\n▷ cutpoint\n▷ t3\n=\n...\na ...\nx2;\n▷ read l2\nspawn fun2();\n▷ t7\n...\n}\n▷ cutpoint\ncilk void fun2()\n▷ t8\n...\n{\nint a;\nx1 = a;\n▷ write l1\n...\n▷ t6\n...\nx2 = a;\nsync;\n▷ write l2\n▷ t9\n...\n...\n}\n}\nreturn 0;\n\nThis program uses two shared variables x1 and x2, whose values are stored in shared-memory\nlocations l1 and l2, respectively. The main function spawns two function instances (the first is an\ninstance of fun1, and the second is an instance of fun2), and then syncs them at a later point. The\ncutpoints in this program are marked by comments, because they could very well be automatically\ngenerated. The transactions are t0, . . . , t9, also marked by comments.\nFigure 2.1 shows the serialization graph of our example program. The solid lines are thread\nedges and the dashed lines are transaction edges. The vertices of this graph are the transactions\nt0, . . . , t9. This diagram only labels the vertices with their transaction subscripts, for ease of reading.\nWe shall maintain this practice throughout the remainder of this thesis.\nFigure 2.1: The serialization graph of our example program.\nOur example program generates this runtime graph as follows. For now, ignore the accesses\nto shared-memory locations. The three function instances are represented by the three rows of\nvertices (the middle row has only one transaction). The top row is the program's main function. It\nfirst spawns function fun1 consisting of transactions t1, t2, and t3, and then spawns function fun2\nconsisting of the single transaction t6. The spawned functions are synced before transaction t9. ♦\nNow that we have specified the constraints in scheduling the transactions of Q, we can formalize\nour understanding of a schedule.\nDefinition 4. A schedule R of an atomized program Q is a total (linear) ordering ≺R on the\ntransactions of Q that satisfies all the thread and transaction constraints of Q. That is, for any\nions t1 and t2, if t1\nE t\n→\no\nr t1\nT\n, t\nt\n→\ntwo transact\nhen t1 ≺R t2.\nExample. Figure 2.2 shows one possible schedule Rex of our example program from Figure 2.1.\nThe diagram is drawn with time moving from left to right (earlier transactions appear to the left of\nlater transactions). Once again, this diagram labels the vertices with their transaction subscripts.\nThe ordering of the transactions in this schedule is t0 ≺Rex t1 ≺Rex t4 ≺Rex t2 ≺Rex t5 ≺Rex t7 ≺Rex\n♦\nt8 ≺Rex t6 ≺Rex t3 ≺Rex t9.\n2.2\nAccess Constraints\nThis section addresses the question of which schedules of an atomized program are equivalent in\nthe sense of exhibiting the same behavior. We first define a notion of equivalence for schedules\nderived from the same program.\nThis definition suggests a new type of constraint, called an\n\nFigure 2.2: One possible schedule of our example program.\n\"access constraint\". We prove that access constraints accurately determine when two schedules are\nequivalent. Finally, we define a \"schedule graph\" for visualizing all the constraints in a schedule.\nOur first step is to define a precise notion of equivalence.\nDefinition 5. Let Q and Q′ be (possibly identical) atomizations of a program P, and let R and\nR′ be schedules of Q and Q′, respectively.\nWe say that R and R′ are equivalent if for each\nshared-memory location l,\n1. corresponding writes of lin R and R′ occur in the same order, and\n2. corresponding reads of lin R and R′ receive values written by corresponding writes.\nWe should note two subtleties of Definition 5. One point is that it is not enough for corre-\nsponding reads to receive the same value; they must in fact receive the value written by the same\ncorresponding writes. The other point is that whole sets of accesses consisting of a write and all\nits subsequent reads cannot be reordered. These subtleties exist because we want a definition of\nequivalence that facilitates dynamic data-race detection.\nThe following theorem shows that our notion of equivalence gives strong guarantees for identical\nbehavior between two equivalent schedules.\nTheorem 1. Let Q and Q′ be (possibly identical) atomizations of a program P, and let R and R′\nbe schedules of Q and Q′, respectively. If R and R′ are equivalent, then for each instruction I in\nP, the following invariants hold: Before and after the corresponding executions of I in R and R′ ,\n1. all local variables whose scopes include I have the same values in R and R′, and\n2. all shared-memory locations accessed by I (if any) have the same values in R and R′ .\nProof. Let ⟨I0, . . . , Ik-1⟩ be the instructions of P in the order that they occur in R. We shall prove\nthe invariants by strong induction on this ordering of the instructions.\nFirst, observe that if the invariants are met before an instruction I executes, then they continue\nto hold true after I executes.\nConsequently, we only need to concentrate on proving that the\ninvariants hold before an instruction executes.\nFor the base case, we note that the first instruction in any schedule must necessarily be the first\ninstruction of the main function instance of P. Therefore, I0 is the first instruction in both R and\nR′. Before I0 executes, all the local variables and shared-memory locations are identical, since no\ninstructions have been executed yet.\nFor the inductive step, assume that the invariants hold for all instructions before some instruc-\ntion Ii in R. The value of a local variable created before Ii was written by the same previous\n\ninstruction I that serially precedes Ii in both schedules, because the serial control flow is deter-\nmined by P and is identical in both schedules. By the inductive hypothesis, we know that the value\nof this local variable is the same in R and R′ after I executes.\nNow, consider the value of a shared-memory location l accessed by Ii. If Ii writes l, then\ncondition 1 in Definition 5 dictates that the previous instruction I to write lmust be the same in\nboth schedules. Similarly, if Ii reads l, then condition 2 in Definition 5 guarantees that the previous\ninstruction I to write lis the same in both schedules. Applying the inductive hypothesis to I tells\nus that the value in lwas the same in both R and R′ after I executed.\nWe now introduce a third type of constraint called an \"access constraint\", which is imposed\nby parallel accesses to shared memory. Access constraints determine how a schedule R may be\nreordered into an equivalent schedule R′, or alternatively, whether two schedules R and R′ are\nequivalent. Unlike thread and transaction constraints, access constraints are defined for a particular\nschedule R as opposed to an atomized program Q.\nDefinition 6. In a schedule R of an atomized program Q, an access constraint exists from\nA\ntransaction t1 to transaction t2, denoted t1 → t2, if\n1. t1 and t2 are in parallel in the control flow of Q,\n2. there exists a shared-memory location lsuch that t1 and t2 both access l, and at least one\nof them writes l, and\n3. t1 appears before t2 in R (i.e. t1 ≺R t2).\nWe can think about the constraints on how a schedule R can be reordered in terms of a graph\nwith transactions as vertices and constraints as edges.\nDefinition 7. The schedule graph of a schedule R of an atomized program Q is the graph\nG = (V, EE, ET, EA), where V is the set of transactions, and the three types of edges are thread\n= {(t1, t2) | t1\nE\n}, t\nt\n→\nT = {(t1, t2) | t1\nT\n}\nt\n→\n, a\nd\nion ed\nd access constraint\ns E\nges E\nt\ne ge\nransac\nn\nE\n= {(t1, t2) | t1\nA\n}\nt\n→\n2 .\nedges E A\nExample. Figure 2.3 shows the schedule graph of our example schedule from Figure 2.2. The access\nconstraint edges are drawn with dotted lines.\nFigure 2.3: The schedule graph of our example schedule from Figure 2.2.\nThe shared-memory accesses in our example program are as follows:\n- location l1 is written by transactions t1 and t8, and read by transaction t4;\n- location l2 is written by transaction t6, and read by transactions t3 and t5.\n\nnts t1\nA t\n→\na\nd t1\nA\n. T\nt\n→\nThe accesses to l1 generate the two access constrai\nhere i\nn\ns no access\nconstraint from t4 to t8 even though they both access l1, because they are not in parallel in the\ncontrol flow of our example program. The multiple accesses to l2 only generate the single access\nnt t6\nA\n. T\nt\n→\ni\nhere is no access constraint from t5 to t6 even though they both access l2,\nt\ncons ra\nbecause they are not in parallel, and there is no access constraint from t5 to t3 even though they\nboth access l2, because neither of them writes l2.\n♦\nThe following lemma proves that access constraints accurately determine equivalence.\nLemma 2. Two schedules R and R′ of an atomized program Q are equivalent if and only if they\nhave the same set of access constraints.\nt t1\nA\nb\nt\n→\ne an ar\nt R′\nProof. Forward direction. L\nbitrary access constraint of R. We shall show th\ne\na\nalso has this access constraint. Conditions 1 and 2 in Definition 6 do not depend on the particular\nschedule, so they apply to R′ as well. All that remains to be shown is condition 3, which requires\nthat t1 ≺R′ t2. We consider three cases.\nCase 1: t1 and t2 both write l. Since t1 and t2 both write l, condition 1 in Definition 5 dictates\nthat if t1 ≺R t2, then it must be that t1 ≺R′ t2 as well.\nCase 2: t1 writes l and t2 only reads l. Let t3 be the last transaction before t2 to write l. It must\nbe the same transaction in both R and R′ because they are equivalent. If t1 = t3, then certainly\nt1 ≺R′ t2. If t1 = t3, then t1 ≺R t3, since t1 also writes l but is not the last transaction to do\nso before t2. Thus, the ordering of the three transactions in R is t1 ≺R t3 ≺R t2. Condition 1 in\nDefinition 5 dictates that t1 ≺R′ t3 as well, and condition 2 dictates that t3 ≺R′ t2 as well, so we\nconclude that t1 ≺R′ t2.\nCase 3: t1 only reads l and t2 writes l. Let t3 be the last transaction before t1 that writes l. It\nmust be the same transaction in both R and R′ because they are equivalent. Thus, the ordering of\nthe three transactions in R is t3 ≺R t1 ≺R t2. Now, if t2 ≺R′ t1, then we must also have t2 ≺R′ t3,\nbecause t3 is the last transaction to write l before t1. But, having t2 ≺R′ t3 contradicts condition\n1 in Definition 5, so we conclude that t1 ≺R′ t2.\nnt t1\nA t\n→\no\nt R′ h\nSince the preceding arguments apply to any access constrai\nf R, we see tha\nas\nall the access constraints of R. By a symmetrical argument, R also has all the access constraints\nof R′. Thus, R and R′ must have the same set of access constraints.\nBackward direction. We shall prove the contrapositive, which says that if R and R′ are not equiv-\nalent, then they do not have the same set of access constraints.\nIf R and R′ are not equivalent due to condition 1 in Definition 5 not being satisfied, then let\nt1 and t2 be transactions that both write a shared-memory location l, and such that t1 ≺R t2 and\nnt t1\nA t\n→\ne\nr R′, so R and R′ d\nt2 ≺R′ t1. Then, the access constrai\nists for R but not f\nt h\nx\no\no no\nave\nthe same set of access constraints.\nIf R and R′ are not equivalent due to condition 2 in Definition 5 not being satisfied, then let t1\nbe a transaction that reads a shared-memory location l, but for which the previous transaction to\nwrite l was t2 in R and t3 in R′. We consider three possibilities. First, if t1 ≺R t3, then the access\nA t\n→\ne\nr R′. S\ni\nists for R but not f\nd, if t1 ≺R′ t2, then the access constrai\nt\nnt t1\nnt\ncons ra\nx\no\necon\nA t\n→\ne\nists for R′ but not for R. Finally, if both t2 and t3 appear before t1 in both R and R′\nt1\nx\n,\nthen the ordering of the three transactions in R must be t3 ≺R t2 ≺R t1 (because t2 is the last\ntransaction to write l before t1), while the ordering in R′ must be t2 ≺R′ t3 ≺R′ t1. But then,\nt3\nA\ni\nt\n→\n2 s an access cons traint that exists for R but not for R′. Thus, in all cases, R and R′ do not\nhave the same set of access constraints.\n\n2.3\nAccess Interactions and Access Assignments\nUltimately, we wish to prove properties about atomized programs as a whole, and not just particular\nschedules. In preparation for doing so, this section defines \"access interactions\", which are the\ncounterpart for an atomized program Q to what access constraints are for a schedule of Q, and the\n\"interaction graph\", which is the counterpart to the schedule graph. Finally, this section answers\nthe question of which subsets of all possible access constraints of an atomized program Q can be\nextended to a scheduling of Q. In doing so, it introduces \"access assignments\", which represent the\nlink between access interactions and access constraints.\nAn \"access interaction\" of an atomized program Q is a bidirectional (symmetric) relation be-\ntween two transactions of Q that share an access constraint in any schedule.\nDefinition 8. In an atomized program Q, an access interaction exists between transaction t1\nA\nA\nand transaction t2, denoted t1 ↔ t2 (or t2 ↔ t1), if\n1. t1 and t2 are in parallel in the control flow of Q, and\n2. there exists a shared-memory location lsuch that t1 and t2 both access l, and at least one\nof them writes l.\nConditions 1 and 2 in Definition 8 are the same as conditions 1 and 2 in Definition 6. If there is\nan access interaction between two transactions t1 and t2, then in any schedule R of Q, there is an\nr t1\nA t\n→\no\nr t2\nA\n, d\nt\n→\nint of eith\nding on how t1 and t2 are ordered with r\nt\nt\naccess cons ra\ne\nepen\nespec\nto each other in R.\nWe can view all the serialization constraints and access interactions of Q in one graph.\nDefinition 9. The interaction graph of an atomized program Q is the graph G = (V, EE, ET, EA),\nwhere V is the set of transactions of Q, and the three types of edges are thread edges E\n=\nE\n{(t1, t2) | t1\nE\n}, t\nt\n→\nT = {(t1, t2) | t1\nT\n}\nt\n→\n, a\nion ed\nd access interaction edges\ns E\nt\nransac\nge\nn\nA = {{t1, t2} | t1\nA\n}\nt\n↔\n2 .\nE\nIn this thesis, we shall sometimes refer simply to an \"access edge\" when it is clear by context\nwhether we mean an access constraint edge or an access interaction edge.\nExample. Figure 2.4 shows the interaction graph of our example program.\nFigure 2.4: The interaction graph of our example program.\ns t1\nA t\n↔\n, a\nA t\nt\n↔\n,\nd t3\nA t\n↔\na\nThe access interaction ed\ne drawn as dotted lines with\nt a\nge\nn\nr\nou\nrrow\nheads. The reader should refer to the text following Figure 2.3 for a discussion of the shared-memory\naccesses in our example program, and why certain pairs generate access edges.\n\nComparing this diagram with Figure 2.3, even though the transactions are named and positioned\ndifferently, we can see that the access constraints in Figure 2.3 are simply the access interactions\nin this diagram with directions selected based on the particular schedule.\n♦\nAlthough each bidirectional access interaction of Q corresponds to an access constraint in one\ndirection or the other in a schedule R, not every assignment of directions to access interactions\n(thus turning them into access constraints) produces a legal schedule. The final question we address\nin this section is that of which subsets of all the possible access constraints can be extended to a\nschedule.\nDefinition 10. An access assignment A of an atomized program Q is a subset of all the possible\naccess constraints, as indicated by the access interactions of Q. For example, if Q has an access\nn t1\nA\n, t\nt\n↔\ns t1\nA t\n→\ninteracti\nhen A may contain neither, either, or both of the access constraint\no\ne s\nsuch that A is a subset of the access constraints of R. In such a case, we say that R realizes A.\nWe can view an access assignment A of an atomized program Q, along with the serialization\nconstraints of Q, as a graph.\nDefinition 11. The assignment graph of an access assignment A of an atomized program Q\nis the graph G = (V, EE, ET, A), where V is the set of transactions, and the three types of edges\nA\n. W\nt\n→\nd t2\nay that an access assignment A is realizable if there exists a schedule R of Q\nan\nE = {(t1, t2) | t1\nE\n}, t\nt\n→\nT = {(t1, t2) | t1\nT\n}\nt\n→\n, a\nhread ed\ntion ed\nd a\ns E\ns E\ne t\nar\nge\nransac\nge\nn\nccess\ninteraction edges from A.\nExample. According to Figure 2.4, the set of all possible access constraints for our example pro-\ngram is {(t1, t4), (t4, t1), (t1, t8), (t8, t1), (t3, t6), (t6, t3)}. Any subset of this set constitutes an access\nassignment of our example program. One particular access assignment is {(t1, t4), (t6, t3)}. Its\nassignment graph is shown in Figure 2.5.\n♦\nFigure 2.5: The assignment graph of one particular access assignment.\nNow we may answer the question of which access assignments are realizable.\nLemma 3. An access assignment A of an atomized program Q is realizable if and only if its\nassignment graph G does not contain any cycles.\nProof. Forward direction.\nWe shall prove the contrapositive, which says that if G contains a\ne t0\n? →\n? t\n→\n· · ·\n? t\n→\n, w\n? t\n→\nk-\nle, then A is not realizable. Let the cycle in G b\nhere each\ncyc\n\npair of consecutive transactions has a constraint between them of one of the three types (thread,\ntransaction, access). Since G is a subgraph of the schedule graph of any schedule R that realizes\nA, any such schedule R must also satisfy all the constraints in the cycle. In particular, we must\nhave t0 ≺R t1 ≺R · · · ≺R tk-1 ≺R t0, which is impossible since t0 cannot precede itself in R.\nBackward direction. If G does not contain any cycles, then we can construct a schedule R that\nrealizes A using a topological-sort algorithm (for example, see [6], pages 549-551).\n\nChapter 3\nThe Definition of a Data Race\nThis chapter defines a data race in the transactions-everywhere environment and proves conditions\nfor the existence of a data race. Section 3.1 first discusses the difference between data-race detection\nin the transactions-everywhere setting and in the conventional locks setting. This discussion leads\nus to the definition of a data race in the transactions-everywhere methodology. The definition is\nbased on the basic assumption of \"correct parallelization\", which informally says that the target\nprogram's spawn-and-sync structure is correct.\nThis assumption is made in the same spirit as\nthe assumption that underlies the conventional definition of a data race.\nSection 3.2 proceeds\nto determine necessary and sufficient conditions (collectively called a \"race assignment\") for an\natomized program to contain a data race.\nThese conditions are easier to work with than the\nprimitive definition of a data race, and they find their application in Chapters 4-5.\n3.1\nDiscussion and Definition\nThis section establishes the definition of a data race in the transactions-everywhere methodology.\nWe begin by discussing why the conventional definition of a data race does not make sense when\nusing transactions everywhere, especially when the transactions are automatically generated. We\nthen explore the primary assumption behind the conventional definition, and develop an assump-\ntion, called \"correct parallelization\", that can be made in the same spirit (albeit weaker) when\nusing transactions everywhere. It turns out that this assumption is difficult to apply directly, so\nthis section uses it to prove that the atomic-threads atomization of the target program is guaranteed\nto be correct. This last fact serves as the basis for the definition of a data race.\nDetecting data races in the transactions-everywhere setting is much different than the corre-\nsponding task in the conventional locks setting. Conventional data-race detectors find places in a\nparallel program where two concurrent threads access the same memory location without holding\na common lock, and at least one of the accesses is a write. If the same algorithm were used on an\natomized program, it would report no data races. The reason is that using transactions is logically\nequivalent to using a single global lock, and using transactions everywhere is analogous to executing\nall instructions, in particular all memory accesses, while holding the common global lock.\nThe absence of conventional data races does not ensure that an atomization is guaranteed to\nbe accurate. From the point of view of data-race detection, the key difference between locks and\ntransactions everywhere is that locks carry the programmer's certification of correctness. Locks do\nnot actually eliminate data races, because the locked sections can still be scheduled in different ways\nthat lead to different answers. Rather, locks carry the assertion that the programmer has thought\n\nabout all the possible orderings of the sections protected by the same lock, and has concluded that\nthere is no error, even though the possibility for nondeterminism exists.\nSince transactions everywhere do not carry any certification by the programmer (especially\nwhen they are automatically generated), it would seem that all potential data-race errors must\nstill be reported, leading to the same amount of thinking required of the programmer as when\nusing locks. This reasoning is incorrect, however. If we make just one basic assumption about\nthe programmer's understanding of the program, then we can avoid reporting many data races,\nbecause they most likely do not lead to errors.\nThe basic assumption we make is that the program has \"correct parallelization\", which infor-\nmally says that the program's spawn-and-sync structure, as the programmer has written it, does\nnot need to be changed to make the program correct. The only thing that may need to be adjusted\nis the atomization.\nDefinition 12. We say a program has correct parallelization if there exists some atomization\nof the program such that all of its possible schedules exhibit correct behavior, as determined by\nthe programmer's intent.\nBecause our definition is based on the programmer's intent, it does not require the atomized\nprogram to behave deterministically, either internally or externally (see [33] for definitions of these\nterms). However, we do restrict the meaning of correct behavior to eliminate efficiency concerns\ndue to differences in atomization or schedule.\nThe assumption that a program has correct parallelization is made in the same spirit as the\nassumption made by conventional data-race detectors that concurrent memory accesses with a com-\nmon lock are safe. In the locks environment, the assumption is that the programmer thought about\ncorrectness when he or she used the functions Cilk_lock and Cilk_unlock. In the transactions-\neverywhere environment, the assumption is that the programmer thought about correctness when\nhe or she used the keywords spawn and sync.\nThe assumption of correct parallelization is difficult to use directly, because while it promises\nthe existence of a correct atomization, it gives us no information about what that atomization\nmay be. In order to check whether a given atomization is correct, we wish to be able to compare\nit against something concrete that we know to be correct. Such a standard is provided by the\nfollowing theorem.\nTheorem 4. If a program P has correct parallelization, then the atomic-threads atomization Q∗ of\nP is guaranteed to be correct.\nProof. Since P has correct parallelization, there exists some atomized version Q of P such that\nall schedules of Q exhibit correct behavior. It suffices for us to show that every schedule of Q∗ is\nequivalent to some schedule of Q, because then the possible behaviors of Q∗ would be a subset of\nthe possible behaviors of Q, all of which we know to be correct.\n∗\nWe map a schedule R∗ of Q into an equivalent schedule R of Q simply by dividing each thread\nin R∗ into its constituent transactions in Q, while keeping all the instructions in their original\norder. Since R∗ and R represent executions of the same instructions in the same order, they are\n∗\nequivalent. We also need to check that R is a legal schedule of Q. Since R∗ is a legal schedule of Q ,\nit satisfies all the thread constraints of Q∗ , which are the same as those of Q, so R satisfies the thread\nconstraints of Q. Also, since each thread of R∗ is divided into transactions of Q without changing\nthe order of those constituent transactions in Q, we see that R also satisfies all the transaction\nconstraints of Q.\n\nTheorem 4 gives us an important tool with which to build a race detector. Given a program\nwith correct parallelization, we always know of one concrete atomization that is guaranteed to be\ncorrect (albeit possibly inefficient). Our strategy shall be to have the data-race detector check\nwhether all the possible behaviors of a given atomization of a program are also possible behaviors\nof the atomic-threads atomization of the same program.\nDefinition 13. In the transactions-everywhere methodology, let P be a program with correct\nparallelization, let Q be an atomized program derived from P, and let Q∗ be the atomic-threads\natomization of P. We say Q contains a data race if there exists a schedule of Q that is not\nequivalent to any schedule of Q∗ .\nJust as data races in the conventional locks setting are not necessarily errors, data races in the\ntransactions-everywhere setting are also not always errors. Instead, we can consider Definition 13\nto be a good heuristic for when to report a possible data-race error.\n3.2\nRace Assignments\nThis section identifies conditions under which an atomized program contains a data race, in hopes\nof developing efficient algorithms to search for these conditions.\nWe first determine conditions\n(collectively named a \"thread cycle\") for when a particular schedule of an atomized program causes\na data race. Then, we extend this result to find conditions (collectively named a \"race assignment\")\nfor when an atomized program contains a data race.\nOur first step is to determine conditions for when a particular schedule causes a data race.\nConsider a schedule R of an atomization Q of a program P, whose atomic-threads atomization is\nQ∗. Since we want to know whether R is equivalent to some schedule of Q∗ , we can think of the\naccess constraints of R as inducing an access assignment A on Q∗. Remember that the transactions\nof Q∗ are just the threads of P.\nDefinition 14. The induced access assignment by R on Q∗ is the access assignment A of Q∗\nnt e1\nA →e2 w\nnt t1\nA →t2 in R\nhat contains an access constrai\nhenever there exists an access constrai\nt\nfrom some transaction t1 in thread e1 to some transaction t2 in thread e2.\nExample. Recall our example schedule from the Figure 2.2 and its schedule graph from Figure\n2.3. The induced access assignment by this schedule on the atomic-threads version of our example\nprogram is shown in Figure 3.1.\n1-2-3\n4-5\n7-8\nFigure 3.1: The induced access assignment by our example schedule from Figure 2.2.\n\nThe transactions of the atomic-threads atomization are simply the threads of the program. Each\naccess constraint edge between two transactions in Figure 2.3 translates into an access constraint\nedge between those transactions' respective threads in Figure 3.1. Although our example does not\nshow it, the reader should note that if a schedule graph were to contain multiple access constraint\nedges from transactions belonging to one thread to transactions belonging to another thread, then\nall those access constraint edges would collapse into one access constraint edge between the two\nthreads in the assignment graph of the induced access assignment.\n♦\nThe following lemma ties the concept of the induced access assignment to our problem. In the\nfollowing proof, we extend our \"≺R\" notation for transaction precedence in a schedule R in the\nnatural way to denote instruction precedence in R as well.\nLemma 5. Let R be a schedule of an atomization Q of a program P, whose atomic-threads at-\nomization is Q∗ . Then, R is equivalent to some schedule of Q∗ if and only if the induced access\nassignment A by R on Q∗ is realizable.\nProof. Forward direction. Let R∗ be a schedule of Q∗ that is equivalent to R. We shall prove that\nt e1\nA\nb\n→ e2\nR ∗ realizes A. L\nint in A. We need to show that e1 ≺R∗ e2, so\nt\ne\ne any access cons ra\nas e1\nA →\na\ne2\nhat R∗ also h\nint. Definition 14 implies that an access constrai\nt\nt\nnt\ns an access cons ra\nA t\n→\ne\nists in R from some transaction t1 in thread e1 to some transaction t2 in thread e2.\nt1\nx\nThen, Definition 6 implies that t1 and t2 both access a shared-memory location l, and at least one\nof them writes l. We consider three cases.\nCase 1: t1 and t2 both write l. Let I1 and I2 be instructions in t1 and t2, respectively, that write\nl. We know that I1 ≺R I2. Now, if e2 ≺R∗ e1, then we would have I2 ≺R∗ I1, causing R and R∗\nnot to be equivalent, which is a contradiction. Thus, we must have e1 ≺R∗ e2.\nCase 2: t1 writes l and t2 only reads l. Let I1 be an instruction in t1 that writes l, and let I2 be\nan instruction in t2 that reads l. Furthermore, let I be the last instruction before I2 that writes l.\nWe know that I1 ≺R I because I1 also writes l but is not the last instruction before I2 to do so.\nNow, if e2 ≺R∗ e1, then we would have I ≺R∗ I1, causing R and R∗ not to be equivalent, which is\na contradiction. Thus, we must have e1 ≺R∗ e2.\nCase 3: t1 only reads l and t2 writes l. Let I1 be an instruction in t1 that reads l, and let I2 be\nan instruction in t2 that writes l. Furthermore, let I be the last instruction before I1 that writes\nl. We know that I ≺R I2 because I ≺R I1 and I1 ≺R I2. Now, if e2 ≺R∗ e1, then we would have\nI2 ≺R∗ I, causing R and R∗ not to be equivalent, which is a contradiction. Thus, we must have\ne1 ≺R∗ e2.\nnt t1\nA\ni\nt\n→\nSince the preceding arguments apply to any access constrai\nn A, we conclude th t\na\nR∗ has all the access constraints of A, which proves that R∗ indeed realizes A.\n∗\nBackward direction. Let R∗ be a schedule of Q that realizes A. We shall prove that R is equivalent\nto R∗. First, construct a schedule R′ of Q by dividing up the threads of R∗ into their constituent\ntransactions in Q, while maintaining the same order for all the instructions. Then, R′ satisfies the\nthread constraints of Q because they are the same as those for Q∗ , and R′ satisfies the transaction\nconstraints of Q because the threads in R∗ are divided up without any reordering of the transactions.\nThus, R′ is a legal schedule of Q.\nNow, we show that R is equivalent to R′, which in turn is equivalent to R∗. That R′ and R∗\nare equivalent follows from the fact that both represent executions of the same instructions in the\nA t\n→\no\nder. To show that R and R′ are equivalent, consider any access constrai\nf R. L\nnt t1\nt\nsame or\ne\nA →\na\ne2\nt1 belong to thread e1 and t2 belong to thread e2. By Definition 14, R∗ must have e1\ns an\n\n(r)\n∗\naccess constraint, which means that we must have e1 ≺R∗ e2. Then, because R′ is derived from R\nwithout any reordering of instructions, it must be true that in R′, all the transactions of e1 appear\nconsecutively before all the transactions of e2 appear consecutively. In particular, t1 ≺R′ t2, so that\ns t1\nA t\n→\na\nR′ also h\ntraint. Since this reasoning applies to any access constraint of\na\ns an access cons\nR, we see that R′ must have all the access constraints of R. In addition, R′ cannot have any other\naccess constraints, because the number of access constraints for a schedule of Q is constant (equal\nto the number of access interactions of Q). Thus, R and R′ have the same set of access constraints,\nand by Lemma 2, they must therefore be equivalent.\nThe following definition and lemma restate the result in Lemma 5 without reference to the\ninduced access assignment.\nDefinition 15. A thread cycle in a schedule graph, interaction graph, or assignment graph is a\nseries of pairs of transactions\n(t0, t1), (t1, t2), . . . , (tk-2, t′\n′\n′\nk-1), (tk-1, t0\n′ )\nsuch that\n1. for each i ∈{0, . . . , k -1}, there is a thread or access edge from ti to ti\n′\n+1, where index\narithmetic is performed modulo k, and\n2. for each i ∈{0, . . . , k -1}, ti\n′ and ti belong to the same thread (and may be the same\ntransaction).\nExample. Recall the example access assignment {(t1, t4), (t6, t3)} of our example program, and its\nassignment graph from Figure 2.5. This assignment graph contains the thread cycle ⟨(t1, t4), (t5, t6),\n(t6, t3)⟩, which is shown in Figure 3.2. Note that each pair of transactions in the thread cycle\nt1\nA\n)\nt\n→\nE t\nt\n→\n,\nA t\nt\n→\n,\nither a thread or access constraint edge (\n. Also, note th\nnt\nt\nreprese\ns e\na\nthe second transaction of one pair and the first transaction of the next pair are in the same thread\n(t4 and t5, t6 and t6, t3 and t1).\n♦\nFigure 3.2: A thread cycle in the assignment graph from Figure 2.5.\nLemma 6. Let G be the schedule graph of a schedule R of an atomization Q of a program P. Then,\nR is equivalent to some schedule of the atomic-threads atomization of P if and only if G does not\ncontain any thread cycles.\nProof. Let Q∗ be the atomic-threads atomization of P. Lemma 5 tells us that R is equivalent\nto some schedule of Q∗ if and only if the induced access assignment A by R on Q∗ is realizable.\nFurthermore, by Lemma 3, we know that A is realizable if and only if its assignment graph GA\ndoes not contain any cycles. Thus, all we need to prove is that GA does not contain any cycles if\n\n′\n′\n′\n\n(r)\n′\n′\n′\n′\n′\nand only if G does not contain any thread cycles, or equivalently, that GA contains a cycle if and\nonly if G contains a thread cycle.\nBelow, we prove that GA contains a cycle if and only if G contains a thread cycle.\norm e0\n? →\n? → e1\n· · ·\n? →\n, w\ne0\n? → e\nk-\nForward direction. If GA contains a cycle c, then c is of the f\nh ere\nany two consecutive threads are connected by either a thread constraint or an access constraint\n(there are no transaction constraints in the atomic-threads atomization). For every thread edge\nei\nE\ni\n→ e +1\ni\ne ti\nE t\n→\ni+1 in G, where ti is the last transacti\nhere is a corresponding thread ed\nn c, t\ng\non\nof ei and ti+1 is the first transaction of ei+1. The reason is that Q and Q∗, both being atomizations\nof the same program P, have the same thread constraints. Also, for every access constraint edge\nei\nA\ni\n→ e +1\ni\nn c, Definition 14 guarantees the existence of an access constraint edge ti\nA t\n→\ni+1\nin G such that ti belongs to ei and t′\nelongs to ei+1.\nThus, G contains the thread cycle\ni+1 b\nC = (t0, t1), (t1, t2), . . . , (tk-2, t′\n′\n′\nk-1), (tk-1, t0\n′ ) .\nBackward direction. If G contains a thread cycle C, then let C = ⟨(t0, t1), (t1, t2), . . . , (tk-2, t′\n,\nk-1)\n(tk-1, t0\n′ )⟩, and furthermore, let ei denote the thread containing ti and ti for each i. For every\ne ti\nE t\n→\nge ei\nE\ni\n→ e +1\ni\ni+1 i\nhread ed\nhere is a corresponding thread ed\nn GA. The reason i\nt\nn c, t\ng\ns\nthat Q and Q∗, both being atomizations of the same program P, have the same thread constraints.\ne ti\nA t\n→\ni+1 i\nAlso, for every access constraint ed\nn C, Definition 14 tells us that there is a resulti\ng\nng\nge ei\nA\ni\n→ e +1\ni\norm e0\n? →\n? → e1\n· · ·\n? →\naint ed\nn GA. Thus, GA contains a cycle c of the f\nt\naccess cons r\n? →\n, w\ne0\nhere any two consecutive threads are connected by either a thread constraint or an\nek-1\naccess constraint.\nWith Lemma 6, we are ready to extend our analysis from schedules to atomized programs. We\ncan now derive exact conditions for when an atomized program contains a data race.\nDefinition 16. A race assignment A of an atomized program Q is an access assignment of Q\nwhose assignment graph contains a thread cycle but does not contain any cycles.\nExample. Once again, recall the example access assignment {(t1, t4), (t6, t3)} of our example pro-\ngram, and its assignment graph from Figure 2.5. The previous example showed that this assignment\ngraph contains a thread cycle. When we examine Figure 2.5, however, we find that it does not\ncontain any cycles. Therefore, the example access assignment {(t1, t4), (t6, t3)} is in fact a race\nassignment of our example program.\n♦\nThe following theorem shows that the existence of a race assignment is a necessary and sufficient\ncondition for the existence of a data race in an atomized program.\nTheorem 7. An atomized program contains a data race if and only if it has a race assignment.\nProof. Throughout this proof, let Q be the atomized program referred to by the theorem, and let\nQ∗ be the atomic-threads atomization of the program that Q is derived from.\nForward direction. If Q contains a data race, then by Definition 13, there exists a schedule R of Q\nthat is not equivalent to any schedule of Q∗. Then by Lemma 6, the schedule graph G of R must\ncontain a thread cycle. Let A be the set of access constraint edges appearing in one thread cycle. I\nclaim that A is a race assignment of Q. The condition that the assignment graph GA of A contain\na thread cycle is satisfied by construction (we picked the elements of A to make the cycle). Also,\nwe know that A is realizable (by R), which by Lemma 3 tells us that GA does not contain any\ncycles, so this condition is satisfied as well.\n\nBackward direction. Let A be a race assignment of Q, and let GA be its assignment graph. Since\nA does not contain any cycles, we know by Lemma 3 that it is realizable. Let R be a schedule of\nQ that realizes A, and let G be the schedule graph of R. Since R realizes A, we know that G is a\nsupergraph of GA, which implies that G also contains the thread cycle that exists in GA. Finally,\nby Lemma 6, we deduce that R is not equivalent to any schedule of Q∗, from which we conclude\nthat Q contains a data race.\n\nChapter 4\nNP-Completeness of Data-Race\nDetection\nThis chapter gives strong evidence that dynamic data-race detection in the transactions-everywhere\nmethodology is intractable in general. It proves that even given the interaction graph of the target\natomized program, searching for data races is an NP-complete problem in general1. In particular,\nthe NP-hardness of data-race detection is proved via a polynomial-time reduction from the problem\nof 3cnf-formula satisfiability (3SAT) to the problem of data-race detection.\nSince the proof is long, it is divided up into four sections. The presentation follows a top-down\napproach. Section 4.1 is an outline of the complete proof. It references definitions and lemmas in\nSections 4.2-4.4 to fill in major parts of the proof.\nIn order to understand this chapter, the reader should be familiar with the theory of NP-\ncompleteness and the problem of 3cnf-formula satisfiability. For a good treatment of these topics,\nsee [37], Sections 7.1-7.4 (pages 225-260).\n4.1\nProof Outline\nThere are two parts to proving the NP-completeness of data-race detection. The first part, proving\nthat the problem of data-race detection belongs in NP, is done in this section. The second part,\nproving that all problems in NP are polynomial-time reducible to the problem of data-race detection,\nis outlined in this section, with the bulk of the work to be filled in by Sections 4.2-4.4.\nFollowing standard practice, we first state our problem as a \"language\" in the computability\nand complexity sense. The language for our problem is\nDATARACE = { G | G is the interaction graph of an atomized\nprogram Q that contains a data race.}\nWe do not worry about the exact details of how G is encoded, as any reasonable encoding (having\nlength polynomial in the number of vertices and edges of G) suffices for the purpose of proving\nNP-completeness.\nHere is the main theorem of this chapter and its proof outline.\n1This chapter does not discuss the obvious problem of a given target program never halting, and thus the dynamic\ndata-race detector never halting.\nThis problem is shared by all data-race detectors that need to run the target\nprogram to completion. It is a well documented fact that the halting problem is undecidable (for example, see [37],\npages 172-173), and so dynamic data-race detection is indeed intractable in this sense.\n\nTheorem 8. DATARACE is NP-complete.\nProof. First, we apply Theorem 7 from the previous chapter to redescribe the set DATARACE in\na form that is easier to work with. Theorem 7 tells us that an atomized program Q contains a data\nrace if and only if it has a race assignment. Consequently, we can write\nDATARACE = { G | G is the interaction graph of an atomized\nprogram Q that has a race assignment.}\nRecall that a race assignment A is an access assignment whose assignment graph GA (which is a\nsubgraph of G) contains a thread cycle but does not contain any cycles. We shall use this definition\nof DATARACE for the remainder of this proof. We shall talk in terms of the existence of a race\nassignment instead of the existence of a data race.\nNow, there are two required conditions for DATARACE to be NP-complete.\nPart 1: DATARACE ∈ NP.\nWe shall describe a verifier V for checking that a given graph G belongs in DATARACE. If\nG ∈ DATARACE, then its program Q has a race assignment. Let A be a minimal race assignment\nof Q, by which we mean that no proper subset of A constitutes a race assignment. Furthermore,\nlet the assignment graph of A be GA. By Definition 16, GA must contain a thread cycle. Let C\nbe a shortest thread cycle in GA. We shall use this thread cycle C as the certificate for G in the\ninput to V . Note that if A is minimal, then any thread cycle in the assignment graph of A must\ncontain all the elements of A as access constraint edges, for otherwise, only those access constraint\nedges that appear in one thread cycle would constitute a race assignment smaller than A. Thus,\nA is simply the set of all access constraint edges in C, meaning that when we provide C to V , we\nare also implicitly providing A to V .\nThe thread cycle C cannot visit the same thread twice, because if it did visit some thread\ne twice, then we can obtain a shorter thread cycle by removing the portion of C from the first\noccurrence of e to the last occurrence of e, which would contradict the fact that C is a shortest\nthread cycle in GA. Consequently, the size of our certificate C is polynomial (in fact at most linear)\nin the size of G.\nThe verifier V checks whether G ∈ DATARACE using the following steps2:\n1. Extract A from C by selecting all the access constraint edges.\n2. Check that A is a subset of the possible access constraint edges, as defined by the access\ninteraction edges in G.\n3. Combine A and G to compute GA.\n4. Check that all the edges of C are contained in GA.\n5. Check that C is a thread cycle.\n6. Check that GA does not contain any cycles (for example, by running a depth-first search\nfrom the first vertex of GA).\nIf all the checks pass, then V declares G ∈ DATARACE. Note that each of these steps can be\ncompleted in polynomial time in the sizes of G and C.\nIf the interaction graph G of an atomized program Q indeed belongs in DATARACE, then by\nthe reasoning in the first paragraph, we see that there exists a certificate C which would prove to\nV that G ∈ DATARACE. On the other hand, if V accepts an input G and C, then there must\n2In our problem context, the verifier V does not need to check that G is a valid interaction graph (although this\ncan be done in polynomial time if desired), because we are only concerned with graphs G that are generated from\nthe completed execution of some atomized program Q.\n\nexist a race assignment A of Q (V actually computes it and checks that it meets the conditions for\na race assignment), and so G ∈DATARACE. This reasoning shows that V is a valid verifier for\nDATARACE.\nSince there exists a verifier that checks membership in DATARACE in polynomial time using\na polynomial-size certificate, we conclude that DATARACE ∈NP.\nPart 2: All languages in NP are polynomial-time reducible to DATARACE.\nWe shall exhibit a polynomial-time reduction from 3SAT to DATARACE. Such a reduction\nsuffices to prove that all languages in NP are polynomial-time reducible to DATARACE because\nit is already known that the same fact is true for 3SAT.\nThe mapping reduction that we use is the function F to be defined in Section 4.2. This function\ncan be computed in polynomial time in the size of the input using the steps laid out in Section 4.2.\nNow let φ be any 3cnf boolean formula, let G = F(φ), and let Q be any atomized program having\nG as its interaction graph. In order to show that F is a mapping reduction, we need to prove that\nφ has a satisfying assignment if and only if Q has a race assignment.\nForward direction. This direction is proved by Theorem 9 of Section 4.3.\nBackward direction. This direction is proved by Theorem 13 of Section 4.4.\n4.2\nReduction Function\nThis section defines a function F from the set of 3cnf boolean formulas to the set of interaction\ngraphs of atomized programs. We define F by describing a procedure for taking a 3cnf formula φ\nand constructing the corresponding interaction graph G = F(φ).\nAt various points in our construction, we shall be spawning an arbitrary number of paths\n(function instances) from a single vertex (transaction), as illustrated in Figure 4.1(a), where a\nparent vertex labeled \"P\" spawns multiple child vertices labeled \"C\".\n(a)\n(b)\nP\nC\nC\nC\nC\nP\nC\nC\nC\nC\nFigure 4.1: The meaning of having one parent vertex spawn multiple child vertices.\nHowever, Cilk allows only one function instance to be spawned at a time. Thus, when our\nconstruction calls for the behavior in Figure 4.1(a), what we shall mean is that a loop is used\n\nto spawn offone child in each iteration. The resulting graph (including the vertices that would\notherwise be hidden) is shown in Figure 4.1(b).\nNow we are ready to discuss the process for constructing the interaction graph G = F(φ) from\nthe 3cnf boolean formula φ.\nStep 1. Before looking at the formula φ, we first begin the construction of the graph G with a\nbase configuration of five vertices, as shown in Figure 4.2. (We continue our practice from previous\nchapters of labeling vertices with their transaction subscripts.) The base configuration includes the\nfirst transaction tf and last transaction tg of G. There are also two paths from tf to tg, one going\nthrough a thread with two transactions tp followed by tr, and the other going through a thread\nwith one transaction tq.\nq\ng\nr\np\nf\nFigure 4.2: The base configuration.\nLater in the construction, we shall add more vertices to G in other paths going from tf to tg.\nHowever, the thread containing tp and tr shall remain the only thread in G that consists of more\nthan one transaction.\nStep 2. Now, we look at the formula φ. Let the variables of φ be x1, . . . , xm. For each variable\nxi, we add a pair of \"variable vertices\" txi and txi to G, representing the two literals of xi.\nLet the clauses of φ be y1, . . . , yn. We shall refer to the literal positions within φ by the clause\nnumber followed by one of the letters a (first), b (second), or c (third). For example, literal position\n2c refers to the third literal position in clause y2. For each clause yj, we add three \"clause vertices\"\ntja, tjb, and tjc to G, representing the three literal positions in yj.\nExample. Consider the example 3cnf formula\nφex = (x1 ∨x2 ∨x2) ∧(x1 ∨x2 ∨x3) ∧(x1 ∨x3 ∨x3).\nSince its variables are x1, x2, and x3, the variable vertices added to Gex = F(φex) are tx1, tx1, tx2,\ntx2, tx3, and tx3. Since it has three clauses, the clause vertices added to Gex are t1a, t1b, t1c, t2a,\nt2b, t2c, t3a, t3b, and t3c. We shall continue to use this example throughout the remainder of our\nconstruction.\n♦\nStep 3. For each literal xi (or xi), we use a \"gadget\" to connect together the variable vertex txi (or\ntxi) with the clause vertices that represent the literal positions in φ that hold xi (or xi). The goal\nis to make sure that all the clause vertices appear in parallel, and that they are all direct ancestors\nof txi (or txi). We consider three cases.\nIn the general case, if a literal xi (or xi) appears in two or more literal positions in φ, then the\ngadget we use consists of a helper vertex, the variable vertex txi (or txi), and all the clause vertices\n\nthat represent literal positions that hold xi (or xi). The connections are simple. The helper vertex\nspawns offall the clause vertices in parallel, and then they are all synced at txi (or txi).\nExample. Figure 4.3 shows the gadget for the literal x2 in our example formula φex. This literal\nappears in literal positions 1b, 1c, and 2b. Therefore, in the gadget for x2, a helper vertex (unlabeled)\nspawns the clause vertices t1b, t1c, and t2b in parallel, and then these three vertices are subsequently\nsynced at tx2.\n♦\nx 2\n1b\n1c\n2b\nFigure 4.3: The gadget for x2, which demonstrates the general case.\nThere are two special cases for our gadget. If a literal xi (or xi) only appears in one literal\nposition in φ, then we cannot simply have a helper vertex that spawns the single clause vertex,\nbecause each spawn must create at least two children. Instead, we let a helper vertex spawn the\nclause vertex and a dummy vertex, which are then synced together at txi (or txi). Finally, if a\nliteral xi (or xi) does not appear in φ at all, then no helper vertex is needed, and the whole gadget\nfor xi (or xi) consists only of the single vertex txi (or txi).\nExample. Figure 4.4(a) shows the gadget for the literal x1 in our example formula φex, which only\nappears in literal position 2a. Since the literal appears in only one literal position, a dummy vertex\nis placed in parallel with the single clause vertex. The helper and dummy vertices are unlabeled.\nFigure 4.4(b) shows the gadget for the literal x2 in φex, which does not appear in any literal\npositions.\n♦\n(a)\n(b)\nx 2\nx 1\n2a\nFigure 4.4: The gadgets for x1 and x2, which demonstrate the special cases.\nStep 4. Next, we connect the gadgets for each of the literals to the base configuration from Step\n1. The connections are simple. Each gadget receives its own path from the first vertex tf to the\nlast vertex tg. Thus, all the gadgets appear in parallel in G. This step completes the serialization\nsubgraph of G.\nExample. Figure 4.5 shows the serialization subgraph of Gex = F(φex). In addition to the two\npaths from the base configuration, there is a path from tf to tg for each of the six gadgets.\n♦\n\np\nr\n1a\n2a\ng\nf\nx 2\nx 1\n3a\n1b\nx 3\n1c\n3c\n2c\nq\nx 2\nx 1\nx 3\n2b\n3b\nFigure 4.5: The serialization subgraph of Gex.\n\nStep 5. Finally, we add to G the following access interaction edges:\n1. tp is connected to both of tx1 and tx1.\n2. For each i ∈{1, . . . , m -1}, both of txi and txi are connected to both of txi+1 and txi+1.\n3. tq is connected to both of txm and txm.\n4. tq is connected to all three of tna, tnb, and tnc.\n5. For each j ∈{1, . . . , n -1}, all three of tja, tjb, and tjc are connected to all three of t(j+1)a,\nt(j+1)b, and t(j+1)c.\n6. tr is connected to all three of t1a, t1b, and t1c.\nThis completes the construction of G.\nExample. Figure 4.6 shows the access interaction edges3 in Gex = F(φex). Following the rules laid\nout in Step 5, tp is connected to both of tx1 and tx1, both of which are connected to both of tx2 and\ntx2, both of which are connected to both of tx3 and tx3, both of which are connected to tq, which\nis connected to all three of t3a, t3b, and t3c, all of which are connected to all three of t2a, t2b, and\nt2c, all of which are connected to all three of t1a, t1b, and t1c, all of which are connected to tr.\n♦\np\nr\n3a\n3b\n3c\nx 1\nx 1\nx 2\nx 2\n2c\n2b\n2a\nq\n1a\n1b\n1c\nx 3\nx 3\nFigure 4.6: The access interaction edges in Gex.\n3For clarity in this diagram, we omit the serialization edges.\n\n4.3\nForward Direction\nThis section proves the forward direction in our reduction from 3SAT to DATARACE.\nIn this section, let F be the function defined in Section 4.2, let φ be any 3cnf boolean formula,\nlet G = F(φ), and let Q be any atomized program having G as its interaction graph.\nTheorem 9. If φ has a satisfying assignment, then Q has a race assignment.\nProof. If φ has a satisfying assignment, then let α be such an assignment. We shall construct a\nrace assignment A for Q. The set A consists of all the edges in a thread cycle C (comprising only\naccess constraint edges) selected from G according to the race assignment α.\nLet the variables of φ be x1, . . . , xm, and let its clauses be y1, . . . , yn. Without reference to α,\nthe form of C is as follows. It begins at tp, then goes to either tx1 or tx1, then goes to either tx2 or\ntx2, then continues on in this fashion until it reaches either txm or txm, then goes to tq, then goes\nto one of tna, tnb, or tnc, then goes to one of t(n-1)a, t(n-1)b, or t(n-1)c, then continues on in this\nfashion until it reaches one of t1a, t1b, or t1c, then finishes by going to tr.\nNotice that for each variable xi, C goes through exactly one of txi or txi. We select which one\naccording to α as follows. If xi = false in α, then we let C go through txi; otherwise (xi = true),\nwe let C go through txi.\nSimilarly, notice that for each clause yj, C goes through exactly one of tja, tjb, or tjc. We select\nwhich one according to α as follows. Since α is a satisfying assignment, every clause in φ must be\nsatisfied under α. In particular, for clause yj, at least one of the three literals in positions ja, jb,\nand jc must be true. We let C go through a clause vertex that corresponds to a literal position\nwhose literal is true in α. If more than one literal position meets this condition, then we select\narbitrarily among them. This completes the selection of C.\nRecall that A is simply the set of all edges in C. We shall now prove that A is indeed a race\nassignment of Q. Certainly the assignment graph GA of A contains a thread cycle, because by\nconstruction it contains the thread cycle C. All that remains is to prove that GA does not contain\nany cycles.\nq\ng\nr\np\nf\nVd\nVh\nVv\nVc\nFigure 4.7: The abstracted structure of GA.\nLet Vv, Vc, Vh, and Vd be the sets of variable vertices, clause vertices, helper vertices, and\ndummy vertices in GA, respectively. Figure 4.7 shows the abstracted structure of GA, with these\n\nfour sets drawn as larger circles. An edge is drawn from one set to another if that type of edge\nexists from some member of the first set to some member of the second set. We shall use Figure\n4.7 to aid in proving that GA does not contain any cycles.\nWe shall exhaustively eliminate all possibilities for the existence of a cycle in GA. First, consider\nthe possibility of a cycle that resides completely within Vv. We note that the vertices in Vv are\nconnected only by the access constraint edges in A. Furthermore, according to the form of C, the\naccess constraint edges in Vv form a path going from either tx1 or tx1 to either txm or txm that never\nrevisits a vertex. Thus, a cycle cannot exist that resides completely in Vv. A similar argument\nshows that a cycle cannot exist that resides completely in Vc. Also, a cycle cannot exist that resides\ncompletely within Vh or Vd, because there are no edges connecting two members of either of these\ntwo sets.\nWe now know that if there is a cycle in GA, then it must traverse the edges shown in Figure\n4.7. In this graph, the only nontrivial strongly connected component is the subgraph containing\nVc, Vv, and tq, shown in Figure 4.8. Thus, this subgraph is the only possible form of a cycle in GA.\nq\nVv\nVc\nFigure 4.8: The only possible form of a cycle in GA.\nFor the purpose of deriving a contradiction, assume that a cycle does exist in the form of Figure\n4.8. Then, this cycle must contain a thread edge from a vertex tja, tjb, or tjc in Vc to a vertex txi\nor txi in Vv. Without loss of generality, assume that the edge is from a vertex tja to a vertex txi\n(the argument is the same for the other cases).\nFirst, note that for tja and txi to be part of the cycle, they both must also be incident on access\nconstraint edges in GA, which means that they both must have been vertices that we selected for\ninclusion in C. Since we selected tja for inclusion in C, the literal in position ja in φ must be true\nin α. Moreover, since a thread edge exists from tja to txi, we know from the definition of F that xi\nis in fact the literal in position ja, and so we find that xi = true in α. On the other hand, since\nwe selected txi for inclusion in C, we know that xi = false in α. This contradiction means that a\ncycle cannot exist in the form of Figure 4.8.\nWe have now eliminated all possibilities for a cycle in GA. Since GA contains the thread cycle\nC but does not contain any cycles, we conclude that A is indeed a race assignment for Q.\n4.4\nBackward Direction\nThis section proves the backward direction in our reduction from 3SAT to DATARACE.\nIn this section, let F be the function defined in Section 4.2, let φ be any 3cnf boolean formula,\nlet G = F(φ), and let Q be any atomized program having G as its interaction graph.\n\nLemma 10. If Q has a race assignment A with assignment graph GA, then any thread cycle C\nin GA must be of the form\n\n. . . , (ti-1, tr), (tp, t′\ni+1), . . .\n(r)\n. That is, C must go through the thread\ncontaining tp and tr, and furthermore it must be that tr is the end of one edge in C and tp is the\nstart of the next edge in C.\nProof. Let epr be the thread containing tp and tr. By the definition of F, every thread other than\nepr in GA consists of only a single transaction. If C does not go through epr, or if C uses only one\nof the two transactions tp and tr whenever it visits epr, then the edges of C form a (transaction)\ncycle in GA, which contradicts the fact that A is a race assignment. Thus, C must go through epr,\nand must use both transactions tp and tr. Furthermore, suppose that when C goes through epr,\nit is always the case that tp is the end of one edge in C and tr is the start of the next edge in C.\nThen, the edges of C combined with the transaction edge tp\nT→tr form a cycle in GA, which again\ncontradicts the fact that A is a race assignment. Thus, when C goes through epr, it must be that\ntr is the end of one edge in C and tp is the start of the next edge in C.\nUsing what we learned from Lemma 10, and because thread cycles can be written down with\nany edge as the starting point, we shall from now on consider any thread cycle contained in the\nassignment graph of a race assignment of Q to have the form ⟨(tp, t′\n1), . . . , (tk-1, tr)⟩. That is, we\nshall consider any such thread cycle to start at tp and end at tr.\nAlso, from now on, let Vv, Vc, Vh, and Vd denote the sets of variable vertices, clause vertices,\nhelper vertices, and dummy vertices in G, respectively. Figure 4.9 shows the abstracted structure\nof the thread and access edges in G, with these four sets drawn as larger circles. An edge is drawn\nfrom one set to another if that type of edge exists from some member of the first set to some\nmember of the second set. We shall use Figure 4.9 to aid in the following proofs.\nq\ng\nr\np\nf\nVd\nVh\nVv\nVc\nFigure 4.9: The abstracted structure of the thread and access edges in G.\nLemma 11. If Q has a race assignment A with assignment graph GA, then any shortest thread\ncycle C in GA must start at tp, then go to Vv, then go to tq, then go to Vc, then end at tr.\nProof. We make two initial observations. The first observation is that C cannot visit the same\nthread twice. The reason is that if it did visit some thread e twice, then we could obtain a shorter\n\nthread cycle by removing the portion of C between the first and last visits to e, which would\ncontradict the fact that C is a shortest thread cycle in GA.\nThe second observation is that, if at any point C visits a vertex in Vv, then it must visit tq at\na later point. We can see this by examining Figure 4.9, which shows the abstracted structure of\nthe thread and access edges in G. These are all the edges in G that can appear in a thread cycle.\nAs can be seen from this diagram, the only vertices that can be reached from Vv, without going\nthrough tq, are tp and tg. However, we know by Lemma 10 (and the ensuing paragraph) that C\nmust end at tr, so C must go through tq at a later point in order to reach tr.\nWith these two observations, we are now ready to prove that C does take the path claimed.\nPlease refer to Figure 4.9 while reading the following steps of reasoning:\n1. First, by Lemma 10 (and the ensuing paragraph), we know that C starts at tp.\n2. From tp, C can only go to Vv.\n3. From Vv, C can go to tp, tg, or tq.\na. If C goes to tp, then it visits tp twice, which is illegal by our first observation.\nb. If C goes to tg, then it hits a dead end.\nc. Thus, C must go to tq.\n4. From tq, C can go to tg, Vv, or Vc.\na. If C goes to tg, then it hits a dead end.\nb. If C goes to Vv, then by our second observation, it must later visit tq for a second time,\nwhich is illegal by our first observation.\nc. Thus, C must go to Vc.\n5. From Vc, C can go to tq, Vv, or tr.\na. If C goes to tq, then it visits tq twice, which is illegal by our first observation.\nb. If C goes to Vv, then by our second observation, it must later visit tq for a second time,\nwhich is illegal by our first observation.\nc. Thus, C must go to tr.\n6. At tr, C can continue on or C can end.\na. If C continues on, then at the end it must return to tr for a second time, which is illegal\nby our first observation.\nb. Thus, C must end.\nThis proves that C must start at tp, then go to Vv, then go to tq, then go to Vc, then end at tr.\nIn the following lemma, we draw some corollaries from Lemma 11.\nLemma 12. If Q has a race assignment A with assignment graph GA, then for any shortest thread\ncycle C in GA, the following properties hold:\n1. C is composed entirely of access constraint edges,\n2. for each variable xi, C goes through at least one of txi or txi, and\n3. for each clause yj, C goes through at least one of tja, tjb, or tjc.\nProof. For the first property, we know from Lemma 11 that C must be of the form tp →Vv →\ntq →Vc →tr. By the definition of F, there can only be access edges from tp to Vv, within Vv, from\nVv to tq, from tq to Vc, within Vc, and from Vc to tr.\nFor the second property, since C enters Vv from tp and exits to tq using only access edges, we\ncan deduce from Step 5 in the definition of F (see Figure 4.6) that C must go through at least one\nof txi or txi for each xi.\nSimilarly, for the third property, since C enters Vc from tq and exits to tr using only access\nedges, we can deduce from Step 5 in the definition of F that C must go through at least one of tja,\ntjb, or tjc for each yj.\n\nFinally, we are ready to prove the backward direction in our reduction.\nTheorem 13. If Q has a race assignment, then φ has a satisfying assignment.\nProof. If Q has a race assignment, then let A be such an assignment, let GA be the assignment\ngraph of A, and let C be a shortest thread cycle in GA. We shall construct a satisfying assignment\nα for φ using C.\nBy Lemma 12, we know that for each variable xi, C must go through at least one of txi or txi.\nIf C goes through only txi, then we set xi = false in α. If C goes through only txi, then we set\nxi = true in α. Finally, if C goes through both txi and txi, then we choose arbitrarily whether\nxi = true or xi = false. As a result of our construction of α, if a literal xi (or xi) is false in α,\nthen C must go through txi (or txi). We shall use this key observation later.\nWe need to prove that this assignment α indeed satisfies φ. By Lemma 12, we know that for\neach clause yj, C goes through at least one of tja, tjb, or tjc. I claim this clause vertex that C\ngoes through (if more than one, then select one arbitrarily) corresponds to a literal position that\nholds a literal that is true in α, and thus clause yj is satisfied under α. If this claim is true, then\nthe theorem follows, since every clause yj would then be satisfied under α, showing that α is a\nsatisfying assignment for φ.\nWe now prove the claim. Without loss of generality, say that C goes through tja (the argument\nis the same for tjb or tjc). Also without loss of generality, say that literal position ja happens\nto hold a literal xi (the argument is the same for xi). For the sake of deriving a contradiction,\nassume that xi = false in α. Then, by the key observation above, C must go through txi. Thus,\nwe know that C goes through both txi and tja. Combining this knowledge with Lemma 11, which\ntells us that the form of C must be tp →Vv →tq →Vc →tr, we find that a portion of C must\nbe a (transaction) path from txi to tja. Finally, since literal position ja holds xi, we know by the\ndefinition of F that there is a thread edge tja\nE→txi. But this edge combined with the portion of\nC that goes from txi to tja forms a cycle in GA, contradicting the fact that A is a race assignment.\nThus, our assumption that xi = false in α must be incorrect. We conclude that xi = true in α\nand that clause yj is indeed satisfied under α by literal xi in position ja.\n\nChapter 5\nAn Algorithm for Approximately\nDetecting Data Races\nWe learned in Chapter 4 that detecting data races in atomized programs is an NP-complete problem\nin general. This fact leads us to search for efficient algorithms that approximately detect data races.\nThis chapter presents one such algorithm.\nSince the complete algorithm is long, it is broken into three parts A, B, and C.\nPart A,\npresented in Section 5.1, involves instrumentation of the target atomized program for recording its\nserialization structure and shared-memory accesses. Part B, presented in Section 5.2, computes the\nleast common ancestors (LCA) structure [1, 35, 21] of the threads and finds the access interactions.\nPart C, presented in Section 5.3, uses the information prepared in the first two parts to search for\na possible data race.\nSection 5.4 states and proves an exact set of conditions under which part C of the algorithm\nreports a data race, and also proves that the algorithm never reports any false negatives. Then,\nit gives an intuitive explanation of the algorithm and discuss how the programmer should use the\nreported information about a possible data race. Finally, it combines the analyses from Sections\n5.1-5.3 to show that the total running time of the algorithm is worst-case quadratic in the size of\nthe target program's interaction graph.\n5.1\nPart A : Serialization Structure and Shared-Memory Accesses\nPart A of the algorithm involves instrumentation of the target atomized program for recording\nits serialization structure and shared-memory accesses. This section first describes the variables\nused in this part of the algorithm. Next, it provides pseudocode with text explanations. For ease\nof understanding, the pseudocode shown is slightly inaccurate, with corrections noted afterward.\nThen, this section analyzes the running time of this part of the algorithm. Finally, it gives brief\narguments about correctness.\nVariables.\nBelow is a list of all the variables used in part A, excluding local variables that are\nonly defined for a very small section of code. The variables are categorized into input, output,\nand local variables. Within each category, the variables are further divided according to the data\nthey contain. Each variable in the list is accompanied by a short text description. A description\nin paragraph form follows the list.\nIn the array lengths below, p denotes the number of function instances in the target program,\n\nm the number of threads, n the number of transactions, and L the number of shared-memory\nlocations. Correspondingly, in the algorithm, function instances are numbered 0 to p -1, threads\nnumbered 0 to m -1, transactions numbered 0 to n -1, and shared-memory locations numbered 0\nto L -1. These letters shall denote the same quantities throughout the remainder of this chapter.\nAlso, the constants true, false, and none are used throughout the algorithm. Since everything is\nnumbered starting from 0, the constant none is defined as -1. In many situations, this definition\neliminates the need to check for none as a special case.\n⋆Input variables: Not applicable.\n⋆Output variables:\n- information about threads:\nm\n▷number of threads\nedge-e1[ 0 .. m -1 ]\n▷first thread constraint\nedge-e2[ 0 .. m -1 ]\n▷second thread constraint\n- information about transactions:\nn\n▷number of transactions\nthr[ 0 .. n -1 ]\n▷enclosing thread\nreads[ 0 .. n -1 ]\n▷list of memory locations read\nwrites[ 0 .. n -1 ]\n▷list of memory locations written\n- information about shared-memory locations:\nreaders[ 0 .. L -1 ]\n▷list of transactions that read a location\nwriters[ 0 .. L -1 ]\n▷list of transactions that write a location\n⋆Local variables:\n- information about function instances:\npnew\n▷next unused function instance index\nparent[ 0 .. p -1 ]\n▷parent function instance\nspawned[ 0 .. p -1 ]\n▷list of spawned function instances\nnext-tran[ 0 .. p -1 ]\n▷next transaction to be executed (defined when executing a\n▷spawned child to remember where to return to)\nfinal-thr[ 0 .. p -1 ]\n▷final thread (only defined when done)\n- information about threads:\nmnew\n▷next unused thread index\n- information about transactions:\nnnew\n▷next unused transaction index\nfun[ 0 .. n -1 ]\n▷enclosing function instance\n- information about the current transaction:\ntcurr\n▷currently executing transaction\nis-new-thread\n▷true if current transaction is start of a new thread\nreads-temp\n▷list of memory locations read by tcurr (repeats allowed)\nwrites-temp\n▷list of memory locations written by tcurr (repeats allowed)\n\nSince part A is the first part of the algorithm, there are no input variables from earlier parts.\nThe output variables that are recorded by part A for use in parts B and C are as follows.\nFirst, part A keeps counts of the number of threads and the number of transactions in the\nprogram, and outputs them in the variables m and n, respectively. An array thr records transaction\nmembership in threads, so that for each transaction t, thr[t] is the index of the thread that t belongs\nto. Note that t denotes the transaction index.\nThe serialization graph is recorded as follows. The outgoing thread edges are stored in two\narrays edge-e1 and edge-e2. Two arrays suffice because any thread can have at most two outgoing\nthread edges. For example, if a thread e has exactly one outgoing thread edge to thread e1, then\nedge-e1[e] = e1 and edge-e2[e] = none.\nThe transaction edges are stored implicitly.\nBy the\nalgorithm's method of numbering transactions, an earlier transaction t1 in a thread always receives\na smaller index than a later transaction t2 in the same thread. Thus, we know there is a path\nfrom t1 to t2 consisting of transaction edges if the transactions belong to the same thread (i.e.\nthr[t1] = thr[t2]) and t1 < t2.\nTwo arrays readers and writers are kept for recording which transactions read or write a loca-\ntion. Each element writers[l] is the head of a linked list of indices of transactions that write l. The\narray readers is similarly defined, except that if a transaction both reads and writes a location l,\nthen the transaction is only recorded in writers[l].\nThe arrays reads and writes are the dual of the arrays readers and writers.\nEach element\nwrites[t] is the head of a linked list of shared-memory locations that are written by transaction\nt.\nThe array reads is similarly defined, except that if a transaction t both reads and writes a\nshared-memory location, then the location is only recorded in writes[t].\nThe variables local to part A are as follows.\nThree counters pnew, mnew, and nnew are employed to keep track of the next unused function\ninstance index, thread index, and transaction index, respectively, for the purpose of allocating new\nindices. Variables mnew and nnew also serve the additional purpose of counting the numbers of\nthreads and transactions, respectively, so that these counts can be output in variables m and n.\nAn array fun records transaction membership in function instances, so that for each transaction t,\nfun[t] is the index of the function instance that t belongs to.\nFor each function instance f, we keep track of the following information. The value parent[f]\nis the index of the parent function instance that spawned f, with parent[f] = none if f is the\nmain function instance. The value spawned[f] is the head of a linked list of function instances that\nwere spawned by f and have not yet been synced. The value next-tran[f] is used to remember the\nindex of the next transaction to be executed after a spawn point in f, during the execution of that\nspawned function instance (and its descendants). Finally, when f returns, final-thr[f] records the\nindex of the final thread in f.\nThroughout part A of the algorithm, tcurr is always set to the index of the currently executing\ntransaction. The boolean variable is-new-thread is used to indicate whether the current transaction\nis the first transaction in a new thread. The linked lists reads-temp and writes-temp record the\nshared-memory locations that have been read and written by tcurr so far, with repeats allowed.\nThus, each time that a shared-memory location lis read or written by tcurr, lis inserted into\nreads-temp or writes-temp, respectively.\nPseudocode and Explanation.\nIn the instrumentation of the target program, we need to add\ncode that runs during the start of the program, during function spawn, return, and sync points,\nduring transaction begin points, during accesses to shared-memory locations, and during transac-\ntion end points. The pseudocode for each situation is provided below. Although the pseudocode is\n\nreasonably self-explanatory, a text explanation follows each piece of pseudocode for completeness.\nThe instrumented program is to be run in serial fashion on one processor, so that the transactions\nand threads are executed in the order that they would appear in the serial elision of the target\nprogram. Specifically, whenever a function instance fparent spawns a function instance fchild, the\nspawned instance fchild always executes first.\nOnly when fchild is completely finished and has\nreturned does the next transaction after the spawn point in fparent execute.\nProgram Start:\npnew ←0\nmnew ←0\nnnew ←0\nfor l←0 to L -1\nList-Empty( readers[l] )\nList-Empty( writers[l] )\ntcurr ←nnew ++\nthr[tcurr] ←mnew ++\nfun[tcurr] ←pnew ++\nList-Empty( spawned[ fun[tcurr] ] )\nparent[ fun[tcurr] ] ←none\nis-new-thread ←true\nThe above code is executed at the start of the target program. Lines 1-3 initialize the counters\npnew, mnew, and nnew. Lines 4-6 initialize the arrays readers and writers, so that all the lists are\ninitially empty. Lines 7-9 allocate indices for the current (first) transaction tcurr and its associated\nthread and function instance. (For any variable v, the notation v++ is used to indicate that v is\nto be incremented at the end of the current statement.) Line 10 initializes the spawned list of the\ncurrent function instance fun[tcurr], which is something that needs to be done at the start of every\nnew function instance. Line 11 sets the parent of the current function instance fun[tcurr] to none,\nbecause fun[tcurr] is the main function instance of the program. Finally, line 12 sets is-new-thread\nto true because a new thread is starting.\nFunction Spawn:\ntnext ←nnew ++\nthr[tnext] ←mnew ++\nfun[tnext] ←fun[tcurr]\ntchild ←nnew ++\nthr[tchild] ←mnew ++\nfun[tchild] ←pnew ++\nList-Empty( spawned[ fun[tchild] ] )\nedge-e1[ thr[tcurr] ] ←thr[tnext]\nedge-e2[ thr[tcurr] ] ←thr[tchild]\n\nparent[ fun[tchild] ] ←fun[tcurr]\nList-Insert( spawned[ fun[tcurr] ], fun[tchild] )\nnext-tran[ fun[tcurr] ] ←tnext\ntcurr ←tchild\nis-new-thread ←true\nThe above code is executed at function spawn points. Lines 1-3 create the next transaction\ntnext following the spawn point and its associated thread. The function instance of tnext is the same\nas the current function instance fun[tcurr]. Lines 4-6 create the first transaction tchild of the child\nfunction instance, and also its associated thread and function instance. Since the child function\ninstance fun[tchild] is new, line 7 initializes its spawned list. Lines 8-9 record the thread edges from\nthe current thread thr[tcurr] (before the spawn point) to the two new threads created, one being the\nnext thread thr[tnext] in the current function instance and the other being the first thread thr[tchild]\nof the child function instance. Line 10 sets the current function instance fun[tcurr] to be the parent\nof the child function instance fun[tchild], and line 11 reciprocally adds the child function instance to\nthe spawned list of the current function instance. Line 12 saves the index of the next transaction\ntnext to be executed in the current function instance, so that we know where to return to when the\nchild function instance is finished. Finally, line 13 changes the value of tcurr to the index of the\nfirst transaction tchild in the child function instance, in preparation for its execution. Line 14 notes\nthat we are beginning a new thread.\nFunction Return:\nfinal-thr[ fun[tcurr] ] ←thr[tcurr]\nif parent[ fun[tcurr] ] = none\nedge-e1[ thr[tcurr] ] ←none\nedge-e2[ thr[tcurr] ] ←none\nm ←mnew\nn ←nnew\nelse\ntcurr ←next-tran[ parent[ fun[tcurr] ] ]\nis-new-thread ←true\nThe above code is executed when a function instance returns. Since we are at the end of the\ncurrent function instance, line 1 records the final thread of the current function instance fun[tcurr]\nto be the current thread thr[tcurr]. Line 2 tests to see if the current function instance is the main\nfunction instance of the program (only the main function instance has its parent equal to none).\nIf so, then we are finished running the target program. To wrap up, lines 3-4 set the outgoing\nthread edges of the current thread thr[tcurr] (which is the last thread of the program) to none,\nand lines 5-6 set the output variables m and n to the final values of the counters mnew and nnew,\nrespectively. Else, if we are not at the end of the main function instance, then line 8 changes the\nvalue of tcurr to the index of the next transaction in the parent function instance parent[ fun[tcurr] ],\nin preparation for its execution. Recall that the setup operations for the next transaction in the\nparent function instance were done when the current function instance was spawned. Finally, line\n9 notes that we are beginning a new thread.\n\nFunction Sync:\ntnext ←nnew ++\nthr[tnext] ←mnew ++\nfun[tnext] ←fun[tcurr]\nedge-e1[ thr[tcurr] ] ←thr[tnext]\nedge-e2[ thr[tcurr] ] ←none\nfor each fchild ∈spawned[ fun[tcurr] ]\nedge-e1[ final-thr[fchild] ] ←thr[tnext]\nedge-e2[ final-thr[fchild] ] ←none\nList-Empty( spawned[ fun[tcurr] ] )\ntcurr ←tnext\nis-new-thread ←true\nThe above code is executed at function sync points. Lines 1-3 create the next transaction tnext\nfollowing the sync point and its associated thread. The function instance of tnext is the same as\nthe current function instance fun[tcurr]. Then, the task is to record thread edges for all the threads\nthat end at this sync point. These include the current thread, as well as the final threads of all\nthe child function instances that were previously spawned by the current function instance, but\nwhich have not yet been synced. Line 4 records the outgoing thread edge from the current thread\nthr[tcurr] to the next thread thr[tnext], and line 5 notes that the current thread does not have a\nsecond outgoing thread edge. Lines 6-8 do the same for the final threads final-thr[fchild] of each\npreviously spawned child function instance fchild. Line 9 empties the spawned list of the current\nfunction instance fun[tcurr], because all previously spawned child function instances have now been\nsynced. Finally, line 10 changes the value of tcurr to the index of the next transaction tnext in\npreparation for its execution, and line 11 notes that we are beginning a new thread.\nTransaction Begin:\nif is-new-thread = true\nis-new-thread ←false\nelse\ntprev ←tcurr\ntcurr ←nnew ++\nthr[tcurr] ←thr[tprev]\nfun[tcurr] ←fun[tprev]\nList-Empty( reads-temp )\nList-Empty( writes-temp )\nThe above code is executed at the beginning of a transaction. Line 1 checks to see if the current\ntransaction is the start of a new thread. If so, then the setup operations for creating the current\ntransaction have already been done, in which case line 2 resets is-new-thread to false. Else, if the\ncurrent transaction is not the start of a new thread, then the current transaction has not yet been\ncreated, and tcurr actually holds the index of the previous transaction. In this case, line 4 saves\nthe index of the previous transaction as tprev, and lines 5-7 create the current transaction tcurr\n\nand set its thread and function instance to be the same as those of tprev. Lines 8-9 empty the lists\nreads-temp and writes-temp in preparation for recording the shared-memory accesses made by the\ncurrent transaction tcurr.\nRead Location l:\nList-Insert( reads-temp, l)\nWrite Location l:\nList-Insert( writes-temp, l)\nOne of the two lines of code above is executed when a shared-memory location is read or written\nby the current transaction, to record the access in reads-temp or writes-temp, respectively. Recall\nthat repeats are allowed in reads-temp and writes-temp.\nTransaction End:\nList-Empty( reads[tcurr] )\nList-Empty( writes[tcurr] )\nfor l←0 to L -1\nprocessed[l] ←false\nfor each l∈writes-temp\nif processed[l] = false\nList-Insert( writes[tcurr], l)\nList-Insert( writers[l], tcurr )\nprocessed[l] ←true\nfor each l∈reads-temp\nif processed[l] = false\nList-Insert( reads[tcurr], l)\nList-Insert( readers[l], tcurr )\nprocessed[l] ←true\nThe above code is executed when a transaction ends. Lines 1-2 initialize the reads and writes\nlists for the current transaction. Lines 3-4 initialize a temporary array processed[ 0 .. L -1 ] of\nboolean values, which are used to keep track of which shared-memory locations lhave already\nhad accesses transferred from reads-temp and writes-temp (which allow repeats) to reads, writes,\nreaders, and writers (which do not allow repeats). Line 5 iterates through writes-temp, which\nstores all the writes to shared memory that were made by the current transaction tcurr, and if a\nlocation lhas not yet been processed, then lines 7-8 insert linto the writes list of tcurr, and also\ninsert tcurr into the writers list of l. Lines 10-14 accomplish the same task as lines 5-9, but for the\ncurrent transaction's reads to shared memory. Note that the array processed is not cleared between\nlines 5-9 and lines 10-14, so that if the current transaction both reads and writes a shared-memory\nlocation l, then only a write would be recorded.\nNotes. Now some notes are in order. The pseudocode presented above for part A of the algorithm\nis actually not completely accurate. It was presented in the above form for ease of explanation\n\nand understanding. Two changes need to be made, however, in order to make part A correct and\nefficient.\nFirst, note that the values p, m, and n are not available at the beginning of the algorithm, so\narrays with these sizes cannot actually be allocated. There are two simple solutions to this dilemma.\nOne solution is to run the target program once for the purpose of determining these values, and\nthen to allocate the arrays and run the instrumented program again as described above. The other\nsolution is simply to use dynamic arrays (for example, see [6], pages 416-425).\nSecond, note that the number of shared-memory locations L is likely a large number, possibly\nmuch larger than the number of shared-memory locations that are actually used. In order not to\nhave the running time and space usage depend on L, we can implement the arrays readers, writers,\nand processed as dynamic perfect hash tables [7]. This change replaces lines 4-6 in the pseudocode\nfor \"Program Start\" and lines 3-4 in the pseudocode for \"Transaction End\" with constant-\ntime hash table initialization statements. This change also makes the running time both amortized\nand expected.\nRunning Time.\nLet us analyze the running time for each piece of pseudocode in turn, and\nthen combine the results. Recall that p is the number of function instances, m is the number of\nthreads, and n is the number of transactions. Also, let us define α to be the number of accesses to\nshared-memory locations.\n- Program Start -- This piece of code executes only once.\nIf readers and writers are\nimplemented as dynamic perfect hash tables, as mentioned in the notes above, then all the\noperations in this piece of code take constant time. Thus, this code uses a total of Θ(1) time.\n- Function Spawn -- This piece of code executes a total of Θ(p) times. The operations in\nthis piece of code all take constant time. Thus, this piece of code uses a total of Θ(p) time.\n- Function Return -- This piece of code executes a total of Θ(p) times. The operations in\nthis piece of code all take constant time. Thus, this piece of code uses a total of Θ(p) time.\n- Function Sync -- Note that there is no requirement for each function sync be associated\nwith one or more function spawns1. As a result, we can only say that this piece of code\nexecutes O(m) times. The only portion of this code that does not take constant time is the\nfor loop at line 6. Over all the times that this piece of code executes, the for loop at line 6\niterates a total of Θ(p) times, since each spawned function instance is synced exactly once.\nThe code inside the for loop takes constant time, so all the iterations take a total of Θ(p)\ntime. Thus, this piece of code uses a total of O(m) + Θ(p) = O(m) time.\n- Transaction Begin -- This piece of code executes Θ(n) times. The operations in this piece\nof code all take constant time. Thus, this piece of code uses a total of Θ(n) time.\n- Read Location and Write Location -- One of these two pieces of code is executed for\neach shared-memory access. Thus, these pieces of code use a total of Θ(α) time.\n- Transaction End -- This piece of code executes Θ(n) times. Over all the times that this\npiece of code executes, the for loops at lines 5 and 10 iterate a total of Θ(α) times, since\nreads-temp and writes-temp simply record all the accesses to shared-memory locations. All\nthe operations outside and inside the for loops take constant time (remember that lines 3-4\nnow take constant time since processed is implemented as a dynamic perfect hash table).\nThus, this piece of code uses a total of Θ(n + α) time.\nFinally, we must not forget that in addition to the execution of our code, the original target program\nmust also execute. Let τ be the running time of the uninstrumented target program. Since p, m,\n1If every sync were to be associated with at least one spawn, then it would be straightforward to prove that\np ≤m < 3p, which would imply that m = Θ(p). However, we do not make this assumption.\n\nn, and α are all O(τ), the total running time for all the code inserted by part A is O(τ). Thus, the\ntotal running time for part A of the algorithm is simply O(τ) + τ = Θ(τ).\nCorrectness.\nWe shall argue that all the output variables in part A have correct values. We\nshall be brief, because the pseudocode in this part of the algorithm is fairly self-explanatory.\nFirst, mnew and nnew are initialized to 0 and incremented each time a new thread or transaction\nis created, respectively, so they are correct counts for the numbers of threads and transactions,\nrespectively. At the end, their values are copied into m and n, so m and n are correct.\nThe values in thr are correct because whenever the current transaction tcurr is the start of a\nnew thread, we allocate a new thread index for thr[tcurr], and whenever the current transaction is\na new transaction within the same thread, then we simply copy the thr value from the previous\ntransaction (line 6 in the code for \"Transaction Begin\").\nFor edge-e1 and edge-e2, we consider four exhaustive cases. A thread e can only end at a spawn\npoint, a sync point, the return point of a spawned function instance, or the return point of the\nmain function instance. If e ends at a spawn point, then lines 8-9 in the code for \"Function\nSpawn\" correctly set edge-e1[e] and edge-e2[e]. If e ends at a sync point, then lines 4-5 in the\ncode for \"Function Sync\" correctly set edge-e1[e] and edge-e2[e]. If e ends at the return point\nof a spawned function instance, then that function instance must have an associated sync point\nin its parent function instance. Thus, lines 7-8 in the code for \"Function Sync\" correctly set\nedge-e1[e] and edge-e2[e]. Finally, if e ends at the return point of the main function instance, then\nlines 3-4 in the code for \"Function Return\" correctly set edge-e1[e] and edge-e2[e].\nWhenever an access is made to shared memory, it is recorded into reads-temp or writes-temp by\nthe code for \"Read Location\" and \"Write Location\". Then, at the end of a transaction, this\ninformation is transferred to reads, writes, readers, and writers, with no repeats because of the use\nof processed (lines 5-14 in the code for \"Transaction End\"). Also, reads-temp and writes-temp\nare cleared at the beginning of every transaction (lines 8-9 in the code for \"Transaction Begin\"),\nso accesses in one transaction do not get mixed with accesses in other transactions. Thus, the arrays\nreads, writes, readers, and writers hold the correct data at the end of part A of the algorithm.\n5.2\nPart B : LCA and Access Interactions\nPart B of the algorithm uses the information recorded in part A to compute the least common\nancestors (LCA) structure of the target program and the access interaction edges. Following the\nsame presentation plan as the previous section, this section first describes the variables used in\nthis part of the algorithm. Next, it provides pseudocode followed by a text explanation. Then, it\nanalyzes the running time. Finally, it gives brief arguments about correctness.\nVariables.\nBelow are the variables used in part B of the algorithm, excluding local dummy\nvariables. A description in paragraph form follows.\n⋆Input variables:\n- information about threads:\nm\n▷number of threads\nedge-e1[ 0 .. m -1 ]\n▷first thread constraint\nedge-e2[ 0 .. m -1 ]\n▷second thread constraint\n\n- information about transactions:\nn\n▷number of transactions\nthr[ 0 .. n -1 ]\n▷enclosing thread\nreads[ 0 .. n -1 ]\n▷list of memory locations read\nwrites[ 0 .. n -1 ]\n▷list of memory locations written\n- information about shared-memory locations:\nreaders[ 0 .. L -1 ]\n▷list of transactions that read a location\nwriters[ 0 .. L -1 ]\n▷list of transactions that write a location\n⋆Output variables:\n- information about threads:\nLCA data structure\n▷data structure used by the LCA algorithm\nthr-edges-a[ 0 .. m -1 ]\n▷adjacency list of threads with which this thread has an\n▷access interaction; the value is the last transaction in a\n▷give thread with which some transaction in this thread\n▷has an access interaction\n- information about transactions:\ntran-edges-a[ 0 .. n -1 ]\n▷adjacency list of threads with whose transactions this\n▷transaction has an access interaction\n⋆Local variables:\n- information about threads:\nthr-edges-a-table[ 0 .. m -1 ][ 0 .. m -1 ]\n▷adjacency matrix of access interactions;\n▷the value is the last transaction in the\n▷second thread that shares an access\n▷interaction with the first thread\n- information about transactions:\ntran-edges-a-table[ 0 .. n -1 ][ 0 .. m -1 ]\n▷adjacency matrix of access interactions;\n▷the value is true if the first transaction\n▷has an access interaction with some\n▷transaction in the second thread\nThe input variables to part B of the algorithm are simply all the output variables from part A,\nwhich have already been described in the previous section. The output variables of part B are as\nfollows.\nOne output item is a data structure containing the LCA information for the threads in the\ntarget program. Many algorithms have been devised that use linear time to precompute an LCA\ndata structure which allows finding the LCA of any two vertices in constant time [1, 35, 21]. For\nconcreteness, we shall assume that we are using the algorithm in [1].\nAlso, we output two arrays of adjacency lists of access interaction edges, in forms that are the\nmost convenient for part C of the algorithm. The array tran-edges-a holds adjacency lists from\ntransactions to threads. For each transaction t, tran-edges-a[t] is a linked list of all the threads\nwith which t has access interactions. That is, a thread e1 ∈tran-edges-a[t] if and only if e1 contains\na transaction t1 such that t\nA↔t1.\n\nThe array thr-edges-a holds adjacency lists from threads to threads, but with a slight twist.\nFor any two threads e and e1, if there is an access interaction between some transaction in e and\nsome transaction in e1, then the last transaction in e1 that has an access interaction with some\ntransaction in e appears in thr-edges-a[e].\nIn other words, let t1 be a transaction in e1; then\nt1 ∈thr-edges-a[e] if and only if t1 has an access interaction with some transaction in e, but no\nlater transaction in e1 has an access interaction with any transaction in e.\nFinally, the local variables tran-edges-a-table and thr-edges-a-table are adjacency matrices with\nthe same information as in tran-edges-a and thr-edges-a, respectively. Part B of the algorithm uses\nthese local variables as intermediate steps in computing tran-edges-a and thr-edges-a.\nPseudocode and Explanation.\nIn part B of the algorithm, we first precompute the LCA\ndata structure. Then, we find the access interactions and store them into the adjacency matrices\ntran-edges-a-table and thr-edges-a-table. Finally, we convert the information in tran-edges-a-table\nand thr-edges-a-table into the arrays tran-edges-a and thr-edges-a of adjacency lists. The pseu-\ndocode is provided below, and the text explanation follows.\nCompute-LCA-and-Edges-a():\nLCA-Precompute( edge-e1, edge-e2, m )\nfor ti ←0 to n -1\nfor ej ←0 to m -1\ntran-edges-a-table[ti][ej] ←false\nfor ei ←0 to m -1\nfor ej ←0 to m -1\nthr-edges-a-table[ei][ej] ←none\nfor ti ←0 to n -1\nfor each l∈writes[ti]\nfor each tj ∈writers[l]\nCheck-For-Edge-a( ti, tj )\nfor each tj ∈readers[l]\nCheck-For-Edge-a( ti, tj )\nfor each l∈reads[ti]\nfor each tj ∈writers[l]\nCheck-For-Edge-a( ti, tj )\nfor ti ←0 to n -1\nList-Empty( tran-edges-a[ti] )\nfor ej ←0 to m -1\nif tran-edges-a-table[ti][ej] = true\nList-Insert( tran-edges-a[ti], ej )\nfor ei ←0 to m -1\nList-Empty( thr-edges-a[ei] )\nfor ej ←0 to m -1\nif thr-edges-a-table[ei][ej] = none\nList-Insert( thr-edges-a[ei], thr-edges-a-table[ei][ej] )\n\nCheck-For-Edge-a( ti, tj ):\nif ti < tj and LCA-Find( thr[ti], thr[tj] ) ∈{thr[ti], thr[tj]}\ntran-edges-a-table[ti][ thr[tj] ] ←true\ntran-edges-a-table[tj][ thr[ti] ] ←true\nif thr-edges-a-table[ thr[ti] ][ thr[tj] ] < tj\nthr-edges-a-table[ thr[ti] ][ thr[tj] ] ←tj\nif thr-edges-a-table[ thr[tj] ][ thr[ti] ] < ti\nthr-edges-a-table[ thr[tj] ][ thr[ti] ] ←ti\nIn the above pseudocode, Compute-LCA-and-Edges-a is the main function, and the sub-\nroutine Check-For-Edge-a has been abstracted out simply because it needs to be done in\nthree places. In the main code, line 1 precomputes the LCA data structure. Lines 2-4 initial-\nize tran-edges-a-table and lines 5-7 initialize thr-edges-a-table.\nLines 8-16 perform the task of finding the access interactions and putting this information into\ntran-edges-a-table and thr-edges-a-table. Line 8 iterates through all the transactions ti. For each\nti, we need to find all the transactions tj with which ti has an access interaction. Transactions ti\nand tj only share an access interaction if ti writes a location l(line 9) and tj either writes l(line\n10) or reads l(line 12), or else if ti reads a location l(line 14) and tj writes l(line 15). In each of\nthese cases, we call the subroutine Check-For-Edge-a to make two additional checks.\nIn the subroutine, line 1 checks that ti < tj, which eliminates redundancy, and line 1 also checks\nthat the LCA of the threads to which ti and tj belong is not equal to either of those threads, which\nmakes sure that ti and tj appear in parallel in the serial control flow of the target program. If these\nchecks pass, then there is an access interaction between ti and tj, so lines 2-3 record this information\ninto tran-edges-a-table, and lines 4-7 record this information into thr-edges-a-table. Note that the\nchecks in lines 4 and 6 ensure that when all the access interactions have been processed, only the\nlatest transactions that represent the end of an access interaction between two threads are recorded\ninto thr-edges-a-table.\nBack in Compute-LCA-and-Edges-a, lines 17-21 move information from tran-edges-a-table\nto tran-edges-a. Line 17 iterates through all the transactions ti. For each ti, line 18 initializes its\nadjacency list tran-edges-a[ti], and lines 19-21 iterate through all the threads and insert the appro-\npriate threads into tran-edges-a[ti]. Similarly, lines 22-26 move information from thr-edges-a-table\nto thr-edges-a. Line 22 iterates through all the threads ei. For each ei, line 23 initializes its adja-\ncency list thr-edges-a[ei], and lines 24-26 iterate through all the threads and insert the appropriate\ntransactions in the appropriate threads into thr-edges-a[ei].\nRunning Time.\nIn Compute-LCA-and-Edges-a, line 1 requires Θ(m) time using the algo-\nrithm in [1]. Lines 2-4 use Θ(nm) time and lines 5-7 use Θ(m2) time. Now we consider lines 8-16.\nSince the reads and writes lists do no have any repeats, every time either of the for loops at lines 9\nand 14 iterates, it must be with a different value of ti or l. This shows that these two for loops iter-\nate a total of O(α) times (recall that α is the total number of accesses to shared-memory locations).\nWithin each iteration of the loop at line 9, we know that the readers and writers lists do not have\nany repeats, so that the for loops at lines 10 and 12 iterate a total of O(n) times. Similarly, within\neach iteration of the loop at line 14, the for loop at line 15 iterates a total of O(n) times. Together,\nwe find that the subroutine Check-For-Edge-a is called a grand total of O(α) · O(n) = O(nα)\ntimes. The operations in the subroutine all take constant time per call, so the total time required\n\nfor lines 8-16 (remembering that line 8 must iterate Θ(n) times) is Θ(n)+O(nα) = O(nα). Finally,\nlines 17-21 use Θ(nm) time and lines 22-26 use Θ(m2) time.\nAdding up all the sections of code, we find that part B of the algorithm uses a total running\ntime of Θ(m) + Θ(nm) + Θ(m2) + O(nα) + Θ(nm) + Θ(m2) = O(nα + nm).\nCorrectness.\nWe shall argue briefly that the output variables for part B of the algorithm have\ncorrect values at the end of part B.\nThe LCA data structure we use comes from [1], and its\ncorrectness is proved in that paper.\nFor tran-edges-a, assume t1 belongs to e1 and t\nA↔t1. We shall show that e1 ∈tran-edges-a[t].\nFirst, we examine what happens in lines 8-16 of Compute-LCA-and-Edges-a. By Definition 8,\nt and t1 are in parallel, and both t and t1 access some shared-memory location l∗, with at least\none of them writing l∗. In each of the three cases determined by which of t and t1 write l∗, it is\nclear that the subroutine Check-For-Edge-a is called in one of lines 11, 13, or 16 with ti and\ntj equal to the smaller and larger, respectively, of t and t1. Within the subroutine, the checks in\nline 1 pass, and one of lines 2 or 3 sets tran-edges-a-table[t][e1] to true. Later, in lines 17-21 of\nCompute-LCA-and-Edges-a, there is an iteration of the inner loop with ti and ej equal t and\ne1, at which point the check in line 20 passes and line 21 adds e1 to tran-edges-a[t], as desired.\nIn the reverse direction, assume that at the end of part B, e1 ∈tran-edges-a[t]. Then e1 could\nonly have been added to tran-edges-a[t] in line 21 of Compute-LCA-and-Edges-a, which means\nthe check in line 20 must have passed, so that tran-edges-a-table[t][e1] = true. This, in turn, could\nonly have been set by one of lines 2 or 3 in the subroutine Check-For-Edge-a. This subroutine\nwas called in one of lines 11, 13, or 16 of Compute-LCA-and-Edges-a, and in each case, there\nmust exist some t1 such that thr[t1] = e1, t and t1 are in parallel, and both t and t1 access the\nsame memory location l∗, with at least one of them writing l∗. Thus, the conditions are satisfied\nfor the existence of an access interaction t\nA↔t1 between t and some transaction in e1.\nThe argument for the correctness of thr-edges-a is almost the same as that for tran-edges-a, so\nwe shall not write it out. We only note the single major difference. For threads e and e1, lines\n4-7 in the subroutine Check-For-Edge-a may be executed many times, once for each access\ninteraction edge between some transaction in e and some transaction in e1. The checks in lines 4\nand 6 of the subroutine ensure that, at the end of part B, thr-edges-a-table[e][e1] contains the last\ntransaction in e1 that shares an access interaction with some transaction in e.\n5.3\nPart C : Searching for a Possible Data Race\nPart C of the algorithm uses the data gathered and computed in the first two parts to search\nfor a possible data race. Following the same presentation plan as the previous two sections, this\nsection first describes the variables used in this part of the algorithm. Then, it provides pseudocode\nfollowed by a text explanation. Finally, it analyzes the running time. Arguments about correctness\nare given in Section 5.4.\nVariables.\nBelow are the variables used in part C of the algorithm, excluding local dummy\nvariables. A description in paragraph form follows.\n⋆Input variables:\n- information about threads:\nm\n▷number of threads\n\nLCA data structure\n▷data structure used by the LCA algorithm\nedge-e1[ 0 .. m -1 ]\n▷first thread constraint\nedge-e2[ 0 .. m -1 ]\n▷second thread constraint\nthr-edges-a[ 0 .. m -1 ]\n▷adjacency list of threads with which this thread has an\n▷access interaction; the value is the last transaction in a\n▷give thread with which some transaction in this thread\n▷has an access interaction\n- information about transactions:\nn\n▷number of transactions\nthr[ 0 .. n -1 ]\n▷enclosing thread\ntran-edges-a[ 0 .. n -1 ]\n▷adjacency list of threads with whose transactions this\n▷transaction has an access interaction\n⋆Output variables: Not applicable.\n⋆Local variables:\n- information for breadth-first search:\nneed-visit\n▷queue of threads that need visiting\ndiscovered[ 0 .. m -1 ]\n▷keeps track of threads that have already been discovered\nThe input variables to part C of the algorithm are output variables from parts A and B, so they\nhave already been described in the previous two sections. There are no output variables since this\nis the final part of the algorithm.\nThe local variable need-visit is a queue used to keep track of threads that have been discovered\nbut not yet visited in a breadth-first search of the threads in the interaction graph. The array\ndiscovered is used to record which threads have previously been discovered and enqueued into\nneed-visit, so as to prevent visiting the same thread multiple times during the search.\nAlso, as shown below, part C of the algorithm returns a pair (ti, tj) of transactions when it finds\na possible data race. Thus, we define a new constant no-race-found = (none, none) for the\nreturn value when no possible data race is found.\nPseudocode and Explanation.\nPart C of the algorithm runs n breath-first searches on the\nthreads of the target program, starting from each of the n transactions.\nIn each breadth-first\nsearch, the goal is to start from a transaction ts and find a thread cycle that leads back to a later\ntransaction tj in the same thread as ts. Moreover, the threads in the cycle must all be in parallel\nwith the starting thread. The pseudocode is provided below, and the text explanation follows.\nFind-Possible-Data-Race():\nfor ts ←0 to n -1\nfor ei ←0 to m -1\ndiscovered[ei] ←false\ndiscovered[ thr[ts] ] ←true\n\nQueue-Empty( need-visit )\nfor each ei ∈tran-edges-a[ts]\ndiscovered[ei] ←true\nQueue-Enq( need-visit, ei )\nwhile Queue-Is-Empty( need-visit ) = false\nei ←Queue-Deq( need-visit )\nif LCA-Find( ei, thr[ts] ) ∈{ei, thr[ts]}\nfor each tj ∈thr-edges-a[ei]\nif thr[ts] = thr[tj] and ts < tj\nreturn (ts, tj)\nif discovered[ thr[tj] ] = false\ndiscovered[ thr[tj] ] ←true\nQueue-Enq( need-visit, thr[tj] )\nfor each ej ∈{edge-e1[ei], edge-e2[ei]}\nif discovered[ej] = false\ndiscovered[ej] ←true\nQueue-Enq( need-visit, ej )\nreturn no-race-found\nThe above pseudocode works as follows. The loop at line 1 iterates through all transactions\nts. Within the loop, in lines 2-21, ts becomes the starting point for a breadth-first search on the\nthreads of the target program. Lines 2-3 initialize the array discovered used to keep track of which\nthreads have been discovered in the search, and line 4 marks the thread thr[ts] of the starting\ntransaction as having been discovered. Line 5 initializes the queue need-visit used to keep track of\nthreads that have been discovered but not yet visited. Lines 6-8 mark all the threads ei that can\nbe reached from the starting transaction ts via access interaction edges as having been discovered\nand enqueues them into need-visit to be processed during the search. (We do not worry about\nthread edges here because we want all the threads that we search through to be in parallel with\nthe starting thread.)\nThe setup for the breath-first search is now complete. Lines 9-21 repeatedly take threads off\nthe queue and process them. The while loop at line 9 checks if there are still threads that need\nvisiting. If so, then line 10 dequeues the next thread ei to be processed. Line 11 checks to make\nsure that ei is in parallel with the starting thread thr[ts].\nAt this point, lines 12-17 expand the breath-first search along the access interaction edges\nadjacent to ei, while lines 18-21 expand the search along thread edges outgoing from ei. The for\nloop at line 12 iterates through all the threads that can be reached from ei via access constraint\nedges. For each such thread thr[tj], line 13 checks to see if it is the starting thread thr[ts], and if so,\nwhether a thread cycle can be completed that ends at a transaction tj which appears later in the\nstarting thread than the starting transaction ts. If such a thread cycle can be made, then a possible\ndata race is reported, in the form of the pair of transactions (ts, tj). Also for each thread thr[tj],\nline 15 checks whether it has previously been discovered, and if not, lines 16-18 mark it as having\nbeen discovered and enqueue it into need-visit for future processing. Line 18 iterates through the\nthreads ej that can be reached from ei via thread edges (there can be at most two), and for each\nsuch thread, lines 19-21 perform the same thread discovery procedure as lines 15-17.\n\nFinally, if all n breadth-first searches beginning from the n transactions do not find a possible\ndata race, then line 22 indicates this by returning no-race-found.\nRunning Time.\nThe main for loop at line 1 iterates Θ(n) times. Within each iteration of this\nmain loop is a breadth-first search of the threads. This breadth-first search is different in two ways\nfrom a regular breadth-first search of the threads. One difference is that it starts from a transaction\n(not a thread), and as the first step only considers threads that are reachable from the starting\ntransaction via access interaction edges (not also thread edges). The other difference is that the\nsearch does not expand when it visits a thread that is not in parallel with the starting thread. Note\nthat both of these differences can only cut down on the time required for a breadth-first search,\nbut never increase it. Thus, we can bound the time required for our breadth-first search by the\ntime required for a regular breadth-first search of the threads. If we let μ denote the number of\npairs of threads that have access interactions between some of their transactions, then the time\nrequired for a regular breadth-first search of the threads is O(m + μ). Thus, the time required for\nour breadth-first search is O(m + μ) as well. Then, over all the iterations of the main loop, the\ntotal time required for part C of the algorithm is Θ(n) · O(m + μ) = O(nm + nμ).\n5.4\nCorrectness and Analysis\nThis section first states and proves exact conditions under which part C of the algorithm reports a\ndata race. Next, it proves that the algorithm never reports any false negatives. Then, it gives an\nintuitive explanation of the algorithm and discusses how the programmer should use the reported\ninformation about a possible data race. Finally, it combines the analyses from Sections 5.1-5.3 to\nshow that the total running time of the algorithm is worst-case quadratic in the size of the target\nprogram's interaction graph.\nThe following theorem gives the conditions under which a possible data race is reported.\nTheorem 14. Part C of the algorithm reports a possible data race if and only if the target program's\ninteraction graph contains a thread cycle C = ⟨(t0, t′\n1), . . . , (tk-1, t′\n0)⟩such that\n1. every thread in the cycle is in parallel with the thread containing t0 (and t′\n0), and\n2. the starting transaction t0 appears before the ending transaction t′\n0 in their common thread.\nProof. Forward direction. Say that part C of the algorithm reports a possible data race in the pair\nof transactions (t, t′). Looking at the pseudocode for Find-Possible-Data-Race, this must have\nhappened during the iteration of the main for loop (line 1) in which ts = t. During this iteration,\na breadth-first search is performed with t as the starting point.\nNote that all threads visited during the breadth-first search are reachable from t via some thread\npath. Furthermore, due to the check in line 11, only threads that are in parallel with the thread\ncontaining t are expanded during the search.\nNow consider the iteration of the while loop at line 9, and within it the iteration of the for\nloop at line 12, during which (t, t′) is returned. Note that tj = t′ during this iteration. From the\nprevious paragraph, we know that the value of ei is some thread that can be reached from t via a\nthread path consisting of threads that are all in parallel with the thread containing t. From line 12,\nwe know that there is an access edge connecting ei to t′. Finally, from line 13, we know that t and\nt′ belong to the same thread and that t appears before t′ in their common thread. Thus, the thread\npath from t to ei combined with the access edge between ei and t′ forms a thread cycle C (starting\nat t and ending at t′) in the target program's interaction graph that satisfies both conditions in the\ntheorem statement.\n\nBackward direction. Say that the interaction graph of the target program contains a thread cycle\nC = ⟨(t0, t′\n1), . . . , (tk-1, t′\n0)⟩that meets the two conditions in the theorem statement. Furthermore,\nlet us assume that t′\n0 is the last transaction in its thread to share an access edge with any transaction\nin the thread containing tk-1, because otherwise we could change the last edge in C to make a\ndifferent thread cycle that does meet this criterion.\nIf part C of the algorithm reports a possible data race that is different from (t0, t′\n0), then we are\ndone. However, if part C does not report any other data race, then we shall prove that part C must\nreport a possible data race in the pair of transactions (t0, t′\n0).\nAt some point in the main for loop at line 1, we have ts = t0. Now let the thread containing\ntk-1 be e. Since part of the thread cycle C is a thread path from t0 to e consisting of threads that\nare all in parallel with the thread containing t, we know that e is processed at some point during\nthe breadth-first search starting from t. At this point, ei = e in the while loop at line 9. Within\nthis iteration of the while loop, at some point tj = t′\n0 in the for loop at line 12, because there is\nan access edge from transaction tk-1 in e to transaction t′\n0, and t′\n0 is the latest transaction in its\nthread for which this is true (note that the edge from tk-1 to t′\n0 cannot be a thread edge because\nthen e would not be in parallel with the thread containing t). It is at this time that the checks in\nline 13 pass and line 14 returns (t0, t′\n0).\nWith the following theorem, we prove that the algorithm never reports any false negatives. (A\nfalse negative occurs when the algorithm reports no-race-found, but in fact the target program\ncontains a data race.)\nTheorem 15. If the target program contains a data race, then part C of the algorithm reports a\npossible data race.\nProof. If the target program contains a data race, then it must have a race assignment A. Let\nGA be the assignment graph of A, and let C = ⟨(t0, t′\n1), . . . , (tk-1, t′\n0)⟩be a thread cycle in GA\nwith the fewest number of access edges. It must be true that for some i, transaction ti appears\nbefore t′\ni in their common thread. The reason is that, otherwise, there would be a cycle in GA\nof the form t0 →t′\n1 ⇝t1 →· · · ⇝tk-1 →t′\n0 ⇝t0, where every path t′\ni ⇝ti consists simply of\ntransaction edges within their common thread, and this cycle would contradict the fact that A is\na race assignment. Now, since the thread cycle C can be rotated to start at any thread, we can\nassume without loss of generality that t0 appears before t′\n0 in their common thread e0.\nAt this point, we consider three cases. We shall prove that only the first case can occur.\nCase 1: All the threads in C are in parallel with e0. In this case, the proof is completed by the\nbackward direction of Theorem 14.\nt 0\nt 0'\ne0\ne\n*\n*\n(a)\nt 0\nt 0'\ne0\n*\n*\ne\n(b)\nFigure 5.1: Diagrams of cases 2 and 3.\n\nCase 2: There exists a thread e in C that is a serial ancestor of e0. This situation is shown in\nFigure 5.1(a), where the dotted-and-dashed lines represent thread paths that combine to make up\nC, and an edge marked with an asterisk represents a path consisting of one or more edges of that\ntype. In this case, there is a thread path from t0 to e that is a portion of C, and a serial path from\ne to e0 using only thread edges. These two paths together form a thread cycle C′, which I claim\nhas fewer access edges than C, thus contradicting the fact that C is a thread cycle with the fewest\nnumber of access edges.\nThe reason that C′ has fewer access edges than C is as follows. Note that C′ is obtained from\nC by removing the portion of C going from e to t′\n0 and adding the serial path from e to e0. Adding\nthe serial path clearly does not add any access edges, so if we can prove that the path from e to t′\ncontains at least one access edge, then we are done. Note that t′\n0 cannot be the first transaction in\ne0 because there is at least one earlier transaction t0. Thus, there can be no thread edge that ends\nat t′\n0. This proves that the last edge in the path from e to t′\n0, which is an edge ending at t′\n0, must\nbe an access edge.\nCase 3: There exists a thread e in C that is a serial descendent of e0. This situation is shown in\nFigure 5.1(b). The proof is similar to that of Case 2. We consider the thread cycle C′ consisting of\nthe portion of C going from e to t′\n0 and the serial path from e0 to e. Note that C′ is obtained from\nC by removing the portion of C going from t0 to e and adding the serial path from e0 to e. Adding\nthe serial path does not add any access edges, while removing the thread path from t0 to e removes\nat least one access edge, because the first edge in this thread path is outgoing from t0, and cannot\nbe a thread edge because t0 is not the last transaction in e0. Thus, C′ contains fewer access edges\nthan C, contradicting the fact that C is a thread cycle with the fewest number of access edges.\nTheorem 14 gives precise conditions for when a possible data race is reported and Theorem 15\ntells us that there are no false negatives, but we still need an intuitive explanation of the algorithm,\nand a clarification on how to use the output information to help eliminate possible data races.\nThe intuition for the algorithm is as follows. We know from Chapter 4 that we (most likely) can-\nnot efficiently detect for race assignments, which can be thought of as thread cycles in the interaction\ngraph of the target program that do not also generate a (transaction) cycle. Thus, we are instead\ngoing to search for thread cycles that have a \"reasonable chance\" of not also generating a cycle.\nWe accomplish this by eliminating from consideration all thread cycles C = ⟨(t0, t′\n1), . . . , (tk-1, t′\n0)⟩\nthat can be immediately turned into a cycle of the form t0 →t′\n1 ⇝t1 →· · · ⇝tk-1 →t′\n0 ⇝t0\nsimply by connecting together each t′\ni and ti (if they are not already the same transaction) using\ntransaction edges in their common thread. This is by far the most common reason for a thread\ncycle to also generate a cycle.\nIn order for a thread cycle C not to directly cause a cycle as described in the previous paragraph,\nsome thread in C must break the potential underlying cycle by having ti precede t′\ni. Without loss\nof generality, we can assume t0 precedes t′\n0 in their common thread e0. This reasoning provides\nthe motivation for condition 2 in the statement of Theorem 14. We can think of the possible data\nrace being exhibited as follows. If all the transactions in C are scheduled in the order that they\nappear in C, starting with t0 and ending with t′\n0, then this schedule would not be equivalent to any\nschedule of the atomic-threads version of the target program. The reason is that in any equivalent\nschedule of the atomic-threads atomization, due to the sequence of constraints in C, e0 must appear\nbefore all the other threads in C as well as appear after all the other threads in C, which is clearly\nimpossible.\nThis explanation of how a possible data race can be exhibited leads naturally to condition 1\nin the statement of Theorem 14. We add the condition that all threads in C must be in parallel\n\nwith e0 because otherwise, the possible data race would not be able to be exhibited in the manner\ndescribed above. Still, we cannot eliminate possible data races simply by intuition, so we need\nthe support given by cases 2 and 3 in the proof of Theorem 15. The proof demonstrates that in\nthe cases shown in Figure 5.1, the algorithm would find some other thread cycle C′ that contains\nonly a subset of the access constraint edges in C. In these situations, C′ is the \"root cause\" of the\nproblem demonstrated by C, and the algorithm keeps searching until it finds the thread cycle C′\nbefore reporting the problem.\nFinally, we shall address how the reported information (t0, t′\n0) about C should be used to elimi-\nnate the possible data race. The programmer should ask himself or herself whether the correctness\nof the shared-memory accesses in t′\n0 depends on the assumption that there are no intervening\nshared-memory accesses by other threads since t0. If the correctness does depend on this assump-\ntion, then the programmer should use constructs in the program (which should be provided by the\ncompiler) to force t0 and t′\n0 to be placed in the same transaction. Intuitively, this change eliminates\nthe possibility that t0 and t′\n0 might be scheduled with intervening transactions that compromise\ncorrectness, while mathematically, this change means that e0 no longer prevents C from directly\ncausing a cycle. If, however, the correctness of t′\n0 does not depend on shared-memory accesses made\nin t0, then the programmer should use constructs in the program (which should be provided by the\ndata-race detection system) to indicate that this possible data race is not an error and should no\nlonger be reported.\nOverall Running Time.\nWe conclude by adding up the running times from the three parts of\nthe algorithm to obtain the overall running time. From Section 5.1, part A of the algorithm requires\nΘ(τ) time, where τ is the running time of the uninstrumented target program. From Section 5.2,\npart B of the algorithm requires O(nα + nm) time, where n is the number of transactions, m is the\nnumber of threads, and α is the number of accesses to shared-memory locations. From Section 5.3,\npart C of the algorithm requires O(nm + nμ) time, where μ is the number of pairs of threads that\nhave access interactions between some of their transactions. Thus, the overall running time of the\nwhole algorithm is Θ(τ) + O(nα + nm) + O(nm + nμ) = O(τ + nα + nm + nμ). Discounting the\ntime required for executing the uninstrumented target program, the running time of the algorithm\nis worst-case quadratic in the size of the interaction graph.\n\nChapter 6\nConclusion\nThis conclusion first summarizes what I have accomplished in this thesis, and then offers some\nsuggestions for future work.\nFirst, I laid out a mathematical foundation for studying the constraints on transaction schedul-\ning. I began by defining thread and transaction constraints, which are the constraints on legal\nschedules of an atomized program imposed by the serial control flow of the program. I then defined\na notion of equivalence for schedules of the same program, from which I derived access constraints,\nwhich are the constraints that determine whether two schedules are equivalent. I extended the\nconcept of access constraints for schedules to access interactions for atomized programs. Finally,\nI defined access assignments, which represent the link between access interactions and access con-\nstraints, and discussed when they are realizable. This foundation of terminology and basic facts\nabout the constraints on transaction scheduling can be reused in future studies of the transactions-\neverywhere methodology, even on topics not directly related to data-race detection.\nI formulated the definition of a data race in the transactions-everywhere setting. An atomized\nprogram is defined to contain a data race if there exists a schedule of it that is not equivalent to any\nschedule of the atomic-threads atomization. This definition is based on the weak assumption that\nthe program has correct parallelization, which is made in the same spirit as the assumption that\nunderlies the conventional definition of a data race. The weakness of the assumption causes the\ndefinition of a data race to become more complicated than the conventional definition. The struc-\nture of a data race also becomes more complicated, as a data race in the transactions-everywhere\nmethodology can involve an arbitrary number of threads and transactions.\nI defined race assignments and proved that the existence of a race assignment is a necessary and\nsufficient condition for an atomized program to contain a data race. I then used race assignments\nto prove, via a polynomial-time reduction from the problem of 3cnf-formula satisfiability, that data-\nrace detection is an NP-complete problem in general. This result suggests that data races most\nlikely cannot be detected efficiently, and that algorithms should be sought which approximately\ndetect data races.\nFinally, I presented an algorithm that approximately detects data races. The algorithm has three\nparts. Part A records the serial control flow and shared-memory accesses of the target program by\ninstrumenting it at key points; part B computes the least common ancestors structure of the threads\nand finds the access interactions; and part C uses the data prepared in the first two parts to search\nfor a possible data race. I stated and proved exact conditions under which a possible data race is\nreported, and showed that the algorithm never reports any false negatives. Intuitively, the algorithm\nsearches for a thread cycle in the target program's interaction graph that does not immediately\nlead to a cycle, and within that thread cycle, a thread that is a primary cause of the possible data\n\nrace. When it finds a possible data race, the algorithm reports two transactions that may have\ntheir correctness compromised by intervening transactions in the schedule. The programmer should\nthen examine whether or not the two transactions have a logical data dependency. If so, then he\nor she would indicate to the compiler that the two transactions should be merged into the same\ntransaction, and if not, then he or she would indicate to the data-race detector that there is in fact\nno error. The algorithm requires running time that is worst-case quadratic in the size of the target\nprogram's interaction graph.\nAt this point, I conclude by offering some suggestions for future work. The issue of how to\nhandle data-race detection in the transactions-everywhere methodology has by no means been fully\nstudied and solved. One basic question is whether there is a better definition of a data race that\nwould allow data races to be efficiently detected in general, while still accurately capturing real-life\ndata-race errors and not making extra assumptions about program correctness. Another question\nis whether other common techniques for detecting data races, in particular static analysis and\ndynamic online detection, can be fruitfully applied in the transactions-everywhere setting.\nImplementation of the proposed algorithm is necessary to test its feasibility in practice. Two\nimportant factors that need to be evaluated are the rate that data races appear in programs using\ntransactions everywhere and the rate that false positives appear. Experimentation with different\nheuristic rules for determining cutpoints may lead to more accurate atomization strategies for\ncompilers that automatically generate transactions everywhere. Also, it may be possible to lower\nthe rate of false positives by designing a better algorithm.\n\nRelated Work\nCharles E. Leiserson and many graduate students in his research group designed and implemented\nCilk [3, 39, 13, 22], a parallel-programming language that is a faithful extension of the C language.\nMingdong Feng, Guang-Ien Cheng, Leiserson, and others developed the Nondeterminator [11, 5, 4,\n38], a data-race detector for Cilk programs using conventional locks.\nOther researchers have studied data-race detection in many contexts. Static data-race detectors\n[30] analyze the target program without executing it. Dynamic race detectors study the behavior\nof the target program through its execution. The latter can be categorized into those that perform\ndetection as the target program runs [8, 9, 11, 34, 27, 12], and those that study a completed\nexecution trace of the target program [29, 15, 10, 31].\nMaurice Herlihy and J. Eliot B. Moss suggested hardware transactional memory [19], a hardware\nmechanism for supporting shared-memory parallel programming. Thomas F. Knight and Morry\nKatz presented similar ideas in the context of functional languages [24, 23].\nNir Shavit, Dan\nTouitou, Herlihy, Victor Luchangco, Mark Moir and others have attempted software transactional\nmemory [36, 18], which provides transactional-memory support in software.\nLeslie Lamport proposed lock-free data structures [25], and Herlihy and Moss suggested wait-\nfree programming [16, 17], both of which are ways to implement highly concurrent shared data\nstructures utilizing atomic instructions that are weaker than transactions.\nLeiserson suggested transactions everywhere [26], a methodology for parallel programming on\nsystems with HTM support that further eases the programmer's job beyond the benefits provided by\nHTM. Cl ement M enier applied the transactions-everywhere methodology to sample Cilk programs\nand collected some statistics on the resulting transactions [28].\n\nBibliography\n[1] Michael A. Bender and Martin Farach-Colton. \"The LCA Problem Revisited\". Latin American\nTheoretial INformatics, pages 88-94. April 2000. http://www.cs.sunysb.edu/~bender/pub/\nlca.ps.\n[2] Robert D. Blumofe. Executing Multithreaded Programs Efficiently. Ph.D. thesis, MIT Depart-\nment of Electrical Engineering and Computer Science. Cambridge, Massachusetts, September\n1995. ftp://theory.lcs.mit.edu/pub/cilk/rdb-phdthesis.ps.Z.\n[3] Robert D. Blumofe, Christopher F. Joerg, Bradley C. Kuszmaul, Charles E. Leiserson, Keith\nH. Randall, and Yuli Zhou. \"Cilk: An Efficient Multithreaded Runtime System\". Proceedings\nof the Fifth ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming\n(PPoPP), pages 207-216. Santa Barbara, California, July 1995. ftp://theory.lcs.mit.edu/\npub/cilk/PPoPP95.ps.Z.\n[4] Guang-Ien Cheng. Algorithms for Data-Race Detection in Multithreaded Programs. Master's\nthesis, MIT Department of Electrical Engineering and Computer Science. Cambridge, Mas-\nsachusetts, June 1998. ftp://theory.lcs.mit.edu/pub/cilk/cheng-thesis.ps.gz.\n[5] Guang-Ien Cheng, Mingdong Feng, Charles E. Leiserson, Keith H. Randall, and Andrew F.\nStark. \"Detecting Data Races in Cilk Programs That Use Locks\". Proceedings of the Tenth\nAnnual ACM Symposium on Parallel Algorithms and Architectures (SPAA '98), pages 298-309.\nPuerto Vallarta, Mexico, June 1998. ftp://theory.lcs.mit.edu/pub/cilk/brelly.ps.gz.\n[6] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduc-\ntion to Algorithms, Second Edition. MIT Press, Cambridge, Massachusetts, 2001. http://\nmitpress.mit.edu/algorithms/.\n[7] Martin Dietzfelbinger, Anna Karlin, Kurt Mehlhorn, Friedhelm Meyer auf der Heide, Hans\nRohnert, and Robert E. Tarjan. \"Dynamic Perfect Hashing: Upper and Lower Bounds\". SIAM\nJournal on Computing, 23(4):738-761. August, 1994. ftp://www.mpi-sb.mpg.de/~mehlhorn/\nftp/DynamicPerfectHashing.ps.\n[8] Anne Dinning and Edith Schonberg. \"An Empirical Comparison of Monitoring Algorithms\nfor Access Anomaly Detection\". Proceedings of the Second ACM SIGPLAN Symposium on\nPrinciples and Practices of Parallel Programming (PPoPP), pages 1-10. Seattle, Washington,\nMarch 1990.\n[9] Anne Dinning and Edith Schonberg. \"Detecting Access Anomalies in Programs with Critical\nSections\". Proceedings of the ACM/ONR Workshop on Parallel and Distributed Debugging,\npages 85-96. ACM Press, May 1991.\n\n[10] Perry A. Emrath, Sanjoy Ghosh, and David A. Padua. \"Event Synchronization Analysis for De-\nbugging Parallel Programs\". Proceedings of Supercomputing '89, pages 580-588. Reno, Nevada,\nNovember, 1989. http://www.csrd.uiuc.edu/reports/839.ps.gz.\n[11] Mingdong Feng and Charles E. Leiserson. \"Efficient Detection of Determinacy Races in\nCilk Programs\". Proceedings of the Ninth Annual ACM Symposium on Parallel Algorithms\nand Architectures (SPAA '97), pages 1-11. Newport, Rhode Island, June 1997. ftp://\ntheory.lcs.mit.edu/pub/cilk/spbags.ps.gz.\n[12] Yaacov Fenster. Detecting Parallel Access Anomalies. Master's thesis, Hebrew University.\nMarch 1998.\n[13] Matteo Frigo, Charles E. Leiserson, and Keith H. Randall. \"The Implementation of the Cilk-5\nMultithreaded Language\". Proceedings of the ACM SIGPLAN '98 Conference on Programming\nLanguage Design and Implementation, pages 212-223. Montreal, Quebec, Canada, June 1998.\nftp://theory.lcs.mit.edu/pub/cilk/cilk5.ps.gz.\n[14] Michael R. Garey and David S. Johnson. Computers and Intractability: A Guide to the Theory\nof NP-Completeness. W. H. Freeman and Company, 1979.\n[15] David P. Helmhold, Charles E. McDowell, and Jian-Zhong Wang. \"Analyzing Traces with\nAnonymous Synchronization\". Proceedings of the 1990 International Conference on Parallel\nProcessing, pages II:70-77. St. Charles, Illinois, August 1990. ftp://ftp.cse.ucsc.edu/pub/\ntr/ucsc-crl-89-42.ps.Z.\n[16] Maurice Herlihy. \"A Methodology for Implementing Highly Concurrent Data Objects\".\nACM Transactions on Programming Languages and Systems, 15(5):745-770. November 1993.\nhttp://www.cs.brown.edu/people/mph/Herlihy93/herlihy93methodology.pdf.\n[17] Maurice Herlihy. \"Wait-Free Synchronization\". ACM Transactions on Programming Lan-\nguages and Systems, 13(1):124-149, January 1991. http://www.cs.brown.edu/people/mph/\nHerlihy91/p124-herlihy.pdf.\n[18] Maurice Herlihy, Victor Luchangco, and Mark Moir. Obstruction-Free Software Transactional\nMemory for Supporting Dynamic Data Structures. Unpublished manuscript. October 2002.\n[19] Maurice Herlihy and J. Eliot B. Moss. \"Transactional Memory: Architectural Support for\nLock-Free Data Structures\". Proceedings of the 1993 International Symposium on Computer\nArchitecture, pages 289-300. San Diego, California, May 1993. http://www.cs.brown.edu/\npeople/mph/HerlihyM93/herlihy93transactional.pdf.\n[20] Maurice Herlihy and J. Eliot B. Moss. Transactional Support for Lock-Free Data Structures\n(Technical Report 92/07). Digital Cambridge Research Lab. Cambridge, Massachusetts, De-\ncember 1992.\n[21] Dov Harel and Robert E. Tarjan. \"Fast Algorithms for Finding Nearest Common Ancestors\".\nSIAM Journal on Computing, 13(2):338-355. May 1984.\n[22] Christopher F. Joerg. The Cilk System for Parallel Multithreaded Computing. Ph.D. thesis,\nMIT Department of Electrical Engineering and Computer Science. Cambridge, Massachusetts,\nJanuary 1996. ftp://theory.lcs.mit.edu/pub/cilk/joerg-phd-thesis.ps.gz.\n\n[23] Morry J. Katz. ParaTran: A Transparent, Transaction Based Runtime Mechanism for Par-\nallel Execution of Scheme. Master's thesis, MIT Department of Electrical Engineering and\nComputer Science. Cambridge, Massachusetts, June 1986.\n[24] Thomas F. Knight. \"An Architecture for Mostly Functional Languages\". Proceedings of the\n1986 ACM Conference on Lisp and Functional Programming, pages 88-93. August 1986.\n[25] Leslie Lamport. \"Concurrent Reading and Writing\". Communications of the ACM, 20(11):806-\n811. 1977.\n[26] Charles E. Leiserson. Personal communication. Cambridge, Massachusetts, July 2002.\n[27] John Mellor-Crummey. \"On-the-Fly Detection of Data Races for Programs with Nested Fork-\nJoin Parallelism\". Proceedings of Supercomputing '91, pages 24-33. Albuquerque, New Mexico,\nNovember 1991.\n[28] Cl ement M enier. Atomicity in Parallel Programming. Unpublished manuscript. Supercom-\nputer Technologies Research Group, MIT Laboratory for Computer Science. Cambridge, Mas-\nsachusetts, September 2002.\n[29] Barton P. Miller and Jong-Deok Choi. \"A Mechanism for Efficient Debugging of Parallel Pro-\ngrams\". Proceedings of the 1998 ACM SIGPLAN Conference on Programming Language De-\nsign and Implementation (PLDI), pages 135-144. Atlanta, Georgia, June 1988.\n[30] Greg Nelson, K. Rustan M. Leino, James B. Saxe, and Raymie Stata. Extended Static Checking\nhome page. http://www.research.compaq.com/SRC/esc/Esc.html.\n[31] Robert H. B. Netzer and Sanjoy Ghosh. \"Efficient Race Condition Detection for Shared-\nMemory Programs with Post/Wait Synchronization\". Proceedings of the 1992 International\nConference on Parallel Processing. August 1992. ftp://ftp.cs.wisc.edu/tech-reports/\nreports/92/tr1084.ps.Z.\n[32] Robert H. B. Netzer and Barton P. Miller. \"On the Complexity of Event Ordering for\nShared-Memory Parallel Program Executions\". Proceedings of the 1990 International Con-\nference on Parallel Processing, pages II:93-97. St. Charles, Illinois, August 1990. ftp://\ngrilled.cs.wisc.edu/technical_papers/complexity.ps.Z.\n[33] Robert H. B. Netzer and Barton P. Miller. \"What Are Race Conditions?\"\nACM Letters\non Programming Languages and Systems, 1(1):74-88. March 1992. http://www.cs.umd.edu/\nprojects/syschat/raceConditions.pdf.\n[34] Itzhak Nudler and Larry Rudolph. \"Tools for the Efficient Development of Efficient Parallel\nPrograms\". Proceedings of the First Israeli Conference on Computer Systems Engineering.\nMay 1986.\n[35] Baruch Schieber and Uzi Vishkin. \"On Finding Lowest Common Ancestors: Simplification and\nParallelization\". SIAM Journal on Computing, 17(6):1253-1262. December 1988.\n[36] Nir Shavit and Dan Touitou. \"Software Transactional Memory\". Proceedings of the 14th Annual\nACM Symposium on Principles of Distributed Computing, pages 204-213. Ottawa, Ontario,\nCanada, 1995. http://theory.lcs.mit.edu/~shanir/stm.ps.\n\n[37] Michael Sipser. Introduction to the Theory of Computation. PWS Publishing Company, Boston,\nMassachusetts, 1997. http://www-math.mit.edu/~sipser/book.html.\n[38] Andrew F. Stark. Debugging Multithreaded Programs that Incorporate User-Level Locking. Mas-\nter's thesis, MIT Department of Electrical Engineering and Computer Science. Cambridge,\nMassachusetts, May 1998. ftp://theory.lcs.mit.edu/pub/cilk/astark-thesis.ps.gz.\n[39] Supercomputing Technologies Group, MIT Laboratory for Computer Science. Cilk 5.3.2 Refer-\nence Manual. Cambridge, Massachusetts, November 2001. http://supertech.lcs.mit.edu/\ncilk/manual-5.3.2.pdf."
    },
    {
      "category": "Resource",
      "title": "lca.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/e9bc5076484a9dd5f73115ef860ccc54_lca.pdf",
      "content": "The LCA Problem Revisited\nMichael A. Bender\n\nMart ın Farach-Colton\n\nSUNY Stony Brook\nRutgers University\nMay 16, 2000\nAbstract\nWe present a very simple algorithm for the Least Common Ancestor problem. We thus dispel the fre\nquently held notion that an optimal LCA computation is unwieldy and unimplementable. Interestingly,\nthis algorithm is a sequentialization of a previously known PRAM algorithm of Berkman, Breslauer,\nGalil, Schieber, and Vishkin [1].\nKeywords: Data Structures, Least Common Ancestor (LCA), Range Minimum Query (RMQ), Cartesian\nTree.\nIntroduction\nOne of the most fundamental algorithmic problems on trees is how to find the Least Common Ancestor\n(LCA) of a pair of nodes. The LCA of nodes\nand\nin a tree is the shared ancestor of\nand\nthat is located\nfarthest from the root. More formally, the LCA Problem is stated as follows: Given a rooted tree\n, how\ncan be preprocessed to answer LCA queries quickly for any pair of nodes. Thus, one must optimize both\nthe preprocessing time and the query time.\nThe LCA problem has been studied intensively both because it is inherently beautiful algorithmically\nand because fast algorithms for the LCA problem can be used to solve other algorithmic problems. In [2],\nHarel and Tarjan showed the surprising result that LCA queries can be answered in constant time after only\nlinear preprocessing of the tree\n. This classic paper is often cited because linear preprocessing is necessary\nto achieve optimal algorithms in many applications. However, it is well understood that the actual algorithm\nDepartment of Computer Science, State University of New York at Stony Brook, Stony Brook, NY 11794-4400, USA.\nSupported in part by ISX Corporation and Hughes Research Laboratories.\nDepartment of Computer Science, Rutgers University, Piscataway, NJ 08855,USA.\nSupported in part by NSF Career Development Award CCR-9501942, NATO Grant CRG 960215, NSF/NIH Grant BIR 94-12594-\n03-CONF.\n\npresented is far too complicated to implement effectively. In [3], Schieber and Vishkin introduced a new\nLCA algorithm. Although their algorithm is vastly simpler than Harel and Tarjan's--indeed, this was the\npoint of this new algorithm--it is far from simple and still not particularly implementable.\nThe folk wisdom of algorithm designers holds that the LCA problem still has no implementable optimal\nsolution. Thus, according to hearsay, it is better to have a solution to a problem that does not rely on LCA\nprecomputation if possible. We argue in this paper that this folk wisdom is wrong.\nIn this paper, we present not only a simplified LCA algorithm, we present a simple LCA algorithm! We\ndevise this algorithm by re engineering an existing complicated LCA algorithm: Berkman, Breslauer, Galil,\nSchieber, and Vishkin [1]. presented a PRAM algorithm that preprocesses and answers queries in\n\ntime and preprocesses in linear work. Although at first glance, this algorithm is not a promising candidate\nfor implementation, it turns out that almost all of the complications are PRAM induced: when the PRAM\ncomplications are excised from this algorithm so that it is lean, mean, and sequential, we are left with an\nextremely simple algorithm.\nIn this paper, we present this re engineered algorithm. Our point is not to present a new algorithm.\nIndeed, we have already noted that this algorithm has appeared as a PRAM algorithm before. The point is to\nchange the folk wisdom so that researchers are free to use the full power and elegance of LCA computation\nwhen it is appropriate.\nThe remainder of the paper is organized as follows. In Section 2, we provide some definitions and initial\nlemmas. In Section 3, we present a relatively slow algorithm for LCA preprocessing. In Section 4, we show\nhow to speed up the algorithm so that it runs within the desired time bounds.\nFinally, in Section 5, we\nanswer some algorithmic questions that arise in the paper but that are not directly related to solving the LCA\nproblem.\nDefinitions\nWe begin by defining the Least Common Ancestor (LCA) Problem formally.\nProblem 1 The Least Common Ancestor (LCA) problem:\nStructure to Preprocess: A rooted tree\nhaving\nnodes.\nQuery: For nodes\nand\nof tree\n, query LCA\n\nreturns the least common ancestor of\nand\nin\n,\nthat is, it returns the node furthest from the root that is an ancestor of both\nand\n. (When the context\nis clear, we drop the subscript\non the LCA.)\nThe Range Minimum Query (RMQ) Problem, which seems quite different from the LCA problem, is, in\nfact, intimately linked.\n\nProblem 2 The Range Minimum Query (RMQ) problem:\nStructure to Preprocess: A length\narray\nof numbers.\nQuery: For indices\nand\nbetween\nand\n, query RMQ\n\nreturns the index of the smallest element\nin the subarray\n\n. (When the context is clear, we drop the subscript\non the RMQ.)\nIn order to simplify the description of algorithms that have both preprocessing and query complexity,\nwe introduce the following notation. If an algorithm has preprocessing time\nand query time\n, we\nwill say that the algorithm has complexity\n\n.\nOur solutions to the LCA problem are derived from solutions to the RMQ problem. Thus, before pro\nceeding, we reduce the LCA problem to the RMQ problem. The following simple lemma establishes this\nreduction.\nLemma 3 If\nthere\nis\nan\n\n-time\nsolution\nfor\nRMQ,\nthen\nthere\nis\nan\n\n-time solution for LCA.\nAs we will see, the\n\nterm in the preprocessing comes from the time needed to create the soon-to-be-\npresented length\n\narray, and the\n\nterm in the query comes from the time needed to convert the\nRMQ answer on this array to an LCA answer in the tree.\nProof: Let be the input tree. The reduction relies on one key observation:\nObservation 4 The LCA of nodes\nand\nis the shallowest node encountered between the visits to\nand to\nduring a depth first search traversal of\n.\nTherefore, the reduction proceeds as follows.\n1. Let array\n\nstore the nodes visited in an Euler Tour of the tree . 1 That is,\nis the\nlabel of the\nth node visited in the Euler tour of .\n2. Let the level of a node be its distance from the root. Compute the Level Array\n\n, where\nis the level of node\n\nof the Euler Tour.\n3. Let the representative of a node in an Euler tour be the index of first occurrence of the node in the\ntour2; formally, the representative of\nis\n\n. Compute the Representative Array\n\n, where is the index of the representative of node .\n1The Euler Tour of is the sequence of nodes we obtain if we write down the label of each node each time it is visited during\na DFS. The array of the Euler tour has length\n\nbecause we start at the root and subsequently output a node each time we\ntraverse an edge. We traverse each of the\n\nedges twice, once in each direction.\n2In fact, any occurrence of\nwill suffice to make the algorithm work, but we consider the first occurrence for the sake of\nconcreteness.\n\nEach of these three steps takes\ntime, yielding\ntotal time. To compute LCA\n\n, we note\nthe following:\nThe nodes in the Euler Tour between the first visits to\nand to\nare\n\n(or\n\n).\nThe shallowest node in this subtour is at index RMQ\n\n, since\nstores the level of the\nnode at\n\n, and the RMQ will thus report the position of the node with minimum level. (Recall\nObservation 4.)\nThe node at this position is\nRMQ\n\n, which is thus the output of LCA\n\n.\nThus, we can complete our reduction by preprocessing Level Array for RMQ. As promised,\nis an array\nof size\n\n, and building it takes time\n\n. Thus, the total preprocessing is\n\n. To\ncalculate the query time observe that an LCA query in this reduction uses one RMQ query in\nand three\narray references at\n\ntime each. The query thus takes time\n\n, and we have completed\nthe proof of the reduction.\nFrom now on, we focus only on RMQ solutions. We consider solutions to the general RMQ problem\nas well as to an important restricted case suggested by the array\n. In array\nfrom the above reduction\nadjacent elements differ by or . We obtain this\nrestriction because, for any two adjacent elements\nin an Euler tour, one is always the parent of the other, and so their levels differ by exactly one. Thus, we\nconsider the -RMQ problem as a special case.\n2.1\nA Na ıve Solution for RMQ\nWe first observe that RMQ has a solution with complexity\n\n: build a table storing answers to\nall of the\n\npossible queries. To achieve\n\npreprocessing rather than the\n\nnaive preprocessing,\nwe apply a trivial dynamic program. Notice that answering an RMQ query now requires just one array\nlookup.\nA Faster RMQ Algorithm\nWe will improve the\n\n-time brute-force table algorithm for (general) RMQ. The idea is to\nprecompute each query whose length is a power of two. That is, for every\nbetween\nand\nand every\n\nbetween 1 and\n, we find the minimum element in the block starting at\nand having length\n, that is,\nwe compute\n\n. Table\ntherefore has size\n\n, and we fill it in\n\ntime\n\nby using dynamic programming. Specifically, we find the minimum in a block of size\n\nby\ncomparing the two minima of its two constituent blocks of size\n\n. More formally,\n\nif\n\nand\n\notherwise.\nHow do we use these blocks to compute an arbitrary RMQ\n\n? We select two overlapping blocks that\nentirely cover the subrange: let\n\nbe the size of the largest block that fits into the range from\nto\n, that\nis let\n\n. Then RMQ\n\ncan be computed by comparing the minima of the following two\nblocks:\nto\n\n(\n\n) and\n\nto (\n\n). These values have already been\ncomputed, so we can find the RMQ in constant time.\nThis gives the Sparse Table (ST) algorithm for RMQ, with complexity\n\n. Notice that\nthe total computation to answer an RMQ query is three additions, 4 array reference and a minimum, in\naddition to two other operations: a log and a floor. These can be seen together as the problem of finding the\nmost significant bit of a word. Notice that we must have one such operation in our algorithm, since Harel\nand Tarjan [2] showed that LCA computation has a lower bound of\n\non a pointer machine.\nFurthermore, the most-significant-bit operation has a very fast table lookup solution.\nBelow, we will use the ST algorithm to build an even faster algorithm for the RMQ problem.\nAn\n\n-Time Algorithm for\nRMQ\nSuppose we have an array\nwith the\nrestriction. We will use a table-lookup technique to precompute\nanswers on small subarrays, thus removing the log factor from the preprocessing. To this end, partition\ninto blocks of size\n\n. Define an array\n\n, where\n\nis the minimum element in the\n\nth block of . Define an equal size array\n, where\nis a position in the th block in which value\n\noccurs. Recall that RMQ queries return the position of the minimum and that the LCA to RMQ reduction\nuses the position of the minimum, rather than the minimum itself. Thus we will use array\nto keep track\nof where the minima in\ncame from.\nThe ST algorithm runs on array\n\nin time\n\n. Having preprocessed\nfor RMQ, consider\nhow we answer any query RMQ\n\nin\n. The indices and\nmight be in the same block, so we have to\npreprocess each block to answer RMQ queries. If\n\nare in different blocks, the we can answer the query\nRMQ\n\nas follows. First compute the values:\n1. The minimum from forward to the end of its block.\n2. The minimum of all the blocks in between between 's block and 's block.\n3. The minimum from the beginning of 's block to\n.\n\nThe query will return the position of the minimum of the three values computed. The second minimum is\nfound in constant time by an RMQ on\n\n, which has been preprocessed using the ST algorithm. But, we\nneed to know how to answer range minimum queries inside blocks to compute the first and third minima,\nand thus to finish off the algorithm. Thus, the in-block queries are needed whether and are in the same\nblock or not.\nTherefore, we focus now only on in-block RMQs. If we simply performed RMQ preprocessing on each\nblock, we would spend too much time in preprocessing. If two block were identical, then we could share\ntheir preprocessing. However, it is too much to hope for that blocks would be so repeated. The following\nobservation establishes a much stronger shared-preprocessing property.\nObservation 5 If two arrays\n\nand\n\ndiffer by some fixed value at each position, that\nis, there is a\nsuch that\n\nfor every\n, then all RMQ answers will be the same for\nand\n.\nIn this case, we can use the same preprocessing for both arrays.\nThus, we can normalize a block by subtracting its initial offset from every element. We now use the\nproperty to show that there are very few kinds of normalized blocks.\n\nLemma 6 There are\n\nkinds of normalized blocks.\nProof: Adjacent elements in normalized blocks differ by or . Thus, normalized blocks are specified\n\nby a vector of length\n\n. There are\n\nsuch vectors.\n\nWe are now basically done. We create\n\ntables, one for each possible normalized block. In each\ntable, we put all\n\nanswers to all in-block queries. This gives a total of\n\ntotal preprocessing of normalized block tables, and\nquery time. Finally, compute, for each block in ,\nwhich normalized block table it should use for its RMQ queries. Thus, each in-block RMQ query takes a\nsingle table lookup.\nOverall, the total space and preprocessing used for normalized block tables and\n\ntables is\n\nand\nthe total query time is\n.\n4.1\nWrapping Up\nWe started out by showing a reduction from the LCA problem to the RMQ problem, but with the key\nobservation that the reduction actually leads to a RMQ problem.\nWe gave a trivial\n\n-time table-lookup algorithm for RMQ, and show how to sparsify the\ntable to get a\n\n-time table-lookup algorithm. We used this latter algorithm on a smaller\nsummary array\n\nand needed only to process small blocks to finish the algorithm. Finally, we notice that\n\nmost of these blocks are the same, from the point of view of the RMQ problem, by using the\nassumption\ngiven by the original reduction.\nA Fast Algorithm for RMQ\nWe have a\n\nRMQ. Now we show that the general RMQ can be solved in the same complex\nity. We do this by reducing the RMQ problem to the LCA problem! Thus, to solve a general RMQ problem,\none would convert it to an LCA problem and then back to a RMQ problem.\nThe following lemma establishes the reduction from RMQ to LCA.\nLemma 7 If there is a\n\nsolution for LCA, then there is a\n\nsolution for RMQ.\nWe will show that the\n\nterm in the preprocessing comes from the time needed to build the Cartesian\nTree of and the\nterm in the query comes from the time needed to covert the LCA answer on this tree\nto an RMQ answer on .\nProof: Let\n\nbe the input array.\nThe Cartesian Tree of an array is defined as follows. The root of a Cartesian Tree is the minimum element\nof the array, and the root is labeled with the position of this minimum. Removing the root element splits the\narray into two pieces. The left and right children of the root are the recursively constructed Cartesian trees\nof the left and right subarrays, respectively.\nA Cartesian Tree can be built in linear time as follows. Suppose\nis the Cartesian tree of\n\n.\nTo build\n\n, we notice that node\n\nwill belong to the rightmost path of\n\n, so we climb up the\nrightmost path of\n\nuntil finding the position where\n\nbelongs. Each comparison either adds an element\nto the rightmost path or removes one, and each node can only join the rightmost path and leave it once. Thus\nthe total time to build\n\nis\n.\nThe reduction is as follows.\nLet\nbe the Cartesian Tree of\n. Recall that we associate with each node in\nthe corresponding\ncorresponding to with the index .\nClaim 7A RMQ\n\nLCA\n\n.\nProof: Consider the least common ancestor,\n, of\nand\nin the Cartesian Tree\n. In the recursive\ndescription of a Cartesian tree,\nis the first node that separates and . Thus, in the array\n, element\n\nis between elements\nand\n\n. Furthermore,\n\nmust be the smallest such element in the subarray\n\nsince otherwise, there would be an smaller element\n\nin\n\nthat would be an ancestor\nof in\n, and and would already have been separated by\n.\n\nMore concisely, since\nis the first element to split\nand , it is between them because it splits them, and\nit is minimal because it is the first element to do so. Thus it is the RMQ.\nWe see that we can complete our reduction by preprocessing the Cartesian Tree\nfor LCA. Tree\n\ntakes time\nto build, and because is an node tree, LCA preprocessing takes\ntime, for a total\nof\ntime. The query then takes\n\n, and we have completed the proof of the reduction.\nReferences\n[1] O. Berkman, D. Breslauer, Z. Galil, B. Schieber, and U. Vishkin. Highly parallelizable problems. In\nProc. of the 21st Ann. ACM Symp. on Theory of Computing, pages 309-319, 1989.\n[2] D. Harel and R. E. Tarjan. Fast algorithms for finding nearest common ancestors. SIAM J. Comput.,\n13(2):338-355, 1984.\n[3] B. Schieber and U. Vishkin. On finding lowest common ancestors: Simplification and parallelization.\nSIAM J. Comput., 17:1253-1262, 1988."
    },
    {
      "category": "Resource",
      "title": "minicourse.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/9cf3ee7891b56e3e966928ba8dced3f3_minicourse.pdf",
      "content": "A\nMinicourse\non\nMultithreaded\nProgramming\nCharles\nE\nLeiserson\nHarald\nProk\nop\nMIT\nLab\noratory\nfor\nComputer\nScience\n\nT\nec\nhnology\nSquare\nCam\nbridge\nMassac\nh\nusetts\n\nJuly\n\nAbstract\nThese\nnotes\ncon\ntain\nt\nw\no\nlectures\nthat\nteac\nh\nm\nultithreaded\nalgorithms\nusing\na\nCilk\nlik\ne\n\nmo\ndel\nThese\nlectures\nw\nere\ndesigned\nfor\nthe\nlatter\npart\nof\nthe\nMIT\nundergraduate\nclass\n\nIntr\no\nduction\nto\nA\nlgorithms\nThe\nst\nyle\nof\nthe\nlecture\nnotes\nfollo\nws\nthat\nof\nthe\ntextb\no\nok\nb\ny\nCormen\nLeiserson\nand\nRiv\nest\n\nbut\nthe\npseudo\nco\nde\nfrom\nthat\ntextb\no\nok\nhas\nb\neen\nCilkied\nto\nallo\nw\nit\nto\ndescrib\ne\nm\nultithreaded\nalgo\nrithms\nThe\nrst\nlecture\nteac\nhes\nthe\nbasics\nb\nehind\nm\nultithreading\nincluding\ndening\nthe\nmeasures\nof\nw\nork\nand\ncriticalpath\nlength\nIt\nculminates\nin\nthe\ngreedy\nsc\nheduling\ntheorem\ndue\nto\nGraham\nand\nBren\nt\n\nThe\nsecond\nlecture\nsho\nws\nho\nw\nparallel\napplications\nincluding\nmatrix\nm\nultiplication\nand\nsorting\ncan\nb\ne\nanalyzed\nusing\ndivideandconquer\nrecurrences\n\nMultithreaded\nprogramming\nAs\nm\nultipro\ncessor\nsystems\nha\nv\ne\nb\necome\nincreasingly\na\nv\nailable\nin\nterest\nhas\ngro\nwn\nin\nparallel\nprogramming\nMultithr\ne\nade\nd\np r\no\ngr\namming\nis\na\nprogramming\nparadigm\nin\nwhic\nh\na\nsingle\nprogram\nis\nbrok\nen\nin\nto\nm\nultiple\nthr\ne\nads\nof\ncon\ntrol\nwhic\nh\ni n\nteract\nto\nsolv\ne\na\nsingle\nproblem\nThese\nnotes\npro\nvide\nan\nin\ntro\nduction\nto\nthe\nanalysis\nof\nm\nultithreaded\nalgorithms\nThis\nresearc\nh\nw\nas\nsupp\norted\nin\npart\nb\ny\nthe\nDefense\nAdv\nanced\nResearc\nh\nPro\njects\nAgency\nD\nARP\nA\nunder\nGran\nt\nF\n\nMo\ndel\nOur\nmo\ndel\nof\nm\nultithreaded\ncomputation\nis\nbased\non\nthe\npro\ncedure\nabstraction\nfound\nin\nvir\ntually\nan\ny\nprogramming\nlanguage\nAs\nan\nexample\nthe\npro\ncedure\nFib\ngiv\nes\na\nm\nultithreaded\nalgorithm\nfor\ncomputing\nthe\nFib\nonacci\nn\num\nb\ners\nFibn\n\nif\nn\n\nthen\nreturn\nn\n\nelse\nx\n\nspa\nwn\nFibn\n\ny\n\nspa\nwn\nFibn\n\nsync\n\nreturn\nx\n\ny\n\nA\nspa\nwn\nis\nthe\nparallel\nanalog\nof\nan\nordinary\nsubroutine\ncall\nThe\nk\neyw\nord\nspa\nwn\nb\nefore\nthe\nsubroutine\ncall\nin\nline\n\nindicates\nthat\nthe\nsubpro\ncedure\nFibn\n\ncan\nexecute\nin\nparallel\nwith\nthe\npro\ncedure\nFibn\nitself\nUnlik\ne\nan\nordinary\nfunction\ncall\nho\nw\nev\ner\nwhere\nthe\nparen\nt\nis\nnot\nresumed\nun\ntil\nafter\nits\nc\nhild\nreturns\nin\nthe\ncase\nof\na\nspa\nwn\nthe\nparen\nt\ncan\ncon\ntin\nue\nto\nexecute\nin\nparallel\nwith\nthe\nc\nhild\nIn\nthis\ncase\nthe\nparen\nt\ngo\nes\non\nto\nspa\nwn\nFibn\n\nIn\ngeneral\nthe\nparen\nt\ncan\ncon\ntin\nue\nto\nspa\nwn\no\nc\nhildren\npro\nducing\na\nhigh\ndegree\nof\nparallelism\nA\npro\ncedure\ncannot\nsafely\nuse\nthe\nreturn\nv\nalues\nof\nthe\nc\nhildren\nit\nhas\nspa\nwned\nun\ntil\nit\nexecutes\na\nsync\nstatemen\nt\nIf\nan\ny\nof\nits\nc\nhildren\nha\nv\ne\nnot\ncompleted\nwhen\nit\nexecutes\na\nsync\nthe\npro\ncedure\nsusp\nends\nand\ndo\nes\nnot\nresume\nun\ntil\nall\nof\nits\nc\nhildren\nha\nv\ne\ncompleted\nWhen\nall\nof\nits\nc\nhildren\nreturn\nexecution\nof\nthe\npro\ncedure\nresumes\nat\nthe\np\noin\nt\nimmediately\nfollo\nwing\nthe\nsync\nstatemen\nt\nIn\nthe\nFib\nonacci\nexample\nthe\nsync\nstatemen\nt\nin\nline\n\nis\nrequired\nb\nefore\nthe\nreturn\nstatemen\nt\nin\nline\n\nto\na\nv\noid\nthe\nanomaly\nthat\nw\nould\no\nccur\nif\nx\nand\ny\nw\nere\nsummed\nb\nefore\neac\nh\nhad\nb\neen\ncomputed\nThe\nspa\nwn\nand\nsync\nk\neyw\nords\nsp\necify\nlo\ngic\nal\nparallelism\nnot\nactual\nparallelism\nThat\nis\nthese\nk\neyw\nords\nindicate\nwhic\nh\nco\nde\nma\ny\np\nossibly\nexecute\nin\nparallel\nbut\nwhat\nac\ntually\nruns\nin\nparallel\nis\ndetermined\nb\ny\na\nsche\nduler\n\nw h i c\nh\nmaps\nthe\ndynamically\nunfolding\ncomputation\non\nto\nthe\na\nv\nailable\npro\ncessors\nW\ne\ncan\nview\na\nm\nultithreaded\ncomputation\nin\ngraphtheoretic\nterms\nas\na\ndynamically\nunfolding\ndag\nG\n\nV\n\nE\n\nas\nis\nsho\nwn\nin\nFigure\n\nfor\nFib\nW\ne\ndene\na\nthr\ne\nad\nto\nb\ne\na\nmaximal\nsequence\nof\ninstructions\nnot\ncon\ntaining\nthe\nparallel\ncon\ntrol\nstatemen\nts\nspa\nwn\nsyncand\nreturn\nThreads\nmak\ne\nup\nthe\nset\nV\nof\nv\nertices\nof\nthe\nm\nultithreaded\ncomputation\ndag\nG\nEac\nh\npro\ncedure\nexecution\nis\na\nlinear\nc\nhain\nof\nthreads\neac\nh\nof\nwhic\nh\nis\nconnected\nto\nits\nsuccessor\nin\nthe\nc\nhain\nb\ny\na\nc\nontinuation\nedge\nWhen\na\nthread\nu\nspa\nwns\na\nthread\nv\n\nthe\ndag\ncon\ntains\na\nsp\nawn\nedge\nu\nv\n\nE\n\na s\nw\nell\nas\na\ncon\ntin\nuation\nedge\nfrom\nu\nto\nus\nsuccessor\nin\nthe\npro\ncedure\nWhen\na\nthread\nu\nreturns\nthe\ndag\ncon\ntains\nan\nedge\nu\nv\n\nwhere\nv\nis\nthe\nthread\nthat\nimmediately\nfollo\nws\nthe\nnext\nsync\nin\nthe\nparen\nt\npro\ncedure\nEv\nery\ncomputation\nstarts\nwith\na\nsingle\ninitial\nthr\ne\nad\nand\nassuming\nthat\nthe\ncomputation\nterminates\nends\nThis\nalgorithm\nis\na\nterrible\nw\na\ny\nto\ncompute\nFib\nonacci\nn\num\nb\ners\nsince\nit\nruns\nin\nexp\nonen\ntial\ntime\nwhen\nlogarithmic\nmetho\nds\nare\nkno\nwn\n\npage\n\nbut\nit\nserv\nes\nas\na\ngo\no\nd\ndidactic\nexample\n\n1111 0000\nfib(3)\nfib(2)\nfib(1)\nfib(1)\nfib(2)\nfib(1)\nfib(0)\nfib(0)\nfib(4)\nFigure\n\nA\ndag\nrepresen\nting\nthe\nm\nultithreaded\ncomputation\nof\nFib\nThreads\nare\nsho\nwn\nas\ncircles\nand\neac\nh\ngroup\nof\nthreads\nb\nelonging\nto\nthe\nsame\npro\ncedure\nare\nsurrounded\nb\ny\na\nrounded\nrectangle\nDo\nwn\nw\nard\nedges\nare\nspa\nwns\ndep\nendencies\nhorizon\ntal\nedges\nrepresen\nt\ncon\ntin\nuation\nde\np\nendencies\nwithin\na\npro\ncedure\nand\nup\nw\nard\nedges\nare\nreturn\ndep\nendencies\nwith\na\nsingle\nnal\nthr\ne\nad\n\nSince\nthe\npro\ncedures\nare\norganized\nin\na\ntree\nhierarc\nh\ny\n\nw\ne\ncan\nview\nthe\ncomputation\nas\na\ndag\nof\nthreads\nem\nb\nedded\nin\nthe\ntree\nof\npro\ncedures\n\nP\nerformance\nMeasures\nTw\no\np\nerformance\nmeasures\nsuce\nto\ngauge\nthe\ntheoretical\neciency\nof\nm\nultithreaded\nalgo\nrithms\nW\ne\ndene\nthe\nwork\nof\na\nm\nultithreaded\ncomputation\nto\nb\ne\nthe\ntotal\ntime\nto\nexecute\nall\nthe\nop\nerations\nin\nthe\ncomputation\non\none\npro\ncessor\nW\ne\ndene\nthe\ncritic\nalp\nath\nlength\nof\na\ncomputation\nto\nb\ne\nthe\nlongest\ntime\nto\nexecute\nthe\nthreads\nalong\nan\ny\npath\nof\ndep\nenden\ncies\nin\nthe\ndag\nConsider\nfor\nexample\nthe\ncomputation\nin\nFigure\n\nSupp\nose\nthat\nev\nery\nthread\ncan\nb\ne\nexecuted\nin\nunit\ntime\nThen\nthe\nw\nork\nof\nthe\ncomputation\nis\n\nand\nthe\ncriticalpath\nlength\nis\n\nWhen\na\nm\nultithreaded\ncomputation\nis\nexecuted\non\na\ngiv\nen\nn\num\nb\ne r\nP\nof\npro\ncessors\nits\nrunning\ntime\ndep\nends\non\nho\nw\necien\ntly\nthe\nunderlying\nsc\nheduler\ncan\nexecute\nit\nDenote\nb\ny\nT\nthe\nrunning\ntime\nof\na\ngiv\nen\ncomputation\non\nP\npro\ncessors\nThen\nthe\nw\nork\nof\nthe\ncomputation\ncan\nb\ne\nview\ned\nas\nT\n\nand\nthe\ncriticalpath\nlength\ncan\nb\ne\nview\ned\nas\nT\n\nThe\nw\nork\nand\ncriticalpath\nlength\ncan\nb\ne\nused\nto\npro\nvide\nlo\nw\ner\nb\no u n d s\non\nthe\nrunning\nP\ntime\non\nP\npro\ncessors\nW\ne\nha\nv\ne\nT\nP\n\nT\nP\n\nsince\nin\none\nstep\na\nP\npro\ncessor\ncomputer\ncan\ndo\nat\nmost\nP\nw\nork\nW\ne\nalso\nha\nv\ne\nT\nP\n\nT\n\nsince\na\nP\npro\ncessor\ncomputer\ncan\ndo\nno\nmore\nw\nork\nin\none\nstep\nthan\nan\ninnitepro\ncessor\ncomputer\n\nThe\nsp\ne\ne\ndup\nof\na\ncomputation\non\nP\npro\ncessors\nis\nthe\nratio\nT\nT\nP\n\nw h i c\nh\nindicates\nho\nw\nman\ny\ntimes\nfaster\nthe\nP\npro\ncessor\nexecution\nis\nthan\na\nonepro\ncessor\nexecution\nIf\nT\nT\n\nP\nP\n\nthen\nw\ne\nsa\ny\nthat\nthe\nP\npro\ncessor\nexecution\nexhibits\nline\nar\nsp\ne\ne\ndup\n\nThe\nmaxim\num\np\nossible\nsp\needup\nis\nT\nT\n\nwhic\nh\nis\nalso\ncalled\nthe\np\nar\nal\nlelism\nof\nthe\ncomputation\nb\necause\nit\nrepresen\nts\nthe\na\nv\nerage\namoun\nt\no f\nw\nork\nthat\ncan\nb\ne\ndone\nin\nparallel\nfor\neac\nh\nstep\nalong\nthe\ncritical\npath\nW\ne\ndenote\nthe\nparallelism\nof\na\ncomputation\nb\ny\nP\n\nGreedy\nSc\nheduling\nThe\nprogrammer\nof\na\nm\nultithreaded\napplication\nhas\nthe\nabilit\ny\nto\ncon\ntrol\nthe\nw\nork\nand\ncriticalpath\nlength\nof\nhis\napplication\nbut\nhe\nhas\nno\ndirect\ncon\ntrol\no\nv\ner\nthe\nsc\nheduling\nof\nhis\napplication\non\na\ngiv\nen\nn\num\nb\ne r\nof\npro\ncessors\nIt\nis\nup\nto\nthe\nrun\ntime\nsc\nheduler\nto\nmap\nthe\ndynamically\nunfolding\ncomputation\non\nto\nthe\na\nv\nailable\npro\ncessors\nso\nthat\nthe\ncomputation\nexecutes\necien\ntly\n\nGo\no\nd\nonline\nsc\nhedulers\nare\nkno\nwn\n\nbut\ntheir\nanalysis\nis\ncompli\ncated\nF\nor\nsimplicit\ny\n\nw\nell\nillustrate\nthe\nprinciples\nb\nehind\nthese\nsc\nhedulers\nusing\nan\noline\ngreedy\nsc\nheduler\nA\ngr\ne\ne\ndy\nsche\nduler\nsc\nhedules\nas\nm\nuc\nh\nas\nit\ncan\nat\nev\nery\ntime\nstep\nOn\na\nP\npro\ncessor\ncomputer\ntime\nsteps\ncan\nb\ne\nclassied\nin\nto\nt\nw\no\nt\nyp\nes\nIf\nthere\nare\nP\nor\nmore\nthreads\nready\nto\nexecute\nthe\nstep\nis\na\nc\nomplete\nstep\nand\nthe\nsc\nheduler\nexecutes\nan\ny\nP\nthreads\nof\nthose\nready\nto\nexecute\nIf\nthere\nare\nfew\ner\nthan\nP\nthreads\nready\nto\nexecute\nthe\nstep\nis\nan\ninc\nomplete\nstep\nand\nthe\nsc\nheduler\nexecutes\nall\nof\nthem\nThis\ngreedy\nstrategy\nis\npro\nv\nably\ng o\no\nd\nTheorem\n\nGraham\n\nBren\nt\n\nA\ng r\ne\ne\ndy\nsche\nduler\nexe\ncutes\nany\nmultithr\ne\nade\nd\nc\nom\nputation\nG\nwith\nwork\nT\nand\ncritic\nalp\nath\nlength\nT\nin\ntime\nT\n\nT\nP\n\nT\n\nP\non\na\nc\nomputer\nwith\nP\npr\no\nc\nessors\nG\nPr\no\nof\nF\nor\neac\nh\ncomplete\nstep\nP\nw\nork\nis\ndone\nb\ny\nthe\nP\npro\ncessors\nTh\nus\nthe\nn\num\nb\ne r\nof\ncomplete\nsteps\nis\nat\nmost\nT\nP\n\nb\necause\nafter\nT\nP\nsuc\nh\nsteps\nall\nthe\nw\nork\nin\nthe\ncomputation\nhas\nb\neen\np\nerformed\nNo\nw\nconsider\nan\nincomplete\nstep\nand\nconsider\nthe\nsub\ndag\nof\nG\nthat\nremains\nto\nb\ne\nexecuted\nWithout\nloss\nof\ngeneralit\ny\n\nw\ne\ncan\nview\neac\nh\nof\nthe\nthreads\nexecuting\nin\nunit\ntime\nsince\nw\ne\ncan\nreplace\na\nlonger\nthread\nwith\na\nc\nhain\nof\nunittime\nthreads\nEv\nery\nthread\nwith\nindegree\n\nis\nready\nto\nb\ne\nexecuted\nsince\nall\nof\nits\npredecessors\nha\nv\ne\nalready\nexecuted\nBy\nthe\ngreedy\nsc\nheduling\np\nolicy\n\nall\nsuc\nh\nthreads\nare\nexecuted\nsince\nthere\nare\nstrictly\nfew\ner\nthan\nP\nsuc\nh\nthreads\nTh\nus\nthe\ncriticalpath\nlength\nof\nG\nis\nreduced\nb\ny\n\nSince\nthe\ncriticalpath\nlength\nof\nthe\nsub\ndag\nremaining\nto\nb\ne\nexecuted\ndecreases\nb\ny\n\neac\nh\nfor\neac\nh\nincomplete\nstep\nthe\nn\num\nb\ne r\nof\nincomplete\nsteps\nis\nat\nmost\nT\n\nEac\nh\nstep\nis\neither\ncomplete\nor\nincomplete\nand\nhence\nInequalit\ny\n\nfollo\nws\nCorollary\n\nA\ngr\ne\ne\ndy\nsche\nduler\nachieves\nline\nar\nsp\ne\ne\ndup\nwhen\nP\n\nO\nP\n\nPr\no\nof\nSince\nP\n\nT\nT\n\nw\ne\nha\nv\ne\nP\n\nO\nT\nT\n\nor\nequiv\nalen\ntly\n\nthat\nT\n\nO\nT\nP\n\nTh\nus\nw\ne\nha\nv\ne\nT\n\nT\nP\n\nT\n\nO\nT\nP\n\nP\n\nCilk\nand\n\nSo\ncrates\nCilk\n\nis\na\nparallel\nm\nultithreaded\nlanguage\nbased\non\nthe\nserial\nprogramming\nlan\nguage\nC\nInstrumen\ntation\nin\nthe\nCilk\nsc\nheduler\npro\nvides\nan\naccurate\nmeasure\nof\nw\nork\nand\ncritical\npath\nCilks\nrandomized\nsc\nheduler\npro\nv\nably\nexecutes\na\nm\nultithreaded\ncomputation\non\na\nP\npro\ncessor\ncomputer\nin\nT\n\nT\nP\n\nO\nT\n\nexp\nected\ntime\nEmpirically\n\nt h e\ns c\nheduler\nP\nac\nhiev\nes\nT\n\nT\nP\n\nT\ntime\nyielding\nnearp\nerfect\nlinear\nsp\needup\nif\nP\n\nP\n\nY\nou\ncan\nread\nmore\nab\nout\nCilk\non\nthe\nW\neb\nat\nhttptheorylcsmitedu\n\ncilk\nAmong\nthe\napplications\nthat\nha\nv\ne\nb\neen\nprogrammed\nin\nCilk\nare\nthe\n\nSo\ncrates\nand\nCilk\nc\nhess\nc\nhesspla\nying\nprograms\nThese\nprograms\nha\nv\ne\nw\non\nn\numerous\nprizes\nin\nin\nterna\ntional\ncomp\netition\nand\nare\nconsidered\nto\nb\ne\namong\nthe\nstrongest\nin\nthe\nw\norld\nAn\nin\nteresting\nanomaly\no\nccurred\nduring\nthe\ndev\nelopmen\nt\no f\n\nSo\ncrates\nwhic\nh\nw\nas\nresolv\ned\nb\ny\nunderstanding\nthe\nmeasures\nof\nw\nork\nand\ncriticalpath\nlength\nThe\n\nSo\ncrates\nprogram\nw\nas\ninitially\ndev\nelop\ned\non\na\npro\ncessor\ncomputer\nat\nMIT\nbut\nit\nw\nas\nin\ntended\nto\nrun\non\na\npro\ncessor\ncomputer\nat\nthe\nNational\nCen\nter\nfor\nSup\nercomputing\nApplications\nNCSA\nat\nthe\nUniv\nersit\ny\nof\nIllinois\nA\nc l e v\ner\noptimization\nw\nas\nprop\nosed\nwhic\nh\nduring\ntesting\nat\nMIT\ncaused\nthe\nprogram\nto\nrun\nm\nuc\nh\nfaster\nthan\nthe\noriginal\nprogram\nNev\nertheless\nthe\noptimization\nw\nas\nabandoned\nb\necause\nan\nanalysis\nof\nw\nork\nand\ncriticalpath\nlength\nindicated\nthat\nthe\nprogram\nw\nould\nactually\nb\ne\nslo\nw\ner\non\nthe\nNCSA\nmac\nhine\nLet\nus\nexamine\nthis\nanomaly\nin\nmore\ndetail\nF\nor\nsimplicit\ny\n\nthe\nactual\ntiming\nn\num\nb\ne r s\nha\nv\ne\nb\neen\nsimplied\nThe\noriginal\nprogram\nran\nin\nT\n\nseconds\nat\nMIT\non\n\npro\ncessors\nThe\noptimized\nprogram\nran\nin\nT\n\nseconds\nalso\non\n\npro\ncessors\nThe\noriginal\nP\nT\nprogram\nhad\nw\nork\nT\n\nseconds\nand\ncriticalpath\nlength\nT\n\nsecond\nUsing\nthe\nform\nula\nT\n\nT\nP\n\nT\nas\na\ng o\no\nd\nappro\nximation\nof\nrun\ntime\nw\ne\ndisco\nv\ner\nthat\nindeed\n\nThe\noptimized\nprogram\nhad\nw\nork\nT\n\nseconds\nand\ncritical\npath\nlength\nT\n\nseconds\nyielding\nT\n\nBut\nno\nw\nlet\nus\ndetermine\nthe\nP\nrun\ntimes\non\n\npro\ncessors\nW\ne\nh a\nv\ne\nT\n\nand\nT\n\nwhic\nh\nis\nt\nwice\nas\nslo\nw\nTh\nus\nb\ny\nusing\nw\nork\nand\ncriticalpath\nlength\nw\ne\ncan\npredict\nthe\np\nerformance\nof\na\nm\nultithreaded\ncomputation\nExercise\n\nSk\netc\nh\nthe\nm\nultithreaded\ncomputation\nthat\nresults\nfrom\nexecuting\nFib\nAssume\nthat\nall\nthreads\nin\nthe\ncomputation\nexecute\nin\nunit\ntime\nWhat\nis\nthe\nw\nork\nof\nthe\ncomputation\nWhat\nis\nthe\ncriticalpath\nlength\nSho\nw\nho\nw\nto\nsc\nhedule\nthe\ndag\non\n\npro\ncessors\nin\na\ngreedy\nfashion\nb\ny\nlab\neling\neac\nh\nthread\nwith\nthe\ntime\nstep\non\nwhic\nh\nit\nexecutes\nExercise\n\nW\nrite\na\nm\nultithreaded\npro\ncedure\nSumA\nwhere\nA\n\nn\nis\nan\narra\ny\n\nw h ic\nh\nuses\ndivideandconquer\nto\nsum\nthe\nelemen\nts\nof\nthe\narra\ny\nA\nin\nparallel\nExercise\n\nPro\nv\ne\nthat\na\ngreedy\nsc\nheduler\nac\nhiev\nes\nthe\nstronger\nb\no u n d\nT\n\nT\n\nT\nP\n\nT\n\nP\nExercise\n\nPro\nv\ne\nthat\nthe\ntime\nfor\na\ngreedy\nsc\nheduler\nto\nexecute\nan\ny\nm\nultithreaded\ncomputation\nis\nwithin\na\nfactor\nof\n\nof\nthe\ntime\nrequired\nb\ny\nan\noptimal\nsc\nheduler\n\nExercise\n\nF\nor\nwhat\nn\num\nb\ne r\nP\nof\npro\ncessors\ndo\nthe\nt\nw\no\nc\nhess\nprograms\ndescrib\ned\nin\nthis\nsection\nrun\nequally\nfast\nExercise\n\nProfessor\nTw\need\ntak\nes\nsome\nmeasuremen\nts\nof\nhis\ndeterministic\nm\nulti\nthreaded\nprogram\nwhic\nh\nis\nsc\nheduled\nusing\na\ngreedy\nsc\nheduler\nand\nnds\nthat\nT\n\nseconds\nand\nT\n\nseconds\nWhat\nis\nthe\nfastest\nthat\nthe\nprofessors\ncomputation\ncould\np\nossibly\nrun\non\n\npro\ncessors\nUse\nInequalit\ny\n\nand\nthe\nt\nw\no\nl o\nw\ner\nb\nounds\nfrom\nInequalities\n\nand\n\nto\nderiv\ne\ny\nour\nansw\ner\n\nAnalysis\nof\nm\nultithreaded\nalgorithms\nW\ne\nn o\nw\nturn\nto\nthe\ndesign\nand\nanalysis\nof\nm\nultithreaded\nalgorithms\nBecause\nof\nthe\ndivide\nandconquer\nnature\nof\nthe\nm\nultithreaded\nmo\ndel\nrecurrences\nare\na\nnatural\nw\na\ny\nto\nexpress\nthe\nw\nork\nand\ncriticalpath\nlength\nof\na\nm\nultithreaded\nalgorithm\nW\ne\nshall\nin\nv\nestigate\nalgorithms\nfor\nmatrix\nm\nultiplication\nand\nsorting\nand\nanalyze\ntheir\np\nerformance\n\nP\narallel\nMatrix\nMultiplication\nT\no\nm\nultiply\nt\nw\no\nn\n\nn\nmatrices\nA\nand\nB\nin\nparallel\nto\npro\nduce\na\nmatrix\nC\n\nw\ne\ncan\nrecursiv\nely\nform\nulate\nthe\nproblem\nas\nfollo\nws\n!\n!\n\n!\nC\nC\nA\nA\nB\nB\n\nC\nC\nA\nA\nB\nB\n!\nA\nB\n\nA\nB\nA\nB\n\nA\nB\nA\n\nB\n\nA\nB\nA\nB\n\nA\nB\nTh\nus\neac\nh\nn\n\nn\nmatrix\nm\nultiplication\ncan\nb\ne\nexpressed\nas\n\nm\nultiplications\nand\n\nadditions\nof\nn\n\nn\nsubmatrices\nThe\nm\nultithreaded\npro\ncedure\nMul\nt\nm\nultiplies\nt\nw\no\nn\n\nn\nmatrices\nwhere\nn\nis\na\np\no\nw\ner\nof\n\nusing\nan\nauxiliary\npro\ncedure\nAdd\nto\nadd\nn\n\nn\nmatrices\nThis\nalgorithm\nis\nnot\ninplace\nAddC\n\nT\n\nn\n\nif\nn\n\nthen\nC\n\nC\n\nT\n\nelse\npartition\nC\nand\nT\nin\nto\nn\n\nn\nsubmatrices\n\nspa\nwn\nAddC\n\nT\n\nn\n\nspa\nwn\nAddC\n\nT\n\nn\n\nspa\nwn\nAddC\n\nT\n\nn\n\nspa\nwn\nAddC\n\nT\n\nn\n\nsync\n\nMul\ntC\n\nA\nB\n\nn\n\nif\nn\n\nthen\nC\n\nA\n\nB\n\nelse\nallo\ncate\na\ntemp\norary\nmatrix\nT\n\nn\n\nn\n\npartition\nA\nB\n\nC\n\nand\nT\nin\nto\nn\n\nn\nsubmatrices\n\nspa\nwn\nMul\ntC\n\nA\n\nB\n\nn\n\nspa\nwn\nMul\ntC\n\nA\n\nB\n\nn\n\nspa\nwn\nMul\ntC\n\nA\n\nB\n\nn\n\nspa\nwn\nMul\ntC\n\nA\n\nB\n\nn\n\nspa\nwn\nMul\ntT\n\nA\n\nB\n\nn\n\nspa\nwn\nMul\ntT\n\nA\n\nB\n\nn\n\nspa\nwn\nMul\ntT\n\nA\n\nB\n\nn\n\nspa\nwn\nMul\ntT\n\nA\n\nB\n\nn\n\nsync\n\nspa\nwn\nAddC\n\nT\n\nn\n\nsync\nThe\nmatrix\npartitionings\nin\nline\n\nof\nMul\nt\nand\nline\n\nof\nadd\ntak\ne\nO\n\ntime\nsince\nonly\na\nconstan\nt\nn\num\nb\ne r\nof\nindexing\nop\nerations\nare\nrequired\nT\no\nanalyze\nthis\nalgorithm\nlet\nA\nn\nb\ne\nthe\nP\npro\ncessor\nrunning\ntime\nof\nAdd\non\nn\n\nn\nmatrices\nand\nlet\nM\nn\nb\ne\nthe\nP\npro\ncessor\nrunning\ntime\nof\nMul\nt\non\nn\n\nn\nmatrices\nThe\nP\nP\nw\nork\nrunning\ntime\non\none\npro\ncessor\nfor\nAdd\ncan\nb\ne\nexpressed\nb\ny\nthe\nrecurrence\nA\nn\n\nA\nn\n\nn\n\nwhic\nh\nis\nthe\nsame\nas\nfor\nthe\nordinary\ndoublenestedlo\nop\nserial\nalgorithm\nSince\nthe\nspa\nwned\npro\ncedures\ncan\nb\ne\nexecuted\nin\nparallel\nthe\ncriticalpath\nlength\nfor\nAdd\nis\nA\nn\n\nA\nn\n\nlg\nn\n\nThe\nw\nork\nfor\nMul\nt\ncan\nb\ne\nexpressed\nb\ny\nthe\nrecurrence\nM\nn\n\nM\nn\n\nA\nn\n\nM\nn\n\nn\n\nn\n\nwhic\nh\nis\nthe\nsame\nas\nfor\nthe\nordinary\ntriplenestedlo\nop\nserial\nalgorithm\nThe\ncriticalpath\nlength\nfor\nMul\nt\nis\nM\nn\n\nM\nn\n\nlg\nn\n\nlg\nn\n\nTh\nus\nthe\nparallelism\nfor\nMul\nt\nis\nM\nn\nM\nn\n\nn\n\nlg\nn\nwhic\nh\nis\nquite\nhigh\nT\no\nm\nultiply\n\nmatrices\nfor\nexample\nthe\nparallelism\nis\nignoring\nconstan\nts\nab\nout\n\nMost\nparallel\ncomputers\nha\nv\ne\nfar\nfew\ner\npro\ncessors\nT\no\na c\nhiev\ne\nhigh\np\nerformance\nit\nis\noften\nadv\nan\ntageous\nfor\nan\nalgorithm\nto\nuse\nless\nspace\nb\necause\nmore\nspace\nusually\nmeans\nmore\ntime\nF\nor\nthe\nmatrixm\nultiplication\nproblem\nw\ne\ncan\neliminate\nthe\ntemp\norary\nmatrix\nT\nin\nexc\nhange\nfor\nreducing\nthe\nparallelism\nOur\nnew\nalgorithm\nMul\ntAdd\np\nerforms\nC\n\nC\n\nA\n\nB\nusing\na\nsimilar\ndivideandconquer\nstrategy\nto\nMul\nt\nMul\ntAddC\n\nA\nB\n\nn\n\nif\nn\n\nthen\nC\n\nC\n\nA\n\nB\n\nelse\npartition\nA\nB\n\nand\nC\nin\nto\nn\n\nn\nsubmatrices\n\nspa\nwn\nMul\ntAddC\n\nA\n\nB\n\nn\n\nspa\nwn\nMul\ntAddC\n\nA\n\nB\n\nn\n\nspa\nwn\nMul\ntAddC\n\nA\n\nB\n\nn\n\nspa\nwn\nMul\ntAddC\n\nA\n\nB\n\nn\n\nsync\n\nspa\nwn\nMul\ntAddC\n\nA\n\nB\n\nn\n\nspa\nwn\nMul\ntAddC\n\nA\n\nB\n\nn\n\nspa\nwn\nMul\ntAddC\n\nA\n\nB\n\nn\n\nspa\nwn\nMul\ntAddC\n\nA\n\nB\n\nn\n\nsync\nLet\nMA\nn\nb\ne\nthe\nP\npro\ncessor\nrunning\ntime\nof\nMul\ntAdd\non\nn\n\nn\nmatrices\nThe\nw\nork\nfor\nMul\ntAdd\nis\nMA\nn\n\nn\n\nfollo\nwing\nthe\nsame\nanalysis\nas\nfor\nMul\nt\nbut\nthe\ncriticalpath\nlength\nis\nno\nw\nP\nMA\nn\n\nMA\nn\n\nn\n\nsince\nonly\n\nrecursiv\ne\ncalls\ncan\nb\ne\nexecuted\nin\nparallel\nTh\nus\nthe\nparallelism\nis\nMA\nnMA\nn\n\nn\n\nOn\n\nmatrices\nfor\nexam\nple\nthe\nparallelism\nis\nignoring\nconstan\nts\nstill\nquite\nhigh\nab\nout\n\nIn\npractice\nthis\nalgorithm\noften\nruns\nsomewhat\nfaster\nthan\nthe\nrst\nsince\nsa\nving\nspace\noften\nsa\nv\nes\ntime\ndue\nto\nhierarc\nhical\nmemory\n\nl\n\nl\n\nAl\n\nAl\n\nA\n\nj\nj\n\nm\n\nAl\n\nAl\n\nB\nFigure\n\nIllustration\nof\nPMer\nge\nThe\nmedian\nof\narra\ny\nA\nis\nused\nto\npartition\narra\ny\nB\n\nand\nthen\nthe\nlo\nw\ner\np\nortions\nof\nthe\nt\nw\no\narra\nys\nare\nrecursiv\nely\nmerged\nas\nin\nparallel\nare\nthe\nupp\ner\np\nortions\n\nP\narallel\nMerge\nSort\nThis\nsection\nsho\nws\nho\nw\nto\nparallelize\nmerge\nsort\nW\ne\nshall\nsee\nthe\nparallelism\nof\nthe\nalgorithm\ndep\nends\non\nho\nw\nw\nell\nthe\nmerge\nsubroutine\ncan\nb\ne\nparallelized\nThe\nmost\nstraigh\ntforw\nard\nw\na\ny\nto\nparallelize\nmerge\nsort\nis\nto\nrun\nthe\nrecursion\nin\nparallel\nas\nis\ndone\nin\nthe\nfollo\nwing\npseudo\nco\nde\nMer\ngeSor\nt\nA\np\nr\n\nif\np\n\nr\n\nthen\nq\nb p\n\nr\nc\n\nspa\nwn\nMer\ngeSor\nt\nA\np\nq\n\nspa\nwn\nMer\ngeSor\nt\nA\nq\n\nr\n\nsync\n\nMer\nge\nA\np\nq\n\nr\n\nThe\nw\nork\nof\nMer\ngeSor\nt\non\nan\narra\ny\nof\nn\nelemen\nts\nis\nT\nn\n\nT\nn\n\nn\n\nn\nlg\nn\n\nsince\nthe\nrunning\ntime\nof\nMer\nge\nis\nn\nSince\nthe\nt\nw\no\nrecursiv\ne\ns p a\nwns\nop\nerate\nin\nparallel\nthe\ncriticalpath\nlength\nof\nMer\ngeSor\nt\nis\nT\nn\n\nT\nn\n\nn\n\nn\n\nConsequen\ntly\n\nthe\nparallelism\nof\nthe\nalgorithm\nis\nT\nnT\nn\n\nlg\nn\nwhic\nh\nis\np u n\ny\n\nThe\nob\nvious\nb\nottlenec\nk\nis\nMer\nge\nThe\nfollo\nwing\npseudo\nco\nde\nwhic\nh\nis\nillustrated\nin\nFigure\n\np\nerforms\nthe\nmerge\nin\nparallel\n\nPMer\nge\nA\n\nl\n\nB\n\nm\nC\n\nn\n\nif\nm\n\nl\n\nwithout\nloss\nof\ngeneralit\ny\n\nlarger\narra\ny\nshould\nb\ne\nrst\n\nthen\nspa\nwn\nPMer\nge\nB\n\nm\nA\n\nl\n\nC\n\nn\n\nelseif\nn\n\nthen\nC\n\nA\n\nelseif\nl\n\nand\nm\n\nthen\nif\nA\n\nB\n\nthen\nC\n\nA\nC\n\nB\n\nelse\nC\n\nB\n\nC\n\nA\n\nelse\nnd\nj\nsuc\nh\nthat\nB\nj\n\nAl\n\nB\nj\n\nusing\nbinary\nsearc\nh\n\nspa\nwn\nPMer\nge\nA\n\nl\n\nB\n\nj\n\nC\n\nl\n\nj\n\nspa\nwn\nPMer\nge\nAl\n\nl\n\nB\nj\n\nm\nC\nl\n\nj\n\nn\n\nsync\nThis\nmerging\nalgorithm\nnds\nthe\nmedian\nof\nthe\nlarger\narra\ny\nand\nuses\nit\nto\npartition\nthe\nsmaller\narra\ny\n\nThen\nthe\nlo\nw\ner\np\nortions\nof\nthe\nt\nw\no\narra\nys\nare\nrecursiv\nely\nmerged\nand\nin\nparallel\nso\nare\nthe\nupp\ner\np\nortions\nT\no\nanalyze\nPMer\nge\nlet\nPM\nn\nb\ne\nthe\nP\npro\ncessor\ntime\nto\nmerge\nt\nw\no\narra\nys\nA\nand\nB\nha\nving\nn\n\nm\n\nl\nelemen\nts\nin\ntotal\nWithout\nloss\nof\ngeneralit\ny\n\nlet\nA\nb\ne\nthe\nlarger\nof\nthe\nt\nw\no\narra\nys\nthat\nis\nassume\nl\n\nm\nW\nell\nanalyze\nthe\ncriticalpath\nlength\nrst\nThe\nbinary\nsearc\nh\nof\nB\ntak\nes\nlg\nm\ntime\nwhic\nh\nin\nthe\nw\norst\ncase\nis\nlg\nn\nSince\nthe\nt\nw\no\nrecursiv\ne\ns p a\nwns\nin\nlines\n\nand\n\nop\nerate\nin\nparallel\nthe\nw\norstcase\ncriticalpath\nlength\nis\nlg\nn\nplus\nthe\nw\norstcase\ncritical\npath\nlength\nof\nthe\nspa\nwn\nop\nerating\non\nthe\nlarger\nsubarra\nys\nIn\nthe\nw\norst\ncase\nw\ne\nm\nust\nmerge\nhalf\nof\nA\nwith\nall\nof\nB\n\nin\nw h ic\nh\ncase\nthe\nrecursiv\ne\ns p a\nwn\nop\nerates\non\nat\nmost\nn\nelemen\nts\nTh\nus\nw\ne\nha\nv\ne\nP\nPM\nn\n\nPM\nn\n\nlg\nn\n\nlg\nn\n\nT\no\nanalyze\nthe\nw\nork\nof\nMer\nge\nobserv\ne\nthat\nalthough\nthe\nt\nw\no\nrecursiv\ne\nspa\nwns\nma\ny\nop\nerate\non\ndieren\nt\nn\num\nb\ners\nof\nelemen\nts\nthey\nalw\na\nys\nop\nerate\non\nn\nelemen\nts\nb\net\nw\neen\nthem\nLet\n\nn\n\nb\ne\nthe\nn\num\nb\ne r\nof\nelemen\nts\nop\nerated\non\nb\ny\nthe\nrst\nspa\nwn\nwhere\n\nis\na\nconstan\nt\nin\nthe\nrange\n\nTh\nus\nthe\nsecond\nspa\nwn\nop\nerates\non\n\nn\nelemen\nts\nand\nthe\nw\norstcase\nw\nork\nsatises\nthe\nrecurrence\nPM\nn\n\nPM\n\nn\n\nPM\n\nn\n\nlg\nn\n\nW\ne\nshall\nsho\nw\nthat\nPM\nn\n\nn\nusing\nthe\nsubstitution\nmetho\nd\nActually\n\nthe\nAkra\nBazzi\nmetho\nd\n\nif\ny\nou\nkno\nw\nit\nis\nsimpler\nW\ne\nassume\ninductiv\nely\nthat\nPM\nn\n\nan\n\nb\nlg\nn\nfor\nsome\nconstan\nts\na\nb\n\nW\ne\nha\nv\ne\nPM\nn\n\na\nn\n\nb\nlg\nn\n\na\n\nn\n\nb\nlg\n\nn\n\nlg\nn\n\nan\n\nblg\n\nn\n\nlg\n\nn\n\nlg\nn\n\nan\n\nblg\n\nlg\nn\n\nlg\n\nlg\nn\n\nlg\nn\n\nan\n\nb\nlg\nn\n\nblg\nn\n\nlg\n\nlg\nn\n\nan\n\nb\nlg\nn\n\nsince\nw\ne\ncan\nc\nho\nose\nb\nlarge\nenough\nso\nthat\nblg\nn\n\nlg\n\ndominates\nlg\nn\nMoreo\nv\ner\nw\ne\ncan\npic\nk\na\nlarge\nenough\nto\nsatisfy\nthe\nbase\nconditions\nTh\nus\nPM\nn\n\nn\nwhic\nh\nis\nthe\nsame\nw\nork\nasymptotically\nas\nthe\nordinary\n\nserial\nmerging\nalgorithm\nT\nW\ne\ncan\nno\nw\nreanalyze\nthe\nMer\ngeSor\nt\nusing\nthe\nPMer\nge\nsubroutine\nThe\nw\nork\nn\nremains\nthe\nsame\nbut\nthe\nw\norstcase\ncriticalpath\nlength\nno\nw\nsatises\nT\nn\n\nT\nn\n\nlg\nn\n\nlg\nn\n\nThe\nparallelism\nis\nno\nw\nn\nlg\nnlg\nn\n\nn\nlg\nn\nExercise\n\nGiv\ne\na n\necien\nt\nand\nhighly\nparallel\nm\nultithreaded\nalgorithm\nfor\nm\nultiply\ning\nan\nn\n\nn\nmatrix\nA\nb\ny\na\nlengthn\nv\nector\nx\nthat\nac\nhiev\nes\nw\nork\nn\n\nand\ncritical\npath\nlg\nn\nAnalyze\nthe\nw\nork\nand\ncriticalpath\nlength\nof\ny\nour\nimplemen\ntation\nand\ngiv\ne\nthe\nparallelism\nExercise\n\nDescrib\ne\na\nm\nultithreaded\nalgorithm\nfor\nmatrix\nm\nultiplication\nthat\nac\nhiev\nes\nw\nork\nn\n\nand\ncritical\npath\nlg\nn\nCommen\nt\ninformally\non\nthe\nlo\ncalit\ny\ndispla\ny\ned\nb\ny\ny\nour\nalgorithm\nin\nthe\nideal\ncac\nhe\nmo\ndel\nas\ncompared\nwith\nthe\nt\nw\no\nalgorithms\nfrom\nthis\nsection\nExercise\n\nW\nrite\na\nCilk\nprogram\nto\nm\nultiply\nan\nn\n\nn\nmatrix\nb\ny\nan\nn\n\nn\nmatrix\nin\nparallel\nAnalyze\nthe\nw\nork\ncriticalpath\nlength\nand\nparallelism\nof\ny\nour\nimplemen\ntation\nY\nour\nalgorithm\nshould\nb\ne\necien\nt\nev\nen\nif\nan\ny\nof\nn\n\nn\n\nand\nn\nare\n\nExercise\n\nW\nrite\na\nCilk\nprogram\nto\nimplemen\nt\nStrassens\nmatrix\nm\nultiplication\nal\ngorithm\nin\nparallel\nas\necien\ntly\nas\ny\nou\ncan\nAnalyze\nthe\nw\nork\ncriticalpath\nlength\nand\nparallelism\nof\ny\nour\nimplemen\ntation\nExercise\n\nW\nrite\na\nCilk\nprogram\nto\nin\nv\nert\na\nsymmetric\nand\np\nositiv\nedenite\nmatrix\nin\nparallel\nHint\nUse\na\ndivideandconquer\napproac\nh\nbased\non\nthe\nideas\nof\nTheorem\n\nfrom\n\nExercise\n\nAkl\nand\nSan\ntoro\n\nha\nv\ne\nprop\nosed\na\nmerging\nalgorithm\nin\nwhic\nh\nthe\nrst\nstep\nis\nto\nnd\nthe\nmedian\nof\nall\nthe\nelemen\nts\nin\nthe\nt\nw\no\nsorted\ninput\narra\nys\nas\nopp\nosed\nto\nthe\nmedian\nof\nthe\nelemen\nts\nin\nthe\nlarger\nsubarra\ny\n\nas\nis\ndone\nin\nPMer\nge\nSho\nw\nthat\nif\nthe\ntotal\nn\num\nb\ner\nof\nelemen\nts\nin\nthe\nt\nw\no\narra\nys\nis\nn\nthis\nmedian\ncan\nb\ne\nfound\nusing\nlg\nn\ntime\non\none\npro\ncessor\nin\nthe\nw\norst\ncase\nDescrib\ne\na\nlinearw\nork\nm\nultithreaded\nmerging\nalgorithm\nbased\non\nthis\nsubroutine\nthat\nhas\na\nparallelism\nof\nn\nlg\nn\nGiv\ne\nand\nsolv\ne\nthe\nrecurrences\nfor\nw\nork\nand\ncriticalpath\nlength\nand\ndetermine\nthe\nparallelism\nImplemen\nt\ny\nour\nalgorithm\nas\na\nCilk\nprogram\n\nExercise\n\nGeneralize\nthe\nalgorithm\nfrom\nExercise\nExercise\n\nto\nnd\narbitrary\norder\nstatistics\nDescrib\ne\na\nmergesorting\nalgorithm\nwith\nn\nlg\nn\nw\nork\nthat\nac\nhiev\nes\na\nparallelism\nof\nn\nlg\nn\nHint\nMerge\nman\ny\nsubarra\nys\nin\nparallel\nExercise\n\nThe\nlength\nof\na\nlongestcommon\nsubsequence\nof\nt\nw\no\nlengthn\nsequences\nx\nand\ny\ncan\nb\ne\ncomputed\nin\nparallel\nusing\na\ndivideandconquer\nm\nultithreaded\nalgorithm\nDenote\nb\ny\nci\nj\n\nthe\nlength\nof\na\nlongest\ncommon\nsubsequence\nof\nx\n\ni\nand\ny\n\nj\n\nFirst\nthe\nm\nultithreaded\nalgorithm\nrecursiv\nely\ncomputes\nci\nj\n\nfor\nall\ni\nin\nthe\nrange\n\ni\n\nn\nand\nall\nj\nin\nthe\nrange\n\nj\n\nn\nThen\nit\nrecursiv\nely\ncomputes\nci\nj\n\nfor\n\ni\n\nn\nand\nn\nj\n\nn\nwhile\nin\nparallel\nrecursiv\nely\ncomputing\nci\nj\n\nfor\nn\ni\n\nn\nand\n\nj\n\nn\nFinally\n\nit\nrecursiv\nely\ncomputes\nci\nj\n\nfor\nn\n\ni\n\nn\nand\nn\n\nj\n\nn\nF\nor\nthe\nbase\ncase\nthe\nalgorithm\ncomputes\nci\nj\n\nin\nterms\nof\nci\n\nj\n\nci\n\nj\n\nand\nci\nj\n\nin\nthe\nordinary\nw\na\ny\n\nsince\nthe\nlogic\nof\nthe\nalgorithm\nguaran\ntees\nthat\nthese\nthree\nv\nalues\nha\nv\ne\nalready\nb\ne e n\ncomputed\nThat\nis\nif\nthe\ndynamic\nprogramming\ntableau\nis\nbrok\nen\nin\nto\nfour\npieces\n!\nI\nI\nI\n\nI\nI\nI\nI\nV\nthen\nthe\nrecursiv\ne\nm\nultithreaded\nco\nde\nw\nould\nlo\nok\nsomething\nlik\ne\nt h i s\nspa\nwn\nI\nsync\nspa\nwn\nI\nI\nspa\nwn\nI\nI\nI\nsync\nspa\nwn\nI\nV\nsync\nAnalyze\nthe\nw\nork\ncriticalpath\nlength\nand\nparallelism\nof\nthis\nalgorithm\nDescrib\ne\nand\nanalyze\nan\nalgorithm\nthat\nis\nasymptotically\nas\necien\nt\nsame\nw\nork\nbut\nmore\nparallel\nMak\ne\nwhatev\ner\nin\nteresting\nobserv\nations\ny\nou\ncan\nW\nrite\nan\necien\nt\nCilk\nprogram\nfor\nthe\nproblem\nReferences\n\nSelim\nG\nAkl\nand\nNicola\nSan\ntoro\nOptimal\nparallel\nmerging\nand\nsorting\nwithout\nmemory\nconicts\nIEEE\nT\nr\nansactions\non\nComputers\nC\nNo\nv\nem\nb\ne r\n\nM\nAkra\nand\nL\nBazzi\nOn\nthe\nsolution\nof\nlinear\nrecurrence\nequations\nComputational\nOptimization\nand\nApplic\nation\n\nRob\nert\nD\nBlumofe\nExe\ncuting\nMultithr\ne\nade\nd\nPr\no\ngr\nams\nEciently\nPhD\nthesis\nDe\npartmen\nt\nof\nElectrical\nEngineering\nand\nComputer\nScience\nMassac\nh\nusetts\nInstitute\nof\nT\nec\nhnology\n\nSeptem\nb\ne r\n\nRob\nert\nD\nBlumofe\nChristopher\nF\nJo\nerg\nBradley\nC\nKuszmaul\nCharles\nE\nLeiserson\nKeith\nH\nRandall\nand\nY\nuli\nZhou\nCilk\nAn\necien\nt\nm\nultithreaded\nrun\ntime\nsystem\nIn\nPr\no\nc\ne\ne\ndings\nof\nthe\nFifth\nA\nCM\nSIGPLAN\nSymp\nosium\non\nPrinciples\nand\nPr\nactic\ne\nof\nPar\nal\nlel\nPr\no\ngr\namming\nPPoPP\npages\n\nSan\nta\nBarbara\nCalifornia\nJuly\n\nRob\nert\nD\nBlumofe\nand\nCharles\nE\nLeiserson\nSc\nheduling\nm\nultithreaded\ncomputations\nb\ny\nw\nork\nstealing\nIn\nPr\no\nc\ne\ne\ndings\nof\nthe\nth\nA\nnnual\nSymp\nosium\non\nF\noundations\nof\nComputer\nScienc\ne\nF\nOCS\npages\n\nSan\nta\nF\ne\nNew\nMexico\nNo\nv\nem\nb\ne r\n\nRic\nhard\nP\n\nBren\nt\nThe\nparallel\nev\naluation\nof\ngeneral\narithmetic\nexpressions\nJournal\nof\nthe\nA\nCM\n\nApril\n\nCilk\nBeta\n\nReference\nMan\nual\nAv\nailable\non\nthe\nIn\nternet\nfrom\nhttptheorylcsmitedu\n\ncilk\n\nThomas\nH\nCormen\nCharles\nE\nLeiserson\nand\nRonald\nL\nRiv\nest\nIntr\no\nduction\nto\nA\nlgo\nrithms\nMIT\nPress\nand\nMcGra\nw\nHill\n\nMatteo\nF\nrigo\nCharles\nE\nLeiserson\nand\nKeith\nH\nRandall\nThe\nimplemen\ntation\nof\nthe\nCilk\nm\nultithreaded\nlanguage\nIn\nA\nCM\nSIGPLAN\n\nConfer\nenc\ne\non\nPr\no\ngr\namming\nL\nanguage\nDesign\nand\nImplementation\nPLDI\npages\n\nMon\ntreal\nCanada\nJune\n\nR\nL\nGraham\nBounds\non\nm\nultipro\ncessing\ntiming\nanomalies\nSIAM\nJournal\non\nApplie\nd\nMathematics\n\nMarc\nh\n\nKeith\nH\nRandall\nCilk\nEcient\nMultithr\ne\nade\nd\nComputing\nPhD\nthesis\nDepartmen\nt\no f\nElectrical\nEngineering\nand\nComputer\nScience\nMassac\nh\nusetts\nInstitute\nof\nT\nec\nhnology\n\nMa\ny"
    },
    {
      "category": "Resource",
      "title": "sort_code.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/697ac6434b96c75a9e35f55799077796_sort_code.pdf",
      "content": "!\n\n\"\n\n#\n$\n\n%\n&\n'\n(\n\n)*+\n,**#\n\n!\"\n\n##\n\n!$%\n\n$%\n\n!&\n\n##\n\n!$%\n\n$%!\n!\n'\n'\n$%!\n'\n'\n\n,\n\n(\n\n)\n\n*\n\n+\n\n, )\n\n*\n\n-./(0\n\n)!\n!\n!)\n'\n\n&!$&%\n\n\"!$\"%\n\n1!$1%\n\n2!$2%\n\n3!$3%\n\n)\n\n,5\n\n-./(0 \" 1\n-./(0 2 3\n-./(0 \" 2\n-./(0 & 1\n-./(0 1 3\n-./(0 & 2\n-./(0 & \"\n-./(0 1 2\n-./(0 \" 1\n\n& \"\n\" 1\n1 2\n2 3\n$&%! &\n$\"%! \"\n$1%! 1\n$2%! 2\n$3%! 3\n'\n\n#\n\n#\n\n\"\n'\n\n:\n!\n*\n;<!\"&&\n4<(+.!\"&&&&&&&\n'\n\n$%\n\n=$;<:%\n>$:%\n\n!&\n\n,\n\n!!\"\n\n!&\n;<:\n##\n=$%!\n\n;\n\n,\n\n,!&\n,1\n,##\n\n<?.(@A8.A+B\n\n!&\n4<(+.\n##\n\n)>\n=#\n\nC :\n#!:\n\n!;<:\n!&\n'\n\n<?.(@A8.A+B\n'\n! 6\n\n,!&\n,1\n,##\n\n<?.(@A8.A+B\n\n!&\n4<(+.\n##\n\n)>\n=#\n\nC :\n#!:\n\n!;<:\n!&\n\n>:\n'\n\n<?.(@A8.A+B\n'\n) DE\n\nE\n\nE FD\n$&%\n:\n\n:!!*\n\n,!&\n,1\n,##\n\n<?.(@A8.A+B\n\n-\n\n!&\n4<(+.\n##\n\n)>\n=#\n\nC :\n#!:\n\n!;<:\n!&\n\n*>\n'\n\n<?.(@A8.A+B\n'\n) DE\n\n*\n\nE FD\n$&%\n\n&\n'\n\n*G\n&&\n\n*\n\nH\n)\n&\nH2\"\n\n\"\n1I\n1\"\n1J\n1I\n\n*\n\nKK\"\n*G\n&&\nL\nL\nKKG\"\nL\n!\n\"&&&&&\n\nL\n\n\"\nM\n2&\n\n&!$&%\n2\"\n\n\"!$\"%\n\n1!$1%\n\n2!$2%\n\n3!$3%\n,\nH*\"9H3\n$&%\nG\"\"&\n\n\"\nM\n,\nH13\"1H3\n$\"%\nG\"&M\n\n\"\n2\"\nM\n,\nH\"33H3\n$1%\nG\"&J\n\n\"\nM\n,\nH\"2IH3\n$2%\nG\"&I\n\n\"\n2I\n*\n2*\n\n)\n\n;\n\n2J\n-./(0 \" 1\n2I\n-./(0 2 3\n\nHIH13H&\n$2%\n\n\"\n2J\n*\n\nH\"*H*H13\n$2%\n\nH\"\"H\"3H&\n$3%\nC\nHIH*H\"*\n$3%\n\n\"\nM\n\nH\"1H\"2H\"3\n$*%\nC\nH*H13H\"*\n$*%\nC\nH\"\"H\"2H\"1\n$9%\n\n\"\n2&\nM\n,\nHJ&H3\n$J%\nG\"&9\nC\nH\"2H\"3H\"1\n$J%\n\n\"\n2M\n*\n2M\n-./(0 \" 2\n\nH2H\"2H&\n$I%\n\n\"\n2I\n*\n\nH\"1H*H\"2\n$I%\n\n\"\n3&\n*\n3&\n-./(0 & 1\n\nH1*HJH&\n$M%\nC\nH2H*H\"1\n$M%\nC\nH*H\"2H\"1\n$\"&%\n\n\"\n2M\n*\n\nH9H\"\"HJ\n$\"&%\nC\nH1*H\"\"H9\n$\"\"%\nC\nH\"\"HJH9\n$\"1%\n\n\"\n*\n3\"\n-./(0 1 3\n-./(0 & 2\n\nH\"H\"\"H&\n$\"2%\n\n\"\n3\"\n*\n\nH9H2H\"\"\n$\"2%\n\nH\"&H1*H&\n$\"3%\nC\nH\"H2H9\n$\"3%"
    },
    {
      "category": "Resource",
      "title": "herlihy_mo93.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/edd23e9638c4675cffce1341254837f1_herlihy_mo93.pdf",
      "content": "Transactional Memory:\nArchitectural Support for Lock-Free Data Structures\nMaurice Herlihy\nDigital Equipment Corporation\nCambridge Research Laboratory\nCambridge MA 02139\nAbstract\nA shared data structure is lock-free if its operations do not\nrequire mutual exclusion. If one process is interrupted in\nthe middle of an operation, other processes will not be\nprevented from operating on that object. In highly con\ncurrent systems, lock-free data structures avoid common\nproblems associated with conventional locking techniques,\nincluding priority inversion, convoying, and difficulty of\navoiding deadlock. This paper introduces transactional\nmemory, a new multiprocessor architecture intended to\nmake lock-free synchronization as efficient (and easy to\nuse) as conventional techniques based on mutual exclu\nsion. Transactional memory allows programmers to de\nfine customized read-modify-write operations that apply\nto multiple, independently-chosen words of memory. It\nis implemented by straightforward extensions to any mul\ntiprocessor cache-coherence protocol. Simulation results\nshow that transactional memory matches or outperforms\nthe best known locking techniques for simple benchmarks,\neven in the absence of priority inversion, convoying, and\ndeadlock.\n1 Introduction\nA shared data structure is lock-free if its operations do not\nrequire mutual exclusion. If one process is interrupted in\nthe middle of an operation, other processes will not be\nprevented from operating on that object. Lock-free data\nJ. Eliot B. Moss\nDept. of Computer Science\nUniversity of Massachusetts\nAmherst, MA 01003\nstructures avoid common problems associated with con\nventional locking techniques in highly concurrent systems:\n\nPriority inversion occurs when a lower-priority pro\ncess is preempted while holding a lock needed by\nhigher-priority processes.\n\nConvoying occurs when a process holding a lock is de-\nscheduled, perhaps by exhausting its scheduling quan\ntum, by a page fault, or by some other kind of interrupt.\nWhen such an interruption occurs, other processes ca\npable of running may be unable to progress.\n\nDeadlock can occur if processes attempt to lock the\nsame set of objects in different orders. Deadlock\navoidance can be awkward if processes must lock mul\ntiple data objects, particularly if the set of objects is\nnot known in advance.\nA number of researchers have investigated techniques for\nimplementing lock-free concurrent data structures using\nsoftware techniques [2, 4, 19, 25, 26, 32]. Experimental\nevidence suggests that in the absence of inversion, con\nvoying, or deadlock, software implementations of lock-\nfree data structures often do not perform as well as their\nlocking-based counterparts.\nThis paper introduces transactional memory, a new mul\ntiprocessor architecture intended to make lock-free syn\nchronization as efficient (and easy to use) as conventional\ntechniques based on mutual exclusion. Transactional mem\nory allows programmers to define customized read-modify-\nwrite operations that apply to multiple, independently-\nchosen words of memory. It is implemented by straightfor\nward extensions to multiprocessor cache-coherence proto\ncols. Simulation results show that transactional memory is\ncompetitive with the best known lock-based techniques for\nsimple benchmarks, even in the absence of priority inver\nsion, convoys, and deadlock.\nIn Section 2, we describe transactional memory and how\nto use it. In Section 3 we describe one way to implement\ntransactional memory, and in Section 4 we discuss some\nPage 1\n\nalternatives. In Section 5 we present some simulation re\nsults, and in Section 6, we give a brief survey of related\nwork.\n2 Transactional Memory\nA transaction is a finite sequence of machine instructions,\nexecuted by a single process, satisfying the following prop\nerties:\n\nSerializability: Transactions appear to execute seri\nally, meaning that the steps of one transaction never\nappear to be interleaved with the steps of another.\nCommitted transactions are never observed by differ\nent processors to execute in different orders.\n\nAtomicity: Each transaction makes a sequence of\ntentative changes to shared memory.\nWhen the\ntransaction completes, it either commits, making its\nchanges visible to other processes (effectively) in\nstantaneously, or it aborts, causing its changes to be\ndiscarded.\nWe assume here that a process executes only one trans\naction at a time. Although the model could be extended\nto permit overlapping or logically nested transactions, we\nhave seen no examples where they are needed.\n2.1 Instructions\nTransactional memory provides the following primitive in\nstructions for accessing memory:\n\nLoad-transactional (LT) reads the value of a shared\nmemory location into a private register.\n\nLoad-transactional-exclusive (LTX) reads the value of\na shared memory location into a private register, \"hint\ning\" that the location is likely to be updated.\n\nStore-transactional (ST) tentatively writes a value\nfrom a private register to a shared memory location.\nThis new value does not become visible to other pro\ncessors until the transaction successfully commits (see\nbelow).\nA transaction's read set is the set of locations read by LT,\nand its write set is the set of locations accessed by LTX or\nST. Its data set is the union of the read and write sets.\nTransactional memory also provides the following in\nstructions for manipulating transaction state:\n\nCommit (COMMIT) attempts to make the transaction's\ntentative changes permanent. It succeeds only if no\nother transaction has updated any location in the trans-\naction's data set, and no other transaction has read any\nlocation in this transaction's write set. If it succeeds,\nthe transaction's changes to its write set become vis\nible to other processes. If it fails, all changes to the\nwrite set are discarded. Either way, COMMIT returns\nan indication of success or failure.\n\nAbort (ABORT) discards all updates to the write set.\n\nValidate (VALIDATE) tests the current transaction sta\ntus. A successful VALIDATE returns True, indicating\nthat the current transaction has not aborted (although\nit may do so later). An unsuccessful VALIDATE re\nturns False, indicating that the current transaction has\naborted, and discards the transaction's tentative up\ndates.\nBy combining these primitives, the programmer can de\nfine customized read-modify-write operations that operate\non arbitrary regions of memory, not just single words. We\nalso support non-transactional instructions, such as LOAD\nand STORE, which do not affect a transaction's read and\nwrite sets.\nFor brevity, we leave undefined how transactional and\nnon-transactional operations interact when applied to the\nsame location.1 We also leave unspecified the precise cir\ncumstances that will cause a transaction to abort. In par\nticular, implementations are free to abort transactions in\nresponse to certain interrupts (such as page faults, quantum\nexpiration, etc.), context switches, or to avoid or resolve\nserialization conflicts.\n2.2 Intended Use\nOur transactions are intended to replace short critical sec\ntions. For example, a lock-free data structure would typ\nically be implemented in the following stylized way (see\nSection 5 for specific examples). Instead of acquiring a\nlock, executing the critical section, and releasing the lock,\na process would:\n1. use LT or LTX to read from a set of locations,\n2. use VALIDATE to check that the values read are consis\ntent,\n3. use ST to modify a set of locations, and\n4. use COMMIT to make the changes permanent. If either\nthe VALIDATE or the COMMIT fails, the process returns\nto Step (1).\n1One sensible way to define such interactions is to consider a LOAD\nor STORE as a transaction that always commits, forcing any conflicting\ntransactions to abort.\nPage 2\n\nA more complex transaction, such as one that chains down a\nlinked list (see Figure 3), would alternate LT and VALIDATE\ninstructions. When contention is high, programmers are\nadvised to apply adaptive backoff [3, 28] before retrying.\nThe VALIDATE instruction is motivated by considerations\nof software engineering. A set of values in memory is in\nconsistent if it could not have been produced by any serial\nexecution of transactions. An orphan is a transaction that\ncontinues to execute after it has been aborted (i.e., after\nanother committed transaction has updated its read set). It\nis impractical to guarantee that every orphan will observe\na consistent read set. Although an orphan transaction will\nnever commit, it may be difficult to ensure that an orphan,\nwhen confronted with unexpected input, does not store into\nout-of-range locations, divide by zero, or perform some\nother illegal action. All values read before a successful\nVALIDATE are guaranteed to be consistent. Of course, VAL\nIDATE is not always needed, but it simplifies the writing of\ncorrect transactions and improves performance by elimi\nnating the need for ad-hoc checks.\nOur transactions satisfy the same formal serializability\nand atomicity properties as database-style transactions (viz.\n[18]), but they are intended to be used very differently. Un\nlike database transactions, our transactions are short-lived\nactivities that access a relatively small number of memory\nlocations in primary memory. The ideal size and duration\nof transactions are implementation-dependent, but, roughly\nspeaking, a transaction should be able to run to completion\nwithin a single scheduling quantum, and the number of\nlocations accessed should not exceed an architecturally-\nspecified limit.\n3 Implementation\nIn this section, we give an overview of an architecture\nthat supports transactional memory. An associated tech\nnical report [20] gives detailed protocols for both bus-\nbased (snoopy cache) and network-based (directory) ar\nchitectures.\nOur design satisfies the following criteria:\n\nNon-transactional operations use the same caches,\ncache controller logic, and coherence protocols they\nwould have used in the absence of transactional mem\nory.\n\nCustom hardware support is restricted to primary\ncaches and the instructions needed to communicate\nwith them.\n\nCommitting or aborting a transaction is an operation\nlocal to the cache. It does not require communicating\nwith other processes or writing data back to memory.\nTransactional memory is implemented by modifying\nstandard multiprocessor cache coherence protocols. We\nexploit access rights, which are usually connected with\ncache residence. In general, access may be non-exclusive\n(shared) permitting reads, or exclusive, permitting writes.\nAt any time a memory location is either (1) not immedi\nately accessible by any processor (i.e., in memory only), (2)\naccessible non-exclusively by one or more processors, or\n(3) accessible exclusively by exactly one processor. Most\ncache coherence protocols incorporate some form of these\naccess rights.\nThe basic idea behind our design is simple: any protocol\ncapable of detecting accessibility conflicts can also detect\ntransaction conflict at no extra cost. Before a processor\ncan load the contents of a location, it must acquire non\nexclusive access to that location. Before another processor\n\ncan store to that location, it must acquire exclusive ac-\ncess, and must therefore detect and revoke\n's access. If\nwe replace these operations with their transactional coun\nterparts, then it is easy to see that any protocol that detects\npotential access conflicts also detects the potential transac-\n\ntion conflict between\nand\n.\nOnce a transaction conflict is detected, it can be re\nsolved in a variety of ways. The implementation described\nhere aborts any transaction that tries to revoke access of a\ntransactional entry from another active transaction. This\nstrategy is attractive if one assumes (as we do) that timer\n(or other) interrupts will abort a stalled transaction after a\nfixed duration, so there is no danger of a transaction holding\nresources for too long. Alternative strategies are discussed\nin [20].\n3.1 Example implementation\nWe describe here how to extend Goodman's \"snoopy\" pro\ntocol for a shared bus [15] to support transactional memory.\n(See [20] for similar extensions to a directory-based proto\ncol.) We first describe the general implementation strategy,\nthe various cache line states, and possible bus cycles. We\nthen describe the various possible actions of the processor\nand the bus snooping cache logic.\n3.1.1 General approach\nTo minimize impact on processing non-transactional loads\nand stores, each processor maintains two caches: a regular\ncache for non-transactional operations, and a transactional\ncache for transactional operations. These caches are ex\nclusive: an entry may reside in one or the other, but not\nboth. Both caches are primary caches (accessed directly\nby the processor), and secondary caches may exist between\nthem and the memory. In our simulations, the regular cache\nis a conventional direct-mapped cache. The transactional\nPage 3\n\nName\nINVALID\nVALID\nDIRTY\nRESERVED\nAccess\nnone\nR\nR, W\nR, W\nShared?\n--\nYes\nNo\nNo\nModified?\n--\nNo\nYes\nNo\nName\nREAD\nRFO\nWRITE\nT READ\nT RFO\nBUSY\nKind\nregular\nregular\nboth\ntrans\ntrans\ntrans\nMeaning\nread value\nread value\nwrite back\nread value\nread value\nrefuse access\nNew access\nshared\nexclusive\nexclusive\nshared\nexclusive\nunchanged\nTable 1: Cache line states\nTable 3: Bus cycles\nName\nEMPTY\nNORMAL\nXCOMMIT\nXABORT\nMeaning\ncontains no data\ncontains committed data\ndiscard on commit\ndiscard on abort\nTable 2: Transactional tags\ncache is a small, fully-associative cache with additional\nlogic to facilitate transaction commit and abort. The over\nall hardware organization is similar to that used by Jouppi\nfor the victim cache [22], and indeed one can readily extend\nthe transactional cache to act as a victim cache as well.\nThe idea is that the transactional cache holds all the\ntentative writes, without propagating them to other proces\nsors or to main memory unless the transaction commits. If\nthe transaction aborts, the lines holding tentative writes are\ndropped (invalidated); if the transaction commits, the lines\nmay then be snooped by other processors, written back to\nmemory upon replacement, etc. We assume that since the\ntransactional cache is small and fully associative it is prac\ntical to use parallel logic to handle abort or commit in a\nsingle cache cycle.\n3.1.2 Cache line states\nFollowing Goodman, each cache line (regular or trans\nactional) has one of the states in Table 1. The possible\naccesses permitted are reads and/or writes; the \"Shared?\"\ncolumn indicates whether sharing is permitted; and the\n\"Modified?\" column indicates whether the line may differ\nfrom its copy in main memory.\nThe transactional cache augments these states with sep\narate transactional tags shown in Table 2, used as follows.\nTransactional operations cache two entries: one with trans\nactional tag XCOMMIT and one XABORT. Modifications are\nmade to the XABORT entry. When a transaction commits,\nit sets the entries marked XCOMMIT to EMPTY, and XABORT\nto NORMAL. When it aborts, it sets entries marked XABORT\nto EMPTY, and XCOMMIT to NORMAL.\nWhen the transactional cache needs space for a new en\ntry, it first searches for an EMPTY entry, then for a NORMAL\nentry, and finally for an XCOMMIT entry. If the XCOMMIT\nentry is DIRTY, it must be written back. Notice that XCOM\nMIT entries are used only to enhance performance. When\na ST tentatively updates an entry, the old value must be\nretained in case the transaction aborts. If the old value is\nresident in the transactional cache and dirty, then it must\neither be marked XCOMMIT, or it must be written back to\nmemory. Avoiding such write-backs can substantially en\nhance performance when a processor repeatedly executes\ntransactions that access the same locations. If contention is\nlow, then the transactions will often hit dirty entries in the\ntransactional cache.\n3.1.3 Bus cycles\nThe various kinds of bus cycles are listed in Table 3. The\nREAD (RFO (read-for-ownership)) cycle acquires shared (ex\nclusive) ownership of the cache line. The WRITE cycle up\ndates main memory when the protocol does write through;\nit is also used when modified items are replaced. Further,\nmemory snoops on the bus so if a modified item is read\nby another processor, the main memory version is brought\nup to date. These cycles are all as in Goodman's original\nprotocol. We add three new cycles. The T READ and T RFO\ncycles are analogous to READ and RFO, but request cache\nlines transactionally. Transactional requests can be refused\nby responding with a BUSY signal. BUSY helps prevent\ntransactions from aborting each other too much. When\na transaction receives a BUSY response, it aborts and re\ntries, preventing deadlock or continual mutual aborts. This\npolicy is theoretically subject to starvation, but could be\naugmented with a queueing mechanism if starvation is a\nproblem in practice.\n3.1.4 Processor actions\nEach processor maintains two flags: the transaction ac\ntive (TACTIVE) flag indicates whether a transaction is in\nprogress, and if so, the transaction status (TSTATUS) flag in\ndicates whether that transaction is active (True) or aborted\nPage 4\n\n(False). The TACTIVE flag is implicitly set when a transac\ntion executes its first transactional operation. (This implicit\napproach seems more convenient than providing an explicit\nstart transaction instruction.) Non-transactional operations\nbehave exactly as in Goodman's original protocol. Trans\nactional instructions issued by an aborted transaction cause\nno bus cycles and may return arbitrary values.2\nWe now consider transactional operations issued by an\nactive transaction (TSTATUS is True). Suppose the operation\nis a LT instruction. We probe the transactional cache for an\nXABORT entry, and return its value if there is one. If there is\nno XABORT entry, but there is a NORMAL one, we change the\nNORMAL entry to an XABORT entry, and allocate a second\nentry with tag XCOMMIT and the same data.3 If there is no\nor NORMAL\nT READ\nXABORT\nentry, then we issue a\ncycle. If\nit completes successfully, we set up two transactional cache\nentries, one tagged XCOMMIT and one XABORT, both with\nwhatever state the Goodman protocol would get on a READ\ncycle. If we get a BUSY response, we abort the transaction\n(set TSTATUS to False, drop all XABORT entries, and set all\nXCOMMIT entries to NORMAL) and return arbitrary data.\nFor\nT RFO\nT READ\nLTX we use a\ncycle on a miss rather than a\n, and change the cache line state to RESERVED if the\nT RFO\nST\nsucceeds. A\nproceeds like a LTX, except it updates\nthe XABORT entry's data. The cache line state is updated as\nin the Goodman protocol with LT and LTX acting like LOAD\nand ST acting like STORE.\nThe VALIDATE instruction returns the TSTATUS flag, and\nif it is False, sets the TACTIVE flag to False and the TSTATUS\nflag to True. The ABORT instruction discards cache en\ntries as previously described, and sets TSTATUS to True and\nTACTIVE to False. Finally, COMMIT returns TSTATUS, sets\nTSTATUS to True and TACTIVE to False, drops all XCOMMIT\ncache entries, and changes all XABORT tags to NORMAL.\nInterrupts and transactional cache overflows abort the\ncurrent transaction.\n3.1.5 Snoopy cache actions\nBoth the regular cache and the transactional cache snoop\non the bus. A cache ignores any bus cycles for lines not\nin that cache. The regular cache behaves as follows. On a\nREAD or T READ, if the state is VALID, the cache returns the\nvalue. If the state is RESERVED or DIRTY, the cache returns\nRFO or T RFO,\nthe value and resets the state to VALID. On a\nthe cache returns the data and invalidates the line.\nThe transactional cache behaves as follows. If TSTATUS\nis False, or if the cycle is non-transactional (READ and\n2As discussed below in Section 4, it is possible to provide stronger\nguarantees on values read by aborted transactions.\n3Different variations are possible here. Also, allocating an entry may\ninvolve replacing a dirty cache entry, in which case it must be written\nback, as previously mentioned.\nRFO), the cache acts just like the regular cache, except that\nit ignores entries with transactional tag other than NORMAL.\nOn T READ, if the state is VALID, the cache returns the value,\nand for all other transactional operations it returns BUSY.\nEither cache can issue a WRITE request when it needs to\nreplace a cache line. The memory responds only to READ,\nT READ, RFO\nT RFO\n, and\nrequests that no cache responds\nto, and to WRITE requests.\n4 Rationale\nIt would be possible to use a single cache for both trans\nactional and non-transactional data. This approach has\ntwo disadvantages: (1) modern caches are usually set as\nsociative or direct mapped, and without additional mech\nanisms to handle set overflows, the set size would deter\nmine the maximum transaction size, and (2) the parallel\ncommit/abort logic would have to be provided for a large\nprimary cache, instead of the smaller transactional cache.\nFor programs to be portable, the instruction set archi\ntecture must guarantee a minimum transaction size, thus\nestablishing a lower bound for the transactional cache size.\nAn alternative approach is suggested by the LimitLESS\ndirectory-based cache coherence scheme of Chaiken, Ku\nbiatowicz, and Agarwal [6]. This scheme uses a fast, fixed-\nsize hardware implementation for directories. If a directory\noverflows, the protocol traps into software, and the software\nemulates a larger directory. A similar approach might be\nused to respond to transactional cache overflow. Whenever\nthe transactional cache becomes full, it traps into software\nand emulates a larger transactional cache. This approach\nhas many of the same advantages as the original LimitLESS\nscheme: the common case is handled in hardware, and the\nexceptional case in software.\nOther transactional operations might be useful. For\nexample, a simple \"update-and-commit\" operation (like\nSTORE COND) would be useful for single-word updates. It\nmight also be convenient for a transaction to be able to\ndrop an item from its read or write set. Naturally, such an\noperation must be used with care.\nOne could reduce the need for VALIDATE instructions by\nguaranteeing that an orphan transaction that applies a LT or\nLTX instruction to a variable always observes some value\npreviously written to that variable. For example, if a shared\nvariable always holds a valid array index, then it would not\nbe necessary to validate that index before using it. Such\na change would incur a cost, however, because an orphan\ntransaction might sometimes have to read the variable's\nvalue from memory or another processor's cache.\nPage 5\n\n5 Simulations\nTransactional memory is intended to make lock-free\nsynchronization as efficient as conventional lock-based\ntechniques. In this section, we present simulation results\nsuggesting that transactional memory is competitive with\nwell-known lock-based techniques on simple benchmarks.\nIndeed, transactional memory has certain inherent advan\ntages: for any object that spans more than a single word of\nmemory, techniques based on mutual exclusion must em\nploy an explicit lock variable. Because transactional mem\nory has no such locks, it typically requires fewer memory\naccesses.\nWe modified a copy of the Proteus simulator [5] to\nsupport transactional memory. Proteus is an execution-\ndriven simulator system for multiprocessors developed by\nEric Brewer and Chris Dellarocas of MIT. The program to\nbe simulated is written in a superset of C. References to\nshared memory are transformed into calls to the simulator,\nwhich manages the cache and charges for bus or network\ncontention. Other instructions are executed directly, aug\nmented by cycle-counting code inserted by a preprocessor.\nProteus does not capture the effects of instruction caches\nor local caches.\nWe implemented two versions of transactional memory,\none based on Goodman's snoopy protocol for a bus-based\narchitecture, and one based on the Chaiken directory pro\ntocol for a (simulated) Alewife machine [1]. Our motive\nin choosing these particular protocols was simply ease of\nimplementation: the Proteus release includes implementa\ntions of both. As noted below, a more complex snoopy\nprotocol could make spin locks more efficient.\nBoth simulated architectures use 32 processors. The\nregular cache is a direct-mapped cache with 2048 lines\nof size 8 bytes, and the transactional cache has 64 8-byte\nlines. In both architectures, a memory access (without\ncontention) requires 4 cycles. The network architecture\nuses a two-stage network with wire and switch delays of 1\ncycle each.\nThe ability to commit and abort transactions quickly is\ncritical to the performance of transactional memory. In\nour simulations, each access to the regular or transac\ntional cache, including transaction commit and abort, is\ncounted as a single cycle. Single-cycle commit requires\nthat the transactional cache provide logic to reset the trans\nactional tag bits in parallel. Moreover, commit must not\nforce newly-committed entries back to memory. Instead,\nin the implementations simulated here, committed entries\nare gradually replaced as they are evicted or invalidated by\nthe ongoing cache coherence protocol.\nWe constructed three simple benchmarks, and com\npared transactional memory against two software mech\nanisms and two hardware mechanisms.\nThe software\nshared int counter;\nvoid process(int work)\n{\nint success = 0, backoff = BACKOFF_MIN;\nunsigned wait;\nwhile (success < work) {\nST(&counter, LTX(&counter) + 1);\nif (COMMIT()) {\nsuccess++;\nbackoff = BACKOFF_MIN;\n}\nelse {\nwait = random() % (01 << backoff);\nwhile (wait--);\nif (backoff < BACKOFF_MAX)\nbackoff++;\n}\n}\n}\nFigure 1: Counting Benchmark\ntypedef struct {\nWord deqs;\nWord enqs;\nWord items[QUEUE_SIZE];\n} queue;\nunsigned queue_deq(queue *q) {\nunsigned head, tail, result;\nunsigned backoff = BACKOFF_MIN\nunsigned wait;\nwhile (1) {\nresult = QUEUE_EMPTY;\ntail = LTX(&q->enqs);\nhead = LTX(&q->deqs);\n/* queue not empty? */\nif (head != tail) {\nresult =\nLT(&q->items[head % QUEUE_SIZE]);\n/* advance counter */\nST(&q->deqs, head + 1);\n}\nif (COMMIT()) break;\n/* abort => backoff */\nwait = random() % (01 << backoff);\nwhile (wait--);\nif (backoff < BACKOFF_MAX)\nbackoff++;\n}\nreturn result;\n}\nFigure 2: Part of Producer/Consumer Benchmark\nPage 6\n\ntypedef struct list_elem{\n/* next to dequeue */\nstruct list_elem *next;\n/* previously enqueued */\nstruct list_elem *prev;\nint value;\n} entry;\nshared entry *Head, *Tail;\nvoid list_enq(entry* new) {\nentry *old_tail;\nunsigned backoff = BACKOFF_MIN;\nunsigned wait;\nnew->next = new->prev = NULL;\nwhile (TRUE) {\nold_tail = (entry*) LTX(&Tail);\nif (VALIDATE()) {\nST(&new->prev, old_tail);\nif (old_tail == NULL) {\nST(&Head, new);\n} else {\nST(&old_tail->next, new);\n}\nST(&Tail, new);\nif (COMMIT()) return;\n}\nwait = random() % (01 << backoff);\nwhile (wait--);\nif (backoff < BACKOFF_MAX)\nbackoff++;\n}\nFigure 3: Part of Doubly-Linked List Benchmark\nmechanisms were (1) test-and-test-and-set (TTS) [30] spin\nlocks with exponential backoff [3, 28], and (2) software\nqueueing [3, 17, 27]. The hardware mechanisms were\n(1) LOAD LINKED/STORE COND (LL/SC) with exponential\nbackoff, and (2) hardware queueing [16]. For a single-word\ncounter benchmark, we ran the LL/SC implementation di\nrectly on the shared variable, while on the others we used\nLL/SC to implement a spin lock. Both software mech\nanisms perform synchronization in-line, and all schemes\nthat use exponential backoff use the same fixed minimum\nand maximum backoff durations. We now give a brief\nreview of these techniques.\nA spin lock is perhaps the simplest way to implement\nmutual exclusion. Each processor repeatedly applies a\ntest-and-set operation until it succeeds in acquiring the\nlock. As discussed in more detail by Anderson [3], this\nna ıve technique performs poorly because it consumes ex\ncessive amounts of processor-to-memory bandwidth. On a\ncache-coherent architecture, the test-and-test-and-set [30]\nprotocol achieves somewhat better performance by repeat\nedly rereading the cached value of the lock (generating no\nmemory traffic), until it observes the lock is free, and then\napplying the test-and-set operation directly to the lock in\nmemory. Even better performance is achieved by introduc\ning an exponential delay after each unsuccessful attempt\nto acquire a lock [3, 27]. Because Anderson and Mellor-\nCrummey et al. have shown that TTS locks with expo\nnential backoff substantially outperform conventional TTS\nlocks on small-scale machines, it is a natural choice for our\nexperiments.\nThe LL operation copies the value of a shared variable\nto a local variable. A subsequent SC to that variable will\nsucceed in changing its value only if no other process has\nmodified that variable in the interim. If the operation does\nnot succeed, it leaves the shared variable unchanged. The\nLL/SC operations are the principal synchronization primi\ntives provided by the MIPS II architecture [29] and Digital's\nAlpha [31]. On a cache-coherent architecture, these oper\nations are implemented as single-word transactions -- a\nSC succeeds if the processor retains exclusive access to the\nentry read by the LL.\nIn software queuing, a process that is unable to acquire\na lock places itself on a software queue, thus eliminating\nthe need to poll the lock. Variations of queue locks have\nbeen proposed by Anderson [3], by Mellor-Crummey and\nScott [27], and by Graunke and Thakkar [17]. Our simula\ntions use the algorithm of Mellor-Crummey and Scott. In\nhardware queuing, queue maintenance is incorporated into\nthe cache coherence protocol itself. The queue's head is\nkept in memory, and unused cache lines are used to hold\nthe queue elements. The directory-based scheme must also\nkeep the queue tail in memory. Our simulations use a\nPage 7\n\nqueuing scheme roughly based on the QOSB mechanism\nof Goodman et al. [16].\n5.1 Counting Benchmark\nIn our first benchmark (code in Figure 1), each of\npro-\ncesses increments a shared counter 216\n\ntimes, where\nranges from 1 to 32. In this benchmark, transactions and\ncritical sections are very short (two shared memory ac\ncesses) and contention is correspondingly high. In Figure\n4, the vertical axis shows the number of cycles needed to\ncomplete the benchmark, and the horizontal axis shows the\nnumber of concurrent processes. With one exception, trans\nactional memory has substantially higher throughput than\nany of the other mechanisms, at all levels of concurrency,\nfor both bus-based and directory-based architectures. The\nexplanation is simple: transactional memory uses no ex\nplicit locks, and therefore requires fewer accesses to shared\nmemory. For example, in the absence of contention, the\nTTS spin lock makes at least five references for each in\ncrement (a read followed by a test-and-set to acquire the\nlock, the read and write in the critical section, and a write to\nrelease the lock). Similar remarks apply to both software\nand hardware queueing.\nBy contrast, transactional memory requires only three\nshared memory accesses (the read and write to the counter,\nand the commit, which goes to the cache but causes no\nbus cycles). The only implementation that outperforms\ntransactional memory is one that applies LL/SC directly to\nthe counter, without using a lock variable. Direct LL/SC\nrequires no commit operation, and thus saves a cache ref\nerence. In the other benchmarks, however, this advantage\nis lost because the shared object spans more than one word,\nand therefore the only way to use LL/SC is as a spin lock.\nSeveral other factors influence performance. Our im\nplementation of hardware queuing suffers somewhat from\nthe need to access memory when adjusting the queue at the\nbeginning and end of each critical section, although this\ncost might be reduced by a more sophisticated implemen\ntation. In the bus architecture, the TTS spin lock suffers\nbecause of an artifact of the particular snoopy cache proto\ncol we adapted [15]: the first time a location is modified, it\nis marked reserved and written back. TTS would be more\nefficient with a cache protocol that leaves the location dirty\nin the cache.\n5.2 Producer/Consumer Benchmark\nIn the producer/consumer benchmark (code in Figure 2),\n\nprocesses share a bounded FIFO buffer, initially empty.\nHalf of the processes produce items, and half consume\nthem. The benchmark finishes when 216 operations have\ncompleted. In the bus architecture (Figure 5), all through\nputs are essentially flat. Transactional memory has higher\nthroughputs than the others, although the difference is not\nas dramatic as in the counting benchmark. In the network\narchitecture, all throughputs suffer somewhat as contention\nincreases, although the transactional memory implementa\ntions suffers least.\n5.3 Doubly-Linked List Benchmark\nIn the doubly-linked list benchmark (code in Figure 3)\nprocesses share a doubly-linked list anchored by head and\ntail pointers. Each process dequeues an item by removing\nthe item pointed to by tail, and then enqueues it by threading\nit onto the list at head. A process that removes the last item\nsets both head and tail to NULL, and a process that inserts\nan item into an empty list sets both head and tail to point to\nthe new item. The benchmark finishes when 216 operations\nhave completed.\nThis example is interesting because it has potential con\ncurrency that is difficult to exploit by conventional means.\nWhen the queue is non-empty, each transaction modifies\nhead or tail, but not both, so enqueuers can (in principle) ex\necute without interference from dequeuers, and vice-versa.\nWhen the queue is empty, however, transactions must mod\nify both pointers, and enqueuers and dequeuers conflict.\nThis kind of state-dependent concurrency is not realizable\n(in any simple way) using locks, since an enqueuer does\nnot know if it must lock the tail pointer until after it has\nlocked the head pointer, and vice-versa for dequeuers. If an\nenqueuer and dequeuer concurrently find the queue empty,\nthey will deadlock. Consequently, our locking implemen\ntations use a single lock. By contrast, the most natural way\nto implement the queue using transactional memory per\nmits exactly this parallelism. This example also illustrates\nhow VALIDATE is used to check the validity of a pointer\nbefore dereferencing it.\nThe execution times appear in Figure 6. The locking\nimplementations have substantially lower throughput, pri\nmarily because they never allow enqueues and dequeues to\noverlap.\n5.4 Limitations\nOur implementation relies on the assumption that transac\ntions have short durations and small data sets. The longer a\ntransaction runs, the greater the likelihood it will be aborted\nby an interrupt or synchronization conflict4. The larger the\ndata set, the larger the transactional cache needed, and (per\nhaps) the more likely a synchronization conflict will occur.\n4The identical concerns apply to current implementations of the\nLOAD LINKED and STORE COND instructions [31, Appendix A].\nPage 8\n\nTTS Lock\n\nMCS Lock\n\nQOSB\n\nTrans. Mem.\n\nLL/SC Direct\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n\n|\n|\n|\n|\n|\n|\n|\n|\n\nElapsed Time (in cycles x 1000)\nElapsed Time (in cycles 1000)\n\n|\n\n|\n|\nConcurrency\n|\nConcurrency\nFigure 4: Counting Benchmark: Bus and Network\n|\n|\n|\n|\n|\n|\nSuch size and length restrictions are reasonable for applica\ntions that would otherwise have used short critical sections,\nbut not for applications that would otherwise lock large ob\njects for a long time (such as navigating a B-link tree with a\nlarge node size). Support for larger and longer transactions\nwould require more elaborate hardware mechanisms.\nThe implementation described here does not guarantee\nforward progress, relying instead on software-level adap\ntive backoff to reduce the abort rate by spacing out con\nflicting transactions. Our simulations suggest that adaptive\nbackoff works reasonably well when conflicting transac\ntions have approximately the same duration. If durations\ndiffer, however, then longer transactions will be more likely\nto abort. Some kind of hardware queueing mechanism [16]\nmight alleviate this limitation.\nThe cache coherence protocols used in our simulations\nprovide a sequentially consistent memory [24]. A number\nof researchers have proposed weaker notions of correct\nness that permit more efficient implementations. These\nalternatives include processor consistency [14], weak con\nsistency [9, 8], release consistency [13], and others5. Most\nof these models guarantee that memory will appear to be\n5See Gharachorloo et al. [12] for concise descriptions of these models\nas well as performance comparisons.\nsequentially consistent as long as the programmer executes\na barrier (or fence) instruction at the start and finish of\neach critical section. The most straightforward way to pro\nvide transactional memory semantics on top of a weakly-\nconsistent memory is to have each transactional instruction\nperform an implicit barrier. Such frequent barriers would\nlimit performance. We believe our implementation can be\nextended to require barriers only at transaction start, finish,\nand validate instructions.\n6 Related Work\nTransactional memory is a direct generalization of the\nLOAD LINKED and STORE COND instructions originally pro-\nposed by Jensen et al. [21], and since incorporated into\nthe MIPS II architecture [29] and Digital's Alpha [31].\nThe LOAD LINKED instruction is essentially the same as\nSTORE COND\nST and COM-\nLTX, and\nis a combination of\nMIT\nLOAD LINKED\n. The\n/\nplement any read-modify-write operation, but it is restricted\nto a single word. Transactional memory has the same flex\nibility, but can operate on multiple, independently-chosen\nwords.\nSTORE COND combination can im-\nWe are not the first to observe the utility of performing\nPage 9\n\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\nElapsed Time (in cycles x 1000)\nElapsed Time (in cycles x 1000)\n\nTTS Lock\n\nMCS Lock\n\nLL/SC Lock\n\nQOSB\n\nTrans. Mem.\n\n|\n|\nConcurrency\n|\nConcurrency\nFigure 5: Producer/Consumer Benchmark: Bus and Network\n|\n|\n|\n|\n|\n|\n|\natomic operations on multiple locations. For example, the\nMotorola 68000 provides a COMPARE&SWAP2 that operates\non two independent locations. Massalin and Pu [25] use\nthis instruction for lock-free list manipulation in an oper\nating system kernel. Transactional memory provides more\npowerful support for this \"lock-free\" style of programming.\nOther work that uses after-the-fact conflict detection to\nrecognize violations of desired correctness conditions in\nclude Gharachorloo and Gibbons [11], who propose an\nimplementation of release consistency that exploits an un\nderlying invalidation-based cache protocol to detect viola\ntions of sequential consistency, and Franklin and Sohi [10],\nwho propose a hardware architecture that optimistically\nparallelizes sequential code at runtime.\nOther researchers who have investigated architectural\nsupport for multi-word synchronization include Knight\n[23], who suggests using cache coherence protocols to add\nparallelism to \"mostly functional\" LISP programs, and the\nIBM 801 [7], which provides support for database-style\nlocking in hardware. Note that despite superficial similar\nities in terminology, the synchronization mechanisms pro\nvided by transactional memory and by the 801 are intended\nfor entirely different purposes, and use entirely different\ntechniques.\nOur approach to performance issues has been heavily\ninfluenced by recent work on locking in multiprocessors,\nincluding work of Anderson [3], Bershad [4], Graunke and\nThakkar [17], and Mellor-Crummey and Scott [27].\n7 Conclusions\nThe primary goal of transactional memory is to make it\neasier to perform general atomic updates of multiple in\ndependent memory words, avoiding the problems of locks\n(priority inversion, convoying, and deadlock). We sketched\nhow it can be implemented by adding new instructions\nto the processor, adding a small auxiliary, transactional\ncache (without disturbing the regular cache), and making\nstraightforward changes to the cache coherence protocol.\nWe investigated transactional memory for its added func\ntionality, but our simulations showed that it outperforms\nother techniques for atomic updates. This is primarily be\ncause transactional memory uses no explicit locks and thus\nperforms fewer shared memory accesses. Since transac\ntional memory offers both improved functionality and bet\nter performance, it should be considered in future processor\narchitectures.\nPage 10\n\n|\nElapsed Time (in cycles x 1000)\n|\n|\n|\n|\n|\n\nElapsed Time (in cycles x 1000)\n\n|\n|\n|\n|\n|\n\nMCS Lock\n\nTTS Lock\n\nLL/SC Lock\n\nQOSB\n\nTrans. Mem.\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\nConcurrency\nConcurrency\nFigure 6: Doubly-Linked List Benchmark: Bus and Network\nReferences\n[6] D. Chaiken, J. Kubiatowicz, and A. Agarwal.\nLimitLESS directories: a scalable cache coherence\n[1] A. Agarwal et al. The MIT Alewife machine: A\nscheme. In Proceedings of the 4th International\nlarge-scale distributed-memory multiprocessor.\nConference on Architectural Support for\nTechnical Report TM-454, MIT Lab for Computer\nProgramming Langauges and Operating Systems,\nScience, 545 Technology Square, Cambridge MA\npages 224-234. ACM, April 1991.\n02139, March 1991. Extended version submitted for\npublication.\n[7] A. Chang and M.F. Mergen. 801 storage:\nArchitecture and programming. ACM Transactions\n[2] J. Allemany and E.W. Felton. Performance issues in\non Computer Systems, 6(1):28-50, February 1988.\nnon-blocking synchronization on shared-memory\nmultiprocessors. In Proceedings of the 11th Annual\n[8] Michel Dubois and Christoph Scheurich. Memory\nACM Symposium on Principles of Distributed\naccess dependencies in shared-memory\nComputing, pages 125-134. ACM, August 1992.\nmultiprocessors. IEEE Transactions on Software\nEngineering, 16(6):660-673, June 1990.\n[3] T.E. Anderson. The performance of spin lock\nalternatives for shared-memory multiprocessors.\n[9] Michel Dubois, Christoph Scheurich, and Fay e\nIEEE Transactions on Parallel and Distributed\nBriggs. Memory access buffering in\nSystems, 1(1):6-16, January 1990.\nmultiprocessors. In Proceedings of the 13th Annual\nInternational Symposium on Computer Architecture,\n[4] B.N. Bershad. Practical considerations for lock-free\npages 434-442, June 1986.\nconcurrent objects. Technical Report\nCMU-CS-91-183, Carnegie Mellon University,\n[10] M. Franklin and G. S. Sohi. The expandable split\nPittsburgh, PA, September 1991.\nwindow paradigm for exploiting fine-grain\nparallelism. In Proceedings of the 19th Annual\n[5] E. A. Brewer, C. N. Dellarocas, A. Colbrook, and\nInternational Symposium on Computer Architecture,\nW. E. Weihl. PROTEUS: A high-performance parallel\narchitecture simulator. Technical Report\nMIT/LCS/TR-516, Massachusetts Institute of\nTechnology, September 1991.\npages 58-67. IEEE, May 1992.\n[11] K. Gharachorloo and P. Gibbons. Detecting\nviolations of sequential consistency. In Proceedings\nPage 11\n|\n|\n\nof the 2nd Annual Symposium on Parallel Algorithms\nand Architectures, pages 316-326, July 1991.\n[12] Kourosh Gharachorloo, Anoop Gupta, and John\nHennessy. Performance evaluation of memory\nconsistency models for shared-memory\nmultiprocessors. In Fourth International Conference\non Architectural Support for Programming\nLanguages and Operating Systems, pages 245-257,\nApril 1991.\n[13] Kourosh Gharachorloo, Dan Lenoski, James Laudon,\nPhillip Gibbons, Anoop Gupta, and John Hennessy.\nMemory consistency and event ordering in scalable\nshared-memory multiprocessors. In Proceedings of\nthe 17th Annual International Symposium on\nComputer Architecture, pages 15-26, June 1990.\n[14] James R. Goodman. Cache consistency and\nsequential consistency. Technical Report 61, SCI\nCommittee, March 1989.\n[15] J.R. Goodman. Using cache memory to reduce\nprocessor-memory traffic. In Proceedings of the 12th\nInternational Symposium on Computer Architecture,\npages 124-131. IEEE, June 1983.\n[16] J.R. Goodman, M.K. Vernon, and P.J. Woest.\nEfficient synchronization primitives for large-scale\ncache-coherent multiprocessors. In Proceedings of\nthe 3rd International Conference on Architectural\nSupport for Programming Languates and Operating\nSystems, pages 64-75, April 1989.\n[17] G. Graunke and S. Thakkar. Synchronization\nalgorithms for shared-memory multiprocessors.\nIEEE Computer, 23(6):60-70, June 1990.\n[18] J.N. Gray. Notes on Database Operating Systems,\npages 393-481. Springer-Verlag, Berlin, 1978.\n[19] M.P. Herlihy. A methodology for implementing\nhighly concurrent data structures. In Proceedings of\nthe Second ACM SIGPLAN Symposium on\nPrinciples and Practice of Parallel Programming,\npages 197-206, March 1990.\n[20] M.P. Herlihy and J.E.B. Moss. Transactional\nmemory: Architectural support for lock-free data\nstructures. Technical Report 92/07, Digital\nCambridge Research Lab, One Kendall Square,\nCambridge MA 02139, December 1992.\n[21] E.H. Jensen, G.W. Hagensen, and J.M. Broughton.\nA new approach to exclusive data access in shared\nmemory multiprocessors. Technical Report\nUCRL-97663, Lawrence Livermore National\nLaboratory, November 1987.\n[22] N. Jouppi. Improving direct mapped cache\nperformance by the addition of a small\nfully-associative cache and prefetch buffers. In 17th\nAnnual Internationall Symposium on Computer\nArchitecture, page 364. ACM SIGARCH, June 1990.\n[23] T. Knight. An achitecture for mostly functional\nlanguages. In Conference on Lisp and Functional\nProgramming, pages 105-112, August 1986.\n[24] Leslie Lamport. How to make a multiprocessor\ncomputer that correctly executes multiprocess\nprograms. IEEE Transactions on Computers,\nC-28(9):241-248, September 1979.\n[25] H. Massalin and C. Pu. A lock-free multiprocessor\nOS kernel. Technical Report CUCS-005-91,\nColumbia University Computer Science Dept., 1991.\n[26] J.M. Mellor-Crummey. Practical fetch-and-phi\nalgorithms. Technical Report Technical Report 229,\nComputer Science Dept., University of Rochester,\nNovember 1987.\n[27] John M. Mellor-Crummey and Michael L. Scott.\nAlgorithms for scalable synchronization on\nshared-memory multiprocessors. ACM Transactions\non Computer Systems, 9(1):21-65, February 1991.\n[28] R. Metcalfe and D. Boggs. Ethernet: distributed\npacket switching for local computer networks.\nCommunications of the ACM, 19(7):395-404, July\n1976.\n[29] MIPS Computer Company. The MIPS RISC\narchitecture.\n[30] L. Rudolph and Z. Segall. Dynamic decentralized\ncache schemes for MIMD parallel processors. In\n11th Annual International Symposium on Computer\nArchitecture, pages 340-347, June 1984.\n[31] R.L. Sites. Alpha Architecture Reference Manual.\nDigital Press, Maynard, MA, 1992.\n[32] J. Wing and C. Gong. Testing and verifying\nconcurrent objects. Journal of Parallel and\nDistributed Computing, 17(2), February 1993.\nPage 12"
    },
    {
      "category": "Resource",
      "title": "feng97efficient.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-895-theory-of-parallel-systems-sma-5509-fall-2003/8313567c7121c474dc870a95c01db76b_feng97efficient.pdf",
      "content": "Efficient Detection of Determinacy Races in Cilk Programs\nMingdong Feng\nDepartment of ISCS\nNational University of Singapore\n10 Lower Kent Ridge Road\nRepublic of Singapore 119260\nAbstract\nA parallel multithreaded program that is ostensibly deterministic\nmay nevertheless behave nondeterministically due to bugs in the\ncode. These bugs are called determinacy races, and they result\nwhen one thread updates a location in shared memory while another\nthread is concurrently accessing the location. We have implemented\na provably efficient determinacy-race detector for Cilk, an algorith\nmic multithreaded programming language. If a Cilk program run on\na given input data set has a determinacy race, our debugging tool,\nwhich we call the \"Nondeterminator,\" guarantees to detect and lo\ncalize the race.\nThe core of the Nondeterminator is an asymptotically effi\ncient serial algorithm (inspired by Tarjan's nearly linear-time least-\ncommon-ancestors algorithm) for detecting determinacy races in\nseries-parallel directed acyclic graphs. For a Cilk program that runs\nin T time on one processor and uses v shared-memory locations,\nthe Nondeterminator runs in O(T α (v\nv)) time, where α is Tarjan's\nfunctional inverse of Ackermann's function, a very slowly growing\nfunction which, for all practical purposes, is bounded above by 4.\nThe Nondeterminator uses at most a constant factor more space than\ndoes the original program. On a variety of Cilk program bench\nmarks, the Nondeterminator exhibits a slowdown of less than 12\ncompared with the serial execution time of the original optimized\ncode, which we contend is an acceptable slowdown for debugging\npurposes.\n\nIntro\nduction\nCilk [5, 20] is an algorithmic multithreaded programming language\nwhose threads can concurrently access (read or write) shared mem\nory without blocking. Many Cilk programs are intended to be de\nterministic, in that a given program produces the same behavior no\nmatter how its threads are scheduled. If a thread updates a location\nwhile another thread is concurrently accessing the location, how\never, a determinacy race occurs, which may cause the program to\nbehave nondeterministically. That is, different runs of the same pro\ngram may produce different behaviors. Determinacy-race bugs are\nnotoriously hard to detect by normal debugging techniques, such as\nThis research was supported in part by the Defense Advanced Research Projects\nAgency under Grant N00014-94-1-0985. Mingdong Feng did this work as a Postdoc\ntoral Fellow in the MIT Laboratory for Computer Science. Parallel computing facilities\nwere provided bythe MIT Xolas Project through a generous donation bySun Microsys\ntems, Inc.\nCharles E. Leiserson\nMIT Laboratory for Computer Science\n545 Technology Square\nCambridge, MA 02139\nUSA\nbreakpointing, because they are not easily repeatable. This paper\ndescribes a system we call the \"Nondeterminator \" to detect deter\nminacy races in Cilk programs.\nDeterminacy races have been given many different names in the\nliterature. For example, they are sometimes called access anoma\nlies [7], data races [12], race conditions [19], or harmful shared-\nmemory accesses [16]. Netzer and Miller [15] clarify different types\nof races and define a general race or determinacy race to be a race\nthat causes a supposedly deterministic program to behave nondeter\nministically. (They also define a data race or atomicity race to be a\nrace in a nondeterministic program involving nonatomic accesses to\ncritical regions.) We prefer the more descriptive term \"determinacy\nrace.\" Emrath and Padua [9] call a deterministic program internally\ndeterministic if the program execution on the given input exhibits\nno determinacy race and externally deterministic if the program has\ndeterminacy races but its output is deterministic because of the com\nmutative and associative operations performed on the shared loca\ntions. Our Nondeterminator program checks whether a Cilk pro\ngram is internally deterministic, and we have also extended the Non\ndeterminator to check whether a Cilk program is externally deter\nministic when the program contains \"atomic accumulations.\"\nTo illustrate how a determinacy race can occur, consider the\nsimple Cilk program shown in Figure 1.\nThe Cilk procedure\nmain forks a subprocedure\nfoo using the\nspawn keyword,\nthereby allowing\nmain to continue executing concurrently with\nthe spawned subprocedure\nfoo. Then, the\nmain procedure\nspawns another instance of\nfoo.\nThe\nsync statement causes\nmain to join with its two subprocedures by suspending until both\nof these instances of\nfoo complete. (In general, the\nsync state\nment causes a procedure to suspend until all subprocedures that\nit has spawned have completed.) A spawn tree for this program,\nwhich illustrates the relationship between Cilk procedures, is shown\nin Figure 2.\nThe parallel control flow of the Cilk program from Figure 1 can\nbe viewed as a directed acyclic graph, or dag, as illustrated in Fig\nure 3. The vertices of the dag represent parallel control constructs,\nand the edges represent Cilk threads, which are maximal sequences\nof instructions not containing any parallel control constructs. In Fig\nure 3, the threads of the program are labeled to correspond to code\nfragments from Figure 1, and the subdags representing the two in\nstances of\nfoo are shaded. (Threads e1 and e2 contain no instruc\ntions.) In this program, both of the parallel instantiations of proce\ndure\nfoo update the shared variable\nx in the\nx\n\nx\n\nstate\nment. This statement actually causes the processor executing the\nthread to perform a read on\nx, increment the value, and then write the\nvalue back into\nx. Since these operations are not atomic, both might\nupdate\nx at the same time. Figure 4 shows how this determinacy race\ncan cause\nx to take on different values if the threads comprising the\ntwo instantiations of\nfoo are scheduled simultaneously.\n\nint\nx\ncilk\nvoid\nfoo\n\nx\n\nx\n\ncilk\nint\nmain\nF\n\nx\n\ne0\n\nspawn\nfoo\nF1\n\ne1\n\nspawn\nfoo\nF2\n\ne2\n\nsync\nprintfx\nis\ndn\nx\ne3\n\nreturn\n\nFigure\n: A simple Cilk program that contains determinacy races. In the\ncomments at the right, the Cilk threads that make up the procedure\nmain\nare labeled.\nfoo\nfoo\nF\nF2\nF1\nmain\nFigure\n: A spawn tree for the Cilk program in Figure 1. Each node cor\nresponds to a procedure, and its children in the tree are the procedures it\nspawns.\nThe Nondeterminator determinacy-race detector takes as input a\nCilk program and an input data set and either determines which loca\ntions in the program are subject to determinacy races when the pro\ngram is run on the data set, or else certifies that the program is race\nfree when run on the data set. In general, a determinacy race occurs\nwhenever two threads access the same location and one of the ac\ncesses is a write. If a determinacy race exists, the Nondeterminator\nlocalizes the bug, providing variable name, file name, line number,\nand dynamic context (state of runtime stack, heap, etc.). The Non\ndeterminator is not a program verifier, because the Nondetermina\ntor cannot certify that the program is race free for all input data sets.\nRather, it is a debugging tool. The Nondeterminator only checks a\nprogram on a particular input data set. What it verifies is that every\npossible scheduling of the program execution produces the same be\nhavior. Moreover, even though the Nondeterminator detects deter\nminacy races in parallel programs, it is itself a serial program.\nThe Nondeterminator was implemented by modifying the ordi\nnary Cilk compiler and runtime system. Each read and write in the\nuser's program is instrumented by the Nondeterminator 's compiler\nto perform determinacy-race checking at runtime. The Nondetermi\nnator then executes the user's program in a serial, depth-first fash\nion (like a C execution), but it performs race-checking actions when\nreads, writes, and parallel control statements occur. Figure 5 shows\nthe performance of the Nondeterminator on several Cilk application\nbenchmarks running on a 167-megahertz SUN Ultrasparc with the\nSolaris 2.5.1 operating system. Since the Nondeterminator is a se\nrial program, our comparisons are with one-processor executions of\nthe benchmarks.\nThe heart of the Nondeterminator 's runtime system is an algo\ne1\ne3\ne0\ne2\nF\nmain\nF1\nfoo\nfoo\nF2\nsync node\nspawn node\nFigure\n: The parallel control-flow dag of the program in Figure 1. A\nspawn node of the dag represents a\nspawn construct, and a sync node repre\nsents a\nsync construct. The edges of the dag are labeled to correspond with\ncode fragments from Figure 1. We assume that the start of the program is\n\"spawned\" by the operating system, and the end of the program is \"synced\"\nby the operating system.\nCase 1\nCase 2\nF1\nF2\ne3\nF1\nF2\ne3\nread\nx\nread\nx\nwrite\nx\nread\nx\nread\nx\nwrite\nx\nwrite\nx\nwrite\nx\n\"x\ni s\n\"\n\"x\ni s\n\"\nFigure\n: An illustration of a determinacy race in the code from Figure 1.\nThe value of the shared variable\nx read and printed by thread e 3 can differ de\npending on how the instructions in the two instances F1 and F2 of the\nfoo\nprocedure are scheduled.\nrithm for determinacy-race detection that we call the SP-bags al\ngorithm, which was inspired by Tarjan's nearly linear-time least-\ncommon-ancestors algorithm [22]. Like Tarjan's algorithm, the SP-\nbags algorithm uses an efficient data structure [6, Chapter 22] to\nmanage disjoint sets of elements. Figure 6 compares the asymptotic\ntime and space for the SP-bags algorithm with other race-detection\nalgorithms in the literature.\nThe remainder of this paper is organized as follows. Section 2\npresents the SP-bags algorithm that underlies the Nondeterminator 's\nruntime system. Section 3 reviews the basic properties of series-\nparallel dags and relates them to the parallel control flow of Cilk\nprograms, and then Section 4 proves that the SP-bags algorithm cor\nrectly detects determinacy races in Cilk programs. The SP-bags al\ngorithm only considers \"pure\" Cilk programs that contain\nspawn\nand\nsync statements, but none of Cilk's more advanced constructs\nthat allow nondeterministic programming. (For a complete speci\nfication of the Cilk language, see [20].) Section 5 shows how to\nextend the SP-bags algorithm to detect determinacy races in more\ngeneral Cilk programs containing atomic \"accumulations,\" where a\nvariable can be updated when a spawned procedure returns. Sec\ntion 6 describes how we implemented the SP-bags algorithm in the\nNondeterminator and provides empirical data on its performance.\nSection 7 discusses related work, and we offer some concluding re\nmarks in Section 8.\n\nThe\nSPbags\nalgo\nrithm\nThis section describes the SP-bags algorithm for determinacy-race\ndetection. We first review the disjoint-set data structure used in the\nalgorithm, and then we present the algorithm itself, which is in\nspired by Tarjan's least-common-ancestors algorithm [22]. Finally,\n\nF1\nF4\nF6\nF6\nF9\nF14\nF8\nF13\nspawn\nspawn\nspawn\nSF9\nPF9\nF2 F3\nF1\nF5\nF7\nF12\nSF1\nPF1\nSF6\nPF6\nF11\nF9\nF10\nF13\n10 11 12\nFigure\n: The slowdown of eight benchmark Cilk programs checked with\nthe Nondeterminator. The slowdown, shown as a dark bar, is the ratio of\nthe Nondeterminator runtime to the original optimized runtime (gcc\nO)\nof the benchmark. For comparison, the slowdown of an ordinary debugging\nversion (gcc\ng) of each benchmark is shown as a light bar.\nmmult\nlu\nsparsky\nhutch\nheat\nfft\nmultisort\nknapsack\nSF13\nPF13\nFigure\n: A snapshot of the SP-bags data structures during the execu\ntion of a Cilk program. The ovals in the figure represent procedures that\nare currently on the runtime stack: F1 spawns F6, which spawns F9, which\nspawns F13. Each procedure contains an S-bag and a P-bag. Each descen\ndant of a completed child of a procedure F belongs either to F's S-bag or to\nF's P-bag. For example, F2, F3, F4, and F5 are descendants of F1 that com\nplete before F1 spawns F6, and so these procedures belong to either F1's S-\nbag or its P-bag. In addition, every procedure F belongs to its own S-bag.\nThe SP-bags algorithm uses the fast disjoint-set data structure [6,\nChapter 22] analyzed by Tarjan [21]. The data structure maintains a\ndynamic collection Σ of disjoint sets and provides three elementary\noperations:\nMakeSet\n(x): Σ\nΣ\n\nff xg\ng.\nUnion(X\nY\n): Σ\nΣ\n\nf X\nY\ng\n\nf X\nY\ng. The sets X and Y are de\nstroyed.\nFindSet(x): Returns the set X\nΣ such that x\nX.\nTarjan shows that any m of these operations on n sets take a total of\nO(mα (m n)) time.\nDuring the execution of the SP-bags algorithm, two \"bags\" of\nprocedure ID's are maintained for every Cilk procedure on the call\nstack, as illustrated in Figure 7. These bags have the following con\ntents:\nThe S-bag SF of a procedure F contains the ID's of those de\nscendants of F's completed children that logically \"precede\"\nthe currently executing thread, as well as the ID for F itself.\nThe P-bag PF of a procedure F contains the ID's of those de\nscendants of F's completed children that operate logically \"in\nparallel\" with the currently executing thread.\nThe S-bags and P-bags are represented as sets using a disjoint-set\ndata structure.\nThe SP-bags algorithm itself is given in Figure 8. As the Cilk pro\ngram executes in a serial, depth-first fashion, the SP-bags algorithm\nperforms additional operations whenever one of the five following\nactions occurs:\nspawn,\nsync,\nreturn,\nwrite, and\nread. The cor\nrectness of the SP-bags algorithm is presented in Section 4, but we\ngive an informal explanation of its operation here.\nAs the SP-bags algorithm executes, it updates the contents of\nthe S-bags and P-bags whenever one of the actions\nspawn,\nsync,\nreturn occurs. Whenever a procedure F is spawned, S F is initially\nmade to contain F, because F's subsequent instructions are in se\nries with its earlier instructions. Whenever a subprocedure F\n0 re\nturns to its parent F, the contents of SF\n0 are emptied into PF, since\nthe procedures in SF\n0 can execute in parallel with any subprocedures\nthat F might spawn in the future before performing a\nsync. When\na\nsync occurs, PF is emptied into its SF, since all of F's previously\nTime\nAlgorithm\nEnglish-Hebrew\nlabeling [16]\nTask\nrecycling [7]\nOffset-span\nlabeling [12]\nSP-bags\nalgorithm\nThread\ncreation &\ntermination\nO( p)\nO(t\n)\nO( p)\nO(α(v v))\n\nPer\naccess\nO( pt\n)\nO(t\n)\nO( p)\nO(α(v v))\n\nSpace\nO(vt\n+ min\n(np vt p))\nO(vt\n+ t2\n)\nO(v\n+ min\n(np vp))\nO(v)\np\nmaximum depth of nested parallelism\nt\nmaximum number of logical concurrent threads\nv\nnumber of shared locations being monitored\nn\nnumber of threads in an execution\nFigure\n: Comparison of determinacy-race detection algorithms. The func\ntion α is the very slowly growing inverse of Ackermann's function intro\nduced by Tarjan in his analysis of an efficient disjoint-set data structure. For\nall conceivably practical inputs, the value of this function is at most 4. The\ntime for the SP-bags algorithm is an amortized bound.\nwe prove that the running time of the algorithm is O(T α (v\nv)) when\nrun on a Cilk program that takes time T on one processor and uses v\nshared-memory locations, where α is Tarjan's functional inverse of\nAckermann's function [21].\nThe SP-bags algorithm is a serial algorithm. It uses the fact that\nany Cilk program can be executed on one processor in a depth-first\n(C-like) fashion and conforms to the semantics of the C program\nthat results when all\nspawn and\nsync keywords are removed. As\nthe SP-bags algorithm executes, it employs several data structures\nto determine which procedure instances have the potential to exe\ncute \"in parallel\" with each other, and is thereby able to check for\ndeterminacy races.\nThe SP-bags algorithm maintains two shadow spaces of shared\nmemory called writer and reader.\nFor each location of shared\nmemory, each shadow space has a corresponding location. Every\nspawned procedure1 is given a unique ID at runtime. For each loca\ntion l in shared memory, the ID of the procedure that wrote the lo\ncation is stored in location l of the writer shadow space. Similarly,\nlocation l of the reader shadow space stores the ID of a procedure\nwhich previously read location l, although in this case, the ID is not\nnecessarily that of the most recent reader. The SP-bags algorithm\nupdates the shadow spaces as it executes.\n1Technically, by \"procedure\" we mean \"procedure instance,\" that is, the runtime\nstate of the procedure.\n\nspawn procedure F:\nSF\n\nMakeSet\n(F\n)\nPF\n0/\nsync in a procedure F:\nSF\n\nUnion(SF\nPF\n)\nPF\n0/\nreturn from procedure F\n0 to F:\nPF\n\nUnion\n(PF\nSF\n)\nwrite a shared location l by procedure F:\nif\nFindSet(reader\n(l\n)) is a P-bag\nor\nFindSet(writer\n(l\n)) is a P-bag\nthen a determinacy race exists\nwriter\n(l\n)\nF\nread a shared location l by procedure F:\nif\nFindSet(writer\n(l\n)) is a P-bag\nthen a determinacy race exists\nif\nFindSet(reader\n(l\n)) is an S-bag\nthen reader\n(l\n)\nF\nFigure\n: The SP-bags algorithm. Whenever one of the five actions occurs\nduring the serial, depth-first execution of a Cilk program, the operations in\nthe figure are performed. Operations for\nspawn,\nsync and\nreturn actions\nmanipulate the S-bags and P-bags of the disjoint-set data structure. Opera\ntions for\nwrite and\nread actions affect the shadow spaces and detect deter\nminacy races.\nspawned subprocedures and their descendants logically precede any\nfuture subprocedures spawned by F.\nDeterminacy races are detected by the code for\nwrite and\nread.\nA race occurs if a procedure F writes a location l and discovers that\neither the previous reader or the previous writer of l belongs to a P-\nbag, which means that F and the past accessor of l operate logically\nin parallel. Similarly, a race occurs whenever F reads a location l\nand discovers that the previous writer is in a P-bag. In the normal\ncase, whenever a location l is written, location l in the writer shadow\nspace is updated to be F. The reader of l is updated to be F when\na read occurs, but only if the previous reader operates logically in\nseries with F. The logic behind this subtle piece of code is explained\nin Section 4, where the SP-bags algorithm is proved correct.\nTo conclude this section, we analyze the asymptotic performance\nof the SP-bags algorithm.\nTheo\nrem\nConsider a Cilk program that executes in time T on\none processor and references v shared memory locations. The SP-\nbags algorithm can be implemented to check this program for deter\nminacy races in O(T α (v\nv)) time using O(v) space.\nProof:\nLet n be the number of spawned procedures during the ex\necution of the SP-bags algorithm, which is also the the total num\nber of procedure ID's used by the algorithm. The total number of\nall MAKE-SET, UNION, and FIND-SET operations is at most the\nserial running time T. Consequently, by using the fast disjoint-\nset data structure analyzed by Tarjan, we obtain a running time of\nO(T α (T\nn)). Since the two shadow spaces take O(v) space and the\ndisjoint-set data structure takes O(n) space, the total space used by\nthe algorithm is O(v\n+ n).\nBy using garbage collection, the time and space can be reduced to\nO(T α (v\nv)) and O(v), respectively. The idea is to run the basic SP-\nbags algorithm for v steps, and then scan through the shadow spaces\nmarking which procedure ID's are in use. Then, we remove the un\nused ID's from the disjoint-set data structure, which can be done in\ne\nG2\nG1\nt\ns\nG2\nG1\nt\ns\nt\ns\n(a)\n(b)\n(c)\nFigure\n: The three ways that a series-parallel dag can be constructed. (a) A\nbase graph. (b) Series composition of two series-parallel dags. (c) Parallel\ncomposition of two series-parallel dags.\nO(vα (v\nv)) time. We repeat the garbage collection every v steps.\nThus, in T time, we perform\ndT\nve garbage collections, resulting\nin a running time of O(T α (v\nv)). Because the amount of space in\nuse after each garbage collection is O(v) and at most O(v) additional\nspace can accumulate during the v steps between garbage collec\ntions, the algorithm uses a total of O(v) space.\nIn practice, it is probably not worthwhile to implement the\ngarbage collection, and the Nondeterminator does not implement it.\nAlso, the worst-case bounds can easily be improved if they are ex\npressed using more detailed parameters than T and v.\n\nSeriespa\nrallel\ndags\nSeries-parallel dags [23] are a straightforward extension of the no\ntion of series-parallel graphs [8, 11, 17]. In this section, we review\nbasic properties of series-parallel dags and show how a Cilk pro\ngram execution corresponds to a series-parallel dag. These proper\nties will be used in Section 4 to prove the correctness of the SP-bags\nalgorithm.\nWe first define various relationships among Cilk threads.\nA\nthread e1 precedes a thread e2, denoted e1\ne2, if there is a path in\nthe Cilk dag that includes both e1 and e2 in that order. Two distinct\nthreads e1 and e2 operate logically in parallel, denoted e1\nk e2, if\ne2 and e2\n\ne1\n\ne1. Informally, e1\ne2 means that e1 must execute\nbefore e2 in any legal scheduling of a Cilk program, while e1\nk e2\nmeans that e1 and e2 can execute at the same time. The precedence\nrelation\nis transitive.\nA series-parallel dag G\n\n( V\nE\n) is a directed acyclic graph with\ntwo distinguished vertices, a source s\nV and a sink t\nV, which is\nconstructed recursively in one of the following ways, as illustrated\nin Figure 9:\nBase: The graph consists of a single edge e connecting the source\ns to the sink t.\nSeries composition: The graph consists recursively of two series-\nparallel dags G1 and G2 with disjoint edge sets in which the\nsource of G1 is s, the sink of G2 is t, and the sink of G1 is the\nsource of G2.\nParallel composition:\nThe graph consists recursively of two\nseries-parallel dags G1 and G2 with disjoint edge sets in which\nthe sources of G1 and G2 are both s and the sinks of G1 and G2\nare both t.\nNote that for series composition, it makes a difference which sub\ngraph precedes the other, but the order of parallel composition does\nnot matter. Moreover, one can prove by induction that any series-\nparallel dag is indeed a dag.\nThe following properties of series-parallel dags are presented\nwithout proof.\nLemm\na\nLet G\n0 be a series-parallel dag, let G be a series-parallel\nsubdag of G\n0 , and let s and t be the source and sink of G, respectively.\nThen, the following properties hold:\n\nF\ne\ne\nF\nF\ne\ne\nF\ne\ne\ne\nF\n\nF\ne\ne\ne\nF\n\nF\ne\ne\nF\ne\ne\ne\ne\nsync block\nsync block\nsync block\nFigure\n: The dag of a spawned Cilk procedure that contains\nspawn and\nsync statements. It consists of a linear sequence of sync blocks (rectangles in the\nfigure) terminated by a\nreturn statement. Each e corresponds to a thread of the Cilk procedure, and each F corresponds to a spawned subprocedure.\n1. There exists a path in G from s to any edge in G.\n2. There exists a path in G from any edge in G to t.\n3. Every path in G\n0 that begins outside of G and enters G passes\nthrough s.\n4. Every path in G\n0 that begins within G and leaves G passes\nthrough t.\nThe following theorem shows that any Cilk parallel control-flow\ndag, such as that in Figure 3, is series-parallel.\nTheo\nrem\nA Cilk parallel control-flow dag is a series-parallel\ndag.\nProof:\nWe use induction on the depth of the Cilk spawn tree. A\nCilk procedure that contains no\nspawn or\nsync statements is a leaf\nof the spawn tree and is trivially a base series-parallel dag.\nConsider a Cilk procedure that contains\nspawn and\nsync state\nments. The parallel control flow of the procedure at runtime can be\nviewed as a linear sequence of sync blocks terminated by a\nreturn\nstatement, where each sync block consists of a sequence of\nspawn\nstatements interleaved with C code and terminated by a\nsync. In\nother words, the execution of a procedure has the form\ne;\nspawn F; e;\nspawn F; e;\n:\n:\n: ;\nspawn F; e;\nsync;\ne;\nspawn F; e;\nspawn F; e;\n:\n:\n: ;\nspawn F; e;\nsync;\n\ne;\nspawn F; e;\nspawn F; e;\n:\n:\n: ;\nspawn F; e;\nsync;\ne;\nreturn;\nwhere each e is a thread of the Cilk procedure, each F is a spawned\nsubprocedure, and each line except the last is a sync block. The\ndag corresponding to the parallel control flow is shown in Figure 10.\nBy induction, the computation arising from each spawned subproce\ndure is a series-parallel dag. Each sync block is a series-parallel dag\ncreated by alternating series and parallel compositions of threads,\nthe spawned procedures, and the spawn and sync nodes. The Cilk\ndag representing the procedure and all its descendants can now be\nassembled by serially composing all the sync blocks.\nA series-parallel dag can be represented by a binary parse tree,\nas illustrated in Figure 11 for the Cilk procedure from Figure 10.\nThe leaf nodes of the parse tree correspond to edges of the dag (Cilk\nthreads), and each internal node is either an S-node S, which corre\nsponds to a series composition of its two children, or a P-node P,\nwhich corresponds to a parallel composition of its children.\nA canonical parse tree for a Cilk dag can be constructed as fol\nlows. We first build a parse tree recursively for each child of the root\nprocedure. For each sync block of the root procedure, we apply al\nternating parallel and series composition on the child parse tree to\nP\nP\nS\nP\nP\nS\nP\nS\nP\nS\nF\ne\nF\nF\nF\ne\ne\ne\ne\ne\nS\ne\nS\nS\nS\nS\nF\nF\ne\nF\ne\ne\nFigure\n: The canonical parse tree for a generic Cilk procedure. The nota\ntion F represents the parse tree of any subprocedure spawned by this proce\ndure, and e represents any thread of the procedure. All nodes in the shaded\nareas belong to the procedure, and the nodes in each oval belong to the same\nsync block. A sequence of S-nodes forms the spine of the parse tree, com\nposing all sync blocks in series. Each sync block contains an alternating se\nquence of S-nodes and P-nodes. Observe that the left child of an S-node in\na sync block is always a thread, and that the left child of a P-node is always\na subprocedure.\ncreate a parse tree for the sync block. Finally, we string the parse\ntrees for the sync blocks together into a spine for the procedure by\napplying a sequence of series compositions to the sync blocks. Sync\nblocks are composed serially, because a\nsync statement is never\npassed until all previously spawned subprocedures have completed.\nThe only ambiguities that might arise in the parse tree occur because\nof the associativity of series composition and the commutativity of\nparallel composition. If, as shown in Figure 11, the alternating S-\nnodes and P-nodes in a sync block always place threads and subpro\ncedures on the left, and the series compositions of the sync blocks\nare applied in order from last to first, then the parse tree is unique.\nSuch a canonical parse tree is shown in Figure 12 for the Cilk dag\nin Figure 3.\nThe canonical parse tree satisfies an interesting property with re\nspect to a serial, depth-first execution of the Cilk program. Specifi\ncally, an ordinary depth-first tree walk (see [6, p. 245]) of the parse\ntree visits the threads of the computation in the same order as the\nthreads are encountered when the Cilk program is executed in a\ndepth-first (C-like) fashion on a single processor.\n\nF:\nS\ne3\nS\ne0\nP\nS\nF1\ne1\nP\ne2\nF2\nFigure\n: The canonical parse tree for the Cilk dag in Figure 3.\n\nCo\nrrectness\nof\nthe\nSPbags\nalgo\nrithm\nIn this section, we prove the correctness of the SP-bags algorithm.\nWe begin by showing how either a precedence relation\nor a paral\nlel relation\nk between two threads in a Cilk dag can be inferred from\nthe threads' least common ancestor in the parse tree of the dag. We\nthen prove a lemma that characterizes the contents of S-bags and P-\nbags during the execution of the SP-bags algorithm. We conclude by\nshowing that the SP-bags algorithm correctly detects determinacy\nraces.\nThe SP-bags algorithm hinges on the notion of the least common\nancestor of two nodes in a tree. Given two nodes x and y in a rooted\ntree, their least common ancestor, denoted LCA(x y), is the deep\nest node in the tree that is a common ancestor of both x and y. Al\nternatively, if one traces the unique simple path from x to y, their\nleast common ancestor is the node on the path that is closest to the\nroot. The next lemma and its corollary show how the least common\nancestor of two threads in the parse tree can be used to determine\nwhether the threads operate logically in parallel or whether one pre\ncedes the other.\nLemma\nLet e1 and e2 be distinct threads in a Cilk dag, and let\nLCA(e1\ne2\n) be their least common ancestor in a parse tree for the\ndag. Then, e1\nk e2 if and only if LCA(e1\ne2\n) is a P-node.\nProof:\n() Assume for the purpose of contradiction that e1\nk e2\nand LCA\n(e1\ne2\n) is an S-node. Let G1 be the graph corresponding\nto the left subtree of LCA(e1\ne2\n), and let G2 be the graph corre\nsponding to the right subtree. By Lemma 2, there exists a path from\ne1 to the sink of G1 and a path from the source of G2 to e2. Since G1\nand G2 are composed in series, the sink of G1 and the source of G2\nare the same node, and hence e 1\ne2, contradicting the assumption\nthat e1\nk e2.\n( ) Assume for the purpose of contradiction that e1\ne2 and\nLCA(e1\ne2\n) is a P-node. Let G1 be the graph corresponding to the\nleft subtree of LCA(e1\ne2\n), and let G2 be the graph corresponding\nto the right subtree. By Lemma 2, the path from e1 to e2 must go\nthrough the sink of G1 and the source of G2. Since G1 and G2 are\ncomposed in parallel, the sink of G1 is the sink of G2 and the source\nof G1 is the source of G2. Thus, we have a path from the sink of G1\nto the source of G1, contradicting the fact that G1 is a dag.\nCo\nrolla\nry\nLet e1 and e2 be distinct threads in a Cilk dag, and let\nLCA(e1\ne2\n) be their least common ancestor in a parse tree for the\ndag. Then, e1\ne2 if and only if LCA\n(e1\ne2\n) is an S-node, e1 is\nin the left subtree of LCA(e1\ne2\n), and e2 is in the right subtree of\nLCA(e1\ne2\n).\nAs an example of the use of Lemma 4, in Figure 12 we have F1\nk\nF2, because LCA(F1\nF2\n) is a P-node. In contrast, since e1 occurs\nto the left of F2 in the parse tree and LCA(e1\nF2\n) is an S-node, by\nCorollary 5 we can conclude that e1\nF2.\nThe SP-bags algorithm takes advantage of relationships among\nthreads that can be derived from the serial, depth-first execution or\nder of the dag. The following two lemmas exploit the depth-first\nexecution order of the algorithm to determine when threads operate\nlogically in parallel.\nLemm\na\nSuppose that three threads e1, e2, and e3 execute in or\nder in a serial, depth-first execution of a Cilk dag, and suppose that\ne1\ne2 and e1\nk e3. Then, we have e2\nk e3.\nProof:\nAssume for the purpose of contradiction that e2\ne3.\nThen, since e1\ne2, we have e1\ne3 by transitivity, contradicting\nthe assumption that e1\nk e3.\nLemm\na\n\nPseudotransitivit\ny\no f\nk Suppose that three threads\ne1, e2, and e3 execute in order in a serial, depth-first execution of a\nCilk dag, and suppose that e1\nk e2 and e2\nk e3. Then, we have e1\nk e3.\nProof:\nConsider the parse tree of the Cilk dag with e1, e2, and e3.\nLet a1\nLCA(e1\ne2\n) and a2\nLCA\n(e2\ne3\n). Lemma 4 implies that\nboth a1 and a2 are P-nodes. Because e1, e2, and e3 execute in or\nder, one can show that either a1 or a2 is the least common ancestor\nof e1 and e3, and since both a1 and a2 are P-nodes, it follows from\nLemma 4 that e1\nk e3.\nFrom the construction of the canonical parse tree for the Cilk dag,\nit is apparent that each procedure in the spawn tree is represented by\nan assembly of threads and internal nodes in the parse tree. We de\nfine the mapping h of threads or nodes in the canonical parse tree\nto procedures in the spawn tree to be the procedurification function\nfor the parse tree. This procedurification function is used in the next\nlemma to relate the S-nodes and P-nodes in the parse tree to proce\ndure ID's in the S-bags and P-bags during the execution of the SP-\nbags algorithm.\nLemm\na\nConsider an execution of the SP-bags algorithm on a\ngiven Cilk dag. Let h be the procedurification function mapping the\ncanonical parse tree for the dag to procedures in the spawn tree. Sup\npose thread e1 is executed before thread e2, and let a\nLCA(e1\ne2\n)\nbe their least common ancestor in the parse tree. If a is an S-node,\nthen the procedure ID for h(e1\n) belongs to the S-bag of h(a) when\ne2 is executed. Similarly, if a is a P-node, then the procedure ID for\nh(e1\n) belongs to the P-bag of h(a) when e2 is executed.\nProof:\nWe shall first show that if a is an S-node of the parse tree,\nthen the procedure ID for h(e1\n) belongs to the S-bag of the pro\ncedure h(a). There are two possibilities (as can be seen from Fig\nure 11) depending on whether a belongs to the spine or a sync block\nof the parse tree.\nIf a belongs to the spine, then e1 belongs to a's left subtree, which\nis rooted in a sync block of the parse tree. At the time thread e2 is\nexecuted, in what bag does the procedure ID for h(e1\n) reside? From\nthe code for the SP-bags algorithm in Figure 8, we can see that when\ne1 is executed, the ID for h(e1\n) is placed in h(e1\n)'s own S-bag by\nthe\nspawn action. From the construction of the canonical parse tree\n(see Figure 11), we observe that either h(e1\n)\nh(a) or h(e1\n) is a de\nscendant of h(a). From the time that e1 is executed to the time e2 is\nexecuted, the only operations that move the ID for h(e1\n) are\nsync\nand\nreturn, which never move the ID down the spawn tree, and\nindeed, h(e1\n)'s ID moves up exactly when one of its ancestors re\nturns. Consequently,when the\nsync corresponding to a is executed,\nh(e1\n)'s ID is placed into the S-bag of h(a), if it is not already there.\nFrom that point until e2 is executed, no operations remove h(e1\n)'s\nID from h(a)'s S-bag.\nIf a belongs to one of h(a)'s sync blocks, then the construction of\nthe canonical parse tree implies that e1 is the left child of a, as can\n\nbe seen in Figure 11. Consequently, we have h(e 1\n)\nh(a), and the\nSP-bags algorithm places the procedure ID for h(e1\n) into h(a)'s S-\nbag at the moment that h(a) is spawned. From that moment until the\ntime e2 is executed, the S-bag of h(a) is never emptied, since h(a)\ndoes not return until after executing e2. Thus, h(e1\n)'s ID belongs to\nh(a)'s S-bag when e2 is executed.\nWe now show that if a is a P-node of the parse tree, then the pro\ncedure ID for h(e1\n) belongs to the P-bag of the procedure h(a). If a\nis a P-node, then the thread e1 belongs to the left subtree of a and the\nthread e2 belongs to a's right subtree. As in the argument for when\na is an S-node in h(a)'s spine, when e2 is executed, the procedure\nh(e1\n) must belong to a bag in the procedure h(a) of their least com\nmon ancestor a. In this case, however, the procedure ID for h(e1\n)\nbelongs to h(a)'s P-bag, since h(e1\n) is a proper descendant of h(a),\nthe ID for h(e1\n) is placed in h(a)'s P-bag when h(a)'s left child re\nturns, and the P-bag of h(a) is not emptied until a's entire sync block\nis executed.\nCo\nrolla\nry\nConsider an execution of the SP-bags algorithm on a\ngiven Cilk dag, and let h be the procedurification function mapping\nthe canonical parse tree for the dag to procedures in the spawn tree.\nSuppose thread e1 is executed before thread e2. Then, e1\ne2 if\nand only if the procedure ID for h(e1\n) belongs to an S-bag when e2\nis executed. Similarly, e1\nk e2 if and only if the procedure ID for\nh(e1\n) belongs to a P-bag when e2 is executed.\nProof:\nCombine Lemma 4, Corollary 5, and Lemma 8.\nWe now prove that the SP-bags algorithm is correct.\nTheo\nrem\nThe SP-bags algorithm detects a determinacy race in\na Cilk program if and only if a determinacy race exists.\nProof:\n() Suppose that the SP-bags algorithm detects a determi\nnacy race when executing a thread e 2. According to the SP-bags al\ngorithm (see Figure 8), one of three cases occurs:\n1. e2 performs a\nwrite and reader\n(l\n) belongs to a P-bag;\n2. e2 performs a\nwrite and writer(l\n) belongs to a P-bag;\n3. e2 performs a\nread and writer\n(l\n) belongs to a P-bag.\nIn the first case, the procedure ID stored in reader\n(l\n) is set by a\nthread e1 which executes before e2 and reads l, and hence by Corol\nlary 9, we have e1\nk e2. Since e1 reads l, e2 writes l, and the two\nthreads operate logically in parallel, a determinacy race exists. The\nother two cases are similar.\n( ) We now show that if a program contains a determinacy race\non a location l, then the SP-bags algorithm reports a determinacy\nrace on location l. Let e1 and e2 be two threads involved in a de\nterminacy race on location l, where if there are several determinacy\nraces on l, we choose the determinacy race whose second thread ex\necutes earliest in the depth-first execution order of the program. By\ndefinition of a determinacy race, we have e1\nk e2, and without loss\nof generality, e1 executes before e2.\nThere are three possible ways the determinacy race could oc\ncur:\n1. e1 writes l and e2 reads l;\n2. e1 writes l and e2 writes l;\n3. e1 reads l and e2 writes l.\nIn each case, let h be the procedurification function mapping threads\nor nodes of the canonical parse tree to procedures in the spawn tree.\nCase 1. Suppose that e1 writes l and e2 reads l. When e2 is ex\necuted, suppose that writer\n(l\n)\nh(e) for some thread e. If e\ne1,\nthen since e1\nk e2, Corollary 9 implies that writer\n(l\n) belongs to a P-\nbag and the determinacy race is reported. If e\ne1, however, then e\nmust be executed after e1 but before e2, because otherwise e's write\nto l would be overwritten by e1's write, and writer(l\n) would likewise\nbe overwritten. We have two possibilities: either e1\ne or e1\nk e.\nIf e1\ne, then we must have e\nk e2 by Lemma 6. Consequently,\nCorollary 9 implies that h(e)\nwriter(l\n) belongs to a P-bag, and the\ndeterminacy race between e and e2 is detected. If e1\nk e, however,\nthen since both e1 and e write l, a write/write determinacy race ex\nists between e1 and e, contradicting the assumption that e2 executes\nearliest in the depth-first execution order of the program, over all\ndeterminacy races on location l.\nCase 2. This case is similar to Case 1.\nCase 3.\nIn this case, when e2 is executed, suppose that\nreader\n(l\n)\nh(e) for some thread e. If e\ne1, then since e1\nk e2,\nCorollary 9 implies that reader\n(l\n) belongs to a P-bag and the de\nterminacy race is reported. Consequently, we may assume that e\n\ne1. We consider two situations depending on whether e1 updates\nreader\n(l\n) when it executes.\nIf e1 updates reader\n(l\n), then consider the sequence of updates to\nreader\n(l\n) from the time e1 executes up to and including the time e\nexecutes. Let the threads performing the updates be e1\ne\n:\n:\n:\nek,\nwhere e1\ne1 and ek\ne. From the code for\nread in Figure 8, we\nmust have for i\n1 2:\n:\n:\nk\n1 that ei\nei+1, since by Corollary 9,\nthe ID of h(ei\n) belongs to an S-bag when ei+1 executes. By tran\nsitivity, therefore, we have e1\ne. Since e1\nk e2, by Lemma 6, it\nfollows that e\nk e2. Consequently, by Corollary 9, the determinacy\nrace between e and e2 is detected.\nIf e1 does not update reader\n(l\n), then when e 1 executes, we must\nhave h(e\n)\nreader\n(l\n) for some thread e\nk e1 that executes be\nfore e1. Since e1\nk e2, by pseudotransitivity (Lemma 7) it follows\nthat e\nk e2. Looking at the sequence of updates of reader\n(l\n) from\nthe execution of e\n0 up to and including the execution of e, we can\nconclude that e\ne. Since e\nk e2, Lemma 6 implies that e\nk e2, and\nhence the determinacy race between e and e 2 is reported.\n\nSupp\no\nrt\nfo\nr\natomic\naccumulation\nCilk supports the atomic accumulation of results returned by\nspawned procedures.\nIf the operators used to augment the ac\ncumulated variable are commutative--they are all\nor\n, for\nexample--we would like the concurrent accessing of the updates not\nto be viewed as races, because the order of accumulation does not\naffect the \"external determinacy\" [9] of the computation. That is,\nthe behavior of the program is deterministic, even though different\nexecutions may cause some variables to pass through different inter\nmediate states. In this section, we show how to extend the SP-bags\nalgorithm to detect determinacy races in Cilk code where races be\ntween accumulations are considered to be \"legal.\"\nConsider the Cilk procedure\nfoo from Figure 13. Cilk guaran\ntees that this code produces the same result for the integer variable\nx no matter how threads are scheduled. The basic idea is that accu\nmulations of this kind are performed atomically with respect to one\nanother, and the updates to\nx are commutative: no matter what order\nthey are executed,\nx has the same value after the\nsync. Thus, even\nthough different executions may cause\nx to pass through different\nintermediate states, the final result is the same. A determinacy race\nin an externally deterministic program is called a legal determinacy\nrace, and it is illegal otherwise.\nCilk guarantees the atomicity of accumulations only for accumu\nlations within the same procedure instance. Accumulations by other\nprocedure instances that operate logically in parallel are not guaran\nteed to be atomic by Cilk's runtime system, and they can cause non-\ndeterminism. Atomicity alone is not sufficient for a race to be legal,\nhowever. It must also involve commutative updates. For example,\nif the accumulation operator \"\" in foo is replaced by the op\nerator \" \", the race is illegal, because the order of execution can\n\nx\ncilk\nint\nfoo\n\nx\n\nspawn\nbar\nx\n\nspawn\nbaz\n\nsync\n\nFigure\n: Anillustration of the use of accumulation in a Cilk program. The\ninteger variable\nx may or may not be local to the procedure\nfoo. Although\ndeterminacy races occur between updates to\nx, the races are legal, since the\nupdates occur atomically.\naffect the final value of\nx, even though the updates are performed\natomically.\nThe SP-bags algorithm can be modified to accommodate legal\nraces. There are two key changes to the data structures. First, we\ncreate a shadow space to record the operator whenever an accumu\nlation or assignment occurs. (The assignment operator\nis consid\nered to be a degenerate accumulation operator which does not com\nmute with any other operators, including itself.) Second, in addition\nto procedure ID's, the SP-bags algorithm assigns each sync block a\ndistinct ID. The sync-block ID and operator are stored in a shadow\nspace whenever an accumulation occurs.\nFigure 14 extends the SP-bags algorithm of Figure 8 to detect the\ndeterminacy races in Cilk code containing accumulations. In ad\ndition to the introduction of a new action\naccumulate that deals\nwith the case when the returned result of a spawned procedure is\naccumulated, only the\nwrite action needs to be extended. A new\nshadow space called operator stores the operator for each location\nin the shared memory. When a procedure F in a sync block B writes\na location l with accumulation operator op, and it discovers that the\nprevious writer of l belongs to a P-bag, a determinacy race occurs\nonly if the previous writer is not B or if the previous writer's opera\ntor does not commutate with op. If no determinacy race occurs, the\noperator of l is updated to op. When a spawned procedure returns\nits result to procedure F and accumulates the result into a location l,\nthe operations are almost the same as the\nwrite action except that\nit is necessary to check whether the current sync block B belongs to\nany bag. If not, the unique ID for B is placed into the P-bag of F, if\nit is not there already.\nTheo\nrem\nThe extended SP-bags algorithm detects a determi\nnacy race in a Cilk program containing accumulations if and only if\nan illegal determinacy race exists.\nProof sketch:\nThe proof is similar to that of Theorem 10. Once\nagain, the \"only if\" direction is straightforward, and the hard part\nis the \"if\" direction. The extended SP-bags algorithm contains an\nadditional check when a thread performs a\nwrite and writer(l\n) be\nlongs to a P-bag. If writer(l\n) is the ID of the current sync block,\nthen l has been accumulated by the returned result of a previously\nspawned procedure in the same sync block. If the operator is also\ncommutative with operator\n(l\n), then the determinacy race is legal,\nbecause the accumulations are performed atomically. Otherwise,\nthe determinacy race is illegal and is reported. Determinacy races\ncaused by the\naccumulat\ne action are checked similarly to the ones\nby the\nwrite action.\n\nThe\nNondeterminato\nr\nThis section presents the implementation of the Nondeterminator,\nour determinacy-race detector for Cilk programs. We discuss how\nwrite a shared location l with operator op by procedure F in sync\nblock B:\nif\nFindSet(reader\n(l\n)) is a P-bag\nthen a determinacy race exists\nif\nFindSet(writer\n(l\n)) is a P-bag\nand\n(writer\n(l\n)\nB\nor op does not commutate with operator\n(l\n))\nthen a determinacy race exists\nwriter(l\n)\nF\noperator\n(l\n)\nop\naccumulate returned result of spawned procedure into a shared\nlocation l with operator op by procedure F in sync block B:\nif\nFindSet(reader\n(l\n)) is a P-bag\nthen a determinacy race exists\nif\nFindSet(writer\n(l\n)) is a P-bag\nand\n(writer\n(l\n)\nB\nor op does not commutate with operator\n(l\n))\nthen a determinacy race exists\nif\nFindSet(B)\n0/\nthen PF\n\nUnion(PF\n\nMakeSet\n(B))\nwriter(l\n)\nB\noperator\n(l\n)\nop\nFigure\n: The extended SP-bags algorithm of Figure 8 for Cilk code con\ntaining accumulations. Operations for the\nwrite action are extended. Oper\nations for the\naccumulat\ne action are performed when atomic accumulation\noccurs.\nthe Nondeterminator implements the SP-bags algorithm by modify\ning the Cilk compiler and runtime system. We describe some mod\nifications to the SP-bags algorithm that enhance the Nondetermina-\ntor's performance. Empirical data from a variety of benchmark Cilk\nprograms shows that the Nondeterminator typically runs in less than\n12 times the execution time of the original optimized program.\nThe first phase of checking a user's Cilk program is to run\nthe code through the Cilk compiler with an option that turns on\ndeterminacy-race detection. This compiler option produces object\ncode with calls to the Nondeterminator 's runtime system for every\nread and write of shared memory. In addition, the compiler inserts\nhooks that allow the Nondeterminator 's runtime system to perform\nactions for every\nspawn,\nsync, and\nreturn.\nAt runtime, before it starts executing the user code, the Nonde\nterminator sets up the reader and writer shadow spaces. We use the\nUnix memory-mapping primitive\nmmap to fix the starting address\nof each shadow space so that the shadow-space address can be ob\ntained quickly from the corresponding shared-memory address. It\nalso initializes the disjoint-set data structure.\nDuring execution of the user program, the Nondeterminator per\nforms the SP-bags algorithm (without garbage collection), modi\nfied slightly to improve performance. First, if the compiler can\ndetermine that a memory reference is to a nonshared memory re\ngion, such as a local variable whose address is never computed,\nno determinacy-race check is necessary, because no determinacy\nrace is possible. Second, we modify the SP-bags algorithm to up\ndate reader\n(l\n), as well as writer(l\n), whenever a write or accumu\nlate to a location l occurs. This change allows us to check only\nreader\n(l\n) in the code for\nwrite and\naccumulate (see Figure 8 and\nFigure 14); and in the code for\nread, we need only check writer(l\n)\nwhen reader\n(l\n) belongs to a P-bag. Third, during the execution of\na thread, we save addresses that have previously been checked in a\nsoftware cache to avoid checking them again within the same thread.\nWe have measured the performance of the Nondeterminator on\n\nProgram\nOriginal\nNondeterminator\nSlowdown\nNumber of actions\nAverage overhead\nCache-hit ratio\n(seconds)\n(seconds)\n(nanoseconds)\nmmult\nlu\nsparsky\nhutch\nheat\nfft\nmultisort\nknapsack\n3.54\n2.36\n14.91\n4.71\n2.60\n4.17\n5.22\n7.39\n32.06\n20.93\n97.02\n48.58\n21.37\n23.03\n57.94\n17.73\n9.05\n8.87\n6.51\n10.31\n8.21\n5.18\n11.09\n2.41\n317,947,466\n184,738,250\n289,381,593\n200,693,060\n125,143,001\n39,411,729\n179,988,858\n34,752,741\n89.69\n100.52\n283.74\n218.57\n149.94\n471.43\n292.86\n298.30\n77.26%\n89.14%\n44.17%\n77.29%\n78.23%\n7.39%\n42.15%\n33.34%\nFigure\n: Eight benchmark Cilk programs that were checked with the Nondeterminator. The slowdown is the ratio of the Nondeterminator runtime and the\noriginal optimized runtime of the benchmark. The total number of actions (spawns, syncs, returns, shared reads, and shared writes) is given, along with the\naverage overhead of the Nondeterminator for each action and the fraction of accesses that hit the Nondeterminator 's software cache.\neight benchmark Cilk programs:\n\nmmult -- Block multiplication of two dense 512\n\n512 matri\nces, written by Keith Randall.\n\nlu -- LU-decomposition of a dense 512\n\n512 matrix, written\nby Robert D. Blumofe.\n\nsparsky -- Cholesky factorization of a sparse 3600\n\nmatrix with 15,100 nonzeros, written by Aske Plaat and Keith\nRandall.\n\nhutch -- Barnes-Hut n-body calculation with 4096 cells,\nwritten by Keith Randall.\n\nheat -- Heat diffusion on a 4096\n\n16 mesh, written by Volker\nStrumpen.\n\nfft -- Fast Fourier transformation of a vector of length 220,\nwritten by Matteo Frigo.\n\nmultisort -- Sort a random permutation of 4 million 32-bit\nintegers, written by Matteo Frigo and Andrew Stark.\n\nknapsack -- Solve the 0-1 knapsack problem on 30 items us\ning branch and bound, written by Matteo Frigo.\nThe results of our tests, which were run on a 167-megahertz SUN\nUltrasparc with the Solaris 2.5.1 operating system, are shown in Fig\nure 15. As we can see from Figure 8 and Figure 14, the SP-bags al\ngorithm is invoked when a\nspawn,\nsync,\nreturn, shared read, or\nshared write occurs. Each of these invocations, which we call an\naction, contributes to the overhead incurred by the Nondetermina\ntor. The number of actions in each benchmark program is given in\nFigure 15.\nWe observe that the average overhead per action varies among\nthese benchmark programs, ranging from 90 nanoseconds to 472\nnanoseconds. The variation is due to the Nondeterminator 's soft\nware cache. Whenever the cache-hit ratio is large (i.e., a thread\nexhibits substantial temporal locality in its shared-memory access\npatterns), relatively few shared read or write accesses need to in\ncur the full overhead of the SP-bags algorithm. Thus, the average\noverhead per action is small. For example, the\nfft and\nknapsack\nprograms exhibit small cache-hit ratios, and thus the overhead per\naction is comparatively high. For other benchmarks, the software\ncache is reasonably effective, and the overhead per action is within\n300 nanoseconds.\nThe Nondeterminator has caught determinacy races in several\nCilk programs. For example, it caught a subtle bug in a program to\nsolve the N-queens puzzle which was included as a programming\nexample in the Cilk software distribution. The goal of the N-queens\npuzzle is to find a configuration of n queens on an n\n\nn chessboard\nsuch that no queen attacks another. The standard backtrack algo\nrithm to solve this puzzle is to place queens row by row, and back\ntrack whenever a developed configuration contains two queens that\nattack each other.\ncilk\nchar\nnqueens\nch\nar\nboard\nint\nn\nint\nrow\n\nchar\nnewboar\nd\n\nnewboard\n\nmallocrow\n\nmemcpyne\nw\nboa\nrd\n\nboard\nrow\nread\nboard\n\nfor\nj\n\nj\n\nn\nj\n\nnewboard\nro\nw\n\nj\n\nwrite\nnewboa\nrd\n\nspawn\nnqueensnew\nbo\nar\nd\nn\nrow\n\nsync\n\nFigure\n: A fragment of a Cilk program solving the N-queens puzzle. A\ndeterminacy race exists involving the commented lines in the code.\nThe recursive Cilk procedure\nnqueens in Figure 16 illustrates the\nbug in the original implementation of this backtrack algorithm. It\nis called with three arguments:\nboard, which is the current con\nfiguration of queens on the chessboard;\nn, which is the size of the\nchessboard; and\nrow, which is the row number where a queen will\nbe placed. Before a queen is placed, space for a new configuration\nnewboar\nd is allocated using\nmalloc so that the child that will be\nrecursively spawned to solve the new configuration does not over\nwrite the storage in the parent. The current configuration\nboard is\ncopied into\nnewboard using\nmemcpy. The\nspawn in the\nfor loop\ncauses the searches to be spawned in parallel to solve configurations\nin which the just-placed queen is in different columns of the current\nrow.\nWhen the\nnqueens code ws run through the Nondeterminator, it\nreported that\nboard and\nnewboard are involved in races. Specifi\ncally, a race exists between the read of\nboard in a spawned subpro\ncedure and the write of\nnewboard in its parent procedure. Since\nthe passed\nboard argument of the subprocedure points to the same\nstorage as the\nnewboard of its parent procedure, when the subpro\ncedure is reading the\nboard in\nmemcpy, the parent procedure may\nbe updating the\nnewboard at the same time, resulting in a deter\nminacy race.\nBesides the N-queens puzzle, several Cilk users have used the\nNondeterminator to discover determinacy-race bugs in their pro\ngrams, which have included a radiosity calculation for graphics ren\ndering, enumeration of magic squares, and an old version of our\nheat-diffusion benchmark. Some Cilk users have not taken advan\ntage of this tool, however, much to their detriment. In a student\n\nassignment at MIT to implement Strassen's matrix multiplication\nalgorithm in Cilk, half of the submitted codes turned out to have\ndeterminacy races that were not detected during the students' re\npeated test runs. These bugs were instantly caught when the instruc\ntors ran the programs through the Nondeterminator. The students\ncould have easily run the Nondeterminator themselves (the theory\nof which was taught in their class), but their overconfidencewas nat\nural, since their code worked on every test run. Indeed, determinacy\nraces are latent bugs that can escape extensive testing, rearing their\nugly heads only intermittently and confounding naive debugging at\ntempts. With the release of the Nondeterminator as part of the over\nall Cilk system, we hope more Cilk programmers will routinely use\nthe Nondeterminator as a debugging tool to produce more reliable\nparallel code.\n\nRelated\nw\no\nrk\nThis section briefly reviews related work on the problem of de\ntecting determinacy races in parallel programs. A comparison of\nthe asymptotic time and space requirements of the Nondetermina\ntor with work in the literature was presented in Figure 6.\nBernstein [3] identifies determinacy races as a cause of nondeter\nministic behavior. Netzer and Miller [15] present a formal model\nfor understanding race conditions in parallel programs, distinguish\ning determinacy races from atomicity races. They reference several\nalgorithms for atomicity-race detection, but we do not discuss this\ntype of race detection here. Static analysis of parallel programs to\nuncover nondeterminacy has been studied extensively, for example,\nin [9, 13]. Various systems have been developed for determinacy-\nrace detection that do not allow nested parallelism, as for exam\nple [2].\nWe now review related work on determinacy-race detection for\nprograms with nested parallelism.\nNudler and Rudolph [16] give an \"English-Hebrew labeling\" al\ngorithm that detects determinacy races in programs with series-\nparallel dependences, but their model also allows messages between\nthreads, which produces a richer and more difficult class of pro\ngrams to check. Their algorithm assigns to each thread a pair of la\nbels: an \"English\" label, which is produced by performing a left-\nto-right preorder numbering on the task tree, and a \"Hebrew\" label,\nwhich is produced symmetrically for a right-to-left ordering. To de\ntermine whether two threads operate logically in parallel, a compar\nison of the labels of two threads suffices.\nDinning and Schonberg [7] improve the performance of the\nEnglish-Hebrew labeling algorithm by \"task recycling,\" but at the\ncost of failing to detect some determinacy races. Each thread (task)\nhas a unique task identifier, and a version number. In order to save\nspace, a task identifier can be reassigned to another thread during\nthe program execution. Each thread also maintains a parent vector\ncontaining the largest version number of its ancestor threads. With\nthe parent vector, checking whether two blocks are logically paral\nlel is reduced to one access of the parent vector and one compar\nison, which are constant-cost operations. Dinning and Schonberg\ngive performance data indicating a slowdown of between 3 and 11\nto check between 50 and 80 percent of potential determinacy races.\nMellor-Crummey [12] proposes a scheme called \"offset-span la\nbeling\" in programs with nested fork-join parallelism, a model that\nexhibits only series-parallel dependences. The idea of his scheme is\nto store a list of labels for each executing thread. Whenever a thread\nspawns, the length of the list grows by one, and whenever a thread\nsyncs, the length is reduced by one. This strategy avoids a problem\nin the English-Hebrew labeling algorithm whereby the length of a\nlabel might grow in proportion to the number of spawn operations\nencountered in the execution path.\nMin and Choi [14] propose a determinacy-race detection algo\nrithm that piggybacks on a protocol for distributed shared-memory.\nThe idea is that a determinacy race occurs when a processor accesses\nmemory that was previously accessed by another processor. Conse\nquently, determinacy-race detection can be performed at the same\ntime as the distributed shared-memory protocol, thereby avoiding\nindividual access checks. This reduced overhead is achieved at the\ncost of additionally storing the history of accesses of each shared lo\ncation, however. Moreover, the length of the history is proportional\nto the depth of nested parallelism.\nSteele [19] proposes a scheme to detect determinacy races in\na programming model with asynchronous threads of control. His\nscheme requires each location to maintain state information record\ning the sequence of threads that have accessed the location as well\nas the type of access performed. In addition, each thread maintains\na responsibility set of which locations it has accessed. The Nonde\nterminator 's reader and writer shadow spaces are similar to Steele's\nlocation state, but rather than keeping lists, in our scheme only a sin\ngle reader and writer need be stored per location. Although he does\nnot mention it, the programs that he is capable of checking exhibit\nseries-parallel dependences. Steele provides an implementation of\nhis algorithm in the Scheme programming language.\nThe space and time requirements of all these determinacy-race\ndetection algorithms are larger than those for the SP-bags algorithm.\nOur algorithm spends almost constant time checking each read and\nwrite access, and it uses only a constant factor more memory than\ndoes the program itself.\n\nConclusion\nTo conclude this paper, we shall discuss some of the open problems\narising out of our work. These problems include how to parallelize\nour algorithm, whether a faster algorithm for determinacy-race de\ntection might exist, and how to tolerate intended nondeterminism\nwhile still catching other determinacy races.\nThe SP-bags algorithm seems inherently serial, because it heavily\nrelies on the serial execution order of the parallel program. Never\ntheless, we feel that it may be possible to develop a parallel version\nof the SP-bags algorithm. We have started investigating a parallel\nscheme in which each of several processors executing the program\nuses the SP-bags algorithm locally, but when a remote child proce\ndure returns, it reconciles its shadow spaces in a manner similar to\nthe BACKER algorithm [4] for maintaining dag consistency. Such a\nresult may be mostly of theoretical interest, however, since debug\nging is usually done in the development phase of a program using\nsmall data sets, and thus typically, the performance of the debugger\nis not a crucial concern.\nLinear-time algorithms for the least-common-ancestors algo\nrithm exist in the literature [10, 18], and it is natural to wonder\nwhether a determinacy-race detector exists that operates in linear\ntime, instead of the almost-linear-time performance of the SP-bags\nalgorithm. The attraction of Tarjan's algorithm, as opposed to exist\ning linear-time algorithms and the seminal algorithm given by Aho,\nHopcroft, and Ullman [1], is that it operates, in Mellor-Crummey's\nwords [12], \"on the fly.\" That is, the least common ancestors can\nbe queried during a simple tree walk without ever requiring the en\ntire tree to be expanded at any time. We expect that the discovery\nof a linear-time on-the-fly least-common-ancestors algorithm would\nhave direct application to determinacy-race detection.\nSome programs may intentionally contain nondeterminism. How\ncan a debugging program, such as the Nondeterminator, tolerate in\ntended nondeterminism while still catching unintentional determi\nnacy races?\nOne strategy that Cilk users have used successfully in debugging\nnondeterministic codes is for the user to \"turn off\" the intentional\n\nnondeterminism in his code so that he can debug a deterministic ver\nsion of his program. Our experience is that intentional nondetermin\nism does not occur in many places in user programs, and the user\nusually has the ability to disable it. For example, in our\nSocrates\nchess-playing program, a switch was included that could turn off the\naspects of the program that produced nondeterministic behavior. Of\ncourse, if the user's bug is in the nondeterministic part of his code,\nthis strategy will not work, but knowing that the deterministic part\ncontains no determinacy races is nevertheless extremely helpful dur\ning debugging.\nAnother strategy that the Nondeterminator supports is to allow\nthe user to turn off monitoring of certain variables. For example,\nour benchmark\nknapsack has an intentional determinacy race when\nindependent threads atomically update the variable containing the\nbound in its branch-and-bound search. To check this code, we sim\nply disabled the monitoring of the location containing the bound. A\ndisadvantage of this strategy, however, is that turning off the moni\ntoring of one location may hide inadvertent nondeterminism in other\nlocations. Thus, it is not clear what is guaranteed when such a pro\ngram passes the Nondeterminator test. Nevertheless, turning off the\nmonitoring of certain locations seems to be a useful strategy.\nThe Nondeterminator has been included in the latest Cilk re\nlease [20]. The Nondeterminator in the release runs about 25 per\ncent slower than the one in this paper. We traded off some perfor\nmance for usability and simplicity. The released version provides\nmore user options in the runtime system, such as a switch for de\nciding whether floating-point operations should be deemed commu\ntative. (They are not, due to round-off error, but sometimes users\nwish to ignore the minor nondeterminacies that result.) To simplify\nthe maintenance of the code, the released version also lacks some\naggressive compiler optimizations that reduce the amount of instru\nmentation. Software, the user's manual, and other related informa\ntion about Cilk and its Nondeterminator are available via the World\nWide Web at\nhttpth\neor\nyl\ncs\nmi\nt\nedu\nc\nil\nk.\nAckno\nwledgments\nWe would like to thank Arvind and his dataflow group at MIT\nfor their insightful discussions about the determinacy-race problem.\nLarry Rudolph of MIT acquainted us with much related work, in\ncluding his own. Bradley Kuszmaul of Yale University was helpful\nin providing pointers on related work. Long ago, Guy Blelloch of\nCarnegie Mellon University pointed out the series-parallel structure\nof Cilk dags. David Karger and Matt Levine of MIT serendipitously\nrenewed our interest in Tarjan's least-common-ancestors algorithm\njust before we discovered it was relevant to the determinacy-race\nproblem. The members of our Cilk development group--Robert\nBlumofe of University of Texas at Austin and Matteo Frigo, Ching\nLaw, Phil Lisiecki, Aske Plaat, Keith Randall, Bin Song, Andrew\nStark, and Volker Strumpen of MIT--provided many helpful sug\ngestions and donated their Cilk application programs for testing.\nAlso, many thanks to Keith (Schwartzenegger) Randall for suggest\ning the name \"Nondeterminator.\"\nReferences\n[1] A. V. Aho, J. E. Hopcroft, and J. D. Ullman. On finding lowest com\nmon ancestor in trees. SIAM Journal on Computing, 5(1):115-132,\nMarch 1976.\n[2] T. R. Allen and D. A. Padua. Debugging Fortran on a shared mem\nory machine. In Proceedings of the 1987 International Conference on\nParallel Processing, pages 721-727, August 1987.\n[3] A. J. Bernstein. Analysis of programs for parallel processing. IEEE\nTransactions on Electronic Computers, EC-15(5):757-763, October\n1966.\n[4] Robert D. Blumofe, Matteo Frigo, Chrisopher F. Joerg, Charles E.\nLeiserson, and Keith H. Randall. An analysis of dag-consistent dis\ntributed shared-memory algorithms.\nIn Proceedings of the Eighth\nAnnual ACM Symposium on Parallel Algorithms and Architectures\n(SPAA), pages 297-308, Padua, Italy, June 1996.\n[5] Robert D. Blumofe, Christopher F. Joerg, Bradley C. Kuszmaul,\nCharles E. Leiserson, Keith H. Randall, and Yuli Zhou.\nCilk: An\nefficient multithreaded runtime system. In Proceedings of the Fifth\nACM SIGPLAN Symposium on Principles and Practice of Parallel\nProgramming (PPoPP), pages 207-216, Santa Barbara, California,\nJuly 1995.\n[6] Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. Intro-\nduction to Algorithms. The MIT Press and McGraw-Hill Book Com\npany, 1990.\n[7] Anne Dinning and Edith Schonberg. An empirical comparison of mon\nitoring algorithms for access anomaly detection. In Proceedings of the\nSecond ACM SIGPLAN Symposium on Principles & Practice of Par-\nallel Programming (PPoPP), pages 1-10. ACM Press, 1990.\n[8] R. J. Duffin. Topology of series-parallel networks. Journal of Mathe-\nmatical Analysis and Applications, 10:303-318, 1965.\n[9] Perry A. Emrath and Davis A. Padua. Automatic detection of nonde\nterminacy in parallel programs. In Proceedings of the Workshop on\nParallel and Distributed Debugging, pages 89-99, Madison, Wiscon\nsin, May 1988.\n[10] Dov Harel and Robert Endre Tarjan. Fast algorithms for finding nearest\ncommon ancestors. SIAM Journal on Computing, 13(2):338-355, May\n1984.\n[11] P. A. MacMahon. The combination of resistances. The Electrician,\nApril 1892.\n[12] John Mellor-Crummey. On-the-fly detection of data races for programs\nwith nested fork-join parallelism.\nIn Proceedings of Supercomput-\ning'91, pages 24-33. IEEE Computer Society Press, 1991.\n[13] John Mellor-Crummey. Compile-time support for efficient data race\ndetection in shared-memory parallel programs. In Proceedings of the\nACM/ONR Workshop on Parallel and Distributed Debugging, pages\n129-139, San Diego, California, May 1993. ACM Press.\n[14] Sang Lyul Min and Jong-Deok Choi. An efficient cache-based access\nanomaly detection scheme. In Proceedings of the Fourth International\nConference on Architectural Support for Programming Languages and\nOperating Systems (ASPLOS), pages 235-244, Palo Alto, California,\nApril 1991.\n[15] Robert H. B. Netzer and Barton P. Miller. What are race conditions?\nACM Letters on Programming Languages and Systems, 1(1):74-88,\nMarch 1992.\n[16] Itzhak Nudler and Larry Rudolph. Tools for the efficient development\nof efficient parallel programs. In Proceedings of the First Israeli Con-\nference on Computer Systems Engineering, May 1986.\n[17] John Riordan and C. E. Shannon. The number of two-terminal series-\nparallel networks.\nJournal of Mathematics and Physics, 21:83-93,\n1942.\n[18] Baruch Schieber and Uzi Vishkin. On finding lowest common ances\ntors: Simplification and parallelization. SIAM Journal on Computing,\n17(6):1253-1262, December 1988.\n[19] Guy L. Steele Jr. Making asynchronous parallelism safe for the world.\nIn Proceedings of the Seventeenth Annual ACM Symposium on Princi-\nples of Programming Languages (POPL), pages 218-231. ACM Press,\n1990.\n[20] Supercomputing Technology Group, Massachusetts Institute of Tech\nnology, 545 Technology Square, Cambridge, Massachusetts 02139.\nCilk-5.0 (Beta 1) Reference Manual, March 1997. Available on the\nWorld Wide Web at URL \"httpthe\nory\nlc\nsm\nit\ned\nu\ncil\nk\".\n[21] Robert Endre Tarjan.\nEfficiency of a good but not linear set union\nalgorithm.\nJournal of the Association for Computing Machinery,\n22(2):215-225, April 1975.\n[22] Robert Endre Tarjan.\nApplications of path compression on bal\nanced trees.\nJournal of the Association for Computing Machinery,\n26(4):690-715, October 1979.\n[23] Jacobo Valdes. Parsing Flowcharts and Series-Parallel Graphs. PhD\nthesis, Stanford University, December 1978. STAN-CS-78-682."
    }
  ]
}