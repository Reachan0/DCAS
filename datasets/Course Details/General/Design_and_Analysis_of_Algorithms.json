{
  "course_name": "Design and Analysis of Algorithms",
  "course_description": "No description found.",
  "topics": [
    "Engineering",
    "Computer Science",
    "Algorithms and Data Structures",
    "Computer Networks",
    "Cryptography",
    "Mathematics",
    "Applied Mathematics",
    "Engineering",
    "Computer Science",
    "Algorithms and Data Structures",
    "Computer Networks",
    "Cryptography",
    "Mathematics",
    "Applied Mathematics"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nRecitations: 1 session / week, 1 hour / session\n\nPrerequisites\n\nThis course is the header course for the Theory of Computation concentration. You are expected, and strongly encouraged, to have taken:\n\n6.006 Introduction to Algorithms\n\n6.042J / 18.062J Mathematics for Computer Science\n\nPetitions for waivers will be considered by the course staff. Students will be responsible for material covered in prerequisites.\n\nCourse Description\n\nThis course assumes that students know how to analyze simple algorithms and data structures from having taken\n6.006\n. It introduces students to the design of computer algorithms, as well as analysis of sophisticated algorithms.\n\nCourse Objectives\n\nUpon completion of this course, students will be able to do the following:\n\nAnalyze the asymptotic performance of algorithms.\n\nWrite rigorous correctness proofs for algorithms.\n\nDemonstrate a familiarity with major algorithms and data structures.\n\nApply important algorithmic design paradigms and methods of analysis.\n\nSynthesize efficient algorithms in common engineering design situations.\n\nCourse Outcomes\n\nStudents who complete the course will have demonstrated the ability to do the following:\n\nArgue the correctness of algorithms using inductive proofs and invariants.\n\nAnalyze worst-case running times of algorithms using asymptotic analysis.\n\nDescribe the divide-and-conquer paradigm and explain when an algorithmic design situation calls for it. Recite algorithms that employ this paradigm. Synthesize divide-and-conquer algorithms. Derive and solve recurrences describing the performance of divide-and-conquer algorithms.\n\nDescribe the dynamic-programming paradigm and explain when an algorithmic design situation calls for it. Recite algorithms that employ this paradigm. Synthesize dynamic-programming algorithms, and analyze them.\n\nDescribe the greedy paradigm and explain when an algorithmic design situation calls for it. Recite algorithms that employ this paradigm. Synthesize greedy algorithms, and analyze them.\n\nExplain the major graph algorithms and their analyses. Employ graphs to model engineering problems, when appropriate. Synthesize new graph algorithms and algorithms that employ graph computations as key components, and analyze them.\n\nExplain the different ways to analyze randomized algorithms (expected running time, probability of error). Recite algorithms that employ randomization. Explain the difference between a randomized algorithm and an algorithm with probabilistic inputs.\n\nAnalyze randomized algorithms. Employ indicator random variables and linearity of expectation to perform the analyses. Recite analyses of algorithms that employ this method of analysis.\n\nExplain what amortized running time is and what it is good for. Describe the different methods of amortized analysis (aggregate analysis, accounting, potential method). Perform amortized analysis.\n\nExplain what competitive analysis is and to which situations it applies. Perform competitive analysis.\n\nCompare between different data structures. Pick an appropriate data structure for a design situation.\n\nExplain what an approximation algorithm is, and the benefit of using approximation algorithms. Be familiar with some approximation algorithms, including algorithms that are PTAS or FPTAS. Analyze the approximation factor of an algorithm.\n\nTextbook\n\nThe primary written reference for the course is:\n\nCormen, Thomas, Charles Leiserson, et al.\nIntroduction to Algorithms\n. 3rd ed. MIT Press, 2009. ISBN: 9780262033848. [Preview with\nGoogle Books\n]\n\nIn previous semesters the course has used the first or second edition of this text. We will be using material and exercise numbering from the third edition, making earlier editions unsuitable as substitutes.\n\nLectures and Recitations\n\nYou are responsible for material presented in lectures, including oral comments made by the lecturer.\n\nYou are also responsible for material presented in recitations. Attendance in recitation has been well correlated in the past with exam performance. Recitations also give you a more personalized opportunity to ask questions and interact with the course staff. Your recitation instructor, together with the lecturers, will assign your final grade.\n\nGrading Policy\n\nThe final grade will be based on the problem sets, two evening quizzes, and a final given during final exam week.\n\nThe grading breakdown is as follows:\n\nACTIVITIES\n\nPERCENTAGES\n\nProblem Sets\n\n30%\n\nQuiz 1\n\n20%\n\nQuiz 2\n\n20%\n\nFinal Exam\n\n30%\n\nAlthough the problem sets account for only 20% of your final grade, you are required to at least attempt them. The following table shows the impact of failing to attempt problems:\n\nQUESTIONS\n\nIMPACT\n\nNone\n\n1/100 of a letter grade\n\n1/10 of a letter grade\n\n1/5 of a letter grade\n\n1/4 of a letter grade\n\n1/3 of a letter grade\n\n1/2 of a letter grade\n\nOne letter grade\n\nTwo letter grades\n\n9 or more\n\nFail the course\n\nPlease observe that this table is for\nquestions\nskipped, not\nproblem sets\n.\n\nCollaboration Policy\n\nThe goal of homework is to give you practice in mastering the course material. Consequently, you are encouraged to collaborate on problem sets. In fact, students who form study groups generally do better on exams than do students who work alone. If you do work in a study group, however, you owe it to yourself and your group to be prepared for your study group meeting. Specifically, you should spend at least 30-45 minutes trying to solve each problem beforehand. If your group is unable to solve a problem, talk to other groups or ask your recitation instructor.\n\nYou must write up each problem solution by yourself without assistance, however, even if you collaborate with others to solve the problem. You are asked on problem sets to identify your collaborators. If you did not work with anyone, you should write \"Collaborators: none.\" If you obtain a solution through research (e.g., on the web), acknowledge your source, but write up the solution in your own words. It is a violation of this policy to submit a problem solution that you cannot orally explain to a member of the course staff.\n\nNo collaboration whatsoever is permitted on quizzes or exams. Plagiarism and other dishonest behavior cannot be tolerated in any academic environment that prides itself on individual accomplishment. If you have any questions about the collaboration policy, or if you feel that you may have violated the policy, please talk to one of the course staff. Although the course staff is obligated to deal with cheating appropriately, we are more understanding and lenient if we find out from the transgressor himself or herself rather than from a third party or discover it on our own.",
  "files": [
    {
      "category": "Assignment",
      "title": "Class on Design and Analysis of Algorithms, Problem Set 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/43d974c6c4bfc5d88086a67e023fd88d_MIT6_046JS15_pset1.pdf",
      "content": "Design and Analysis of Algorithms\nFebruary 5, 2015\nMassachusetts Institute of Technology\nProfs. Erik Demaine, Srini Devadas, and Nancy Lynch\nProblem Set 1\nProblem Set 1\nThis problem set is due at 11:59pm on Thursday, February 12, 2015.\nThis assignment, like later assignments, consists of exercises and problems. Hand in solutions\nto the problems only. However, we strongly advise that you work out the exercises also, since they\nwill help you learn the course material. You are responsible for the material they cover.\nEach submitted solution should start with\nyour name, the course number, the problem number, your recitation section, the date, and the\nnames of any students with whom you collaborated.\nWe will often ask you to \"give an algorithm\" to solve a problem. Your write-up should take the\nform of a short essay. Start by defining the problem you are solving and stating what your results\nare. Then provide:\n1. A description of the algorithm in English and, if helpful, pseudo-code.\n2. A proof (or proof sketch) for the correctness of the algorithm.\n3. An analysis of the running time.\nWe will give full credit only for correct solutions that are described clearly.\nExercise 1-1. Asymptotic Growth\nSort all the functions below in increasing order of asymptotic (big-O) growth. If some have the\nsame asymptotic growth, then be sure to indicate that. As usual, lg means base 2.\n1. 5n\n2. 4 lg n\n3. 4 lg lg n\n4. n4\n5. n1/2 lg4 n\n(lg n)5 lg n\n6.\nlg n\n7. n\n5n\n8.\n9. 4n4\n44n\n10.\n55n\n11.\n6.046J/18.410J\nPlease turn in each problem solution separately.\n\nProblem Set 1\n55n\n12.\nn\n13. n\n1/5\nn/4\n14. n\n15. (n/4)n/4\nExercise 1-2. Solving Recurrences\nGive asymptotic upper and lower bounds for T (n) in each of the following recurrences. Assume\nthat T (n) is constant for n ≤ 2. Make your bounds as tight as possible, and justify your answers.\n(a) T (n) = 4T (n/4) + 5n\n(b) T (n) = 4T (n/5) + 5n\n(c) T (n) = 5T (n/4) + 4n\n(d) T (n) = 25T (n/5) + n2\n(e) T (n) = 4T (n/5) + lg n\n(f) T (n) = 4T (n/5) + lg5 n √ n\n(g) T (n) = 4T ( √ n) + lg5 n\n(h) T (n) = 4T ( √ n) + lg2 n\n√\n(i) T (n) = T ( n) + 5\n(j) T (n) = T (n/2) + 2T (n/5) + T (n/10) + 4n\nProblem 1-1. Restaurant Location [25 points]\nDrunken Donuts, a new wine-and-donuts restaurant chain, wants to build restaurants on many\nstreet corners with the goal of maximizing their total profit.\nThe street network is described as an undirected graph G = (V, E), where the potential restaurant\nsites are the vertices of the graph. Each vertex u has a nonnegative integer value pu, which describes\nthe potential profit of site u. Two restaurants cannot be built on adjacent vertices (to avoid self-\ncompetition). You are supposed to design an algorithm that outputs the chosen set U ⊆ V of sites\n\nthat maximizes the total profit\nu∈U pu.\nFirst, for parts (a)-(c), suppose that the street network G is acyclic, i.e., a tree.\n(a) [5 points]\nConsider the following \"greedy\" restaurant-placement algorithm: Choose the highest-\nprofit vertex u0 in the tree (breaking ties according to some order on vertex names) and\nput it into U. Remove u0 from further consideration, along with all of its neighbors\nin G. Repeat until no further vertices remain.\nGive a counterexample to show that this algorithm does not always give a restaurant\nplacement with the maximum profit.\n\nProblem Set 1\n(b) [9 points]\nGive an efficient algorithm to determine a placement with maximum profit.\n(c) [6 points]\nSuppose that, in the absence of good market research, DD decides that all sites are\nequally good, so the goal is simply to design a restaurant placement with the largest\nnumber of locations. Give a simple greedy algorithm for this case, and prove its\ncorrectness.\n(d) [5 points]\nNow suppose that the graph is arbitrary, not necessarily acyclic. Give the fastest cor\nrect algorithm you can for solving the problem.\nProblem 1-2. Radio Frequency Assignment [25 points]\nProf. Wheeler at the Federal Communications Commission (FCC) has a huge pile of requests from\nradio stations in the Continental U.S. to transmit on radio frequency 88.1 FM. The FCC is happy to\ngrant all the requests, provided that no two of the requesting locations are within Euclidean distance\n1 of each other (distance 1 might mean, say, 20 miles). However, if any are within distance 1, Prof.\nWheeler will get annoyed and reject the entire set of requests.\nSuppose that each request for frequency 88.1 FM consists of some identifying information plus\n(x, y) coordinates of the station location. Assume that no two requests have the same x coordinate,\nand likewise no two have the same y coordinate. The input includes two sorted lists, Lx of the\nrequests sorted by x coordinate and Ly of the requests sorted by y coordinate.\n(a) [3 points]\nSuppose that the map is divided into a square grid, where each square has dimensions\n1 × 1\n2. Why must the FCC reject the set of requests if two requests are in, or on the\nboundary of, the same square?\n(b) [14 points]\nDesign an efficient algorithm for the FCC to determine whether the pile of requests\ncontains two that are within Euclidean distance 1 of each other; if so, the algorithm\nshould also return an example pair. For full credit, your algorithm should run in\nO(n lg n) time, where n is the number of requests.\nHint: Use divide-and-conquer, and use Part (a).\n(c) [8 points]\nDescribe how to modify your solution for Part (b) to determine whether there are three\nrequests, all within distance 1 of each other. For full credit, your algorithm should run\nin O(n lg n) time, where n is the number of requests.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Class on Design and Analysis of Algorithms, Solutions to Problem Set 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/b14791a78432996a1d403e8b1deb0749_MIT6_046JS15_pset1sols.pdf",
      "content": "Design and Analysis of Algorithms\nFebruary 15, 2015\nMassachusetts Institute of Technology\nProfs. Erik Demaine, Srini Devadas, and Nancy Lynch\nProblem Set 1 Solutions\nProblem Set 1 Solutions\nThis problem set is due at 11:59pm on Thursday, February 12, 2015.\nExercise 1-1. Asymptotic Growth\nSort all the functions below in increasing order of asymptotic (big-O) growth. If some have the\nsame asymptotic growth, then be sure to indicate that. As usual, lg means base 2.\n1. 5n\n2. 4 lg n\n3. 4 lg lg n\n4. n4\n5. n1/2 lg4 n\n(lg n)5 lg n\n6.\nlg n\n7. n\n5n\n4n4\n8.\n9.\n44n\n10.\n55n\n11.\n55n\n12.\nn\n13. n\n1/5\nn/4\n14. n\n15. (n/4)n/4\nSolution: 4 lg lg n < 4 lg n < n1/2 lg4 (n) < 5n < n4\n< (lg n)5 lg n\n1/5\n< nlg n < nn\n< 5n < 55n\n< (n/4)n/4 < nn/4 < 4n4 < 44n < 55n\nExercise 1-2. Solving Recurrences\nGive asymptotic upper and lower bounds for T (n) in each of the following recurrences. Assume\nthat T (n) is constant for n ≤ 2. Make your bounds as tight as possible, and justify your answers.\n6.046J/18.410J\n\nProblem Set 1 Solutions\n(a) T (n) = 4T (n/4) + 5n\nSolution: T (n) = Θ(n lg n), Case 2 of the Master Theorem.\n(b) T (n) = 4T (n/5) + 5n\nSolution: T (n) = Θ(n), Case 3 of the Master Theorem.\n(c) T (n) = 5T (n/4) + 4n\nSolution: T (n) = Θ(nlog4(5)) = Θ(nlg\n√\n5), Case 1 of the Master Theorem.\n(d) T (n) = 25T (n/5) + n2\nSolution: T (n) = Θ(n2 lg(n)). Case 2 of the Master Theorem.\n(e) T (n) = 4T (n/5) + lg n\nSolution: T (n) = Θ(nlog5 4). Case 1 of the Master Theorem.\n(f) T (n) = 4T (n/5) + lg5 n √ n\nSolution: T (n) = Θ(nlog5 4). Case 1 of the Master Theorem.\n(g) T (n) = 4T ( √ n) + lg5 n\nSolution: Change variables. Assume that n is a power of 2, let n = 2m . We get\nT (2m) = 4T (2m/2) + m5 .\nIf we define S(m) = T (2m), then we get the recurrence S(m) = S(m/2) + m5. By\ncase 3 of the Master Theorem, S(m) = Θ(m5), that is, T (2m) = Θ(m5).\nChanging the variable back (m = lg n), we have T (n) = Θ(lg5(n)).\n(h) T (n) = 4T ( √ n) + lg2 n\nSolution: Similar to the previous case. Let n = 2m. We get T (2m) = 4T (2m/2)+ m2 .\nLetting S(m) = T (2m), we get S(m) = 4S(m/2) + m2 . By case 2 of the Master\nTheorem, S(m) = Θ(m2 lg m), that is, T (2m) = Θ(m2 lg m).\nChanging the variable back (m = lg n), we have T (n) = Θ(lg2 n lg lg n).\n\nProblem Set 1 Solutions\n√\n(i) T (n) = T ( n) + 5\nSolution: Again similar. Let n = 2m. We get T (2m) = T (2m/2) + 5.\nLetting S(m) = T (2m), we get S(m) = S(m/2)+5. This solves to S(m) = Θ(lg m),\nso T (2m) = Θ(lg m). Changing the variable back, we get T (n) = Θ(lg lg n).\n(j) T (n) = T (n/2) + 2T (n/5) + T (n/10) + 4n\nSolution: Θ(n lg n), using explicit trees.\nProblem 1-1. Restaurant Location [25 points]\nDrunken Donuts, a new wine-and-donuts restaurant chain, wants to build restaurants on many\nstreet corners with the goal of maximizing their total profit.\nThe street network is described as an undirected graph G = (V, E), where the potential restaurant\nsites are the vertices of the graph. Each vertex u has a nonnegative integer value pu, which describes\nthe potential profit of site u. Two restaurants cannot be built on adjacent vertices (to avoid self-\ncompetition). You are supposed to design an algorithm that outputs the chosen set U ⊆ V of sites\n\nthat maximizes the total profit\nu∈U pu.\nFirst, for parts (a)-(c), suppose that the street network G is acyclic, i.e., a tree.\n(a) [5 points]\nConsider the following \"greedy\" restaurant-placement algorithm: Choose the highest-\nprofit vertex u0 in the tree (breaking ties according to some order on vertex names) and\nput it into U. Remove u0 from further consideration, along with all of its neighbors\nin G. Repeat until no further vertices remain.\nGive a counterexample to show that this algorithm does not always give a restaurant\nplacement with the maximum profit.\nSolution: E.g., the tree could be a line (9, 10, 9).\n(b) [9 points]\nGive an efficient algorithm to determine a placement with maximum profit.\nSolution:\n[Algorithm]\nWe can use dynamic programming to solve the problem in O(n) time. First pick any\nnode u0 as the root of the tree and sort all the nodes following a depth-first search\n\nProblem Set 1 Solutions\n(DFS). Store the sorted nodes in array N. Due to the definition of DFS, a parent node\nappears earlier than all of its children in N.\nFor each node v, define A(v), the best cost of a placement in the subtree rooted at v if\nv is included, and B(v), the best cost of a placement in the subtree rooted at v if v is\nnot included. The following recursion equations can be developed for A() and B():\nIf v is a leaf, then A(v) = pv and B(v) = 0.\nIf v is not a leaf, then\nA(v) = pv +\nB(u)\nu∈v.children\nB(v) =\nmax(A(u), B(u))\nu∈v.children\nFor each node v in N, in the reverse order, compute A(v) and B(v). Finally the\nmax(A(u0), B(u0)) is the maximum profit.\nThe placement achieving this maximum profit can be derived by recursively compar\ning A() and B() starting from the root. The root u0 should be included if A(u0) >\nB(u0) and excluded otherwise. If u0 is excluded, we go to all of u0's children and\nrepeat the step; if u0 is included, we go to all of u0's grandchildren and repeat the\nstep. This algorithm outputs an optimal placement by making one pass of the tree.\n[Correctness]\nIn the base case where v is a leaf node, the algorithm outputs the optimal placement\nwhich is to include the node.\nIn an optimal placement, a node v is either included, which removes all its children, or\nnot, which adds no constraints. By induction, if all the children of v have correct A()\nand B() values, then A(v) and B(v) will also be correct and the maximum profit at v\nis derived. Since the array N is sorted using DFS and processed in the reverse order,\nchild nodes are guaranteed to be processed before their parents.\n[Timing Analysis]\nSorting all nodes using DFS takes O(n) time. The time to compute A(v) and B(v),\ngiven the values for the children, is proportional to degree(v). So the total time is the\nsum of all the degrees of all the nodes, which is O(|E|) where |E| is the total number\nof edges. For a tree structure, |E| = n - 1 so the time for finding all A() and B() is\nO(n). Finally, using the derived A() and B() to find the optimal placement visits each\nnode once and thus is O(n).\nOverall, the algorithm has O(n) complexity.\n(c) [6 points]\nSuppose that, in the absence of good market research, DD decides that all sites are\nequally good, so the goal is simply to design a restaurant placement with the largest\nnumber of locations. Give a simple greedy algorithm for this case, and prove its\ncorrectness.\nSolution:\nP\nP\n\nProblem Set 1 Solutions\n[Algorithm]\nSimilar to Part (b), pick any node u0 as the root of the tree and sort all the nodes\nfollowing a depth-first search (DFS) and store the sorted nodes in array N. For each\nvalid node in N, in the reverse order, include it and remove its parent from N.\n[Correctness]\nClaim: This greedy algorithm yields an optimal placement.\nLemma 1: For any tree with equal node weights, there is an optimal placement that\ncontains all the leaves.\nIf there exists an optimal placement O that excludes some leaf v. Simply add v to O\nand if necessary, remove its parent. The result is no worse than placement O. Repeat\nfor all leaves until we have an optimal placement with all the leaves included.\nThe main claim can then be proved using the Lemma 1.\nDue to DFS sorting, the first valid node in N, in reverse order, must be a leaf node.\nAccording to Lemma 1, it can be included in the optimal placement and its parent\nexcluded. When nodes from N are processed in reverse order, each processed node is\nthe last valid one in the current N and is thus a leaf. And including the leaf is part of\nan optimal solution for the corresponding subtree. Overall, an optimal placement is\nderived for the original tree.\n[Timing Analysis]\nSorting nodes using DFS takes O(n) time. The greedy algorithm also takes O(n) time\nsince it processes each node once. Overall the algorithm has O(n) complexity.\n(d) [5 points]\nNow suppose that the graph is arbitrary, not necessarily acyclic. Give the fastest cor\nrect algorithm you can for solving the problem.\nSolution:\nA simple algorithm is to try all possible subsets of vertices for U (2V\nsubsets in total), test whether each has the required independence property (only one\nnode should be included for each edge, |E| edges in total), compute the total profit\nfor each valid solution (which takes O(V )), and take the best. This algorithm runs in\nO(2V |E|) time. This is the intended solution.\nIn fact, this problem is exactly Maximum Independent Set problem, which is known\nto be NP-complete. So unless P = NP, it has no polynomial-time algorithm. (In fact,\nassuming something stronger called the Exponential Time Hypothesis, there is no\n2o(V )-time algorithm.)\nProblem 1-2. Radio Frequency Assignment [25 points]\nProf. Wheeler at the Federal Communications Commission (FCC) has a huge pile of requests from\nradio stations in the Continental U.S. to transmit on radio frequency 88.1 FM. The FCC is happy to\n\nProblem Set 1 Solutions\ngrant all the requests, provided that no two of the requesting locations are within Euclidean distance\n1 of each other (distance 1 might mean, say, 20 miles). However, if any are within distance 1, Prof.\nWheeler will get annoyed and reject the entire set of requests.\nSuppose that each request for frequency 88.1 FM consists of some identifying information plus\n(x, y) coordinates of the station location. Assume that no two requests have the same x coordinate,\nand likewise no two have the same y coordinate. The input includes two sorted lists, Lx of the\nrequests sorted by x coordinate and Ly of the requests sorted by y coordinate.\n(a) [3 points]\nSuppose that the map is divided into a square grid, where each square has dimensions\n2 × 2\n1 . Why must the FCC reject the set of requests if two requests are in, or on the\nboundary of, the same square?\n√\nSolution: Because they are within distance 1. ( 2/2 < 1.)\n(b) [14 points]\nDesign an efficient algorithm for the FCC to determine whether the pile of requests\ncontains two that are within Euclidean distance 1 of each other; if so, the algorithm\nshould also return an example pair. For full credit, your algorithm should run in\nO(n lg n) time, where n is the number of requests.\nHint: Use divide-and-conquer, and use Part (a).\nSolution:\n[Algorithm]\nThe intended divide-and-conquer algorihtm is as follows.\nDivide: Use list Lx to find a vertical line that divides the requests into two subsets of\nsize approximately n/2, and does not pass through any of the requests. The Lx and\nLy for both subsets should be computed.\nConquer: If a subset contains less than or equal to a constant C (e.g., C = 2) requests,\ncheck if any pair of them is within distance 1 and return the pair if it exists. Otherwise\nif the subset contains more than C requests, divide the subset into smaller subsets.\nMerge: We only need to check pairs of requests in a \"stripe\" S of width 2 centered\non the boundary between the two halves. We merge all the requests inside S into a\n(possibly reduced) list L, and still keep them sorted by y coordinates. For a request\nr ∈ L, check the distance between r and 7 requests in L following r (which have\nlarger y coordinates). If any pair has distance within 1, return the pair.\n[Correctness]\nAfter the divide and conquer phase, if the two subproblems do not find any violation,\nthe only place violations can still happen is in the stripe S, which we check in the\nmerging phase. In the merging phase, it is sufficient to check only 7 requests following\nr for the following reason.\n\nProblem Set 1 Solutions\nIf we divide stripe S into squares of size 1/2. Each square contains at most one re\nquest, otherwise a pair within the same square should have been detected by the sub\nproblems. For a request r ∈ L, a following request within distance 1 must be inside a\n2-by-1 square spanning across the boundary. There are only 8 such squares. Thus, ex\ncluding r itself, only 7 following requests need to be checked. (Requests with smaller\ny coordinates have already been checked against r.)\n[Timing Analysis]\nDividing the requests takes O(n) time. The merging phase goes through each request\nin the stripe, and requires constant time for each request. So the merging phase takes\nO(n) time. We have the following recursion for the time complexity.\nT (n) ≤ 2T (n/2) + cn\nwhich solves to O(n lg n) according to master theorem.\n[Other Solutions]\nThere are other solutions that do not use divide and conquer. Most closely related,\none can directly divide Lx into maximal clusters where the points in each cluster have\nan x extent of at most 1. Then one can compare points between adjacent clusters\nusing a similar algorithm to the above. Less related, if one allows computing the floor\nof a coordinate, we can assign points to their 2\n1 × 1\n2 squares, and then use a hash\ntable to check for conflicts within the square or with the eight adjacent squares for\nrequests. But a hash table needs O(n) worse-case time for insertion, though O(1)\nexpected time. This makes the worst-case overall runtime O(n2). One can instead use\na balanced BST, to reduce the worst-case runtime to O(n lg n).\n(c) [8 points]\nDescribe how to modify your solution for Part (b) to determine whether there are three\nrequests, all within distance 1 of each other. For full credit, your algorithm should run\nin O(n lg n) time, where n is the number of requests.\nSolution:\nThe solution is similar to Part (b). For the base case, compute the distance for each\n3-tuple in each subset if the number of requests within a subset is less than or equal to\nC (e.g., C = 3).\nIf none is found, then the remaining possibility is a triangle with two points on one\nside of the line and one point on the other. Proceed as in Part (b), find the stripe S,\ndivide it into squares.\nNow for each request in the reduced list, check its distance from the same constant\nnumber of following requests as above, but now looking for two such requests. For\neach pair, also check the distance between the two requests in the pair.\nThe extra work is still O(n), so we get the same O(n lg n) bound.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Class on Design and Analysis of Algorithms, Problem Set 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/4a1b44e7e3f34aa85adb782d70048210_MIT6_046JS15_pset2.pdf",
      "content": "Design and Analysis of Algorithms\nFebruary 12, 2015\nMassachusetts Institute of Technology\nProfs. Erik Demaine, Srini Devadas, and Nancy Lynch\nProblem Set 2\nProblem Set 2\nThis problem set is due at 11:59pm on Thursday, February 19, 2015.\nThis assignment, like later assignments, consists of exercises and problems. Hand in solutions\nto the problems only. However, we strongly advise that you work out the exercises also, since they\nwill help you learn the course material. You are responsible for the material they cover.\nEach submitted solution should start with\nyour name, the course number, the problem number, your recitation section, the date, and the\nnames of any students with whom you collaborated.\nWe will often ask you to \"give an algorithm\" to solve a problem. Your write-up should take the\nform of a short essay. Start by defining the problem you are solving and stating what your results\nare. Then provide:\n1. A description of the algorithm in English and, if helpful, pseudo-code.\n2. A proof (or proof sketch) for the correctness of the algorithm.\n3. An analysis of the running time.\nWe will give full credit only for correct solutions that are described clearly.\nExercise 2-1. Read CLRS, Sections 30.1 and 30.2.\nExercise 2-2. Exercise 30-2.3.\nExercise 2-3. Exercise 30-2.4.\nExercise 2-4. Read CLRS, Chapter 18.\nExercise 2-5. Exercise 18.2-5\nExercise 2-6. Exercise 18.3-2\nProblem 2-1. Pattern Matching [25 points]\nSuppose you are given a source string S[0 . . n - 1] of length n, consisting of symbols a and b.\nSuppose further that you are given a pattern string P [0 . . m - 1] of length m « n, consisting of\nsymbols a, b, and ∗, representing a pattern to be found in string S. The symbol ∗ is a \"wild card\"\nsymbol, which matches a single symbol, either a or b. The other symbols must match exactly.\nThe problem is to output a sorted list M of valid \"match positions\", which are positions j in S\nsuch that pattern P matches the substring S[j . . j + |P | - 1]. For example, if S = a b a b b a b and\nP = a b ∗, then the output M should be [0, 2].\n6.046J/18.410J\nPlease turn in each problem solution separately.\n\nProblem Set 2\n(a) [4 points] Describe a straightforward, na ıve algorithm to solve the problem. Your\nalgorithm should run in time O(nm).\n(b) [12 points] Give an algorithm to solve the problem by reducing it to the problem\nof polynomial multiplication. Specifically, describe how to convert strings S and P\ninto polynomials such that the product of the polynomials allows you to determine the\nanswer M. Give examples to illustrate your polynomial representation of the inputs\nand your way of determining outputs from the product, based on the example S and\nP strings given above.\n(c) [3 points] Suppose you combine your solution to Part (b) with an FFT algorithm for\npolynomial multiplication, as presented in Lecture 3. What is the time complexity of\nthe resulting solution to the string matching problem?\n(d) [6 points] Now consider the same problem but with a larger symbol alphabet. Specif\nically, suppose you are given a representation of a DNA strand as a string D[0 . . n-1]\nof length n, consisting of symbols A, C, G, and T ; and you are given a pattern string\nP [0 . . m - 1] of length m « n, consisting of symbols A, C, G, T , and ∗.\nThe problem is, again, to output a sorted list M of valid \"match positions\", which are\npositions j in D such that pattern P matches the substring D[j . . j + |P | - 1]. For\nexample, if D = A C G A C C A T and P = A C ∗ A, then the output M should be\n[0, 3].\nBased on your solutions to Parts (b) and (c), give an efficient algorithm for this setting.\nIllustrate your algorithm on the example above.\nProblem 2-2. Combining B-trees [25 points]\nConsider a new B-tree operation COMBINE(T1, T2, k). This operation takes as input two B-trees T1\nand T2 with the same minimum degree parameter t, plus a new key k that does not appear in either\nT1 or T2. We assume that all the keys in T1 are strictly smaller than k and all the keys in T2 are\nstrictly larger than k. The COMBINE operation produces a new B-tree T , with the same minimum\ndegree t, whose keys are those in T1, those in T2, plus k. In the process, it destroys the original\ntrees T1 and T2.\nIn this problem, you will design an algorithm to implement the COMBINE operation. Your algo\nrithm should run in time O(|h1 - h2| + 1), where h1 and h2 are the heights of trees T1 and T2\nrespectively. In analyzing the costs, you should regard t as a constant.\n(a) [5 points] First consider the special case of the problem in which h1 is assumed to be\nequal to h2. Give an algorithm to combine the trees that runs in constant time.\n(b) [5 points] Consider another special case, in which h1 is assumed to be exactly equal\nto h2 + 1. Give a constant-time algorithm to combine the trees.\n(c) [5 points] Now consider the more general case in which h1 and h2 are arbitrary.\nBecause the algorithm must work in such a small amount of time, and must work\nfor arbitrary heights, a first step is to develop a new kind of augmented B-tree data\n\nProblem Set 2\nstructure in which each node x always carries information about the height of the\nsubtree below x. Describe how to augment the common B-tree insertion and deletion\noperations to maintain this information, while still maintaining the asymptotic time\ncomplexity of all operations.\n(d) [10 points] Now give an algorithm for combining two B-trees T1 and T2, in the general\ncase where h1 and h2 are arbitrary. Your algorithm should run in time O(|h1 -h2|+1).\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Class on Design and Analysis of Algorithms, Solutions to Problem Set 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/d608c09d3f727411722e3ff5a95b103f_MIT6_046JS15_pset2sols.pdf",
      "content": "Design and Analysis of Algorithms\nFebruary 19, 2015\nMassachusetts Institute of Technology\nProfs. Erik Demaine, Srini Devadas, and Nancy Lynch\nProblem Set 2 Solutions\nProblem Set 2 Solutions\nThis problem set is due at 11:59pm on Thursday, February 19, 2015.\nExercise 2-1. Read CLRS, Sections 30.1 and 30.2.\nExercise 2-2. Exercise 30-2.3.\nExercise 2-3. Exercise 30-2.4.\nExercise 2-4. Read CLRS, Chapter 18.\nExercise 2-5. Exercise 18.2-5\nExercise 2-6. Exercise 18.3-2\nProblem 2-1. Pattern Matching [25 points]\nSuppose you are given a source string S[0 . . n - 1] of length n, consisting of symbols a and b.\nSuppose further that you are given a pattern string P [0 . . m - 1] of length m « n, consisting of\nsymbols a, b, and ∗, representing a pattern to be found in string S. The symbol ∗ is a \"wild card\"\nsymbol, which matches a single symbol, either a or b. The other symbols must match exactly.\nThe problem is to output a sorted list M of valid \"match positions\", which are positions j in S\nsuch that pattern P matches the substring S[j . . j + |P | - 1]. For example, if S = a b a b b a b and\nP = a b ∗, then the output M should be [0, 2].\n(a) [4 points] Describe a straightforward, na ıve algorithm to solve the problem. Your\nalgorithm should run in time O(nm).\nSolution: One can explicitly check every possible starting position s ∈{0, 1, . . . , n-\nm} by checking whether each entry in P matches from s to s + m - 1.\nNAIVE-ALGORITHM(S, P )\n1 M = [ ]\n2 for s = 0 to n - m\nvalid = TRUE\nfor j = 0 to m - 1\nif P [j] = ∗ and P [j] = S[s + j]\nvalid = FALSE\nif valid\nM.APPEND(s)\n9 return M\n6.046J/18.410J\n\nProblem Set 2 Solutions\n(b) [12 points] Give an algorithm to solve the problem by reducing it to the problem\nof polynomial multiplication. Specifically, describe how to convert strings S and P\ninto polynomials such that the product of the polynomials allows you to determine the\nanswer M. Give examples to illustrate your polynomial representation of the inputs\nand your way of determining outputs from the product, based on the example S and\nP strings given above.\nSolution: Let's represent a by 1, b by -1, and ∗ by 0. We will use these representa\ntions instead of the original symbols in this solution.\nNotice that P matches S starting from position j, 0 ≤ j ≤ n - m, if and only if for\nevery i, 0 ≤ i ≤ m - 1, either P [i] = 0 or S[j + i]P [i] = 1. This is true if and only if\nm-1\nm\nS[j + i]P [i] = k,\ni=0\nwhere k is the number of non-∗ symbols in P .\nWe would like to express these summations as coefficients of a product of polynomi\nals. Let x be a variable. Represent S as\nfS (x) = S[0] + S[1]x + · · · + S[n - 1]x n-1 .\nRepresent P as\nfP (x) = Q[0] + Q[1]x + · · · + Q[m - 1]x m-1 ,\nwhere each Q[i] = P [m - 1 - i]. Thus, we have reversed the order of the coefficients\nin this last representation.\nSuppose C is the product of the S polynomial and the P polynomial. Then the coeffi\ncient of xm-1+j in C is\nmm\n-1\nS[m - 1 + j - i]Q[i],\ni=0\nwhich is equal to\nmm\n-1\nS[i + j]P [i].\ni=0\nThis is the same as the sum above. To obtain the output M, we simply examine all\nthe coefficients of C, outputting position number j, 0 ≤ j ≤ n - m, exactly if the\ncoefficient of xm-1+j is equal to k, the total number of non-∗ symbols in P . We output\nthese in order of increasing j, as required.\nIn the example above, S is represented by\nfS (x) = 1 - x + x 2 - x 3 - x 4 + x 5 - x 6\n\nProblem Set 2 Solutions\nand P by\nfP (x) = -x + x 2 .\nThe product C is\n-x + 2x 2 - 2x 3 + 2x 4 - 2x 6 + 2x 7 - x 8 .\nThe number k is equal to 2, so the terms of interest are 2x2 , 2x4, and 2x7. These would\nyield j = 0, 2, 5, but 5 is ruled out because we are only considering j ≤ n - m =\n7 - 3 = 4.\n(c) [3 points] Suppose you combine your solution to Part (b) with an FFT algorithm for\npolynomial multiplication, as presented in Lecture 3. What is the time complexity of\nthe resulting solution to the string matching problem?\nSolution: It's O(n lg n). It takes time O(n lg n) to perform the needed DFT and\ninverse DFT algorithms, and O(n) for producing inputs for the DFT algorithm and\nextracting M from the outputs.\n(d) [6 points] Now consider the same problem but with a larger symbol alphabet. Specif\nically, suppose you are given a representation of a DNA strand as a string D[0 . . n-1]\nof length n, consisting of symbols A, C, G, and T ; and you are given a pattern string\nP [0 . . m - 1] of length m « n, consisting of symbols A, C, G, T , and ∗.\nThe problem is, again, to output a sorted list M of valid \"match positions\", which are\npositions j in D such that pattern P matches the substring D[j . . j + |P | - 1]. For\nexample, if D = A C G A C C A T and P = A C ∗ A, then the output M should be\n[0, 3].\nBased on your solutions to Parts (b) and (c), give an efficient algorithm for this setting.\nIllustrate your algorithm on the example above.\nSolution: Use a reduction. Encode A as a a, C as a b, G as b a, T as b b, and ∗ as ∗∗.\nUse our previous solution to solve the problem on the resulting string, obtaining a list\nM ' of positions.\nThe final output list M will consist of just the even numbers from the list M ', all\ndivided by 2.\nThis will take the time it takes to convert and then solve the original problem on arrays\nof length 2n and 2m respectively:\nO(2n + 2n lg(2n)) = O(n lg n).\n\nProblem Set 2 Solutions\nProblem 2-2. Combining B-trees [25 points]\nConsider a new B-tree operation COMBINE(T1, T2, k). This operation takes as input two B-trees T1\nand T2 with the same minimum degree parameter t, plus a new key k that does not appear in either\nT1 or T2. We assume that all the keys in T1 are strictly smaller than k and all the keys in T2 are\nstrictly larger than k. The COMBINE operation produces a new B-tree T , with the same minimum\ndegree t, whose keys are those in T1, those in T2, plus k. In the process, it destroys the original\ntrees T1 and T2.\nIn this problem, you will design an algorithm to implement the COMBINE operation. Your algo\nrithm should run in time O(|h1 - h2| + 1), where h1 and h2 are the heights of trees T1 and T2\nrespectively. In analyzing the costs, you should regard t as a constant.\n(a) [5 points] First consider the special case of the problem in which h1 is assumed to be\nequal to h2. Give an algorithm to combine the trees that runs in constant time.\nSolution: Construct a new root node for T consisting of the root nodes for T1 and T2,\nwith the root of T1 at the left, and with k inserted between the keys of the two original\nroots. Then, if the number of keys in the resulting root node is at least 2t - 1 then split\nthe root node around its median, forming a new root node containing one key and two\nchild nodes containing at least t - 1 keys apiece.\n\nProblem Set 2 Solutions\nCOMBINE(T1, T2, k)\n1 T = T1\n2 R = T.root\n3 R2 = T2.root\n4 i = R.n + 1\n[ Merging the two roots into one\n6 T.ki = k\n7 for j = 1 to R2.n\ni = i + 1\nT.ki = R2.kj\nT.ci = R2.cj\n11 T.ci+1 = R2.cR2 .n+1\n12 R.n = i + 1\n13 if R.n ≥ 2t - 1\n[ Splitting node\n\nR.n\nmid =\ndummy = ALLOCATE-NODE()\ndummy.n = 1\ndummy.k1 = R.kmid\n[ Allocating Children\nC1 = ALLOCATE-NODE()\nC1.n = mid - 1\nC2 = ALLOCATE-NODE()\nC2.n = R.n - mid\n[ Creating first child\nC1.c1 = T.c1\nfor j = 1 to C1.n\nC1.kj = T.kj\nC1.cj+1 = T.cj+1\n[ Creating second child\nC2.c1 = T.cmid+1\nfor j = 1 to C2.n\nC1.kj = T.kj+mid\nC1.cj+1 = T.cj+mid+1\ndummy.c1 = C1\ndummy.c2 = C2\nT.root = dummy\n37 return T\n(b) [5 points] Consider another special case, in which h1 is assumed to be exactly equal\nto h2 + 1. Give a constant-time algorithm to combine the trees.\n\nProblem Set 2 Solutions\nSolution:\nAppend k to the right end of the right child node of T1 and append the root of T2 to\nthat. Clearly this preserves sorted order. Now the right child may have anywhere from\nt + 1 to 4t - 1 keys. If it has 2t - 1 or more keys, then split it around its median key.\nIf that causes the root node to have 2t - 1 nodes then split that around its median key,\nthus adding another level to the tree.\nCOMBINE(T1, T2, k)\n1 x = T1.root\n2 x = x.cx.n+1\n3 r = T2.root\n4 n = x.n + 1 + r.n\n5 [ Append k to the rightmost child\n6 x.kx .n+1 = k\n7 [ Append the root of T2 to the node\n8 x.cx .n+2 = r.c1\n9 for j = 1 to r.n\nx.kx .n+j +2 = r.kj\nx.cx .n+j +3 = r.cj+1\n12 [ Split node if too big\n13 if n ≥ 2t - 1\np = x.parent\nn = p.n\nB-TREE-SPLIT-CHILD(p, n)\n(c) [5 points] Now consider the more general case in which h1 and h2 are arbitrary.\nBecause the algorithm must work in such a small amount of time, and must work\nfor arbitrary heights, a first step is to develop a new kind of augmented B-tree data\nstructure in which each node x always carries information about the height of the\nsubtree below x. Describe how to augment the common B-tree insertion and deletion\noperations to maintain this information, while still maintaining the asymptotic time\ncomplexity of all operations.\nSolution: Augment the tree by adding a height attribute for each node. The height of\na leaf node is 0. For internal nodes, HEIGHT(x) = HEIGHT(x.c1) + 1.\nInsertion: New nodes are added during splitting. The newly allocated node in a split\nhas the same height as the original node. The only other time a node is added is when\nthe root is split. This is done by making the root the child of a dummy node and then\nsplitting it. The height of the new root is set to one greater than the height of the old\nroot.\nDeletion: In deletion, no nodes are deleted except the root. Since height values are\nindexed starting at the leaf, deletion does not affect node heights.\n\nProblem Set 2 Solutions\nWith these additions, the asymptotic running time for insertion and deletion is that\nsame as before, O(lg n).\n(d) [10 points] Now give an algorithm for combining two B-trees T1 and T2, in the general\ncase where h1 and h2 are arbitrary. Your algorithm should run in time O(|h1 -h2|+1).\nSolution:\nIf |h1 - h2| < 2, use part (a) or (b). Otherwise, assume that h1 > h2 + 1 (h2 > h1 + 1\nworks symmetrically). Let x be the rightmost node of T1 at level h2. Add k at the\nright end of x and append the root of T2 to that. Now node x may have anywhere from\nt + 1 to 4t - 1 keys. If it has 2t - 1 or more keys, then split it around its median key.\nThe split may propagate upwards, possibly as far as the root. So, the time complexity\ndepends linearly on the height difference O(|h1 - h2|).\nCOMBINE(T1, T2, k)\n1 T = T1\n2 h1 = T1.height\n3 h2 = T2.height\n4 x = T.root\n5 [ Move to the rightmost node at level h2\n6 for j = 1 to h1 - h2\nn = x.n\nx = x.cn\n9 r = T2.root\n10 n = x.n + 1 + r.n\n11 [ Append k to the node\n12 x.kx .n+1 = k\n13 [ Append the root of T2 to the node\n14 x.cx .n+2 = r.c1\n15 for j = 1 to r.n\nx.kx .n+j +2 = r.kj\nx.cx .n+j +3 = r.cj+1\n18 [ Split node if too big\n19 if n ≥ 2t - 1\np = x.parent\nn = p.n\nB-TREE-SPLIT-CHILD(p, n)\n23 return T\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Class on Design and Analysis of Algorithms, Problem Set 3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/c6f512681e64389b446a981c7d963323_MIT6_046JS15_pset3.pdf",
      "content": "Design and Analysis of Algorithms\nFebruary 19, 2015\nMassachusetts Institute of Technology\n\nProfs. Erik Demaine, Srini Devadas, and Nancy Lynch\nProblem Set 3\nProblem Set 3\nThis problem set is due at 11:59pm on Thursday, February 26, 2015.\nEach submitted solution should start with\nyour name, the course number, the problem number, your recitation section, the date, and the\nnames of any students with whom you collaborated.\nExercise 3-1. Read CLRS, Section 20.3.\nExercise 3-2. Exercise 20-3.1.\nExercise 3-3. Exercise 20-3.2.\nProblem 3-1. Variants on van Emde Boas [25 points]\nFor each of the following variants on the van Emde Boas data structures (presented in Lecture 4\nand CLRS, Section 20.3), carefully describe what changes are needed to the pseudocode (from\neither lecture or the textbook), and analyze the costs of the vEB operations INSERT, DELETE, and\nSUCCESSOR, comparing them with the costs of the same operations for the original vEB structure.\n(a) [7 points] Instead of dividing the structure into u1/2 groups of u1/2 numbers each, use\nu1/3 groups of u2/3 numbers each.\n(b) [18 points] In addition to excluding the minimum element from lower-level vEB struc\ntures, also exclude the maximum element from lower-level vEB structures (and store\nit in the already existing max attribute). (Use the original division into u1/2 groups of\nu1/2 numbers here.)\n6.046J/18.410J\nPlease turn in each problem solution separately.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Class on Design and Analysis of Algorithms, Solutions to Problem Set 3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/3e83fa6454e5ebe6bf1bcb1b4a122f6d_MIT6_046JS15_pset3sols.pdf",
      "content": "Design and Analysis of Algorithms\nFebruary 26, 2015\nMassachusetts Institute of Technology\nProfs. Erik Demaine, Srini Devadas, and Nancy Lynch\nProblem Set 3 Solutions\nProblem Set 3 Solutions\nThis problem set is due at 11:59pm on Thursday, February 26, 2015.\nExercise 3-1. Read CLRS, Section 20.3.\nExercise 3-2. Exercise 20-3.1.\nExercise 3-3. Exercise 20-3.2.\nProblem 3-1. Variants on van Emde Boas [25 points]\nFor each of the following variants on the van Emde Boas data structures (presented in Lecture 4\nand CLRS, Section 20.3), carefully describe what changes are needed to the pseudocode (from\neither lecture or the textbook), and analyze the costs of the vEB operations INSERT, DELETE, and\nSUCCESSOR, comparing them with the costs of the same operations for the original vEB structure.\n(a) [7 points] Instead of dividing the structure into u1/2 groups of u1/2 numbers each, use\nu1/3 groups of u2/3 numbers each.\nSolution:\nThe pseudocode doesn't change, except for the different division into\nclusters. The operations MIN and MAX take constant time.\nFor MEMBER, we have the recurrence\nT (u) = T (u 2/3) + c.\nPreviously, we had the recurrence T (u) = T (u1/2) + c, which solved (see CLRS) to\nT (u) = O(c lg lg u) = O(c log2 log2 u).\nFor the new recurrence, following the same solution method, we get\nO(c log3/2 log2 u),\nwhich is the same order of magnitude, with a slightly larger constant.\nFor SUCCESSOR, PREDECESSOR, INSERT, and DELETE, we get the recurrence\nT (u) = max{T (u 1/3), T (u 2/3)} + c = T (u 2/3) + c.\nSo we have the same analysis as for MEMBER, yielding\nO(c log3/2 log2 u) = O(lg lg u).\n6.046J/18.410J\n\nProblem Set 3 Solutions\n(b) [18 points] In addition to excluding the minimum element from lower-level vEB struc\ntures, also exclude the maximum element from lower-level vEB structures (and store\nit in the already existing max attribute). (Use the original division into u1/2 groups of\nu1/2 numbers here.)\nSolution: We rewrite portions of the code in CLRS Section 20.3 that involve min\nand max . The order-of-magnitude of the complexity does not change.\nThe initialization of the empty data structure is as before. For MIN and MAX queries,\nthe code is unchanged. Because we are treating MIN and MAX symmetrically, SUC\nCESSOR and PREDECESSOR are now symmetric with each other.\nSUCCESSOR: Start with the code on p. 551. Lines 1-11 and 14-15 are unchanged.\nLines 12-13 must be modified, however, to take into account the case where the suc\ncessor may reside in no cluster at all; this is similar to lines 13-14 of the old predeces\nsor code on p. 552. Thus, in place of the current lines 12-13, we write:\nVEB-TREE-SUCCESSOR(V, x)\n10 [ This replaces lines 12-13 in the original code.\n11 if succ-cluster = NIL\nif V.max = NIL and x < V.max\nreturn V.max\nelse return NIL\nPREDECESSOR: Now predecessor is symmetric with successor. In fact, the predeces\nsor code stays the same as the code on page 552.\nINSERT: The modified code is as follows:\nONE-ELEMENT-TREE-INSERT(V, x)\n1 [ This should be called when V.min = V.max\n2 if x > V.min\nV.max = x\n4 else V.min = x\n\nProblem Set 3 Solutions\nVEB-TREE-INSERT(V, x)\n1 if V.min == NIL\nVEB-EMPTY-TREE-INSERT(V,x)\n3 elseif V.min == V.max\nONE-ELEMENT-TREE-INSERT(V,x)\n5 else\nif x < V.min\nexchange x with V.min\nelseif x > V.max\nexchange x with V.max\nif VEB-TREE-MINIMUM(V.cluster[high(x)]) == NIL\nVEB-TREE-INSERT(V.summary,high(x))\nVEB-EMPTY-TREE-INSERT(V.cluster[high(x)],low(x))\nelse\nVEB-TREE-INSERT(V.cluster[high(x)],low(x))\nDELETE: The modified code is as follows.\nVEB-TREE-DELETE(V, x)\n1 if V.min == V.max\nV.min = NIL\nV.max = NIL\n4 elseif VEB-TREE-MINIMUM(V.summary) == NIL\nif x = V.min\nV.min = V.max\nelseif x = V.max\nV.max = V.min\n9 else\nif x == V.min\nfirst-cluster = VEB-TREE-MINIMUM(V.summary)\nx = index(first-cluster,\nVEB-TREE-MINIMUM(V.cluster[first-cluster]))\nV.min = x\nelseif x == V.max\nlast-cluster = VEB-TREE-MAXIMUM(V.summary)\nx = index(last-cluster,\nVEB-TREE-MAXIMUM(V.cluster[last-cluster]))\nV.max = x\nVEB-TREE-DELETE(V.cluster[high(x)], low(x))\nif VEB-TREE-MINIMUM(V.cluster[high(x)], low(x)) == NIL\nVEB-TREE-DELETE(V.summary, high(x))\nWe explain the edits to the code in CLRS p. 554. Lines 1-3 stay the same since they\n\nProblem Set 3 Solutions\nare simply testing the 1-element special case for V . Now we add a new 2-element\nspecial case after line 3. Note that summary is empty because the structure contains\njust the max and min and neither appears in the clusters.\nVEB-TREE-DELETE(V, x)\n[ Lines 1-3 are as in the book\n1 if V.min == V.max\nV.min = NIL\nV.max = NIL\n4 elseif VEB-TREE-MINIMUM(V.summary) == NIL\n[ This deals with the case where the summary is empty\nif x = V.min\nV.min = V.max\nelseif x = V.max\nV.max = V.min\nAt this point in the execution, we know that the structure contains at least 3 elements.\nThus, we don't need the base case, lines 4-8, since each base structure can contain at\nmost two elements. Starting from line 9, a lot changes, since we are treating min and\nmax symmetrically. So we can write:\nVEB-TREE-DELETE(V, x)\n8 [ Lines 4-8 from the original precede this.\n9 else\nif x == V.min\n[ The logic from lines 10-12 in the original goes here\nelseif x == V.max\n[ The logic from lines 10-12 in the original goes here\n[ but with VEB-TREE-MAXIMUM(V.summary), last-cluster,\n[ VEB-TREE-MAXIMUM(V.cluster[last-cluster]),\n[ and setting V.max = x in the final line\nFor this code, note that the clusters of V cannot all be empty because V contains at\nleast 3 elements. So the operations above on V.summary actually return values.\nThe net effect of these lines is to reset x so it now refers to an element to be deleted\nfrom some cluster within V . The new x may be placed in min or max , if appropriate.\nAfter this code, keep line 13 from the original Delete code, which deletes the element\nfrom its cluster, as well as lines 14-15, which delete the cluster from the summary if\nnecessary. We omit lines 16-23, because they address the case where we are deleting\nthe maximum element of V , which we have already handled.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Class on Design and Analysis of Algorithms, Problem Set 4",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/5eb7c1134350c89889dcaf7ec3fd609a_MIT6_046JS15_pset4.pdf",
      "content": "Design and Analysis of Algorithms\nFebruary 26, 2015\nMassachusetts Institute of Technology\n\nProfs. Erik Demaine, Srini Devadas, and Nancy Lynch\nProblem Set 4\nProblem Set 4\nThis problem set is due at 11:59pm on Thursday, March 5, 2015.\nEach submitted solution should start with\nyour name, the course number, the problem number, your recitation section, the date, and the\nnames of any students with whom you collaborated.\nExercise 4-1. Read CLRS, Chapter 17.\nExercise 4-2. Exercise 17.1-3.\nExercise 4-3. Exercise 17.2-2.\nExercise 4-4. Exercise 17.3-2.\nExercise 4-5. Read CLRS, Chapter 7.\nExercise 4-6. Exercise 7.1-3.\nExercise 4-7. Exercise 7.2-5.\nExercise 4-8. Exercise 7.4-4.\nProblem 4-1. Extreme FIFO Queues [25 points]\nDesign a data structure that maintains a FIFO queue of integers, supporting operations ENQUEUE,\nDEUEUE, and FIND-MIN, each in O(1) amortized time. In other words, any sequence of m op\nerations should take time O(m). You may assume that, in any execution, all the items that get\nenqueued are distinct.\n(a) [5 points] Describe your data structure. Include clear invariants describing its key\nproperties. Hint: Use an actual queue plus auxiliary data structure(s) for bookkeeping.\n(b) [5 points] Describe carefully, in words or pseudo-code, your ENQUEUE, DEQUEUE\nand FIND-MIN procedures.\n(c) [5 points] Prove that your operations give the right answers. Hint: You may want to\nprove that their correctness follows from your data structure invariants. In that case\nyou should also sketch arguments for why the invariants hold.\n(d) [10 points] Analyze the time complexity: the worst-case cost for each operation, and\nthe amortized cost of any sequence of m operations.\n6.046J/18.410J\nPlease turn in each problem solution separately.\n\nProblem Set 4\nProblem 4-2. Quicksort Analysis [25 points]\nIn this problem, we will analyze the time complexity of QUICKSORT in terms of error probabilities,\nrather than in terms of expectation. Suppose the array to be sorted is A[1 . . n], and write xi for the\nelement that starts in array location A[i] (before QUICKSORT is called). Assume that all the xi\nvalues are distinct.\nIn solving this problem, it will be useful to recall a claim from lecture. Here it is, slightly restated:\nClaim: Let c > 1 be a real constant, and let α be a positive integer. Then, with probability at least\n1 - n\nα , 3(α + c) lg n tosses of a fair coin produce at least c lg n heads.\nNote: High probability bounds, and this Claim, will be covered in Tuesday's lecture.\n(a) [5 points] Consider a particular element xi. Consider a recursive call of QUICKSORT\non subarray A[p . . p+m-1] of size m ≥ 2 which includes element xi. Prove that, with\nprobability at least 1\n2 , either this call to QUICKSORT chooses xi as the pivot element,\nor the next recursive call to QUICKSORT containing xi involves a subarray of size at\nmost 3\n4 m.\n(b) [9 points] Consider a particular element xi. Prove that, with probability at least 1- n\n2 ,\nthe total number of times the algorithm compares xi with pivots is at most d lg n, for\na particular constant d. Give a value for d explicitly.\n(c) [6 points] Now consider all of the elements x1, x2, . . . , xn. Apply your result from\npart (b) to prove that, with probability at least 1 - n\n1 , the total number of comparisons\nmade by QUICKSORT on the given array input is at most d'n lg n, for a particular\nconstant d'. Give a value for d' explicitly. Hint: The Union Bound may be useful for\nyour analysis.\n(d) [5 points] Generalize your results above to obtain a bound on the number of compar\nisons made by QUICKSORT that holds with probability 1 - n\nα , for any positive integer\nα, rather than just probability 1 - n\n1 (i.e., α = 1).\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Class on Design and Analysis of Algorithms, Solutions to Problem Set 4",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/8cc8292f1b04e9adc43546295b2ecae3_MIT6_046JS15_pset4sols.pdf",
      "content": "Design and Analysis of Algorithms\nMarch 5, 2015\nMassachusetts Institute of Technology\nProfs. Erik Demaine, Srini Devadas, and Nancy Lynch\nProblem Set 4 Solutions\nProblem Set 4 Solutions\nThis problem set is due at 11:59pm on Thursday, March 5, 2015.\nExercise 4-1. Read CLRS, Chapter 17.\nExercise 4-2. Exercise 17.1-3.\nExercise 4-3. Exercise 17.2-2.\nExercise 4-4. Exercise 17.3-2.\nExercise 4-5. Read CLRS, Chapter 7.\nExercise 4-6. Exercise 7.1-3.\nExercise 4-7. Exercise 7.2-5.\nExercise 4-8. Exercise 7.4-4.\nProblem 4-1. Extreme FIFO Queues [25 points]\nDesign a data structure that maintains a FIFO queue of integers, supporting operations ENQUEUE,\nDEUEUE, and FIND-MIN, each in O(1) amortized time. In other words, any sequence of m op\nerations should take time O(m). You may assume that, in any execution, all the items that get\nenqueued are distinct.\n(a) [5 points] Describe your data structure. Include clear invariants describing its key\nproperties. Hint: Use an actual queue plus auxiliary data structure(s) for bookkeeping.\nSolution: For example, we might use a FIFO queue Main and an auxiliary linked\nlist, Min, satisfying the following invariants:\n1. Item x appears in Min if and only if x is the minimum element of some tail-\nsegment of Main.\n2. Min is sorted in increasing order, front to back.\n(b) [5 points] Describe carefully, in words or pseudo-code, your ENQUEUE, DEQUEUE\nand FIND-MIN procedures.\nSolution:\n6.046J/18.410J\n\nProblem Set 4 Solutions\nENQUEUE(x)\n1 Add x to the end of Main.\n2 Starting at the end of the list, examine elements of Min and remove those that are larger than x; stop ex\n3 Add x to the end of Min.\nDEQUEUE()\n1 Remove and return the first element x of Main.\n2 If x is the first element in Min, remove it.\nFIND-MIN()\n1 Return the first element of Min.\n(c) [5 points] Prove that your operations give the right answers. Hint: You may want to\nprove that their correctness follows from your data structure invariants. In that case\nyou should also sketch arguments for why the invariants hold.\nSolution:\nThis solution is for the choices of data structure and procedures given\nabove; your own may be different.\nThe only two operations that return answers are DEQUEUE and FIND-MIN. DEQUEUE\nreturns the first element of Main, which is correct because Main maintains the actual\nqueue. FIND-MIN returns the first element of Min. This is the smallest element of\nMin because Min is sorted in increasing order (by Invariant 2 above). The smallest\nelement of Main is the minimum of the tail-segment consisting of all of Main, which\nis the smallest of all the tail-mins of Main. This is the smallest element in Min (by\nInvariant 1). Therefore, FIND-MIN returns the smallest element of Main, as needed.\nProofs for the invariants: The invariants are vacuously true in the initial state. We\nargue that ENQUEUE and DEQUEUE preserve them; FIND-MIN does not affect them.\nIt is easy to see that both operations preserve Invariant 2: Since a DEQUEUE opera\ntion can only remove an element from Min, the order of the remaining elements is\npreserved. For ENQUEUE(x), we remove elements from the end of Min until we find\none that less than x, and then add x to the end of Min. Because Min was in sorted\norder prior to the ENQUEUE(x), when we stop removing elements, we know that all\nthe remaining elements in Min are less than x. Since we do not change the order of\nany elements previously in Min, all the elements are still in sorted order.\nSo it remains to prove Invariant 1. There are two directions:\n- The new Min list contains all the tail-mins.\nENQUEUE(x): x is the minimum element of the singleton tail-segment of Main and\nit is added to Min. Additionally, since every tail-segment now contains the value\nx, all elements with value greater than x can no longer be tail-mins. So, after their\nremoval, Min still contains all the tail-mins.\n\nProblem Set 4 Solutions\nDEQUEUE of element x: The only element that could be removed from Min is x.\nIt is OK to remove x, because it can no longer be a tail-min since it is no longer in\nMain. All other tail-mins are remain in Min.\n- All elements of the new Min are tail-mins.\nENQUEUE(x): x is the only value that is added to Min. It is the min of the singleton\ntail-segment. Every other element y remaining in the Min list was a tail-min before\nthe ENQUEUE and is less than x. So y is still a tail-min after the ENQUEUE.\nDEQUEUE of element x: Then we claim that, if x is in Min before the operation,\nit is the first element of Min and therefore is removed from Min as well. Now, if\nx is in Min, it must be the minimum element of some tail of Main. This tail must\ninclude the entire queue, since x is the first element of Main. So x must be the\nsmallest element in Min, which means it is the first element of Min. Every other\nelement y in Min was a tail-min before the DEQUEUE, and is still a tail-min after\nthe DEQUEUE.\n(d) [10 points] Analyze the time complexity: the worst-case cost for each operation, and\nthe amortized cost of any sequence of m operations.\nSolution: DEQUEUE and FIND-MIN are O(1) operations, in the worst case.\nENQUEUE is O(m) in the worst case. To see that the cost can be this large, suppose\nthat ENQUEUE operations are performed for the elements 2, 3, 4, . . . , m - 1, m, in or\nder. After these, Min contains {2, 3, 4, . . . , m - 1, m}. Then perform ENQUEUE(1).\nThis takes Ω(m) time because all the other entries from Min are removed one by one.\nHowever, the amortized cost of any sequence of m operations is O(m). To see this,\nwe use a potential argument. First, define the actual costs of the operations as follows:\nThe cost of any FIND-MIN operation is 1. The cost of any DEQUEUE operation is 2,\nfor removal from Main and possible removal from Min. The cost of an ENQUEUE\noperation is 2 + s, where s is the number of elements removed from Min. Define the\npotential function Φ = |Min|.\nNow consider a sequence o1, o2, . . . , om of operations and let ci denote the actual\ncost of operation oi. Let Φi denote the value of the potential function after exactly\ni operations; let Φ0 denote the initial value of Φ, which here is 0. Define the amortized\ncost cˆi of operation instance oi to be ci + Φi - Φi-1.\nWe claim that cˆi ≤ 2 for every i. If we show this, then we know that the actual cost of\nthe entire sequence of operations satisfies:\nm\nm\nm\nm\nm\nm\nci =\ncˆi + Φ0 - Φm ≤\ncˆi ≤ 2m.\ni=1\ni=1\ni=1\nThis yields the needed O(m) amortized bound.\n\nProblem Set 4 Solutions\nTo show that cˆi ≤ 2 for every i, we consider the three types of operations. If oi is a\nFIND-MIN operation, then\ncˆi = 1 + Φi - Φi-1 = 1 < 2.\nIf oi is a DEQUEUE, then since the lengths of the lists cannot increase, we have:\ncˆi = ci + Φi - Φi-1 ≤ 2 + 0 ≤ 2.\nIf oi is an ENQUEUE, then\ncˆi = ci + Φi - Φi-1 ≤ 2 + s - s = 2,\nwhere s is the number of elements removed from Min. Thus, in every case, cˆi ≤ 2, as\nclaimed.\nAlternatively, we could use the accounting method. Use the same actual costs as\nabove. Assign each ENQUEUE an amortized cost of 3, each DEQUEUE an amortized\ncost of 2, and each FIND-MIN an amortized cost of 1. Then we must argue that\nm\nm\nm\nm\ncˆi ≥\nci\ni=1\ni=1\nfor any sequence of operations and costs as above. This is so because each ENQUEUE(x)\ncontributes an amortized cost of 3, which covers its own actual cost of 2 plus the pos\nsible cost of removing x from Min later.\nProblem 4-2. Quicksort Analysis [25 points]\nIn this problem, we will analyze the time complexity of QUICKSORT in terms of error probabilities,\nrather than in terms of expectation. Suppose the array to be sorted is A[1 . . n], and write xi for the\nelement that starts in array location A[i] (before QUICKSORT is called). Assume that all the xi\nvalues are distinct.\nIn solving this problem, it will be useful to recall a claim from lecture. Here it is, slightly restated:\nClaim: Let c > 1 be a real constant, and let α be a positive integer. Then, with probability at least\n1 - n\nα , 3(α + c) lg n tosses of a fair coin produce at least c lg n heads.\nNote: High probability bounds, and this Claim, will be covered in Tuesday's lecture.\n(a) [5 points] Consider a particular element xi. Consider a recursive call of QUICKSORT\non subarray A[p . . p+m-1] of size m ≥ 2 which includes element xi. Prove that, with\nprobability at least 1\n2 , either this call to QUICKSORT chooses xi as the pivot element,\nor the next recursive call to QUICKSORT containing xi involves a subarray of size at\nmost 3\n4 m.\n\nProblem Set 4 Solutions\nSolution: Suppose the pivot value is x. If l m J + 1 ≤ x ≤ m -lm J, then both\nsubarrays produced by the partition have size at most 3\nm . Moreover, the number of\nvalues of x in this range is at least m , so the probability of choosing such a value is at\nleast 1\n2 . Then either xi is the pivot value or it is in one of the two segments.\n(b) [9 points] Consider a particular element xi. Prove that, with probability at least 1- n\n2 ,\nthe total number of times the algorithm compares xi with pivots is at most d lg n, for\na particular constant d. Give a value for d explicitly.\nSolution: We use part (a) and the Claim. By part (a), each time QUICKSORT is called\nfor a subarray containing xi, with probability at least 1\n2 , either xi is chosen as the pivot\nvalue or else the size of the subarray containing xi reduces to at most 3\n4 of what it\nwas before the call. Let's say that a call is \"successful\" if either of these two cases\nhappens. That is, with probability at least 1\n2 , the call is successful.\nNow, at most log4/3 n successful calls can occur for subarrays containing xi during an\nexecution, because after that many successful calls, the size of the subarray containing\nxi would be reduced to 1. Using the change of base formula for logarithms,\nlog4/3 n = c lg n, where c = log4/3 2.\nNow we can model the sequence of calls to QUICKSORT for subarrays containing xi\nas a sequence of tosses of a fair coin, where heads corresponds to successful calls.\nBy the Claim, with c = log4/3 2 and α = 2, we conclude that, with probability at\nleast 1 - n\n1 , we have at least c lg n successful calls within d lg n total calls, where\nd = 3(2 + c). Each comparison of xi with a pivot occurs as part of one of these calls,\nso with probability at least 1 - n\n1 , the total number of times the algorithm compares\nxi with pivots is at most d lg n = 3(2 + c) lg n = 3(2 + log4/3 2) lg n. The required\nvalue of d is 3(2 + log4/3 2) ≤ 14.\n(c) [6 points] Now consider all of the elements x1, x2, . . . , xn. Apply your result from\npart (b) to prove that, with probability at least 1 - n\n1 , the total number of comparisons\nmade by QUICKSORT on the given array input is at most d'n lg n, for a particular\nconstant d'. Give a value for d' explicitly. Hint: The Union Bound may be useful for\nyour analysis.\nSolution: Using a union bound for all the n elements of the original array A, we get\nthat, with probability at least 1 - n(n\n1 ) = 1 - n\n1 , every value in the array is compared\nwith pivots at most d lg n times, with d as in part (b). Therefore, with probability at\nleast 1 - n\n1 , the total number of such comparisons is at most dn lg n. Using d' = d\nworks fine.\nSince all the comparisons made during execution of QUICKSORT involve comparison\nof some element with a pivot, we get the same probabilistic bound for the total number\nof comparisons.\n\nProblem Set 4 Solutions\n(d) [5 points] Generalize your results above to obtain a bound on the number of compar\nisons made by QUICKSORT that holds with probability 1- n\n1 , for any positive integer\nα\nα, rather than just probability 1 - n\n1 (i.e., α = 1).\nSolution: The modifications are easy. The Claim and part (a) are unchanged. For\npart (b), we now prove that with probability at least 1 - n\n1 , the total number of\nα+1\ntimes the algorithm compares xi with pivots is at most d lg n, for d = 3(α + c). The\nargument is the same as before, but we use the Claim with the value of α instead of 2.\nThen for part (c), we show that with probability at least 1 - n\n1 , the total number\nα\nof times the algorithm compares any value with a pivot is at most dn lg n, where\nd = 3(α + c).\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Class on Design and Analysis of Algorithms, Problem Set 5",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/45952953cb13d077bd35d4397d67fa7c_MIT6_046JS15_pset5.pdf",
      "content": "Design and Analysis of Algorithms\nMarch 13, 2015\nMassachusetts Institute of Technology\nProfs. Erik Demaine, Srini Devadas, and Nancy Lynch\nProblem Set 5\nProblem Set 5\nThis problem set is due at 11:59pm on Friday, March 20, 2015.\nExercise 5-1. Read CLRS, Chapter 11.\nExercise 5-2. Exercise 11.3-5.\nExercise 5-3. Read CLRS, Chapter 14.\nExercise 5-4. Exercise 14.3-3.\nExercise 5-5. Read CLRS, Chapter 15.\nExercise 5-6. Exercise 15.1-4.\nExercise 5-7. Exercise 15.2-4.\nExercise 5-8. Exercise 15.4-5.\nProblem 5-1. New Operations for Skip Lists [25 points]\nThis problem will demonstrate that skip lists, and some augmentations of skip lists, can answer\nsome queries about \"nearby\" elements efficiently. In a dynamic-set data structure, the query\nFINGER-SEARCH(x, k) is given a node x in the data structure and a key k, and it must return\na node y in the data structure that contains key k. (You may assume that such a node y in fact\nappears in the data structure.) The goal is for FINGER-SEARCH to run faster when nodes x and y\nare nearby in the data structure.\n(a) [12 points] Write pseudocode for FINGER-SEARCH(x, k) for skip lists. Assume all\nkeys in the skip list are distinct. Assume that the given node x is in the level-0 list,\nand the operation should return a node y that stores k in the level-0 list.\nYour algorithm should run in O(lg m) steps with high probability, where m = 1 +\n|rank(x.key) - rank(k)|. Here, rank(k) refers to the rank (index) of a key k in the\nsorted order of the dynamic set (when the procedure is invoked). \"High probability\"\nhere is with respect to m; more precisely, for any positive integer α, your algorithm\n6.046J/18.410J\nPlease turn in your solution to all problems in a single pdf file. Your submitted solution should\nstart with your name, the course number, your recitation section, the date, and the names of any\nstudents with whom you collaborated.\n\nProblem Set 5\nshould run in O(lg m) time with probability at least 1 - m\n1 . (The constant implicit in\nα\nthis O may depend on α.) Analyze your algorithm carefully.\nIn writing your code, you may assume that the implementation of skip lists stores both\na -inf at the front of each level as well as a +inf as the last element on each level.\nAnother query on a dynamic set is RANK-SEARCH(x, r): given a node x in the data structure\nand positive integer r, return a node y in the data structure that contains the key whose rank is\nrank(x) + r. You may assume that such a node appears in the data structure. Rank search can be\nimplemented efficiently in skip lists, but this requires augmenting the nodes of the skip list with\nnew information.\n(b) [6 points] Define a way of augmenting skip lists that will support efficient rank search,\nand show that your augmentation does not increase the usual high-probability order-\nof-magnitude time bounds for the SEARCH, INSERT, and DELETE operations.\n(c) [7 points] Now write pseudocode for RANK-SEARCH(x, r) for skip lists. Again as\nsume that all keys in the skip list are distinct. Assume that the given node x is in the\nlevel-0 list, and the operation should return a node y in the level-0 list.\nYour algorithm should run in O(lg m) steps with high probability with respect to m =\nr+1. More precisely, for any positive integer α, your algorithm should run in O(lg m)\ntime with probability at least 1 - m\n1 . Analyze your algorithm carefully.\nα\nProblem 5-2. Choosing Prizes [25 points]\nIn this problem, you are presented with a collection of n prizes from which you are allowed to\nselect at most m prizes, where m < n. Each prize p has a nonnegative integer value, denoted\np.value. Your objective is to maximize the total value of your chosen prizes.\nThe problem has several variations, described in parts (a)-(d) below. In each case, you should\ngive an efficient algorithm to solve the problem, and analyze your algorithm's time and space\nrequirements.\nIn parts (a)-(c), the prizes are presented to you as a sequence P = (p1, p2, . . . , pn), and your\nalgorithm must output a subsequence S of P . In other words, the selected prizes S (|S| = m) must\nbe listed in the same order as they are in P .\n(a) [4 points] Give an algorithm that returns a subsequence S = (s1, s2, . . .) of P of\n\nlength at most m, for which\nj sj .value is maximum. Analyze your algorithm in\nterms of n and m.\n(b) [7 points] Now suppose there are two types of prizes, type A and type B. Each prize's\ntype is given as an attribute p.type. Give an algorithm that returns a subsequence\n\nS = (s1, s2, . . .) of P of length at most m, for which\nj sj .value is maximum, subject\nto the new constraint that, in S, all the prizes of type A must precede all the prizes of\ntype B. Analyze your algorithm in terms of n and m.\n\nProblem Set 5\n(c) [7 points] As in part (a), there is only one type of prize. Give an algorithm that returns\na subsequence S = (s1, s2, . . .) of P of length at most m, for which\nj sj .value is\nmaximum, subject to the new constraint that, in S, the values of the prizes must form\na non-decreasing sequence. Analyze your algorithm in terms of n and m.\nIn part (d), the prizes are represented by a rooted binary tree T , with root vertex r, where each\nvertex u has an associated prize, u.prize. Let P be the set of prizes in the tree. As before, each\nprize p has a nonnegative integer attribute p.value.\n(d) [7 points] Give an algorithm that returns a set S of at most m prizes for which\ns∈S s.value is maximum, subject to the new constraint that, for any s ∈ S that is\nassociated with a non-root node u of T , the prize at node u.parent is also in S. (This\nimplies that the selected prizes must be associated with nodes that form a connected\nsubtree of T rooted at r.)\nP\nP\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Class on Design and Analysis of Algorithms, Solutions to Final Exam",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/c212df0eabebc6b2c271bd4a63480d70_MIT6_046JS15_finalsols.pdf",
      "content": "Design and Analysis of Algorithms\nMay 23, 2015\nMassachusetts Institute of Technology\n6.046J/18.410J\nProfs. Erik Demaine, Srini Devadas, and Nancy Lynch\nFinal Solutions\nFinal Solutions\n- Do not open this exam booklet until you are directed to do so. Read all the instructions first.\n- The exam contains 10 problems, with multiple parts. You have 180 minutes to earn 180\npoints.\n- This exam booklet contains 20 pages, including this one.\n''\n- This exam is closed book. You may use three double-sided letter (81\n× 11'' ) or A4 crib\nsheets. No calculators or programmable devices are permitted. Cell phones must be put\naway.\n- Do not waste time deriving facts that we have studied. Just cite results from class.\n- When we ask you to \"give an algorithm\" in this exam, describe your algorithm in English\nor pseudocode, and provide a short argument for correctness and running time. You do not\nneed to provide a diagram or example unless it helps make your explanation clearer.\n- Do not spend too much time on any one problem. Generally, a problem's point value is an\nindication of how many minutes to spend on it.\n- Show your work, as partial credit will be given. You will be graded not only on the correct\nness of your answer, but also on the clarity with which you express it. Please be neat.\n- Good luck!\nQ Title\nPoints Parts Grade\nQ Title\nPoints Parts Grade\nTrue or False\nBe the Computer\nUberstructure\nStartups are Hard\nMeancorp\nLoad Balancing\nForgetful Forrest\nDistributed Coloring\nPiano Recital\nTotal\nName:\n\n6.046J/18.410J Final Solutions\nName\nProblem 1. True or False. [56 points] (14 parts)\nCircle T or F for each of the following statements to indicate whether the statement is true or false\nand briefly explain why.\n(a) T F [4 points] Suppose algorithm A has two steps, and A succeeds if both the steps\nsucceed. If the two steps succeed with probability p1 and p2 respectively, then A\nsucceeds with probability p1p2.\nSolution: False. Unless the two steps are independent.\n(b) T F [4 points] If the divide-and-conquer convex hull algorithm (from Lecture 2) used\na Θ(n2) strategy to discover the maximum and minimum tangents, the overall\nalgorithm would run in Θ(n2 log n) time.\nSolution: False. The recurrence would be T (n) = 2T (n\n2 ) + Θ(n2) whose solu\ntion is T (n) = Θ(n2).\n(c) T F [4 points] In order to get an expected Θ(n log n) runtime for \"paranoid\" quick\nsort (from Lecture 3), we require the recursive divide step to split the array into\ntwo subarrays each of at least 1\n4 the size of the original array.\nSolution: False. As long as it is a constant fraction of the original array, we can\nget the bound.\n(d) T F [4 points] A binary min-heap with n elements supports INSERT in O(log n)\namortized time and DELETE-MIN in 0 amortized time.\nSolution: True. Same amortization as in class for insert/delete in 2-3 trees.\n\n6.046J/18.410J Final Solutions\nName\n(e) T F [4 points] The hash family H = {h1, h2} is universal, where h1, h2 : {1, 2, 3} →\n{0, 1} are defined by the following table:\n1 2 3\nh1\n0 1 0\nh2\n1 0 1\n(For example, h1(3) = 0.)\nSolution: False. Consider elements 1 and 3: h1 and h2 both cause a collision\nbetween them, so in particular a uniformly random hash function chosen from\nH causes a collision between 1 and 3 with probability 1, greater than the 1/2\nallowed for universal hashing (since there are 2 hash buckets).\n(f) T F [4 points] Recall the O(n3 lg n) matrix-multiplication algorithm to compute\nshortest paths, where we replaced the matrix-multiplication operator pair (∗, +)\nwith (+, min). If we instead replace the operator pair with (+, ∗), then we com\npute the product of the weights of all paths between each pair of vertices.\nSolution: False. If the graph has a cycle, there are infinitely many paths be\ntween some pairs of vertices, so the product ought to be ±inf, yet the matrix-\nmultiplication algorithm will compute finite values if the original matrix has all\nfinite values (e.g., a clique).\n(g) T F [4 points] Negating all the edge weights in a weighted undirected graph G and\nthen finding the minimum spanning tree gives us the maximum-weight spanning\ntree of the original graph G.\nSolution: True.\n(h) T F [4 points] In a graph with unique edge weights, the spanning tree of second-\nlowest weight is unique.\nSolution: False, can construct counter-example.\n\n6.046J/18.410J Final Solutions\nName\n(i) T F [4 points] In the recursion of the Floyd-Warshall algorithm:\nd(k)\nuv = min{d(k-1)\nuv\n, d(k-1)\nuk\n+ d(k-1)\nkv\n},\nd(k)\nuv represents the length of the shortest path from vertex u to vertex v that con\ntains at most k edges.\nSolution: False. d(k)\nuv is the length of the shortest path from vertex u to vertex v\nthat only uses vertex {1, 2, · · · k} as intermediate nodes.\n(j) T F [4 points] Consider a network of processes based on an arbitrary undirected\ngraph G = (V, E) with a distinguished vertex v0 ∈ V . The process at each\nvertex v ∈ V starts with a positive integer xv. The goal is for the process at\nv0 to compute the maximum maxv∈V xv. There is an asynchronous distributed\nalgorithm that solves this problem using O(diam2d) time and O(E + diam · n)\nmessages.\nSolution: True.\nUsing the algorithm from Problem 10-2, we can construct a BFS tree rooted at\nv0 within the given time and message bounds. The root process can broadcast a\nsignal telling all the processes that the tree is completed. Then the processes can\nuse the tree for convergecasting their values, computing the max as the messages\nmove up the tree. The broadcast and convergecast phases do not exceed the\nbounds for the BFS construction.\n(k) T F [4 points] Suppose a file server stores a hash of every file in addition to the\nfile contents. When you download a file from the server, you also download\nthe hash and confirm that it matches the file. This system securely verifies that\nthe downloaded file has not been modified by an adversary, provided the hash\nfunction has collision resistance.\nSolution: False. This scheme is not secure because the adversary can simply\nreplace the file with any file and the hash of that file, and you cannot tell the\ndifference.\n\n6.046J/18.410J Final Solutions\nName\n(l) T F [4 points] Suppose Alice, Bob, and Charlie secretly generate a, b and c, respec\na\nc\ntively, and publish g mod p, gb mod p, and g mod p, where p is a prime. Then,\nAlice, Bob, and Charles can each compute gabc mod p as a shared secret known\nonly to the three of them.\nSolution: False. For example, Alice only knows a, gb and gc, so she can compute\ngab and gac but not gabc .\n(m) T F [4 points] The number of memory transfers used by the best cache-oblivious\nalgorithm is always at least the number of memory transfers used by the best\nexternal-memory algorithm for the same problem.\nSolution: True. Make implicit memory transfers explicit, using LRU.\n(n) T F [4 points] If there is a time-optimal divide-and-conquer algorithm for a problem,\nthen that algorithm is also optimal with respect to memory transfers in the cache-\noblivious model.\nSolution: False. Example: binary search.\n\n6.046J/18.410J Final Solutions\nName\n\nProblem 2. Uberstructure [10 points] (1 part)\nDesign a data structure that maintains a dynamic set S of n elements subject to the following\noperations and time bounds:\nOperation\nEffect\nTime Bound\n1. INSERT(x, S)\n2. DELETE(x, S)\n3. SUCCESSOR(x, S)\n4. FIND-MIN(S)\n5. SEARCH(x, S)\nInsert x into S.\nDelete x from S.\nFind the smallest element in S larger than x.\nReturn the smallest element in S.\nReturn TRUE if element x is in S.\nO(log n)\nO(log n)\nO(log n)\nO(1)\nO(1)\nexpected amortized\nexpected amortized\nworst-case\nworst-case\nexpected\nDescribe how the operations are implemented on your data structure and justify their runtime.\nSolution: Use a balanced binary search tree and a hash table. Augment the root of the balanced\nbinary search tree with the value of the minimum element.\nINSERT: Insert the element in both the balanced binary search tree and hash table. If the element\nis smaller than the current min, update the root's stored min value. Insertion into the tree requires\nO(log n) worst-case time, and insertion into the hash table requires O(1) expected amortization\ntime, for a total of O(log n) expected amortized. (In fact, with high probability, insertion into the\nhash table requires at most O(log n).)\nDELETE: Find the item in the tree and the hash table, and delete it from both. Rebalance the tree\nas necessary and update the root's min value if the minimum element has been deleted. Deletion,\nincluding rebalancing, costs O(log n), and it takes O(log n) to find the minimum element using\nthe binary search tree.\nFIND-MIN: Return the min value stored at the root node. This takes O(1) worst-case time.\nSEARCH: Check whether the element exists in the hash table. This takes O(1) expected time.\n\n6.046J/18.410J Final Solutions\nName\nProblem 3. Meancorp [15 points] (2 parts)\nYou are in charge of the salary database for Meancorp, which stores all employee salaries in a 2-3\ntree ordered by salary. Meancorp compiles regular reports to the Department of Fairness about the\nsalary for low-income employees in the firm. You are asked to implement a new database operation\nAVERAGE(x) which returns the average salary of all employees whose salary is at most x.\n(a) [10 points] What extra information needs to be stored at each node? Describe how to\nanswer an AVERAGE(x) query in O(lg n) time using this extra information.\nSolution: Each node x should store x.size -- the size of the subtree rooted at x --\nand x.sum -- the sum of all the key values in the subtree rooted at x. For a value\nx > 0, let Sx be the set of all keys less than or equal to x. Let Ax and Bx be the sum\nand the size of Sx.\nWe can compute Ax as follows. Let u be the leaf with smallest key larger than x.\nFinding u from the root only takes O(lg n) time by using SEARCH in a 2-3 tree. Now\nconsider the path from the root of the tree to u. Clearly, Ax is the sum of all leaves\nthat are on the left of this path. Therefore, Ax can be computed by summing up all\ny.sum's for every node y that is a left sibling of a node in the path. Since there are\nonly lg n such nodes y's, computing Ax only takes O(lg n) time.\nComputing Bx is similar: instead of summing up y.sum, we sum up y.size. There\nfore, it also takes O(lg n) time to compute Bx.\nTherefore, AVERAGE(x) which is Ax can be answered in O(lg n)) time.\nBx\n(b) [5 points] Describe how to modify INSERT to maintain this information. Briefly\njustify that the worst-case running time for INSERT remains O(lg n).\nSolution: Maintaining x.size is similar to what was covered in recitation and home\nwork. Maintaining x.sum is exactly the same: when a node x gets inserted, we simply\nincrease y.sum for every ancestor y of x by the amount x.key. When a node splits,\nwe recompute the x.sum attribute for the split nodes and its parent. Hence, INSERT\nstill runs in worst-case time O(lg n).\n\n6.046J/18.410J Final Solutions\nName\nProblem 4. Forgetful Forrest [15 points] (3 parts)\nProf. Forrest Gump is very forgetful, so he uses automatic calendar reminders for his appointments.\nFor each reminder he receives for an event, he has a 50% chance of actually remembering the event\n(decided by an independent coin flip).\n(a) [5 points] Suppose we send Forrest k reminders for each of n events. What is the\nexpected number of appointments Forrest will remember? Give your answer in terms\nof k and n.\nSolution:\nThese are all independent events. So linearity of expectation applies.\nEach given event has been remembered with probability 1 - 2-k . So in expectation\nn(1 - 2-k) appointments are remembered.\n(b) [5 points] Suppose we send Forrest k reminders for a single event. How should we\nset k with respect to n so that Forrest will remember the event with high probability,\ni.e., 1 - 1/nα?\nSolution: This problem is equivalent to how many times we must flip a coin to get a\nhead with high probability. The probability of k tails in a row is 1/2k . Thus exactly\nα lg n coin flips suffice.\n(c) [5 points] Suppose we send Forrest k reminders for each of n events. How should\nwe set k with respect to n so that Forrest will remember all the events with high\nprobability, i.e., 1 - 1/nα?\nSolution: We must send at least k = Ω(lg n) reminders, because we needed this\nmany reminders to remember one event with high probability.\nIf we send k = (α + 1) lg n reminders, then each event is remembered with proba\nbility 1 - 1/nα+1 . By a union bound, we know that all events are remembered with\nprobability 1 - 1/nα. So, the number of reminders needed is k = O(lg n).\n\n6.046J/18.410J Final Solutions\nName\nProblem 5. Piano Recital [15 points] (3 parts)\nProf. Chopin has a piano recital coming up, and in preparation, he wants to learn as many pieces\nas possible. There are m possible pieces he could learn. Each piece i takes pi hours to learn.\nProf. Chopin has a total of T hours that he can study by himself (before getting bored). In addition,\nhe has n piano teachers. Each teacher j will spend up to tj hours teaching. The teachers are very\nstrict, so they will teach Prof. Chopin only a single piece, and only if no other teacher is teaching\nhim that piece.\nThus, to learn piece i, Prof. Chopin can either (1) learn it by himself by spending pi of his T self-\nlearning budget; or (2) he can choose a unique teacher j (not chosen for any other piece), learn\ntogether for min{pi, tj } hours, and if any hours remain (pi > tj ), learn the rest using pi - tj hours\nof his T self-learning budget. (Learning part of a piece is useless.)\n(a) [6 points] Assume that Prof. Chopin decides to learn exactly k pieces. Prove that he\nneeds to consider only the k lowest pis and the k highest tj s.\nSolution: Assume there exists a selection of teachers and pieces for learning k pieces.\nLet the set of lowest k pieces be Pk. If there is a piece in our selection that is ∈/ Pk,\nthen we must have a piece in Pk not in the final selection. If we swap the one with\nthe higher cost (∈/ Pk) with the one with lower cost (∈ Pk), the new selection thus\nmade will still be valid, because if the higher time cost was fulfilled in the previous\nselection, the lower time cost in the new selection will still be fulfilled. In this way,\nwe can swap pieces until all of them are ∈ Pk.\nSimilarly, we can swap the teachers for those of higher value until they are the ones\nwith the k highest times.\n\n6.046J/18.410J Final Solutions\nName\n(b) [5 points] Assuming part (a), give an efficient greedy algorithm to determine whether\nProf. Chopin can learn exactly k pieces. Argue its correctness.\nSolution: Let us sort all the teachers and pieces in increasing order beforehand. Call\nthe sorted lists P and T . We see that if a solution exists, there is also one in which P1\nis paired with Tn-k+1, P2 is paired with Tn-k+2 and so on.\nSo for each 1 ≤ i ≤ k, the greedy algorithm checks if Pi ≤ Tn-k+i. If it is, then we\ndon't need to use the shared time for this piece. If it is not, we need to use Tn-k+i - Pi\nof the shared time. We can add up these values. In the end, if the total shared time we\nneed is > T , we return false. Otherwise, we return true.\nThis takes O(k) time, apart from the initial sorting.\n(c) [4 points] Using part (b) as a black box, give an efficient algorithm that finds the\nmaximum number of pieces Prof. Chopin can learn. Analyze its running time.\nSolution: Notice that if kmax is the maximum value of pieces we can learn, we can\nalso learn k pieces for any k ≤ kmax. This suggests that we binary search over the\nvalue of k.\nWe try O(log n) values during the binary search, and checking each value takes O(n)\ntime. This takes O(n log n) time. The sorting also took O(n log n) time, so the algo\nrithm takes O(n log n) time overall.\n\n6.046J/18.410J Final Solutions\nName\nProblem 6. Be the Computer [14 points] (3 parts)\nConsider the following flow network and initial flow f. We will perform one iteration of the\nEdmonds-Karp algorithm.\n10:10$\n13:15$\n0:15$\n10:10$\n14:30$\n4:6$\n1:4$\n2:4$\n7:9$\n7:10$\n8:9$\n1:15$\n0:15$\n2:3$\n7:7$\ns\"\n4\"\nt\"\n5\"\n6\"\n2\"\n3\"\n7\"\n(a) [5 points] Draw the residual graph Gf of G with respect to f.\nSolution:\n10#\n2#\n15#\n10#\n14#\n4#\n1#\n2#\n2#\n7#\n1#\n1#\n15#\n1#\n7#\ns\"\n4\"\nt\"\n5\"\n6\"\n2\"\n3\"\n7\"\n2#\n8#\n2#\n13#\n2#\n3#\n16#\n7#\n14#\n3#\n(b) [4 points] List the vertices in the shortest augmenting path, that is, the augmenting\npath with the fewest possible edges.\nSolution:\ns → 3 → 2 → 5 → t\nor\ns → 3 → 2 → 6 → t\n\n6.046J/18.410J Final Solutions\nName\n(c) [5 points] Perform the augmentation. What is the value of the resulting flow?\nSolution: 26. The augmenting flow has value 1.\n\n6.046J/18.410J Final Solutions\nName\nProblem 7. Startups are Hard [20 points] (3 parts)\nFor your new startup company, Uber for Algorithms, you are trying to assign projects to employees.\nYou have a set P of n projects and a set E of m employees. Each employee e can only work on\none project, and each project p ∈ P has a subset Ep ⊆ E of employees that must be assigned to p\nto complete p. The decision problem we want to solve is whether we can assign the employees to\nprojects such that we can complete (at least) k projects.\n(a) [5 points] Give a straightforward algorithm that checks whether any subset of k\nprojects can be completed to solve the decisional problem. Analyze its time com\nplexity in terms of m, n, and k.\n\nSolution: For each\nn\nk subsets of k projects, check whether any employee is required\nby more than one project. This can be done simply by going each of the k projects p,\nmarking the employees in Ep as needed, and if any employee is marked twice, then\nthis subset fails. Output \"yes\" if any subset of k project can be completed, and \"no\"\notherwise.\n\nThe time complexity is\nn\nk ·m because there are\nn\nk subsets of size k and we pay O(m)\ntime per subset (because all but one employee will be marked only once). Asymptoti\ncally, this is (n/k)km.\n(b) [5 points] Is your algorithm in part (a) fixed-parameter tractable? Briefly explain.\nSolution: No. An FPT algorithms requires a time complexity of nO(1)f(k). By con\ntrast, in our running time, the exponent on n increases with k.\n\n6.046J/18.410J Final Solutions\nName\n(c) [10 points] Show that the problem is NP-hard via a reduction from 3D matching.\nRecall the 3D matching problem: You are given three sets X, Y , Z, each of size m;\na set T ⊆ X × Y × Z of triples; and an integer k. The goal is to determine whether\nthere is a subset S ⊆ T of (at least) k disjoint triples.\nSolution: Each (x, y, z) ∈ T becomes a project that requires employees E(x,y,z) =\n{ex, ey, ez}. Thus n = |T |, E = X ∪ Y ∪ Z, and m = |X| + |Y | + |Z|. We set k to be\nthe same in both problems. The size of the matching is equal to the number of projects\nthat can be completed because both problems model disjointness: if k projects can be\ncompleted, a subset S of size k can be found, and vice versa. The reduction takes\npolynomial time.\n\n6.046J/18.410J Final Solutions\nName\nProblem 8. Load Balancing [15 points] (2 parts)\nSuppose you need to complete n jobs, and the time it takes to complete job i is ti. You are\ngiven m identical machines M1, M2, . . . , Mm to run the jobs on. Each machine can run only one\njob at a time, and each job must be completely run on a single machine. If you assign a set\nm\nJj ⊆{1, 2, . . . , n} of jobs to machine Mj , then it will need Tj =\nti time. Your goal is to\ni∈Jj\npartition the n jobs among the m machines to minimize maxi Ti.\n(a) [5 points] Describe a greedy approximation algorithm for this problem.\nSolution: Let Jj to be the set of jobs that Mj will run, and Tj to be the total time it\nm\nmachine Mj is busy (i.e., Tj =\ni∈Jj ti). Initially, Jj = ∅, and Tj = 0 for all j.\nFor i = 1, . . . , n, assign job i to machine Mj such that Tj = min1≤k≤m(Tk). That is,\nJj = Jj ∪ i and Tj = Tj + ti. Output Jj 's.\nThis runs in O(n lg m) time by keeping a min-heap of the machines based on the\ncurrent total runtime of each machine.\nSolution: Alternate solution: Sort jobs in non-increasing order. Without loss of gen\nerality, let the jobs in order be t1, . . . , tn. Let Jj = {tk:k≡j mod m}. Variations of\nthis algorithm also works, with different sorting orders and assignments. This takes\nO(n lg n) time to sort the jobs.\n\n6.046J/18.410J Final Solutions\nName\n(b) [10 points] Show that your algorithm from part (a) is a 2-approximation algorithm.\nHint: Determine an ideal bound on the optimal solution OPT. Then consider the ma\nchine MT with the longest TT, and the last job i∗ that was added to it.\nm\nSolution: A lower bound to the optimal is L = max( 1\nti, maxi(ti)) since the\nm\n1≤i≤n\nbest you can do is to evenly divide the fractional jobs, and it has to run for at least as\nlong as the longest job.\nNow let MT be the machine that runs for the longest, and let i∗ be the last job that was\nassigned to MT using the greedy algorithm. Let Tj\n∗ be the total run time of all jobs of\nMj immediately before assigning i∗; TT\n∗ = minj Tj\n∗. Then we have\n\n∗\nT ∗\nm · T ≤\n=\nti ≤\nti ≤ m · L,\nT\nj\n1≤j≤m\n1≤i≤i∗\n1≤i≤n\nwhich implies that TT\n∗ ≤ L. Putting it together, we have TT = TT\n∗ + ti∗ ≤ L + ti∗ ≤\n2L ≤ 2OP T . Therefore, this is a 2-approximation algorithm.\nSolution: Proof for alternate solution: Let L be the lower defined above. Consider the\nlongest job tn. Let k ≡ n mod m, and let Sk = Tk - tn. It must be that Sk ≤ Tj for\nall j: for j > k, we only added elements at least as large as every element of Sk. For\nj < k, there are a = im l jobs, and the last a - 1 jobs in Jj are greater the first a - 1\nn\njobs of Jk due to the jobs being sorted, which shows that Sk ≤ Tj . Then\nn\n\nti = tn + Sk +\nTj ≥ tn + mSk.\ni=1\nj=k\n#\nm\nTherefore, mSk ≤\nn\ni=1 ti, which implies Sk ≤ L. Since tn ≤ L also, we get that\ntn + Sk ≤ 2L ≤ 2OP T .\n\n6.046J/18.410J Final Solutions\nName\nProblem 9. Distributed Coloring [20 points] (3 parts)\nConsider an undirected graph G = (V, E) in which every vertex has degree at most Δ. Define a\nnew graph G ' = (V ' , E ' ), the Cartesian product of G with a clique of size Δ + 1. Specifically, V '\nis the set of pairs (v, i) for all vertices v ∈ V and integers i with 0 ≤ i ≤ Δ, and E ' consists of\ntwo types of edges:\n1. For each edge {u, v} ∈ E, there is an edge between (u, i) and (v, i) in E ', for all 0 ≤ i ≤ Δ.\n(Thus, each index i forms a copy of G.)\n2. For each vertex v ∈ V , there is an edge between (v, i) and (v, j) in E ', for all i\nj with\n=\n0 ≤ i, j ≤ Δ. (Thus each v forms a (Δ + 1)-clique.)\nHere is an example of this transformation with Δ = 3:\nFigure 1: Graph G.\nFigure 2: The Cartesian product G ' of G and a clique of size 4.\n(a) [8 points] Let S be any maximal independent set of G ' (i.e., adding any other vertex to\nS would violate independence). Prove that, for each vertex v ∈ V , S contains exactly\none of the Δ + 1 vertices in V ' of the form (v, i). Hint: Use the Pigeonhole Principle.\nSolution: It cannot contain more than one, since all of these are connected in G ' and\nthat would violate independence.\nNow suppose for contradiction that, for some particular u, S contains no vertices of\nthe form (u, i). Then by maximality, every vertex of the form (u, i) must have some\nG '-neighbor in S. Since that neighbor is not of the form (u, ∗), it must be of the form\n(v, i), for some v with (u, v) ∈ E.\nThus, each of the Δ+1 vertices of the form (u, i) has some neighbor of the form (v, i)\nin S, where (u, v) ∈ E. Since u has at most Δ neighbors in G, by the Pigeonhole\nPrinciple, there must be two different values of i, say i1 and i2, for which there is a\nsingle v such that (u, ii) is a G '-neighbor of (v, i1), (u, i2) is a G '-neighbor of (v, i2),\nand both (v, i1) and (v, i2) are in S. That is a contradiction because S can contain at\nmost one vertex of the form (v, ∗).\n\n6.046J/18.410J Final Solutions\nName\n(b) [8 points] Now consider a synchronous network of processes based on the graph G,\nwhere every vertex knows an upper bound Δ on the degree. Give a distributed algo\nrithm to find a vertex (Δ + 1)-coloring of G, i.e., a mapping from vertices in V to\ncolors in {0, 1, . . . , Δ} such that adjacent vertices have distinct colors. The process\nassociated with each vertex should output its color. Argue correctness.\nHint: Combine part (a) with Luby's algorithm.\nSolution: The \"colors\" will be chosen from {0, 1, . . . , Δ}.\nThe nodes of G simulate an MIS algorithm for G ' . Specifically, the node associated\nwith vertex u of G simulates the Δ + 1 nodes associated with vertices of the form\n(u, i) of G ' . The algorithm produces an MIS S for G ', where each node of G learns\nwhich of its simulated nodes correspond to vertices in S. By Part (a), for each vertex u\nof G, there is a unique color i such that (u, i) ∈ S; the node associated with u chooses\nthis color i.\nObviously, this strategy uses at most Δ + 1 colors. To see that no two neighbors in G\nare colored with the same color, suppose for contradiction that neighbors u and v are\ncolored with the same color, say i. That means that both (u, i) and (v, i) are in S. But\n(u, i) and (v, i) are neighbors in G ', contradicting the independence property for S.\nAn alternative solution that many students wrote involved executing Δ + 1 instances\nof Luby's MIS directly on G, in succession. In each instance i, the winners are colored\nwith color i. Then we remove just the winners before executing the next instance. This\nworks, but leaves out some details w.r.t. synchronizing the starts of the successive\ninstances. Also, its performance is quite a bit worse than the recommended solution\nabove.\n(c) [4 points] Analyze the expected time and communication costs for solving the color\ning problem in this way, including the cost of Luby's algorithm.\nSolution: The costs are just those of solving MIS on G '; the final decisions are local\nand don't require any extra rounds.\nTime (number of rounds): The expected time to solve MIS on G ' is O(lg (n · Δ)),\nbecause the number of nodes in G ' is n · (Δ + 1). The O(lg (n · Δ)) bound can be\nsimplified to O(lg n).\nCommunication (number of messages): The expected number of messages is O(E lg n),\ncorresponding to O(lg n) rounds and messages on all edges (in both directions) at each\nround.\n\n6.046J/18.410J Final Solutions\nName\nSCRATCH PAPER\n\n6.046J/18.410J Final Solutions\nName\nSCRATCH PAPER\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Design and Analysis of Algorithms, Final Exam",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/4c82af3a9820b038e2caba09d023de12_MIT6_046JS15_final.pdf",
      "content": "Design and Analysis of Algorithms\nMay 20, 2015\nMassachusetts Institute of Technology\n6.046J/18.410J\nProfs. Erik Demaine, Srini Devadas, and Nancy Lynch\nFinal Exam\nFinal Exam\n- Do not open this exam booklet until you are directed to do so. Read all the instructions first.\n- The exam contains 9 problems, with multiple parts. You have 180 minutes to earn 180 points.\n- This exam booklet contains 18 pages, including this one.\n\"\"\n- This exam is closed book. You may use three double-sided letter (81\n× 11\"\" ) or A4 crib\nsheets. No calculators or programmable devices are permitted. Cell phones must be put\naway.\n- Do not waste time deriving facts that we have studied. Just cite results from class.\n- When we ask you to \"give an algorithm\" in this exam, describe your algorithm in English\nor pseudocode, and provide a short argument for correctness and running time. You do not\nneed to provide a diagram or example unless it helps make your explanation clearer.\n- Do not spend too much time on any one problem. Generally, a problem's point value is an\nindication of how many minutes to spend on it.\n- Show your work, as partial credit will be given. You will be graded not only on the correct\nness of your answer, but also on the clarity with which you express it. Please be neat.\n- Good luck!\nQ Title\nPoints Parts Grade\nQ Title\nPoints Parts Grade\nTrue or False\nBe the Computer\nUberstructure\nStartups are Hard\nMeancorp\nLoad Balancing\nForgetful Forrest\nDistributed Coloring\nPiano Recital\nTotal\nName:\n\n6.046J/18.410J Final Exam\nName\nProblem 1. True or False. [56 points] (14 parts)\nCircle T or F for each of the following statements to indicate whether the statement is true or false\nand briefly explain why.\n(a) T F [4 points] Suppose algorithm A has two steps, and A succeeds if both the steps\nsucceed. If the two steps succeed with probability p1 and p2 respectively, then A\nsucceeds with probability p1p2.\n(b) T F [4 points] If the divide-and-conquer convex hull algorithm (from Lecture 2) used\na Θ(n2) strategy to discover the maximum and minimum tangents, the overall\nalgorithm would run in Θ(n2 log n) time.\n(c) T F [4 points] In order to get an expected Θ(n log n) runtime for \"paranoid\" quick\nsort (from Lecture 3), we require the recursive divide step to split the array into\ntwo subarrays each of at least 1\n4 the size of the original array.\n(d) T F [4 points] A binary min-heap with n elements supports INSERT in O(log n)\namortized time and DELETE-MIN in 0 amortized time.\n\n6.046J/18.410J Final Exam\nName\n(e) T F [4 points] The hash family H = {h1, h2} is universal, where h1, h2 : {1, 2, 3} →\n{0, 1} are defined by the following table:\n1 2 3\nh1\n0 1 0\nh2\n1 0 1\n(For example, h1(3) = 0.)\n(f) T F [4 points] Recall the O(n3 lg n) matrix-multiplication algorithm to compute\nshortest paths, where we replaced the matrix-multiplication operator pair (∗, +)\nwith (+, min). If we instead replace the operator pair with (+, ∗), then we com\npute the product of the weights of all paths between each pair of vertices.\n(g) T F [4 points] Negating all the edge weights in a weighted undirected graph G and\nthen finding the minimum spanning tree gives us the maximum-weight spanning\ntree of the original graph G.\n(h) T F [4 points] In a graph with unique edge weights, the spanning tree of second-\nlowest weight is unique.\n\n6.046J/18.410J Final Exam\nName\n(i) T F [4 points] In the recursion of the Floyd-Warshall algorithm:\n(k-1)\n(k-1)\nd(k) = min{d(k-1), d\n+ d\n},\nuv\nuv\nuk\nkv\n(k)\nduv represents the length of the shortest path from vertex u to vertex v that con\ntains at most k edges.\n(j) T F [4 points] Consider a network of processes based on an arbitrary undirected\ngraph G = (V, E) with a distinguished vertex v0 ∈ V . The process at each\nvertex v ∈ V starts with a positive integer xv. The goal is for the process at\nv0 to compute the maximum maxv∈V xv. There is an asynchronous distributed\nalgorithm that solves this problem using O(diam2d) time and O(E + diam · n)\nmessages.\n(k) T F [4 points] Suppose a file server stores a hash of every file in addition to the\nfile contents. When you download a file from the server, you also download\nthe hash and confirm that it matches the file. This system securely verifies that\nthe downloaded file has not been modified by an adversary, provided the hash\nfunction has collision resistance.\n\n6.046J/18.410J Final Exam\nName\n(l) T F [4 points] Suppose Alice, Bob, and Charlie secretly generate a, b and c, respec\na\nc\ntively, and publish g mod p, gb mod p, and g mod p, where p is a prime. Then,\nAlice, Bob, and Charles can each compute gabc mod p as a shared secret known\nonly to the three of them.\n(m) T F [4 points] The number of memory transfers used by the best cache-oblivious\nalgorithm is always at least the number of memory transfers used by the best\nexternal-memory algorithm for the same problem.\n(n) T F [4 points] If there is a time-optimal divide-and-conquer algorithm for a problem,\nthen that algorithm is also optimal with respect to memory transfers in the cache-\noblivious model.\n\n6.046J/18.410J Final Exam\nName\n\nProblem 2. Uberstructure [10 points] (1 part)\nDesign a data structure that maintains a dynamic set S of n elements subject to the following\noperations and time bounds:\nOperation\nEffect\nTime Bound\n1. INSERT(x, S)\n2. DELETE(x, S)\n3. SUCCESSOR(x, S)\n4. FIND-MIN(S)\n5. SEARCH(x, S)\nInsert x into S.\nDelete x from S.\nFind the smallest element in S larger than x.\nReturn the smallest element in S.\nReturn TRUE if element x is in S.\nO(log n)\nO(log n)\nO(log n)\nO(1)\nO(1)\nexpected amortized\nexpected amortized\nworst-case\nworst-case\nexpected\nDescribe how the operations are implemented on your data structure and justify their runtime.\n\n6.046J/18.410J Final Exam\nName\nProblem 3. Meancorp [15 points] (2 parts)\nYou are in charge of the salary database for Meancorp, which stores all employee salaries in a 2-3\ntree ordered by salary. Meancorp compiles regular reports to the Department of Fairness about the\nsalary for low-income employees in the firm. You are asked to implement a new database operation\nAVERAGE(x) which returns the average salary of all employees whose salary is at most x.\n(a) [10 points] What extra information needs to be stored at each node? Describe how to\nanswer an AVERAGE(x) query in O(lg n) time using this extra information.\n(b) [5 points] Describe how to modify INSERT to maintain this information. Briefly\njustify that the worst-case running time for INSERT remains O(lg n).\n\n6.046J/18.410J Final Exam\nName\nProblem 4. Forgetful Forrest [15 points] (3 parts)\nProf. Forrest Gump is very forgetful, so he uses automatic calendar reminders for his appointments.\nFor each reminder he receives for an event, he has a 50% chance of actually remembering the event\n(decided by an independent coin flip).\n(a) [5 points] Suppose we send Forrest k reminders for each of n events. What is the\nexpected number of appointments Forrest will remember? Give your answer in terms\nof k and n.\n(b) [5 points] Suppose we send Forrest k reminders for a single event. How should we\nset k with respect to n so that Forrest will remember the event with high probability,\ni.e., 1 - 1/nα?\n(c) [5 points] Suppose we send Forrest k reminders for each of n events. How should\nwe set k with respect to n so that Forrest will remember all the events with high\nprobability, i.e., 1 - 1/nα?\n\n6.046J/18.410J Final Exam\nName\nProblem 5. Piano Recital [15 points] (3 parts)\nProf. Chopin has a piano recital coming up, and in preparation, he wants to learn as many pieces\nas possible. There are m possible pieces he could learn. Each piece i takes pi hours to learn.\nProf. Chopin has a total of T hours that he can study by himself (before getting bored). In addition,\nhe has n piano teachers. Each teacher j will spend up to tj hours teaching. The teachers are very\nstrict, so they will teach Prof. Chopin only a single piece, and only if no other teacher is teaching\nhim that piece.\nThus, to learn piece i, Prof. Chopin can either (1) learn it by himself by spending pi of his T self-\nlearning budget; or (2) he can choose a unique teacher j (not chosen for any other piece), learn\ntogether for min{pi, tj } hours, and if any hours remain (pi > tj ), learn the rest using pi - tj hours\nof his T self-learning budget. (Learning part of a piece is useless.)\n(a) [6 points] Assume that Prof. Chopin decides to learn exactly k pieces. Prove that he\nneeds to consider only the k lowest pis and the k highest tj s.\n\n6.046J/18.410J Final Exam\nName\n(b) [5 points] Assuming part (a), give an efficient greedy algorithm to determine whether\nProf. Chopin can learn exactly k pieces. Argue its correctness.\n(c) [4 points] Using part (b) as a black box, give an efficient algorithm that finds the\nmaximum number of pieces Prof. Chopin can learn. Analyze its running time.\n\n6.046J/18.410J Final Exam\nName\nProblem 6. Be the Computer [14 points] (3 parts)\nConsider the following flow network and initial flow f. We will perform one iteration of the\nEdmonds-Karp algorithm.\n10:10$\n13:15$\n0:15$\n10:10$\n14:30$\n4:6$\n1:4$\n2:4$\n7:9$\n7:10$\n8:9$\n1:15$\n0:15$\n2:3$\n7:7$\ns\"\n4\"\nt\"\n5\"\n6\"\n2\"\n3\"\n7\"\n(a) [5 points] Draw the residual graph Gf of G with respect to f.\ns\"\n4\"\nt\"\n5\"\n6\"\n2\"\n3\"\n7\"\n(b) [4 points] List the vertices in the shortest augmenting path, that is, the augmenting\npath with the fewest possible edges.\n(c) [5 points] Perform the augmentation. What is the value of the resulting flow?\n\n6.046J/18.410J Final Exam\nName\nProblem 7. Startups are Hard [20 points] (3 parts)\nFor your new startup company, Uber for Algorithms, you are trying to assign projects to employees.\nYou have a set P of n projects and a set E of m employees. Each employee e can only work on\none project, and each project p ∈ P has a subset Ep ⊆ E of employees that must be assigned to p\nto complete p. The decision problem we want to solve is whether we can assign the employees to\nprojects such that we can complete (at least) k projects.\n(a) [5 points] Give a straightforward algorithm that checks whether any subset of k\nprojects can be completed to solve the decisional problem. Analyze its time com\nplexity in terms of m, n, and k.\n(b) [5 points] Is your algorithm in part (a) fixed-parameter tractable? Briefly explain.\n\n6.046J/18.410J Final Exam\nName\n(c) [10 points] Show that the problem is NP-hard via a reduction from 3D matching.\nRecall the 3D matching problem: You are given three sets X, Y , Z, each of size m;\na set T ⊆ X × Y × Z of triples; and an integer k. The goal is to determine whether\nthere is a subset S ⊆ T of (at least) k disjoint triples.\n\n6.046J/18.410J Final Exam\nName\nProblem 8. Load Balancing [15 points] (2 parts)\nSuppose you need to complete n jobs, and the time it takes to complete job i is ti. You are\ngiven m identical machines M1, M2, . . . , Mm to run the jobs on. Each machine can run only one\njob at a time, and each job must be completely run on a single machine. If you assign a set\nm\nJj ⊆{1, 2, . . . , n} of jobs to machine Mj , then it will need Tj =\nti time. Your goal is to\ni∈Jj\npartition the n jobs among the m machines to minimize maxi Ti.\n(a) [5 points] Describe a greedy approximation algorithm for this problem.\n(b) [10 points] Show that your algorithm from part (a) is a 2-approximation algorithm.\nHint: Determine an ideal bound on the optimal solution OPT. Then consider the ma\nchine Me with the longest Te, and the last job i∗ that was added to it.\n\n6.046J/18.410J Final Exam\nName\nProblem 9. Distributed Coloring [20 points] (3 parts)\nConsider an undirected graph G = (V, E) in which every vertex has degree at most Δ. Define a\nnew graph G \" = (V \" , E \" ), the Cartesian product of G with a clique of size Δ + 1. Specifically, V \"\nis the set of pairs (v, i) for all vertices v ∈ V and integers i with 0 ≤ i ≤ Δ, and E \" consists of\ntwo types of edges:\n1. For each edge {u, v} ∈ E, there is an edge between (u, i) and (v, i) in E \", for all 0 ≤ i ≤ Δ.\n(Thus, each index i forms a copy of G.)\n2. For each vertex v ∈ V , there is an edge between (v, i) and (v, j) in E \", for all i\nj with\n=\n0 ≤ i, j ≤ Δ. (Thus each v forms a (Δ + 1)-clique.)\nHere is an example of this transformation with Δ = 3:\nFigure 1: Graph G.\nFigure 2: The Cartesian product G \" of G and a clique of size 4.\n(a) [8 points] Let S be any maximal independent set of G \" (i.e., adding any other vertex to\nS would violate independence). Prove that, for each vertex v ∈ V , S contains exactly\none of the Δ + 1 vertices in V \" of the form (v, i). Hint: Use the Pigeonhole Principle.\n\n6.046J/18.410J Final Exam\nName\n(b) [8 points] Now consider a synchronous network of processes based on the graph G,\nwhere every vertex knows an upper bound Δ on the degree. Give a distributed algo\nrithm to find a vertex (Δ + 1)-coloring of G, i.e., a mapping from vertices in V to\ncolors in {0, 1, . . . , Δ} such that adjacent vertices have distinct colors. The process\nassociated with each vertex should output its color. Argue correctness.\nHint: Combine part (a) with Luby's algorithm.\n(c) [4 points] Analyze the expected time and communication costs for solving the color\ning problem in this way, including the cost of Luby's algorithm.\n\n6.046J/18.410J Final Exam\nName\nSCRATCH PAPER\n\n6.046J/18.410J Final Exam\nName\nSCRATCH PAPER\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Class on Design and Analysis of Algorithms, Quiz 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/dc92909c424ece61e77b3ec13569098f_MIT6_046JS15_quiz1.pdf",
      "content": "Design and Analysis of Algorithms\nMarch 12, 2015\nMassachusetts Institute of Technology\n6.046J/18.410J\nProfs. Erik Demaine, Srini Devadas, and Nancy Lynch\nQuiz 1\nQuiz 1\n- Do not open this quiz booklet until you are directed to do so. Read all the instructions first.\n- The quiz contains 7 problems, with multiple parts. You have 120 minutes to earn 120 points.\n- This quiz booklet contains 12 pages, including this one.\n\"\"\n- This quiz is closed book. You may use one double-sided letter (8 1\n2 × 11\"\" ) or A4 crib sheet.\nNo calculators or programmable devices are permitted. Cell phones must be put away.\n- Do not waste time deriving facts that we have studied. Just cite results from class.\n- When we ask you to \"give an algorithm\" in this quiz, describe your algorithm in English\nor pseudocode, and provide a short argument for correctness and running time. You do not\nneed to provide a diagram or example unless it helps make your explanation clearer.\n- Do not spend too much time on any one problem. Generally, a problem's point value is an\nindication of how many minutes to spend on it.\n- Show your work, as partial credit will be given. You will be graded not only on the correct\nness of your answer, but also on the clarity with which you express it. Please be neat.\n- Good luck!\nProblem Title\nPoints Parts Grade Initials\nTrue or False\nFast Fourier Transform\nYellow Brick Road\nAmortized Analysis\nVerifying Polynomial Multiplication\nDynamic Programming\nMedian of Sorted Arrays\nTotal\nName:\n\n6.046J/18.410J Quiz 1\nName\nProblem 1. True or False. [40 points] (10 parts)\nCircle T or F for each of the following statements to indicate whether the statement is true or false\nand briefly explain why.\n(a) T F [4 points] With all equal-sized intervals, a greedy algorithm based on earliest\nstart time will always select the maximum number of compatible intervals.\n(b) T F [4 points] The problem of weighted interval scheduling can be solved in O(n log n)\ntime using dynamic programming.\n(c) T F [4 points] If we divide an array into groups of 3, find the median of each group,\nrecursively find the median of those medians, partition, and recurse, then we can\nobtain a linear-time median-finding algorithm.\n(d) T F [4 points] If we used the obvious Θ(n2) merge algorithm in the divide-and\nconquer convex-hull algorithm, the overall time complexity would be Θ(n2 log n).\n(e) T F [4 points] Van Emde Boas sort (where we insert all numbers, find the min, and\nthen repeatedly call SUCCESSOR) can be used to sort n = lg u numbers in O(lg u·\nlg lg lg u) time.\n\n6.046J/18.410J Quiz 1\nName\n(f) T F [4 points] Van Emde Boas on n integers between 0 and u - 1 supports successor\nqueries in O(lg lg u) worst-case time using O(n) space.\n(g) T F [4 points] In the potential method for amortized analysis, the potential energy\nshould never go negative.\n(h) T F [4 points] The quicksort algorithm that uses linear-time median finding to run in\nworst-case O(n log n) time requires Θ(n) auxiliary space.\n(i) T F [4 points] Searching in a skip list takes Θ(log n) time with high probability, but\ncould take Ω(2n) time with nonzero probability.\n(j) T F [3 points] The following collection H = {h1, h2, h3} of hash functions is uni\nversal, where each hash function maps the universe U = {A, B, C, D} of keys\ninto the range {0, 1, 2} according to the following table:\nx\nA B C D\nh1(x) 1\nh2(x) 0\nh3(x) 2\n\n6.046J/18.410J Quiz 1\nName\nProblem 2. Fast Fourier Transform (FFT). [5 points] (1 part)\nBen Bitdiddle is trying to multiply two polynomials using the FFT. In his trivial example, Ben sets\na = (0, 1) and b = (0, 1), both representing 0 + x, and calculates:\nA = F(a) = B = F(b) = (1, -1),\nC = A ∗ B = (1, 1),\nc = F -1(C) = (1, 0).\nSo c represents 1 + 0 · x, which is clearly wrong. Point out Ben's mistake in one sentence; no\ncalculation needed. (Ben swears he has calculated FFT F and inverse FFT F -1 correctly.)\n\n6.046J/18.410J Quiz 1\nName\nProblem 3. Yellow Brick Road. [10 points] (1 part)\nProf. Gale is developing a new Facebook app called \"Yellow Brick Road\" for maintaining a user's\ntimeline, here represented as a time-ordered list e0, e1, . . . , en-1 of n (unchanging) events. (In\nFacebook, events can never be deleted, and for the purposes of this problem, don't worry about\ninsertions either.) The app allows the user to mark an event ei as yellow (important) or grey\n(unimportant); initially all events are grey. The app also allows the user to jump to the next yellow\nevent that comes after the event ei currently on the screen (which may be yellow or grey). More\nformally, you must support the following operations:\n1. MARK-YELLOW(i): Mark ei yellow.\n2. MARK-GREY(i): Mark ei grey.\n3. NEXT-YELLOW(i): Find the smallest j > i such that ej is yellow.\nGive the fastest data structure you can for this problem, measured according to worst-case time.\nThe faster your data structure, the better.\nHint: Use a data structure you have seen in either 6.006 or 6.046 as a building block.\n\n6.046J/18.410J Quiz 1\nName\nProblem 4. Amortized Analysis. [15 points] (1 part)\nDesign a data structure to maintain a set S of n distinct integers that supports the following two\noperations:\n1. INSERT(x, S): insert integer x into S.\n\n2. REMOVE-BOTTOM-HALF(S): remove the smallest\nn\nintegers from S.\nDescribe your algorithm and give the worse-case time complexity of the two operations. Then\ncarry out an amortized analysis to make INSERT(x, S) run in amortized O(1) time, and REMOVE\nBOTTOM-HALF(S) run in amortized 0 time.\n\n6.046J/18.410J Quiz 1\nName\nProblem 5. Verifying Polynomial Multiplication. [15 points] (4 parts)\nThis problem will explore how to check the product of two polynomials. Specifically, we are given\nthree polynomials:\nn\nn-1\np(x) = anx + an-1x\n+ · · · + a0,\nn\nn-1\nq(x) = bnx + bn-1x\n+ · · · + b0,\n2n\n2n-1\nr(x) = c2nx + c2n-1x\n+ · · · + c0.\nWe want to check whether p(x)·q(x) = r(x) (for all values x). Via FFT, we could simply compute\np(x) · q(x) and check in O(n log n) time. Instead, we aim to achieve O(n) time via randomization.\n(a) [5 points] Describe an O(n)-time randomized algorithm for testing whether p(x) ·\nq(x) = r(x) that satisfies the following properties:\n1. If the two sides are equal, the algorithm outputs YES.\n2. If the two sides are unequal, the algorithm outputs NO with probability at least 1\n2 .\n(b) [2 points] Prove that your algorithm satisfies Property 1.\n\n6.046J/18.410J Quiz 1\nName\n(c) [3 points] Prove that your algorithm satisfies Property 2.\nHint: Recall the Fundamental Theorem of Algebra: A degree-d polynomial has (at\nmost) d roots.\n(d) [5 points] Design a randomized algorithm to check whether p(x) · q(x) = r(x) that is\ncorrect with probability at least 1 - ε. Analyze your algorithm in terms of n and 1/ε.\n\n6.046J/18.410J Quiz 1\nName\nProblem 6. Dynamic Programming. [15 points] (2 parts)\nProf. Child is cooking from her garden, which is arranged in grid with n rows and m columns.\nEach cell (i, j) (1 ≤ i ≤ n, 1 ≤ j ≤ m) has an ingredient growing in it, with tastiness given by\na positive value Ti,j . Prof. Child doesn't like cooking \"by the book\". To prepare dinner, she will\nstand at a cell (i, j) and pick one ingredient from each quadrant relative to that cell. The tastiness\nof her dish is the product of the tastiness of the four ingredients she chooses. Help Prof. Child find\nan O(nm) dynamic programming algorithm to maximize the tastiness of her dish.\nHere the four quadrants relative to a cell (i, j) are defined as follows:\ntop-left = {all cells (a, b) | a < i, b < j},\nbottom-left = {all cells (a, b) | a > i, b < j},\ntop-right = {all cells (a, b) | a < i, b > j},\nbottom-right = {all cells (a, b) | a > i, b > j}.\nBecause Prof. Child needs all four quadrants to be non-empty, she can only stand on cells (i, j)\nwhere 1 < i < n and 1 < j < m.\n(a) [10 points] Define TLi,j to be maximum tastiness value in the top-left quadrant of\ncell (i, j): TLi,j = max{Ta,b | 1 ≤ a ≤ i, 1 ≤ b ≤ j}. Find a dynamic programming\nalgorithm to compute TLi,j , for all 1 < i < n and 1 < j < m, in O(nm) time.\n\n6.046J/18.410J Quiz 1\nName\n(b) [5 points] Use the idea in part (a) to obtain an O(nm) algorithm to find the tastiest\ndish.\n\n6.046J/18.410J Quiz 1\nName\nProblem 7. Median of two sorted arrays. [20 points] (3 parts)\nFinding the median of a sorted array is easy: return the middle element. But what if you are given\ntwo sorted arrays A and B, of size m and n respectively, and you want to find the median of all the\nnumbers in A and B? You may assume that A and B are disjoint.\n(a) [3 points] Give a na ıve algorithm running in Θ(m + n) time.\n(b) [10 points] If m = n, give an algorithm that runs in Θ(lg n) time.\n\n6.046J/18.410J Quiz 1\nName\n(c) [7 points] Give an algorithm that runs in O(lg(min{m, n})) time, for any m and n.\nDon't spend too much time on this question!\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Class on Design and Analysis of Algorithms, Solutions to Quiz 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/5b6cd99674ea0c866c942b72aa9e7289_MIT6_046JS15_quiz1sols.pdf",
      "content": "Design and Analysis of Algorithms\nMarch 16, 2015\nMassachusetts Institute of Technology\n6.046J/18.410J\nProfs. Erik Demaine, Srini Devadas, and Nancy Lynch\nQuiz 1 Solutions\nQuiz 1 Solutions\n- Do not open this quiz booklet until you are directed to do so. Read all the instructions first.\n- The quiz contains 7 problems, with multiple parts. You have 120 minutes to earn 120 points.\n- This quiz booklet contains 12 pages, including this one.\n\"\"\n- This quiz is closed book. You may use one double-sided letter (8 1\n2 × 11\"\" ) or A4 crib sheet.\nNo calculators or programmable devices are permitted. Cell phones must be put away.\n- Do not waste time deriving facts that we have studied. Just cite results from class.\n- When we ask you to \"give an algorithm\" in this quiz, describe your algorithm in English\nor pseudocode, and provide a short argument for correctness and running time. You do not\nneed to provide a diagram or example unless it helps make your explanation clearer.\n- Do not spend too much time on any one problem. Generally, a problem's point value is an\nindication of how many minutes to spend on it.\n- Show your work, as partial credit will be given. You will be graded not only on the correct\nness of your answer, but also on the clarity with which you express it. Please be neat.\n- Good luck!\nProblem Title\nPoints Parts Grade Initials\nTrue or False\nFast Fourier Transform\nYellow Brick Road\nAmortized Analysis\nVerifying Polynomial Multiplication\nDynamic Programming\nMedian of Sorted Arrays\nTotal\nName:\n\n6.046J/18.410J Quiz 1 Solutions\nName\nProblem 1. True or False. [40 points] (10 parts)\nCircle T or F for each of the following statements to indicate whether the statement is true or false\nand briefly explain why.\n(a) T F [4 points] With all equal-sized intervals, a greedy algorithm based on earliest\nstart time will always select the maximum number of compatible intervals.\nSolution: True. The algorithm is equivalent to the earliest finish time algorithm.\n(b) T F [4 points] The problem of weighted interval scheduling can be solved in O(n log n)\ntime using dynamic programming.\nSolution: True. The algorithm was covered in recitation.\n(c) T F [4 points] If we divide an array into groups of 3, find the median of each group,\nrecursively find the median of those medians, partition, and recurse, then we can\nobtain a linear-time median-finding algorithm.\nSolution: False. T (n) = T (n/3) + T (2n/3) + O(n) does not solve to T (n) =\nO(n). The array has to be broken up into groups of at least 5 to obtain a linear-\ntime algorithm.\n(d) T F [4 points] If we used the obvious Θ(n2) merge algorithm in the divide-and\nconquer convex-hull algorithm, the overall time complexity would be Θ(n2 log n).\nSolution: False. The time complexity would satisfy the recurrence T (n) =\n2T (n/2) + Θ(n2), which solves to Θ(n2) by the Master Theorem.\n(e) T F [4 points] Van Emde Boas sort (where we insert all numbers, find the min, and\nthen repeatedly call SUCCESSOR) can be used to sort n = lg u numbers in O(lg u·\nlg lg lg u) time.\nSolution: False. Inserting into the tree and then finding all the successors will\ntake n lg lg(u) time, which in terms of u is lg(u) · lg lg(u).\n\n6.046J/18.410J Quiz 1 Solutions\nName\n(f) T F [4 points] Van Emde Boas on n integers between 0 and u - 1 supports successor\nqueries in O(lg lg u) worst-case time using O(n) space.\nSolution: False. We use Θ(u) space or do randomization.\n(g) T F [4 points] In the potential method for amortized analysis, the potential energy\nshould never go negative.\nSolution: True.\n(h) T F [4 points] The quicksort algorithm that uses linear-time median finding to run in\nworst-case O(n log n) time requires Θ(n) auxiliary space.\nSolution: False. It can be implemented with O(log n) auxiliary space.\n(i) T F [4 points] Searching in a skip list takes Θ(log n) time with high probability, but\ncould take Ω(2n) time with nonzero probability.\nSolution: True. A skip list could be of any height with nonzero probability,\ndepending on its random choices.\nAlternative solution: False. We can limit the height of the skip list to O(n) or\nO(lg n) to get O(n) worse-case cost.\nCommon mistake 1: We go through each element at least once. (Wrong because\nwe also need to \"climb up\" the skip lists.\nCommon mistake 2: The worst case is when none of the elements is promoted,\nor all of the elements are promoted to the same level, in which cases skip lists\nbecome a linked list.\n(j) T F [3 points] The following collection H = {h1, h2, h3} of hash functions is uni\nversal, where each hash function maps the universe U = {A, B, C, D} of keys\ninto the range {0, 1, 2} according to the following table:\nx\nA B C D\nh1(x) 1\nh2(x) 0\nh3(x) 2\nSolution: False. A and C collide with probability 2/3.\n\n6.046J/18.410J Quiz 1 Solutions\nName\nProblem 2. Fast Fourier Transform (FFT). [5 points] (1 part)\nBen Bitdiddle is trying to multiply two polynomials using the FFT. In his trivial example, Ben sets\na = (0, 1) and b = (0, 1), both representing 0 + x, and calculates:\nA = F(a) = B = F(b) = (1, -1),\nC = A ∗ B = (1, 1),\nc = F -1(C) = (1, 0).\nSo c represents 1 + 0 · x, which is clearly wrong. Point out Ben's mistake in one sentence; no\ncalculation needed. (Ben swears he has calculated FFT F and inverse FFT F -1 correctly.)\nSolution: The resulting polynomial is of degree 2, so Ben need to pad a and b with zeroes. (Or\nBen need at least 3 samples to do FFT).\nHere is the correct calculaton (not required in the solution). Let a = b = (0, 1, 0, 0); then,\nA = F(a) = B = F(b) = (1, -i, -1, i)\nC = A ∗ B = (1, -1, 1, -1)\nc = F -1(C) = (0, 0, 1, 0)\nwhich represents x2. It is also OK to set a = b = (0, 1, 0).\nCommon mistake 1: A ∗ B should be convolution.\nCommon mistake 2: Ben should reverse b.\nBoth mistakes confuse the relation between convolution, polynomial multiplication and FFT cal\nculation. If one computes convolution directly (without FFT), one needs to reverse the second\nvector. FFT provides a faster way to compute convolution and polynomial multiplication (these\ntwo are the same thing). Using the FFT, one should perform FFT on the two original vectors (no\nreversal). Then, after the FFT, one only needs to do element-wise multiplication (as opposed to\nconvolution), which Ben performed correctly.\n\n6.046J/18.410J Quiz 1 Solutions\nName\nProblem 3. Yellow Brick Road. [10 points] (1 part)\nProf. Gale is developing a new Facebook app called \"Yellow Brick Road\" for maintaining a user's\ntimeline, here represented as a time-ordered list e0, e1, . . . , en-1 of n (unchanging) events. (In\nFacebook, events can never be deleted, and for the purposes of this problem, don't worry about\ninsertions either.) The app allows the user to mark an event ei as yellow (important) or grey\n(unimportant); initially all events are grey. The app also allows the user to jump to the next yellow\nevent that comes after the event ei currently on the screen (which may be yellow or grey). More\nformally, you must support the following operations:\n1. MARK-YELLOW(i): Mark ei yellow.\n2. MARK-GREY(i): Mark ei grey.\n3. NEXT-YELLOW(i): Find the smallest j > i such that ej is yellow.\nGive the fastest data structure you can for this problem, measured according to worst-case time.\nThe faster your data structure, the better.\nHint: Use a data structure you have seen in either 6.006 or 6.046 as a building block.\nSolution: Initialization takes O(n lg(lg(n))) time to insert all the yellow elements into a VEB\ntree, V .\nMore importantly, each operation takes O(lg lg(n)) time. When a user asks to MARK-YELLOW(i),\nthen call V.insert(i) which takes O(lg lg(n)) time. When a user asks to MARK-GREY(i), then\ncall V.delete(i) which takes O(lg lg(n)) time. When a user asks to NEXT-YELLOW(i), then call\nV.successor(i) which takes O(lg lg(n)) time.\nAnother slower solution used an AVL tree in place of a vEB for an O(lg(n)) runtime for the\noperations.\nCommon mistake 1: Claiming operations took O(lg lg(u)). The universe size is exactly n, and\nthe term u was undefined.\nCommon mistake 2: Inserting both yellow and grey elements into the same data structure without\nan augmentation to keep track of whether any yellow elements existed within children. Next-\nYellow could take O(n) in the worst case.\nCommon mistake 3: Use a skip list to keep track of all yellow elements. Some operations would\ntake O(lg(n)) with high probability, but O(n) worst case.\nCommon mistake 4: Using a skip list capped at 2 levels, doubly linked list, or hash table. Some\noperations would take O(n) worst case.\n\n6.046J/18.410J Quiz 1 Solutions\nName\nProblem 4. Amortized Analysis. [15 points] (1 part)\nDesign a data structure to maintain a set S of n distinct integers that supports the following two\noperations:\n1. INSERT(x, S): insert integer x into S.\n\n2. REMOVE-BOTTOM-HALF(S): remove the smallest\nn\nintegers from S.\nDescribe your algorithm and give the worse-case time complexity of the two operations. Then\ncarry out an amortized analysis to make INSERT(x, S) run in amortized O(1) time, and REMOVE\nBOTTOM-HALF(S) run in amortized 0 time.\nSolution:\nUse a singly linked list to store those integers. To implement INSERT(x, S), we append the new\ninteger to the end of the linked list. This takes Θ(1) time. To implement REMOVE-BOTTOM\nHALF(S), we use the median finding algorithm taught in class to find the median number, and then\ngo through the list again to delete all the numbers smaller or equal than the median. This takes\nΘ(n) time.\nSuppose the runtime of REMOVE-BOTTOM-HALF(S) is bounded by cn for some constant c. For\namortized analysis, use Φ = 2cn as our potential function. Therefore, the amortized cost of an\ninsertion is 1 + ΔΦ = 1 + 2c = Θ(1). The amortized cost of REMOVE-BOTTOM-HALF(S) is\ncn + ΔΦ = cn + (-2c × n\n2 ) = 0.\n\n6.046J/18.410J Quiz 1 Solutions\nName\nProblem 5. Verifying Polynomial Multiplication. [15 points] (4 parts)\nThis problem will explore how to check the product of two polynomials. Specifically, we are given\nthree polynomials:\nn\nn-1\np(x) = anx + an-1x\n+ · · · + a0,\nn\nn-1\nq(x) = bnx + bn-1x\n+ · · · + b0,\n2n\n2n-1\nr(x) = c2nx + c2n-1x\n+ · · · + c0.\nWe want to check whether p(x)·q(x) = r(x) (for all values x). Via FFT, we could simply compute\np(x) · q(x) and check in O(n log n) time. Instead, we aim to achieve O(n) time via randomization.\n(a) [5 points] Describe an O(n)-time randomized algorithm for testing whether p(x) ·\nq(x) = r(x) that satisfies the following properties:\n1. If the two sides are equal, the algorithm outputs YES.\n2. If the two sides are unequal, the algorithm outputs NO with probability at least 1\n2 .\nSolution: Pick a value a ∈ [1, 4n], and check whether p(a)q(a) = r(a). The algo\nrithm outputs YES if the two sides are equal, and NO otherwise. It takes O(n) time to\nevaluate the three polynomials of degree O(n). Thus the overall running time of the\nalgorithm is O(n).\n(b) [2 points] Prove that your algorithm satisfies Property 1.\nSolution: If p(x) · q(x) = r(x), then both sides will evaluate to the same thing for\nany input.\n\n6.046J/18.410J Quiz 1 Solutions\nName\n(c) [3 points] Prove that your algorithm satisfies Property 2.\nHint: Recall the Fundamental Theorem of Algebra: A degree-d polynomial has (at\nmost) d roots.\nSolution: s(x) = r(x) - p(x) · q(x) is a degree-2n polynomial, and thus has at most\n2n roots. Then\n2n\nPr{s(a) = 0} ≤\n=\n4n\nsince a was picked from a set of size 4n.\n(d) [5 points] Design a randomized algorithm to check whether p(x) · q(x) = r(x) that is\ncorrect with probability at least 1 - ε. Analyze your algorithm in terms of n and 1/ε.\nSolution: We run part a m times, and output YES if and only if all answers output\nYES. In other words, we amplify the probability of success via repetition.\nm\nOur test works with probability ≥ 1 -\n. Thus we need\nm\n≤ ε\n⇒ m ≥ log .\nε\n\n6.046J/18.410J Quiz 1 Solutions\nName\nProblem 6. Dynamic Programming. [15 points] (2 parts)\nProf. Child is cooking from her garden, which is arranged in grid with n rows and m columns.\nEach cell (i, j) (1 ≤ i ≤ n, 1 ≤ j ≤ m) has an ingredient growing in it, with tastiness given by\na positive value Ti,j . Prof. Child doesn't like cooking \"by the book\". To prepare dinner, she will\nstand at a cell (i, j) and pick one ingredient from each quadrant relative to that cell. The tastiness\nof her dish is the product of the tastiness of the four ingredients she chooses. Help Prof. Child find\nan O(nm) dynamic programming algorithm to maximize the tastiness of her dish.\nHere the four quadrants relative to a cell (i, j) are defined as follows:\ntop-left = {all cells (a, b) | a < i, b < j},\nbottom-left = {all cells (a, b) | a > i, b < j},\ntop-right = {all cells (a, b) | a < i, b > j},\nbottom-right = {all cells (a, b) | a > i, b > j}.\nBecause Prof. Child needs all four quadrants to be non-empty, she can only stand on cells (i, j)\nwhere 1 < i < n and 1 < j < m.\n(a) [10 points] Define TLi,j to be maximum tastiness value in the top-left quadrant of\ncell (i, j): TLi,j = max{Ta,b | 1 ≤ a ≤ i, 1 ≤ b ≤ j}. Find a dynamic programming\nalgorithm to compute TLi,j , for all 1 < i < n and 1 < j < m, in O(nm) time.\nSolution: When trying to calculate TLi,j, we see that the maximum can be at cell\n(i, j). If not, it must lie either in the rectangle from (1, 1) to (i, j - 1), or the rectangle\nfrom (1, 1) to (i - 1, j), or both. These three overlapping cases cover our required\nrectangle. We have then,\nTLi,j = max{Ti,j , TLi-1,j , TLi,j-1}\nFor the base cases, we can just set TL0,j = TLi,0 = 0 for all valid values of i and j.\nWe can compute the DP value for each state in O(1) time. There are nm states, so\nour algorithm is O(nm).\n\n6.046J/18.410J Quiz 1 Solutions\nName\n(b) [5 points] Use the idea in part (a) to obtain an O(nm) algorithm to find the tastiest\ndish.\nSolution: In part (a) we calculated range maximum for the top-left quadrant. We\ncan similarly define range maximums for the other quadrants. Let BLi,j = max{Ta,b |\ni ≤ a ≤ n, 1 ≤ b ≤ j}, TRi,j = max{Ta,b | 1 ≤ a ≤ i, j ≤ b ≤ m}, and\nBRi,j = max{Ta,b | i ≤ a ≤ n, j ≤ b ≤ m}. Each of these can be computed in\nO(nm) time similar to TL.\nTo calculate the tastiest dish Prof. Child can cook when she stands at cell (i, j) (1 <\ni < n and 1 < j < m), we now just need to compute the product TLi-1,j-1BLi+1,j-1TRi-1,j+1BRi+1,j+1\nand pick the maximum product. This can be done in O(nm) time.\n\n6.046J/18.410J Quiz 1 Solutions\nName\nProblem 7. Median of two sorted arrays. [20 points] (3 parts)\nFinding the median of a sorted array is easy: return the middle element. But what if you are given\ntwo sorted arrays A and B, of size m and n respectively, and you want to find the median of all the\nnumbers in A and B? You may assume that A and B are disjoint.\n(a) [3 points] Give a na ıve algorithm running in Θ(m + n) time.\nSolution: Merge the two sorted arrays (which takes O(m + n) time) and find the\nmedian using linear-time selection.\n(b) [10 points] If m = n, give an algorithm that runs in Θ(lg n) time.\nSolution: Pick the median m1 for A and median m2 for B. If m1 = m2, return\nm1. If m1 > m2, remove the second half of A and the first half of B. Then we get\ntwo subarrays with size n/2. Repeat until both arrays are smaller than a constant.\nm1 < m2 is symmetric.\n\n6.046J/18.410J Quiz 1 Solutions\nName\n(c) [7 points] Give an algorithm that runs in O(lg(min{m, n})) time, for any m and n.\nDon't spend too much time on this question!\nSolution: Without loss of generality, assume |A| = m > n = |B|. We can safely\nremove elements A[0 : m-\nn ] and A[m+\nn : m - 1] because none of these elements can\nbe the median of A + B. After this process, we get two arrays of size approximately\nn. Then we can run part (b). The complexity is Θ(lg(min(m, n)))\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Class on Design and Analysis of Algorithms, Quiz 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/cd9e97527ebe84e79211f8f59a75c9e6_MIT6_046JS15_quiz2.pdf",
      "content": "Design and Analysis of Algorithms\nApril 16, 2015\nMassachusetts Institute of Technology\n6.046J/18.410J\nProfs. Erik Demaine, Srini Devadas, and Nancy Lynch\nQuiz 2\nQuiz 2\n- Do not open this quiz booklet until you are directed to do so. Read all the instructions first.\n- The quiz contains 6 problems, with multiple parts. You have 120 minutes to earn 120 points.\n- This quiz booklet contains 10 pages, including this one.\n\"\"\n- This quiz is closed book. You may use two double-sided letter (81\n2 × 11\"\" ) or A4 crib sheets.\nNo calculators or programmable devices are permitted. Cell phones must be put away.\n- Do not waste time deriving facts that we have studied. Just cite results from class.\n- When we ask you to \"give an algorithm\" in this quiz, describe your algorithm in English\nor pseudocode, and provide a short argument for correctness and running time. You do not\nneed to provide a diagram or example unless it helps make your explanation clearer.\n- Do not spend too much time on any one problem. Generally, a problem's point value is an\nindication of how many minutes to spend on it.\n- Show your work, as partial credit will be given. You will be graded not only on the correct\nness of your answer, but also on the clarity with which you express it. Please be neat.\n- Good luck!\nProblem Title\nPoints Parts Grade Initials\nTrue or False\nWho Charged the Electric Car?\nPlanning Ahead\nMaze Marathoner\n6.046 Carpool\nPaths and/or Cycles\nTotal\nName:\n\n6.046J/18.410J Quiz 2\nName\nProblem 1. True or False. [40 points] (10 parts)\nCircle T or F for each of the following statements to indicate whether the statement is true or false\nand briefly explain why.\n(a) T F [4 points]\nUsing similar techniques used in Strassen's matrix multiplication algorithm, the\nFloyd-Warshall algorithm's running time can be improved to O(V log2 7).\n(b) T F [4 points]\nFor graphs G = (V, E) where E = O(V 1.5), Johnson's algorithm is asymptoti\ncally faster than Floyd-Warshall.\n(c) T F [4 points]\nConsider the directed graph where each vertex represents a subproblem in a dy\nnamic program, and there is an edge from p to q if and only if subproblem p\ndepends on (recursively calls) subproblem q. Then this graph is a directed rooted\ntree.\n(d) T F [4 points]\nIn a connected, weighted graph, every lowest weight edge is always in some\nminimum spanning tree.\n(e) T F [4 points]\nFor a connected, weighted graph with n vertices and exactly n edges, it is possible\nto find a minimum spanning tree in O(n) time.\n\n6.046J/18.410J Quiz 2\nName\n(f) T F [4 points]\nFor a flow network with an integer capacity on every edge, the Ford-Fulkerson\nalgorithm runs in time O((V + E) |f|) where |f| is the maximum flow.\n(g) T F [4 points]\nLet C = (S, V \\ S) be a minimum cut in a flow network. If we strictly increase\nthe capacity of every edge across C, then the maximum flow of the network must\nincrease.\n(h) T F [4 points]\nEvery linear program has a unique optimal solution.\nAlternative solution: False. There could be no solutions at all.\nAlternative solution: False. There could be no solutions at all.\n(i) T F [4 points]\n3SAT cannot be solved in polynomial time, even if P = NP.\n(j) T F [4 points]\nRepeatedly selecting a vertex of maximum degree, and deleting the incident\nedges, is a 2-approximation algorithm for Vertex Cover.\n\n6.046J/18.410J Quiz 2\nName\nProblem 2. Who Charged the Electric Car? [20 points] (3 parts)\nProf. Musk is driving his Nikola electric car from Boston to New York. He wants to take the\nshortest path, but his car can only drive m miles before needing to charge. Fortunately, there are\nFuriouscharger charging stations on the way from Boston to New York, which instantaneously\ncharge the battery to full.\nThe road network is given to you as a weighted undirected graph G = (V, E, w) along with the\nsubset C ⊆ V of vertices that have charging stations. Each weight w(e) denotes the (positive)\nlength of road e. The goal is to find a shortest path from node s ∈ V to node t ∈ V that does not\ntravel more than m miles between charging stations. Assume that s, t ∈ C.\n(a) [4 points] Draw the shortest path from Boston to New York in the following graph if\nm = inf. Charging stations are marked as circles.\nNew York\nBoston\n(b) [4 points] Draw the shortest path from Boston to New York in the following (identical)\ngraph if m = 100.\nNew York\nBoston\n(c) [12 points] Give an algorithm to solve the problem. For full credit, your algorithm\nshould run in O(V E + V 2 log V ) time.\n\n6.046J/18.410J Quiz 2\nName\nProblem 3. Planning Ahead [10 points] (1 part)\nYou have N psets due right now, but you haven't started any of them, so they are all going to be\nlate. Each pset requires di days to complete, and has a cost penalty of ci per day. So if pset i ends\nup being finished t days late, then it incurs a penalty of t · ci. Assume that once you start working\non a pset, you must work on it until you finish it, and that you cannot work on multiple psets at the\nsame time.\nFor example, suppose you have three problem sets: 6.003 takes 3 days and has a penalty of 12\npoints/day, 6.046 takes 4 days and has a penalty of 20 points/day, and 6.006 takes 2 days and has\na peanlty of 4 points/day. The best order is then 6.046, 6.003, 6.006 which results in a penalty of\n20 · 4 + 12 · (4 + 3) + 4 · (3 + 4 + 2) = 200 points.\nGive a greedy algorithm that outputs an ordering of the psets that minimizes the total penalty for\nall the psets. Analyze the running time and prove correctness.\n\n6.046J/18.410J Quiz 2\nName\nProblem 4. Maze Marathoner [20 points] (3 parts)\nA group of m teens need to escape a maze, represented by a directed graph G = (V, E). The teens\nall start at a common vertex s ∈ V , and all need to get to the single exit at t ∈ V . Every night, each\nteen can choose to remain where they are, or traverse an edge to a neighboring vertex (which takes\nexactly one night to traverse). However, each edge e ∈ E has an associated capacity c(e), meaning\nthat at most c(e) teens can traverse the edge during the same night. The goal is to minimize the\nnumber of nights required for all teens to escape by reaching the goal t.\n(a) [3 points] First look at the special case where the maze is just a single path of length\n|E| from s to t, and all the edges have capacity 1 (see below). Exactly how many\nnights are required for the teens to escape?\ns\nm\nt\n(b) [7 points] The general case is more complex. Assume for now that we have a \"magic\"\nalgorithm that calculates whether the teens can all escape using ≤ k nights. The magic\nalgorithm runs in polynomial time: kα T (V, E, m) where α = O(1).\nGive an algorithm to calculate the minimum number of nights to escape, by making\ncalls to the magic algorithm. Analyze your time complexity in terms of V , E, m, α,\nand T (V, E, m).\n\n6.046J/18.410J Quiz 2\nName\n(c) [10 points] Now give the \"magic\" algorithm, and analyze its time complexity.\nHint: Transform the problem into a max-flow problem by constructing a graph G \" =\n(V \" , E \" ) where V \" = {(v, i) | v ∈ V, 0 ≤ i ≤ k}. What should E \" be?\n\n6.046J/18.410J Quiz 2\nName\nProblem 5. 6.046 Carpool [10 points] (1 part)\nThe n people in your dorm want to carpool to 34-101 during the m days of 6.046. On day i, some\nsubset Si of people actually want to carpool (i.e., attend lecture), and the driver di must be selected\nfrom Si. Each person j has a limited number of days fj they are willing to drive.\nGive an algorithm to find a driver assignment di ∈ Si for each day i such that no person j has to\ndrive more than their limit fj . (The algorithm should output \"no\" if there is no such assignment.)\nHint: Use network flow.\nFor example, for the following input with n = 3 and m = 3, the algorithm could assign Penny to\nDay 1 and Day 2, and Leonard to Day 3.\nPerson\nDay 1 Day 2 Day 3 Driving limit\n1 (Penny)\nX\nX\nX\n2 (Leonard)\nX\nX\n3 (Sheldon)\nX\nX\n\n6.046J/18.410J Quiz 2\nName\nProblem 6. Paths and/or Cycles [20 points] (2 parts)\nA Hamiltonian path on a directed graph G = (V, E) is a path that visits each vertex in V exactly\nonce. Consider the following variants on Hamiltonian path:\n(a) [10 points] Give a polynomial-time algorithm to determine whether a directed graph\nG contains either a cycle or a Hamiltonian path (or both).\n(b) [10 points] Show that it is NP-hard to decide whether a directed graph G \" contains\nboth a cycle and a Hamiltonian Path, by giving a reduction from the HAMILTONIAN\nPATH problem: given a graph G, decide whether it has a Hamiltonian path. (Recall\nfrom recitation that the HAMILTONIAN PATH problem is NP-complete.)\n\n6.046J/18.410J Quiz 2\nName\nSCRATCH PAPER\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Class on Design and Analysis of Algorithms, Quiz 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/44146f3104fa80a52514265d070ebc40_MIT6_046JS15_quiz2sols.pdf",
      "content": "Design and Analysis of Algorithms\nApril 20, 2015\nMassachusetts Institute of Technology\n6.046J/18.410J\nProfs. Erik Demaine, Srini Devadas, and Nancy Lynch\nQuiz 2 Solutions\nQuiz 2 Solutions\n- Do not open this quiz booklet until you are directed to do so. Read all the instructions first.\n- The quiz contains 6 problems, with multiple parts. You have 120 minutes to earn 120 points.\n- This quiz booklet contains 11 pages, including this one.\n\"\"\n- This quiz is closed book. You may use two double-sided letter (81\n2 × 11\"\" ) or A4 crib sheets.\nNo calculators or programmable devices are permitted. Cell phones must be put away.\n- Do not waste time deriving facts that we have studied. Just cite results from class.\n- When we ask you to \"give an algorithm\" in this quiz, describe your algorithm in English\nor pseudocode, and provide a short argument for correctness and running time. You do not\nneed to provide a diagram or example unless it helps make your explanation clearer.\n- Do not spend too much time on any one problem. Generally, a problem's point value is an\nindication of how many minutes to spend on it.\n- Show your work, as partial credit will be given. You will be graded not only on the correct\nness of your answer, but also on the clarity with which you express it. Please be neat.\n- Good luck!\nProblem Title\nPoints Parts Grade Initials\nTrue or False\nWho Charged the Electric Car?\nPlanning Ahead\nMaze Marathoner\n6.046 Carpool\nPaths and/or Cycles\nTotal\nName:\n\n6.046J/18.410J Quiz 2 Solutions\nName\nProblem 1. True or False. [40 points] (10 parts)\nCircle T or F for each of the following statements to indicate whether the statement is true or false\nand briefly explain why.\n(a) T F [4 points]\nUsing similar techniques used in Strassen's matrix multiplication algorithm, the\nFloyd-Warshall algorithm's running time can be improved to O(V log2 7).\nSolution: False. There is no way to define negation.\n(b) T F [4 points]\nFor graphs G = (V, E) where E = O(V 1.5), Johnson's algorithm is asymptoti\ncally faster than Floyd-Warshall.\nSolution: True. O(V E + V 2 log V ) = o(V 3) when E = o(V 2).\n(c) T F [4 points]\nConsider the directed graph where each vertex represents a subproblem in a dy\nnamic program, and there is an edge from p to q if and only if subproblem p\ndepends on (recursively calls) subproblem q. Then this graph is a directed rooted\ntree.\nSolution: False. It is a Directed Acyclic Graphic (DAG).\n(d) T F [4 points]\nIn a connected, weighted graph, every lowest weight edge is always in some\nminimum spanning tree.\nSolution: True. It can be the first edge added by Kruskal's algorithm.\n(e) T F [4 points]\nFor a connected, weighted graph with n vertices and exactly n edges, it is possible\nto find a minimum spanning tree in O(n) time.\nSolution: True. This graph only contains one cycle, which can be found by a\nDFS. Just remove the heaviest edge in that cycle.\n\n6.046J/18.410J Quiz 2 Solutions\nName\n(f) T F [4 points]\nFor a flow network with an integer capacity on every edge, the Ford-Fulkerson\nalgorithm runs in time O((V + E) |f|) where |f| is the maximum flow.\nSolution: True. There can be O(|f|) iterations because each iteration increases\nthe flow by at least 1.\n(g) T F [4 points]\nLet C = (S, V \\ S) be a minimum cut in a flow network. If we strictly increase\nthe capacity of every edge across C, then the maximum flow of the network must\nincrease.\nSolution: False. There could be another min cut whose capacity does not change.\nThen the max flow remains the same.\n(h) T F [4 points]\nEvery linear program has a unique optimal solution.\nSolution: False. There can be many optimal solutions if the objective function is\nparallel to one of the constrains.\nAlternative solution: False. There could be no solutions at all.\nAlternative solution: False. There could be no solutions at all.\n(i) T F [4 points]\n3SAT cannot be solved in polynomial time, even if P = NP.\nSolution: False. If P = NP, then all problems in P are also NP-hard, and these\nproblems have polynomial-time algorithms.\n(j) T F [4 points]\nRepeatedly selecting a vertex of maximum degree, and deleting the incident\nedges, is a 2-approximation algorithm for Vertex Cover.\nSolution: False: it can be as bad as a log-log approximation, see L17 notes.\n\n6.046J/18.410J Quiz 2 Solutions\nName\nProblem 2. Who Charged the Electric Car? [20 points] (3 parts)\nProf. Musk is driving his Nikola electric car from Boston to New York. He wants to take the\nshortest path, but his car can only drive m miles before needing to charge. Fortunately, there are\nFuriouscharger charging stations on the way from Boston to New York, which instantaneously\ncharge the battery to full.\nThe road network is given to you as a weighted undirected graph G = (V, E, w) along with the\nsubset C ⊆ V of vertices that have charging stations. Each weight w(e) denotes the (positive)\nlength of road e. The goal is to find a shortest path from node s ∈ V to node t ∈ V that does not\ntravel more than m miles between charging stations. Assume that s, t ∈ C.\n(a) [4 points] Draw the shortest path from Boston to New York in the following graph if\nm = inf. Charging stations are marked as circles.\nNew York\nBoston\nSolution:\nNew York\nBoston\n(b) [4 points] Draw the shortest path from Boston to New York in the following (identical)\ngraph if m = 100.\nNew York\nBoston\nSolution:\nNew York\nBoston\n(c) [12 points] Give an algorithm to solve the problem. For full credit, your algorithm\nshould run in O(V E + V 2 log V ) time.\nSolution: Our algorithm consists of two steps - the first step involves running John\nson's algorithm on the original graph G to obtain shortest path lengths for every pair\n\n6.046J/18.410J Quiz 2 Solutions\nName\nof vertices. Let δ(u, v) represent the length of the shortest path between vertices u and\nv in G.\nFor the second step, we build a graph G \" with vertex set C. For every pair of vertices\nu and v in the new graph G \", draw an edge between u and v with weight δ(u, v) if\nδ(u, v) ≤ m and inf otherwise.\nNow, run Dijkstra's algorithm on G \" between Boston and New York to get the shortest\npath. (Note that New York and Boston have charging stations and so are vertices in\nthe graph G \").\nRunning Johnson's algorithm on the original graph G takes O(V E + V 2 log V ). Cre\nating the graph G \" takes O(E) time, and running Dijkstra's algorithm on G \" takes\nO(V 2 + V log V ) time; this gives a total runtime complexity of O(V E + V 2 log V ).\n\n6.046J/18.410J Quiz 2 Solutions\nName\nProblem 3. Planning Ahead [10 points] (1 part)\nYou have N psets due right now, but you haven't started any of them, so they are all going to be\nlate. Each pset requires di days to complete, and has a cost penalty of ci per day. So if pset i ends\nup being finished t days late, then it incurs a penalty of t · ci. Assume that once you start working\non a pset, you must work on it until you finish it, and that you cannot work on multiple psets at the\nsame time.\nFor example, suppose you have three problem sets: 6.003 takes 3 days and has a penalty of 12\npoints/day, 6.046 takes 4 days and has a penalty of 20 points/day, and 6.006 takes 2 days and has\na peanlty of 4 points/day. The best order is then 6.046, 6.003, 6.006 which results in a penalty of\n20 · 4 + 12 · (4 + 3) + 4 · (3 + 4 + 2) = 200 points.\nGive a greedy algorithm that outputs an ordering of the psets that minimizes the total penalty for\nall the psets. Analyze the running time and prove correctness.\nSolution: Sort by increasing di/ci and do the problem sets in that order. This takes O(N log N)\ntime.\nProof - If unsorted, we can improve by swapping.\ndi/ci > dj /cj =⇒ cj di + (cidi + cj dj ) > cidj + (cidi + cj dj)\n(1)\n=⇒ cj (di + dj ) + cidi > ci(di + dj ) + cj dj\n(2)\n\n6.046J/18.410J Quiz 2 Solutions\nName\nProblem 4. Maze Marathoner [20 points] (3 parts)\nA group of m teens need to escape a maze, represented by a directed graph G = (V, E). The teens\nall start at a common vertex s ∈ V , and all need to get to the single exit at t ∈ V . Every night, each\nteen can choose to remain where they are, or traverse an edge to a neighboring vertex (which takes\nexactly one night to traverse). However, each edge e ∈ E has an associated capacity c(e), meaning\nthat at most c(e) teens can traverse the edge during the same night. The goal is to minimize the\nnumber of nights required for all teens to escape by reaching the goal t.\n(a) [3 points] First look at the special case where the maze is just a single path of length\n|E| from s to t, and all the edges have capacity 1 (see below). Exactly how many\nnights are required for the teens to escape?\ns\nm\nt\nSolution: |E| + m - 1 or |V | + m - 2. Em or V m will get partial credits.\n(b) [7 points] The general case is more complex. Assume for now that we have a \"magic\"\nalgorithm that calculates whether the teens can all escape using ≤ k nights. The magic\nalgorithm runs in polynomial time: kα T (V, E, m) where α = O(1).\nGive an algorithm to calculate the minimum number of nights to escape, by making\ncalls to the magic algorithm. Analyze your time complexity in terms of V , E, m, α,\nand T (V, E, m).\nSolution: Do a binary search. A sequential scan will get partial credits.\nThe maximum number of nights can be bounded by O(E+m) (or O(V +m), O(Em))\naccording to part(a). Therefore, we need to run the \"magic\" algorithm O(log(E + m))\ntimes. Each run takes no more than O((E + m)αT (V, E, m)) time. So in total, the\nruntime is O((E + m)α log(E + m)T (V, E, m)).\nCommon mistake 1: runtime O(kα log(E + m)T (V, E, m)). k should not appear in\nthe runtime. You need to find the bound on k.\nCommon mistake 2: using sequential scan runtime O((E + m)αT (V, E, m)). The\nn n\nα\nα+1)\nsummation is calculated incorrectly, should be\ni=1 n = O(n\n\n6.046J/18.410J Quiz 2 Solutions\nName\n(c) [10 points] Now give the \"magic\" algorithm, and analyze its time complexity.\nHint: Transform the problem into a max-flow problem by constructing a graph G \" =\n(V \" , E \" ) where V \" = {(v, i) | v ∈ V, 0 ≤ i ≤ k}. What should E \" be?\nSolution: Model this as a max flow problem. Construct a graph G \" = (V \" , E \" ) where\nV \" = {(v, i) | v ∈ V, 0 ≤ i ≤ k}. For all 0 ≤ i ≤ k-1, connect (v, i) to (v, i+1) with\ncapacity inf (or m); this represents teens can stay at a vertex for the night. For every\nedge (u, v) in the original graph, connect (u, i) to (v, i + 1) with capacity c((u, v));\nthis represents c((u, v)) teens can travel from u to v in a night.\nThe new source s \" is the vertex (s, 0) and the new sink t \" is the vertex (t, k - 1). If the\nmax flow from s \" to t \" is no less than m, then people can escape within k nights.\nRuntime: Both of the following are accepted.\nThere are V = O(kV \" ) vertices and E \" = O(k(V + E)) edges in G \" . Applying\nEdmonds-Karp algorithm, the total time complexity is O(V E \"2) = O(k3V (V + E)2).\nIf using Ford-Fulkerson runtime, notice that we can actually stop if the max flow\nreaches m. So at most m iterations are needed. Runtime can be O(m(V \" + E \" )) =\nO(mk(V + E)).\nCommon mistake 1: connect (v, i) to (u, i) instead of (v, i) to (u, i + 1).\nCommon mistake 2: no edge from (v, i) to (v, i + 1).\nBoth solutions above are equivalent of running max flow on the original graph, be\ncause there could be very long path (> k) with large capacity (> m) in the original\ngraph. In that case, max flow is larger than m, but teens cannot escape in k nights.\n\n6.046J/18.410J Quiz 2 Solutions\nName\nProblem 5. 6.046 Carpool [10 points] (1 part)\nThe n people in your dorm want to carpool to 34-101 during the m days of 6.046. On day i, some\nsubset Si of people actually want to carpool (i.e., attend lecture), and the driver di must be selected\nfrom Si. Each person j has a limited number of days fj they are willing to drive.\nGive an algorithm to find a driver assignment di ∈ Si for each day i such that no person j has to\ndrive more than their limit fj . (The algorithm should output \"no\" if there is no such assignment.)\nHint: Use network flow.\nFor example, for the following input with n = 3 and m = 3, the algorithm could assign Penny to\nDay 1 and Day 2, and Leonard to Day 3.\nPerson\nDay 1 Day 2 Day 3 Driving limit\n1 (Penny)\nX\nX\nX\n2 (Leonard)\nX\nX\n3 (Sheldon)\nX\nX\nSolution: First, we create a graph with following vertices:\n1.a super source s and a super sink t\n2.vertex pi for each person who wants to carpool\n3.vertex dj for each day of the class.\nThen create the following edges:\n1.s to pi with capacity of fj\n2.pi to dj with capacity of 1 if person i needs to carpool on day j\n3.dj to t with weight 1 for all j.\nFinally, run max flow from s to t, and find f. If |f| = m, return that person i will drive on day j if\nthe edge (pi, dj ) has non-zero flow. If |f| < m, then return no valid assignment.\nAt a high level, the graph represents a matching between the driver and the days he/she will be\ndriving. The capacity from s to pi will ensure that no pi drives more than li days (flow conserva\ntion). The non-zero flows from pi to dj means that pi will drive on day dj . The capacity of dj to t\nwill ensure that no more than 1 driver will be assigned to a particular day (flow conservation). If\n|f| = m, then all vertices dj has an incoming flow, and therefore all dj has a valid driver assign\nment. If |f| < m, then there is at least one dj that has no incoming flow, and therefore does not\nhave a driver assigned to it. By maximality of the flow, this means that there does not exist a valid\ndriver assignment.\nFord-Fulkerson algorithm would run in O(nm2) since there are at most nm + 2 edges (bipar\ntite graph), and the max flow is bounded by m. Running Edmonds-Karp would run in O((n +\nm)(nm)2) = O(n3m2 + n2m3), which is slower in this case.\n\n6.046J/18.410J Quiz 2 Solutions\nName\nProblem 6. Paths and/or Cycles [20 points] (2 parts)\nA Hamiltonian path on a directed graph G = (V, E) is a path that visits each vertex in V exactly\nonce. Consider the following variants on Hamiltonian path:\n(a) [10 points] Give a polynomial-time algorithm to determine whether a directed graph\nG contains either a cycle or a Hamiltonian path (or both).\nSolution: To solve the problem, we simply run DFS on G. If a cycle exists, DFS will\ntraverse a vertex twice and can report the cycle. If no cycle exists, then the graph is a\nDAG.\nIf the graph is a DAG, then we can run a topological sort on the graph. If there is a\ncomplete, or unique, ordering of every vertex in the graph, the graph has a Hamiltonian\nPath, and we accept the graph.\n(b) [10 points] Show that it is NP-hard to decide whether a directed graph G \" contains\nboth a cycle and a Hamiltonian Path, by giving a reduction from the HAMILTONIAN\nPATH problem: given a graph G, decide whether it has a Hamiltonian path. (Recall\nfrom recitation that the HAMILTONIAN PATH problem is NP-complete.)\nSolution: We construct a graph G \" = (V \" , E \" ) from G, where\nV \" = {u1, u2, u3} ∪ V\nE \" = {(u1, u2), (u2, u3), (u3, u1)} ∪{(u1, V ) : v ∈ V } ∪ E\nG \" always has a cycle of length 3 - (u1, u2, u3). For any Hamiltonian Path P in G,\n(u2, u3, u1, P ) is a Hamiltonian Path in G \" . For any Hamiltonian Path P \" in G \" , P \"\nmust be of the form (u2, u3, u1, P ), where P is a Hamiltonian path for G. Thus in all\ncases, solving B(G \") is equivalent to solving Hamiltonian Path for G.\nAn alternate solution used the algorithm for cycle and Hamiltonian Path as an oracle.\nAn input graph to the Hamiltonian Path is used as an input to the cycle and Hamil\ntonian Path algorithm. If the cycle and Hamiltonian Path algorithm accepts, then the\noriginal graph is also accepted. However, if the cycle and Hamiltonian path algorithm\nrejects the input, we use DFS to determine if the graph contains a cycle and the cycle\nor Hamiltonian Path algorithm from part a. If DFS cannot find a cycle but the cycle\nor Hamiltonian path algorithm accepts, then we accept the graph, and reject the graph\notherwise.\n\n6.046J/18.410J Quiz 2 Solutions\nName\nSCRATCH PAPER\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Class on Design and Analysis of Algorithms, Lecture 1 Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/ff2a7015ff913fc01f0381d8f5126a9a_MIT6_046JS15_lec01.pdf",
      "content": "Lecture 1\nIntroduction\nSpring 2015\nLecture 1: Introduction\n6.006 pre-requisite:\n- Data structures such as heaps, trees, graphs\n- Algorithms for sorting, shortest paths, graph search, dynamic programming\nCourse Overview\nThis course covers several modules:\n1. Divide and Conquer - FFT, Randomized algorithms\n2. Optimization - greedy and dynamic programming\n3. Network Flow\n4. Intractibility (and dealing with it)\n5. Linear programming\n6. Sublinear algorithms, approximation algorithms\n7. Advanced topics\nTheme of today's lecture\nVery similar problems can have very different complexity. Recall:\n- P: class of problems solvable in polynomial time. O(nk) for some constant k.\nShortest paths in a graph can be found in O(V 2) for example.\n- NP: class of problems verifiable in polynomial time.\nHamiltonian cycle in a directed graph G(V, E) is a simple cycle that contains\neach vertex in V .\nDetermining whether a graph has a hamiltonian cycle is NP-complete but ver\nifying that a cycle is hamiltonian is easy.\n- NP-complete: problem is in NP and is as hard as any problem in NP.\nIf any NPC problem can be solved in polynomial time, then every problem in\nNP has a polynomial time solution.\n6.046J\n\nLecture 1\nIntroduction\nSpring 2015\nInterval Scheduling\nRequests 1, 2, . . . , n, single resource\ns(i) start time, f(i) finish time, s(i) < f(i) (start time must be less than finish\ntime for a request)\nTwo requests i and j are compatible if they don't overlap, i.e., f(i) ≤ s(j) or\nf(j) ≤ s(i).\nIn the figure below, requests 2 and 3 are compatible, and requests 4, 5 and 6 are\ncompatible as well, but requests 2 and 4 are not compatible.\nGoal: Select a compatible subset of requests of maximum size.\nClaim: We can solve this using a greedy algorithm.\nA greedy algorithm is a myopic algorithm that processes the input one piece at a\ntime with no apparent look ahead.\nGreedy Interval Scheduling\n1. Use a simple rule to select a request i.\n2. Reject all requests incompatible with i.\n3. Repeat until all requests are processed.\n6.046J\n\nLecture 1\nIntroduction\nSpring 2015\nPossible rules?\n1. Select request that starts earliest, i.e., minimum s(i).\nLong one is earliest. Bad :(\n2. Select request that is smallest, i.e., minimum f(i) - s(i).\nSmallest. Bad :(\n3. For each request, find number of incompatibles, and select request with mini\nmum such number.\nLeast # of incompatibles. Bad :(\n4. Select request with earliest finish time, i.e., minimum f(i).\nClaim 1. Greedy algorithm outputs a list of intervals\n< s(i1), f(i1) >, < s(i2), f(i2) >, . . . , < s(ik), f(ik) >\nsuch that\ns(i1) < f(i1) ≤ s(i2) < f(i2) ≤ . . . ≤ s(ik) < f(ik)\nProof. Simple proof by contradiction - if f(ij ) > s(ij+1), interval j and j +1 intersect,\nwhich is a contradiction of Step 2 of the algorithm!\nClaim 2. Given list of intervals L, greedy algorithm with earliest finish time produces\nk∗ intervals, where k∗ is optimal.\nProof. Induction on k∗ .\nBase case: k∗ = 1 - this case is easy, any interval works.\nInductive step: Suppose claim holds for k∗ and we are given a list of intervals\nwhose optimal schedule has k∗ + 1 intervals, namely\nS ∗ [1, 2, . . . , k ∗ + 1] =< s(j1), f(j1) >, . . . , < s(jk∗+1), f(jk∗+1) >\n6.046J\n\nLecture 1\nIntroduction\nSpring 2015\nSay for some generic k, the greedy algorithm gives a list of intervals\nS[1, 2, . . . , k] =< s(i1), f(i1) >, . . . , < s(ik), f(ik) >\nBy construction, we know that f(i1) ≤ f(j1), since the greedy algorithm picks the\nearliest finish time.\nNow we can create a schedule\nS ∗∗ =< s(i1), f(i1) >, < s(j2), f(j2) >, . . . , < s(jk∗+1), f(jk∗+1) >\nsince the interval < s(i1), f(i1) > does not overlap with the interval < s(j2), f(j2) >\nand all intervals that come after that. Note that since the length of S∗∗ is k∗ + 1, this\nschedule is also optimal.\nNow we proceed to define L' as the set of intervals with s(i) ≥ f(i1).\nSince S∗∗ is optimal for L, S∗∗[2, 3, . . . , k∗ + 1] is optimal for L', which implies\nthat the optimal schedule for L' has k∗ size.\nWe now see by our initial inductive hypothesis that running the greedy algorithm\non L' should produce a schedule of size k∗ . Hence, by our construction, running the\ngreedy algorithm on L' gives us S[2, . . . , k].\nThis means k - 1 = k∗ or k = k∗ + 1, which implies that S[1, . . . , k] is indeed\noptimal, and we are done.\nWeighted Interval Scheduling\nEach request i has weight w(i). Schedule subset of requests that are non-overlapping\nwith maximum weight.\nA key observation here is that the greedy algorithm no longer works.\nDynamic Programming\nWe can define our sub-problems as\nRx = {j ∈ R|s(j) ≥ x}\nHere, R is the set of all requests.\nIf we set x = f(i), then Rx is the set of requests later than request i.\nTotal number of sub-problems = n (one for each request)\nOnly need to solve each subproblem once and memoize.\nWe try each request i as a possible first. If we pick a request as the first, then the\nremaining requests are Rf (i).\n6.046J\n\nLecture 1\nIntroduction\nSpring 2015\nNote that even though there may be requests compatible with i that are not in\nRf(i), we are picking i as the first request, i.e., we are going in order.\nopt(R) = max (w(i) + opt(Rf (i)))\n1≤i≤n\nTotal running time is O(n2) since we need O(n) time to solve each sub-problem.\nTurns out that we can actually reduce the overall complexity to O(n log n). We\nleave this as an exercise.\nNon-identical machines\nAs before, we have n requests {1, 2, . . . , n}. Each request i is associated with a start\ntime s(i) and finish time f(i), m different machine types as well τ = {T1, . . . , Tm}.\nEach request i is associated with a set Q(i) ⊆ τ that represents the set of machines\nthat request i can be serviced on.\nEach request has a weight of 1. We want to maximize the number of jobs that\ncan be scheduled on the m machines.\nThis problem is in NP, since we can clearly check that a given subset of jobs with\nmachine assignments is legal.\nCan k ≤ n requests be scheduled? This problem is NP-complete.\nMaximum number of requests that should be scheduled? This problem is NP-hard.\nDealing with intractability\n1. Approximation algorithms: Guarantee within some factor of optimal in poly\nnomial time.\n2. Pruning heuristics to reduce (possible exponential) runtime on \"real-world\"\nexamples.\n3. Greedy or other sub-optimal heuristics that work well in practice but provide\nno guarantees.\n6.046J\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Class on Design and Analysis of Algorithms, Lecture 1 Notes, Handwritten",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/009c51db8900141fb181971f2bb826f3_MIT6_046JS15_writtenlec1.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Class on Design and Analysis of Algorithms, Lecture 2 Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/7463c413c944ed72b46a3c3d02b49448_MIT6_046JS15_lec02.pdf",
      "content": "Lecture 2\nDivide and Conquer\nSpring 2015\nLecture 2: Divide and Conquer\n- Paradigm\n- Convex Hull\n- Median finding\nParadigm\nGiven a problem of size n divide it into subproblems of size n\nb , a ≥ 1, b > 1. Solve each\nsubproblem recursively. Combine solutions of subproblems to get overall solution.\nn\nT(n) = aT( ) + [work for merge]\nb\nConvex Hull\nGiven n points in plane\nS = {(xi, yi)|i = 1, 2, . . . , n}\nassume no two have same x coordinate, no two have same y coordinate, and no\nthree in a line for convenience.\nConvex Hull ( CH(S) ): smallest polygon containing all points in S.\np\nq\nr\ns\nt\nu\nv\nCH(S) represented by the sequence of points on the boundary in order clockwise\nas doubly linked list.\n6.046J\n\nLecture 2\nDivide and Conquer\nSpring 2015\np\nq\nr\ns\nt\nBrute force for Convex Hull\nTest each line segment to see if it makes up an edge of the convex hull\n- If the rest of the points are on one side of the segment, the segment is on the\nconvex hull.\n- else the segment is not.\nO(n2) edges, O(n) tests ⇒ O(n3) complexity\nCan we do better?\nDivide and Conquer Convex Hull\nSort points by x coord (once and for all, O(n log n))\nFor input set S of points:\n- Divide into left half A and right half B by x coords\n- Compute CH(A) and CH(B)\n- Combine CH's of two halves (merge step)\nHow to Merge?\nL\nA\nB\na1\na2\na3\na4\na5\nb1\nb2\nb3\n- Find upper tangent (ai, bj ). In example, (a4, b2) is U.T.\n- Find lower tangent (ak, bm). In example, (a3, b3) is L.T.\n6.046J\n\nLecture 2\nDivide and Conquer\nSpring 2015\n- Cut and paste in time Θ(n).\nFirst link ai to bj , go down b ilst till you see bm and link bm to ak, continue along\nthe a list until you return to ai. In the example, this gives (a4, b2, b3, a3).\nFinding Tangents\nAssume ai maximizes x within CH(A) (a1, a2, . . . , ap). b1 minimizes x within CH(B)\n(b1, b2, . . . , bq)\nL is the vertical line separating A and B. Define y(i, j) as y-coordinate of inter\nsection between L and segment (ai, bj ).\nClaim: (ai, bj ) is uppertangent iff it maximizes y(i, j).\nIf y(i, j) is not maximum, there will be points on both sides of (ai, bj ) and it\ncannot be a tangent.\nAlgorithm: Obvious O(n2) algorithm looks at all ai, bj pairs. T(n) = 2T(n/2) +\nΘ(n2) = Θ(n2).\ni = 1\n\nj = 1\n\n3 while (y(i, j + 1) > y(i, j) or y(i - 1, j) > y(i, j))\nif (y(i, j + 1) > y(i, j)) [ move right finger clockwise\nj = j + 1( mod q)\nelse\ni = i - 1( mod p) [ move left finger anti-clockwise\nreturn\n\n(ai, bj ) as upper tangent\nSimilarly for lower tangent.\nn\nT(n) = 2T( ) + Θ(n) = Θ(n log n)\nIntuition for why Merge works\nap-1\nb3\nb4\nb2\nap\nb1\na2\na1\nbq\nbq-1\n6.046J\n\nLecture 2\nDivide and Conquer\nSpring 2015\na1, b1 are right most and left most points. We move anti clockwise from a1,\nclockwise from b1. a1, a2, . . . , aq is a convex hull, as is b1, b2, . . . , bq. If ai, bj is such\nthat moving from either ai or bj decreases y(i, j) there are no points above the (ai, bj )\nline.\nThe formal proof is quite involved and won't be covered.\nMedian Finding\nGiven set of n numbers, define rank(x) as number of numbers in the set that are ≤ x.\nFind element of rank ln+1 J (lower median) and I n+1 l (upper median).\nClearly, sorting works in time Θ(n log n).\nCan we do better?\nB\nx\nC\nk - 1 elements\nh - k elements\nSelect(S, i)\nPick\n\nx ∈ S [ cleverly\n2 Compute k = rank(x)\n3 B = {y ∈ S|y < x}\n4 C = {y ∈ S|y > x}\n5 if k = i\nreturn\n\nx\n\n7 else if k > i\nreturn Select(B, i)\n9 else if k < i\nreturn Select(C, i - k)\nPicking x Cleverly\nNeed to pick x so rank(x) is not extreme.\n- Arrange S into columns of size 5 (I n\n5 l cols)\n- Sort each column (bigger elements on top) (linear time)\n- Find \"median of medians\" as x\n6.046J\n\nLecture 2\nDivide and Conquer\nSpring 2015\nx\nmedians\nlarger\nsmaller\n> x\n< x\nHow many elements are guaranteed to be > x?\nHalf of the I n\n5 l groups contribute at least 3 elements > x except for 1 group with\nless than 5 elements and 1 group that contains x.\nAt lease 3(I n l -2) elements are > x, and at least 3(I n l -2) elements are < x\nRecurrence:\n\nO(1),\nfor n ≤140\nT (n) =\n(1)\nT (I n\n5 l) + T ( 7\nn + 6), Θ(n), for n > 140\nSolving the Recurrence\nMaster theorem does not apply. Intuition n\n5 + 7\nn < n.\nProve T (n) ≤cn by induction, for some large enough c.\nTrue for n ≤140 by choosing large c\nn\n7n\nT (n) ≤cI l + c(\n+ 6) + an\n(2)\ncn\n7nc\n≤\n+ c +\n+ 6c + an\n(3)\ncn\n= cn + (-\n+ 7c + an)\n(4)\nIf c ≥ 70\nn\nc + 10a, we are done. This is true for n ≥140 and c ≥20a.\n6.046J\n\nLecture 2\nDivide and Conquer\nSpring 2015\nAppendix 1\nExample\nL\na1\na2\na3\nb1\nb2\nb3\na4\nb4\na3, b1 is upper tangent. a4 > a3, b2 > b1 in terms of Y coordinates.\na1, b3 is lower tangent, a2 < a1, b4 < b3 in terms of Y coordinates.\nai, bj is an upper tangent. Does not mean that ai or bj is the highest point.\nSimilarly, for lower tangent.\n6.046J\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Class on Design and Analysis of Algorithms, Lecture 2 Notes, Handwritten",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/5c6cf10a900ea288d97dce247a9a5a9d_MIT6_046JS15_writtenlec2.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Class on Design and Analysis of Algorithms, Lecture 3 Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/3dc84e78914f226665357234d6f4e25c_MIT6_046JS15_lec03.pdf",
      "content": "Lecture 3\nFast Fourier Transform\nSpring 2015\nLecture 3: Divide and Conquer:\nFast Fourier Transform\n- Polynomial Operations vs. Representations\n- Divide and Conquer Algorithm\n- Collapsing Samples / Roots of Unity\n- FFT, IFFT, and Polynomial Multiplication\nPolynomial operations and representation\nA polynomial A(x) can be written in the following forms:\nn-1\nA(x) = a0 + a1x + a2x + · · · + an-1x\nn1\n\nk\n=\nakx\nk=0\n= (a0, a1, a2, . . . , an-1) (coefficient vector)\nThe degree of A is n -1.\nOperations on polynomials\nThere are three primary operations for polynomials.\n1. Evaluation: Given a polynomial A(x) and a number x0, compute A(x0). This\ncan be done in O(n) time using O(n) arithmetic operations via Horner's rule.\n- Horner's Rule: A(x) = a0 +x(a1 + x(a2 +· · · x(an-1) · · · )). At each step,\na sum is evaluated, then multiplied by x, before beginning the next step.\nThus O(n) multiplications and O(n) additions are required.\n2. Addition: Given two polynomials A(x) and B(x), compute C(x) = A(x) +\nB(x) (∀x). This takes O(n) time using basic arithmetic, because ck = ak + bk.\n6.046J\n\nLecture 3\nFast Fourier Transform\nSpring 2015\n3. Multiplication: Given two polynomials A(x) and B(x), compute C(x) =\nk\nA(x)·B(x) (∀x). Then ck =\nj=0 aj bk-j for 0 ≤k ≤2(n-1), because the degree\nof the resulting polynomial is twice that of A or B. This multiplication is then\nequivalent to a convolution of the vectors A and reverse(B). The convolution\nis the inner product of all relative shifts, an operation also useful for smoothing\netc. in digital signal processing.\n- Naive polynomial multiplication takes O(n2).\n- O(nlg 3) or even O(n1+ε) (∀ε > 0) is possible via Strassen-like divide-and\nconquer tricks.\n- Today, we will compute the product in O(n lg n) time via Fast Fourier\nTransform!\nRepresentations of polynomials\nFirst, consider the different representations of polynomials, and the time necessary\nto complete operations based on the representation.\nThere are 3 main representations to consider.\n1. Coefficient vector with a monomial basis\n2. Roots and a scale term\n- A(x) = (x -r0) · (x -r1) · · · · · (x -rn-1) · c\n- However, it is impossible to find exact roots with only basic arithmetic\noperations and kth root operations. Furthermore, addition is extremely\nhard with this representation, or even impossible. Multiplication simply\nrequires roots to be concatenated, and evaluation can be completed in\nO(n).\n3. Samples: (x0, y0), (x1, y1), . . . , (xn-1, yn-1) with A(xi) = yi (∀i) and each xi\nis distinct. These samples uniquely determine a degree n -1 polynomial A,\naccording to the Lagrange and Fundamental Theorem of Algebra. Addition\nand multiplication can be computed by adding and multiplying the yi terms,\nassuming that the xi's match. However, evaluation requires interpolation.\nThe runtimes for the representations and the operations is described in the table\nbelow, with algorithms for the operations versus the representations.\n6.046J\n\nLecture 3\nFast Fourier Transform\n6.046J Spring 2015\nAlgorithms vs.\nRepresentations\nCoefficients Roots Samples\nEvaluation\nO(n)\nO(n)\nO(n2)\nAddition\nO(n)\ninf\nO(n)\nMultiplication\nO(n2)\nO(n)\nO(n)\nWe combine the best of each representation by converting between coefficients and\nsamples in O(n lg n) time.\nHow? Consider the polynomial in matrix form.\n\n⎡1\nx\nn\nx2\n\n· · · x 0\n-⎤⎡a0\n\nn\n⎤\n\n⎡y0\n\n⎤\n\n⎢ 1\nx1\nx1\n· · · x 1\n-\n\n⎥ ⎢ a1 ⎥\n⎢ y\n\n⎢\n\n1 ⎥\n\nV · A =\n⎢\n\nx2\nx2\n\n· · · x n-1\n⎢\n⎢\n⎥⎢\n⎥\na2\n⎥\n⎢\n⎥\n\n⎥\n=\n\n⎢\ny\n⎥\n⎢\n\n⎢ .\n\n.\n.\n.\n.\n⎥\n⎢\n⎥\n\n.\n.\n\n⎢\n. .\n\n⎢\n.\n.\n\n⎥\n.\n.\n.\n.\n.\n.\n.\n.\n⎥⎢\n⎥\n⎢\n⎥\n⎥⎢\n⎥\n⎢\n⎥ ⎢ . ⎥\n⎢ .\n⎥⎥\n⎣\n\n1 xn\nn-1 · · ·\n-1 x2\nx n\nn\n-\n⎦⎣\na\n⎦\n⎣\n⎦\n-1\nn-1\nyn-1\nwhere V is the Vandermonde matrix with entries vjk = xk\nj.\nThen we can convert between coefficients and samples using the matrix vector\nproduct V · A, which is equivalent to evaluation. This takes O(n2).\nSimilarly, we can samples to coefficients by solving V \\Y (in MATLAB (r)notation).\nThis takes O(n3) via Gaussian elimination, or O(n2) to compute A = V -1 · Y , if V -1\nis precomputed.\nTo do better than Θ(n2) when converting between coefficients and samples, and\nvice versa, we need to choose special values for x0, x1, . . . , xn\n1. Thus far, we have\n-\nonly made the assumption that the xi values are distinct.\nDivide and Conquer Algorithm\nWe can formulate polynomial multiplication as a divide and conquer algorithm with\nthe following steps for a polynomial A(x) ∀ x ∈ X.\n1. Divide the polynomial A into its even and odd coefficients:\n1 n\n2 -1\n\nX1\n\nk\nAeven(x) =\na2kx = (a0, a2, a4, . . .\nk=0\n)\nl n -1\nJ\nA\nk\nodd(x) =\nX\n\na2k+1x = (a1, a3, a5, . . .\nk=0\n)\n\nLecture 3\nFast Fourier Transform\nSpring 2015\n2. Recursively conquer Aeven(y) for y ∈ X2 and Aodd(y) for y ∈ X2, where X2 =\n{x2 | x ∈ X}.\n3. Combine the terms. A(x) = Aeven(x2) + x · Aodd(x2) for x ∈ X.\nHowever, the recurrences for this algorithm is\n(\n)\nn\nT(n, |X|) = 2 · T\n, |X|\n+ O(n + |X|)\n= O(n 2)\nwhich is no better than before.\nWe can do better if X is collapsing: either |X| = 1 (base case), or |X2| = |X| and\nX2 is (recursively) collapsing. Then the recurrence is of the form\n( )\nn\nT(n) = 2 · T\n+ O(n) = O(n lg n).\nRoots of Unity\nCollapsing sets can be constructed via square roots. Each of the following collapsing\nsets is computing by taking all square roots of the previous set.\n1. {1}\n2. {1, -1}\n3. {1, -1, i, -i}\n√\n√\n4. {1, -1, ± 2 (1 + i), ± 2 (-1 + i)}, which lie on a unit circle\nWe can repeat this process and make our set larger and larger by finding more and\nmore points on this circle. These points are called the nth roots of unity. Formally,\nthe nth roots of unity are n x's such that xn = 1. These points are uniformly spaced\naround the unit circle in the complex plane (including 1). These points are of the\nform (cos θ, sin θ) = cos θ+i sin θ = eiθ by Euler's Formula, for θ = 0, 1 τ, 2 τ, . . . , n-1 τ\nn\nn\nn\n(where τ = 2π).\niθ)2\ni(2θ)\nThe nth roots of unity where n = 2£ form a collapsing set, because (e\n= e\n=\ni(2θ mod τ )\nn\ne\n. Therefore the even nth roots of unity are equivalent to the 2 nd roots of\nunity.\n6.046J\n\nLecture 3\nFast Fourier Transform\nSpring 2015\nFFT, IFFT, and Polynomial Multiplication\nWe can take advantage of the nth roots of unity to improve the runtime of our\npolynomial multiplication algorithm. The basis for the algorithm is called the Discrete\nFourier Transform (DFT).\nThe DFT allows the transformation between coefficients and samples, computing\nA → A∗ = V · A for xk = eiτk/n where n = 2£, where A is the set of coefficients and\nA∗\n∗\nn-1 iτjk/n\nis the resulting samples. The individual terms a =\ne\n· aj .\nj\nj=0\nFast Fourier Transform (FFT)\nThe FFT algorithm is an O(n lg n) divide and conquer algorithm for DFT, used by\nGauss circa 1805, and popularized by Cooley and Turkey and 1965. Gauss used the\nalgorithm to determine periodic asteroid orbits, while Cooley and Turkey used it to\ndetect Soviet nuclear tests from offshore readings.\nA practical implementation of FFT is FFTW, which was described by Frigo and\nJohnson at MIT. The algorithm is often implemented directly in hardware, for fixed\nn.\nInverse Discrete Fourier Transform\nThe Inverse Discrete Fourier Transform is an algorithm to return the coefficients\nof a polynomial from the multiplied samples. The transformation is of the form\nA∗ → V -1 · A∗ = A.\nIn order to compute this, we need to find V -1 , which in fact has a very nice\nstructure.\n\nClaim 1. V -1 = n\n1 V , where V is the complex conjugate of V . 1\n1Recall the complex conjugate of p + qi is p - qi.\n6.046J\nP\n\nLecture 3\nFast Fourier Transform\nSpring 2015\nProof. We claim that P = V · V = nI:\n\npjk = (row j of V ) · (col. k of V )\nn-1\n=\neijτm/neikτm/n\nm=0\nn-1\nijτm/n -ikτm/n\n=\ne\ne\nm=0\nn-1\ni(j-k)τm/n\n=\ne\nm=0\nn-1\nNow if j = k, pjk =\nm=0 = n. Otherwise it forms a geometric series.\nn-1\ni(j-k)τ/n)m\npjk = =\n(e\nm=0\niτ(j-k)/n)n - 1\n(e\n= eiτ (j-k)/n - 1\n= 0\nbecause eiτ = 1. Thus V -1 = n\n1 V , because V · V = nI.\nThis claim says that the Inverse Discrete Fourier Transform is equivalent to the\nikτ/n\nDiscrete Fourier Transform, but changing xk from e\nto its complex conjugate\ne-ikτ/n, and dividing the resulting vector by n. The algorithm for IFFT is analogous\nto that for FFT, and the result is an O(n lg n) algorithm for IDFT.\nFast Polynomial Multiplication\nIn order to compute the product of two polynomials A and B, we can perform the\nfollowing steps.\n1. Compute A∗ = FFT(A) and B∗ = FFT(B), which converts both A and B\nfrom coefficient vectors to a sample representation.\n2. Compute C∗ = A∗ · B∗ in sample representation in linear time by calculating\nC∗ = A∗ · B∗ (∀k).\nk\nk\nk\n3. Compute C = IFFT(C∗), which is a vector representation of our final solution.\n6.046J\nX\nX\nX\nP\nX\n\nLecture 3\nFast Fourier Transform\nSpring 2015\nApplications\nFourier (frequency) space many applications. The polynomial A∗ = FFT(A) is com\nplex, and the amplitude |a ∗| represents the amplitude of the frequency-k signal, while\nk\narg(ak\n∗ ) (the angle of the 2D vector) represents the phase shift of that signal. For ex\nample, this perspective is particularly useful for audio processing, as used by Adobe\nAudition, Audacity, etc.:\n- High-pass filters zero out high frequencies\n- Low-pass filters zero out low frequencies\n- Pitch shifts shift the frequency vector\n- Used in MP3 compression, etc.\n6.046J\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Class on Design and Analysis of Algorithms, Lecture 3 Notes, Handwritten",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/da0bca8839760018d580ccab98f3e3e1_MIT6_046JS15_writtenlec3.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Class on Design and Analysis of Algorithms, Lecture 4 Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/49c8fa24dffce58052c90d46ac800387_MIT6_046JS15_lec04.pdf",
      "content": "Lecture 4\nvan Emde Boas\nSpring 2015\nLecture 4: Divide and Conquer:\nvan Emde Boas Trees\n- Series of Improved Data Structures\n- Insert, Successor\n- Delete\n- Space\nThis lecture is based on personal communication with Michael Bender, 2001.\nGoal\nWe want to maintain n elements in the range {0, 1, 2, . . . , u - 1} and perform Insert,\nDelete and Successor operations in O(log log u) time.\n- If n = nO(1) or n(log n)O(1), then we have O(log log n) time operations\n- Exponentially faster than Balanced Binary Search Trees\n- Cooler queries than hashing\n- Application: Network Routing Tables\n- u = Range of IP Addresses → port to send\n(u = 232 in IPv4)\nWhere might the O(log log u) bound arise ?\n- Binary search over O(log u) elements\n- Recurrences\n\nlog u\n- T(log u) = T\n+ O(1)\n√\n- T(u) = T( u) + O(1)\nImprovements\nWe will develop the van Emde Boas data structure by a series of improvements on\na very simple data structure.\n6.046J\n\nLecture 4\nvan Emde Boas\nSpring 2015\nBit Vector\nWe maintain a vector V of size u such that V[x] = 1 if and only if x is in the set.\nNow, inserts and deletes can be performed by just flipping the corresponding bit\nin the vector. However, successor/predecessor requires us to traverse through the\nvector to find the next 1-bit.\n- Insert/Delete: O(1)\n- Successor/Predecessor: O(u)\nFigure 1: Bit vector for u = 16. THe current set is {1, 9, 10, 15}.\nSplit Universe into Clusters\n√\nWe can improve performance by splitting up the range {0, 1, 2, . . . , u - 1} into\nu\n√\n√\nclusters of size\nu. If x = i\nu + j, then V[x] = V.Cluster[i][j].\n√\nlow(x) = x mod\nu = j\n\nx\nhigh(x) =\n√\n= i\nu\n√\nindex(i, j) = i\nu + j\nV.Cluster[0]\nV.Cluster[1]\nV.Cluster[2]\nV.Cluster[3]\n√\nFigure 2: Bit vector (u = 16) split into\n16 = 4 clusters of size 4.\n- Insert:\n- Set V.cluster[high(x)][low(x)] = 1\nO(1)\n6.046J\n\nLecture 4\nvan Emde Boas\nSpring 2015\n- Mark cluster high(x) as non-empty\nO(1)\n- Successor:\n√\n- Look within cluster high(x)\nO( u)\n√\n- Else, find next non-empty cluster i\nO( u)\n√\n- Find minimum entry j in that cluster\nO( u)\n√\n- Return index(i, j)\nTotal = O( u)\nRecurse\n√\nThe three operations in Successor are also Successor calls to vectors of size\nu. We\ncan use recursion to speed things up.\n√\n√\n- V.cluster[i] is a size-\nu van Emde Boas structure (∀ 0 ≤ i <\nu)\n√\n- V.summary is a size-\nu van Emde Boas structure\n- V.summary[i] indicates whether V.cluster[i] is nonempty\nINSERT(V, x)\nInsert(V.cluster[high(x)], low[x])\nInsert(V.summary, high[x])\nSo, we get the recurrence:\n√\nT(u) = 2T( u) + O(1)\n\nlog u\nT'(log u) = 2T'\n+ O(1)\n=⇒ T(u) = T'(log u) = O(log u)\nSUCCESSOR(V, x)\n1 i = high(x)\n2 j = Successor(V.cluster[i], j)\n3 if j == inf\ni = Successor(V.summary, i)\nj = Successor(V.cluster[i], -inf)\n6 return index(i, j)\n6.046J\n\nLecture 4\nvan Emde Boas\nSpring 2015\n√\nT(u) = 3T( u) + O(1)\nlog u\nT ' (log u) = 3T '\n+ O(1)\n(log u) = O((log u)log 3) ≈O((log u)1.585)\n=⇒ T(u) = T '\nTo obtain the O(log log u) running time, we need to reduce the number of re\ncursions to one.\nMaintain Min and Max\nWe store the minimum and maximum entry in each structure. This gives an O(1)\ntime overhead for each Insert operation.\nSUCCESSOR(V, x)\n1 i = high(x)\n2 if low(x) < V.cluster[i].max\nj = Successor(V.cluster[i], low(x))\n4 else i = Successor(V.summary, high(x))\nj = V.cluster[i].min\n6 return index(i, j)\n√\nT(u) = T( u) + O(1)\n=⇒ T(u) = O(log log u)\nDon't store Min recursively\nThe Successor call now needs to check for the min separately.\nif x < V.min : return V.min\n(1)\n6.046J\n\nLecture 4\nvan Emde Boas\nSpring 2015\nINSERT(V, x)\n1 if V.min == None\nV.min = V.max = x\nI O(1) time\nreturn\n4 if x < V.min\nswap(x ↔ V.min)\n6 if x > V.max\nV.max = x)\n8 if V.cluster[high(x) == None\nInsert(V.summary, high(x))\nI First Call\nInsert(V.cluster[high(x)], low(x))\nI Second Call\nIf the first call is executed, the second call only takes O(1) time. So\n√\nT(u) = T( u) + O(1)\n=⇒ T(u) = O(log log u)\nDELETE(V, x)\n1 if x == V.min\nI Find new min\ni = V.summary.min\nif i = None\nV.min = V.max = None\nI O(1) time\nreturn\nV.min = index(i, V.cluster[i].min)\nI Unstore new min\n7 Delete(V.cluster[high(x)], low(x))\nI First Call\n8 if V.cluster[high(x)].min == None\nDelete(V.summary, high(x))\nI Second Call\n10 I Now we update V.max\n11 if x == V.max\n12 if V.summary.max = None\n13 else\ni = V.summary.max\nV.max = index(i, V.cluster[i].max)\nIf the second call is executed, the first call only takes O(1) time. So\n√\nT(u) = T( u) + O(1)\n=⇒ T(u) = O(log log u)\n6.046J\n\nLecture 4\nvan Emde Boas\nSpring 2015\nLower Bound [Patrascu & Thorup 2007]\nEven for static queries (no Insert/Delete)\n- Ω(log log u) time per query for u = n(log n)O(1)\n- O(n · poly(log n)) space\nSpace Improvements\nWe can improve from Θ(u) to O(n log log u).\n- Only create nonempty clusters\n- If V.min becomes None, deallocate V\n- Store V.cluster as a hashtable of nonempty clusters\n- Each insert may create a new structure Θ(log log u) times (each empty insert)\n- Can actually happen [Vladimir ˇ\nat]\nCun\n- Charge pointer to structure (and associated hash table entry) to the structure\nThis gives us O(n log log u) space (but randomized).\nIndirection\nWe can further reduce to O(n) space.\n- Store vEB structure with n = O(log log u) using BST or even an array\n=⇒O(log log n) time once in base case\n- We use O(n/ log log u) such structures (disjoint)\nn\n=⇒O(\n· log log u) = O(n) space for small\nlog log u\n- Larger structures \"store\" pointers to them\nn\n=⇒O(\n· log log u) = O(n) space for large\nlog log u\n- Details: Split/Merge small structures\n6.046J\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Class on Design and Analysis of Algorithms, Lecture 4 Notes, Handwritten",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/a10b0c190edb4ebc21e732a62c4f3767_MIT6_046JS15_writtenlec4.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Class on Design and Analysis of Algorithms, Lecture 5 Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/13e2c7d165259712327af0af312a068e_MIT6_046JS15_lec05.pdf",
      "content": "Lecture 5\nAmortization\nSpring 2015\nLecture 5: Amortization\nAmortized analysis is a powerful technique for data structure analysis, involving\nthe total runtime of a sequence of operations, which is often what we really care\nabout. This lecture covers:\n- Different techniques of amortized analysis\n- aggregate method\n- accounting method\n- charging method\n- potential method\n- Examples of amortized analysis\n- table doubling\n- binary counter\n- 2-3 tree and 2-5 tree\nTable doubling\n(Recall from 6.006) We want to store n elements in a table of size m = Θ(n). One\nidea is to double m whenever n becomes larger than m (due to insertions). The cost\nto double a table of size m is clearly Θ(m) = Θ(n), which is also the worse case cost\nof an insertion.\nBut what is the total cost of n insertions? It is at most\n20 + 21 + 22 + · · · + 2lg n = Θ(n).\nIn this case, we say each insertion has Θ(n)/n = Θ(1) amortized cost.\nAggregate Method\nThe method we used in the above analysis is the aggregate method: just add up the\ncost of all the operations and then divide by the number of operations.\ntotal cost of k operations\namortized cost per operation =\nk\nAggregate method is the simplest method. Because it's simple, it may not be able\nto analyze more complicated algorithms.\n6.046J\n\nLecture 5\nAmortization\nSpring 2015\nAmortized Bound Definition\nAmortized cost can be, but does not have to be, average cost. We can assign any\namortized cost to each operation, as long as they \"preserve the total cost\", i.e., for\nany sequence of operations,\namortized cost ≥\nactual cost\nwhere the sum is taken over all operations.\nFor example, we can say a 2-3 tree achieves O(1) amortized cost per create, O(lg n ∗)\namortized cost per insert, and 0 amortized cost per delete, where n ∗ is the maximum\nsize of the 2-3 tree during the entire sequence of operations. The reason we can claim\nthis is that for any sequence of operations, suppose there are c creations, i insertions\nand d ≤ i deletions (cannot delete from an empty tree), the total amortized cost is\nasymptotically the same as the total actual cost:\nO(c + i lg n ∗ + 0d) = O(c + i lg n ∗ + d lg n ∗ )\nLater, we will tighten the amortized cost per insert to O(lg n) where n is the\ncurrent size.\nAccounting Method\nThis method allows an operation to store credit into a bank for future use, if its\nassigned amortized cost > its actual cost; it also allows an operation to pay for its\nextra actual cost using existing credit, if its assigned amortized cost < its actual cost.\nTable doubling\nFor example, in table doubling:\n- if an insertion does not trigger table doubling, store a coin represnting c = O(1)\nwork for future use.\n- if an insertion does trigger table doubling, there must be n/2 elements that are\ninserted after the previous table doubling, whose coins have not been consumed.\nUse up these n/2 coins to pay for the O(n) table doubling. See figure below.\n- amortized cost for table doubling: O(n) - c · n/2 = 0 for large enough c.\n- amortized cost per insertion: 1 + c = O(1).\n6.046J\n\nLecture 5\nAmortization\nSpring 2015\nan element\na unused coin\ntable doubling due to the next insert\n2-3 trees\nNow let's try the accounting method on 2-3 trees. Our goal is to show that insert has\nO(lg n) amortized cost and delete has 0 amortized cost. Let's try a natural approach:\nsave a O(lg n) coin for inserting an element, and use this coin when we delete this\nelement later. However, we will run into a problem: by the time we delete the element,\nthe size of the tree may have got bigger n' > n, and the coin we saved is not enough\nto pay for the lg n' actual cost of that delete operation! This problem can be solved\nusing the charging method in the next section.\nCharging Method\nThe charging method allows operations to charge cost retroactively to past operations.\namortized cost of an operation = actual cost of this operation\n- total cost charged to past operations\n+ total cost charged by future operations\nTable doubling and halving\nFor example, in table doubling, when the table doubles from m to 2m, we can charge\nΘ(m) cost to the m/2 insert operations since the last doubling. Each insert is charged\nby Θ(1), and will not be charged again. So the amortized cost per insert is Θ(1).\nNow let's extend the above example with table halving. The motivation is to save\nspace when with deletes. If the table is down to 1/4 full, n = m/4, we shrink the\ntable size from m to m/2 at Θ(m) cost. This way, the table is half full again after\nany resize (doubling or shrinking). Now each table doubling still has ≥ m/2 insert\noperations to charge to, and each table halving has ≥ m/4 delete operations to charge\nto. So the amortized cost per insert or delete is still Θ(1).\n6.046J\n\nLecture 5\nAmortization\nSpring 2015\nFree deletion in 2-3 trees\nFor another example, let's consider insertion and deletion in 2-3 trees. Again, our\ngoal is to show that insert has O(lg n) amortized cost, where n is the size of the tree\nwhen that insert happens, and delete has 0 amortized cost.\nInsert does not need to charge anything.\nDelete will charge an insert operation. But we will not charge the insert of the\nelement to be deleted, because we will run into the same problem as the accounting\nmethod. Instead, each delete operation will charge the insert operation that brought\nthe tree to its current size n. Each insert is still charged at most once, because for\nthe tree size to reach n again, another insert must happen.\nPotential Method\nThis method defines a potential function Φ that maps a data structure (DS) configu\nration to a value. This function Φ is equivalent to the total unused credits stored up\nby all past operations (the bank account balance). Now\namortized cost of an operation = actual cost of this operation + ΔΦ\nand\namortized cost =\nactual cost + Φ(final DS) - Φ(initial DS).\nIn order for the amortized bound to hold, Φ should never go below Φ(initial DS)\nat any point. If Φ(initial DS) = 0, which is usually the case, then Φ should never go\nnegative (intuitively, we cannot \"owe the bank\").\nRelation to accounting method\nIn accounting method, we specify ΔΦ, while in potential method, we specify Φ. One\ndetermines the other, so the two methods are equivalent. But sometimes one is more\nintuitive than the other.\nBinary counter\nOur first example of potential method is incrementing a binary counter. E.g.,\nincrement\n↓\n6.046J\n\nLecture 5\nAmortization\nSpring 2015\nCost of increment is Θ(1 + #1), where #1 represents the number of trailing 1 bits.\nSo the intuition is that 1 bits are bad.\nDefine Φ = c · #1. Then for large enough c,\namortized cost = actual cost + ΔΦ\n= Θ(1 + #1) + c(-#1 + 1)\n= Θ(1)\nΦ(initial DS) = 0 if the counter starts at 000 · · · 0. This is necessary for the above\namortized analysis. Otherwise, Φ may become smaller than Φ(initial DS).\nInsert in 2-3 trees\nInsert can cause O(lg n) splits in the worst case, but we can show it causes only O(1)\namortized splits. First consider what causes a split: insertion into a 3-node (a node\nwith 3 children). In that case, the 3-node needs to split into two 2-nodes.\nSo 3-nodes are bad. We define Φ = the number of 3-nodes. Then ΔΦ ≤ 1 -\nthe number of splits. Amortized number of splits = actual number of splits + ΔΦ =\n1. Φ(initial DS) = 0 if the tree is empty initially.\nThe above analysis holds for any (a, b)-tree, if we define Φ to be the number of\nb-nodes.\nIf we consider both insertion and deletion in 2-3 trees, can we claim both O(1) splits\nfor insert, and O(1) merges for delete? The answer is no, because a split creates two\n2-nodes, which are bad for merge. In the worse case, they may be merged by the next\ndelete, and then need split again on the next insert, and so on.\nWhat do we solve this problem? We need to prevent split and merge from creating\n'bad' nodes.\nInsert and delete in 2-5 trees\nWe can claim O(1) splits for insert, and O(1) merges for delete in 2-5 trees.\nIn 2-5 trees, insertion into a 5-node (a node with 5 children) causes it to split into\ntwo 3-nodes.\npromote to parent\ne\n5 k e y s\n5 k\ny s\n6 children\n3 children 3 children\n6.046J\n\nLecture 5\nAmortization\nSpring 2015\nDeletion from a 2-node causes it to merge with another 2-node to form a 3-node.\ndemote from parent\nx\nx 1\nkey demoted\n2 children\n3 children\n1 child left\n5-nodes and 2-nodes are bad. We define Φ = # of 5-nodes + # of 2-nodes.\nAmortized splits and merges = 1. Φ(initial DS) = 0 if the tree is empty initially.\nThe above analysis holds for any (a, b)-tree where b > 2a, because splits and\nmerges do not produce bad nodes. We define Φ to be the number of b-nodes plus the\nnumber of a nodes.\nNote: The potential examples could also be done with the accounting method by\nplacing coins on 1s (binary counter) or 2/5-nodes ((2, 5)-trees).\n6.046J\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Class on Design and Analysis of Algorithms, Lecture 5 Notes, Handwritten",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/c4f64b6049a576654cc89f8449b8b6ea_MIT6_046JS15_writtenlec5.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class on Design and Analysis of Algorithms, Recitation 1 Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/7c15f31b5a1aa1991313d1b4544cc137_MIT6_046JS15_Recitation1.pdf",
      "content": "Design and Analysis of Algorithms\nFebruary 6, 2015\nMassachusetts Institute of Technology\nProfs. Erik Demaine, Srini Devadas and Nancy Lynch\nRecitation 1\nMatrix Multiplication and the Master Theorem\n1 Weighted interval scheduling\nConsider requests 1,. . . ,n. For request i, s(i) is the start time and f(i) is the finish time, s(i) < f(i).\nTwo requests i and j are compatible if they don't overlap, i.e., f(i) ≤ s(j) or f(j) ≤ s(i). Each\nrequet i has a weight w(i). Goal: schedule the subset of compatible requests with maximum\nweight.\n1.1 The n log n dynamic programming solution\nSort requests in earliest finish time order.\nf(1) ≤ f(2) ≤ · · · ≤ f(n)\nDefinition p(j) for interval j is the largest index i < j such that request i and j are compatible.\nArray M[0 . . . n] holds the optimal solution's values. M[k] is the maximum weight if requests\nfrom 1 to k are considered.\n1 M[0] = 0\n2 for j = 1 to n\nM[j] = max(w(j) + M[p(j)], M[j - 1])\nOnce we have M, the optimal solution can be derived by tracing it back in O(n) time. Sorting\nrequests in earliest finish time takes O(n log n) time. And the whole algorithm takes O(n log n)\ntime.\n2 Strassen\n2.1 Matrix Multiplication\nTake matrices A, B, multiply row i of A by column j of B to fill in entry i,j of resulting matrix, C.\nRunning time is Θ(n3) on square matrices, where n is the dimension of each matrix.\n6.046J/18.410J\n\nRecitation 1: Matrix Multiplication and the Master Theorem\n2.2 The Strassen Algorithm\n- powerful early application of Divide and Conquer\n- not the fastest matrix multiplication (though it was at time of discovery)\n- Don Coppersmith, Shmuel Winograd, Andrew Stothers, and Vassilevska Williams con\ntributed to the current fastest method. See http://en.wikipedia.org/wiki/\nCoppersmith-Winograd_algorithm for details.\n2.2.1 Steps\n- Make A, B each 2k x 2k by filling remaining rows/columns with 0:\n- Why can you do this?\n∗ Each dimension increases by less than a factor of 2\n∗ Even with traditional Θ(n3) matrix multiplication, this makes the running time\nalways increase by a factor of less than 8, not dependent on the magnitude of N,\nand constant factors are always ignored when discussing complexity.\n- Partition A, B, and C (elements unknown for C, but same dimensions) into 4 matrices of\ndimension 2k-1 each\n- We can see that the 4 submatrices of C can be found by standard matrix multiplications of A\nand B, using the submatrices as \"elements\"\nA1,1 A1,2\nB1,1 B1,2\nC1,1 C1,2\nA =\n,\nB =\n,\nC =\nA2,1 A2,2\nB2,1 B2,2\nC2,1 C2,2\nC1,1 = A1,1B1,1 + A1,2B2,1\nC1,2 = A1,1B1,2 + A1,2B2,2\nC2,1 = A2,1B1,1 + A2,2B2,1\nC2,2 = A2,1B1,2 + A2,2B2,2\n- Optimization is derived from the fact that matrix addition is much, much simpler than mul\ntiplication (Θ(n2) instead of Θ(n3))\n\nRecitation 1: Matrix Multiplication and the Master Theorem\nDefine M1 = (A1,1 + A2,2)(B1,1 + B2,2)\nM2 = (A2,1 + A2,2)B1,1\nM3 = A1,1(B1,2 - B2,2)\nM4 = A2,2(B2,1 - B1,1)\nM5 = (A1,1 + A1,2)B2,2\nM6 = (A2,1 - A1,1)(B1,1 + B1,2)\nM7 = (A1,2 - A2,2)(B2,1 + B2,2)\nThus, C1,1 = M1 + M4 - M5 + M7 = A1,1B1,1 + A1,2B2,1\nC1,2 = M3 + M5 = A1,1B1,2 + A1,2B2,2\nC2,1 = M2 + M4 = A2,1B1,1 + A2,2B2,1\nC2,2 = M1 - M2 + M3 + M6 = A2,1B1,2 + A2,2B2,2\nProof of correctness follows from arithmetic.\nWe can recursively calculate each of the above submatrices using equally-sized submatrices of\nA1,1, etc., which is why we needed dimensions of 2n instead of merely even dimensions.\nWhen you have C, strip out rows/columns of 0s that correspond to the same parts of A and B.\n- Each recursive step takes 7 multiplications and 18 additions, instead of 8 multiplications\n- We can see that this would be less efficient than 8 multiplications for small matrices. For a\n2-element matrix being broken into 4 1-element matrices, it's over triple the work!\nlog2(7)) ≈ Θ(n2.8074)\nRunning time: T(n) = Θ(n\nHow do we get this value? (next up)\n3 Master Theorem\n3.1 General use\nGeneral form of a recurrence:\nT (n) = aT (n/b) + f(n)\nlogb(a))\n- f(n) polynomially less than nlogb(a): T (n) = Θ(n\n- f(n) is Θ(nlogb(a) logk(n)), where k ≥ 0: T (n) = Θ(f(n) log(n)) = Θ(nlogb(a) logk+1(n))\n- nlogb(a) polynomially less than f(n), and af(n/b) ≤ cf(n) for some constant c < 1 and all\nsufficiently large n: T (n) = Θ(f(n))\nIf nlogb(a) is greater, but not polynomially greater, than f(n), the Master Theorem cannot be\nused to determine a precise bound.\n(e.g. T (n) = 2T (n/2) + Θ(n/ log(n)))\n\nl m\nRecitation 1: Matrix Multiplication and the Master Theorem\n3.2 Strassen Runtime\nNow, think about Strassen's algorithm. It performs 7 multiplications and 18 additions/subtractions\neach iteration. The addition is performed directly; the multiplications are done recursively using\nthe Strassen Algorithm.\nIn each recursive step, we divide the matrix into 4 parts; however, remember that we consider\nthe running time in terms of the dimension of the matrix, not the total number of elements.\nThus, the recurrence becomes\nT (n) = 7T (n/2) + 18Θ(n 2) = 7T (n/2) + Θ(n 2)\nWe can then examine the Master Theorem:\nnlog2(7)) is polynomially greater than n2\nThus, Θ(nlog2(7)) is the solution to the recurrence.\n3.3 Median Finding\nm v\nn\nProve that T (n) = T ( 5 ) + T (7\nn + 6) + Θ(n) solves for T (n) = Θ(n).\nProof: We use the substitution method (details can be found in the CLRS textbook) to solve\nthe recurrence. We first guess the form of the answer to be O(n), and try to prove that T (n) ≤ dn\nfor some value of d. We first assume that the bound holds for all positive m < n, and thus it holds\nm v\nn\nfor T ( 5 ) and T (7\nn + 6) if n is large enough. Substituting into the recurrence yields\nn\n7n\nT (n) = T (\n) + T (\n+ 6) + cn\n(1)\nn\n7n\n≤ d( + 1) + d(\n+ 6) + cn\n(2)\n=\ndn + 7d + cn\n(3)\n≤ dn\n(4)\nThe last inequality holds if d > 10c when n is large enough.\n3.4 Extra details\nDrawing a recursion tree using the recurrence T (n) = 4T (n/2) + Θ(n2) will show why the log\nlogb(a)\nfactor is used if f(n) is not polynomially greater than n\n. Think of the total amount of work\nthat must be done.\nFeel free to examine T (n) = 4T (n/2) + Θ(n2 log(n)) to see why the solution must be\nΘ(n2 log2(n)) instead of just Θ(n2 log(n)).\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class on Design and Analysis of Algorithms, Recitation 2 Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/8fd15588f584c518b6ec83aedb2b96c9_MIT6_046JS15_Recitation2.pdf",
      "content": "Design and Analysis of Algorithms\nFebruary 13, 2015\nMassachusetts Institute of Technology\nProfs. Erik Demaine, Srini Devadas and Nancy Lynch\nRecitation 2\n2-3 Trees and B-Trees\n1 Recap\n1.1 Balanced Binary Search Trees\nBinary Search Trees that guarantee O(log(n)) height by rebalancing after Insert/Delete operations.\n1.2 Example Insertion and Rotation\nInsert 1\nRotate\n2 2-3 Trees\n2.1 Properties\n2-3 Trees are balanced search trees. Every node with children (non-leaf) has either two children\n(2-node) and consists of one piece of data, or has three children (3-node) and consists of 2 pieces\nof data.\nB\nA\nA\nD\nB\nC\n2-Node\nE\nC\n3-Node\n- Every non-leaf is a 2-node or 3-node\n- All leaves are at the same level\n- All non-leaves branch\n6.046J/18.410J\n\nRecitation 2: 2-3 Trees and B-Trees\n- All data is kept in sorted order\n- Every leaf node will contain one or two fields\n- Height ≤ lgn (More dense than BST)\n2.2 Example 2-3 Tree\n2.3 Search\nVery similar to Binary Search. Start at the root node, and traverse down tree in order. Tree is sorted\nso only need to look at one node for each level of tree. Because of this Search runtime is O(lg(n))\nSearch(14)\n\nRecitation 2: 2-3 Trees and B-Trees\n2.4 Insert\nInsert(X) Steps:\n- Search for element X for where it would go in a leaf of the tree.\n- Insert element X into where it would go.\n- While there is overflow, a node has more than 3 elements, Split node into left half, median,\nand right half Then promote median up a level. If there is a parent, add it to the node. If\nthere is no parent, create a new node of just the Median node as the new root.\n- Runtime is O(lg(n))\n\nRecitation 2: 2-3 Trees and B-Trees\n2.5 Delete\nDelete(X) Steps:\n- Swap item to delete with inorder successor if item is not already a leaf.\n- Redistribute and merge nodes if there is underflowing in order to get back to a correct 2-3\ntree\n- Runtime is O(lg(n))\n\nRecitation 2: 2-3 Trees and B-Trees\n2.5.1 Redistribute Example\n\n'HOHWH\n\n'HOHWH\n\n'HOHWH\n\n'HOHWH\n\nRecitation 2: 2-3 Trees and B-Trees\n2.5.2 Merge Example\nDelete 30\nDelete 30\n3 B-Trees\nB-Trees are tree data structures that store sorted data. B-Trees can be seen as a generalization of\nBinary Search Trees where nodes can have more than one key/value and more than two children.\nSimilar to BSTs, they support search, insertion and deletion in logarithmic time.\n3.1 Properties\nA B-tree has a parameter called the minimum degree or branching factor. For the purposes of our\ndiscussion let the branching factor be B.\n- For any non leaf node, the number of children is one greater than the number of keys in that\nnode.\n- Every non-root node contains at least B - 1 keys. Consequently, all internal (non-leaf and\nnon-root) nodes have at least B children.\n- Every node contains at most 2B - 1 keys. Consequently, all nodes have at most 2B children.\n- All the leaves are at the same depth.\n\nRecitation 2: 2-3 Trees and B-Trees\nThe keys is a B-tree are sorted in a similar fashion to BSTs. Consider a node x with C children.\nLet's say that x has keys k1 < k2 < ... < kC . For ease of notation, we define k0 = inf and\nkn + 1 = -inf. If K belongs to the ith(1 ≤ i ≤ n + 1) sub-tree of x, then ki-1 ≤ K ≤ ki.\n- Search time is O(lg(n))\n- Insert/Delete time is O(lg(n)) if B = O(1)\n3.2 Why B-Trees\n- Caches read whole blocks of data, and want entire block useful\n- Set parameter B equal to block size\n- O(logb(n)) block reads per Search, Insert, Delete operations.\nB-Trees are used by most databases and filesystems:\n-Databases: Sleepycat/BerkelyDB, MySQL, SQLite\n-Filesystems: MacOS HFS/HFS+, ReiserFS, Windows NTFS, Linux ext3, shmfs\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class on Design and Analysis of Algorithms, Recitation 3 Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/d25d9d3ba96321326601c8f6dd073e60_MIT6_046JS15_Recitation3.pdf",
      "content": "Design and Analysis of Algorithms\nFebruary 20, 2015\nMassachusetts Institute of Technology\nProfs. Erik Demaine, Srini Devadas and Nancy Lynch\nRecitation 3\nUnion-Find and Amortization\n1 Introduction\nA union-find data structure, also known as a disjoint-set data structure, is a data structure that can\nkeep track of a collection of pairwise disjoint sets S = {S1, S2, . . . , Sr} containing a total of n\nelements. Each set Si has a single, arbitrarily chosen element that can serve as a representative for\nthe entire set, denoted as rep[Si].\nSpecifically, we wish to support the following operations:\n- MAKE-SET(x), which adds a new set {x} to S with rep[{x}] = x.\n- FIND-SET(x), which determines which set Sx ∈S contains x and returns rep[Sx].\n- UNION(x, y), which replaces Sx and Sy with Sx ∪ Sy in S for any x, y in distinct sets Sx, Sy.\nFurthermore, we want to make these as efficient as possible. We will go through the process of\ncreating a data structure that, in an amortized sense, performs spectacularly.\n1.1 Motivation\nUnion-find data structure is used in many different algorithms. A natural use case of union find\nis keeping track of connected component in an undirected graph where nodes and edges could be\nadded dynamically. We start with the initial connected components as Si's. As we add a node v\nalong with its edges E = {(v, vj )}, we either\n1. Call MAKE-SET if E = ∅ to make a new component.\n2. Call FIND-SET(vj ) to find the component node v will be connected to, and call UNION to\nconnect all those components connected through v.\nAnother use case of this data structure is in Kruskal's algorithm which you will see in future\nlectures.\n6.046J/18.410J\n\nRecitation 3: Union-Find and Amortization\nFigure 1: A simple doubly linked list.\n2 Starting Out\n2.1 Linked List Solution\nA na ıve way to solve this problem is to represent each set with a doubly linked list like so:\nWe may also have a data structure, such as a hash table mapping elements to pointers, that\nallows us to access each linked list node in constant time.\nWe define rep[Si] to be the element at the head of the list representing Si.\nHere are the algorithms for each operation:\n- MAKE-SET(x) initializes x as a lone node. This takes Θ(1) time in the worst case.\n- FIND-SET(x) walks left in the list containing x until it reaches the front of the list. This\ntakes Θ(n) time in the worst case.\n- UNION(x, y) walks to the tail of Sx and to the head of Sy and concatenates the two lists\ntogether. This takes Θ(n) time in the worst case.\n2.2 Augmenting the Linked List\nWe can improve the behavior of our linked list solution by augmenting the linked lists such that\neach node has a pointer to its representative and that we also keep track of the tail of the list and\nthe number of elements in the list at any time, which we will call its weight.\nFigure 2: An augmented linked list.\nNow, FIND-SET(x) can run in Θ(1) time.\n\nRecitation 3: Union-Find and Amortization\nWe also change the behavior of UNION(x, y) to concatenate the lists containing x and y, up\ndating the rep pointers for all the elements in y.\nHowever, this could still require Θ(n) time in the worst case! Imagine if we called MAKE-SET\nfor every integer between 1 and n and then called UNION(n - 1, n), UNION(n - 2, n - 1), . . .,\nUNION(1, 2), where at the ith union we modify a list of length i. The total cost of all the calls to\nUNION is 1 + 2 + . . . + (n - 1) = Θ(n2).\n3 First Improvement: Smaller into Larger\n(This part is updated and covered in Quiz 1 review session.)\nHowever, we can solve this by forcing UNION to merge the smaller list into the larger list. If\nwe do this, the above scenario will only modify a list of length 1 every time, resulting in Θ(n)\nrunning time for these n - 1 UNIONs.\nClaim. With the first improvement, the amortized running time of n calls to MAKE-SET followed\nby n calls to UNION is O(n lg n).\nProof.\nWe first use the aggregate method. Suppose the cost of moving a rep pointer is 1. Monitor\nsome element x and the set Sx that contains it. After MAKE-SET(x), we have weight[Sx] = 1.\nWhen we call UNION(x, y), one of the following will happen:\n- If weight[Sx] > weight[Sy], then rep[x] stays unchanged, so we need not pay anything on\nx's behalf, and weight[Sx] will only increase.\n- If weight[Sx] ≤ weight[Sy], we pay 1 to update rep[x], and weight[Sx] at least doubles.\nSx can double at most lg n times, so we update rep[x] at most lg n times. Therefore across all the\nn elements, we get an amortized running time of O(n lg n). D\nProof.\nThe proof is very similar using the accounting method or the charging method. When we\ncall UNION(x, y), suppose |Sx| < |Sy| without loss of generality. We will charge every element\ni ∈ Sx because their pointers rep[i]'s are updated. Similarly, each element i can be charged at most\nlg n times, because |Si| at least doubles.\nIf we use the accounting method, each MAKE-SET(x) stores lg n coins into the bank for use in\nfuture UNION. D\nProof.\nNow we would like use the potential method. The potential method is usually the \"in\nverse\" of the accounting method. The potential function can be viewed as the balance in the bank\naccount. In this case, we define the potential function to be\n\nΦ =\nlg n - lg |Si|\ni\nwhere the sum is taken over every element i in the structure (n is an upper bound on the total\nnumber of elements).\n\nRecitation 3: Union-Find and Amortization\nWhen we call MAKE-SET(x), |Sx| = 1, so the amortized cost is O(1) + ΔΦ = O(lg n). This\nmeans each MAKE-SET(x) stores lg n coins into the bank.\nWhen we call UNION(x, y), again suppose |Sx| < |Sy|. The actual cost is |Sx|. ΔΦ < -|Sx|,\nbecause every element in i ∈ Sx now has |Si| doubled. This means we withdraw |Sx| coins from\nthe bank to pay for this UNION operation. So the amortized cost of UNION is |Sx| + ΔΦ < 0. D\n4 Forest of Trees Representation\nOne interesting observation we can make is that we don't really care about the links between nodes\nin the linked list; we only care about the rep pointers. Essentially, a list can be represented as a\nforest of trees, where rep[x] will be the root of the tree that contains x:\nFigure 3: A forest of trees representation of a disjoint set data structure.\nThe algorithms will now be as follows:\n- MAKE-SET(x) initializes x as a lone node. This takes Θ(1) time in the worst case.\n- FIND-SET(x) climbs the tree containing x to the root. This takes Θ(height) time.\n- UNION(x, y) climbs to the roots of the trees containing x and y and merges sets the parent\nof rep[y] to rep[x]. This takes Θ(height) time.\n4.1 Adapting the First Improvement\nOur first trick can be modified to fit the forest of trees representation by merging the tree with the\nsmaller height into the tree with the bigger height. One can then show that the height of a tree will\nstill be O(lg n).\n\nRecitation 3: Union-Find and Amortization\nFigure 4: The data structure after calling UNION(x1, y1).\n5 Second Improvement: Path Compression\nLet's now improve FIND-SET. When we climb the tree, we learn the representatives of every inter\nmediate node we pass. Therefore we should redirect the rep pointers so a future call to FIND-SET\nwon't have to do the same computations multiple times.\nFigure 5: The data structure after calling FIND-SET(x8).\nClaim. Let n be the total number of elements we're keeping track of. With the second improvement\nalone, the amortized running time of m operations (including FIND-SET) is O(m lg n).\nProof.\nAmortization by potential function. Let us define weight[xi] to be the number of elements\nin the subtree rooted at xi.\n\nRecitation 3: Union-Find and Amortization\nFigure 6: The same tree as in Figure 5, but redrawn. Notice that the xi's are much more compact\nthan Figure 4.\n\nLet Φ(x1, . . . , xn) =\ni lg weight[xi]. If UNION(xi, xj) attaches xj 's subtree's root as a child\nof xi's subtree's root, it only increases the weight of the root of Sxi . The increase is at most lg n,\nbecause there are only n elements at most. The weights of all the other elements stay unchanged.\nNow consider each step from child c to an ancestor p made by FIND-SET(xi) moves c's subtree\nout of p's subtree. If, at any particular step, weight[c] ≥ 1\n2 weight[p], then the potential decreases\nby at least 1, which pays for the move. Furthermore, there can be at most lg n steps for which\nweight[c] < 1\n2 weight[p], since each one decreases the size of the tree we're looking in by more\nthan half. D\n6 Why Don't We Do Both?\nIf we do both improvements to the tree representation, we get spectacular behavior where the\namortized cost of each operation is almost constant!\n6.1 CLRS-Ackermann Function\nWe define a function, Ak(j), with a similar structure to the well-known Ackermann function, as\nfollows:\n\nj + 1\nif k = 0\nAk(j) =\nAj+1\nk-1(j) if k ≥ 1\nBecause you're repeatedly iterating Ak-1 many times when k ≥ 1, this function explodes very\nquickly:\n\nRecitation 3: Union-Find and Amortization\nA0(1) = 2\nA1(1) = 3\nA2(1) = 7\nA3(1) = 2047\n.\n22..\nA4(1) >\nNow consider its \"inverse\" as follows:\nα(n) = min{k : Ak(1) ≥ n}\nWhile not technically constant, this value is pretty darn close, even for very large values of n.\nFor practical purposes, you can easily assume that α(n) ≤ 4.\n6.2 How Spectacular?\nTheorem. With both improvements improvement, the amortized running time of m operations is\nO(mα(n)).\nThe proof is very long and very tricky. If you're interested, check out Section 21.4 in CLRS.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class on Design and Analysis of Algorithms, Recitation 4 Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/08020963ae819d4c6dcf177983bb7c6b_MIT6_046JS15_Recitation4.pdf",
      "content": "Design and Analysis of Algorithms\nFebruary 27, 2015\nMassachusetts Institute of Technology\nProfs. Erik Demaine, Srini Devadas and Nancy Lynch\nRecitation 4\nRandomized Select and Randomized Quicksort\n1 Randomized Select\nThe algorithm RANDOMIZED-SELECT selects out the k-th order statistics of an arbitrary array.\n1.1 Algorithm\nThe algorithm RANDOMIZED-SELECT works by partitioning the array A according to\nRANDOMIZED-PARTITION, and recurses on one of the resulting arrays.\nRANDOMIZED-SELECT(A, p, r, i)\n1 if p = r\nthen return A[p]\n3 q ← RANDOMIZED-PARTITION(A, p, r)\n4 k ← q - p + 1\n5 if i ≤ k\nthen return RANDOMIZED-SELECT(A, p, q, i)\nelse return RANDOMIZED-SELECT(A, q + 1, r, i - k)\nRANDOMIZED-PARTITION(A, p, r)\n1 i ← RANDOM(p, r)\n2 exchange A[p] ↔ A[i]\n3 return PARTITION(A, p, r)\nBoth of the algorithms above are as in CLRS.\n1.2 Analysis of Running Time\nLet T (n) be the expected running time Randomized Select. We would like to write out a recursion\nfor it.\nLet Ei denote the event that the random partition divides the array into two arrays of size i and\nn - i. Then we see that\nn-1\nn\nT (n) ≤ n +\nPr(Ei) (max (T (i), T (n - i))) ,\n(1)\ni=0\n6.046J/18.410J\n\nRecitation 4: Randomized Select and Randomized Quicksort\nwhere by taking the max we assume that we are recursing on the larger subarray (hence we have\nthe less than or equal sign).\nFor simplicity, let us assume that n is even. Note that max (T (i), T (n - i)) is always the same\nas max (T (n - i), T (i)). This allows us to extend the chain of inequalities to\nn/2-1\nn\nT (n) ≤ n + 2\nPr(Ei) (max (T (i), T (n - i))) .\n(2)\ni=0\nAlso, since the partition element is chosen randomly, it is equally likely to partition the array\ninto sizes 0, 1, · · · , n - 1. So Pr(Ei) = n\n1 for all i. This leads us to\nn/2-1\nn\nT (n) ≤ n +\n(max (T (i), T (n - i))) .\n(3)\nn i=0\nWe will not show, via substitution, that T (n) = O(n).\nTheorem 1 Let T (n) denote the expected running time of randomized select. Then T (n) = O(n).\nProof.\nWe will show by the method of substitution. Let's say that T (n) ≤ cn, and check that it\nworks.\nWe must first check the base case. This is obvious, however, since T (n') is a constant for some\n'\nsmall constant n .\nNow let us check the inductive case. Assume that T (k) ≤ ck for all k < n, and we now want\nto show that T (n) ≤ cn.\nn/2-1\nn/2-1\nn\nn\nT (n) ≤ n +\n(max (T (i), T (n - i))) ≤ n +\n(max (ci, c(n - i))) .\n(4)\nn\nn\ni=0\ni=0\nWe note that that this is the same as\nn-1\nn\nn +\nci.\n(5)\nn i=n/2\n\nn-1\nn-1\nThe term 2\n(ci) is the same as 2c\ni. So we get\nn\ni=n/2\nn\ni=n/2\n⎛\n⎞\nn-1\n\nn\nT (n) ≤ n + c ⎝ 2\ni⎠ ≤ n + c (3n/4) = n\n1 + 3c\n.\n(6)\nn\ni=n/2\nHence if we take c = 4 (which works for the case T (1) ≤ 4 as well) we get\n\n3 ∗ 4\nT (n) ≤ n\n1 +\n= n (1 + 3) = 4n,\n(7)\nas we wanted.\n\nRecitation 4: Randomized Select and Randomized Quicksort\n2 Randomized Quicksort\n2.1 Algorithm\nThe algorithm RANDOMIZED-QUICKSORT works by partitioning the array A, and recursively\nsorts both partitions.\nRANDOMIZED-QUICKSORT(A, p, r)\n1 if p < r\nthen q ← RANDOMIZED-PARTITION(A, p, r)\nRANDOMIZED-QUICKSORT(A, p, q - 1)\nRANDOMIZED-QUICKSORT(A, q + 1, r)\n2.2 Analysis of Running Time\nLet T (n) be the expected running time Randomized Quicksort. Let Ei denote the event that the\narray is partitioned into two arrays of size i and n - i - 1. The pivot value is not included in either\npartition. Then we have\nn-1\nn\n(8)\n≤\n--\nT ( )\nPr(E )(T (i) + T (\ni\n1) + Θ( ))\nn\nn\nn ,\ni\ni=0\nBecause Pr(Ei) = n\n1 for all i, we have\nn\nn-1\nT (n) ≤\n(T (i) + T (n - i - 1) + Θ(n)),\nn i=0\n(9)\nn\nn-1\nT (n) ≤\nT (i) + Θ(n),\nn i=0\n(10)\nn\nThe same as Randomized select, we use induction to prove that T (n) = Θ(n log n). Suppose\nT (n) ≤ cn log n for some constant c > 0. Notice the fact that\nn-1\ni log i ≤ 1 n 2 log n - 1 n ,\n(11)\ni=0\nThen for the inductive step, we have\nn\nn-1\nT (n) ≤\nci log i + Θ(n),\nn i=0\n2c 1\n( n 2 log n - n 2) + Θ(n),\n(12)\n(13)\nT (n) ≤ n 2\ncn\nT (n) ≤ cn log n - (\n- Θ(n)),\n(14)\nWhen c is chosen large enough, T (n) ≤ cn log n.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class on Design and Analysis of Algorithms, Recitation 5 Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/d187472426117c2ebec700a22c209b3f_MIT6_046JS15_Recitation5.pdf",
      "content": "Design and Analysis of Algorithms\nMarch 6, 2015\nMassachusetts Institute of Technology\nProfs. Erik Demaine, Srini Devadas and Nancy Lynch\nRecitation 5\nDynamic Programming\nRectangular blocks\nGiven a set of n rectangular three-dimensional blocks, where block Bi has length li, width wi, and\nheight hi, all real numbers, find the maximum height of a tower of blocks that is as tall as possible,\nusing any subset of the blocks, such that the tower respects the following constraints:\n1. Blocks cannot be rotated: the length always refers to the east-west direction, the width is\nalways north-south, and the height is always up-down.\n2. Block Bi can be stacked on top of block Bj only if li ≤ lj and wi ≤ wj , that is, the two\ndimensions of the base of block Bi are no greater than those of block Bj .\nSub-problem definition\nFirst we assume that our blocks {1, 2, . . . , n} are arranged in non-increasing order of length and\nthen breadth - note that this is easy to do as a pre-processing step that takes O(n log n) time.\nGiven this ordering of blocks, we define H[i] to be the height of the tallest tower that has block\nBi at the top.\nRecursive formulation\nWe see that H[i] can be expressed as,\nH[i] = hi +\nmax\nH[j]\nj<i;lj ≥li;wj ≥wi\nIf there exists no compatible box in the above maximization, the max term equals 0.\nOur base case here is H[1] = h1.\nIn this problem, we're guessing over all possible legal blocks under the top block (block Bi in\nthis case).\nThe final answer is then maxi H[i].\nRuntime analysis\nTotal number of sub-problems here is O(n), and the total time required to solve each sub-problem\nis O(n), which means that the total running time of the Dynamic Programming part of this algo\nrithm is O(n2).\nNote that sorting the boxes in non-increasing order of lengths and then breadths takes O(n log n)\ntime, which means the total running time of this algorithm is O(n log n + n2) = O(n2).\n6.046J/18.410J\n\nRecitation 5: Dynamic Programming\nCounting Boolean Parenthesizations\nGiven a boolean expression consisting of a string of the symbols True, False, AND, OR, and\nXOR, count the number of ways to parenthesize the expression such that it will evaluate to True.\nFor example, there are 2 ways to parenthesize True AND False XOR True such that it eval\nuates to True.\nSub-problem definition\nLet T [i, j] be the number of ways of parenthesizing the string S[i : j] such that the expression\nbetween indices i and j evaluates to True, and F [i, j] be the number of ways of parenthesizing\nthe string S[i : j] such that the expression between indices i and j evaluates to False. Here\nindices i and j are inclusive.\nRecursive formulation\nThen, given that T ot[i, j] = T [i, j] + F [i, j], we see that T [i, j] and F [i, j] can be expressed by the\nfollowing recursive formulations,\n⎧\n⎨ T [i, k] · T [k + 1, j]\nif S[k] = AND\nT ot[i, k] · T ot[k + 1, j] - F [i, k] · F [k + 1, j]\nif S[k] =\nOR\nT [i, k] · F [k + 1, j] + F [i, k] · T [k + 1, j]\nif S[k] = XOR\nF [i, k] · F [k + 1, j]\nif S[k] =\nOR\nT ot[i, k] · T ot[k + 1, j] - T [i, k] · T [k + 1, j]\nif S[k] = AND\nT [i, k] · T [k + 1, j] + F [i, k] · F [k + 1, j]\nif S[k] = XOR\nj-1\nk=i\nj\nT [i, j] =\n⎩\n⎧\n⎨\nj\nj-1\nk=i\nF [i, j] =\n⎩\nWe have the following base cases, for all i between 1 and n,\nT [i, i] = 1\nif S[i] = True\nT [i, i] = 0\nif S[i] = False\nF [i, i] = 0\nif S[i] = True\nF [i, i] = 1\nif S[i] = False\nThe final answer is T [1, n] (the total number of ways of obtaining a final value of True given that\nthe entire string S[1 : n] is used.\nHere, we are making a guess over the positions of the outermost parentheses in the expression\nsub-stringed between i and j.\nRuntime analysis\nTotal number of sub-problems here is O(n2), and the amount of time required to solve each sub\nproblem is O(n), so the total runtime complexity of the algorithm is O(n3).\n\nRecitation 5: Dynamic Programming\nMake change\nGiven a value N, if we want to make change for N cents, and we have infinite supply of each of\nS = {S1, S2, .., Sm} valued coins, what's the minimum number of coins needed to get to a total of\nN? For simplicity, assume that S1 > S2 > . . . > Sm.\nSub-problem definition\nLet C[p] be the minimum number of coins need to make change for p cents using coins of denom\ninations S1, S2, . . . , Sm.\nRecursive formulation\nIf p > 0,\nC[p] = min C[p - Si] + 1\ni:Si≤p\nOur base case is C[0] = 0.\nThe final answer is C[N].\nIn this formulation, we make a guess on which coin denomination Si belongs to the optimal\nconfiguration.\nRuntime analysis\nThe total number of sub-problems here is O(N), and the total time required to solve each sub\nproblem is O(m), so the total runtime complexity of this algorithm is O(mN).\nNote that this runtime is pseudo-polynomial (similar to Knapsack).\nOther problems\n- (Warmup) A robot starts from the top left corner (1, 1) of a M × N grid. The goal of the\nrobot is to reach right bottom (M, N). At each step the robot can make one of the two\nchoices - move one cell right, move one cell bottom. Write a function which takes M and N\nas arguments and returns the total number of paths the robot can take to reach its destination.\n- Consider an input text consisting of n words, each of lengths l1, l2, . . . , ln characters. We\nwant to print this text as neatly as possible, with the restriction that each line can hold only\na maximum of M characters. The goal here is to minimize the sum over all lines except the\nlast, of the cubes of the numbers of extra space characters at the end of each line. The number\nnj\nof extra characters at the end of a line that consists of words i through j is M -j+i+\nk=i lk.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class on Design and Analysis of Algorithms, Recitation 6 Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/151160c7c18d3f87159b7482823ff61c_MIT6_046JS15_Recitation6.pdf",
      "content": "Design and Analysis of Algorithms\nMarch 20, 2015\nMassachusetts Institute of Technology\nProfs. Erik Demaine, Srini Devadas and Nancy Lynch\nRecitation 6\nGreedy Algorithms\nProcess Scheduling\nYou have a computer and n processes with processing times t1, ..., tn. You have to pick the order\nin which to run the processes. Let pi denote the ith process you run. Then, the completion time Ci\nPi\nfor process pi is defined as Cpi =\ntpj , i.e., the sum of times for all the processes up till this\nj=1\nP\none ends. You have to minimize the average completion time, i.e.\nn\n.\ni=1 Cpi\nGreedy solution\nThis problem has a well known greedy solution, known as the Shortest Processing Time First\n(SPTF) rule. We perform the processes in order of lowest processing time. Let us prove this.\nAssume tpi ≥ tpj and i < j. If we interchange pi and pj , then the completion time of every\nthing from process i to j reduces by tpi - tpj , which is non-negative, and the completion time for\neverything process j onwards remains unchanged. Thus, the interchanged order of processes is\nless than or equal to the original.\nIn this manner, we can sort the process times by performing two-swaps one by one, and we\nwill only decrease our average completion time. We can do this in O(n log n) time.\nOnline version\nThis problem has the same solution when processes can be added dynamically. If a process with a\nlower processing time than the remaining processing time of the current one is added, we switch\nto that one and complete it first. The proof is similar.\n6.046J/18.410J\n\nRecitation 6: Greedy Algorithms\nEvent Overlap problem\nYou have n events on your calendar, defined as intervals with a start time si and a finish time fi.\nThe events might overlap, and you want to attend all the events, so you are going to create k clones\nof yourself to achieve this. You want to minimize the number of clones you need, k. A clone can\nattend a certain non-overlapping subset of events.\nGreedy solution\nWe sort the intervals by start time. We start with 0 clones and dynamically assign each interval to\none of these clones. We maintain the finish time of the last interval assigned to each of these clones.\nAs we iterate through the sorted list, for each interval, if it starts after the last finished event for\none of the clones, we can assign this interval to the clone without an overlap. So we assign this\nevent to it and update its finish time. If it is not compatible with any of the clones, we create a new\nclone and assign this event to it.\nCorrectness\nLet us consider the event that corresponds to adding the mth clone. Suppose it happens when\nconsidering interval (si, fi). This means that m - 1 previously considered intervals overlap with\nthis. But, since they all start before si (since we sorted by start time), that means that at time si,\nthere are at least m concurrent intervals. This means that the optimal solution uses ≥ m clones.\nImplementation\nHere is an O(n log n) implementation. We maintain a min-heap of finish times of each clone's last\ninterval. When adding an interval (si, fi), if the minimum finish time on the heap is ≥ si, all of the\nclones are incompatible, and so we create a new clone by adding fi to the heap. If the minimum\nfinish time on the heap < si, we can add this interval to it, and so we pop from the heap and add\nfi.\nNote This problem can in fact be generalized to decomposing any partial ordering into chains\nby Dilworth's theorem.\n\nRecitation 6: Greedy Algorithms\nFractional Make-Change\nLet us consider the make-change problem from last recitation, with a few modifications. Instead of\ncoins, we have m kinds of metals. Given a value N, we want to make change for N cents. Metal\ntype i has value Si per kilogram, and we have ni kilograms of it. As these metals are in molten\nform, and we have an infinite precision scale, we can give out non-integral weights also.\nThis means that if we choose to use ki kilograms of type i, where 0 ≤ ki ≤ ni, the value given\nP\nout will be Siki. The objective now is to minimize the total weight of the metals used\nki\nGreedy solution\nOur greedy intuition for the original make change problem now works. We take the most valuable\nmetal, and try to fulfill as much of the remaining value with it as possible.\nIn other words, we sort the metals in decreasing order of value per kilogram and set remaining\nvalue r = N. As we iterate through the sorted list from i = 1 to m, if Sini < r, we set r = r-Sini\nand add ni kg of metal i to our set. Otherwise, if Sini ≥ r, we add S\nr\ni kg of metal i to our collection,\nand set r = r - Si S\nr\ni = 0. We break at this point, as our requirement is fulfilled.\nCorrectness\nThe proof follows by cut-and-paste. Let's say we have w kg unused metal i and we are using w\nkg of metal j in our optimal solution, such that Si > Sj . Then, we could replace the w of metal j\nSj\nSj\nwith w\nmore of metal i. As Si > Sj , w\n< w, so we have that much of metal i by hypothesis.\nSi\nSi\nFurther, our total weight strictly decreases. This contradicts the assumption that our solution was\noptimal.\nWe conclude that we will always exhaust the more valuable metal before using a less valuable\none, so our greedy algorithm is correct.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class on Design and Analysis of Algorithms, Recitation 7 Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/ba74f89434fc261d9366ebf136c04b52_MIT6_046JS15_Recitation7.pdf",
      "content": "Design and Analysis of Algorithms\nApril 3, 2015\nMassachusetts Institute of Technology\nProfs. Erik Demaine, Srini Devadas and Nancy Lynch\nRecitation 7\nNetwork Flow and Matching\nEdmonds-Karp Analysis\nRecall: Edmonds-Karp is an efficient implementation of the Ford-Fulkerson method which selects\nshortest augmenting paths in the residual graph. It assigns a weight of 1 to every edge and runs\nBFS to find a breadth-first shortest path from s to t in Gf .\nMonotonicity Lemma\nLemma. Let δ(v) = δf (s, v) be the breadth-first distance from s to v in Gf . During the Edmonds-\nKarp algorithm, δ(v) increases monotonically.\nProof:\nSuppose that augmenting a flow f on G produces a new flow f /. Let δ/(v) = δf' (s, v). We will\nshow that δ/(v) ≥ δ(v) by induction on δ/(v).\nBase Case: δ/(v) = 0. This implies that v = s, and since δ(s) = 0 and distance can never\nbe negative, it follows δ/(s) ≥ δ(s).\nInductive Case: Assume inductive hypothesis holds for any u where δ/(u) < δ/(v). We will\nshow that it is also hods for v.\nConsider a breadth-first path s → · · · → u → v in Gf' . We must have δ/(v) = δ/(u) + 1,\nsince subpaths of shortest paths are also shortest paths. Also note that by our inductive assumption\nδ/(u) ≥ δ(u), because δ/(u) < δ/(v). Certainly, (u, v) ∈ Ef' . We will now prove that δ/(v) ≥ δ(v)\nin both cases where (u, v) ∈ Ef and (u, v) ∈ Ef .\nCase 1: (u, v) ∈ Ef . Here we have:\nδ(v) ≤ δ(u) + 1\ntriangle inequality\n≤ δ/(u) + 1\ninductive assumption\n(1)\n= δ/(v)\nbreadth-first path\nTherefore δ/(v) ≥ δ(v) and monotonicity of δ(v) is established.\n6.046J/18.410J\n\nRecitation 7: Network Flow and Matching\nCase 2: (u, v) ∈ Ef . Here, the only way (u, v) ∈ Ef ' is if the augmenting path p that produced f /\nfrom f must have included (v, u). Moreover, p is a breadth first path in Gf :\np = s → · · · → v → u\nThus, we have:\nδ(v) = δ(u) - 1\nbreadth-first path\n≤ δ/(u) - 1\ninductive assumption\n(2)\n= δ/(v) - 2\nbreadth-first path\n< δ/(v)\nthereby establishing monotonicity for this case, too. D\nCounting Flow Augmentations\nTheorem. The number of flow augmentations in the Edmonds-Karp algorithm is O(V E).\nProof:\nFor an augmenting path p, define cf (p) = min{cf (u, v) ∈ p}.\nLet p be an augmenting path, and suppose that we have cf (p) = cf (u, v) for edge (u, v) ∈ p. Then,\nwe say that (u, v) is critical, and it disappears from the residual graph after flow augmentation.\nThis is because during augmentation, the residual capacity of every edge in p decreases by cf (p)\nas that much new flow is pushed through the augmenting path. And since cf (u, v) - cf (p) = 0,\nthe edge disappears after augmentation.\nThe first time an edge (u, v) is critical, we have δ(v) = δ(u)+1 since p is a breadth-first path. After\nthe augmentation, we must wait until (v, u) is on an augmenting path before (u, v) can be critical\nagain. Let δ/ be the distance function in the residual network when (v, u) is on an augmenting path.\nThen, we have:\nδ/(u) = δ/(v) + 1\nbreadth-first path\n≥ δ(v) + 1\nmonotonicity\n(3)\n= δ(u) + 2\nbreadth-first path\nHence between each occurrence of an edge (u, v) as critical, δ(u) increases by at least 2. And since\nδ(u) starts out non-negative and can be at most |V | - 1 until the vertex is unreachable, each edge\ncan be critical O(V ) times. And since the residual graph contains O(E) edges, the total number of\nflow augmentations is O(V E). D\nCorollary. The Edmonds-Karp maximum-flow algorithm runs in O(V E2) time.\nProof: Breadth-First Search runs in O(E) time, and there are O(V E) augmentations. All other\nbookkeeping is O(V ) per augmentation.\n\nRecitation 7: Network Flow and Matching\nApplications of Network Flow\nVertex Cover\nGiven an undirected graph G = (V, E), we say that a set S ⊆ V of vertices covers G, if for every\nedge (u, v) ∈ E, S contains either u or v. The Vertex Cover problem is now to find S such that S\ncovers G and |S| is minimal.\nVertex Cover is NP-Hard in general graphs but polynomial time solvable in bipartite graphs.\nBipartite Vertex Cover\nGiven a bipartite graph G = (L U R, E ⊆ L × R), find the set S such that S covers G and |S| is\nminimal.\nSolution: Given G, define the following Flow Network H:\n- Create a new source vertex s and add edges of capacity 1 from s to every vertex in L\n- Create a new sink vertex t and add edges of capacity 1 from every vertex in R to t\n- Direct all edges in E from L to R and assign each edge inf capacity\nRun Maximum Flow in H and return the value.\nFor example, consider the following graph H constructed from G = ({L1, L2, L3}U{R1, R2, R3}, E)\nwhere E consists of the shown edges:\ns\nL1\nL2\nL3\nR1\nR2\nR3\nt\ninf\ninf\ninf\ninf\ninf\nIn this example, the Maximum Flow is 2, and the minimal vertex cover is Q = {L1, R3} and\n|Q| = 2.\n\nRecitation 7: Network Flow and Matching\nCorrectness of Bipartite Vertex Cover as Maximum Flow\nClaim 1: Every Vertex Cover Q of H defines an (S, T ) cut of a finite value c(S, T ).\nProof: Let Q = QL U QR where QL = Q ∩ L and QR = Q ∩ R. Then define the cut (S, T )\nas follows:\nS = {s} ∪ QR ∪ (L\\QL)\nT = {t} ∪ QL ∪ (R\\QR)\nProof by picture:\ns\nQL\nL\\QL\nR\\QR\nQR\nt\nNote that there cannot be any edges going from L\\QL to R\\QR because if there were such an edge,\nboth endpoint vertices would not be covered and would contradict that Q was a valid vertex cover.\nFrom the picture it is clear that (S, T ) is indeed a cut in H. It is also clear that c(S, T ) = QL + QR\nbecause the only edges that cross the cut (S, T ) are all the edges from s to QL and from QR to t,\nand each of them have capacity 1. D\nClaim 1 implies that c(S∗, T ∗) ≤|Q∗| where Q∗ is the minimum Vertex Cover of G and (S∗, T ∗)\nis the minimum cut in H.\nClaim 2: For any finite cut (S, T ) in H, the set Q = (S ∩ R) ∪ (T ∩ L) is a Vertex Cover of\nG.\nProof: Observe the following picture:\n\nRecitation 7: Network Flow and Matching\ns\nT ∩ L\nS ∩ L\nT ∩ R\nS ∩ R\nt\nNote that there cannot be any edges going from S ∩ L to T ∩ R because that would make c(S, T )\ninfinite and contradict the assumption that the cut must be of finite capacity. From this picture,\nit is clear that every edge in G has at least one end point in either T ∩ L or S ∩ R and indeed\nQ = (S ∩ R) ∪ (T ∩ L) covers G. It is also clear that |Q| = |S ∩ R| + |T ∩ L| = c(S, T ). D\nClaim 2 implies that |Q∗| ≤ c(S∗, T ∗) where (S∗, T ∗) is the minimum cut in H and Q∗ is the\nminimum Vertex Cover of G.\nPunchline:\nBy claims 1 and 2, the size of the minimum Vertex Cover of G, |Q∗| is equal to the size of minimum\ncut (S∗, T ∗). And since the Maximum Flow is equal to the Minimum Cut, we can use Maximum\nFlow to solve Bipartite Vertex Cover in the way described above.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class on Design and Analysis of Algorithms, Recitation 8 Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/1dee182d301235b901119a0ff57f05e2_MIT6_046JS15_Recitation8.pdf",
      "content": "Design and Analysis of Algorithms\nApril 10, 2015\nMassachusetts Institute of Technology\nProfs. Erik Demaine, Srini Devadas and Nancy Lynch\nRecitation 8\nNP-Complete Problems\n1 Definitions\nP: The set of all decision problems D for which there exists a polynomial time algorithm A such\nthat A(x) = D(x). We consider a polynomial time algorithm to be \"efficient\".\nNP: The set of all decision problems D for which there exists a polynomial time verification\nalgorithm V such that for all inputs x, D(x) = 1 if and only if there exists a polynomial-length\n\"certificate\" y such that V (x, y) = T rue.\nIn order to compare the \"hardness\" of solving different problems, we use reductions! The idea\nis that if I have two problems A and B, if I can show that I can solve A by using a black box that\nsolves B, then I can understand the difficulty of solving A in terms of the difficulty of solving B\nplus the work required to transform a solution of B to the solution of A.\n\"Karp\"-Reduction: Let A : X →{0, 1} and B : Y →{0, 1} be decision problems. A is\npolytime reducible to B, or \"A reduces to B\" (A ∝ B), if there exists a function R : X → Y that\ntransforms inputs to A into inputs to B such that A(x) = B(R(x)). The following picture shows\nhow this reduction leads to an algorithm for A which simply builds upon the algorithm for B.\nSolving A is no harder than solving B. In other words, if solving B is \"easy\" (i.e. B ∈ P ), then\nsolving A is easy (A ∈ P ). Equivalently, if A is \"hard\", then B is \"hard\". Given an algorithm for\nB, we can easily construct an algorithm for A.\nNP-Hard: A decision problem D is NP-Hard if all problems Q in NP are polynomial time re\nducible to it (Q ∝ D for all Q ∈ NP ). That is, given an efficient algorithm which solves an\nNP-Hard problem D, we can construct an efficient algorithm for any problem in NP.\nNP-Complete: A decision problem D is NP-Complete if it is in NP and is NP-Hard.\n6.046J/18.410J\n\nRecitation 8: NP-Complete Problems\n2 Reducing Hamiltonian Cycle to Hamiltonian Path\nHamiltonian Cycle: Given a directed graph G = (V, E), is there a cycle that visits every vertex\nexactly once?\nHamiltonian Path Given a directed graph G = (V, E), is there a path that visits every vertex\nexactly once?\nGiven that Hamiltonian Cycle is NP-Complete, we prove that Hamiltonian Path is NP-Complete.\n1. Show Hamiltonian Path is ∈ NP\nTo prove this, we need to prove that there exists a verifier V(x, y). Let x = G be a \"yes\"\ninput. Let y be a path P that satisfies the condition.\nWe can verify that the path traverses every vertex exactly once, then check the path to ensure\nthat every edge in the path is an edge in the graph. Naively, it takes O(n2) to check that\nevery vertex is traversed exactly once and O(nm) to check that every edge in the path is in\nthe graph.\n2. Show Hamiltonian Path is ∈ NP-Hard.\nWe prove this by giving a Karp-reduction of Hamiltonian Cycle to Hamiltonian Path.\n(a) Given an instance of Hamiltonian Cycle G, choose an arbitrary node v and split it into\ntwo nodes v\" and v\"\" . All directed edges into v now have v\" as an endpoint, and all edges\nleaving v leave v\"\" instead. We call this new graph G\" . The transformation takes at most\nO(E).\n(b) If there is a Hamiltonian Cycle in G, there is a Hamiltonian Path on G\" . We can use the\n\"\nedges of the cycle on G as the path on G\" , but our path must begin on v\"\" and end on v .\n(c) If there is a Hamiltonian Path on on G\" , there is a Hamiltonian Cycle on G. The path\n\"\nmust begin at v\"\" and end at v\" , since there are no edges into v\"\" or out of v . Thus we\ncan use the path on G\" as a cycle on G once v\" and v\"\" are remerged.\n3. This proves Hamiltonian Cycle reduces to Hamiltonian Path in polynomial time, which\nmeans that Hamiltonian Path is at least as hard as Hamiltonian Cycle, so Hamiltonian Path\nis NP-Complete.\n\nRecitation 8: NP-Complete Problems\n3 Reducing 3 Dimensional Matching to 4 Partition\n3DM: Given 3 disjoint sets X, Y, Z, which each contain n elements, and triples T ⊂ X × Y × Z,\nis there a a subset S ⊂ T such that each element a ∈ X ∪ Y ∪ Z is in exactly one s ∈ S.\n4-Partition: Given n integers {a1, a2, . . . , an} is there a partition into n subsets of 4 elements\ne\neach with the same sum t =\nA/n . Each integer ai ∈ ( t , t ) (which ensures that each subset has\nexactly 4 elements).\nGiven that 3DM is NP-Complete, we prove that 4-Partition is NP-Complete.\n1. Show 4-Partition ∈ NP We can verify a potential solution by checking that the elements in\neach partition sum to t, and that no element is used more than once. This takes at most O(n2)\nnaively.\n2. Show 4-Partition ∈ NP\nWe prove this by giving a Karp-reduction of 3DM to 4-Partition.\n(a) Given an input X, Y, Z and triples T to 3DM, we create numbers in base r, where r =\ne\n100 ∗\n(X ∪ Y ∪ Z). We note that N[xi] denotes the number of times an element xi\nappears in a triple in T.\nThen we create the \"actual\" numbers as part of our set of n integers.\ni. For every element xi ∈ X: 10r4 + ir3 + 1\nii. For every element yj ∈ Y : 10r4 + jr2 + 2\niii. For every element zk ∈ Z: 10r4 + kr + 4\nWe also add the following \"dummy\" numbers to our set of n integers.\ni. For every element xi ∈ X: N[xi] - 1 copies of the number 11r4 + ir3 + 1\nii. For every element yj ∈ Y : N[yj ] - 1 copies of the number 11r4 + jr2 + 2\niii. For every element zk ∈ z: N[zk] - 1 copies of the number 8r4 + kr + 4\nFinally, we add a \"triple\" number for each triple in T .\ni. For every triple (xi, yj , zk): 10r4 - ir3 - jr2 - kr + 8\nWe set our target sum t = 40r4 +15, which can be achieved by adding a \"triple element\"\n10r4 - ir3 - jr2 - kr + 8 with three \"actual elements\" or a \"triple element\" with three\n\"dummy elements\". Additionally, we have chosen r sufficiently large that no other\ncombination of \"actual\", \"dummy\", or \"triple\" elements will meet the target sum.\n(b) If S ⊂ T is a solution to 3DM, we can construct a 4 Partition solution. For every\ntriple (xi, yj , zk) = s ∈ S, we create a 4 element set out of the corresponding \"triple\nelement\" and the three \"actual elements\" corresponding to xi, yj , zk which will sum\nto 40r4 + 15. The remaining matchings will correspond to partitions with a \"triple\"\nelement and three \"dummy\" elements corresponding to xi, yj , zk which will still sum\nto 40r4 + 15. Thus we can form sets of 4 elements, and we have a 4 Partition.\n\nRecitation 8: NP-Complete Problems\n(c) Suppose we are given a solution to 4 Partition. Then consider any 4 element set in this\n4 Partition. By considering the the sum of the element sizes modulo r, r2, r3, r4, andr5\nwe show that this set contains one element corresponding to each of the members in a\ntriple, and that all three elements are either \"actual\" elements or \"dummy\" elements.\nIf the triple contains only \"actual elements\", it is part of the 3DM solution S ⊂ T ,\notherwise it is not.\nIf B equals the sum of the four elements, we know that B mod r = 15. This is possible\nonly if the 4 elements correspond to a triple element, and one element each from X, Y,\nand Z.\nThe sum B mod r2 = 0r + 15, which is possible only if the triple element and the\ndummy or actual element for zk match, ensuring that we use a triple element and the\ncorresponding zk element in our set.\nThe sum B mod r3 = 0r2 + 15, which is possible only if the triple element and the\ndummy or actual element for yj match, ensuring that we use a triple element and the\ncorresponding yj element in our set.\nThe sum B mod r4 = 0r3 + 15, which is possible only if the triple element and the\ndummy or actual element for xi match, ensuring that we use a triple element and a xi\nelement in our set.\nThe sum B mod r5 = 40r4 + 15, which is possible only if the triple element and only\nthree dummy elments or three actual elements were used.\nThus, we can guarantee that if we have a 4 Partition, each set in the partition corre\nsponds to a 3DM matching in S or an unused matching.\n3. This proves 3DM reduces to 4-Partition in polynomial time, which means that 4-Partition is\nat least as hard as 3DM, so 4-Partition is NP-hard.\n\nRecitation 8: NP-Complete Problems\n4 Reducing Clique to Independent Set\nClique: Given graph G = (V, E) and integer k, is there a set of vertices C ⊆ V with |C| = k that\nform a complete graph?\nIndependent Set: Given graph G = (V, E) and integer k, is there a set of vertices I ⊆ V with\n|I| = k such that for any u, v ∈ I, (u, v) ∈/ E?\nGiven that Clique is NP-Complete, we prove that Independent Set is NP-complete.\n1. Show Independent Set ∈ NP\nTo prove this, we need to prove there exists a verifier V(x, y). Let x = (G, k) be a \"yes\"\ninput. Let y be I that satisfies the condition.\nIt takes O(|I|) to check that |I| = k. It takes O(|I|2) to check that for every u, v ∈ V \" ,\n(u, v) ∈/ E. This checks in polynomial time that the certificate y proves that x is a valid\ninput. Therefore, Independent Set is in NP.\n2. Show Independent Set ∈ NP-hard\nWe prove this by giving a Karp-reduction of Clique to Independent Set.\n(a) Given an input x = (G, k) to Clique, create input G \" which has the same vertices, but\nhas edge (u, v) if and only if (u, v) ∈/ E. This takes O(|E|) time, so this reduction\ntakes polynomial time.\n(b) If I is a set of vertices that form a k-Independent Set for G \", then C = I is a k-Clique\nfor G because for u, v ∈ I, Independent Set says that (u, v) ∈/ E \", but this implies that\n(u, v) ∈ E for the Clique problem due to the method of construction. This shows that\nthere are edges between every pair of nodes in C. In addition |C| = k, and so C is a\nk-clique.\n(c) If C is a set of vertices that form a k-Clique in G, then I = C is a k-Independent set\nfor G \" . This is because u, v ∈ C implies that (u, v) ∈ E for Clique, and this implies\nthat (u, v) ∈/ E \" for Independent Set. Since |I| = |C| = k, this shows that there are k\nelements in the construction G \" that are not adjacent to each other.\n3. This proves Clique reduces to Independent Set in polynomial time, which means that Inde\npendent Set is at least as hard as Clique, so k-Independent Set is NP-hard.\n\nRecitation 8: NP-Complete Problems\n5 Reducing Vertex Cover to Set Cover\nVertex Cover: Given graph G = (V, E) and integer k, does there exist Y ⊆ V such that |Y | = k\nand for each (u, v) ∈ E, either u ∈ Y or v ∈ Y (or both)?\nSet Cover: Given a set S of n elements {1, 2, . . . , n} and m sets S1, . . . , Sm where Si ⊆ S,\ndoes there exist a set of k sets Si1 , . . . , Sik such that Si1 ∪· · · ∪ Sik = S?\nGiven that Vertex Cover is NP-complete, we prove that Set Cover is NP-Complete.\n1. Show Set Cover ∈ NP: To prove this, we need to prove there exists a verifier V(x, y). Let\nx = S, S1, . . . , Sm be a \"yes\" input. Let y be Si1 , . . . , Sik that satisfies the condition.\nIn O(k) time, we can figure whether or not we have exactly k sets. In O(kn) time we\ncan determine whether or not all all elements in S is in the union. This proves we have a\npolynomial time verifier, which means that Set Cover is in NP.\n2. Show Set Cover ∈ NP-Hard: To prove this, we reduce Vertex Cover to Set Cover.\n(a) For an input x = (G, k) to Vertex Cover, we make R(x) = S, S1, . . . , Sm. Let S be\nthe set of all edges ej ∈ E. For each vi ∈ V , create set Si. This set contains the set of\nedges ej that touch vi. This new input is polynomial because |S| = |E| and each set Si\nhas size at most |E| and there are |V | sets.\n(b) If there is a k-Vertex Cover, there is a k-Set Cover. If the vertex vi ∈ V \" is part of the\nvertex cover, then Si is part of the set cover. Since every edge ej ∈ E is incident to\nsome vertex vi ∈ V \", this means that every element ej ∈ S is covered the set Si.\n(c) If there is a k-Set Cover, there is a k-Vertex Cover. If Si is in the set cover, choose vi to\nbe in the vertex cover. Every element ej is contained in some set Si. By construction,\nthis means every edge ej is incident to the vertex vi that got chosen. Since there are k\nsets, there will be k vertices chosen for the vertex cover.\n\nRecitation 8: NP-Complete Problems\n6 Prove Max2SAT is NP-Complete: Reducing from Clique\nClique(G, k): Given graph G = (V, E) and integer k, is there a set of vertices U ⊆ V with |U| ≥ k\nthat form a complete graph (k-clique)?\nMax2SAT(C, X, k): Given a CNF formula consisting of clauses C = {c1, c2, . . . cn} and lit\nerals X = {x1, x2, . . . xk} such that each clause involves exactly two literals, does there exist an\nassignment of TRUE/FALSE to the literals such that at least k clauses are satisfied?\n6.1 Show Max2SAT ∈ NP\nTo prove this, we need to prove there exists a polytime verifier. Given any \"yes\" input (CNF\nformula), the \"witness/certificate\" is an assignment of TRUE/FAlSE to the literals that satisfies at\nleast k clauses. A polytime verifier can simple evaluate each clause of the CNF formula given the\nassignment, and verify that at least k of them are satisfied.\n6.2 Show Max2SAT ∈ NP-Hard:\n1. To prove this, we reduce Clique to Max2SAT. Given an input (G, k) to Clique, we will create\nan input (C, X, k \" ) to Max2SAT such that \"yes\" instances of Clique map to \"yes\" instances\nof Max2SAT, and \"no\" instances of Clique map to \"no\" instances of Max2SAT.\n(a) In designing the CNF formula, we use literals x1, x2, . . . xn to represent the n vertices\nin the graph. We want to design clauses that act as constraints to enforce that x1 =\nTRUE corresponds to vertex 1 being chosen in the clique.\n(b) Recall that a set of vertices U is a clique if for all i, j ∈ U, (i, j) ∈ E. Equivalently (by\nthe contrapositive), U is a clique if for all i, j ∈ V such that (i, j) ∈/ E, either i /∈ U or\nj /∈ U. We will use these constraints to design the clauses in our CNF formula.\n(c) Therefore, for every \"non-edge\" (i, j) ∈/ E, we have a clause (¬xi ∨¬xj ). This means\nthat for every non-edge, at least one of the endpoints must not be in the clique.\n(d) However, these clauses could also be satisfied by setting all literals xi to FALSE. In\norder to encourage choosing cliques with more vertices, for every vertex i we introduce\nthe clauses (xi ∨ z) ∧ (xi ∧¬z), where z is a new literal. In order to satisfy these two\nclauses, xi must be true.\n(e) Choose k \" = number of non-edges + |V | + k.\nTherefore, the input to Max2SAT is defined by\n- literals X = V ∪{z},\n- clauses C = {(¬xi∨¬xj ) for all non-edges (i, j)}∪{(xi∨z)∧(xi∧¬z) for all vertices i},\n- and k \" = |non-edges| + |V | + k.\n\nRecitation 8: NP-Complete Problems\n2. Show that if Clique is \"yes\", then Max2SAT is \"yes\". Given a clique U ⊆ V in graph G\nsuch that |U| ≥ k, we can use an assigment such that for all i ∈ U, xi is TRUE, for all\ni /∈ U, xi is FALSE, and z is TRUE. This is an assigment that satisfies at least k \" clauses in\nour CNF formula. First, for all non-edges (i, j), the clause (¬xi ∨¬xj ) is satisfied because\nU is a clique. For all i ∈ U, both (xi ∨ z) and (xi ∧¬z) are satisfied. For all ı ∈/ U, (xi ∨ z)\nis satisfied while (xi ∧¬z) is not satisfied. Therefore, the number of satisfied clauses is\n|non-edges| + 2|U| + |V \\ U| = |non-edges| + |V | + |U| ≥ k \" .\n3. Show that if Max2SAT is \"yes, then Clique is \"yes\". Given an assignment for the literals\nX such that at least k \" clauses are satisfied, we will show that we can find a k-clique in the\noriginal graph. If for all (i, j) ∈/ E, (¬xi ∨¬xj ) is satisfied, then the literals that are set\nto TRUE form a clique in graph G that has size ≥ k (by construction). However, if the\ncurrent assignment does not correspond to a clique, then we will show that we can modify\nthe assigment to find a clique of size at least k. For every clause (¬xi ∨¬xj ) that is not\nsatisfied, we change xi to FALSE (doesn't matter which one we pick), thus satisfying this\nclause. This will cause one of (xi ∨ z) or (xi ∧¬z) to become not satisfied. In addition, xi\nmay also appear in other \"non-edges\" clause. Therefore, the net change is that the number\nof clauses satisfied can only increase or stay the same by this modification. We continue to\nchoose unsatisfied \"non-edge\" clauses and modifying the assignment in this way until all the\n\"non-edge\" clauses are satisfied. Note that the number of unsatisfied \"non-edge\" clauses is\nmonotonically decreasing in each modification. Therefore, we obtain an assignment which\ncorresponds to a clique (since all \"non-edge\" clauses are satisfied). In addition the clique\nhas size at least k because the number of satisfied clauses is still at least k \", since every\nmodification does not decrease the number of satisfied clauses.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class on Design and Analysis of Algorithms, Recitation 9 Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/9233649044c6d4308f0c6ffe4752b9f5_MIT6_046JS15_Recitation9.pdf",
      "content": "Design and Analysis of Algorithms\nApril 22, 2015\nMassachusetts Institute of Technology\nProfs. Erik Demaine, Srini Devadas and Nancy Lynch\nRecitation 9\nApproximation Algorithms: Traveling Salesman Problem\nIn this recitation, we will be studying the Traveling Salesman Problem (TSP): Given an undi\nrected graph G(V, E) with non-negative integer cost c(u, v) for each edge (u, v) ∈ E, find the\nHamiltonian cycle with minimum cost.\n1 Metric TSP\nTSP is an NP-complete problem, and therefore there is no known efficient solution. In fact, for\nthe general TSP problem, there is no good approximation algorithm unless P = NP . There is,\nhowever, a known 2-approximation for the metric TSP. In metric TSP, the cost function satisfies\nthe triangular inequality:\nc(u, w) ≤ c(u, v) + c(v, w)∀u, v, w ∈ V.\nThis also implies that any shortest paths satisfy the triangular inequality as well: d(u, w) ≤\nd(u, v) + d(v, w). The metric TSP is still an NP-complete problem, even with this constraint.\n2 MST Approximation Algorithm\nWhen you remove an edge from a Hamiltonian cycle, you get a spanning tree. We know how to\nfind minimum spanning trees efficiently. Using this idea, we create an approximation algorithm\nfor minimum weight Hamiltonian cycle.\nThe algorithm is as follows: Find the minimum spanning tree T of G rooted at some node r.\nLet H be the list of vertices visited in pre-order tree walk of T starting at r. Return the cycle that\nvisits the vertices in the order of H.\n2.1 Approximation Ratio\nWe will now show that the MST-based approximation is a 2-approximation for the metric TSP\nproblem. Let H∗ be the optimal Hamiltonian cycle of graph G, and let c(R) be the total weight of\nall edges in R. Furthermore, let c(S) for a list of vertices S be the total weight of the edges needed\nto visit all vertices in S in the order they appear in S.\nLemma 1 c(T ) is a lower bound of c(H∗).\nProof.\nRemoving any edge from H∗ results in a spanning tree. Thus the weight of MST must be\nsmaller than that of H∗ .\nLemma 2 c(S/) ≤ c(S) for all S/ ⊂ S.\n6.046J/18.410J\n\nRecitation 9: Approximation Algorithms: Traveling Salesman Problem\nProof.\nConsider S/ = S -{v}. WLOG, assume that vertex v was removed from a subsequence\nu, v, w of S. Then in S/, we have u → w rather than u → v → w. By triangular inequality, we\nknow that c(u, w) ≤ c(u, v) + c(v, w). Therefore c(S) is non-increasing, and c(S/) ≤ c(S) for all\nS/ ⊂ S.\nConsider the walk W performed by traversing the tree in pre-order. This walk traverses each\nedge exactly twice, meaning c(W ) = 2c(T ). We also know that removing duplicates from W\nresults in H. By Lemma 1, we know that c(T ) ≤ c(H∗). By Lemma 2, we know that c(H) ≤\nc(W ). Putting it all together, we have c(H) ≤ c(W ) = 2c(T ) ≤ 2c(H∗).\n3 Christofides Algorithm\nWe can improve on the MST algorithm by slightly modifying the MST. Define an Euler tour of a\ngraph to be a tour that visits every edge in the graph exactly once.\nAs before, find the minimum spanning tree T of G rooted at some node r. Compute the\nminimum cost perfect matching M of all the odd degree vertices, and add M to T to create T /. Let\nH be the list of vertices of Euler tour of T / with duplicate vertices removed. Return the cycle that\nvisits vertices in the order of H.\n3.1 Approximation Ratio\nWe will show that the Christofies algorithm is a 2\n3 -approximation algorithm for the metric TSP\nproblem. We first note that an Euler tour of T / = T ∪ M exists because all vertices are of even\ndegree. We now bound the cost of the matching M.\nLemma 3 c(M) ≤ 1\n2 c(H∗).\nProof.\nConsider the optimal solution H/ to the TSP of just the odd degree vertices of T . We\ncan break H/ to two perfect matchings M1 and M2 by taking every other edge. Because M is\nthe minimum cost perfect matching, we know that c(M) ≤ min(c(M1), c(M2)). Furthermore,\nbecause H/ only visits a subset of the graph, c(H/) ≤ c(H∗). Therefore, 2c(M) ≤ c(H/) ≤\nc(H∗) ⇒ c(M) ≤ 1 c(H∗).\nThe cost of Euler tour of T / is c(T ) + c(M) since it visits all edges exactly once. We know that\nc(T ) ≤ c(H∗) as before (Lemma 1). Using Lemma 3 along with Lemma 1, we get c(T )+ c(M) ≤\nc(H∗) + 2\n1 c(H∗) = 2\n3 c(H∗). Finally, removing duplicates further reduces the cost by triangular\ninequality. Therefore, c(H) ≤ c(T /) = c(T ) + c(M) ≤ 3\n2 c(H∗).\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class on Design and Analysis of Algorithms, Recitation 10 Notes",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/2218648e1afd6e487e114d451aa54e2d_MIT6_046JS15_Recitation10.pdf",
      "content": "Design and Analysis of Algorithms\nMay 1, 2015\nMassachusetts Institute of Technology\nProfs. Erik Demaine, Srini Devadas and Nancy Lynch\nRecitation 10\nDistributed Algorithms\n1 Review\nWe are going to review content from lecture that might be confusing.\nSome key ideas from lecture include:\n1. Synchronous vs. asynchronous network models (based on undirected graphs). In the syn\nchronous model all nodes move in lockstep. They all send messages, then receive messages\nand then do computation. Then, after computation they send messages again and re-start the\ncycle. In the asynchronous model any finite amount of time can pass in between rounds.\nThis finite amount of time can be different for different nodes. This allows for many pos\nsible interleavings. (Note: analysis of the time for asynchronous models tends to add some\nassumption of nodes finishing a round by time t for some t. However, this is not used for\ncorrectness. For correctness any amount of time can pass, for time analysis we need some\nkind of bound. )\n2. There are many possible ways to judge how costly a protocol is. We can count number of\nmessages, number of bits, number of rounds or do asynchronous time analysis.\n3. There were many proof methods discussed in class. E.G., invariants, probabilistic methods\nfor breaking symmetry, etc.\n4. Synchronous Leader Election: A canonical symmetry-breaking problem. In symmetric\ngraphs like the clique, this is impossible if the processes are identical and deterministic,\nbut has solutions if the processes have UIDs or can use randomness.\n5. Synchronous Maximal Independent Set: Another interesting symmetry-breaking problem.\nThe goal is to have the set of nodes that are on by the end of execution be a maximal inde\npendent set. Luby's algorithm works for this. Every round nodes randomly pick an ID from\n[1, n5]. Then every node sends to all neighbors the ID. A node that has a higher ID than\nall of its neighbors selects to be in the set. It tells all of its neighbors that it is in the MIS,\nthose neighbors are then not in the MIS. We repeat rounds after that. This will terminate in\nO(lg(n)) time.\n6. Synchronous Breadth-First Spanning Tree construction.\nSynchronous: v0 sends out a message to all neighbors. Those neighbors send out messages,\netc. Nodes will save, from the first node they get a message from that that node is their\nparent.\nThis is the simple algorithm from class. The message complexity is O(|E|).\n6.046J/18.410J\n\nRecitation 10: Distributed Algorithms\n7. Termination using convergecast. Go over this in detail.\n8. Synchronous Shortest-Paths Tree construction. Algorithm (uses relaxation). Correctness.\nAnalysis. Child pointers. Simple applications.\n9. Termination using convergecast, with corrections.\n10. Asynchronous distributed algorithms. Go over the model assumptions. Process automata,\nchannel automata, compose them. Max computation example.\n11. Asynchronous BFS tree construction. What goes wrong if we just run the synchronous\nalgorithm asynchronously. A correct algorithm (uses relaxation). Correctness, analysis.\nTermination using convergecast.\n12. Asynchronous SP tree construction. Algorithm (with lots of relaxation). Analysis. Surpris\ning back worst-case example.\n2 New Questions\n2.1 Leader election in a ring with UIDs\nAssume that the processes have UIDs and a consistent sense of direction (ports labeled \"left\"\n(clockwise) or \"right\"(counterclockwise)).\nConsider the synchronous setting first.\nSimple algorithm: Everyone sends their UID clockwise. When they receive their own back,\nthey know they have received all the UIDs. Then the largest UID process can declare itself the\nleader.\nTime: n Messages: n2\nQ: What if we want to save messages?\nIdea: Throw away any arriving message that is less than your own, or less than the maximum\nyou ever saw. But these don't help the order of magnitude in the worst case. Construct an example.\nQ: Can we beat O(n2)?\nHirshberg-Sinclair is one solution, involving searching to successively doubled distances. Specif\nically, each process does the following.\n- Send out a message with its UID and a message hop count h and direction d, (UID, h, d).\n- If you receive another nodes message (UID, h, d) forward (UID, h - 1, d) in the direction\nd only if the hop count is greater than 0 and its UID is greater than or equal to any other UID\nyou have seen before. Whenever the message is forwarded, its hop count is decremented by\none. If it is greater than any other UID you have seen and the hop count is 0, forward it in\nthe opposite direction of d.\n- If a node doesn't receive a message it sent out back, then it sends no new messages.\n\nRecitation 10: Distributed Algorithms\n- If a node receives its (UID, h, left) message on its right and its (UID, h, right) message\non its left then these messages made it all the way around the circle and thus the UID is the\nlargest in the circle. This node becomes the leader.\nHow many messages does this send in the worst case? By the end of the round where the\nhop lengths are 2r there can be at most n/2r nodes that still send messages. Each node sending\nmessages in round r will have hop counts of 2r resulting in at most 2r+1 messages sent. Then the\ntotal number of messages can take\nlg(\\\nn)\n2i+2n/2i = 4n lg(n).\ni=0\nThis is a big improvement! O(n lg(n)) instead of O(n2).\nQ: What about an asynchronous ring?\nThe above still works.\n2.2 An asynchronous algorithm for counting the nodes\nAssume that the graph has at least two nodes. Assume a root process i0 at graph vertex v0.\nQ: Give a simple algorithm that allows process i0 to compute the total number of nodes in the\nnetwork.\nSet up a spanning tree, e.g., using the simple synchronous spanning tree run asynchronously.\nIn the asynchronous setting, this produces some spanning tree, which is not guaranteed to produce\nthe BFS spanning tree. But it is OK for our purpose here (counting the nodes.) (This also won't\nhave the optimal time complexity, because of the timing anomaly). Finally, add a convergecast that\nalso accumulates the sum on the way up.\nWork with the class to develop the code. Do this in three stages: 1. The asynchronous (non-BF)\nspanning tree code from the lecture slides. 2. Add actions for child pointers. 3. Then add actions\nfor convergecasting the number of nodes.\nAnalyze time cost: O(|E|).\nAll the processes will use the strategy of putting messages into FIFO send queues for their\nneighbors, and then have separate output actions that actually send the messages, starting from the\nheads of the queues. Thus, all processes will have the following transition definition. We will omit\nthem later.\noutput send(m)u,v, m a message, v ∈ Γ(u)\nPrecondition: m = head(send(v))\nEffect: remove head of send(v)\n2.2.1 Part 1: Setting up a tree\nFirst search message becomes your parent. Pass search messages on.\n\nRecitation 10: Distributed Algorithms\nProcess v0\nState variables:\nfor each v ∈ Γ(v0), send(v), a queue, initially (search)\nTransitions:\ninput receive(search)v,v0 , v ∈ Γ(v0)\nEffect: none\nProcess u, u = v0\nState variables:\nparent ∈ Γ(u) ∪ {⊥}, initially ⊥\nfor each v ∈ Γ(u), send(v), a queue, initially empty\nTransitions:\ninput receive(search)v,u, v ∈ Γ(u)\nEffect:\nif parent = ⊥ then\nparent := v\nfor each v ∈ Γ(u), add search to send(v)\n2.2.2 Part 2: Adding child pointers\nNow send parent(true) or parent(false) responses. Keep track of who has responded, and which of\nthese have responded parent(true)--the latter are the children.\nProcess v0\nState variables:\nresponded ⊆ Γ(v0), initially ∅\nchildren ⊆ Γ(v0), initially ∅\nfor each v ∈ Γ(v0), send(v), a queue, initially (search)\nTransitions:\ninput receive(search)v,v0 , v ∈ Γ(v0)\nEffect: add parent(false) to send(v)\n\nRecitation 10: Distributed Algorithms\ninput receive(parent(b))v,v0 , b a Boolean, v ∈ Γ(v0)\nEffect:\nif b then\nchildren := children ∪{v}\nresponded := responded ∪{v}\nProcess u, u = v0\nState variables:\nparent ∈ Γ(u) ∪ {⊥}, initially ⊥\nresponded ⊆ Γ(u), initially ∅\nchildren ⊆ Γ(u), initially ∅\nfor each v ∈ Γ(u), send(v), a queue, initially empty\nTransitions:\ninput receive(search)v,u, v ∈ Γ(u)\nEffect:\nif parent = ⊥ then\nparent := v\nadd parent(true) to send(v)\nelse add parent(false) to send(v)\ninput receive(parent(b))v,u, b a Boolean, v ∈ Γ(u)\nEffect:\nif b then\nchildren := children ∪{v}\nresponded := responded ∪{v}\n2.2.3 Part 3: Adding the convergecast for the count\nNow convergecast the count up the tree.\nProcess v0\nState variables:\nresponded ⊆ Γ(v0), initially ∅\nchildren ⊆ Γ(v0), initially ∅\ndone ⊆ Γ(v0), initially ∅\ntotal, a nonnegative integer, initially 0\nfor each v ∈ Γ(v0), send(v), a queue, initially (search)\n\nRecitation 10: Distributed Algorithms\nTransitions:\ninput receive(search)v,v0 , v ∈ Γ(v0)\nEffect: add parent(false) to send(v)\ninput receive(parent(b))v,v0 , b a Boolean, v ∈ Γ(v0)\nif b then\nchildren := children ∪{v}\nresponded := responded ∪{v}\ninput receive(done(k))v,v0 , k a nonnegative integer, v ∈ Γ(v0)\nEffect:\ndone := done ∪{v}\ntotal := total +k\nif done = Γ(v0) then (the final output is the value in total).\nProcess u, u v0\nState variables:\nparent ∈ Γ(u) ∪ {⊥}, initially ⊥\nresponded ⊆ Γ(u), initially ∅\nchildren ⊆ Γ(u), initially ∅\ndone ⊆ Γ(u), initially ∅\ntotal, a nonnegative integer, initially 0\nfor each v ∈ Γ(u), send(v), a queue, initially empty\nTransitions:\ninput receive(search)v,u, v ∈ Γ(u)\nEffect:\nif parent = ⊥ then\nparent := v\nadd parent(true) to send(v)\nelse add parent(false) to send(v)\ninput receive(parent(b))v,u, b a Boolean, v ∈ Γ(u)\nEffect:\nif b then\nchildren := children ∪{v}\nresponded := responded ∪{v}\nif responded = Γ(u) and children = ∅ then\n=\n\nRecitation 10: Distributed Algorithms\nadd done(1) to send(parent)\ninput receive(done(k))v,u, k a nonnegative integer, v ∈ Γ(u)\nEffect:\ndone := done ∪{v}\ntotal := total +k\nif responded = Γ(u) and done = children then\nadd done(total + 1) to send(parent)\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n6.046J / 18.410J Design and Analysis of Algorithms\nSpring 2015\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    }
  ]
}