{
  "course_name": "Computational Cognitive Science",
  "course_description": "This course is an introduction to computational theories of human cognition. Drawing on formal models from classic and contemporary artificial intelligence, students will explore fundamental issues in human knowledge representation, inductive learning and reasoning. What are the forms that our knowledge of the world takes? What are the inductive principles that allow us to acquire new knowledge from the interaction of prior knowledge with observed data? What kinds of data must be available to human learners, and what kinds of innate knowledge (if any) must they have?",
  "topics": [
    "Engineering",
    "Systems Engineering",
    "Computational Science and Engineering",
    "Science",
    "Cognitive Science",
    "Engineering",
    "Systems Engineering",
    "Computational Science and Engineering",
    "Science",
    "Cognitive Science"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nOverview\n\nThis course is an introduction to computational theories of human cognition. Drawing on formal models from classic and contemporary artificial intelligence, we will explore fundamental issues in human knowledge representation, inductive learning and reasoning. What are the forms that our knowledge of the world takes? What are the inductive principles that allow us to acquire new knowledge from the interaction of prior knowledge with observed data? What kinds of data must be available to human learners, and what kinds of innate knowledge (if any) must they have? Class sessions will comprise a mixture of lectures and discussion. Readings will include seminal and state-of-the-art research papers from the cognitive, AI, and machine learning literatures, as well as textbook chapters and tutorials on technical approaches. Assignments will consist of several problem sets and a final modeling project or paper.\n\nWe will cover a range of formal modeling approaches and their applications to understanding core areas of cognition. Cognitive science topics will include:\n\nConcept Learning and Categorization\n\nReasoning about Natural Kinds\n\nLearning Causal Relations\n\nThe Structure and Formation of Intuitive Theories of Physical, Biological and Social Systems\n\nThe Acquisition of Natural Language (syntax and semantics)\n\nTheory of Mind: How we Understand the Behavior and Mental States of Other People\n\nFormal modeling topics will include:\n\nBayesian Inference and Hierarchical Bayesian Models\n\nFrameworks for Knowledge Representation: First-order Logic, Formal Grammars, Associative Networks, Taxonomic Hierarchies, Relational Schemas\n\nProbabilistic and Causal Graphical Models\n\nRelational Probabilistic Models\n\nControlling Complexity: Minimum Description Length, Bayesian Occam's Razor, Nonparametric Bayesian Models\n\nInductive Logic Programming\n\nSampling Algorithms for Inference in Complex Probabilistic Models\n\nThe syllabus will balance presentations of state-of-the-art material with a broad historical perspective. Depth of presentation will vary across topics, from brief overviews in some areas to more technical and detailed coverage in others.\n\nPrerequisites\n\nThe pre-requisite is a class in probability or statistics (e.g.,\n9.07\n, Statistical Methods in Brain and Cognitive Science,\n18.05\n, Introduction to Probability and Statistics,\n6.041\n, Probabilistic Systems Analysis and Applied Probability). A class in artificial intelligence or machine learning would be helpful but is not necessary, as the relevant material will be reviewed in this class. Experience in programming (particularly in a high-level language such as MATLAB(r)) is desirable.\n\nReadings\n\nThere is no single required text for this class. Russell, Stuart J., and Peter Norvig.\nArtificial Intelligence: A Modern Approach.\n2nd ed. Upper Saddle River, N.J.: Prentice Hall/Pearson Education, 2003. ISBN: 0137903952, is strongly recommended as background reading on relevant formal models. Readings will consist of papers from the cognitive literature and background material from AIMA and several texts and tutorials in machine learning.\n\nDiscussion Board\n\nShort (≈ 1 paragraph) responses to the readings or assigned questions are due by 10 am on the day of class. You should post these directly to the MIT server discussion board.\n\nYou must submit 20 notes for full credit, with no more than two posts counting in any one week. These can include short responses to other peoples' posts, as long as the responses are thoughtful and in some way address the assigned readings and questions.\n\nAssignments\n\nFour Problem Sets (involving minimal programming in MATLAB(r) or another high-level programming language)\n\nA Final Project or Paper on Cognitive Modeling\n\nFor Graduate Students: At the level of a short conference paper\n\nFor Undergraduates: Critically discuss, or implement and extend an existing\n\nmodel\n\nParticipation in Class and on the Web-based Discussion Board\n\nGrading\n\nThe grade distribution is approximately 40% problem sets, 40% project, and 20% discussion/participation.",
  "files": [
    {
      "category": "Resource",
      "title": "dec_2_2004_final.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-66j-computational-cognitive-science-fall-2004/844d8ae30bc87145bf798611983c6a9f_dec_2_2004_final.pdf",
      "content": "Approaches to structure learning\n- Constraint-based learning (Pearl, Glymour, Gopnik):\n- Assume structure is unknown, no knowledge of\nparameterization or parameters\n- Bayesian learning (Heckerman, Friedman/Koller):\n- Assume structure is unknown, arbitrary parameterization.\n- Theory-based Bayesian inference (T & G):\n- Assume structure is partially unknown, parameterization is\nknown but parameters may not be. Prior knowledge about\nstructure and parameterization depends on domain theories\n(derived from ontology and mechanisms).\n\nAdvantages/Disadvantages of the\nconstraint-based approach\n- Deductive\n- Domain-general\n- No essential role for domain knowledge:\n- Knowledge of possible causal structures not\nneeded.\n- Knowledge of possible causal mechanisms not\nused.\n- Requires large sample sizes to make reliable\ninferences.\n\nThe Blicket detector\nGopnick, A., and D. M. Sobel. \"Detecting Blickets: How Young\nChildren use Information about Novel Causal Powers in Categorization\nand Induction.\" Child Development 71 (2000): 1205-1222.\nImage removed due to copyright considerations. Please see:\n\nGopnick, A., and D. M. Sobel. \"Detecting Blickets: How Young\nChildren use Information about Novel Causal Powers in Categorization\nand Induction.\" Child Development 71 (2000): 1205-1222.\nImage removed due to copyright considerations. Please see:\n\nThe Blicket detector\n- Can we explain these inferences using\nconstraint-based learning?\n- What other explanations can we come up\nwith?\n\nConstraint-based model\n-\nData:\n-\nd0: A=0, B=0, E=0\n-\nd1: A=1, B=1, E=1\n-\nd2: A=1, B=0, E=1\n-\nConstraints:\n-\nA, B not independent\n-\nA, E not independent\n-\nB, E not independent\n-\nB, E independent conditional on the presence of A\n-\nA, E not independent conditional on the absence of B\n-\nUnknown whether B, E independent conditional on the absence of A.\n-\nGraph structures consistent with constraints:\nGopnick, A., and D. M. Sobel. \"Detecting Blickets: How Young\nChildren use Information about Novel Causal Powers in\nCategorization and Induction.\" Child Development 71 (2000):\n1205-1222.\nE\nA\nB\nE\nA\nB\nNOTE: Also have A, B independent conditional on the presence of\nE. Does that eliminate the hypothesis that B is a blicket?\nImage removed due to copyright considerations. Please see:\n\nConstraint-based inference\n- Data:\n- d1: A=1, B=1, E=1\n- d2: A=1, B=0, E=1\n- d0: A=0, B=0, E=0\n- Conditional independence constraints:\n- B, E independent conditional on A\n- B, A independent conditional on E\n- A, E correlated, unconditionally or conditional on B\n- Inferred causal structure:\n- B is not a blicket.\n- A is a blicket.\nImagine sample sizes\nmultiplied by 100....\n(Gopnik, Glymour et al., 2002)\nE\nA\nB\n\nWhy not use constraint-based\nmethods + fictional sample sizes?\n- No degrees of confidence.\n- No principled interaction between data and\nprior knowledge.\n- Reliability becomes questionable.\n- \"The prospect of being able to do psychological\nresearch without recruiting more than 3 subjects\nis so attractive that we know there must be a\ncatch in it.\"\n\nA deductive inference?\n- Causal law: detector activates if and only if\none or more objects on top of it are blickets.\n- Premises:\n- Trial 1: A B on detector - detector active\n- Trial 2: A on detector - detector active\n- Conclusions deduced from premises and\ncausal law:\n- A: a blicket\n- B: can't tell (Occam's razor not a blicket?)\n\nWhat kind of Occam's razor?\n- Classical all-or-none form:\n- \"Causes should not be multiplied without\nnecessity.\"\n- Constraint-based: faithfulness\n- Bayesian: probability\n\nFor next time\n- Come up with slides on Theory-based\nBayesian causal inference.\n- Combine current teaching slides, which\nemphasize Bayes versus constraint-based,\nwith Leuven slides, which emphasize a\nsystematic development of the theory.\n- Incorporate (if time) cross-domains, plus\nAB-AC.\n\nApproaches to structure learning\n- Constraint-based learning (Pearl, Glymour, Gopnik):\n- Assume structure is unknown, no knowledge of\nparameterization or parameters\n- Bayesian learning (Heckerman, Friedman/Koller):\n- Assume structure is unknown, arbitrary parameterization.\n- Theory-based Bayesian inference (T & G):\n- Assume structure is partially unknown, parameterization is\nknown but parameters may not be. Prior knowledge about\nstructure and parameterization depends on domain theories\n(derived from ontology and mechanisms).\n\nFor next year\n- Include deductive causal reasoning as one\nof the methods. It goes back a long time....\n\nCritical differences between Bayesian\nand Constraint-based learning\n- Basis for inferences:\n- Constraint-based inference based on just\nqualitative independence constraints.\n- Bayesian inference based on full probabilistic\nmodels (generated by domain theory).\n- Nature of inferences:\n- Constraint-based inferences are deductive.\n- Bayesian inferences are probabilistic.\n\nBayesian causal inference\nData X\nCausal hypotheses h\nBayes:\nA\nB\nC\nD\nE\n,1\n,0\n,1\n,1\n,0\n,1\n,0\n,0\n,1\n,0\n,1\n,1\n,1\n,1\n,1\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\nE\nC\nx\nE\nB\nA\nx\nE\nD\nC\nB\nA\nx\nE\nD\nC\nB\nA\nx\nE\nD\nC\nB\nA\nx\nA\nB\nC\nD\nE\n)\n(\n)\n|\n(\n)\n|\n(\nh\nP\nh\nX\nP\nX\nh\nP\n∝\n\nWhy be Bayesian?\n- Explain how people can reliably acquire\ntrue causal beliefs given very limited data:\n- Prior causal knowledge: Domain theory\n- Causal inference procedure: Bayes\n- Understand how symbolic domain theory\ninteracts with rational statistical inference:\n- Theory generates the hypothesis space of\ncandidate causal structures.\n\nRole of domain theory\n- Determines prior over models, P(h)\n- Causally relevant attributes of objects and\nrelations between objects: variables\n- Viable causal relations: edges\n- Determines likelihood function for each\nmodel, P(X|h), via (perhaps abstract or\n\"light\") mechanism knowledge:\n- How each effect depends functionally on its\ncauses:\n])\n[\nparents\n|\n(\nV\nV\nP\n])\nparents[\n(\nV\nf\nV\nθ\n⇐\n\nBayesian causal inference\nData X\nCausal hypotheses h\nBayes:\nA\nB\nC\nD\nE\n,1\n,0\n,1\n,1\n,0\n,1\n,0\n,0\n,1\n,0\n,1\n,1\n,1\n,1\n,1\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\nE\nC\nx\nE\nB\nA\nx\nE\nD\nC\nB\nA\nx\nE\nD\nC\nB\nA\nx\nE\nD\nC\nB\nA\nx\nA\nB\nC\nD\nE\n)\n(\n)\n|\n(\n)\n|\n(\nh\nP\nh\nX\nP\nX\nh\nP\n∝\n∏\n∈\n=\n}\n,\n,\n,\n,\n{\n])\n[\nparents\n|\n(\n)\nmodel\n\ncausal\n|\n,\n,\n,\n,\n(\nE\nD\nC\nB\nA\nV\nV\nV\nP\nE\nD\nC\nB\nA\nP\n\n(Bottom-up) Bayesian causal\nlearning in AI\n- Typical goal is data mining, with no strong\ndomain theory.\n- Uninformative prior over models P(h)\n- Arbitrary parameterization (because no\nknowledge of mechanism), with no strong\nexpectations of likelihoods P(X|h).\n- Results not that different from constraint-\nbased approaches, other than more precise\nprobabilistic representation of uncertainty.\n\n\"Backwards blocking\"\n(Sobel, Tenenbaum & Gopnik, 2004)\n- Two objects: A and B\n- Trial 1: A B on detector - detector active\n- Trial 2: A on detector - detector active\n- 4-year-olds judge whether each object is a blicket\n- A: a blicket (100% of judgments)\n- B: probably not a blicket (66% of judgments)\nGopnick, A., and D. M. Sobel. \"Detecting Blickets: How Young\nChildren use Information about Novel Causal Powers in Categorization\nand Induction.\" Child Development 71 (2000): 1205-1222.\nImage removed due to copyright considerations. Please see:\n\nTheory\n- Ontology\n- Types: Block, Detector, Trial\n- Predicates:\nContact(Block, Detector, Trial)\nActive(Detector, Trial)\n- Constraints on causal relations\n- For any Block b and Detector d, with probability q :\nCause(Contact(b,d,t), Active(d,t))\n- Functional form of causal relations\n- Causes of Active(d,t) are independent mechanisms, with\ncausal strengths wi. A background cause has strength w0.\nAssume a near-deterministic mechanism: wi ~ 1, w0 ~ 0.\n\nTheory\n- Ontology\n- Types: Block, Detector, Trial\n- Predicates:\nContact(Block, Detector, Trial)\nActive(Detector, Trial)\nE\nA\nB\n\nTheory\n- Ontology\n- Types: Block, Detector, Trial\n- Predicates:\nContact(Block, Detector, Trial)\nActive(Detector, Trial)\nB\nA\nE\nA = 1 if Contact(block A, detector, trial), else 0\nB = 1 if Contact(block B, detector, trial), else 0\nE = 1 if Active(detector, trial), else 0\n\nTheory\n- Constraints on causal relations\n- For any Block b and Detector d, with probability q :\nCause(Contact(b,d,t), Active(d,t))\nP(h00) = (1 - q)2\nP(h10) = q(1 - q)\nh00 : h10 :\nh01 : h11 :\nE\nA\nB\nE\nA\nB\nE\nA\nB\nE\nA\nB\nP(h01) = (1 - q) q\nP(h11) = q2\nNo hypotheses with\nE B, E A,\nA B, etc.\n= \"A is a blicket\"\nE\nA\n\nTheory\n- Functional form of causal relations\n- Causes of Active(d,t) are independent mechanisms, with\ncausal strengths wb. A background cause has strength w0.\nAssume a near-deterministic mechanism: wb ~ 1, w0 ~ 0.\nP(h00) = (1 - q)2\nP(h10) = q(1 - q)\nP(h01) = (1 - q) q\nP(h11) = q2\nA\nB\nE\nB\nA\nE\nB\nA\nE\nB\nA\nE\nP(E=1 | A=0, B=0): 0 0 0\nP(E=1 | A=1, B=0): 0 0 1\nP(E=1 | A=0, B=1): 0 1 0\nP(E=1 | A=1, B=1): 0 1 1\n\"Activation law\": E=1 if and only if A=1 or B=1.\n\nTheory\n- Functional form of causal relations\n- Causes of Active(d,t) are independent mechanisms, with\ncausal strengths wb. A background cause has strength w0.\nAssume a near-deterministic mechanism: wb ~ 1, w0 ~ 0.\nP(E=1 | A=0, B=0): w0 w0 w0 w0\nP(E=1 | A=1, B=0): w0\nw0\nwb + (1 - wb) w0\nwb + (1 - wb) w0\nP(E=1 | A=0, B=1): w0\nwb + (1 - wb) w0\nw0\nwb + (1 - wb) w0\nP(E=1 | A=1, B=1): w0\nwb + (1 - wb) w0 wb + (1 - wb) w0 1 - (1 - wb)2 (1 - wo)\nE\nB\nA\nwb\nE\nB\nwb\nA\nwb\nE\nB\nA\nwb\nE\nB\nA\nP(h00) = (1 - q)2\nP(h10) = q(1 - q)\nP(h01) = (1 - q) q\nP(h11) = q2\n\"Noisy-OR law\"\n\nBayesian inference\n- Evaluating causal network hypotheses in\nlight of data:\n- Inferring a particular causal relation:\n∑\n∈\n=\nH\nj\nh\nj\nj\ni\ni\ni\nh\nP\nh\nd\nP\nh\nP\nh\nd\nP\nd\nh\nP\n)\n(\n)\n|\n(\n)\n(\n)\n|\n(\n)\n|\n(\n∑\n∈\n→\n=\n→\nH\nj\nh\nj\nj\nd\nh\nP\nh\nE\nA\nP\nd\nE\nA\nP\n)\n|\n(\n)\n|\n(\n)\n|\n(\n\nModeling backwards blocking\nP(h00) = (1 - q)2\nP(h10) = q(1 - q)\nP(h01) = (1 - q) q\nP(h11) = q2\nA\nB\nE\nB\nA\nE\nB\nA\nE\nB\nA\nE\nP(E=1 | A=0, B=0): 0 0 0\nP(E=1 | A=1, B=0): 0 0 1\nP(E=1 | A=0, B=1): 0 1 0\nP(E=1 | A=1, B=1): 0 1 1\nq\nq\nh\nP\nh\nP\nh\nP\nh\nP\nd\nE\nB\nP\nd\nE\nB\nP\n-\n=\n+\n+\n=\n→\n)\n(\n)\n(\n)\n(\n)\n(\n)\n|\n(\n)\n|\n(\n\nModeling backwards blocking\nq\nh\nP\nh\nP\nh\nP\nd\nE\nB\nP\nd\nE\nB\nP\n-\n=\n+\n=\n→\n)\n(\n)\n(\n)\n(\n)\n|\n(\n)\n|\n(\nP(E=1 | A=1, B=1): 0 1 1\nE\nB\nA\nE\nB\nA\nE\nB\nA\nE\nB\nA\nP(h00) = (1 - q)2\nP(h10) = q(1 - q)\nP(h01) = (1 - q) q\nP(h11) = q2\n\nModeling backwards blocking\nP(E=1 | A=1, B=0): 0 1 1\nP(E=1 | A=1, B=1): 1 1 1\nE\nB\nA\nE\nB\nA\nE\nB\nA\nP(h10) = q(1 - q)\nP(h01) = (1 - q) q\nP(h11) = q2\nq\nq\nh\nP\nh\nP\nd\nE\nB\nP\nd\nE\nB\nP\n-\n=\n=\n→\n)\n(\n)\n(\n)\n|\n(\n)\n|\n(\n\nAfter each trial, adults judge the probability that each\nobject is a blicket.\nTrial 1\nTrial 2\nB\nA\nI. Pre-training phase: Blickets are rare . . . .\nII. Backwards blocking phase:\nManipulating the prior\n\n- \"Rare\" condition: First observe 12 objects\non detector, of which 2 set it off.\nFigure by MIT OCW.\n1 AB\nAB\nA\nB\nBaseline\nAfter AB trial\nAfter A trial\nPEOPLE\n(N = 12)\nBAYES\n\n- \"Common\" condition: First observe 12\nobjects on detector, of which 10 set it off.\nFigure by MIT OCW.\nAB\nAB\nA\nB\nBaseline\nAfter AB trial\nAfter A trial\nPEOPLE\n(N = 12)\nBAYES\n\nManipulating the priors of\n4-year-olds\n(Sobel, Tenenbaum & Gopnik, 2004)\nI. Pre-training phase: Blickets are rare.\nTrial 1\nTrial 2\nB\nA\nII. Backwards blocking phase:\nRare condition:\nA: 100% say \"a blicket\"\nB: 25% say \"a blicket\"\nCommon condition:\nA: 100% say \"a blicket\"\nB: 81% say \"a blicket\"\n\nInferences from ambiguous data\nI. Pre-training phase: Blickets are rare . . . .\nTrial 1\nTrial 2\nB\nA\nII. Two trials: A B detector, B C detector\nC\nAfter each trial, adults judge the probability that each\nobject is a blicket.\n\nSame domain theory generates hypothesis\nspace for 3 objects:\n- Hypotheses: h000 = h100 =\nh010 = h001 =\nh110 = h011 =\nh101 = h111 =\n- Likelihoods:\nE\nA\nB\nC\nE\nA\nB\nC\nE\nA\nB\nC\nE\nA\nB\nC\nE\nA\nB\nC\nE\nA\nB\nC\nE\nA\nB\nC\nE\nA\nB\nC\nif A = 1 and A\nE exists,\nor B = 1 and B\nE exists,\nor C = 1 and C\nE exists,\nelse 0.\nP(E=1| A, B, C; h) = 1\n\n- \"Rare\" condition: First observe 12 objects\non detector, of which 2 set it off.\nFigure by MIT OCW.\nPEOPLE\n(N = 20)\nBAYES\nABC\nAB\nA\nBC\nBaseline\nAfter AB trial\nAfter AC trial\nC\n\nAmbiguous data with 4-year-olds\nI. Pre-training phase: Blickets are rare.\nTrial 1\nTrial 2\nB\nA\nII. Two trials: A B detector, B C detector\nC\nFinal judgments:\nA: 87% say \"a blicket\"\nB or C: 56% say \"a blicket\"\n\nFinal judgments:\nA: 87% say \"a blicket\"\nB or C: 56% say \"a blicket\"\nTrial 1\nTrial 2\nB\nA\nI. Pre-training phase: Blickets are rare.\nII. Two trials: A B detector, B C detector\nAmbiguous data with 4-year-olds\nC\nBackwards blocking (rare)\nA: 100% say \"a blicket\"\nB: 25% say \"a blicket\"\n\nThe role of causal mechanism\nknowledge\n- Is mechanism knowledge necessary?\n- Constraint-based learning using χ2 tests of\nconditional independence.\n- How important is the deterministic functional\nform of causal relations?\n- Bayes with \"probabilistic independent generative\ncauses\" theory (i.e., noisy-OR parameterization\nwith unknown strength parameters; c.f., Cheng's\ncausal power).\n\nBayes with correct theory:\nIndependence test with fictional sample sizes:\nFigure by MIT OCW.\nFigure by MIT OCW.\nAB\nAB\nA\nB\nAB\nAB\nA\nB\nABC\nAB\nA\nC\nBC\nBaseline\nAfter AC trial\nAfter AB trial\nPEOPLE (N=12)\nBAYES\nPEOPLE (N=12)\nBAYES\nPEOPLE (N=20)\nBAYES\nAB\nAB\nA\nB\nAB\nAB\nA\nB\nABC\nAB\nA\nC\nBC\nBaseline\nAfter AC trial\nAfter AB trial\n\nBayes with correct theory:\nBayes with \"noisy sufficient causes\" theory:\nFigure by MIT OCW.\nAB\nAB\nA\nB\nAB\nAB\nA\nB\nABC\nAB\nA\nC\nBC\nBasline\nAfter AC trial\nAfter AB trial\nFigure by MIT OCW.\nAB\nAB\nA\nB\nAB\nAB\nA\nB\nABC\nAB\nA\nC\nBC\nBaseline\nAfter AC trial\nAfter AB trial\nPEOPLE (N=12)\nBAYES\nPEOPLE (N=12)\nBAYES\nPEOPLE (N=20)\nBAYES\n\nBlicket studies: summary\n- Theory-based Bayesian approach explains\none-shot causal inferences in physical\nsystems.\n- Captures a spectrum of inference:\n- Unambiguous data: adults and children make\nall-or-none inferences\n- Ambiguous data: adults and children make\nmore graded inferences\n- Extends to more complex cases with hidden\nvariables, dynamic systems, ...."
    },
    {
      "category": "Resource",
      "title": "nov_2_2004_final.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-66j-computational-cognitive-science-fall-2004/dccb70206f5533197431b697861f7327_nov_2_2004_final.pdf",
      "content": "Outline\n- Non-parametric models for categorization:\nexemplars, neural networks\n- Controlling complexity in statistical models\n\nBayesian classification\nImage removed due to copyright considerations.\nThe task: Observe x generated from c1 or c2, compute:\n)\n(\n)\n|\n(\n)\n(\n)\n|\n(\n)\n(\n)\n|\n(\n)\n|\n(\nc\np\nc\nx\np\nc\np\nc\nx\np\nc\np\nc\nx\np\nx\nc\np\n+\n=\nDifferent approaches vary in how they represent p(x|cj).\n\nNon-parametric approaches\n- Allow more complex form for p(x|cj), to be\ndetermined by the data.\n- E.g., kernel density estimation:\n∑\n=\n-\n-\n∝\nn\nk\nx\nx\nj\nk\ne\nn\nc\nx\np\n)\n/(\n||\n||\n)\n|\n(\nσ\nImage removed due to\ncopyright considerations.\n- Equivalent to exemplar model\n- Observe n examples: x1, ..., xk\n- Smoothness (\"specificity\"): σ\n\n- Equivalent to exemplar model\n- Observe n examples: x1, ..., xk\n- Smoothness (\"specificity\"):\n∑\n=\n-\n-\n∝\nn\nk\nx\nx\nj\nk\ne\nn\nc\nx\np\n)\n/(\n||\n||\n)\n|\n(\nσ\nImage removed due to\ncopyright considerations.\nσ\n- Learning: Bayesian framework\n- Hypothesis space is all smooth density functions f.\n- Maximize p( f | x1, ..., xk).\n- Prior p( f ) favors larger .\n- Likelihood p(x1, ..., xk | f ) favors smaller .\nσ\nσ\n\nNearest neighbor classification\nImage removed due to copyright considerations.\nTheorem: In the limit of infinite data, classification according\nto nearest neighbor is approximately Bayes-optimal.\n\nNosofsky's exemplar model\n- Motivating example:\nBefore learning\nAfter learning:\nselective\nattention\n\nNosofsky's exemplar model\n- Math:\n- Probability of responding category J to\nstimulus i:\n- Similarity of stimulus i to exemplar j:\n- Distance from stimulus i to exemplar j:\nImage removed due to copyright considerations.\nImage removed due to copyright considerations.\nImage removed due to copyright considerations.\nwm: attentional weight for dimension m ~ inverse variance\n\nConcept learning experiments\n- Simple artificial stimuli, e.g.\n- Learn to discriminate members of two mutually\nexclusive categories, with repeated presentations of a\nsmall set of stimuli.\n\nSix types of classifications:\nImage removed due to copyright considerations.\n\nAccurate fits to human data\nImage removed due to copyright considerations.\n\nHow are attentional weights\ndetermined?\n- By the modeler: tune to fit behavioral data\n\nHow are attentional weights\ndetermined?\n- By the modeler: tune to fit behavioral data\n- By the learner: discriminative learning\n- Maximize discriminability\nof the training set.\nImage removed due to copyright considerations.\n\nGenerative vs. Discriminative\nModels\n- Generative approach:\n- Separately model class-conditional densities\np(x | cj) and priors p(cj).\n- Use Bayes' rule to compute posterior probabilities:\n- Discriminative approach:\n- Directly model posterior probabilities p(cj | x)\n)\n(\n)\n|\n(\n)\n(\n)\n|\n(\n)\n(\n)\n|\n(\n)\n|\n(\nc\np\nc\nx\np\nc\np\nc\nx\np\nc\np\nc\nx\np\nx\nc\np\n+\n=\n\nGenerative vs. Discriminative\nImage removed due to copyright considerations.\n\nDiscriminative methods based on\nfunction approximation\n- Perceptrons\n- Neural networks\n- Support vector machines\nImage removed due to copyright considerations.\n\nDiscriminative methods based on\nfunction approximation\nx1\nx2\ny\nw2\nw1\n- Perceptrons\n)\n(\nx\nw\nx\nw\ny\n+\n=θ\nImage removed due to copyright considerations.\n))\nexp(\n/(\n)\n(\nz\nz\n-\n+\n=\nθ\n\nWeight space\nImage removed due to copyright considerations.\n\nThe perceptron hypothesis space\n- Linearly separable classes\nImage removed due to copyright considerations.\n\nPerceptron learning\n- Gradient descent on error surface in weight\nspace:\nImage removed due to copyright considerations.\n\nPerceptron learning\n- Gradient descent on error surface in weight\nspace:\n)\n(∑\n=\nj\nj\njx\nw\ny\nθ\ny\ny\nError\n-\n=\n*\n1 Error\nE =\nj\nj\nw\nError\nError\nw\nE\n∂\n∂\n×\n=\n∂\n∂\nj\nw\ny\nError\n∂\n∂\n×\n-\n=\nj\nj\nj\nw\nE\nw\nw\n∂\n∂\n×\n-\n←\nα\noutput\ncorrect\n\n* =\ny\n)\n(\n)\n( *\n∑\n′\n×\n×\n-\n-\n=\nj\nj\nj\nj\nx\nw\nx\ny\ny\nθ\nj\nj\nj\nj\nx\nx\nw\nError\n×\n′\n×\n-\n=\n∑\n)\n(\nθ\n\nDiscriminative methods based on\nfunction approximation\nain\nain\nahid\nj\nwhid\nj2\nwhid\nj1\n- Neural networks\nImage removed due to copyright considerations.\n\nThe benefit of hidden units\nridge = θ (sigmoid+sigmoid) bump = θ (ridge+ridge)\nImage removed due to copyright considerations.\n\nNeural network learning\n- Gradient descent on error surface in weight\nspace (\"backpropagation\"):\nImage removed due to copyright considerations.\n\nNeural network learning\n- Gradient descent on error surface in weight\nspace (\"backpropagation\"):\nE(w)\nw\n\nBackpropagation as a model of\nhuman learning?\n- Kruschke: Are neural networks trained with\nbackpropagation a good model of human\ncategory learning?\n- Originally, backpropagation was not\nintended as a precise model of learning.\n- Rather, a tool for learning representations.\n- But does it learn the right kind of\nrepresentations?\n\nTwo learning tasks\n\"Filtration\": easy to learn\nImage removed due to copyright considerations.\n\"Condensation\": hard to learn\nImage removed due to copyright considerations.\n\nHuman learning data\nImage removed due to copyright considerations.\n\nConventional neural network\nImage removed due to copyright considerations.\n\nModel versus Data\nImage removed due to copyright considerations.\nPeople\nBackprop\n\nALCOVE network\nImage removed due to copyright considerations.\n\nDifferences between the models\n- Dimension-specific attentional weights\n- Hidden unit activation functions\nImage removed due to copyright considerations.\n\nModel versus Data\nImage removed due to copyright considerations.\nPeople\nALCOVE\n\nModel versus Data\nImage removed due to copyright considerations.\nPeople\nBackprop + attentional\nweights (c.f. ARD)\n\n\"Catastrophic forgetting\"\n- Stimuli for category-learning experiment\nImage removed due to copyright considerations.\n\nHuman learning data\nImage removed due to copyright considerations.\n\nALCOVE\nImage removed due to copyright considerations.\n\nConventional neural network\nImage removed due to copyright considerations.\n\nQuestions about neural networks\n- Why do they have such a bad rap?\n- To what extent are neural networks brain-\nlike?\n- They take a long time to train. Is that a\ngood thing or a bad thing from the\nstandpoint of cognitive modeling?"
    },
    {
      "category": "Resource",
      "title": "nov_4_2004_final.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-66j-computational-cognitive-science-fall-2004/4f68798f979c9662d232adf39772f476_nov_4_2004_final.pdf",
      "content": "Outline\n- Problems with neural networks\n- Support Vector Machines\n- Controlling complexity in statistical models\n\nQuestions about neural networks\n- Why do they have such a bad rap?\n- To what extent are neural networks brain-\nlike?\n- They take a long time to train. Is that a\ngood thing or a bad thing from the\nstandpoint of cognitive modeling?\n\nModels versus Data\nImages removed due to\ncopyright considerations.\n\nNumber game\n- Neural networks\n- Delta rule learning\neven\nmultiple\nof 10\npower\nof 2\nmultiple\nof 3\n0.1\n0.1\n0.1\n0.0\n\nNumber game\n- Neural networks\n- Delta rule learning\neven\nmultiple\nof 10\npower\nof 2\nmultiple\nof 3\n0.18\n0.18\n0.1\n0.0\n\nNumber game\n- Neural networks\n- Delta rule learning\neven\nmultiple\nof 10\npower\nof 2\nmultiple\nof 3\n0.24\n0.24\n0.1\n0.0\n\nNumber game\n- Neural networks\n- Delta rule learning\neven\nmultiple\nof 10\npower\nof 2\nmultiple\nof 3\n0.28\n0.28\n0.14\n0.0\n\n- Similarity to exemplars\n- Average similarity:\nAlternative models\n)\n,\n(\nsim\n|\n|\n)\n|\n(\nj\nX\nx\nx\ny\nX\nX\nC\ny\np\nj∑\n∈\n=\n∈\nImages removed due to\ncopyright considerations.\n60 80 10 30\n60 52 57 55\nData\nModel (r = 0.80)\n\nBayes (without basic-level bias)\nBayes (with basic-level bias)\nImages removed due to\ncopyright considerations.\nImages removed due to\ncopyright considerations.\n\nQuestions about neural networks\n- Why do they have such a bad rap?\n- To what extent are neural networks brain-\nlike?\n- They take a long time to train. Is that a\ngood thing or a bad thing from the\nstandpoint of cognitive modeling?\n(Kruschke)\nImage removed due to\ncopyright considerations.\n\nOutline\n- Problems with neural networks\n- Support Vector Machines\n- Controlling complexity in statistical models\n\nSupport Vector Machines\n(SVMs)\n- Problems with neural networks\n- Flexible nonparametric classifiers, but slow to\ntrain and no good generalization guarantees\n- Problems with perceptrons\n- Good generalization guarantees and fast\ntraining, but only for a limited parametric\nfamily of problems (linearly separable classes).\n- SVMs seek the best of both worlds.\n\nThe virtue of high-dimensional\nfeature spaces\nImage removed due to\ncopyright considerations.\n\nThe virtue of high-dimensional\nfeature spaces\nImage removed due to\ncopyright considerations.\n\nThe SVM approach\n- Embed data in d-dimensional feature space\n(d >> # data points, maybe infinite).\n- Find optimal separating hyperplane in\nfeature space.\n- What makes this possible:\n- For d large enough, all categorization problems\nbecome linearly separable.\n\nThe SVM approach\n- Embed data in d-dimensional feature space\n(d >> # data points, maybe infinite).\n- Find optimal separating hyperplane in\nfeature space.\n- What makes this possible:\n- Computations depend only inner products\nbetween feature vectors, which can be\nexpressed as a simple kernel on inputs, e.g.:\n)\n(\n)\n(\n)\n(\n)\n(\n)\n(\nj\ni\nj\ni\nx\nx\nz\nz\n⋅\n=\n⋅\n\nThe SVM approach\n- Embed data in d-dimensional feature space\n(d >> # data points, maybe infinite).\n- Find optimal separating hyperplane in\nfeature space.\n- What makes this possible:\n- A wide range of simple kernels define very\nhigh-dimensional (and useful) feature spaces:\n)\n(\n)\n(\n)\n(\n)\n(\n)\n(\n)\n(\n)\n(\n)\n(\n||)\n||\nexp(\n)\n1(\nj\ni\nj\ni\nk\nj\ni\nj\ni\nx\nx\nz\nz\nx\nx\nz\nz\n-\n-\n=\n⋅\n⋅\n+\n=\n⋅\n\nThe original Perceptron idea\n- Embed data in d-dimensional feature space\n(d >> # data points, maybe infinite).\n- Find optimal separating hyperplane in\nfeature space.\n- Problems:\n- Didn't know the \"kernel trick\", but inspired by\nneural receptive fields. (c.f. Minsky & Papert)\n- Didn't have a good concept of \"optimal\nseparating hyperplane\". In high-dimensional\nfeature spaces, infinitely many errorless planes.\n\nMaximum margin hyperplane\n- Depends only on the\n\"support vectors\":\npoints closest to the\nboundary between\nclasses.\n- PAC-style guarantees\nof good generalization:\nlog |H| ~ # of support\nvectors\nImage removed due to\ncopyright considerations.\n\nSVMs and neural networks\n- SVMs have many of the attractive features\nof neural networks, but not all.\n- No sharing ofweights\n(parameters)\nacross many\nrelated\nlearning\ntasks.\nImage removed due to\ncopyright considerations.\n\nSVMs and neural networks\n- SVMs also preserve some of the limitations\nof neural networks.\n- No learning from just one or a few positive\nexamples.\n- No natural way to build in prior knowledge\nabout categories.\n- No explicit representation of learned concepts\nor abstractions.\n\nEvaluating models for concept\nlearning\n- Dimensions:\n- Causal versus Referential inference\n- Parametric versus Non-parametric\n- Generative versus Discriminative\n- Which of these approaches are most suited\nfor understanding human learning?\n\n- Dimensions:\n- Causal versus Referential inference\n- Parametric versus Non-parametric\n- Generative versus Discriminative\n- Issues:\n- All-or-none versus graded generalization\n- Learning from very few labeled examples\n- Incorporating unlabeled examples\n- Incorporating prior knowledge\n- Forming abstractions and theories\n- Learning \"new\" concepts\n- Trading off complexity with fit to data\n\nOutline\n- Problems with neural networks\n- Support Vector Machines\n- Controlling complexity in statistical models\n\nOverfitting in neural networks\nImage removed due to copyright considerations.\n\nOverfitting is a universal problem\n- Concept learning as search: subset principle\n- Bayesian concept learning: size principle\n- Categorization with generative models\n- Categorization with discriminative models\nImage removed due to\ncopyright considerations.\nImage removed due to\ncopyright considerations.\n\nHow to control model complexity?\n- Traditional \"model\ncontrol parameters\"\n- Early stopping\n- Weight decay\n- Slow learning rate\n- Bottleneck number\nof hidden units\nImage removed due to\ncopyright considerations.\n\nHow to choose control parameters?\n- Cross-validation\n- Separate data into\n\"training set\" and\n\"validation set\"\n(simulated test data)\n- Learn on training set\nuntil validation error\nstops decreasing.\nImage removed due to\ncopyright considerations.\n\nCross-validation\n- Advantages:\n- Intuitive\n- Works in practice\n- Disadvantages\n- Theoretical justification unclear.\n- Unclear how to choose training/validation split.\n- Doesn't use all of the data.\n- Difficult to apply to many control parameters.\n\nMonte Carlo Cross-validation\n- Consider many different random\ntraining/test splits.\n- Smythe: Application to choosing the correct\nnumber of components in a mixture model.\n- Disadvantages\n- Theoretical justification unclear.\n- Unclear how to choose training/validation split.\n- Doesn't use all of the data.\n- Difficult to apply to many control parameters.\n- Slow."
    },
    {
      "category": "Resource",
      "title": "oct_5_2004_final.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-66j-computational-cognitive-science-fall-2004/cb8a24f5fb2c65120a508e6e5eb5c8c7_oct_5_2004_final.pdf",
      "content": "So...\n... why do we keep having this debate:\nrules/symbols vs. prototypes/connections?\n\nSo...\nThe real problem: a spurious contest between\nlogic and probability.\n- Neither logic nor probability on its own is\nsufficient to account for human cognition:\n- Generativity\n- Systematicity\n- Recursion and abstraction\n- Flexibility\n- Effective under great uncertainty (e.g., sparse data)\n- What we really need is to understand how logic\nand probability can work together.\n\nSo...\nThe real problem: a spurious contest between\nlogic and probability.\n- A confusion between knowledge\nrepresentations and inference processes:\nGradedness or fuzziness doesn't necessarily mean that\nthe knowledge representations lack structure or rules\n-- merely that the inference processes incorporate\nuncertainty.\n- Probabilistic inference over structured\nrepresentations is what we need.\n\nSo...\n... why do we keep having this debate:\nrules/symbols vs. prototypes/connections?\n... why has it taken Cognitive Science much\nlonger to get over it than AI?\n\nIntroduction to Bayesian\ninference\n\nRepresentativeness in reasoning\nWhich sequence is more likely to be produced\nby flipping a fair coin?\nHHTHT\nHHHHH\n\nA reasoning fallacy\nKahneman & Tversky: people judge the\nprobability of an outcome based on the\nextent to which it is representative of the\ngenerating process.\n\nNot wired for probability?\n- Slovic, Fischhoff, and Lichtenstein (1976):\n- \"It appears that people lack the correct\nprograms for many important judgmental\ntasks.... it may be argued that we have not had\nthe opportunity to evolve an intellect capable of\ndealing conceptually with uncertainty.\"\n- Gould (1992):\n- \"Our minds are not built (for whatever reason)\nto work by the rules of probability.\"\n\nAristotle (4th century B.C.)\n- In On the heavens, Aristotle asks whether the stars move\nindependently or whether they are all fixed to some sphere.\n- He observes that stars moving in large circles (near the\ncelestial equator) take the same time to rotate as those near the\npolestar, which rotate in small circles.\n- Infers a common cause: \"If, on the other hand, the\narrangement was a chance combination, the coincidence in\nevery case of a greater circle with a swifter movement of the\nstar contained in it is too much to believe. In one or two\ncases, it might not inconceivably fall out so, but to imagine it\nin every case alike is a mere fiction. Besides, chance has no\nplace in that which is natural, and what happens everywhere\nand in every case is no matter of chance.\"\n\nHalley. \"Motuum Cometarum in Orbe Parabolico Elementa Astronomica.\"\nIn \"Astronomiae Cometiae Synopsis.\" Philisophical Transactions (1705).\nTranscript available at http://www.seds.org/~spider/spider/Comets/halley_p.html\nImage removed due to copyright considerations. Please see:\n____________________________________________________________________\n\nHalley. \"Motuum Cometarum in Orbe Parabolico Elementa Astronomica.\"\nIn \"Astronomiae Cometiae Synopsis.\" Philisophical Transactions (1705).\nTranscript available at http://www.seds.org/~spider/spider/Comets/halley_p.html\nImage removed due to copyright considerations. Please see:\n____________________________________________________________________\n\nA reasoning fallacy\nKahneman & Tversky: people judge the\nprobability of an outcome based on the\nextent to which it is representative of the\ngenerating process.\nBut how does \"representativeness\" work?\n\nPredictive versus inductive\nreasoning\nH\nD\nHypothesis\nData\n\nPredictive versus inductive\nreasoning\nPrediction\ngiven\n?\nH\nD\nLikelihood:\n)\n| H\nD\n(\nP\n\nPredictive versus inductive\nreasoning\nPrediction\ngiven\n?\nInduction\n?\ngiven\nH\nD\nRepresentativeness\nLikelihood:\n)\n| H\nD\n(\nP\n\nBayes' rule\nFor data D and a hypothesis H, we have:\n)\n(\n)\n|\n(\n)\n(\n)\n|\n(\nD\nP\nH\nD\nP\nH\nP\nD\nH\nP\n=\n- \"Posterior probability\":\n- \"Prior probability\":\n- \"Likelihood\":\n)\n|\n(\nD\nH\nP\n)\n(H\nP\n)\n|\n(\nH\nD\nP\n\nThe origin of Bayes' rule\n- A simple consequence of using probability\nto represent degrees of belief\n- For any two random variables:\n)\n|\n(\n)\n(\n)\n(\n)\n|\n(\n)\n(\n)\n(\nB\nA\nP\nB\nP\nB\nA\nP\nA\nB\nP\nA\nP\nB\nA\nP\n=\n∧\n=\n∧\n)\n|\n(\n)\n(\n)\n|\n(\n)\n(\nA\nB\nP\nA\nP\nB\nA\nP\nB\nP\n=\n)\n(\n)\n|\n(\n)\n(\n)\n|\n(\nB\nP\nA\nB\nP\nA\nP\nB\nA\nP\n=\n\nWhy represent degrees of belief\nwith probabilities?\n- Cox Axioms\n- necessary to cohere with common sense\n- \"Dutch Book\" + Survival of the Fittest\n- if your beliefs do not accord with the laws of\nprobability, then you can always be out-gambled by\nsomeone whose beliefs do so accord.\n- Provides a theory of learning\n- a common currency for combining prior knowledge and\nthe lessons of experience.\n\nCox Axioms (via Jaynes)\n- Degrees of belief are represented by real numbers.\n- Qualitative correspondence with common sense,\ne.g.:\n- Consistency:\n- If a conclusion can be reasoned in more than one way,\nthen every possible way must lead to the same result.\n- All available evidence should be taken into account when\ninferring a degree of belief.\n- Equivalent states of knowledge should be represented with\nequivalent degrees of belief.\n- Accepting these axioms implies Bel can be\nrepresented as a probability measure.\n)]\n(\n[\n)\n(\nA\nBel\nf\nA\nBel\n=\n¬\n)]\n|\n(\n),\n(\n[\n)\n(\nA\nB\nBel\nA\nBel\ng\nB\nA\nBel\n=\n∧\n\nProbability as propositional logic\nwith uncertainty\n- All of probability theory can be derived\nfrom these two laws (plus propositional\nlogic):\n- That's good: simple, elegant principles.\n- That's bad: how to work with structured\nrepresentations? More on that later....\n)\n|\n(\n)\n|\n(\n=\n¬\n+\nI\nA\nP\nI\nA\nP\n)\n|\n(\n)\n,\n|\n(\n)\n|\n,\n(\n)\n|\n(\nI\nB\nP\nI\nB\nA\nP\nI\nB\nA\nP\nI\nB\nA\nP\n×\n=\n≡\n∧\n\nBayesian inference\n- Bayes' rule:\n- What makes a good scientific argument?\nP(H|D) is high if:\n- Hypothesis is plausible: P(H) is high\n- Hypothesis strongly predicts the observed data:\nP(D|H) is high\n- Data are surprising: P(D) is low\n)\n(\n)\n|\n(\n)\n(\n)\n|\n(\nD\nP\nH\nD\nP\nH\nP\nD\nH\nP\n=\n\nA more useful form of Bayes\n- Random variable X denotes a set of\nmutually exclusive exhaustive propositions\n(states of the world):\n- A useful rule: conditionalization\n}\n,\n,\n{ 1\nn\nx\nx\nX\nK\n=\n)\n(\n=\n=\n∑\ni\ni\nx\nX\nP\n)\n(\n)\n|\n(\n)\n(\nj\nj\ni\nj\ni\ny\nY\nP\ny\nY\nx\nX\nP\nx\nX\nP\n=\n=\n=\n=\n=\n∑\n\nA more useful form of Bayes\n- Random variable X denotes a set of\nmutually exclusive exhaustive propositions\n(states of the world):\n- Bayes' rule for more than two hypotheses:\n}\n,\n,\n{ 1\nn\nx\nx\nX\nK\n=\n)\n(\n=\n=\n∑\ni\ni\nx\nX\nP\n)\n(\n)\n|\n(\n)\n(\n)\n|\n(\nd\nD\nP\nh\nH\nd\nD\nP\nh\nH\nP\nd\nD\nh\nH\nP\n=\n=\n=\n=\n=\n=\n=\n\nA more useful form of Bayes\n- Random variable X denotes a set of\nmutually exclusive exhaustive propositions\n(states of the world):\n- Bayes' rule for more than two hypotheses:\n}\n,\n,\n{ 1\nn\nx\nx\nX\nK\n=\n)\n(\n=\n=\n∑\ni\ni\nx\nX\nP\n)\n|\n(\n)\n(\n)\n|\n(\n)\n(\n)\n|\n(\ni\ni\ni\nh\nH\nd\nD\nP\nh\nH\nP\nh\nH\nd\nD\nP\nh\nH\nP\nd\nD\nh\nH\nP\n=\n=\n=\n=\n=\n=\n=\n=\n=\n∑\n\nA more useful form of Bayes\n- Random variable X denotes a set of\nmutually exclusive exhaustive propositions\n(states of the world):\n- Bayes' rule for more than two hypotheses:\n}\n,\n,\n{ 1\nn\nx\nx\nX\nK\n=\n)\n(\n=\n=\n∑\ni\ni\nx\nX\nP\n)\n|\n(\n)\n(\n)\n|\n(\n)\n(\n)\n|\n(\ni\ni\ni\nh\nd\nP\nh\nP\nh\nd\nP\nh\nP\nd\nh\nP\n∑\n=\n\nSherlock Holmes\n- \"How often have I said to you that when you have\neliminated the impossible whatever remains,\nhowever improbable, must be the truth?\" (The\nSign of the Four)\n)\n|\n(\n)\n(\n)\n|\n(\n)\n(\n)\n|\n(\ni\ni\ni\nh\nd\nP\nh\nP\nh\nd\nP\nh\nP\nd\nh\nP\n∑\n=\n\nSherlock Holmes\n- \"How often have I said to you that when you have\neliminated the impossible whatever remains,\nhowever improbable, must be the truth?\" (The\nSign of the Four)\n)\n|\n(\n)\n(\n)\n|\n(\n)\n(\n)\n|\n(\n)\n(\n)\n|\n(\ni\nh\nh\ni\nh\nd\nP\nh\nP\nh\nd\nP\nh\nP\nh\nd\nP\nh\nP\nd\nh\nP\ni∑\n=\n+\n=\n\nSherlock Holmes\n- \"How often have I said to you that when you have\neliminated the impossible whatever remains,\nhowever improbable, must be the truth?\" (The\nSign of the Four)\n)\n|\n(\n)\n(\n)\n|\n(\n)\n(\n)\n|\n(\n)\n(\n)\n|\n(\ni\nh\nh\ni\nh\nd\nP\nh\nP\nh\nd\nP\nh\nP\nh\nd\nP\nh\nP\nd\nh\nP\ni∑\n=\n+\n=\n= 0\n\nSherlock Holmes\n- \"How often have I said to you that when you have\neliminated the impossible whatever remains,\nhowever improbable, must be the truth?\" (The\nSign of the Four)\n)\n|\n(\n)\n(\n)\n|\n(\n)\n(\n)\n|\n(\n=\n=\nh\nd\nP\nh\nP\nh\nd\nP\nh\nP\nd\nh\nP\n\nSherlock Holmes\n- \"How often have I said to you that when you have\neliminated the impossible whatever remains,\nhowever improbable, must be the truth?\" (The\nSign of the Four)\n)\n|\n(\n)\n(\n)\n|\n(\n)\n(\n)\n|\n(\n=\n=\nh\nd\nP\nh\nP\nh\nd\nP\nh\nP\nd\nh\nP\n> 0\n\nA reasoning fallacy\nKahneman & Tversky: people judge the\nprobability of an outcome based on the\nextent to which it is representative of the\ngenerating process.\n\nHypotheses in coin flipping\nDescribe processes by which D could be generated\nD = HHTHT\ngenerative\nmodels\n- Fair coin, P(H) = 0.5\n- Coin with P(H) = θ\n- Markov model\n- Hidden Markov model\n- ...\n\nRepresenting generative models\n- Graphical model notation\n- Pearl (1988), Jordan (1998)\n- Variables are nodes, edges\nindicate dependency\n- Directed edges show causal\nprocess of data generation\nd1\nd2\nd3\nd4\nFair coin: P(H) = 0.5\nd1\nd2\nd3\nd4\nMarkov model:\nP(di+1|di) = 0.7 if di+1\ndi\n= 0.3 if di+1 = di\n=\nHHTHT\nd1 d2 d3 d4 d5\n\nModels with latent structure\nd1\nd2\nd3\nd4\nP(H) = θ\nθ\n- Not all nodes in a graphical\nmodel need to be observed\n- Some variables reflect latent\nstructure, used in generating\nD but unobserved\nHHTHT\nd1 d2 d3 d4 d5\nd1\nd2\nd3\nd4\nHidden Markov model:\nsi {Fair coin, Trick coin}\ns1\ns2\ns3\ns4\n∈\n\nCoin flipping\n- Comparing two simple hypotheses\n- P(H) = 0.5 vs. P(H) = 1.0\n- Comparing simple and complex hypotheses\n- P(H) = 0.5 vs. P(H) = θ\n- Comparing infinitely many hypotheses\n- P(H) = θ : Infer θ\n\nCoin flipping\n- Comparing two simple hypotheses\n- P(H) = 0.5 vs. P(H) = 1.0\n- Comparing simple and complex hypotheses\n- P(H) = 0.5 vs. P(H) = θ\n- Comparing infinitely many hypotheses\n- P(H) = θ : Infer θ\n\nCoin flipping\nHHTHT\nHHHHH\nWhat process produced these sequences?\n\nComparing two simple hypotheses\n- Contrast simple hypotheses:\n- H1: \"fair coin\", P(H) = 0.5\n- H2:\"always heads\", P(H) = 1.0\n- Bayes' rule:\n- With two hypotheses, use odds form\n)\n(\n)\n|\n(\n)\n(\n)\n|\n(\nD\nP\nH\nD\nP\nH\nP\nD\nH\nP\n=\n\nBayes' rule in odds form\nP(H1|D) P(D|H1) P(H1)\nP(H2|D) P(D|H2) P(H2)\nD:\ndata\nH1, H2:\nmodels\nP(H1|D):\nposterior probability H1 generated the data\nP(D|H1):\nlikelihood of data under model H1\nP(H1):\nprior probability H1 generated the data\n= x\n\nComparing two simple hypotheses\nP(H1|D) P(D|H1) P(H1)\nP(H2|D) P(D|H2) P(H2)\nD:\nHHTHT\nH1, H2:\n\"fair coin\", \"always heads\"\nP(D|H1) =\n1/25\nP(H1) =\n?\nP(D|H2) =\nP(H2) =\n1-?\nP(H1|D) / P(H2|D) = infinity\n= x\n\nComparing two simple hypotheses\nP(H1|D) P(D|H1) P(H1)\nP(H2|D) P(D|H2) P(H2)\nD:\nHHTHT\nH1, H2:\n\"fair coin\", \"always heads\"\nP(D|H1) =\n1/25\nP(H1) =\n999/1000\nP(D|H2) =\nP(H2) =\n1/1000\nP(H1|D) / P(H2|D) = infinity\n= x\n\nComparing two simple hypotheses\nP(H1|D) P(D|H1) P(H1)\nP(H2|D) P(D|H2) P(H2)\nD:\nHHHHH\nH1, H2:\n\"fair coin\", \"always heads\"\nP(D|H1) =\n1/25\nP(H1) =\n999/1000\nP(D|H2) =\nP(H2) =\n1/1000\nP(H1|D) / P(H2|D) ≈30\n= x\n\nComparing two simple hypotheses\nP(H1|D) P(D|H1) P(H1)\nP(H2|D) P(D|H2) P(H2)\nD:\nHHHHHHHHHH\nH1, H2:\n\"fair coin\", \"always heads\"\nP(D|H1) =\n1/210\nP(H1) =\n999/1000\nP(D|H2) =\nP(H2) =\n1/1000\nP(H1|D) / P(H2|D) ≈1\n= x\n\nThe role of theories\nThe fact that HHTHT looks representative of\na fair coin and HHHHH does not reflects our\nimplicit theories of how the world works.\n- Easy to imagine how a trick all-heads coin\ncould work: high prior probability.\n- Hard to imagine how a trick \"HHTHT\" coin\ncould work: low prior probability."
    },
    {
      "category": "Resource",
      "title": "dec_7_2004_final.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-66j-computational-cognitive-science-fall-2004/c8ba2455fbb1a744abeae59908086b0f_dec_7_2004_final.pdf",
      "content": "Outline\n- Theory-based Bayesian causal induction\n- On intuitive theories: their structure,\nfunction, and origins\n\nDomain theory generates hypothesis\nspace of causal models\n- Ontology of types and predicates.\n- What is there?\n- Constraints on causal relations between\npredicates.\n- What can/must/is likely to cause what?\n- Functional forms of causal relations.\n- How does an effect depend functionally on its\ncauses?\n\nDomain theory generates hypothesis\nspace of causal models\n- Ontology of types and predicates.\n- What is there? Nodes.\n- Constraints on causal relations between\npredicates.\n- What can/must/is likely to cause what? Edges.\n- Functional forms of causal relations.\n- How does an effect depend functionally on its\ncauses? Parameterizations and parameters.\n\nTheories as probabilistic logic\n- Ontology\n- Types: Block, Detector, Trial\n- Predicates:\nContact(Block, Detector, Trial)\nActive(Detector, Trial)\n- Constraints on causal relations\n- For any Block b and Detector d, with probability q :\nCause(Contact(b,d,t), Active(d,t))\n- Functional form of causal relations\n- Causes of Active(d,t) are independent mechanisms, with\ncausal strengths wi. A background cause has strength w0.\nAssume a near-deterministic mechanism: wi ~ 1, w0 ~ 0.\n\nBayesian inference\n- Evaluating causal network hypotheses in\nlight of data:\n- Inferring a particular causal relation:\n∑\n∈\n=\nH\nj\nh\nj\nj\ni\ni\ni\nh\nP\nh\nd\nP\nh\nP\nh\nd\nP\nd\nh\nP\n)\n(\n)\n|\n(\n)\n(\n)\n|\n(\n)\n|\n(\n∑\n∈\n→\n=\n→\nH\nj\nh\nj\nj\nd\nh\nP\nh\nE\nA\nP\nd\nE\nA\nP\n)\n|\n(\n)\n|\n(\n)\n|\n(\n\nAfter each trial, adults judge the probability that each\nobject is a blicket.\nTrial 1\nTrial 2\nB\nA\nI. Pre-training phase: Blickets are rare . . . .\nII. Backwards blocking phase:\nManipulating the prior\n\nRare condition\nCommon condition\n1 AB\nAB\nA\nB\nAB\nAB\nA\nB\nAB\nAB\nA\nB\nBaseline\nAfter AB trial\nAfter A trial\nPEOPLE\n(N = 12)\nBAYES\nAB\nAB\nA\nB\nBaseline\nAfter AB trial After A trial\nPEOPLE\n(N = 12)\nBAYES\nFigure by MIT OCW.\n\nManipulating the priors of\n4-year-olds\n(Sobel, Tenenbaum & Gopnik, 2004)\nI. Pre-training phase: Blickets are rare.\nTrial 1\nTrial 2\nB\nA\nII. Backwards blocking phase:\nRare condition:\nA: 100% say \"a blicket\"\nB: 25% say \"a blicket\"\nCommon condition:\nA: 100% say \"a blicket\"\nB: 81% say \"a blicket\"\n\nInferences from ambiguous data\nI. Pre-training phase: Blickets are rare . . . .\nTrial 1\nTrial 2\nB\nA\nII. Two trials: A B detector, B C detector\nC\nAfter each trial, adults judge the probability that each\nobject is a blicket.\n\nSame domain theory generates hypothesis\nspace for 3 objects:\n- Hypotheses: h000 = h100 =\nh010 = h001 =\nh110 = h011 =\nh101 = h111 =\n- Likelihoods:\nE\nA\nB\nC\nE\nA\nB\nC\nE\nA\nB\nC\nE\nA\nB\nC\nE\nA\nB\nC\nE\nA\nB\nC\nE\nA\nB\nC\nE\nA\nB\nC\nif A = 1 and A\nE exists,\nor B = 1 and B\nE exists,\nor C = 1 and C\nE exists,\nelse 0.\nP(E=1| A, B, C; h) = 1\n\n- \"Rare\" condition: First observe 12 objects\non detector, of which 2 set it off.\nPEOPLE\n(N = 20)\nBAYES\nABC\nAB\nA\nBC\nBaseline\nAfter AB trial\nAfter AC trial\nC\nFigure by MIT OCW.\n\nAmbiguous data with 4-year-olds\nI. Pre-training phase: Blickets are rare.\nTrial 1\nTrial 2\nB\nA\nII. Two trials: A B detector, B C detector\nC\nFinal judgments:\nA: 87% say \"a blicket\"\nB or C: 56% say \"a blicket\"\n\nFinal judgments:\nA: 87% say \"a blicket\"\nB or C: 56% say \"a blicket\"\nTrial 1\nTrial 2\nB\nA\nI. Pre-training phase: Blickets are rare.\nII. Two trials: A B detector, B C detector\nAmbiguous data with 4-year-olds\nC\nBackwards blocking (rare)\nA: 100% say \"a blicket\"\nB: 25% say \"a blicket\"\n\nThe role of causal mechanism\nknowledge\n- Is mechanism knowledge necessary?\n- Constraint-based learning using χ2 tests of\nconditional independence.\n- How important is the deterministic functional\nform of causal relations?\n- Bayes with \"probabilistic independent generative\ncauses\" theory (i.e., noisy-OR parameterization\nwith unknown strength parameters; c.f., Cheng's\ncausal power).\n\nBayes with correct theory:\nIndependence test with fictional sample sizes:\nAB\nAB\nA\nB\nAB\nAB\nA\nB\nABC\nAB\nA\nC\nBC\nBaseline\nAfter AC trial\nAfter AB trial\nPEOPLE (N=12)\nBAYES\nPEOPLE (N=12)\nBAYES\nPEOPLE (N=20)\nBAYES\nAB\nAB\nA\nB\nAB\nAB\nA\nB\nABC\nAB\nA\nC\nBC\nBaseline\nAfter AC trial\nAfter AB trial\nFigure by MIT OCW.\nFigure by MIT OCW.\n\nBayes with correct theory:\nBayes with \"noisy sufficient causes\" theory:\nAB\nAB\nA\nB\nAB\nAB\nA\nB\nABC\nAB\nA\nC\nBC\nBaseline\nAfter AC trial\nAfter AB trial\nPEOPLE (N=12)\nBAYES\nPEOPLE (N=12)\nBAYES\nPEOPLE (N=20)\nBAYES\nAB\nAB\nA\nB\nAB\nAB\nA\nB\nABC\nAB\nA\nC\nBC\nBasline\nAfter AC trial\nAfter AB trial\nFigure by MIT OCW.\nFigure by MIT OCW.\n\nBlicket studies: summary\n- Theory-based Bayesian approach explains\none-shot causal inferences in physical\nsystems.\n- Captures a spectrum of inference:\n- Unambiguous data: adults and children make\nall-or-none inferences\n- Ambiguous data: adults and children make\nmore graded inferences\n- Extends to more complex cases with hidden\nvariables, dynamic systems, ....\n\nLearning a probabilistic\ncausal relation\nGiven a random\nsample of mice:\nExpressed Y\nDid not\nexpress Y\nInjected\nwith X\nNot injected\nwith X\n- \"To what extent does chemical X cause gene Y\nto be expressed?\"\n- Or, \"What is the probability that X causes Y?\"\n\n- Ontology\n- Types: Chemical, Gene, Mouse\n- Predicates:\nInjected(Mouse, Chemical)\nExpressed(Mouse, Gene)\n- Constraints on causal relations\n- For any Chemical c and Gene g, with prior probability q :\nCause(Injected(m,c), Expressed(m,g))\n- Functional form of causal relations\n- Causes of Expressed(m,g) are independent probabilistic\nmechanisms, with causal strengths wi. An independent\nbackground cause is always present with strength w0.\nTheory\n\nJudging the probability that C E\n(Buehner & Cheng, 1997; 2003)\nImage removed due to copyright considerations.\n\nParameter estimation models\nImage removed due to\ncopyright considerations.\n\nHow important is the theory?\nImage removed due to copyright considerations.\n\nLearning causal structure without\ncausal mechanism knowledge\n- Constraint-based: χ2 test of independence.\n- \"Mechanism-free\" Bayes: no constraints on\nthe functional form of P(E|B,C)\nImage removed due to copyright considerations.\nImage removed due to copyright considerations.\n\nData for inhibitory causes\nImage removed due to copyright considerations.\n\nProbabilistic causal relations:\nsummary\n- Much weaker theory: causes may have any\ndegree of strength.\n- Thus, inferences about causal structure still\ngraded after many observations.\n\nThe stick-ball machine\nT. Kushnir, A. Gopnik, L Schulz, and D. Danks. \"Inferring Hidden Causes.\"\nProceedings of the Twenty-Fifth Annual Meeting of the Cognitive Science Society.\nBoston, MA: Cognitive Science Society, 2003, pp. 699-703.\nA\nB\n\nInferring hidden causal structure\nCommon unobserved cause\n4 x\n2 x\n2 x\nIndependent unobserved causes\n1 x\n2 x\n2 x\n2 x\n2 x\nOne observed cause\n2 x\n4 x\nT. Kushnir, A. Gopnik, L Schulz, and D. Danks. \"Inferring Hidden\nCauses.\" Proceedings of the Twenty-Fifth Annual Meeting of the\nCognitive Science Society. Boston, MA: Cognitive Science\nImage removed due to copyright considerations. Please see:\nSociety, 2003, pp. 699-703.\n\n- Ontology\n- Types: Ball, Source, Trial\n- Predicates:\nMoves(Ball, Trial)\nMoves(Source, Trial)\n- Constraints on causal relations\n- Possible for any Ball or Source x and any Ball b:\nCause(Moves(x,t), Moves(b,t))\n- Functional form of causal relations\n- Causes of Moves(b,t) are independent mechanisms, with\ncausal strengths β.\n- If Moves(x,t) has no causes, it occurs with probability α.\nTheory\n\nImage removed due to copyright considerations.\nc.f., infinite\nmixture model\n\nImage removed due to copyright considerations.\n\nModeling bi-directional\ninfluences\nT. Kushnir, A. Gopnik, L Schulz, and D. Danks. \"Inferring Hidden Causes.\" In Proceedings of the Twenty-Fifth\nAnnual Meeting of the Cognitive Science Society. Boston, MA: Cognitive Science Society, 2003, pp. 699-703.\nImage removed due to copyright considerations. Please see:\n\nImage removed due to copyright considerations. Please see:\nT. Kushnir, A. Gopnik, L Schulz, and D. Danks. \"Inferring\nHiddenCauses.\" In Proceedings of the Twenty-Fifth Annual\nMeeting of the Cognitive Science Society. Boston: Cognitive\nScience Society, 2003, pp. 699-703.\nCommon unobserved cause\nIndependent unobserved causes\nOne observed cause\nα = 0.3\nω = 0.8\nr = 0.94\nImage removed due to copyright considerations.\nImage removed due to copyright considerations.\nImage removed due to copyright considerations.\n\nStick-ball studies: summary\n- More flexible theory: causal mechanisms\nare not deterministic, hidden causes may\nexist.\n- Thus, inferences require more data, and are\nnot quite all-or-none.\n\nNitro X\n- More extreme test of ability to infer hidden causes\n- single datapoint\n- no mention of hidden cause in instructions\n- More sophisticated physical theory\n- Importance of statistical inference\n\nNitro X\n\nTest trials\n- Show explosions involving multiple cans\n- allows inferences about causal structure\n- For each trial, choose one of:\n- chain reaction\n- spontaneous explosions\n- other\n- No explicit reference to a hidden cause\n\nImage removed due to copyright considerations.\nc.f., infinite\nmixture model\n\nImage removed due to copyright considerations.\n\nPeople are sensitive to statistical evidence for a hidden cause\nImage removed due to copyright considerations.\n\nNitro-X studies: summary\n- Perceptual inferences about hidden causal\nstructure can be explained in the same theory-\nbased Bayesian framework.\n- Perceptual settings are information-rich: a single\nobservation is often sufficient to provide strong\nevidence for a hidden cause.\n- Some questions:\n- What do we gain by describing causal reasoning, causal\nlearning, and causal perception in the same framework?\n- Are there any kinds of causal inferences that can't be\nexplained in this framework?\n\nTheories + Bayes in causal induction\n- Theory is a logical structure that generates a space\nof probabilistic causal models -- the hypothesis\nspace for Bayesian inference.\n- Parallel with theory-based Bayesian models for\nlearning about concepts and properties (c.f., Ta'\ngrammars for domain structures).\n- Something missing: learning the theory?\n- General theme: merging of two major traditions in\ncog sci and AI, previously seen as opposing:\n- structured symbolic representations\n- probabilistic inference and statistical learning\n\nOutline\n- Theory-based Bayesian causal induction\n- On intuitive theories: their structure,\nfunction, and origins\n\nTwo lines of questioning\n- How do we infer networks of causal relations?\n- Given only observational data (correlation)?\n- Given just a few observations?\n- In the presence of hidden causes?\n- With dynamic systems?\n- How to characterize intuitive causal theories?\n(e.g., theories of physics, biology, mind)\n- What is the content of causal theories?\n- How are they used in causal inference?\n- How are the theories themselves learned?\n\nSome background on theories\n- A primary motivation comes from cognitive\ndevelopment. (But related to learning in AI...)\n- The big question: what develops from\nchildhood to adulthood?\n- One extreme: basically everything\n- Totally new ways of thinking, e.g. logical thinking.\n- The other extreme: basically nothing\n- Just new facts (specific beliefs), e.g., trees can die.\n- Intermediate view: something important\n- New theories, new ways of organizing beliefs.\n\nOutline\n- What are causal theories?\n- Bayes nets\n- Probabilistic graph grammars\n- Probabilistic logics (TBB, RPMs, BLOG)\n- How are theories used in causal inference?\n- Generate constrained, domain-dependent hypothesis\nspaces for domain-general Bayesian inference\n- How can theories be learned?\n- Bayesian inference in a space of possible theories.\n- Example: Learning concepts based on causal relations.\n\nCausal networks in cognition\n- Learning networks of causal relations\n- e.g., Danks & McKenzie, Griffiths &\nTenenbaum, Schulz & Gopnik, Sloman &\nLagnado, Sobel, Steyvers & Tenenbaum,\nWaldmann\n- Judging the strength of cause-effect relations\n- e.g., Buehner & Cheng, Cheng & Novick,\nShanks, White\n- Categories as causal networks\n- e.g., Ahn, Rehder, Waldmann\n\nCausal networks = theories?\nExample: network of diseases, effects, and causes\nTheory-like properties:\n- Generates hypotheses for causal inference (diagnosis).\n- Provides causal explanation of observed correlations.\n- Supports intervention and action planning.\nHigh Fat\nDiet\nWorking in\nFactory\nHeart\nDisease\nFever\nChest\nPain\nCoughing\nHeadache\nSmoking\nStressful\nLifestyle\nLung\nCancer\nBronchitis\nFlu\nFigure by MIT OCW.\n\nCausal networks = theories?\nExample: network of diseases, effects, and causes\nMissing a key level of abstraction:\nDomain-specific laws that constrain the causal\nnetwork structures considered when learning and\nreasoning in a given domain.\nHigh Fat\nDiet\nWorking in\nFactory\nHeart\nDisease\nFever\nChest\nPain\nCoughing\nHeadache\nSmoking\nStressful\nLifestyle\nLung\nCancer\nBronchitis\nFlu\nFigure by MIT OCW.\n\nDifferent causal networks,\nSame domain theory\nDifferent domain theories\nHigh Fat\nDiet\nWorking in\nFactory\nFever\nHeadache\nSmoking\nStressful\nLifestyle\nBronchitis\nFlu\nHigh Fat\nDiet\nHeart\nDisease\nFever\nHeadache\nSmoking\nStressful\nLifestyle\nLung\nCancer\nBronchitis\nFlu\nWorking in\nFactory\nHeart\nDisease\nLung\nCancer\nCoughing\nChest\nPain\nCoughing\nChest\nPain\nFigure by MIT OCW.\nFigure by MIT OCW.\nHigh Fat\nDiet\nWorking in\nFactory\nHeart\nDisease\nFever\nChest\nPain\nCoughing\nHeadache\nSmoking\nStressful\nLifestyle\nLung\nCancer\nBronchitis\nFlu\nWorking in\nFactory\nHeart\nDisease\nFever\nChest\nPain\nCoughing\nHeadache\nSmoking\nStressful\nLifestyle\nLung\nCancer\nBronchitis\nFlu\nHigh Fat\nDiet\n\nThe grammar analogy\nDifferent semantic networks,\nSame linguistic grammar\nDifferent grammars\nPeople\nCats\nBees\nTrees\nImagine\nSleep\nFly\nGrow\nPeople\nCats\nBees\nTrees\nImagine\nSleep\nFly\nGrow\nPeople\nCats\nBees\nTalk\nSleep\nFly\nGrow\nTrees\nPeople\nCats\nBees\nTalk\nSleep\nFly\nGrow\nTrees\n\nThe grammar analogy\n\"The grammar of a language can be viewed as a theory\nof the structure of this language. Any scientific theory\nis based on a certain finite set of observations and, by\nestablishing general laws stated in terms of certain\nhypothetical constructs, it attempts to account for these\nobservations, to show how they are interrelated, and to\npredict an indefinite number of new phenomena....\nSimilarly, a grammar is based on a finite number of\nobserved sentences... and it 'projects' this set to an\ninfinite set of grammatical sentences by establishing\ngeneral 'laws'... [framed in terms of] phonemes, words,\nphrases, and so on....\"\nChomsky (1956), \"Three models for the description of language\"\n\nTheories in Cognitive\nDevelopment\n\"A theory consists of three interrelated components: a\nset of phenomena that are in its domain, the causal laws\nand other explanatory mechanisms in terms of which the\nphenomena are accounted for, and the concepts in terms\nof which the phenomena and explanatory apparatus are\nexpressed.\"\nCarey (1985), \"Constraints on semantic development\"\n\nTheories as graph grammars\nClasses = {C}\nLaws = {C C}\nClasses = {B, D, S}\nLaws = {B D, D S}\n( : possible causal link)\nClasses = {B, D, S}\nLaws = {S D}\nFigure by MIT OCW.\nHigh Fat\nDiet\nWorking in\nFactory\nFever\nHeadache\nSmoking\nStressful\nLifestyle\nBronchitis\nFlu\nHeart\nDisease\nLung\nCancer\nCoughing\nChest\nPain\nHigh Fat\nDiet\nHeart\nDisease\nFever\nHeadache\nSmoking\nStressful\nLifestyle\nLung\nCancer\nBronchitis\nFlu\nWorking in\nFactory\nCoughing\nChest\nPain\nFigure by MIT OCW.\nFigure by MIT OCW.\nHigh Fat\nDiet\nWorking in\nFactory\nHeart\nDisease\nFever\nChest\nPain\nCoughing\nHeadache\nSmoking\nStressful\nLifestyle\nLung\nCancer\nBronchitis\nFlu\nWorking in\nFactory\nHeart\nDisease\nFever\nChest\nPain\nCoughing\nHeadache\nSmoking\nStressful\nLifestyle\nLung\nCancer\nBronchitis\nFlu\nHigh Fat\nDiet\n\nTheories as graph grammars\nStronger regularities:\nClasses = {B, D, S}\nLaws = {B D, D S}\n( : necessary causal link)\nClasses = {B, D, S}\nLaws = {B D, D S}\n( : possible causal link)\nAlso could have probabilistic\nlink rules.\nFigure by MIT OCW.\nHigh Fat\nDiet\nSmoking\nStressful\nLifestyle\nHeart\nDisease\nLung\nCancer\nBronchitis\nFlu\nFever\nChest\nPain\nHeadache\nCoughing\nWorking in\nFactory\nFigure by MIT OCW.\nHigh Fat\nDiet\nWorking in\nFactory\nHeart\nDisease\nFever\nChest\nPain\nCoughing\nHeadache\nSmoking\nStressful\nLifestyle\nLung\nCancer\nBronchitis\nFlu\nWorking in\nFactory\nHeart\nDisease\nFever\nChest\nPain\nCoughing\nHeadache\nSmoking\nStressful\nLifestyle\nLung\nCancer\nBronchitis\nFlu\nHigh Fat\nDiet\n\nNatural Language Grammar\nAbstract classes: N, V, ....\nProduction rules: S [N V], ....\n(\"Nouns precede verbs\")\nN\n{people, cats, bees, trees, ....}\nV\n{talks, sleep, fly, grow, ....}\nLinguistic Grammar\nSyntactic structures\nObserved sentences\nin the language\nCausal Theory\nAbstract classes: D, S, ....\nCausal laws: [D S], ....\n(\"Diseases cause symptoms\")\nD\n{flu, bronchitis, lung cancer,....}\nS\n{fever, cough, chest pain, ....}\nCausal Theory\nCausal network structures\nObserved data\nin the domain\nThe grammar analogy\n\nSpecific theories versus\n\"framework theories\"\nWellman (1990; Gelman & Wellman, 1992)\nCausal Theory\nCausal network structures\nObserved data\n\"Specific theories are detailed\nscientific formulations about a\ndelimited set of phenomena.\"\n\nSpecific theories versus\n\"framework theories\"\nWellman (1990; Gelman & Wellman, 1992)\nCausal Theory\nCausal network structures\nObserved data\n\"Framework theories outline the\nontology and the basic causal\ndevices for their specific\ntheories, thereby defining a\ncoherent form of reasoning\nabout a particular set of\nphenomena.\"\n\"Specific theories are detailed\nscientific formulations about a\ndelimited set of phenomena.\""
    },
    {
      "category": "Resource",
      "title": "oct_7_2004_final.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-66j-computational-cognitive-science-fall-2004/86f460b09c18f12a8f4d75bfd3efd95e_oct_7_2004_final.pdf",
      "content": "Outline\n- Bayesian Ockham's Razor\n- Bayes nets (directed graphical models)\n- Computational motivation: tractable reasoning\n- Cognitive motivation: causal reasoning\n- Sampling methods for approximate inference\n\nCoin flipping\n- Comparing two simple hypotheses\n- P(H) = 0.5 vs. P(H) = 1.0\n- Comparing simple and complex hypotheses\n- P(H) = 0.5 vs. P(H) = θ\n- Comparing infinitely many hypotheses\n- P(H) = θ : Infer θ\n\nComparing simple and complex hypotheses\nd1\nd2\nd3\nd4\nP(H) = θ\nθ\nvs.\nd1\nd2\nd3\nd4\nFair coin, P(H) = 0.5\n- Which provides a better account of the data:\nthe simple hypothesis of a fair coin, or the\ncomplex hypothesis that P(H) = θ ?\n\nComparing simple and complex hypotheses\n- P(H) = θ is more complex than P(H) = 0.5 in\ntwo ways:\n- P(H) = 0.5 is a special case of P(H) = θ\n- for any observed sequence D, we can choose θ\nsuch that D is more probable than if P(H) = 0.5\nn\nN\nn\nD\nP\n-\n-\n=\n)\n1(\n)\n|\n(\nθ\nθ\nθ\nBernoulli Distribution:\nn = # of heads in D\nN = # of flips in D\n\nComparing simple and complex hypotheses\nProbability\nn\nN\nn\nD\nP\n-\n-\n=\n)\n1(\n)\n|\n(\nθ\nθ\nθ\nθ = 0.5\nD = HHHHH\n\nComparing simple and complex hypotheses\nProbability\nn\nN\nn\nD\nP\n-\n-\n=\n)\n1(\n)\n|\n(\nθ\nθ\nθ\nθ = 0.5\nθ = 1.0\nD = HHHHH\n\nComparing simple and complex hypotheses\nProbability\nD = HHTHT\nn\nN\nn\nD\nP\n-\n-\n=\n)\n1(\n)\n|\n(\nθ\nθ\nθ\nθ = 0.5\nθ = 0.6\n\nComparing simple and complex hypotheses\n- P(H) = θ is more complex than P(H) = 0.5 in\ntwo ways:\n- P(H) = 0.5 is a special case of P(H) = θ\n- for any observed sequence X, we can choose θ\nsuch that X is more probable than if P(H) = 0.5\n- How can we deal with this?\n- Some version of Ockham's razor:?\n- Bayes: just the law of conservation of belief!\n\nComparing simple and complex hypotheses\nP(H1|D) P(D|H1) P(H1)\nP(H2|D) P(D|H2) P(H2)\nComputing P(D|H1) is easy:\nCompute P(D|H2) by averaging over θ:\n= x\n∫\n=\n)\n|\n(\n)\n|\n(\n)\n|\n(\nθ\nθ\nθ\nd\nH\np\nD\nP\nH\nD\nP\nN\nn\nN\nn\nH\nD\nP\n/\n)\n/\n1(\n)\n/\n1(\n)\n|\n(\n=\n-\n=\n-\n\nComparing simple and complex hypotheses\nP(H1|D) P(D|H1) P(H1)\nP(H2|D) P(D|H2) P(H2)\nComputing P(D|H1) is easy:\nCompute P(D|H2) by averaging over θ:\n= x\nN\nn\nN\nn\nH\nD\nP\n/\n)\n/\n1(\n)\n/\n1(\n)\n|\n(\n=\n-\n=\n-\n∫\n=\n)\n|\n(\n)\n|\n(\nθ\nθ d\nD\nP\nH\nD\nP\n(assume uniform\nprior on θ)\n\nComparing simple and complex hypotheses\nP(H1|D) P(D|H1) P(H1)\nP(H2|D) P(D|H2) P(H2)\nComputing P(D|H1) is easy:\nCompute P(D|H2) by averaging over θ:\n= x\nN\nn\nN\nn\nH\nD\nP\n/\n)\n/\n1(\n)\n/\n1(\n)\n|\n(\n=\n-\n=\n-\n∫\n-\n-\n=\n)\n1(\n)\n|\n(\nθ\nθ\nθ\nd\nH\nD\nP\nn\nN\nn\n\nComparing simple and complex hypotheses\nP(H1|D) P(D|H1) P(H1)\nP(H2|D) P(D|H2) P(H2)\nComputing P(D|H1) is easy:\nCompute P(D|H2) by averaging over θ:\n= x\nN\nn\nN\nn\nH\nD\nP\n/\n)\n/\n1(\n)\n/\n1(\n)\n|\n(\n=\n-\n=\n-\n)!\n(\n)!\n(!\n)\n1(\n)\n|\n(\n+\n-\n=\n-\n= ∫\n-\nN\nn\nN\nn\nd\nH\nD\nP\nn\nN\nn\nθ\nθ\nθ\n\n(How is this an average?)\n- Consider a discrete approximation with 11\nvalues of θ, from 0 to 1 in steps of 1/10:\n∑\n=\n=\n=\n=\n)\n|\n/\n(\n)\n/\n|\n(\n)\n|\n(\ni\nH\ni\np\ni\nD\nP\nH\nD\nP\nθ\nθ\n)\n1(\n)\n/\n|\n(\n)\n|\n(\n∑\n=\n=\n=\ni\ni\nD\nP\nH\nD\nP\nθ\n∫\n=\n)\n|\n(\n)\n|\n(\nθ\nθ d\nD\nP\nH\nD\nP\n(c.f., )\n\nComparing simple and complex hypotheses\nProbability\nD = HHHHH\nN\nH\nD\nP\n)\n|\n(\n1 =\n∫\n-\n-\n=\n)\n1(\n)\n|\n(\nθ\nθ\nθ\nd\nH\nD\nP\nn\nN\nn\n\nComparing simple and complex hypotheses\nProbability\nN\nH\nD\nP\n)\n|\n(\n1 =\n∫\n-\n-\n=\n)\n1(\n)\n|\n(\nθ\nθ\nθ\nd\nH\nD\nP\nn\nN\nn\nD = HHHHH\n\nLaw of conservation of belief\n- Two different stages\n- Prior over model parameter:\nIn a model with a wider range of parameter\nvalues, each setting of the parameters\ncontributes less to the model predictions.\n)\n(\n=\n=\n∑\ni\ni\nx\nX\nP\n)\n|\n(\n=\n∫\nθ\nθ\nd\nH\np\n\nLaw of conservation of belief\n- Two different stages\n- Prior over model parameter:\n- Likelihood (probability over data):\nA model that predicts some data sets very well\nmust predict others very poorly.\n)\n(\n=\n=\n∑\ni\ni\nx\nX\nP\n)\n|\n(\n=\n∫\nθ\nθ\nd\nH\np\n∑∫\n∑\n=\n=\n=\n=\nd\nd\nd\nH\np\nd\nD\nP\nH\nd\nD\nP\n)\n|\n(\n)\n|\n(\n)\n|\n(\nθ\nθ\nθ\nθ\n\nBayesian Ockham's Razor\nImage removed due to copyright\nconsiderations.\n\nTwo alternative models\n- Fudged Newton\n- A new planet: Vulcan?\n- Matter rings around the sun?\n- Sun is slightly lopsided.\n- Exponent in Universal law of gravitation is\n2 + ε instead of 2.\n- Each version of this hypothesis has a fudge\nfactor, whose most likely value we can estimate\nempirically . . . .\n\n- Simplifying assumption: predictions of\nfudged Newton are Gaussian around 0.\nImage removed due to copyright considerations.\n\nMore formally....\nε : fudge factor\n∫\n=\nε\nε\n)\n|\n,\n(\n)\n|\n(\nM\nd\np\nM\nd\np\nImage removed due to copyright\nconsiderations.\n∫\n=\nε\nε\nε\n)\n|\n(\n)\n,\n|\n(\nM\np\nM\nd\np\n)\n,\n|\n(\nmax\nM\nd\np\nε\nε\n≤\n\nTwo alternative models\n- Fudged Newton\n- Einstein: General Relativity + experimental\nerror (+/- 2 arc seconds/century).\n\nComparing the models\nImage removed due to copyright considerations.\n\nWhere is Occam's razor?\n- Why not a more \"complex\" fudge, in which\nthe Gaussian can vary in both mean and\nvariance?\nImage removed due to copyright considerations.\n\nBayesian Occam's razor\n- Recall: predictions of a model are the\nweighted average over all parameter values.\n- Only a small set of parameter values fit the\ndata well, so average fit is poor.\nσ\nμ\nσ\nμ\nσ\nμ\nσ\nμ\nd\nd\nM\np\nM\np\nM\nd\np\nM\nd\np\n)\n|\n(\n)\n|\n(\n)\n,\n,\n|\n(\n)\n|\n(\n,∫\n=\nImage removed due to copyright considerations.\n\nLaw of conservation of belief\n)\n(\n=\n=\n∑\ni\ni\nx\nX\nP\n- Two different stages\n- Priors over model parameters:\n- Likelihood (probability over data):\nA model that can predict many possible data sets\nmust assign each of them low probability.\n)\n|\n,\n(\n,\n=\n∫\nσ\nμ\nσ\nμ\nσ\nμ\nd\nd\nM\np\n)\n|\n,\n(\n)\n,\n,\n|\n(\n)\n|\n(\n,\n=\n= ∫∫\n∫\ndx\nd\nd\nM\np\nM\nx\np\ndx\nM\nx\np\nx\nx\nσ\nμ\nσ\nμ\nσ\nμ\nσ\nμ\n\nBayesian Occam's Razor\n)\n|\n(\n\nall\n=\n=\n∑\n∈\nM\nd\nD\np\nD\nd\nFor any model M,\nM1\nM2\np (D = d \\ M)\nFigure by MIT OCW.\n\nOckham's Razor in curve fitting\n-20\n-10\nFigure by MIT OCW.\n\n-20\n-10\n-20\n-10\n-20\n-10\n-20\n-10\nFigure by MIT OCW.\n\nA model that can predict many possible data sets must assign each of\nthem low probability.\n-20\n-10\n-20\n-10\n-20\n-10\nM1\nM2\nM3\nM1\nM2\nM3\np (D = d \\ M)\nData\nD\n)\n|\n(\n\nall\n=\n=\n∑\n∈\nM\nd\nD\np\nD\nd\nFigure by MIT OCW.\nFigure by MIT OCW.\n\nHierarchical prior\n1st order poly\n2nd order poly\n3rd order poly\n. . . .\n\n- Assume y is a linear function of x plus Gaussian\nnoise:\n- Linear regression is maximum likelihood: Find the\nfunction f: x\ny that makes the data most likely.\nLikelihood function for regression\nImage removed due to copyright considerations.\n\n- Assume y is a linear function of x plus Gaussian\nnoise:\n- Linear regression is maximum likelihood: Find the\nfunction f: x\ny that makes the data most likely.\nLikelihood function for regression\nImage removed due to copyright considerations.\n\n- Assume y is a linear function of x plus Gaussian\nnoise:\n- Not the maximum likelihood function....\nLikelihood function for regression\nImage removed due to copyright considerations.\n\nFor best fitting version of each model:\nPrior\nLikelihood\nhigh\nlow\nmedium\nhigh\nvery very very\nvery low\nvery high\n-20\n-10\n-20\n-10\n-20\n-10\nM1\nM2\nM3\nFigure by MIT OCW.\n\nSome questions\n- Is the Bayesian Ockham's razor \"purely\nobjective\"?\n\nSome questions\n- Is the Bayesian Ockham's razor \"purely\nobjective\"? No.\n- Priors matter. (What about uninformative\npriors?)\n- Choice of description language/basis\nfunctions/hypothesis classes matters.\n- Classes of hypotheses + priors = theory.\n(c.f. Martian grue, coin flipping)\n\n- What do we gain from Bayes over\nconventional Ockham's razor?\n\n- What do we gain from Bayes over\nconventional Ockham's razor?\n- Isolates all the subjectivity in the choice of\nhypothesis space and priors\n- Gives a canonical way to measure simplicity.\n- A common currency for trading off simplicity and\nfit to the data: probability.\n- A rigorous basis for the intuition that \"the\nsimplest model that fits is most likely to be true\".\n- Measure of complexity not just # of parameters.\n- Depends on functional form of the model\n\nThree one-parameter models for\n10-bit binary sequences\n- Model 1:\n- Choose parameter α between 0 and 1.\n- Round(10*α) 0's followed by [10 - Round(10*α)] 1's.\n- Model 2:\n- Choose parameter α between 0 and 1.\n- Draw 10 samples from Bernoulli distribution (weighted\ncoin flips) with parameter α.\n- Model 3:\n- Choose parameter α between 0 and 1.\n- Convert-to-binary(Round(2^10*α)).\n\n- What do we gain from Bayes over\nconventional Ockham's razor?\n- Isolates all the subjectivity in the choice of\nhypothesis space and priors\n- Gives a canonical way to measure simplicity.\n- A common currency for trading off complexity\nand fit to the data: probability.\n- A rigorous basis for the intuition that \"the simplest\nmodel that fits is most likely to be true\".\n- Measure of complexity not just # of parameters.\n- Depends on functional form of the model\n- Depends on precise shape of priors (e.g., different\ndegrees of smoothness)\n\nTwo infinite-parameter models\nfor regression\nM1\nM1\nM2\nM2\np (D = d \\ M)\np (f M1)\np (f M2)\nSmooth f\nSmooth f\nBumpy f\nBumpy f\nData\nD\nFigure by MIT OCW.\n\nOutline\n- Bayesian Ockham's Razor\n- Bayes nets (directed graphical models)\n- Computational motivation: tractable reasoning\n- Cognitive motivation: causal reasoning\n- Sampling methods for approximate inference\n\nDirected graphical models\nX3\nX4\nX5\nX1\nX2\n- Consist of\n- a set of nodes\n- a set of edges\n- a conditional probability distribution for each\nnode, conditioned on its parents, multiplied\ntogether to yield the distribution over variables\n- Constrained to directed acyclic graphs (DAG)\n- AKA: Bayesian networks, Bayes nets\n\nUndirected graphical models\n- Consist of\n- a set of nodes\n- a set of edges\n- a potential for each clique, multiplied together to\nyield the distribution over variables\n- Examples\n- statistical physics: Ising model\n- early neural networks (e.g. Boltzmann machines)\n- low- and mid-level vision\nX1\nX2\nX3\nX4\nX5\n\nProperties of Bayesian networks\n- Efficient representation and inference\n- exploiting dependency structure makes it easier\nto work with distributions over many variables\n- Causal reasoning\n- directed representations elucidates the role of\ncausal structure in learning and reasoning\n- model for non-monotonic reasoning (esp.\n\"explaining away\" or causal discounting).\n- reasoning about effects of interventions\n(exogenous actions on a causal system)\n\nEfficient representation and inference\n- Three binary variables: Cavity, Toothache, Catch\n\nEfficient representation and inference\n- Three binary variables: Cavity, Toothache, Catch\n- Specifying P(Cavity, Toothache, Catch) requires 7\nparameters.\n- e.g., 1 for each set of values:\n,\n, ..., minus 1 because it's a\nprobability distribution\n- e.g., chain of conditional probabilities:\n)\n,\n|\n(\n),\n,\n|\n(\n),\n,\n|\n(\n),\n,\n|\n(\n),\n|\n(\n),\n|\n(\n),\n(\ncav\nache\ncatch\nP\ncav\nache\ncatch\nP\ncav\nache\ncatch\nP\ncav\nache\ncatch\nP\ncav\nache\nP\ncav\nache\nP\ncav\nP\n¬\n¬\n¬\n¬\n¬\n)\n,\n,\n(\ncatch\nache\ncav\nP\n)\n,\n,\n(\ncatch\nache\ncav\nP\n¬\n\nEfficient representation and inference\n- Three binary variables: Cavity, Toothache, Catch\n- Specifying P(Cavity, Toothache, Catch) requires 7\nparameters.\n- With n variables, we need 2n -1 parameters\n- Here n=3. Realistically, many more: X-ray, diet, oral\nhygiene, personality, . . . .\n- Problems:\n- Intractable storage, computation, and learning\n- Doesn't really correspond to the world's structure, or\nwhat we know of the world's structure.\n\nConditional independence\n- Probabilistically: all three variables are dependent,\nbut Toothache and Catch are independent given\nthe presence or absence of Cavity.\n- Causally: Toothache and Catch are both effects of\nCavity, via independent causal mechanisms.\n\nConditional independence\n- Probabilistically: all three variables are dependent,\nbut Toothache and Catch are independent given\nthe presence or absence of Cavity.\n- Causally: Toothache and Catch are both effects of\nCavity, via independent causal mechanisms.\n- In probabilistic terms:\n)\n,\n|\n(\n)\n|\n(\n)\n|\n(\ncav\nache\ncatch\nP\ncav\nache\nP\ncav\ncatch\nache\nP\n=\n∧\n)\n,\n|\n(\n)\n|\n(\n)\n|\n(\ncav\nache\ncatch\nP\ncav\nache\nP\ncav\ncatch\nache\nP\n¬\n¬\n=\n∧\n¬\n[\n]\n)\n,\n|\n(\n)\n|\n(\ncav\nache\ncatch\nP\ncav\nache\nP\n¬\n-\n=\n[Without conditional independence]\n\nConditional independence\n- Probabilistically: all three variables are dependent,\nbut Toothache and Catch are independent given\nthe presence or absence of Cavity.\n- Causally: Toothache and Catch are both effects of\nCavity, via independent causal mechanisms.\n- In probabilistic terms:\n- With n pieces of evidence, x1, ..., xn, we need 2n\nconditional probabilities:\n)\n|\n(\n)\n|\n(\n)\n|\n(\ncav\ncatch\nP\ncav\nache\nP\ncav\ncatch\nache\nP\n=\n∧\n)\n|\n(\n)\n|\n(\n)\n|\n(\ncav\ncatch\nP\ncav\nache\nP\ncav\ncatch\nache\nP\n¬\n=\n∧\n¬\n[\n]\n)\n|\n(\n)\n|\n(\ncav\ncatch\nP\ncav\nache\nP\n-\n=\n)\n|\n(\n),\n|\n(\ncav\nx\nP\ncav\nx\nP\ni\ni\n¬\n[With conditional independence]\n\nA simple Bayes net\n- Graphical representation of relations between a set of\nrandom variables:\n- Causal interpretation: independent local mechanisms\n- Probabilistic interpretation: factorizing complex terms\nCavity\nToothache\nCatch\n∏\n∈\n=\n}\n,\n,\n{\n])\n[\nparents\n|\n(\n)\n,\n,\n(\nC\nB\nA\nV\nV\nV\nP\nC\nB\nA\nP\n)\n(\n)\n|\n,\n(\n)\n,\n,\n(\nCav\nP\nCav\nCatch\nAche\nP\nCav\nCatch\nAche\nP\n=\n)\n(\n)\n|\n(\n)\n|\n(\nCav\nP\nCav\nCatch\nP\nCav\nAche\nP\n=\n\nA more complex system\nBattery\n- Joint distribution sufficient for any inference:\nRadio\nIgnition\nGas\nStarts\nOn time to work\n)\n|\n(\n)\n,\n|\n(\n)\n(\n)\n|\n(\n)\n|\n(\n)\n(\n)\n,\n,\n,\n,\n,\n(\nS\nO\nP\nG\nI\nS\nP\nG\nP\nB\nI\nP\nB\nR\nP\nB\nP\nO\nS\nG\nI\nR\nB\nP\n=\n)\n(\n)\n,\n,\n,\n,\n,\n(\n)\n(\n)\n,\n(\n)\n|\n(\n,\n,\n,\nG\nP\nO\nS\nG\nI\nR\nB\nP\nG\nP\nG\nO\nP\nG\nO\nP\nS\nI\nR\nB∑\n=\n=\n∑\n=\nB\nB\nA\nP\nA\nP\n)\n,\n(\n)\n(\n\"marginalization\"\n\nA more complex system\nBattery\n- Joint distribution sufficient for any inference:\nRadio\nIgnition\nGas\nStarts\nOn time to work\n)\n|\n(\n)\n,\n|\n(\n)\n(\n)\n|\n(\n)\n|\n(\n)\n(\n)\n,\n,\n,\n,\n,\n(\nS\nO\nP\nG\nI\nS\nP\nG\nP\nB\nI\nP\nB\nR\nP\nB\nP\nO\nS\nG\nI\nR\nB\nP\n=\n)\n(\n)\n|\n(\n)\n,\n|\n(\n)\n(\n)\n|\n(\n)\n|\n(\n)\n(\n)\n(\n)\n,\n(\n)\n|\n(\n,\n,\n,\nG\nP\nS\nO\nP\nG\nI\nS\nP\nG\nP\nB\nI\nP\nB\nR\nP\nB\nP\nG\nP\nG\nO\nP\nG\nO\nP\nS\nI\nR\nB ∑\n=\n=\n\nA more complex system\nBattery\n- Joint distribution sufficient for any inference:\nRadio\nIgnition\nGas\nStarts\nOn time to work\n)\n|\n(\n)\n,\n|\n(\n)\n(\n)\n|\n(\n)\n|\n(\n)\n(\n)\n,\n,\n,\n,\n,\n(\nS\nO\nP\nG\nI\nS\nP\nG\nP\nB\nI\nP\nB\nR\nP\nB\nP\nO\nS\nG\nI\nR\nB\nP\n=\n∑∑\n⎟\n⎟\n⎠\n⎞\n⎜\n⎜\n⎝\n⎛\n=\n=\nS\nI\nB\nS\nO\nP\nG\nI\nS\nP\nB\nI\nP\nB\nP\nG\nP\nG\nO\nP\nG\nO\nP\n)\n|\n(\n)\n,\n|\n(\n)\n|\n(\n)\n(\n)\n(\n)\n,\n(\n)\n|\n(\n,\n\nA more complex system\nBattery\n- Joint distribution sufficient for any inference:\n- General inference algorithms via local computations\n- for graphs without loops: belief propagation\n- in general: variable elimination, junction tree\nRadio\nIgnition\nGas\nStarts\nOn time to work\n)\n|\n(\n)\n,\n|\n(\n)\n(\n)\n|\n(\n)\n|\n(\n)\n(\n)\n,\n,\n,\n,\n,\n(\nS\nO\nP\nG\nI\nS\nP\nG\nP\nB\nI\nP\nB\nR\nP\nB\nP\nO\nS\nG\nI\nR\nB\nP\n=\n\nMore concrete representation\nBurglary\nAlarm\nEarthquake\nJohnCalls\nMaryCalls\n\nMore concrete representation\nBurglary\nAlarm\nEarthquake\nJohnCalls\nMaryCalls\nP(B)\n0.001\nP(E)\n0.002\nB E P(A|B,E)\n0 0 0.001\n0 1 0.29\n1 0 0.94\n1 1 0.95\nA P(J|A)\n0 0.05\n1 0.90\nA P(M|A)\n0 0.01\n1 0.70\n\"CPT\"\n\nParameterizing the CPT\nSize of CPT is exponential in number of\nparents. Often use a simpler parameterization\nbased on knowledge of how causes interact.\n\nParameterizing the CPT\nSize of CPT is exponential in number of\nparents. Often use a simpler parameterization\nbased on knowledge of how causes interact.\n- Logical OR: Independent deterministic causes\nBurglary\nAlarm\nEarthquake\nB E P(A|B,E)\n0 0 0\n0 1 1\n1 0 1\n1 1 1\n\nParameterizing the CPT\nSize of CPT is exponential in number of\nparents. Often use a simpler parameterization\nbased on knowledge of how causes interact.\n- Noisy OR: Independent probabilistic causes\nBurglary\nAlarm\nEarthquake\nB E P(A|B,E)\n0 0 0\n0 1 wB\n1 0 wE\n1 1 wB +(1-wB )wE\n\nParameterizing the CPT\nSize of CPT is exponential in number of\nparents. Often use a simpler parameterization\nbased on knowledge of how causes interact.\n- AND: cause + enabling condition\nBurglary\nAlarm\nElectricity\nB E P(A|B,E)\n0 0 0\n0 1 0\n1 0 0\n1 1 1 (or wB)\n\nParameterizing the CPT\nSize of CPT is exponential in number of\nparents. Often use a simpler parameterization\nbased on knowledge of how causes interact.\n- Logistic: Independent probabilistic causes\nwith varying strengths wi and a threshold θ\nChild 1 upset\nParent upset\nChild 2 upset\nC1 C2 P(Pa|C1,C2)\n0 0\n0 1\n1 0\n1 1\n[\n]\n[\n]\n[\n]\n[\n])\nexp(\n/\n)\nexp(\n/\n)\nexp(\n/\n)\nexp(\n/\nw\nw\nw\nw\n-\n-\n+\n-\n+\n-\n+\n+\nθ\nθ\nθ\nθ\n\nExplaining away\n- Logical OR: Independent deterministic causes\nBurglary\nAlarm\nEarthquake\nB E P(A|B,E)\n0 0 0\n0 1 1\n1 0 1\n1 1 1\n\nExplaining away\n- Logical OR: Independent deterministic causes\nBurglary\nAlarm\nEarthquake\nB E P(A|B,E)\n0 0 0\n0 1 1\n1 0 1\n1 1 1\nA priori, no correlation between B and E:\n∑\n=\nA\nA\nE\nB\nP\nE\nB\nP\n)\n,\n,\n(\n)\n,\n(\n\nExplaining away\n- Logical OR: Independent deterministic causes\nBurglary\nAlarm\nEarthquake\nB E P(A|B,E)\n0 0 0\n0 1 1\n1 0 1\n1 1 1\nA priori, no correlation between B and E:\n)\n(\n)\n(\n)\n,\n|\n(\n)\n,\n(\nE\nP\nB\nP\nE\nB\nA\nP\nE\nB\nP\nA∑\n=\n∏\n∈\n=\n}\n,\n,\n{\n])\n[\nparents\n|\n(\n)\n,\n,\n(\nC\nB\nA\nV\nV\nV\nP\nC\nB\nA\nP\n\nExplaining away\n- Logical OR: Independent deterministic causes\nBurglary\nAlarm\nEarthquake\nB E P(A|B,E)\n0 0 0\n0 1 1\n1 0 1\n1 1 1\nA priori, no correlation between B and E:\n)\n(\n)\n(\n)\n,\n|\n(\n)\n,\n(\nE\nP\nB\nP\nE\nB\nA\nP\nE\nB\nP\nA∑\n=\n=1, for any values of B and E\n\nExplaining away\n- Logical OR: Independent deterministic causes\nBurglary\nAlarm\nEarthquake\nB E P(A|B,E)\n0 0 0\n0 1 1\n1 0 1\n1 1 1\nA priori, no correlation between B and E:\n)\n(\n)\n(\n)\n,\n(\nE\nP\nB\nP\nE\nB\nP\n=\n\nExplaining away\n- Logical OR: Independent deterministic causes\nBurglary\nAlarm\nEarthquake\nB E P(A|B,E)\n0 0 0\n0 1 1\n1 0 1\n1 1 1\nAfter observing A=1 ...\n)1\n(\n)\n(\n)\n(\n)\n,\n|1\n(\n)1\n|\n,\n(\n=\n=\n=\n=\nA\nP\nE\nP\nB\nP\nE\nB\nA\nP\nA\nE\nB\nP\n/\n)\n(\n)\n(\n=\n=\nE\nP\nB\nP\nAssume\n)\n(\n)\n(\n)\n,\n|1\n(\nE\nP\nB\nP\nE\nB\nA\nP\n=\n∝\n\nExplaining away\n- Logical OR: Independent deterministic causes\nBurglary\nAlarm\nEarthquake\nB E P(A|B,E)\n0 0 0\n0 1 1\n1 0 1\n1 1 1\nAfter observing A=1 ...\n)\n,\n|1\n(\n)1\n|\n,\n(\nE\nB\nA\nP\nA\nE\nB\nP\n=\n∝\n=\n\nExplaining away\n- Logical OR: Independent deterministic causes\nBurglary\nAlarm\nEarthquake\nB E P(A|B,E)\n0 0 0\n0 1 1\n1 0 1\n1 1 1\nAfter observing A=1 ...\n... P(B|A=1) = 2/3\nB and E are anti-correlated\n)\n,\n|1\n(\n)1\n|\n,\n(\nE\nB\nA\nP\nA\nE\nB\nP\n=\n∝\n=\n\nExplaining away\n- Logical OR: Independent deterministic causes\nBurglary\nAlarm\nEarthquake\nB E P(A|B,E)\n0 0 0\n0 1 1\n1 0 1\n1 1 1\nAfter observing A=1, E=1 ...\n... P(B|A=1) = 1/2\nBack to P(B).\n)1\n,\n|1\n(\n)1\n,1\n|\n(\n=\n=\n∝\n=\n=\nE\nB\nA\nP\nE\nA\nB\nP\n\"Explaining away\" or\n\"Causal discounting\"\n\nExplaining away\n- Depends on the functional form (the\nparameterization) of the CPT\n- OR or Noisy-OR: Discounting\n- AND: No Discounting\n- Logistic: Discounting or Augmenting\n\n- Observing rain, Wet becomes more active.\n- Observing grass wet, Rain and Sprinkler become\nmore active.\n- Observing grass wet and sprinkler, Rain cannot\nbecome less active. No explaining away!\n- Excitatory links: Rain\nWet, Sprinkler\nWet\nSpreading activation or recurrent\nneural networks\nRain\nSprinkler\nGrass Wet\n\nSpreading activation or recurrent\nneural networks\nRain\nSprinkler\nGrass Wet\n- Observing grass wet, Rain and Sprinkler become\nmore active.\n- Observing grass wet and sprinkler, Rain becomes\nless active: explaining away.\n- Excitatory links: Rain\nWet, Sprinkler\nWet\n- Inhibitory link: Rain\nSprinkler\n\nSpreading activation or recurrent\nneural networks\nRain\n- Each new variable requires more inhibitory\nconnections.\n- Interactions between variables are not causal.\n- Not modular.\n- Whether a connection exists depends on what other\nconnections exist, in non-transparent ways.\n- Combinatorial explosion.\nSprinkler\nGrass Wet\nBurst pipe\n\nSummary\nBayes nets, or directed graphical models, offer\na powerful representation for large\nprobability distributions:\n- Ensure tractable storage, inference, and\nlearning\n- Capture causal structure in the world and\ncanonical patterns of causal reasoning.\n- This combination is not a coincidence."
    },
    {
      "category": "Resource",
      "title": "nov_9_2004_final.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-66j-computational-cognitive-science-fall-2004/876b9e443585b9fa1a4882bab77ccacf_nov_9_2004_final.pdf",
      "content": "Outline\n- Controlling complexity in Bayesian neural\nnetworks\n- Controlling complexity in infinite mixture\nmodels\n- Discussion\n- Computational strengths and weaknesses\n- Cognitive relevance\n\nHow to choose control parameters?\n- Bayesian Occam's razor\n\nDemo\n- Smaller weights (higher α) yield simpler\nmodels\n- neural_net.m\n- architecture:\n- 2 inputs\n- 1 output\n- 100 hidden units\n\nTwo approaches to choosing\ncontrol parameters\n- Evidence maximization (traditional Bayesian\nOccam's razor).\n- Automatic relevance determination (ARD).\n\nHow to choose control parameter?\n- Bayesian Occam's razor\n\nEvidence maximization\nImage removed due to\ncopyright considerations.\nθ :\nWeight\nspace\n\nBayesian Occam's Razor\npeak height x width\nImages removed due to\ncopyright considerations.\n\nBayesian Occam's Razor\npeak height\nx width\nImages removed due to\ncopyright considerations.\n\nBayesian Occam's Razor\nImages removed due to\ncopyright considerations.\n\nBayesian Occam's Razor\nImages removed due to\ncopyright considerations.\n\nMultiple levels of inference\nImage removed due to\ncopyright considerations.\nDifferent architectures:\n# number of hidden layers,\nkinds of hidden units, etc.\n\nAutomatic Relevance Determination\n- Relation to Kruschke's \"Backprop with attentional\nweights on inputs\".\n- Could specify different classes of features, and learn\nwhich class is most relevant for a given classification.\n- Shape and material properties in word learning\n- Internal anatomy versus surface markings in biological\nclassification.\n- Applied to weights from hidden units to output units,\ncan effectively infer \"size\" of bottleneck hidden layer.\n- Can apply the same idea to other probabilistic\nmodels, e.g., sparseness priors in generative models.\n\nComparison with cross-validation\n- Advantages:\n- Clear theoretical justification.\n- Uses all of the data.\n- Works with many control parameters.\n- Optimize over control parameters in parallel to (or\ninstead of) optimizing over model parameters.\n- Works well in practice (Neal's ARD triumph)\n- Disadvantages\n- Not as intuitive\n\nComparison with SVMs\n- A deep similarity\n- Classification using a model with as many free\nparameters as possible.\n- Control complexity via sparseness\n- Some differences\n- SVM (max margin hyperplane) uses data vectors\nsparsely, while ARD uses features sparsely.\n- SVM is rotationally invariant; ARD is not.\n- ARD solution may be more interpretable.\n- ARD idea more extendable.\n\nComparison with SVMs\n- What makes a good model?\n- SVM (PAC learning approach): high probability of\ngood generalization\n- Bayesian Occam's razor: most likely to be the\nmodel that generated the data.\n- In a non-parametric setting, generalization\nguarantees seem desirable.\n- PAC-Bayesian theorems (MacAllester, 1998 ff)\n\n- PAC-Bayes error bounds for stochastic\nmodel selection (McAllester 1998):\n- Given model class T, classify by choosing\nconsistent hypotheses in T in proportion to their\nprobability.\n- For any model class T and any d > 0, with\nprobability 1- d over the choice of an I.I.D.\nsample of m labeled instances Yobs, the\nexpected error rate of classifying based on\nis bounded by:\nm\nm\nδ\nT\nY\np\nobs\nln\n)\n|\n(\nln\n+\n+\n+\nLabel evidence:\nThe better the model class fits the observed\nlabels, the tighter the bound on generalization.\n\nComparison with SVMs\n- What makes a good model?\n- SVM (PAC learning approach): high probability of\ngood generalization\n- Bayesian Occam's razor: most likely to be the\nmodel that generated the data.\n- In a non-parametric setting, generalization\nguarantees seem desirable.\n- PAC-Bayesian theorems (MacAllester, 1998 ff)\n- PAC-Bayes-MDL (Langford and Blum)\n\nOutline\n- Controlling complexity in Bayesian neural\nnetworks\n- Controlling complexity in infinite mixture\nmodels\n- Discussion\n- Computational strengths and weaknesses\n- Cognitive relevance\n\nOutline\n- Controlling complexity in Bayesian neural\nnetworks\n- Controlling complexity in infinite mixture\nmodels\n- Discussion\n- Computational strengths and weaknesses\n- Cognitive relevance\n\nAdvantages of the infinite mixture relative\nto finite model w/ Bayesian Occam's razor\n- Allows number of classes to grow as indicated by the\ndata.\n- Doesn't require that we commit to a fixed -- or even\nfinite -- number of classes.\n- Computationally much simpler than applying\nBayesian Occam's razor to finite mixture models of\nvarying sizes, or thorough cross-validation\nprocedures. Experience this yourself....\n- Use of MCMC avoids problem of local minima in EM\napproach to learning finite mixture models.\n- BUT: Do we lose the \"objective\" nature of our\ncomplexity control?\n\nUnsupervised learning of\ntopic hierarchies\n(Blei, Griffiths, Jordan & Tenenbaum, NIPS 2003)\nBlei, D., T. L. Griffiths, M. I. Jordan, and J. B. Tenenbaum. \"Hierarchical Topic Models and the\nNested Chinese Restaurant Process.\" Advances in Neural Information Processing Systems 16 (2004).\nImage removed due to copyright considerations. Please see:\n\nA generative model for hierarchies\nNested Chinese Restaurant Process:\nImage removed due to\ncopyright considerations.\nImage removed due to\ncopyright considerations.\n\nJ. ACM abstracts\nImage removed due to\ncopyright considerations.\nImage removed due to\ncopyright considerations."
    },
    {
      "category": "Resource",
      "title": "sept_9_2004.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-66j-computational-cognitive-science-fall-2004/b921ea18146b47f94d0109fa446eb50c_sept_9_2004.pdf",
      "content": "9.66 / 9.914 Computational\nCognitive Science\nJosh Tenenbaum\n\nWhat is this class?\n- An attempt to see how recent work in\ncomputation (AI, machine learning,\nstatistics) can inform some of the core\nquestions of cognitive science.\n- ... and vice versa.\n\nThe questions\n- What forms does our knowledge of the\nworld take?\n- What are the inductive principles that allow\nus to acquire new knowledge from the\ninteraction of prior knowledge with\nobserved data?\n- What kinds of data must be available to\nhuman learners, and what kinds of innate\nknowledge (if any) must they have?\n\nGoals for the term\n- Setting up the big questions.\n- Providing some computational tools for\nanswering them rigorously and precisely.\n- Exploring some case studies in cognition:\n- Concept learning and categorization.\n- Learning causal relations.\n- The structure and formation of intuitive theories of\nphysical, biological and social systems.\n- The acquisition of natural language (syntax and\nsemantics).\n- Theory of mind: how we understand the behavior and\nmental states of other people.\n\nRequirements and grading\n- Four problem sets. (40%)\n- Minimal programming in Matlab.\n- Experience working with real cognitive data.\n- For students already familiar with the relevant\ncomputational techniques, e.g., through 6.825\nor 6.867, a more extensive modeling project\ncan be substituted for one or more problem sets.\n\nRequirements and grading\n- Term paper or project. (40%)\n- Due on the last day of the term (before finals).\n- Two options:\n- Theoretical paper (15-20 pages).\n- Computational modeling project + short write-up\n(e.g. 4 page conference format).\n- Undergraduates may implement and extend an\nexisting model.\n- Graduate students must make an original\nresearch contribution.\n\nRequirements and grading\n- Discussion notes responding to the readings (and\nquestions) for each week. (20%)\n- Short (~1 paragraph) responses to the readings and\nquestions -- not just summaries.\n- Due by 10 am on the day of class.\n- Submit electronically.\n- Must submit 20 notes for full credit. These can include\nshort responses to other people's posts, as long as the\nresponses are thoughtful and in some way address the\nreadings and questions.\n\nRelation to other classes\n- 9.52\n- 9.012\n- Other computational classes in course 9\n- 6.034 or 6.825\n- 6.867\n\nBackground?\n- Grad or undergrad?\n- Matlab? C or Java?\n- Taken graduate AI? Undergraduate AI? Machine\nlearning?\n- Who knows about:\n- Bayes' rule\n- Eigenvector\n- Support vector machine\n- Bayes net\n- Markov-equivalent networks\n- First-order logic\n- Probabilistic relational models\n\nThe problem of induction\n\nInduction versus Deduction\n- Deductive reasoning:\n- Inductive reasoning:\nSocrates is a man.\nAll men are mortal.\nSocrates is mortal.\nSocrates is a man.\nSocrates is mortal.\nAll men are mortal.\n\nInduction versus Deduction\n- Deductive reasoning:\n- Conclusion follows with certainty.\n- Validity depends only on syntax (form).\n- Argument evaluation is objective, independent\nof other knowledge available.\nPlato is a snark.\nAll snarks are boojums.\nPlato is a boojum.\n\nAristotle\n- Prior analytics: How can we reliably infer\nnew truths from known truths?\n- A theory: Symbolic logic.\n- The syllogism:\nSocrates is a man\nAll men are mortal\nSocrates is a mortal\nAll B are C\nA is a B\nA is a C\nPlato is a snark\nAll snarks are boojums\nPlato is a boojum\n. . .\n\nInduction versus Deduction\n- Inductive reasoning:\n- Conclusion follows with more or less probability.\n- Probability depends on semantics (meaning).\n- Argument evaluation is subjective, depends on\nother knowledge available.\nSocrates is a man.\nSocrates is 47 years old.\nAll men are 47 years old?\n\nAristotle\n- Prior analytics: How can we reliably infer\nnew truths from known truths?\n- A theory: Symbolic logic.\n- A schema for induction:\nx1 is P\nx2 is P\nxn is P\nx1, x2, . . . , xn are all the X's\nEvery X is P\n. . .\n\nAristotle\n- Prior analytics: How can we reliably infer\nnew truths from known truths?\n- A theory: Symbolic logic.\n- A schema for induction:\nx1 is P\nx2 is P\nxn is P\nx1, x2, . . . , xn are all the X's\nEvery X is P\n. . .\nHorses are long-lived\nMen are long-lived\nCamels are long-lived\nHorses, men, and camels\nare all the bile-less animals\nAll bile-less animals are\nlong-lived\n\nThe great problem of philosophy\n- John Stuart Mill (A System of Logic, 1843):\n\"Why is a single instance, in some cases,\nsufficient for a complete induction, while in\nothers myriads of concurring instances, without\na single exception known or presumed, go such\na very little way towards establishing a general\nproposition? Whoever can answer this question\nknows more of the philosophy of logic than the\nwisest of the ancients, and has solved the\nproblem of Induction.\"\n\nComputational approaches to\ninduction\nTwo main ingredients:\n- Knowledge representation: how to capture\nthe structure of the world\n- Statistics: how to capture the structure of\nthe world.\n\nStructure versus statistics\nRules\nLogic\nSymbols\nStatistics\nSimilarity\nTypicality\nImage removed due to\ncopyright considerations.\n\nA better metaphor\nImage removed due to copyright considerations.\n\nA better metaphor\nImage removed due to copyright considerations.\n\nStructure and statistics\nImage removed due to copyright considerations.\n\nCognition as inductive\ninference\n\nInduction in everyday reasoning\n- Generalizing from examples.\nSquirrels can get avian flu.\nGorillas can get avian flu.\nHorses can get avian flu.\n\nLearning concepts and words\nfrom examples\nImage removed due to copyright considerations.\n\nThe objects of planet Gazoob\nImage removed due to copyright considerations.\n\nInduction in everyday reasoning\n- Generalizing from examples.\n- Diagnosis of causes given effects.\n- A woman at age 40 tests positive on a routine\nmammogram screening. How likely is it that\nshe has breast cancer?\n\nInference from novel events\n- Is a woman's chance of breast cancer higher or\nlower if:\n- she tests positive twice in a row?\n- she tests positive first, takes it again and tests\nnegative the second time?\n- she is 45 years old instead of 40?\n- she lives near a nuclear power plant?\n- she has never had a routine screening, but instead\nchose to get a mammogram because she felt a\nlump?\n\nInduction in everyday reasoning\n- Generalizing from examples.\n- Diagnosis of causes given effects.\n- Inferring causal relations from patterns of\ncorrelation.\n- A drug intended to treat a chronic medical\ncondition is found to improve the condition in\n141/250 test patients. Does the drug work?\n- Does prayer improve recovery from illness?\n\nInduction in everyday reasoning\n- Generalizing from examples.\n- Diagnosis of causes given effects.\n- Inferring causal relations from patterns of\ncorrelation.\n- Discovering hidden causes from patterns of\ncoincidence.\n- Is a cluster of disease cases just due to chance,\nor to a previously unknown cause?\n- Guillon-Barre, AIDS, Lyme disease.\n\nVisual perception\nImage removed due to copyright considerations.\n\nAmbiguity in visual perception\nThree-dimensional:\nTwo-dimensional:\n\nThe checkerboard illusion\nsame\nThe visual system wants to\ninfer the color of the checks\nin the world, not the gray\nvalue in the image.\nThe \"illusion\" reflects the\nsuccessful design of the\nvisual system, not a quirky\nfailure.\nInference to the\ncasually deepest\nexplanation:\nCourtesy of Edward Adelson. Used with permission.\n\nApparent motion\n- Visual system parses ambiguous experience\ninto objects under several assumptions:\n- Objects typically do not disappear and appear\nspontaneously.\n- Objects typically follow \"simple\" space-time\ntrajectories.\n\nThe perception of causality\n- Michotte demos\n- Heider and Simmel\n\nMarr's three levels\n- Level 1: Computational theory\n- What is the goal of the computation, and what is the\nlogic by which it is carried out?\n- Level 2: Representation and algorithm\n- How is information represented and processed to\nachieve the computational goal?\n- Level 3: Hardware implementation\n- How is the computation realized in physical or\nbiological hardware?\n\nLanguage\n- Parsing:\n- Two cars were reported stolen by the Groveton police\nyesterday.\n- The judge sentenced the killer to die in the electric chair\nfor the second time.\n- No one was injured in the blast, which was attributed to\na buildup of gas by one town official.\n- One witness told the commissioners that she had seen\nsexual intercourse taking place between two parked\ncars in front of her house.\n\nLanguage\n- Parsing\n- Acquisition:\n- Learning the English past tense (rule vs.\nexceptions)\n- Learning the Spanish or Arabic past tense\n(multiple rules plus exceptions)\n- Learning verb argument structure (\"give\" vs.\n\"donate\")\n- Learning to be bilingual.\n\nIntuitive theories\n- Physics\n- Parsing: Inferring support relations, or the causal\nhistory and properties of an object.\n- Acquisition: Learning about gravity and support.\n- Gravity -- what's that?\n- Contact is sufficient\n- Mass distribution and location is important\n- Psychology\n- Parsing: Mind reading, or causal attribution.\n- Acquisition: Learning about agents\n- Recognizing intentionality, but without mental state reasoning\n- Reasoning about beliefs and desires\n- Reasoning about plans, rationality and \"other minds\".\n\nSome philosophical puzzles\n\nHume's problem\n- Can induction be justified?\n- E.g., Can we really know the sun will rise\ntomorrow?\n- Only on the assumption of uniformity of\nnature: the future will be like the past.\n- But what's the justification for that?\n\nA modern answer to Hume?\nComputational learning theory:\n- PAC (Probably Approximately Correct):\nGiven that a rule f has held for examples 1 ... n,\ncan we say with high probability (1-δ) that the\nrule will hold in most (1-ε) future cases?\n- Often the answer is \"yes\", even without\nknowing how the examples were generated.\n- But... still requires uniformity of nature.\n\nGoodman's problem\n- Why do some hypotheses receive\nconfirmation from examples but not others?\n- \"All piece of copper conduct electricity\": yes\n- \"All men in this room are third sons\": no\n- Distinguishing lawlike hypotheses from\naccidental hypotheses is not easy:\n- \"All emeralds are green\"\n- \"All emeralds are grue\", where grue means \"if\nobserved before t, green; else, blue.\"\n\nResponses to Goodman\n- First instinct is a syntactic response:\n- Hypotheses without arbitrary free\nparameters are more lawlike.\n- Simpler (shorter) hypotheses are more\nlawlike.\n\nSyntactic levers for induction\n- Which hypothesis is better supported by the\nevidence?\n- All blickets are chromium.\n- All blickets are chromium and arch-shaped.\n- All blickets are chromium or arch-shaped.\n- Which curve is best supported by the data?\nFigure by MIT OCW.\n\nResponses to Goodman\n- Hypotheses without arbitrary free\nparameters are more lawlike.\n- Simpler (shorter) hypotheses are more\nlawlike.\n- But \"green\" and \"grue\" are logically\nsymmetric:\n- To a Martian who sees grue and bleen, green\njust means \"if observed before t, grue; else,\nbleen.\"\n\nResponses to Goodman\n- Hypotheses without arbitrary free\nparameters are more lawlike.\n- Simpler (shorter) hypotheses are more\nlawlike.\n- But \"green\" and \"grue\" are logically\nsymmetric.\n- Lawlike is a semantic (not syntactic) notion,\nand depends on prior subjective knowledge\n(not strictly objective world structure).\n\nThe origin of good hypotheses\n- Nativism\n- Plato, Kant\n- Chomsky, Fodor\n- Empiricism\n- Strong: Watson, Skinner\n- Weak: Bruner, cognitive psychology, statistical\nmachine learning\n- Constructivism\n- Goodman, Piaget, Carey, Gopnik\n- AI threads....\n\nPlato\n- Meno: Where does our knowledge of\nabstract concepts (e.g., virtue, geometry)\ncome from?\n- The puzzle: \"A man cannot enquire about\nthat which he does not know, for he does\nnot know the very subject about which he\nis to enquire.\"\n\nPlato\n- Meno: Where does our knowledge of\nabstract concepts (e.g., virtue, geometry)\ncome from?\n- A theory: Learning as \"recollection\".\n- The Talmud's version:\n\"Before we are born, while in our mother's womb, the Almighty\nsends an angel to sit beside us and teach us all the wisdom we will\never need to know about living. Then, just before we are born, the\nangel taps us under the nose (forming the philtrum, the\nindentation that everyone has under their nose), and we forget\neverything the angel taught us.\"\n\nPlato meets Matlabtm\nWhat is the relation between y and x?\n0.1\n1.5\n2.5\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.5\nFigure by MIT OCW.\n\nPlato meets Matlabtm\nWhat is the relation between y and x?\n0.1\n1.5\n2.5\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.5\nFigure by MIT OCW.\n\nPlato meets Matlabtm\nWhat is the relation between y and x?\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n-1.5\n1.5\n-0.5\n0.5\n-1\nFigure by MIT OCW.\n\nPlato meets Matlabtm\nWhat is the relation between y and x?\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n-1.5\n1.5\n-0.5\n0.5\n-1\nFigure by MIT OCW.\n\nThe legacy of Plato\n- \"A man cannot enquire about that which he\ndoes not know, for he does not know the\nvery subject about which he is to enquire.\"\n- We can't learn abstractions from data if in\nsome sense we didn't already know what to\nlook for.\n- Chomsky's \"poverty of the stimulus\" argument\nfor the innateness of language.\n- Fodor's argument for the innateness of all\nconcepts.\n\nThe origin of good hypotheses\n- Nativism\n- Plato, Kant\n- Chomsky, Fodor\n- Empiricists\n- Strong: Watson, Skinner\n- Weak: Bruner, cognitive psychology, statistical\nmachine learning\n- Constructivists\n- Goodman, Piaget, Carey, Gopnik\n- AI threads....\n\nImage removed due to copyright considerations.\nBruner, Jerome S., Jacqueline J. Goodnow, and George Austin. A Study in\nThinking. Somerset, NJ: Transaction Publishers, 1986. ISBN: 0887386563.\nPlease see:\n\nImage removed due to copyright considerations. Please see:\nHull. \"Qualitative Aspects of the Evolution of Concepts.\"\nPsychological Monograph 28, no. 123 (1920).\n\nFigure by MIT OCW.\n\"striped and\nthree borders\"\n\nFodor's critique\n- This isn't really concept learning, it's just belief\nfixation.\n- To learn the rule \"striped and three borders\", the learner\nmust already have the concepts \"striped\", \"three\nborders\", and \"and\", and the capacity to put these\ncomponents together.\n- In other words, the learner already has the concept, and\nis just forming a new belief about how to respond on\nthis particular task.\n- More generally, all inductive learning seems to\nrequire the constraints of a hypothesis space -- so\nthe learner must begin life with all the concepts\nthey will ever learn. How depressing.\n\nFodor's critique\nRaises major questions for cognitive\ndevelopment, machine learning, and AI:\n- Is it ever possible to learn truly new concepts,\nwhich were not part of your hypothesis space to\nbegin with?\n- What conceptual resources must be innate?\n- Objects?\n- First-order logic?\n- Recursion?\n- Causality?\n\nThe origin of good hypotheses\n- Nativism\n- Plato, Kant\n- Chomsky, Fodor\n- Empiricists\n- Strong: Watson, Skinner\n- Weak: Bruner, cognitive psychology, statistical\nmachine learning\n- Constructivists\n- Goodman, Piaget, Carey, Gopnik\n- AI threads....\n\nGoodman's answer to Goodman\n- More lawlike hypotheses are based on\n\"entrenched\" predicates: green is more entrenched\nthan grue.\n- How does a predicate become entrenched? Is it\nsimple statistics: how often the predicate has\nsupported successful inductions in the past?\n- Suppose grue means \"If observed on Earth, green;\nif on Mars, blue.\"\n- Entrenchment could come through experience, but\ncould also derive from a causal theory. Theory\nsupported by experience seems best.\n\nHow do theories work?\n- See this look? It's called \"chromium\".\n- Here are some blickets:\n- Which hypothesis is more lawlike?\n- \"All blickets are chromium\"\n- \"All blickets are chromirose\", where chromirose means\n\"if observed before t, chromium; else rose-colored.\"\n\nHow do theories work?\n- Theories depend on abstract categories.\n- E.g., chromium is a kind of color or material.\n- Abstract categories depend on theories.\n- E.g., species, magnetic pole\n- Theories support hypotheses for completely\nnovel situations.\n- Big open questions:\n- What is a theory, formally?\n- How are theories learned?"
    },
    {
      "category": "Resource",
      "title": "oct_12_2004_fin.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-66j-computational-cognitive-science-fall-2004/0c18b39d66e8cc6e2125099193dc722e_oct_12_2004_fin.pdf",
      "content": "Compression in Bayes nets\n- A Bayes net compresses the joint\nprobability distribution over a set of\nvariables in two ways:\n- Dependency structure\n- Parameterization\n- Both kinds of compression derive from\ncausal structure:\n- Causal locality\n- Independent causal mechanisms\n\nDependency structure\n=\n)\n,\n,\n,\n,\n(\nM\nJ\nA\nE\nB\nP\n)\n|\n(\n)\n|\n(\n)\n,\n|\n(\n)\n(\n)\n(\nA\nM\nP\nA\nJ\nP\nE\nB\nA\nP\nE\nP\nB\nP\nBurglary\nAlarm\nEarthquake\nJohnCalls\nMaryCalls\n=\n)\n,\n,\n( 1\nn\nV\nV\nP\nK\n∏\n=\nn\ni\ni\ni\nV\nV\nP\n])\n[\nparents\n|\n(\nGraphical model asserts:\n\nDependency structure\n)\n,\n,\n,\n|\n(\n)\n,\n,\n|\n(\n)\n,\n|\n(\n)\n|\n(\n)\n(\nJ\nA\nE\nB\nM\nP\nA\nE\nB\nJ\nP\nE\nB\nA\nP\nB\nE\nP\nB\nP\n=\n)\n,\n,\n,\n,\n(\nM\nJ\nA\nE\nB\nP\nBurglary\nAlarm\nEarthquake\nJohnCalls\nMaryCalls\n∏\n=\n-\nn\ni\ni\ni\nV\nV\nV\nP\n)\n,\n,\n|\n(\nK\n=\n)\n,\n,\n( 1\nn\nV\nV\nP\nK\nFor any distribution:\n\nParameterization\nBurglary\nAlarm\nEarthquake\nJohnCalls\nMaryCalls\nP(B)\n0.001\nP(E)\n0.002\nB E P(A|B,E)\n0 0 0.001\n0 1 0.29\n1 0 0.94\n1 1 0.95\nA P(J|A)\n0 0.05\n1 0.90\nA P(M|A)\n0 0.01\n1 0.70\nFull CPT\n\nParameterization\nBurglary\nAlarm\nEarthquake\nJohnCalls\nMaryCalls\nP(B)\n0.001\nP(E)\n0.002\nA P(J|A)\n0 0.05\n1 0.90\nA P(M|A)\n0 0.01\n1 0.70\nB E P(A|B,E)\n0 0 0\n0 1 wB = 0.29\n1 0 wE = 0.94\n1 1 wB +(1-wB )wE\nNoisy-OR\nFIX FOR NEXT\nYEAR Wb and We\n\nOutline\n- The semantics of Bayes nets\n- role of causality in structural compression\n- Explaining away revisited\n- role of causality in probabilistic inference\n- Sampling algorithms for approximate\ninference in graphical models\n\nOutline\n- The semantics of Bayes nets\n- role of causality in structural compression\n- Explaining away revisited\n- role of causality in probabilistic inference\n- Sampling algorithms for approximate\ninference in graphical models\n\nGlobal semantics\n∏\n=\n=\nn\ni\ni\ni\nn\nV\nV\nP\nV\nV\nP\n])\n[\nparents\n|\n(\n)\n,\n,\n(\nK\nJoint probability distribution factorizes into product\nof local conditional probabilities:\nBurglary\nAlarm\nEarthquake\nJohnCalls\nMaryCalls\n=\n)\n,\n,\n,\n,\n(\nM\nJ\nA\nE\nB\nP\n)\n|\n(\n)\n|\n(\n)\n,\n|\n(\n)\n(\n)\n(\nA\nM\nP\nA\nJ\nP\nE\nB\nA\nP\nE\nP\nB\nP\n\nGlobal semantics\n∏\n=\n=\nn\ni\ni\ni\nn\nV\nV\nP\nV\nV\nP\n])\n[\nparents\n|\n(\n)\n,\n,\n(\nK\nJoint probability distribution factorizes into product\nof local conditional probabilities:\nBurglary\nAlarm\nEarthquake\nJohnCalls\nMaryCalls\nNecessary to assign a probability to any possible world, e.g.\n)\n|\n(\n)\n|\n(\n)\n,\n|\n(\n)\n(\n)\n(\n)\n,\n,\n,\n,\n(\na\nm\nP\na\nj\nP\ne\nb\na\nP\ne\nP\nb\nP\nm\nj\na\ne\nb\nP\n¬\n¬\n¬\n¬\n=\n¬\n¬\n\nLocal semantics\nGlobal factorization is equivalent to a set of constraints\non pairwise relationships between variables.\n\"Markov property\": Each node is conditionally\nindependent of its non-descendants given its parents.\nZ1j\nZnj\nYn\nY1\nX\nUm\nU1\nFigure by MIT OCW.\n\nLocal semantics\nGlobal factorization is equivalent to a set of constraints\non pairwise relationships between variables.\n\"Markov property\": Each node is conditionally\nindependent of its non-descendants given its parents.\nAlso: Each node is marginally\n(a priori) independent of any\nnon-descendant unless they\nshare a common ancestor.\nZ1j\nZnj\nYn\nY1\nX\nUm\nU1\nFigure by MIT OCW.\n\nLocal semantics\nGlobal factorization is equivalent to a set of constraints\non pairwise relationships between variables.\nEach node is conditionally independent of all others\ngiven its \"Markov blanket\": parents, children,\nchildren's parents.\nFigure by MIT OCW.\nYn\nX\nU1\nZ1j\nY1\nUm\nZnj\n\nExample\nBurglary\nAlarm\nEarthquake\nJohnCalls\nMaryCalls\nJohnCalls and MaryCalls are marginally (a priori) dependent, but\nconditionally independent given Alarm. [\"Common cause\"]\nBurglary and Earthquake are marginally (a priori) independent,\nbut conditionally dependent given Alarm. [\"Common effect\"]\n\nConstructing a Bayes net\n- Model reduces all pairwise dependence and\nindependence relations down to a basic set\nof pairwise dependencies: graph edges.\n- An analogy to learning kinship relations\n- Many possible bases, some better than others\n- A basis corresponding to direct causal\nmechanisms seems to compress best.\n\nAn alternative basis\nSuppose we get the direction of causality wrong...\nBurglary\nAlarm\nEarthquake\nJohnCalls\nMaryCalls\n- Does not capture the dependence between callers:\nfalsely believes P(JohnCalls, MaryCalls) =\nP(JohnCalls) P(MaryCalls).\n\nAn alternative basis\nSuppose we get the direction of causality wrong...\nBurglary\nAlarm\nEarthquake\nJohnCalls\nMaryCalls\n- Inserting a new arrow captures this correlation.\n- This model is too complex: does not believe that\nP(JohnCalls, MaryCalls|Alarm) =\nP(JohnCalls|Alarm) P(MaryCalls|Alarm)\n\nAn alternative basis\nSuppose we get the direction of causality wrong...\nBurglary\nAlarm\nEarthquake\nJohnCalls\nMaryCalls\n- Does not capture conditional dependence of causes\n(\"explaining away\"): falsely believes that\nP(Burglary, Earthquake|Alarm) =\nP(Burglary|Alarm) P(Earthquake|Alarm)\n\nAn alternative basis\nSuppose we get the direction of causality wrong...\nBurglary\nAlarm\nEarthquake\nJohnCalls\nMaryCalls\n- Another new arrow captures this dependence.\n- But again too complex: does not believe that\nP(Burglary, Earthquake) =\nP(Burglary)P(Earthquake)\n\nSuppose we get the direction of causality wrong...\nBurglary\nAlarm\nEarthquake\nJohnCalls\nMaryCalls\nBillsCalls\nPowerSurge\n- Adding more causes or effects requires a\ncombinatorial proliferation of extra arrows. Too\ngeneral, not modular, too many parameters....\n\nConstructing a Bayes net\n- Model reduces all pairwise dependence and\nindependence relations down to a basic set of\npairwise dependencies: graph edges.\n- An analogy to learning kinship relations\n- Many possible bases, some better than others\n- A basis corresponding to direct causal\nmechanisms seems to compress best.\n- Finding the minimal dependence structure\nsuggests a basis for learning causal models.\n\nOutline\n- The semantics of Bayes nets\n- role of causality in structural compression\n- Explaining away revisited\n- role of causality in probabilistic inference\n- Sampling algorithms for approximate\ninference in graphical models\n\nExplaining away\n- Logical OR: Independent deterministic causes\nBurglary\nAlarm\nEarthquake\nB E P(A|B,E)\n0 0 0\n0 1 1\n1 0 1\n1 1 1\n\nExplaining away\n- Logical OR: Independent deterministic causes\nBurglary\nAlarm\nEarthquake\nB E P(A|B,E)\n0 0 0\n0 1 1\n1 0 1\n1 1 1\nA priori, no correlation between B and E:\n)\n(\n)\n(\n)\n,\n(\ne\nP\nb\nP\ne\nb\nP\n=\n\nExplaining away\n- Logical OR: Independent deterministic causes\nBurglary\nAlarm\nEarthquake\nB E P(A|B,E)\n0 0 0\n0 1 1\n1 0 1\n1 1 1\nAfter observing A = a ...\n)\n(\n)\n(\n)\n|\n(\n)\n|\n(\na\nP\nb\nP\nb\na\nP\na\nb\nP\n=\n= 1\n\nExplaining away\n- Logical OR: Independent deterministic causes\nBurglary\nAlarm\nEarthquake\nB E P(A|B,E)\n0 0 0\n0 1 1\n1 0 1\n1 1 1\nAfter observing A = a ...\n)\n(\n)\n(\n)\n|\n(\na\nP\nb\nP\na\nb\nP\n=\n)\n(b\nP\n>\nMay be a big increase if P(a) is small.\n\nExplaining away\n- Logical OR: Independent deterministic causes\nBurglary\nAlarm\nEarthquake\nB E P(A|B,E)\n0 0 0\n0 1 1\n1 0 1\n1 1 1\nAfter observing A = a ...\n)\n(\n)\n(\n)\n(\n)\n(\n)\n(\n)\n|\n(\ne\nP\nb\nP\ne\nP\nb\nP\nb\nP\na\nb\nP\n-\n+\n=\n)\n(b\nP\n>\nMay be a big increase if P(b), P(e) are small.\n\nExplaining away\n- Logical OR: Independent deterministic causes\nBurglary\nAlarm\nEarthquake\nB E P(A|B,E)\n0 0 0\n0 1 1\n1 0 1\n1 1 1\nAfter observing A = a, E= e, ...\n)\n|\n(\n)\n|\n(\n)\n,\n|\n(\n)\n,\n|\n(\ne\na\nP\ne\nb\nP\ne\nb\na\nP\ne\na\nb\nP\n=\nBoth terms = 1\n\nExplaining away\n- Logical OR: Independent deterministic causes\nBurglary\nAlarm\nEarthquake\nB E P(A|B,E)\n0 0 0\n0 1 1\n1 0 1\n1 1 1\nAfter observing A = a, E= e, ...\n)\n|\n(\n)\n|\n(\n)\n,\n|\n(\n)\n,\n|\n(\ne\na\nP\ne\nb\nP\ne\nb\na\nP\ne\na\nb\nP\n=\n\"Explaining away\" or\n\"Causal discounting\"\n)\n(\n)\n|\n(\nb\nP\ne\nb\nP\n=\n=\n\nExplaining away\n- Depends on the functional form (the\nparameterization) of the CPT\n- OR or Noisy-OR: Discounting\n- AND: No Discounting\n- Logistic: Discounting from parents with\npositive weight; augmenting from parents with\nnegative weight.\n- Generic CPT: Parents become dependent when\nconditioning on a common child.\n\nParameterizing the CPT\n- Logistic: Independent probabilistic causes\nwith varying strengths wi and a threshold θ\nChild 1 upset\nParent upset\nChild 2 upset\nC1 C2 P(Pa|C1,C2)\n0 0\n0 1\n1 0\n1 1\n[\n]\n[\n]\n[\n]\n[\n])\nexp(\n/\n)\nexp(\n/\n)\nexp(\n/\n)\nexp(\n/\nw\nw\nw\nw\n-\n-\n+\n-\n+\n-\n+\n+\nθ\nθ\nθ\nθ\nP(Pa|C1,C2)\nAnnoyance = C1* w1 +C2* w2\nThreshold θ\n\nContrast w/ conditional reasoning\nRain\nGrass Wet\nSprinkler\n- Formulate IF-THEN rules:\n- IF Rain THEN Wet\n- IF Wet THEN Rain\nIF Wet AND NOT Sprinkler\nTHEN Rain\n- Rules do not distinguish directions of inference\n- Requires combinatorial explosion of rules\n\nSpreading activation or recurrent\nneural networks\nBurglary\nEarthquake\nAlarm\n- Observing earthquake, Alarm becomes more active.\n- Observing alarm, Burglary and Earthquake become\nmore active.\n- Observing alarm and earthquake, Burglary cannot\nbecome less active. No explaining away!\n- Excitatory links: Burglary\nAlarm, Earthquake\nAlarm\n\nSpreading activation or recurrent\nneural networks\nBurglary\nEarthquake\nAlarm\n- Observing alarm, Burglary and Earthquake become\nmore active.\n- Observing alarm and earthquake, Burglary becomes\nless active: explaining away.\n- Excitatory links: Burglary\nAlarm, Earthquake\nAlarm\n- Inhibitory link: Burglar\nEarthquake\n\nSpreading activation or recurrent\nneural networks\nBurglary\nPower surge\nEarthquake\nAlarm\n- Each new variable requires more inhibitory\nconnections.\n- Interactions between variables are not causal.\n- Not modular.\n- Whether a connection exists depends on what other\nconnections exist, in non-transparent ways.\n- Combinatorial explosion of connections\n\nThe relation between PDP and\nBayes nets\n- To what extent does Bayes net inference\ncapture insights of the PDP approach?\n- To what extent do PDP networks capture or\napproximate Bayes nets?\n\nSummary\nBayes nets, or directed graphical models, offer\na powerful representation for large\nprobability distributions:\n- Ensure tractable storage, inference, and\nlearning\n- Capture causal structure in the world and\ncanonical patterns of causal reasoning.\n- This combination is not a coincidence.\n\nStill to come\n- Applications to models of categorization\n- More on the relation between causality and\nprobability:\n- Learning causal graph structures.\n- Learning causal abstractions (\"diseases\ncause symptoms\")\n- What's missing from graphical models\nCausal structure\nStatistical dependencies\n\nOutline\n- The semantics of Bayes nets\n- role of causality in structural compression\n- Explaining away revisited\n- role of causality in probabilistic inference\n- Sampling algorithms for approximate\ninference in graphical models\n\nMotivation\n- What is the problem of inference?\n- Reasoning from observed variables to\nunobserved variables\n- Effects to causes (diagnosis):\nP(Burglary = 1|JohnCalls = 1, MaryCalls = 0)\n- Causes to effects (prediction):\nP(JohnCalls = 1|Burglary = 1)\nP(JohnCalls = 0, MaryCalls = 0|Burglary = 1)\n\nMotivation\n- What is the problem of inference?\n- Reasoning from observed variables to\nunobserved variables.\n- Learning, where hypotheses are\nrepresented by unobserved variables.\n- e.g., Parameter estimation in coin flipping:\nd1\nd2\nd3\nd4\nP(H) = θ\nθ\n\nMotivation\n- What is the problem of inference?\n- Reasoning from observed variables to\nunobserved variables.\n- Learning, where hypotheses are\nrepresented by unobserved variables.\n- Why is it hard?\n- In principle, must consider all possible\nstates of all variables connecting input\nand output variables.\n\nA more complex system\nBattery\n- Joint distribution sufficient for any inference:\nRadio\nIgnition\nGas\nStarts\nOn time to work\n)\n|\n(\n)\n,\n|\n(\n)\n(\n)\n|\n(\n)\n|\n(\n)\n(\n)\n,\n,\n,\n,\n,\n(\nS\nO\nP\nG\nI\nS\nP\nG\nP\nB\nI\nP\nB\nR\nP\nB\nP\nO\nS\nG\nI\nR\nB\nP\n=\n)\n(\n)\n,\n,\n,\n,\n,\n(\n)\n(\n)\n,\n(\n)\n|\n(\n,\n,\n,\nG\nP\nO\nS\nG\nI\nR\nB\nP\nG\nP\nG\nO\nP\nG\nO\nP\nS\nI\nR\nB∑\n=\n=\n∑\n=\nB\nB\nA\nP\nA\nP\n)\n,\n(\n)\n(\n\"marginalization\"\n\nA more complex system\nBattery\n- Joint distribution sufficient for any inference:\nRadio\nIgnition\nGas\nStarts\nOn time to work\n)\n|\n(\n)\n,\n|\n(\n)\n(\n)\n|\n(\n)\n|\n(\n)\n(\n)\n,\n,\n,\n,\n,\n(\nS\nO\nP\nG\nI\nS\nP\nG\nP\nB\nI\nP\nB\nR\nP\nB\nP\nO\nS\nG\nI\nR\nB\nP\n=\n)\n(\n)\n|\n(\n)\n,\n|\n(\n)\n(\n)\n|\n(\n)\n|\n(\n)\n(\n)\n(\n)\n,\n(\n)\n|\n(\n,\n,\n,\nG\nP\nS\nO\nP\nG\nI\nS\nP\nG\nP\nB\nI\nP\nB\nR\nP\nB\nP\nG\nP\nG\nO\nP\nG\nO\nP\nS\nI\nR\nB ∑\n=\n=\n\nA more complex system\nBattery\n- Joint distribution sufficient for any inference:\nRadio\nIgnition\nGas\nStarts\nOn time to work\n)\n|\n(\n)\n,\n|\n(\n)\n(\n)\n|\n(\n)\n|\n(\n)\n(\n)\n,\n,\n,\n,\n,\n(\nS\nO\nP\nG\nI\nS\nP\nG\nP\nB\nI\nP\nB\nR\nP\nB\nP\nO\nS\nG\nI\nR\nB\nP\n=\n∑∑\n⎟\n⎟\n⎠\n⎞\n⎜\n⎜\n⎝\n⎛\n=\n=\nS\nI\nB\nS\nO\nP\nG\nI\nS\nP\nB\nI\nP\nB\nP\nG\nP\nG\nO\nP\nG\nO\nP\n)\n|\n(\n)\n,\n|\n(\n)\n|\n(\n)\n(\n)\n(\n)\n,\n(\n)\n|\n(\n,\n\nA more complex system\nBattery\n- Joint distribution sufficient for any inference:\n- Exact inference algorithms via local computations\n- for graphs without loops: belief propagation\n- in general: variable elimination or junction tree, but these\nwill still take exponential time for complex graphs.\nRadio\nIgnition\nGas\nStarts\nOn time to work\n)\n|\n(\n)\n,\n|\n(\n)\n(\n)\n|\n(\n)\n|\n(\n)\n(\n)\n,\n,\n,\n,\n,\n(\nS\nO\nP\nG\nI\nS\nP\nG\nP\nB\nI\nP\nB\nR\nP\nB\nP\nO\nS\nG\nI\nR\nB\nP\n=\n\nSampling possible worlds\n>\n¬\n<\nwet\nrain\nsprinkler\ncloudy\n,\n,\n,\nAs the sample gets larger,\nthe frequency of each\npossible world approaches\nits true prior probability\nunder the model.\nHow do we use these\nsamples for inference?\n>\n¬\n¬\n<\nwet\nrain\nsprinkler\ncloudy\n,\n,\n,\n>\n¬\n¬\n<\nwet\nrain\nsprinkler\ncloudy\n,\n,\n,\n>\n¬\n¬\n<\nwet\nrain\nsprinkler\ncloudy\n,\n,\n,\n>\n¬\n¬\n¬\n<\nwet\nrain\nsprinkler\ncloudy\n,\n,\n,\n>\n¬\n<\nwet\nrain\nsprinkler\ncloudy\n,\n,\n,\n>\n¬\n¬\n¬\n¬\n<\nwet\nrain\nsprinkler\ncloudy\n,\n,\n,\n. . .\n\nSummary\n- Exact inference methods do not scale well to\nlarge, complex networks\n- Sampling-based approximation algorithms can\nsolve inference and learning problems in arbitrary\nnetworks, and may have some cognitive reality.\n- Rejection sampling, Likelihood weighting\n- Cognitive correlate: imagining possible worlds\n- Gibbs sampling\n- Neural correlate: Parallel local message-passing\ndynamical system\n- Cognitive correlate: \"Two steps forward, one step\nback\" model of cognitive development"
    },
    {
      "category": "Resource",
      "title": "oct_14_2004_fin.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-66j-computational-cognitive-science-fall-2004/b32b69be2bcdeb03d2f1eb021c8c6dab_oct_14_2004_fin.pdf",
      "content": "Outline\n- Bayesian parameter estimation\n- Hierarchical Bayesian models\n- Metropolis-Hastings\n- A more general approach to MCMC\n\nCoin flipping\n- Comparing two simple hypotheses\n- P(H) = 0.5 vs. P(H) = 1.0\n- Comparing simple and complex hypotheses\n- P(H) = 0.5 vs. P(H) = θ\n- Comparing infinitely many hypotheses\n- P(H) = θ : Infer θ\n\nComparing infinitely many hypotheses\n- Assume data are generated from a model:\n- What is the value of θ ?\n- each value of θ is a hypothesis H\n- requires inference over infinitely many hypotheses\nd1\nd2\nd3\nd4\nθ\nP(H) = θ\n\nComparing infinitely many hypotheses\n- Flip a coin 10 times and see 5 heads, 5 tails.\n- P(H) on next flip? 50%\n- Why? 50% = 5 / (5+5) = 5/10.\n- \"Future will be like the past.\"\n- Suppose we had seen 4 heads and 6 tails.\n- P(H) on next flip? Closer to 50% than to 40%.\n- Why? Prior knowledge.\n\nIntegrating prior knowledge and data\n)\n(\n)\n|\n(\n)\n(\n)\n|\n(\nD\nP\nH\nD\nP\nH\nP\nD\nH\nP\n=\nP(θ | D) ∝P(D | θ ) P(θ )\n- Posterior distribution P(p | D) is a probability\ndensity over θ = P(H)\n- Need to work out likelihood P(D | θ ) and\nspecify prior distribution P(θ )\n\nLikelihood and prior\n- Likelihood:\nP(D | θ ) = θ NH (1-θ ) NT\n- NH: number of heads\n- NT: number of tails\n- Prior:\nP(θ ) ∝pFH-1 (1-p)FT-1\n?\n\nA simple method of specifying priors\n- Imagine some fictitious trials, reflecting a\nset of previous experiences\n- strategy often used with neural networks or\nbuilding invariance into stat. machine vision.\n- e.g., F ={1000 heads, 1000 tails} ~ strong\nexpectation that any new coin will be fair\n- In fact, this is a sensible statistical idea...\n\nLikelihood and prior\n- Likelihood:\nP(D | θ ) = θ NH (1-θ ) NT\n- NH: number of heads\n- NT: number of tails\n- Prior:\nP(θ ) ∝θ FH-1 (1-θ ) FT-1\n- FH: fictitious observations of heads\n- FT: fictitious observations of tails\nBeta(FH,FT)\n\nLikelihood and prior\n- Likelihood:\nP(D | θ ) = θ NH (1-θ ) NT\n- NH: number of heads\n- NT: number of tails\n- Prior:\nP(θ ) =\nθ FH-1 (1-θ ) FT-1\n- FH: fictitious observations of heads\n- FT: fictitious observations of tails\nΓ(FH+FT)\nΓ(FH) Γ(FT)\n\nLikelihood and prior\n- Likelihood:\nP(D | θ ) = θ NH (1-θ ) NT\n- NH: number of heads\n- NT: number of tails\n- Prior:\nΓ(FH+FT)\nΓ(FH) Γ(FT)\n∫\n∫\n0 P(θ ) dθ =\nθ FH-1 (1-θ ) FT-1dθ = 1\nA very useful integral\n\nLikelihood and prior\n- Likelihood:\nP(D | θ ) = θ NH (1-θ ) NT\n- NH: number of heads\n- NT: number of tails\n- Prior:\nΓ(FH+FT)\nΓ(FH) Γ(FT)\n∫\n∫\n0 P(θ ) dθ =\nθ FH-1 (1-θ ) FT-1dθ = 1\nAlso useful: Γ(x) = (x-1)!\nΓ(x+1) = x Γ(x)\n\nShape of the Beta prior\n0.25\n0.25\n0.5\n0.5\n0.75\n0.75\n0.5\n0.5\n0.25\n1.5\n0.75\n0.25\n0.5\n0.75\nX\nX\nX\nX\nProbability\nProbability\nProbability\nProbability\nFH = 2, FT = 0.5\nFH = 2, FT = 2\nFH = 0.5, FT = 2\nFH = 0.5, FT = 0.5\nFigure by MIT OCW.\n\nShape of the Beta prior\nFigure by MIT OCW.\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nP (p)\nFH = FT = 1\nFH = FT = 3\nFH = FT = 1000\n\nBayesian parameter learning\n- Likelihood: Bernoulli(θ )\nP(D | θ ) = θ NH (1-θ ) NT\n- NH, NT: number of heads, tails observed\n- Prior: Beta(FH,FT)\nP(θ ) ∝θ FH-1 (1-θ ) FT-1\n- FH, FT: fictitious observations of heads, tails\n- Posterior: Beta(NH+FH, NT+FT)\nP(θ | D) ∝θ NH+FH-1 (1-θ ) NT+FT-1\n=\nθ NH+FH-1 (1-θ ) NT+FT-1\nΓ(NH+FH+NT+FT)\nΓ(NH+FH) Γ(NT+FT)\n\nBayesian parameter learning\nd1\nd2\nd3\nd4\nθ\nD = NH,NT\n- Likelihood: Bernoulli(θ )\nP(D | θ ) = θ NH (1-θ ) NT\n- NH, NT: number of heads, tails observed\n\nBayesian parameter learning\nd1\nd2\nd3\nd4\nθ\nFH,FT\nD = NH,NT\n- Prior: Beta(FH,FT)\nP(θ | FH, FT) ∝θ FH-1 (1-θ ) FT-1\n- FH, FT: fictitious observations of heads, tails\n\nBayesian parameter learning\nd1\nd2\nd3\nd4\nθ\nFH,FT\nD = NH,NT\n- Posterior: Beta(NH+FH, NT+FT)\nP(θ | D, FH, FT) ∝θ NH+FH-1 (1-θ ) NT+FT-1\n=\nθ NH+FH-1 (1-θ ) NT+FT-1\nΓ(NH+FH+NT+FT)\nΓ(NH+FH) Γ(NT+FT)\n\nConjugate priors\n- A prior p(θ ) is conjugate to a likelihood\nfunction p(D | θ ) if the posterior has the same\nfunctional form of the prior.\n- Different parameter values in the prior and\nposterior reflect the impact of observed data.\n- Parameter values in the prior can be thought of as a\nsummary of \"fictitious observations\".\n- Exist for many standard distributions\n- all exponential family models\n- e.g., Beta is conjugate to Bernoulli (coin-flipping)\n\nBayesian parameter learning\nd1\nd2\nd3\nd4\nθ\nFH,FT\nH\nD = NH,NT\n- Posterior predictive distribution:\nP(H|D=NH, NT; FH, FT) = ?\n\nBayesian parameter learning\nd1\nd2\nd3\nd4\nθ\nFH,FT\nH\nD = NH,NT\n- Posterior predictive distribution:\n∫\nP(H|D, FH, FT) =\nP(H|θ ) P(θ | D, FH, FT) dθ\n\nBayesian parameter learning\nd1\nd2\nd3\nd4\nθ\nFH,FT\nH\nD = NH,NT\n- Posterior predictive distribution:\n∫\nP(H|D, FH, FT) =\nθ P(θ | D, FH, FT) dθ\n\nBayesian parameter learning\nd1\nd2\nd3\nd4\nθ\nFH,FT\nH\nD = NH,NT\n- Posterior predictive distribution:\n∫\nP(H|D, FH, FT) =\nθ θ NH+FH-1 (1-θ ) NT+FT-1 dθ\nΓ(NH+FH+NT+FT)\nΓ(NH+FH) Γ(NT+FT)\n\nBayesian parameter learning\nd1\nd2\nd3\nd4\nθ\nFH,FT\nH\nD = NH,NT\n- Posterior predictive distribution:\n∫\nP(H|D, FH, FT) =\nθ θ NH+FH-1 (1-θ ) NT+FT-1 dθ\nΓ(NH+FH+NT+FT)\nΓ(NH+FH) Γ(NT+FT)\n\nBayesian parameter learning\nd1\nd2\nd3\nd4\nθ\nFH,FT\nH\nD = NH,NT\n- Posterior predictive distribution:\n∫\nP(H|D, FH, FT) =\nθ NH+FH (1-θ ) NT+FT-1 dθ\nΓ(NH+FH+NT+FT)\nΓ(NH+FH) Γ(NT+FT)\n\nBayesian parameter learning\nd1\nd2\nd3\nd4\nθ\nFH,FT\nH\nD = NH,NT\n- Posterior predictive distribution:\nP(H|D, FH, FT) =\nΓ(NH+FH+NT+FT)\nΓ(NH+FH) Γ(NT+FT)\nΓ(NH+FH+NT+FT+1)\nΓ(NH+FH+1) Γ(NT+FT)\nx\nΓ(x+1) = x Γ(x)\n\nBayesian parameter learning\nd1\nd2\nd3\nd4\nθ\nFH,FT\nH\nD = NH,NT\n- Posterior predictive distribution:\nP(H|D, FH, FT) =\nΓ(NH+FH+NT+FT)\nΓ(NH+FH) Γ(NT+FT)\n(NH+FH+NT+FT) Γ(NH+FH+NT+FT)\n(NH+FH) Γ(NH+FH) Γ(NT+FT)\nx\n\nBayesian parameter learning\nd1\nd2\nd3\nd4\nθ\nFH,FT\nH\nD = NH,NT\n- Posterior predictive distribution:\nP(H|D, FH, FT) =\n(NH+FH+NT+FT)\n(NH+FH)\n\nSome examples\n- e.g., F ={1000 heads, 1000 tails} ~ strong\nexpectation that any new coin will be fair\n- After seeing 4 heads, 6 tails, P(H) on next\nflip = 1004 / (1004+1006) = 49.95%\n- e.g., F ={3 heads, 3 tails} ~ weak\nexpectation that any new coin will be fair\n- After seeing 4 heads, 6 tails, P(H) on next\nflip = 7 / (7+9) = 43.75%\nPrior knowledge too weak\n\nBut... flipping thumbtacks\n- e.g., F ={4 heads, 3 tails} ~ weak expectation\nthat tacks are slightly biased towards heads\n- After seeing 2 heads, 0 tails, P(H) on next flip\n= 6 / (6+3) = 67%\n- Some prior knowledge is always necessary to\navoid jumping to hasty conclusions...\n- Suppose F = { }: After seeing 2 heads, 0 tails,\nP(H) on next flip = 2 / (2+0) = 100%\n\nOrigin of prior knowledge\n- Tempting answer: prior experience\n- Suppose you have previously seen 2000\ncoin flips: 1000 heads, 1000 tails\n- By assuming all coins (and flips) are alike,\nthese observations of other coins are as\ngood as observations of the present coin\n- e.g., 200 coins flipped 10 times each.\n\nHierarchical priors\nd1\nd2\nd3\nd4\nFH,FT\nd1\nd2\nd3\nd4\nθ1\nd1\nd2\nd3\nd4\nθ ~ Beta(FH,FT)\nCoin 1\nCoin 2\nCoin 200\n...\nθ2\nθ200\n- Latent structure captures what is common to all\ncoins, and also their individual variability\n\nProblems with simple empiricism\n- Haven't really seen 2000 coin flips, or any flips of a\nthumbtack\n- Prior knowledge is stronger than raw experience justifies\n- Haven't seen exactly equal number of heads and tails\n- Prior knowledge is smoother than raw experience justifies\n- Should be a difference between observing 2000 flips\nof a single coin versus observing 10 flips each for 200\ncoins, or 1 flip each for 2000 coins\n- Prior knowledge is more structured than raw experience\n\nA simple theory\n- \"Coins are manufactured by a standardized\nprocedure that is effective but not perfect.\"\n- Justifies generalizing from previous coins to the\npresent coin.\n- Justifies smoother and stronger prior than raw\nexperience alone.\n- Explains why seeing 10 flips each for 200 coins is\nmore valuable than seeing 2000 flips of one coin.\n- \"Tacks are asymmetric, and manufactured to\nless exacting standards.\"\n\nHierarchical priors\nFH,FT\nphysical knowledge\nd1\nd2\nd3\nd4\nθ1\nθ200\n...\nFH,FT\nd1\nd2\nd3\nd4\nθ1\nCoin 1\nTack 1\nθ200\n...\nCoins\nThumbtacks\n- Qualitative beliefs (e.g. symmetry) can influence\nestimation of continuous properties (e.g. FH, FT)\n\nHierarchical priors\nFH,FT\nphysical knowledge\nd1\nd2\nd3\nd4\nθ1\nθ200\n...\nFH,FT\nd1\nd2\nd3\nd4\nθ1\nCoin 1\nTack 1\nθ200\n...\nCoins\nThumbtacks\n- Explains why 10 flips of 200 coin are better than 2000\nflips of a single coin: more informative about FH, FT,\nassuming parameters not too large for new kind of coin.\n\nStability versus Flexibility\n- Can all domain knowledge be represented\nwith conjugate priors?\n- Suppose you flip a coin 25 times and get all\nheads. Something funny is going on ...\n- But with F ={1000 heads, 1000 tails},\nP(heads) on next flip = 1025 / (1025+1000)\n= 50.6%. Looks like nothing unusual.\n- How do we balance stability and flexibility?\n- Stability: 6 heads, 4 tails θ ~ 0.5\n- Flexibility: 25 heads, 0 tails\nθ ~ 1\n\nHierarchical priors\n- Higher-order hypothesis: is this\ncoin fair or unfair?\n- Example probabilities:\n- P(fair) = 0.99\n- P(θ |fair) is Beta(1000,1000)\n- P(θ |unfair) is Beta(1,1)\n- 25 heads in a row propagates up,\naffecting θ and then P(fair|D)\nd1\nd2\nd3\nd4\nθ\nFH,FT\nfair/unfair?\nP(fair|25 heads) P(25 heads|fair) P(fair)\nP(unfair|25 heads) P(25 heads|unfair) P(unfair)\n=\n= 9 x 10-5\nθ\nθ\nθ\nd\np\nD\nP\nD\nP\n)\nfair\n|\n(\n)\n|\n(\n)\nfair\n|\n(\n0∫\n=\n\nHierarchical priors\n- Higher-order hypothesis: is this\ncoin fair or unfair?\n- Example probabilities:\n- P(fair) = 0.99\n- P(θ |fair) is Beta(1000,1000)\n- P(θ |unfair) is Beta(1,1)\n- 25 heads in a row propagates up,\naffecting θ and then P(fair|D)\nd1\nd2\nd3\nd4\nθ\nFH,FT\nsocial knowledge\nPhysical knowledge\nfair/unfair?\nP(fair|25 heads) P(25 heads|fair) P(fair)\nP(unfair|25 heads) P(25 heads|unfair) P(unfair)\n=\n= 9 x 10-5\n\nSummary\n- Learning the parameters of a generative\nmodel as Bayesian inference.\n- Conjugate priors\n- an elegant way to represent simple kinds of prior\nknowledge.\n- Hierarchical Bayesian models\n- integrate knowledge across instances of a system,\nor different systems within a domain.\n- can incorporate abstract theoretical knowledge.\n- inference may get difficult....\n\nOther questions\n- Learning isn't just about parameter\nestimation\n- How do we learn the functional form of a\nvariable's distribution?\n- How do we learn model structure? Theories?\n- Can we \"grow\" levels of abstraction?\n- How do hierarchical Bayesian models\naddress the Grue problem? Do we care?\n- The \"topics\" model for semantics as an\nexample of applying hierarchical Bayesian\nmodeling to cognition. Probably next time.\n\nTopic models of semantic structure: e.g., Latent\nDirichlet Allocation (Blei, Ng, Jordan)\n- Each document in a corpus is associated with a\ndistribution θ over topics.\n- Each topic t is associated with a distribution φ(t)\nover words.\nBlei, David, Andrew Ng, and Michael Jordan. \"Latent Dirichlet Allocation.\"\nJournal of Machine Learning Research 3 (Jan 2003): 993-1022.\nImage removed due to copyright considerations. Please see:\n\nA selection of topics (TASA)\nFIELD\nMAGNETIC\nMAGNET\nWIRE\nNEEDLE\nCURRENT\nCOIL\nPOLES\nIRON\nCOMPASS\nLINES\nCORE\nELECTRIC\nDIRECTION\nFORCE\nMAGNETS\nBE\nMAGNETISM\nPOLE\nINDUCED\nSCIENCE\nSTUDY\nSCIENTISTS\nSCIENTIFIC\nKNOWLEDGE\nWORK\nRESEARCH\nCHEMISTRY\nTECHNOLOGY\nMANY\nMATHEMATICS\nBIOLOGY\nFIELD\nPHYSICS\nLABORATORY\nSTUDIES\nWORLD\nSCIENTIST\nSTUDYING\nSCIENCES\nBALL\nGAME\nTEAM\nFOOTBALL\nBASEBALL\nPLAYERS\nPLAY\nFIELD\nPLAYER\nBASKETBALL\nCOACH\nPLAYED\nPLAYING\nHIT\nTENNIS\nTEAMS\nGAMES\nSPORTS\nBAT\nTERRY\nJOB\nWORK\nJOBS\nCAREER\nEXPERIENCE\nEMPLOYMENT\nOPPORTUNITIES\nWORKING\nTRAINING\nSKILLS\nCAREERS\nPOSITIONS\nFIND\nPOSITION\nFIELD\nOCCUPATIONS\nREQUIRE\nOPPORTUNITY\nEARN\nABLE\nSTORY\nSTORIES\nTELL\nCHARACTER\nCHARACTERS\nAUTHOR\nREAD\nTOLD\nSETTING\nTALES\nPLOT\nTELLING\nSHORT\nFICTION\nACTION\nTRUE\nEVENTS\nTELLS\nTALE\nNOVEL\nMIND\nWORLD\nDREAM\nDREAMS\nTHOUGHT\nIMAGINATION\nMOMENT\nTHOUGHTS\nOWN\nREAL\nLIFE\nIMAGINE\nSENSE\nCONSCIOUSNESS\nSTRANGE\nFEELING\nWHOLE\nBEING\nMIGHT\nHOPE\nWATER\nFISH\nSEA\nSWIM\nSWIMMING\nPOOL\nLIKE\nSHELL\nSHARK\nTANK\nSHELLS\nSHARKS\nDIVING\nDOLPHINS\nSWAM\nLONG\nSEAL\nDIVE\nDOLPHIN\nUNDERWATER\nDISEASE\nBACTERIA\nDISEASES\nGERMS\nFEVER\nCAUSE\nCAUSED\nSPREAD\nVIRUSES\nINFECTION\nVIRUS\nMICROORGANISMS\nPERSON\nINFECTIOUS\nCOMMON\nCAUSING\nSMALLPOX\nBODY\nINFECTIONS\nCERTAIN\n\nA selection of topics (TASA)\nFIELD\nMAGNETIC\nMAGNET\nWIRE\nNEEDLE\nCURRENT\nCOIL\nPOLES\nIRON\nCOMPASS\nLINES\nCORE\nELECTRIC\nDIRECTION\nFORCE\nMAGNETS\nBE\nMAGNETISM\nPOLE\nINDUCED\nSCIENCE\nSTUDY\nSCIENTISTS\nSCIENTIFIC\nKNOWLEDGE\nWORK\nRESEARCH\nCHEMISTRY\nTECHNOLOGY\nMANY\nMATHEMATICS\nBIOLOGY\nFIELD\nPHYSICS\nLABORATORY\nSTUDIES\nWORLD\nSCIENTIST\nSTUDYING\nSCIENCES\nBALL\nGAME\nTEAM\nFOOTBALL\nBASEBALL\nPLAYERS\nPLAY\nFIELD\nPLAYER\nBASKETBALL\nCOACH\nPLAYED\nPLAYING\nHIT\nTENNIS\nTEAMS\nGAMES\nSPORTS\nBAT\nTERRY\nJOB\nWORK\nJOBS\nCAREER\nEXPERIENCE\nEMPLOYMENT\nOPPORTUNITIES\nWORKING\nTRAINING\nSKILLS\nCAREERS\nPOSITIONS\nFIND\nPOSITION\nFIELD\nOCCUPATIONS\nREQUIRE\nOPPORTUNITY\nEARN\nABLE\nSTORY\nSTORIES\nTELL\nCHARACTER\nCHARACTERS\nAUTHOR\nREAD\nTOLD\nSETTING\nTALES\nPLOT\nTELLING\nSHORT\nFICTION\nACTION\nTRUE\nEVENTS\nTELLS\nTALE\nNOVEL\nMIND\nWORLD\nDREAM\nDREAMS\nTHOUGHT\nIMAGINATION\nMOMENT\nTHOUGHTS\nOWN\nREAL\nLIFE\nIMAGINE\nSENSE\nCONSCIOUSNESS\nSTRANGE\nFEELING\nWHOLE\nBEING\nMIGHT\nHOPE\nWATER\nFISH\nSEA\nSWIM\nSWIMMING\nPOOL\nLIKE\nSHELL\nSHARK\nTANK\nSHELLS\nSHARKS\nDIVING\nDOLPHINS\nSWAM\nLONG\nSEAL\nDIVE\nDOLPHIN\nUNDERWATER\nDISEASE\nBACTERIA\nDISEASES\nGERMS\nFEVER\nCAUSE\nCAUSED\nSPREAD\nVIRUSES\nINFECTION\nVIRUS\nMICROORGANISMS\nPERSON\nINFECTIOUS\nCOMMON\nCAUSING\nSMALLPOX\nBODY\nINFECTIONS\nCERTAIN\n\nJoint models of syntax and semantics\n(Griffiths, Steyvers, Blei & Tenenbaum, NIPS 2004)\n- Embed topics model inside an nth order\nHidden Markov Model:\nGriffiths, T. L., M. Steyvers, D. M. Blei, and J. B. Tenenbaum.Integrating Topics\nand Syntax. Advances in Neural Information Processing Systems 17 (2005).\nImage removed due to copyright considerations. Please see:\n\nGriffiths, T. L., M. Steyvers, D. M. Blei, and J. B. Tenenbaum.Integrating Topics\nand Syntax. Advances in Neural Information Processing Systems 17 (2005).\nSemantic classes\nPLANTS\nPLANT\nLEAVES\nSEEDS\nSOIL\nROOTS\nFLOWERS\nWATER\nFOOD\nGREEN\nSEED\nSTEMS\nFLOWER\nSTEM\nLEAF\nANIMALS\nROOT\nPOLLEN\nGROWING\nGROW\nGOLD\nIRON\nSILVER\nCOPPER\nMETAL\nMETALS\nSTEEL\nCLAY\nLEAD\nADAM\nORE\nALUMINUM\nMINERAL\nMINE\nSTONE\nMINERALS\nPOT\nMINING\nMINERS\nTIN\nDOCTOR\nPATIENT\nHEALTH\nHOSPITAL\nMEDICAL\nCARE\nPATIENTS\nNURSE\nDOCTORS\nMEDICINE\nNURSING\nTREATMENT\nNURSES\nPHYSICIAN\nHOSPITALS\nDR\nSICK\nASSISTANT\nEMERGENCY\nPRACTICE\nBOOK\nBOOKS\nREADING\nINFORMATION\nLIBRARY\nREPORT\nPAGE\nTITLE\nSUBJECT\nPAGES\nGUIDE\nWORDS\nMATERIAL\nARTICLE\nARTICLES\nWORD\nFACTS\nAUTHOR\nREFERENCE\nNOTE\nBEHAVIOR\nSELF\nINDIVIDUAL\nPERSONALITY\nRESPONSE\nSOCIAL\nEMOTIONAL\nLEARNING\nFEELINGS\nPSYCHOLOGISTS\nINDIVIDUALS\nPSYCHOLOGICAL\nEXPERIENCES\nENVIRONMENT\nHUMAN\nRESPONSES\nBEHAVIORS\nATTITUDES\nPSYCHOLOGY\nPERSON\nCELLS\nCELL\nORGANISMS\nALGAE\nBACTERIA\nMICROSCOPE\nMEMBRANE\nORGANISM\nFOOD\nLIVING\nFUNGI\nMOLD\nMATERIALS\nNUCLEUS\nCELLED\nSTRUCTURES\nMATERIAL\nSTRUCTURE\nGREEN\nMOLDS\nMAP\nNORTH\nEARTH\nSOUTH\nPOLE\nMAPS\nEQUATOR\nWEST\nLINES\nEAST\nAUSTRALIA\nGLOBE\nPOLES\nHEMISPHERE\nLATITUDE\nPLACES\nLAND\nWORLD\nCOMPASS\nCONTINENTS\nFOOD\nFOODS\nBODY\nNUTRIENTS\nDIET\nFAT\nSUGAR\nENERGY\nMILK\nEATING\nFRUITS\nVEGETABLES\nWEIGHT\nFATS\nNEEDS\nCARBOHYDRATES\nVITAMINS\nCALORIES\nPROTEIN\nMINERALS\nImage removed due to copyright considerations. Please see:\n\nGriffiths, T. L., M. Steyvers, D. M. Blei, and J. B. Tenenbaum.Integrating Topics\nand Syntax. Advances in Neural Information Processing Systems 17 (2005).\nSyntactic classes\nBE\nMAKE\nGET\nHAVE\nGO\nTAKE\nDO\nFIND\nUSE\nSEE\nHELP\nKEEP\nGIVE\nLOOK\nCOME\nWORK\nMOVE\nLIVE\nEAT\nBECOME\nMORE\nSUCH\nLESS\nMUCH\nKNOWN\nJUST\nBETTER\nRATHER\nGREATER\nHIGHER\nLARGER\nLONGER\nFASTER\nEXACTLY\nSMALLER\nSOMETHING\nBIGGER\nFEWER\nLOWER\nALMOST\nON\nAT\nINTO\nFROM\nWITH\nTHROUGH\nOVER\nAROUND\nAGAINST\nACROSS\nUPON\nTOWARD\nUNDER\nALONG\nNEAR\nBEHIND\nOFF\nABOVE\nDOWN\nBEFORE\nONE\nSOME\nMANY\nTWO\nEACH\nALL\nMOST\nANY\nTHREE\nTHIS\nEVERY\nSEVERAL\nFOUR\nFIVE\nBOTH\nTEN\nSIX\nMUCH\nTWENTY\nEIGHT\nHE\nYOU\nTHEY\nI\nSHE\nWE\nIT\nPEOPLE\nEVERYONE\nOTHERS\nSCIENTISTS\nSOMEONE\nWHO\nNOBODY\nONE\nSOMETHING\nANYONE\nEVERYBODY\nSOME\nTHEN\nTHE\nHIS\nTHEIR\nYOUR\nHER\nITS\nMY\nOUR\nTHIS\nTHESE\nA\nAN\nTHAT\nNEW\nTHOSE\nEACH\nMR\nANY\nMRS\nALL\nGOOD\nSMALL\nNEW\nIMPORTANT\nGREAT\nLITTLE\nLARGE\n*\nBIG\nLONG\nHIGH\nDIFFERENT\nSPECIAL\nOLD\nSTRONG\nYOUNG\nCOMMON\nWHITE\nSINGLE\nCERTAIN\nSAID\nASKED\nTHOUGHT\nTOLD\nSAYS\nMEANS\nCALLED\nCRIED\nSHOWS\nANSWERED\nTELLS\nREPLIED\nSHOUTED\nEXPLAINED\nLAUGHED\nMEANT\nWROTE\nSHOWED\nBELIEVED\nWHISPERED\nImage removed due to copyright considerations. Please see:\n\nCorpus-specific factorization\n(NIPS)\nGriffiths, T. L., M. Steyvers, D. M. Blei, and J. B. Tenenbaum.Integrating Topics\nand Syntax. Advances in Neural Information Processing Systems 17 (2005).\nImage removed due to copyright considerations. Please see:\n\nSyntactic classes in PNAS\nIN\nFOR\nON\nBETWEEN\nDURING\nAMONG\nFROM\nUNDER\nWITHIN\nTHROUGHOUT\nTHROUGH\nTOWARD\nINTO\nAT\nINVOLVING\nAFTER\nACROSS\nAGAINST\nWHEN\nALONG\nARE\nWERE\nWAS\nIS\nWHEN\nREMAIN\nREMAINS\nREMAINED\nPREVIOUSLY\nBECOME\nBECAME\nBEING\nBUT\nGIVE\nMERE\nAPPEARED\nAPPEAR\nALLOWED\nNORMALLY\nEACH\nTHE\nTHIS\nITS\nTHEIR\nAN\nEACH\nONE\nANY\nINCREASED\nEXOGENOUS\nOUR\nRECOMBINANT\nENDOGENOUS\nTOTAL\nPURIFIED\nTILE\nFULL\nCHRONIC\nANOTHER\nEXCESS\nSUGGEST\nINDICATE\nSUGGESTING\nSUGGESTS\nSHOWED\nREVEALED\nSHOW\nDEMONSTRATE\nINDICATING\nPROVIDE\nSUPPORT\nINDICATES\nPROVIDES\nINDICATED\nDEMONSTRATED\nSHOWS\nSO\nREVEAL\nDEMONSTRATES\nSUGGESTED\nLEVELS\nNUMBER\nLEVEL\nRATE\nTIME\nCONCENTRATIONS\nVARIETY\nRANGE\nCONCENTRATION\nDOSE\nFAMILY\nSET\nFREQUENCY\nSERIES\nAMOUNTS\nRATES\nCLASS\nVALUES\nAMOUNT\nSITES\nRESULTS\nANALYSIS\nDATA\nSTUDIES\nSTUDY\nFINDINGS\nEXPERIMENTS\nOBSERVATIONS\nHYPOTHESIS\nANALYSES\nASSAYS\nPOSSIBILITY\nMICROSCOPY\nPAPER\nWORK\nEVIDENCE\nFINDING\nMUTAGENESIS\nOBSERVATION\nMEASUREMENTS\nREMAINED\nBEEN\nMAY\nCAN\nCOULD\nWELL\nDID\nDOES\nDO\nMIGHT\nSHOULD\nWILL\nWOULD\nMUST\nCANNOT\nTHEY\nALSO\nBECOME\nMAG\nLIKELY\n\nSemantic highlighting\nDarker words are more likely to have been generated from the\ntopic-based \"semantics\" module:\n\nOutline\n- Bayesian parameter estimation\n- Hierarchical Bayesian models\n- Metropolis-Hastings\n- A more general approach to MCMC\n\nMotivation\n- Want to compute P(h|evidence):\n- General problem with complex models: sum\nover alternative hypotheses is intractable.\n∑\n′\n′\n′\n=\nh\nh\nP\nh\nevidence\nP\nh\nP\nh\nevidence\nP\nevidence\nh\nP\n)\n(\n)\n|\n(\n)\n(\n)\n|\n(\n)\n|\n(\n\nMarkov chain Monte Carlo\n- Sample from a Markov chain which\nconverges to posterior distribution\n- After an initial \"burn in\" period,\nsamples are independent of starting\nconditions.\nImage removed due to\ncopyright considerations.\n\nWhat's a Markov chain?\nx\nx\nx\nx\nx\nx\nx\nx\nTransition matrix\nP(x(t+1)|x(t)) = T(x(t),x(t+1))\n- States of chain are variables of interest\n- Transition matrix chosen to give posterior\ndistribution as stationary distribution\n\nGibbs sampling\n- Suppose (1) we can factor hypotheses into\nindividual state variables, h = <h1, h2, ..., hn>;\n- and (2) we can easily compute\nP(hi|h-i, evidence), where\nh-i = h1\n(t+1), h2\n(t+1),..., hi-1\n(t+1)\n, hi+1\n(t)\n, ..., hn\n(t)\n- Then use Gibbs sampling:\n- Cycle through variables h1, h2, ..., hn\n- Draw hi(t+1) from P(hi|h-i, evidence)\n\nGibbs sampling\nImage removed due to copyright considerations.\n(MacKay, 2002)\n\nMotivation for Metropolis-Hastings\n- Want to compute P(h|evidence):\n- We have a probabilistic model that allows\nus to compute P(evidence|h) and P(h).\n- We can compute relative posteriors:\n∑\n′\n′\n′\n=\nh\nh\nP\nh\nevidence\nP\nh\nP\nh\nevidence\nP\nevidence\nh\nP\n)\n(\n)\n|\n(\n)\n(\n)\n|\n(\n)\n|\n(\n)\n(\n)\n|\n(\n)\n(\n)\n|\n(\n)\n|\n(\n)\n|\n(\nj\nj\ni\ni\nj\ni\nh\nP\nh\nevidence\nP\nh\nP\nh\nevidence\nP\nevidence\nh\nP\nevidence\nh\nP\n=\n\nMetropolis-Hastings algorithm\n- Transitions have two parts:\n- proposal distribution: Q(h(t+1)| h(t))\n- acceptance: take proposals with probability\nA(h(t+1)| h(t)) = min{ 1, }\nP(h(t+1)|evidence) Q(h(t)| h(t +1))\nP(h(t)|evidence) Q(h(t+1)| h(t))\n\nMetropolis-Hastings algorithm\nComplex unknown posterior distribution\nFigure by MIT OCW.\nComplex Unknown Posterior Distribution\n\nMetropolis-Hastings algorithm\nComplex unknown posterior distribution\ne.g., Gaussian proposal distribution\nComplex Unknown Posterior Distribution\nFigure by MIT OCW.\n\nMetropolis-Hastings algorithm\nComplex unknown posterior distribution\ne.g., Gaussian proposal distribution\nComplex Unknown Posterior Distribution\nFigure by MIT OCW.\n\nMetropolis-Hastings algorithm\nComplex unknown posterior distribution\ne.g., Gaussian proposal distribution\nFigure by MIT OCW.\nComplex Unknown Posterior Distribution\nA (h(t+1) h(t)) = 0.5\n\nMetropolis-Hastings algorithm\nComplex unknown posterior distribution\ne.g., Gaussian proposal distribution\nComplex Unknown Posterior Distribution\nFigure by MIT OCW.\n\nMetropolis-Hastings algorithm\nComplex unknown posterior distribution\ne.g., Gaussian proposal distribution\nComplex Unknown Posterior Distribution\nA (h(t+1) h(t)) = 1\nFigure by MIT OCW.\n\nAdvanced topics\n- What makes a good proposal distribution?\n- \"Goldilocks principle\"\n- May be data-dependent\n- Connections to simulated annealing\n- Integration versus optimization\n- MCMC at different temperatures\n- MCMC over model structures\n- Reversible jump MCMC\n\nRelation to simulated annealing\nComplex unknown cost function\nComplex Unknown Cost Function\nFigure by MIT OCW.\n\nWhy MCMC is important\n- Simple\n- Can be used with just about any kind of\nprobabilistic model, including complex\nhierarchical structures\n- Always works pretty well, if you're willing\nto wait a long time\n(cf. Backpropagation for neural networks.)\n\nA model for cognitive\ndevelopment?\n- Some features of cognitive development:\n- Small, random, dumb, local steps\n- Takes a long time\n- Can get stuck in plateaus or stages\n- \"Two steps forward, one step back\"\n- Over time, intuitive theories get consistently\nbetter (more veridical, more powerful, broader\nscope).\n- Everyone reaches basically the same state\n(though some take longer than others)."
    },
    {
      "category": "Resource",
      "title": "cocosci.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-66j-computational-cognitive-science-fall-2004/22f8cd28f6cf6c0b5edd18c1fe0b0ef9_cocosci.pdf",
      "content": "Learning Novel Concepts in the Kinship Domain\nDaniel M. Roy\nComputer Science and Artificial Intelligence Laboratory\nMassachusetts Institute of Technology\nAbstract\nThis paper addresses the role that novel concepts play in learning good theories. To concretize the discussion, I use\nHinton's kinship dataset as motivation throughout the paper. The standpoint taken in this paper is that the most compact\ntheory that describes a set of examples is the preferred theory--an explicit Occam's Razor. The kinship dataset is a good\ntest-bed for thinking about relational concept learning because it contains interesting patterns that will undoubtedly be\npart of a compact theory describing the examples. To begin with, I describe a very simple computational level theory for\ninductive theory learning in first-order logic that precisely states that the most compact theory is preferred. In addition,\nI illustrate the obvious result that predicate invention is a necessary part of any system striving for compact theories.\nI present derivations within the Inductive Logic Programming (ILP) framework that show how the intuitive theories of\nfamily trees can be learned. These results suggest that encoding regular equivalence directly into the training sets of ILP\nsystems can improve learning performance. To investigate theories resulting from optimization, I devise an algorithm that\nworks with a very strict language bias allowing all consistent rules to be entertained and explicitly optimized over for small\ndatasets. The algorithm, which can be viewed as a special case implementation of ILP, is capable of learning a theory of\nkinship comparable in compactness to the intuitive theories humans use regularly. However, this alternative approach falls\nshort as it is incapable of inventing the unary predicate sex to learn a more compact theory. Finally, I comment on the\nphilosophical position of extreme nativism in light of the ability of these systems to invent primitive concepts not present\nin the training data.\nIntroduction\neral because of the semi-decidability of first order logic, there\nhas been great success at the algorithmic level in the field of\nInductive Logic Programming (ILP). The problem ILP ad-\nThe core of the intuitive theory of kinship in western culture dresses is: learn a first-order logic theory that, together with\nis the family tree, from which any number of queries about provided background knowledge, logically entails a set of ex-\nkinship relationships can be answered. Could a machine, pre-\namples (Nienhuys-Cheng and de Wolf, 1997).\nsented with the kinship relationships between individuals in\na family, learn the intuitive family tree representation?\nUsing the ILP framework, it is possible to show how inverse\nresolution can devise all three of the basis set predicates that\nThis paper focuses heavily on a dataset introduced in Hinton comprise the family tree representation. The most interesting\n(1986). In this dataset, a group of individuals are related by result is the discovery of sex which requires that logical en-\nthe following relations: father, mother, husband, wife, son, codings of regular equivalence classes can be combined in an\ndaughter, brother, sister, uncle, aunt, nephew, niece. The inverse resolution step to generate the new predicate.3 This\nfamily tree representation efficiently encodes all of these rela\nresult suggests that explicitly encoding regular equivalence\ntionships using a basis set composed of spousal relationships,\nparent/child relationships and the sex attribute.1 To learn and other second-order properties of relational datasets may\ncontribute to their learnability.\nthis theory, a machine would have to first invent the basis set\nand then redefine the existing relations in terms of this basis To investigate the computational level, I devise a special-case\nset.2 How could a machine discover such a basis set?\nversion of ILP that is optimized to use a very strict set of\nAccording to my computational level theory, the basis set is restrictions on the type of theories it can entertain. By trad-\nnot discovered at all. Rather, it is a byproduct of an opti-\ning expressibility for tractability, it is possible to explicitly\nmization process that searches for the most compact theory optimize over the set of all possible rules for each relation in-\nthat entails a set of examples. At the algorithmic level, the dividually. Unfortunately, optimizing across the relations is\nbasis set could possibly be discovered through the process of intractable. The resulting rules can be further compressed by\nlocal optimizations that lead to more compact theories.\nusing inverse resolution to invent new predicates that simplify\nexisting ones. The resulting theory for the kinship domain is\nWhile the computational approach is not computable in gen-\ncomparable in compactness to the family tree representation.\n1The sex of the individual is often implicitly specified by the gender of the name.\n2Personal communication and class notes of J. Tenenbaum (Tenenbaum, 2004)\n3The regular equivalence classes for the kinship dataset are all pairs of generations and sex in the family tree White and Reitz (1983); Kemp\net al. (2004).\n\nComputational Level\nHere is a short, computational account of inductive learn\ning that explicitly prefers compact theories. An example in\nthis framework shows why novel concepts necessarily arise in\nlearning the most compact theory.\nEssentially, given examples E and background knowledge B,\nwe are searching for the shortest first-order logic theory T such\nthat B^T |= E. If we do not restrict the form of the theory T\nthen this optimization problem is not computable as logical\nentailment is semi-decidable in full clausal logic (Nienhuys-\nCheng and de Wolf, 1997). A possible variant of this opti\nmization is to bound the number of steps required to prove\nentailment. For example, find the shortest theory that de\nscribes the kinship examples which takes fewer than n steps\nto prove entailment. If we restrict T to Horn clauses then\nwe can guarantee decidability of entailment (Nienhuys-Cheng\nand de Wolf, 1997) and, therefore, we can solve the original\noptimization problem. The search through programs can be\nordered by evaluating B ^ T |= E for all programs of length\none, then all programs of length two, and so on. The process\nis bounded above by the length of the input examples. The\nfirst such program that logically entails the examples is the\nmost compact theory.\nOf course, the time complexity of this optimization grows\ncombinatorially with additional objects, examples, and pred\nicates; Investigating non-trivial concept invention by direct\noptimization is clearly intractable.\nThe above computational theory is closely related to the Kol\nmogorov complexity. While inductive theory learning is con\ncerned with developing the most compact theories that ex\nplain a set of examples, the Kolmogorov complexity is simply\nequal to the length of the most compact theory (up to an ad\nditive constant). More precisely, the Kolmogorov complexity\nof a set of examples, K(E), satisfies the following inequality\nwith respect to the length of the compact theory found by the\nabove optimization, K(E) (Grunwald and Vitanyi, 2004):\nK(E) < K(E) + O(1)\nA quick example shows how search for a compact theory nec\nessarily involves the invention of new predicates. Consider\nFigure 1.\nOn the left of this figure are a set of examples. This example\nassumes a \"closed world,\" which implies that if E |⇐= β then\nwe can assume ¬β (Nienhuys-Cheng and de Wolf, 1997). In\na set of complete ground clauses, this means that any pair\n≡A, B⊆ not mentioned in some relation R implies ¬R(A, B).\nIn addition, the set of objects mentioned in the examples are\ndistinct and no other objects exist. On the right is the most\ncompact theory composed of Horn clauses.4 The theory em\nploys a predicate not present in the examples. This example\nproves that predicate invention is a necessary feature of op\ntimal compression. Unfortunately, it is intractable to deter\nmine the optimal theory for examples of even moderate size\nby brute-force search. There have been several attempts to\ntackle this problem with more elegance. One such attempt\nis called Inductive Logic Programming (Nienhuys-Cheng and\nde Wolf, 1997).\nAlgorithmic Level\nILP is an algorithmic level approach to the above computa\ntional level theory. ILP searches through the space of theories\nguided by heuristics that seek out compact theories that are\nconsistent with the examples. However, ILP systems make no\nguarantee of optimality. In particular, very few ILP systems\navailable are capable of predicate invention, a necessary pre\nrequisite for optimal theories as shown in the previous section.\nInventing New Predicates in ILP\nRegardless of the performance of actual ILP systems on these\nproblems, it is useful to ask whether these systems could,\nin principle, derive the expected relationships from the data.\nFirst, given the basis set, could an ILP system learn the in\ntuitive theories? The PROGOL ILP system developed by\nMuggleton can learn a rule for aunt given parent and sister\npredicates and a few positive examples (Muggleton, 2004).\nThat answered, could an ILP system learn the parent predi\ncate on its own?\nThere are several methods in the ILP literature by which pred\nicates can be invented (Nienhuys-Cheng and de Wolf, 1997,\npg. 176) (Muggleton and Buntine, 1988). The most straight\nforward was developed in Muggleton and Buntine (1988) and\nis known as inverse resolution. Figure 2 shows the mechanism\nof intra-construction, one of two types of inverse resolution.\np ( , B,\np ( , C,\nq ( B\np ( , q,\nq ( C\nFigure 2: Predicate invention via inverse resolution: intra-construction\nReturning back to the idea of compression and deriving the\nmost compact theory, how does the intra-construction rule\naffect the compactness of the resulting theory? By defining\na metric | · | of the complexity of a term and assuming that\nthe metric operates compositionally (the metric of a complex\nterm is the sum of the metric of its parts), it can be shown\nthat the intra-construction results in a more compact theory\nin the case that:\n2|p, , | + |B, C|\n>\n|p, , | + 3|q| + |B, C|\n|p, , |\n>\n3|q|\n(1)\nAssuming that | · | = 1 , then p, β and need only contain\n3 clauses between them to make this derivation more optimal\nthan the antecedent.\nReturning to the kinship example, Figure 3 is a derivation of\nthe parent predicate using the above intra-construction rule.\nEssentially, the invention of the parent predicate is the result\nof a pressure to search for a compact theory. In the derivation\n4The program being written to prove this (by explicitly searching) is unfinished. However, while I am not proof positive this is the most compact\ntheory in Horn clauses, I am fairly certain.\n\nA\nB\nC\nD\nA\nB\nC\nD\nR(A, B)\nY(A)\nR(B, A)\nY(B)\nR(A, C)\nY(C)\nR(C, A)\nR(x, y) ( Y(y)\nR(B, C)\nR(C, B)\nR(D, A)\nR(D, B)\nR(D, C)\nFigure 1: Learning Novel Concepts via Optimal Compression (Horn Clauses)\nbelow, two rules for grandfather differ in that one describes a\nmother's father and the other a father's father. These differ\nences are combined in an intra-construction derivation, creat\ning the parent predicate.\ngrandfather(x, y)\n(\nmother(z, y), father(x, z)\ngrandfather(x, y)\n(\nfather(z, y), father(x, z)\ngrandmother(x, y)\n(\nmother(z, y), mother(x, z)\ngrandmother(x, y)\n(\nfather(z, y), mother(x, z)\nparent(x, y)\n(\nmother(x, y)\nparent(x, y)\n(\nfather(x, y)\ngrandfather(x, y)\n(\nparent(z, y), father(x, z)\ngrandmother(x, y)\n(\nparent(z, y), mother(x, z)\nFigure 3: Intra-construction derivation of parent\nFigure 4, below, shows how the spouse predicate could be in\nvented. Again, the invention of the spouse predicate aids to\nthe compactness of the theory. In this example we have two\nrules that describe the set of positive examples of mother-\nin-law. They differ solely in whether the clause describes a\nhusband's mother or a wife's mother. Via intra-construction,\nthe differences between these two rules are merged into a new\npredicate, spouse, and the original rule is rewritten to use this\nnew predicate.\nmother-in-law(x, y)\n(\nwife(z, y), mother(x, z)\nmother-in-law(x, y)\n(\nhusband(z, y), mother(x, z)\nfather-in-law(x, y)\n(\nwife(z, y), father(x, z)\nfather-in-law(x, y)\n(\nhusband(z, y), father(x, z)\nspouse(x, y)\n(\nwife(x, y)\nspouse(x, y)\n(\nhusband(x, y)\nmother-in-law(x, y)\n(\nspouse(z, y), mother(x, z)\nfather-in-law(x, y)\n(\nspouse(z, y), father(x, z)\nFigure 4: Intra-construction derivation of spouse\nWhat about the sex predicate? Using only the predicates\navailable in the system it is unclear how an intra-construction\nrule could produce a predicate whose meaning can be under\nstood to represent the sex of an individual. Inverse resolution\nalone is insufficient. However, if the example dataset is aug\nmented with predicates that describe the regular equivalence\nof the kinship tree, it then becomes clear how the sex predicate\ncould be invented.5\nIn the derivation below, a binary predicate REGE(x, c) as\nserts that an object x belongs to an equivalence class c. Us\ning the same intra-construction rule used to derive spouse and\nparent, it now becomes clear how the sex predicate could be\ninvented.\nmother(x, y)\n(\nparent(x, y), REGE(x, 1)\nmother(x, y)\n(\nparent(x, y), REGE(x, 3)\nmother(x, y)\n(\nparent(x, y), REGE(x, 5)\nfather(x, y)\n(\nparent(x, y), REGE(x, 2)\nfather(x, y)\n(\nparent(x, y), REGE(x, 4)\nfather(x, y)\n(\nparent(x, y), REGE(x, 6)\nfemale(x, y)\n(\nREGE(x, 1)\nfemale(x, y)\n(\nREGE(x, 3)\nfemale(x, y)\n(\nREGE(x, 5)\nmale(x, y)\n(\nREGE(x, 2)\nmale(x, y)\n(\nREGE(x, 4)\nmale(x, y)\n(\nREGE(x, 6)\nmother(x, y)\n(\nparent(x, y), female(x)\nfather(x, y)\n(\nparent(x, y), male(x)\nFigure 5: Intra-construction derivation of male/female\nThis result is interesting as it suggests that latent informa\ntion in networks of relationships (like regular equivalence)\ncould aid in the learning of relational data. Perhaps regular\nequivalence and similar meta-data that make explicit latent\nstructure could act as \"kernel tricks\" for ILP systems, adding\nadditional dimensions to the input data to make it easier to\nlearn.\n5If we were working within second-order logic, we could have the system recognize and report the regular equivalence classes automatically. This\nsuggest that moving to the higher order logics may provide real advantages in learnability.\n\nGraph Compression\nIn this section, an alternative to generic inductive logic pro\ngramming is introduced that works by compressing graph rep\nresentations of sets of ground binary relations. The system\ncan be understood as performing a similar task as ILP with a\nlanguage bias that restricts the form of the theories the sys\ntem is capable of entertaining. The algorithm uses inverse\nresolution to invent new predicates when patterns exist that\nmatch the antecedent of the intra-construction rule.\nLanguage Bias\nMost ILP systems operate with a language bias that is ei\nther implicit (built into the system) or user-specified. The\nlanguage bias can take the form of a restriction in the length\nof clauses, types of literals, or even grammars of allowable\nclauses. A language bias limits the search space of possible\nhypothesis. In this system, the examples presented to the\nsystem are positive examples of binary predicates. Negative\nexamples are implied via a closed world assumption. Such a\ncollection of binary predicates can be represented as a graph\nwhose edges represent relations between objects.\nThe system learns theories of a very strict form which makes\nthe compression problem very simple. The grammar of this\nsubset of first-order clausal logic is shown as Figure 7. It\nshould be explicitly noted that this language bias is highly\nspecific to the kinship relationships we wish to learn. The\nresulting system is by no means intended to be considered a\nserious contribution to ILP. Instead, it is an attempt to think\nabout the problems normally tackled by ILP from a stand\npoint more aligned with the (compression-based) computa\ntional theory outlined earlier. Because of the strict language\nbias, we are able to greedily optimize to find compact theories.\nIn summary, the language bias was chosen to simplify the com\npression algorithm. Regardless, the combination of predicate\ninvention with this simple compression mechanism results in\nthe formation of compact theories of kinship.\nR(A, B)\nR(x, y)\n(\nR1(n1 , x),\nR2(n2 , n1 ),\n. . .\nR3(y, nm ),\nx, n1 , ..., nm , y are distinct\n8i.R 6= Ri\nFigure 7: Language Bias\nAn interesting aspect of this language bias is that each rule\ncan be at most the length of the number of objects because\nthe grammar requires that the universally quantified variables\nbe distinct. Therefore, there are a finite number of theories\nthat can describe any finite set of positive examples. In addi\ntion, the grammar prevents recursive definitions by requiring\nthat the head relation not exist in the body of the clause.\nThese restrictions tradeoff expressibility for tractability. We\nuse these simplifications to our advantage to allow some ex\ntent of explicit optimization in the process of devising compact\ntheories.\nProblem Description\nThis section describes the graph compression problem setup.\nWe are given a graph G = ≡N, E⊆, where N is a set of nodes\n\nand E is a set of labeled edges N × N × . We are interested\nin finding a new graph G0 = ≡N, E0⊆, where E0 E, and a set\nof rules R of the form in Figure 7 such that:\narg minR |R| + |E0|\nR1(G0) ∧G G\nwhere | · | is a metric we use to measure complexity, and\nR1(G0) is the application of the rules to the graph until a\nfixed point is reached. The fixed point, R1(G0), is equivalent\nto the original graph G once all edges whose labels do not\nexist in G have been removed.\nEven this (much simpler) optimization problem is intractable\n(optimizing over the ordering of relations grows as O(n!)\nwhere n is the number of relations.6). Therefore, we de\ncompose the optimization into separate optimizations for each\nheuristic and greedily choose one heuristic at a time. Once\nchosen, the remaining relations are re-optimized separately.\nThis process continues until all the relations are learned.\nGreedy Implementation\nThe implementation of the graph compression optimization is\nwritten in MzScheme and is available in Appendix A. The\ninput to the system is a graph representing the examples.\nThe algorithm first creates all valid rules for each of the rela\ntions in the graph (there are finitely many as explained above).\nThis is done by creating a set of candidate rules for every\npositive instance of each relation. Candidate rules for a re\nlation El(x, y) are created by enumerating every acyclic path\nbetween nodes x and y using only edges with labels l0 ⇐= l.\nThis candidate list is then pruned by verifying that no can\ndidate rule implies a negative example. For each relation,\nthere is a set of consistent candidate rules for each positive\ninstance. These paths are then ordered according to a com\nplexity measure that prefers short rules that explain the most\npositive examples. The final candidate rule for a relation is\nthen formed by the disjunction of the best candidate rule for\neach positive instance. In practice, a single rule or disjunc\ntion of a few general rules describes the entire set of positive\ninstances. By construction, the rule is consistent with the\npositive and negative examples.\nAt each iteration, a relation rule is chosen from the candi\ndates by picking the most compact rule that conflicts with\nthe fewest other rules (e.g. removing sister early in the pro\ncess causes other rules to become much larger while removing\n6Arguably this is not proven and could be solvable efficiently. However, there does not appear to be a problem decomposition that would lead\nto a dynamic programming solution.\n\nA\nB\nC\nE\nR\nS\nR\nHazel(A). Yellow(B). Yellow(C). Hazel(E).\nR(B, A). S(C, B). R(E, B).\nFigure 6: Sample Graph and First-order Logic Ground Clause Equivalent\naunt has little affect on later rules). To prevent mutual recur\nsion, all candidate rules using the selected relation are pruned.\nThis process repeats until there are no consistent rules that\ndescribe the remaining relations. These relations are the basis\nrelations, meaning that all the other relations are definable in\nterms of these relations.\nThese rules are then improved by looking for ways to com\nbine disjunctions by predicate invention via inverse resolu\ntion. These new relations are instantiated as new edges in\nthe graph. This process of generating the basis relations and\nlooking for new ways to combine disjunctions continues until\nthere are no candidate disjunctions remaining. At this point,\na final basis set and derived set are generated. The basis set\nare ground clauses and the derived set are rules. By construc\ntion, the \"application\" of the rules to the set of ground clauses\nrecreates the original graph.\nExample: Kinship Data The graph compression algo\nrithm was designed specifically with the kinship dataset in\nmind. In the kinship dataset, a group of individuals are\ndescribed by a set of relations such as mother, father, son,\ndaughter, uncle, aunt, sister, etc.. The usual representation\nof kinship relationships in western culture is the family tree.\nThis representations is a very efficient way of representing the\ndata (see Figure 8) and is most likely used because of this\nquality. The complete specification of every relationship in\na sizable family tree will be much larger than the equivalent\nspecification of the family tree and the set of rules to derive\nthe remaining relations.\nThe kinship dataset contains certain patterns, each of which\nis addressed by a specific area in the graph compression al\ngorithm. Kinship relationships are defined in terms of paths\nbetween individuals and the names of the steps in these paths.\nOur language bias matches this observation exactly and there\nfore we can expect to find theories that match our intuitive\nones.\nWalk-through The first pass through the kinship data re\nsults in the following rules:\nbasis set:\n(husband mother wife son daughter)\nrules (derived set):\n((father ((mother husband) . 6))\n(sister ((father daughter) . 3))\n(brother ((father son) . 3))\n(niece\n((husband sister daughter) . 1)\n((sister daughter) . 1)\n((brother daughter) . 1)\n((wife brother daughter) . 1))\n(nephew ((niece brother) . 4))\n(aunt ((mother brother wife) . 2) ((father sister)\n(uncle ((aunt husband) . 4)))\n. 2))\nThe basis set is husband, mother, wife, son, daughter. Two of\nthe disjunctions for the niece rule overlap. Intra-construction\ncan derive a new predicate we know as sibling that is the dis\njunction of sister and brother. With the new predicate added\nto the graph, the rules are re-learned.\nbasis set:\n(husband mother wife daughter)\nderived set:\n((son ((daughter sibling) . 6))\n(father ((mother husband) . 6))\n(sister ((father daughter) . 3))\n(brother ((father son) . 3))\n(niece\n((husband sister daughter) . 1)\n((sibling daughter) . 2)\n((wife sibling daughter) . 1))\n(nephew ((niece brother) . 4))\n(aunt ((mother sibling wife) . 2) ((father sibling) . 2))\n(uncle ((aunt husband) . 4)))\nNo further simplifications can be found by applying intra-\nconstruction and so the basis/derived sets represents the final\nsolution of the graph compression algorithm. Because the\nalgorithm is able to learn only a very restricted set of theo\nries, there is no way that the algorithm could learn the com\nmon family tree representation because this representation\nrequires clauses not allowed by the language bias. In addition\nthe graph compression mechanism cannot be extended to new\nlanguage biases. However, the final basis set encodes the fun\ndamental aspects of the family tree representation (husband,\nmother, wife, daughter), albeit less efficiently than one that\nincludes a sex attribute and uses it to further compress the\nrepresentation. Another disappointing aspect of these results\nis that the rules, though compact, do not match those gen\nerally used by humans to explain kinship relationships. For\nexample, the uncle rule is simply \"aunt's husband\" which is\ntechnically correct with respect to this dataset, but not as\nintuitive as \"parent's brother (or brother-in-law).\" The rea\nson the algorithm performs in this manner is that it greedily\nchooses the best rule at each iteration, largely ignoring the\neffect of these decisions on the overall optimality of the re\nsulting theory. While the descriptions of the family relations\nare sometimes non-intuitive, they are as compact as those in\nthe intuitive theory. 7\n7Personal communication and class notes of J. Tenenbaum (Tenenbaum, 2004)\n\nA\nB\nC\nD\nE\nF\nG\nH\nK\nI\nL\nJ\nFigure 8: Efficient Family Tree Representation: Horizontal edges represent marriage, Vertical represent parent/child links. The nodes are colored according\nto their regular equivalence classes.\nPhilosophy\nOne of the central questions in philosophy of mind and cogni\ntive science is how humans generate new concepts. Extreme\nnativists, like Jerry Fodor, deny this is even possible, believ\ning instead that we are born with every concept we use in life\nand that learning is simply the process of recalling these in\nnate concepts (Laurence and Margolis, 2002). His thesis relies\non the assumption that the primitive concepts that compose\ncomplex concepts are themselves indivisible and cannot be de\nfined in terms of smaller parts. However, his position can be\nundermined if we can show how new primitive concepts--not\ndefined in terms of other concepts--can be learned. Are hu\nmans born with the concept of object? Showing that such a\nconcept is learnable would not only be a fantastic result but\nwould answer a question philosophers have wrestled with for\nmillennia.\nMost attempts to define concept learning avoid the issues\nraised by Fodor by relying on operational definitions of how\nconcepts are learned; Concepts are the processes that emerge\nwhen a mechanism interacts with its environment. This oper\national approach describes several empiricist attempts to show\nhow concepts can be bootstrapped from experience. Gary\nDrescher's Schema Mechanism, based on his interpretation of\nPiaget's work with young children, is one such attempt.\nIn essence, this work describes how a computational mecha\nnism can learn a theory for a relational dataset by construct\ning new relations and redefining the original relations in terms\nof these new concepts in such a way that the resulting the\nory is more compact than the original. One argument is that\nsuch a mechanism is learning new concepts because these new\nrelations are not defined in terms of existing concepts.\nThe only possibly novel concept learned in the kinship dataset\nis that of sex. Both parent and spouse, while not present in the\noriginal dataset, are complex concepts formed by the disjunc\ntion of simpler concepts. The sex predicate is unique in that it\nis a new concept that appears seemingly from nowhere, help\ning to define the kinship dataset. To understand whether sex\nis a novel, primitive concept, we must consider how the sex\npredicate arises at both the algorithmic and computational\nlevels.\nAt the algorithmic level (ILP), it appears that the sex predi\ncate is, in fact, not novel as it is the result of an inverse reso\nlution step that combines latent information described by the\nregular equivalence of the dataset. Just as parent and spouse\nare complex concepts defined in terms of simpler ones, sex is\na complex concept formed by the disjunction of all objects is\nthe female equivalence classes.8\nAt the computational level the opposite seems to be true. As\nif by magic, new predicates are invented that result in the\nmost compact theory that entails a set of examples. In the\nkinship case, we could imagine that the most compact the\nory uses a representation similar to that of the family tree\nand a set of rules that define the remaining relationships. At\nthe computational level, these new concepts appear automat\nically. However, our inability to explain why and when novel\nconcepts appear should not bestow upon those concepts any\nsort of special status. If the sex predicate arises from the la\ntent structure in the dataset, then the sex predicate is not\ntruly novel because the latent structure is already present in\nthe examples and need only be squeezed out. Therefore, the\nconcept of sex is already present in the data and the work\npresented in this paper does not undermine Fodor's nativism.\nHowever, even if we were to assume the position of nativism,\nbecause there is no efficient way of finding these new concepts,\nintelligence may in fact be regarded as the ability to efficiently\ndevise compact theories, in which case, the fact that every\nthing is already known in a strict theoretical sense is of little\nconsequence.\nConclusion\nA computational approach that optimally compresses a set of\nexamples necessarily requires the invention of predicates. Dis\ncovering these predicates is intractable at the computational\nlevel. However, at the algorithmic level, with methods such\nas those employed by ILP, we can discover predicates use\nful for compressing a theory by using inverse resolution. For\nthe kinship domain it was shown how an ILP system could\nin principle learn the three fundamental predicates that com\nprise the family tree representation. While these predicates\nare novel in the sense that they are not strictly present in\n8In the kinship theory presented in class, the parent, spouse and sex predicates are the basis set in which the remaining of the relationships\nare defined. This basis set could have as easily been husband, father and sex. However, while rearranging the ground clauses and rules such that\nthe new predicates are the ground clauses (the basis set) does elevate the status of these predicates to primitive concepts, it similarly demotes the\nstatus of the original predicates to that of complex concepts. I have primarily concerned myself with how these novel concepts can be learned.\nOnce they are invented, rearrangements can be made to optimize further. I believe that explaining how they arise is the most important aspect.\nSimply inventing new predicates is not a serious algorithmic level theory as it entails an impossibly large expansion in the search space.\n\nthe examples, their invention requires that they be defined in\nterms of primitive concepts. In the case of the sex predicate\nthis requires that we augment the system with explicit repre\nsentations of the regular equivalence of the examples. Finally,\nbecause the predicates arise from inverse resolution, they are\nnot truly novel, primitive concepts and thus do not undermine\nthe philosophical position of extreme nativism.\nBibliography\nR. Cilibrasi and P. Vitanyi. Clustering by compression. Submitted\nto IEEE Trans. Infomat. Th., 2004.\nJ. Feldman. Minimization of boolean complexity in human concept\nlearning. Nature, 407:63-633, 2000.\nP. Grunwald and P. Vitanyi.\nShannon Information and Kol\nmogorov Complexity. Submitted to IEEE Trans. Infomat. Th.,\n2004.\nG. E. Hinton. Learning distributed representations of concepts.\nIn Proc. Ann. Conf. of the Cognitive Science Society, volume 1,\n1986.\nC. Kemp, T. L. Griffiths, and J. B. Tenenbaum. Discovering latent\nclasses in relational data. MIT AI Lab Memo, 19, 2004.\nS. Laurence and E. Margolis. Radical concept nativism. Cognition,\n86:22-55, 2002.\nD. Marr. Artificial intelligence: A personal view. Artificial Intelli\ngence, 9:37-48, 1977.\nS. Muggleton. Progol software. http://www.doc.ic.ac.uk/~shm/\nprogol.html, December 2004.\nS. Muggleton and W. Buntine. Machine invention of first order\npredicates by inverting resolution. In Proceedings of the 5th\nInternational Workshop on Machine Learning, pages 339-351.\nMorgan Kaufmann, 1988.\nS. Muggleton and L. D. Raedt. Inductive logic programming: The\nory and methods. J. Log. Program., 19/20:629-679, 1994.\nS.-H. Nienhuys-Cheng and R. de Wolf. Foundations of Inductive\nLogic Programming, volume 1228. February 1997.\nD. Page and A. Srinivasan. ILP: a short look back and a longer\nlook forward. Journal of Machine Learning Research, 4:415-430,\n2003.\nJ. B. Tenenbaum. Personal communications, November 2004.\nD. R. White and K. P. Reitz. Graph and semigroup homomor\nphisms social networks. 5:193-234, 1983.\n\nA\nGraph Compression Code\nkinship.scm\n(define kinship-examples\n'(\n(father Christopher Arthur)\n(father Christopher Victoria)\n(father Andrew James)\n(father Andrew Jennifer)\n(father James Colin)\n(father James Charlotte)\n(mother Penelope Arthur)\n(mother Penelope Victoria)\n(mother Christine James)\n(mother Christine Jennifer)\n(mother Victoria Colin)\n(mother Victoria Charlotte)\n(parent Christopher Arthur)\n(parent Christopher Victoria)\n(parent Andrew James)\n(parent Andrew Jennifer)\n(parent James Colin)\n(parent James Charlotte)\n(parent Penelope Arthur)\n(parent Penelope Victoria)\n(parent Christine James)\n(parent Christine Jennifer)\n(parent Victoria Colin)\n(parent Victoria Charlotte)\n(husband Christopher Penelope)\n(husband Andrew Christine)\n(husband Arthur Margaret)\n(husband James Victoria)\n(husband Charles Jennifer)\n(wife Penelope Christopher)\n(wife Christine Andrew)\n(wife Margaret Arthur)\n(wife Victoria James)\n(wife Jennifer Charles)\n(spouse Christopher Penelope)\n(spouse Andrew Christine)\n(spouse Arthur Margaret)\n(spouse James Victoria)\n(spouse Charles Jennifer)\n(spouse Penelope Christopher)\n(spouse Christine Andrew)\n(spouse Margaret Arthur)\n(spouse Victoria James)\n(spouse Jennifer Charles)\n(son Arthur Christopher)\n(son Arthur Penelope)\n(son James Andrew)\n(son James Christine)\n(son Colin Victoria)\n(son Colin James)\n(daughter Victoria Christopher)\n(daughter Victoria Penelope)\n(daughter Jennifer Andrew)\n(daughter Jennifer Christine)\n(daughter Charlotte Victoria)\n(daughter Charlotte James)\n(brother Arthur Victoria)\n(brother James Jennifer)\n(brother Colin Charlotte)\n(sister Victoria Arthur)\n(sister Jennifer James)\n(sister Charlotte Colin)\n(sibling Arthur Victoria)\n(sibling James Jennifer)\n(sibling Colin Charlotte)\n(sibling Victoria Arthur)\n(sibling Jennifer James)\n(sibling Charlotte Colin)\n(uncle Arthur Colin)\n(uncle Charles Colin)\n(uncle Arthur Charlotte)\n(uncle Charles Charlotte)\n(aunt Jennifer Colin)\n(aunt Margaret Colin)\n(aunt Jennifer Charlotte)\n(aunt Margaret Charlotte)\n(nephew Colin Arthur)\n(nephew Colin Jennifer)\n(nephew Colin Margaret)\n(nephew Colin Charles)\n(niece Charlotte Arthur)\n(niece Charlotte Jennifer)\n(niece Charlotte Margaret)\n(niece Charlotte Charles)))\n(define (kinship-regular-equivalence)\n'((christopher 1)\n(andrew 1)\n(penelope 2)\n(christine 2)\n(margaret 3)\n(victoria 3)\n(jennifer 3)\n(arthur 4)\n(james 4)\n(charles 4)\n(colin 5)\n\n(charlotte 6)))\n(define first-literal car)\n(define remaining-literals cdr)\n(define get-relation car)\n(define get-objects cdr)\n(define (remove-last lst)\n(reverse (cdr (reverse lst))))\n(define (length< l1 l2)\n(< (length l1) (length l2)))\n(define (first-n lst n)\n(cond ((or (<= n 0) (not (pair? lst))) '())\n(else\n(cons (car lst) (first-n (cdr lst) (- n 1))))))\n(define (union . l)\n(let loop ((l l)\n(b (list)))\n(if (null? l)\nb\n(loop (cdr l)\n(let loop ((a (car l))\n(b b))\n(if (null? a)\nb\n(if (member (car a) b)\n(loop (cdr a) b)\n(loop (cdr a) (cons (car a) b)))))))))\n(define (setdiff a b)\n(filter (lambda (aval) (not (member aval b))) a))\n(define (intersect a . bs)\n(let loop ((a a)\n(bs bs))\n(cond ((null? bs) a)\n((null? a) a)\n(else\n(loop\n(filter (lambda (aval) (member aval (car bs))) a)\n(cdr bs))))))\n(define (extract extractor lst)\n(let loop ((lst lst)\n(items '()))\n(if (null? lst)\nitems\n(loop (cdr lst)\n(union (extractor (car lst))\nitems)))))\n(define (extract-relations examples)\n(extract (lambda (literal) (list (get-relation literal))) examples))\n(define (extract-objects examples)\n(extract get-objects examples))\n(define (query-examples examples queryliteral)\n(if (null? examples)\n#f\n(let ((literal (first-literal examples)))\n(or (equal? literal queryliteral)\n(query-examples (remaining-literals examples)\nqueryliteral)))))\n(define (build-graph relations relnames objects objnames adjgraphs examples coloring)\n(list relations relnames objects objnames adjgraphs examples coloring))\n(define (vector-select v lst)\n(let loop ((i (vector-length v))\n(lst lst)\n(result '()))\n(if (> i 0)\n(loop (+ i 1)\n(cdr lst)\n(if (= 1 (vector-ref v (- i 1)))\n(cons (car lst) result)\nresult))\nresult)))\n(define (find lst)\n(let loop ((i 0)\n(result '())\n(lst lst))\n(if (null? lst)\n(reverse result)\n(loop (+ i 1)\n(if (car lst)\n(cons i result)\nresult)\n(cdr lst)))))\n(define (index-of i lst)\n(find (map (lambda (q) (equal? i q)) lst)))\n(define (first-index-of i lst)\n(car (index-of i lst)))\n(define identity (lambda (x) x))\n(define graph-relations car)\n(define graph-relnames cadr)\n(define graph-objects caddr)\n(define graph-objnames cadddr)\n(define graph-adjgraphs (lambda (graph) (cadddr (cdr graph))))\n(define graph-examples (lambda (graph) (cadddr (cddr graph))))\n(define graph-coloring (lambda (graph) (cadddr (cdddr graph))))\n(define (graph-select-relation graph relation)\n(vector-ref (graph-adjgraphs graph) relation))\n(define (graph-get-all-edges graph relation)\n(let ((relgraph (graph-select-relation graph relation)))\n(apply append\n(map (lambda (obj1)\n(map (lambda (obj2)\n(list obj1 obj2))\n(find (vector->list (vector-ref relgraph obj1)))))\n\n(build-list (vector-length relgraph) identity)))))\n(define (graph-get-edges graph relation node)\n(let ((relgraph (graph-select-relation graph relation)))\n(find (vector->list (vector-ref relgraph node)))))\n(define edge-start-node car)\n(define edge-end-node cadr)\n(define (path-last-node path) (caar path))\n(define (graph-query-edge graph relation obj1 obj2)\n(vector-ref (vector-ref (vector-ref (graph-adjgraphs graph) relation) obj1) obj2))\n(define (graph-list-edges graph relation obj1)\n(find (vector->list (vector-ref (vector-ref (graph-adjgraphs graph) relation) obj1))))\n(define (parse-examples examples coloring)\n; take a list of examples, extract the relations, generate graphs and return\n; lists of objects, the graphs, etc\n(let* ((relations (extract-relations examples))\n(relnames (map symbol->string relations))\n(objects (extract-objects examples))\n(objnames (map symbol->string objects))\n(adjgraphs (build-vector\n(length relations)\n(lambda (relation)\n(build-vector\n(length objects)\n(lambda (obj1)\n(build-vector\n(length objects)\n(lambda (obj2)\n(query-examples examples\n(list (list-ref relations relation)\n(list-ref objects obj1)\n(list-ref objects obj2))))))))))\n(coloring (lambda (object)\n(if (symbol? object)\n(cadr (assoc object coloring))\n(cadr (assoc (list-ref relations object) coloring))))))\n(build-graph relations relnames objects objnames adjgraphs examples coloring)))\n(define kinship-graph (parse-examples kinship-examples kinship-regular-equivalence))\n(require (lib \"list.ss\" \"srfi/1\"))\n(define (complexity rules)\n(let loop ((complexity 0)\n(rules rules))\n(if (null? rules)\ncomplexity\n(let ((rule (car rules)))\n(loop (+ complexity\n(apply + 1 (map (lambda (subrule) (length (car subrule))) (cdr rule))))\n(cdr rules))))))\n(define (reachable-set graph relationpath startnode)\n(let loop ((queue (list (list startnode)))\n(relationpath relationpath))\n(if (or (null? relationpath) (null? queue))\n(map car queue) ; grab end nodes\n(let ((relation (car relationpath)))\n(loop (append-map (lambda (partial-path)\n(let ((lastnode (car partial-path)))\n(filter pair?\n(map (lambda (node)\n(if (member node partial-path)\n'()\n(cons node partial-path)))\n(graph-list-edges graph relation lastnode)))))\nqueue)\n(cdr relationpath))))))\n(define (trim-invalid graph relation revrelationpaths)\n; remove rules that imply edges that are negative\n;(printf \"trimming... \")\n(map (lambda (revrules)\n(filter (lambda (revrule)\n(let ((rule (reverse revrule)))\n(let loop ((startnode (- (length (graph-objects graph)) 1)))\n;(printf \"checking ~a (~a) ~n\" startnode (map (lambda (relation) (list-ref (graph-relations graph) relation)) rule))\n(if (< startnode 0)\n#t ; rule ok!\n(let ((reachable (reachable-set graph rule startnode))\n(realreachable (graph-list-edges graph relation startnode)))\n;(printf \"reachable ~a~nreal reach ~a~n~n\" reachable realreachable)\n; need to make sure that all of positives are in reachable\n; need to make sure that none of negatives are in reachable\n(and (null? (setdiff reachable\nrealreachable))\n(loop (- startnode 1))))))))\nrevrules))\nrevrelationpaths))\n(define (findrelationpaths graph relation disallowedrelations maxsize)\n(let ((allowable (setdiff (build-list (length (graph-relations graph)) identity)\n(union (list relation) disallowedrelations))))\n(let loop ((fcpaths '())\n(edges (graph-get-all-edges graph relation)))\n(printf \"~n\")\n(if (null? edges)\nfcpaths\n(let ((edge (car edges)))\n;(printf \"learning about ~a(~a,~a)~n\"\n;\n(list-ref (graph-relnames graph) relation)\n;\n(list-ref (graph-objnames graph) (edge-start-node edge))\n;\n(list-ref (graph-objnames graph) (edge-end-node edge)))\n(loop (cons\n(compress-paths (findpaths graph\nallowable\n(edge-start-node edge)\n(edge-end-node edge)\nmaxsize))\nfcpaths)\n(cdr edges)))))))\n(define (choose-covering setofrelationpaths)\n; one idea is to calculate covering-size for each relationpath and then greedily choose\n; the largest covering until the entire set is covered... i tihnk there is a optimum way\n; of doing this using dynamic programming (!!)\n(let ((setofrelationpaths (map (lambda (relationpaths)\n(map (lambda (rp)\n(cons rp (apply +\n\n(map (lambda (rps)\n(if (member rp rps) 1 0))\nsetofrelationpaths))))\nrelationpaths))\nsetofrelationpaths)))\n(let ((sorted-rps (map (lambda (rps)\n(quicksort rps (lambda (rp1 rp2)\n(or (> (cdr rp1) (cdr rp2))\n(and (= (cdr rp1) (cdr rp2))\n(length< (car rp1) (car rp2)))))))\nsetofrelationpaths)))\n(if (member '() sorted-rps)\n'no-rule\n(union (map car sorted-rps))))))\n(define (pick-optimal-covering sorted-rps)\n(if (member '() sorted-rps)\n'no-rule\n(union (map car sorted-rps))))\n(define (create-order setofrelationpaths)\n(printf \"ordering... \")\n; one idea is to calculate covering-size for each relationpath and then greedily choose\n; the largest covering until the entire set is covered... i tihnk there is a optimum way\n; of doing this using dynamic programming (!!)\n(let ((setofrelationpaths (map (lambda (relationpaths)\n(map (lambda (rp)\n(cons rp (apply +\n(map (lambda (rps)\n(if (member rp rps) 1 0))\nsetofrelationpaths))))\nrelationpaths))\nsetofrelationpaths)))\n(let ((sorted-rps (map (lambda (rps)\n(quicksort rps (lambda (rp1 rp2)\n(or (> (cdr rp1) (cdr rp2))\n(and (= (cdr rp1) (cdr rp2))\n(length< (car rp1) (car rp2)))))))\nsetofrelationpaths)))\nsorted-rps)))\n(define (build-rule-from-paths graph dop)\n(if (member '() dop)\n'no-rule\n(map (lambda (conjunction)\n(cons\n(map (lambda (relation)\n(list-ref (graph-relations graph) relation))\n(car conjunction))\n(cdr conjunction)))\ndop)))\n(define (findpaths graph allowable startnode endnode maxsize)\n(printf \"finding paths... \")\n(let loop ((queue (list (list (list startnode -1))))\n(successpaths '())\n(count 0))\n;(printf \"queue = ~a~n\" queue)\n;(printf \"~a \" (length queue))\n(if (or (null? queue) (null? maxsize) (>= count maxsize))\nsuccesspaths\n(let* ((path (car queue))\n(queue (cdr queue))\n(lastnode (path-last-node path)))\n;(display path)\n;(display lastnode)(newline)\n(if (equal? lastnode endnode)\n(loop queue\n(cons path successpaths)\n(+ count 1))\n(loop (append queue\n(append-map\n(lambda (relationgraph)\n(filter-map\n(lambda (object-node)\n(if (and (not (member object-node (map car path)))\n(graph-query-edge graph relationgraph lastnode object-node))\n(begin\n;(printf \"new path! ~a~n\" (cons (list object-node relationgraph) path))\n(cons (list object-node relationgraph) path))\n#f))\n(build-list (length (graph-objects graph)) identity)))\nallowable))\nsuccesspaths\ncount))))))\n(define (compress-paths paths)\n(printf \"compressing... \")\n(let loop ((paths paths)\n(cpaths '()))\n(if (null? paths)\ncpaths\n(let ((path (remove-last (map cadr (car paths))))\n(paths (cdr paths)))\n(if (member path cpaths)\n(loop paths cpaths)\n(loop paths (cons path cpaths)))))))\n(define (remove-nth lst n)\n(cond ((null? lst) lst)\n((<= n 0) (cdr lst))\n(else\n(cons (car lst) (remove-nth (cdr lst) (- n 1))))))\n(define (generate-allrules graph allowable donotdefine)\n(if (file-exists? \"allrules.output\")\n(printf \"FILE ALREADY EXISTS!\")\n(let* ((disallowedrelations (setdiff (graph-relations graph) allowable))\n(allrules (map (lambda (relation)\n(let* ((junk (printf \"~ngenerating allrules for ~a\" relation))\n(relationindex (first-index-of relation (graph-relations graph)))\n(disallowedrelations (map (lambda (relation) (first-index-of relation (graph-relations graph)))\ndisallowedrelations))\n(relationpaths (findrelationpaths graph relationindex disallowedrelations 100))\n;(relationpaths (map (lambda (lst) (first-n lst 100)) relationpaths))\n(relationpaths (trim-invalid graph relationindex relationpaths))\n(relationpaths (create-order relationpaths)))\nrelationpaths))\n(setdiff (graph-relations graph) donotdefine))))\n\n(with-output-to-file \"allrules.output\" (lambda () (write allrules))))))\n(define (optcompress graph donotdefine elimination-order)\n(let* ((allrules (with-input-from-file \"allrules.output\" (lambda () (read)))))\n(let elimloop ((rules (list))\n(allrules allrules)\n(relations (setdiff (graph-relations graph) donotdefine))\n(elimination-order elimination-order))\n(printf \"finding new rule~n\")\n;(pretty-print rules)\n(if (null? allrules)\nrules\n(let* ((coverings (map (lambda (relationpaths)\n(pick-optimal-covering relationpaths))\nallrules))\n(possiblerules (map (lambda (rule relation)\n(cons relation rule))\ncoverings relations))\n(possiblerules (filter (lambda (x) (not (equal? (cdr x) 'no-rule)))\npossiblerules))\n(possiblerules (map (lambda (rule)\n(cons (apply + (map (lambda (otherrule)\n(if (ormap (lambda (subrule)\n(member (first-index-of (car rule) (graph-relations graph))\n(car subrule)))\n(cdr otherrule))\n1 0))\npossiblerules))\nrule))\npossiblerules))\n(sortedrules (quicksort possiblerules\n(lambda (r1 r2)\n(cond ((< (car r1) (car r2)) #t)\n((> (car r1) (car r2)) #f)\n((< (length (cddr r1)) (length (cddr r2))) #t)\n((> (length (cddr r1)) (length (cddr r2))) #f)\n(else\n(< (apply max (map cdr (cddr r1)))\n(apply max (map cdr (cddr r2)))))))))\n(junk (pretty-print sortedrules))\n(sortedrules (map cdr sortedrules)))\n(if (null? sortedrules)\nrules\n(let* ((newrule (if (null? elimination-order)\n(first sortedrules)\n(car (filter (lambda (rule) (equal? (car rule) (car elimination-order))) sortedrules))))\n(rel (car newrule)))\n(elimloop\n(cons (cons (car newrule) (build-rule-from-paths graph (cdr newrule))) rules)\n(map (lambda (posexamples)\n(map (lambda (posexample)\n(filter (lambda (rule)\n(not (member (first-index-of rel (graph-relations graph)) (car rule))))\nposexample))\nposexamples))\n(remove-nth allrules (first-index-of rel relations)))\n(setdiff relations (list rel))\n(if (null? elimination-order)\nelimination-order\n(cdr elimination-order))))))))))\n; DO NOT DEFINE\n(define do-not-define '(parent spouse sibling))\n; ALLOWABLE RELATIONS\n;(define allowablerelations (graph-relations kinship-graph))\n(define allowablerelations (setdiff (graph-relations kinship-graph) '(parent spouse)))\n;(define allowablerelations '(husband mother wife son daughter))\n;(define allowablerelations '(husband parent wife spouse son daughter))\n; GENERATE ALL RULES and COMPRESS\n(generate-allrules kinship-graph allowablerelations do-not-define)\n(define kinship-rules (optcompress kinship-graph do-not-define '(uncle))) (pretty-print kinship-rules)\n;(define derived-set (setdiff (graph-relations kinship-graph) (map caar kinships-rules)))\n(complexity kinship-rules)\n(setdiff (graph-relations kinship-graph) (append (map car kinship-rules) do-not-define))"
    },
    {
      "category": "Resource",
      "title": "cognativemodel.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-66j-computational-cognitive-science-fall-2004/44d424a966bb91a84388b763d408eb96_cognativemodel.pdf",
      "content": "A Cognitive Model of Design Pattern Selection\n\nArturo Hinojosa\n\nSubmitted To:\n\nProf. Joshua Brett Tenenbaum\nDepartment of Brain and Cognitive Sciences\nMassachusetts Institute of Technology\n\nIntroduction: The Software Engineering Process\n\nDesign and implementation are the two main phases of software engineering. The\ndecisions made during each phase are equally important to creating a viable system. Reckless\nexecution of the engineering process can have a dramatic impact on the efficiency and quality of\nthe final product. It is imperative for success that engineers consider all aspects of the challenge\nset before them and then craft a solution capable of handling any possible contingency. The\nsolution must not only be able to expect the unexpected, but do so in a timely manner and in a\nfashion useful to the final user of the system. A brilliant design that takes an hour to compute a\nsolution is of little use to a user who needs the information in minutes. Carefully fashioned and\nwell defined designs are the foundation of good engineering, but even the most comprehensive\nand verbose design needs an equally appropriate implementation to be a successful solution.\nWhile a creative mind capable of abstract thought is the key to good and innovative designs,\ndeciding on an implementation is a much more calculating process. The implementation must\nabide by the decisions made in the design, and must also balance the needs of the users with the\noperation of the system.\n\nIn spite of its design dependent nature, implementation can also be a very open ended\nproblem with many possible solutions. Software engineers are usually only given a few diagrams\nand a very superficial outline of the system behavior. The system designer establishes the criteria\nfor the operation of the system, but it is up to the programmer to select how to realize the design.\nThe programmer must evaluate the desired system behavior and choose an implementation that\nnot only meets the needs of the design, but does so efficiently given the requirements of the user\nand the established operational parameters. It is often the case that there is no single\ndeterministic solution for the challenge. The same solution may have several different\nimplementations, each with its own benefits and consequences. Some implementations are more\nappropriate under certain contexts, and knowing how to choose the best solution often takes\nyears of experience and a heavy education in algorithm design, discrete mathematics, and\ngeneral software engineering principles. The goal of this project is to develop a cognitive model\nof the process software engineers employ to select a specific implementation for a system design.\nUsing inverse entailment and statistical data about real world implementation decisions, the aim\nof this research is to infer a set of general criteria that guide the engineer to the best possible\nimplementation.\n\nBackground: The Gang of Four and Contemporary Research\n\nAlmost every software engineer is familiar with the design patterns established by the\n\"Gang of Four,\" Gamma, Helm, Johnson, and Vlissides ( 1977 ). Their book is standard course\nmaterial for software engineering classes and can be found on the office book shelves of\ncountless programmers. They propose twenty three implementation patterns for common\nprogramming challenges that are widely accepted as the de-facto solutions when developing\nsoftware. As Kung, Bhambhani, Shah, and Pancholi ( 2003 ) write in their paper, \"design\npatterns provide a proven structure and characteristics for building highly maintainable and\nextensible software.\"\nThe Gang of Four divide their design patterns into three categories: Creational Patterns,\nStructural Patterns, and Behavioral Patterns. This project will focus on the final category, the\nbehavioral patterns. This is the most diverse subgroup and has the most implementation\n\nstrategies. The design solutions in this category apply to the execution responsibilities of the\nsoftware. It includes paradigms for data distribution, subsystem interfaces, and dynamic system\nbehavior. Each pattern possesses a unique set of advantages that can be exploited under specific\nsystem conditions.\nBecause of the relative young nature of software engineering and design patterns, there\nhas yet to be exhaustive research into the implications and criteria for design pattern selection.\nThe most relevant effort has been by Kung, et al. ( 2003 ) to develop what they term an \"Expert\nSystem for Suggesting Design Patterns,\" ( ESSDP ). Their approach is based on a user-query\ninterface and a predefined knowledge base to systematically narrow selections based on the user\nprovided information. They categorize their questions into five types based on significance and\ncontext. Each question carries a weight to help resolve conflicts. There approach can be outlined\nin five main steps:\n\n1. Identify the circumstances in which a pattern can be applied.\n2. Refine the circumstances with sub conditions.\n3. Formulate questions to ask the user.\n4. Classify the questions according to their level of significance.\n5. Assign thresholds to patterns and weights to questions.\n\nThe model being developed in this project will attempt to adapt their approach in order to\nidentify the features that are most revealing about pattern selection instead of trying to make a\ndefinite suggestion. Where as Kung, et al. ( 2003 ) studied the design patterns themselves, this\nresearch will focus on real world design choices and try to extrapolate some relationships based\non those examples.\nThe cognitive model will employ the Progol programming language created by\nMuggleton ( 1995 ). Progol is an inductive logic programming language that uses inverse\nentailment to make assertions about classifications. It has been applied to research in machine\nlearning, natural language processing, and other fields of artificial intelligence.\n\nTheory: Mode Directed Inverse Entailment\n\nThe necessary approach to create any artificially intelligent system is to use the context\nprovided by the subject world to infer new information about the causal relationships between\nthe members of the world. Learning can happen by one of two methods. The first method is\ndeduction, by which an observer uses a known set of true theories, T, to derive a set of\nconsequences, E. Deductive inference is the result of repetitive applications of provable rules of\ninference. Each inference can be rationalized by using the previous assumptions. The second\nmethod is inductive learning, by which an observer uses the set of observed consequences to\nderive theories about their cause. Inductive inference may follow from the application of non\nprovable and often incorrect rules of inference. The conclusions drawn from inductive inference\nhave only the statistical support from a finite set of available data. The learner must sometimes\nneed a probability distribution over the hypothesis space to assist them in their resolution. This\ntype of Bayesian learning can help to solidify results and provides stronger statistical support.\n\nInductive logic programming implementations rely on inductive inference to develop a\npredicate basis for classification. Accurate results depend on the amount of noise in the data set\nand having a statistically significant diverse sampling of the information. Background\n\nknowledge guides the classification process along with the examples. Each statement from the\nbackground knowledge is treated as a conjecture. These conjectures are the foundation for a new\nset of conjectures which try to explain the examples. According to Muggleton (1995) there are\nfive main approaches to creating an inductive logic programming system:\n\n1. Inverse resolution in propositional logic.\n2. Inverse resolution in first order definite clause logic.\n3. Determinate relative least general generalization.\n4. Inverse implication.\n5. Mode directed inverse entailment.\n\nEach of these is based on Duce's six rules for inductive inference. The most relevant for\nthis research are the final two rules, intra-construction ( D.1 ) and inter-construction ( D.2 ).\n\npAA,B\n\npAA,C\nD.1 qAB\npAA,q\nqAC\n\npAA,B\n\npAA,C\nD.2 pAr,B\nrAA\nqAr,C\n\nThese rules apply single depth inversion to resolve relationships between the variables and\nconstants. It is possible then to create a hierarchal mechanism for deriving the example set from\nbackground knowledge. This is the basic foundation for the Progol learning engine. The\nunderlying theory of Progol is explained by Muggleton ( 1995 ) in his introductory paper.\nInductive logic systems use a background knowledge base, B, and a set of examples, E, to derive\na hypothesis, H, about the relationships between the data. This can be stated formally as:\n\nI.1 B /\\ H ╞ E\n\nThe goal of the learning is to derive the simplest hypothesis H which is consistent across\nall of the examples. Each clause of the hypothesis space must explain at least one of the\nexamples. If this were not true, then the clause would be redundant and H would not be the\nsimplest explanation. Furthermore, by constraining H and E to be single Horn clauses, and\napplying the law of contraposition, ⌐H and ⌐E are found to be ground skolemized unit clauses.\nWith the existential quantifiers removed, and all that remains are universal assertions about the\nexamples and hypothesis.\n\nI.2 B /\\ ⌐E ╞ ⌐H\n\nIf ⌐C is a conjunction of ground literals true for all models of B /\\ ⌐E, and since ⌐H must\nbe true for all models by definition, it possible to further conclude that:\n\nI.3 B /\\ ⌐E ╞ ⌐C ╞ ⌐H\n\nI.4 C ╞ H\n\nThis relationship between the conjunctive clause and the hypothesis implies that a subset\nsolution for H can be derived by taking those clauses that Θ-subsume C. A complete solution for\nall candidates in H can therefore be calculated by considering all the clauses that Θ-subsume the\nsub-saturants of C.\nThe derivation rests on model theory as opposed to resolution proof theory, meaning\nlearning is achieved through observation in contrast to deduction. Because of the allowable error\ntolerated in this type of learning, this more general approach is optimal in the case of loose\ncategorization because the system does not try to over burden itself trying to resolve\ncontradictions in the data. Given the subjective nature of design pattern selection, this type of\nflexibility is important.\n\nDesign: Creating a Cognitive Model\n\nThe foundation for this research is to use positive examples from real world situations to\ndraw inferences about the specific characteristics of the data that lead to categorization. While\nGold demonstrated in his 1967 paper that positive examples where insufficient to fully capture\nand learn a language ( think of language in this case as an abstract set of rules, or a grammar,\nwhich can define a design pattern selection ), Muggleton (1997) shows that positive examples\ncan be enough to guide learning to an acceptable level of low error. Gold's argument is based on\nthe fact that for every set of positive examples, there are at least two candidate hypotheses. The\nfirst is the language of all possible descriptions, and the second is the more complex language\nwhich exactly accepts the examples. The approach taken by Progol is to use these two languages\nas bounds for a search and come to a compromise which can categorize the data using the\nsimplest hypothesis. To do so, the model must be able to perform the following:\n\n1. Use a set of features that can fully define the examples and allow the engine to\ndiscriminate between their characteristics.\n2. Develop a relationship mapping between the features of the examples and the design\npatterns they employ.\n3. Make qualitative assessments of the relevance of each feature and make rational\ndecisions about its role in categorization.\n4. Express each example as a compilation of simple predicate statements.\n\nThe most important step in designing the model is assessing the characteristic features of\nthe different examples. These features must reflect the demands on the software system, the\nstatic and dynamic behavior of the system, as well as the function of the system. The examples\ncome from different sources, and inherently carry the bias of the original designer. Where one\ndesigner chose to use one implementation, another designer might well have chosen another. It is\nimportant then to choose features that are specific enough to reveal the characteristics of the\nproblem, yet are general enough so the learning focuses on why the designer chose to use that\nimplementation, as opposed to why another designer would have chosen another.\nMany external factors, not accounted for in this model, have significant effects on the\ndesigner's choice. Experience with certain patterns, preferences for certain patterns, and other\nenvironmental influences all factor into the design choice. This model attempts to take an\nempirical view of implementation process, and tries to minimize the influence of these effects by\nstudying several examples for each design pattern and using multiple sources.\n\nMaterials and Apparatus: Implementing a Progol System\n\nThe goal of the prolog model is to determine a set of parameters which decide a design\npattern implementation given a set of requirements on the system. A prolog system is composed\nof four distinct sections:\n\n1. Mode declarations ( both head and body )\n2. Type declarations\n3. Background knowledge\n4. Examples\n\nMode Declarations\n\nThis section defines the predicate relationships for the logic system. These declarations\nexplicitly state the form and necessary arguments for each type of predicate statement. Using\nthese declarations, the Progol system establishes the syntax rules for the rest of the model. Mode\ndeclarations have two types: head and body. Head declarations are the predicate relation the\nsystem is trying to decide. Body declarations are supporting predicates that are relevant to the\nhead declaration. These declarations take the form of:\n\n:- mode_type( recall, predicate_name( #/+/-argument_1, #/+/-argument_2,..., #/+/-argument_n ).\n\nThe recall argument is a number or symbol that declares a bound for the possible\nsimultaneous solutions to the predicate statement. If the bound is unknown, the symbol '*' can\nbe used to declare an unbound predicate statement. The symbols that precede each argument\ndeclare if it is an input variable, output variable, or constant. Alternatively, instead of the place-\nmarker variable, the mode declaration can also use a normal statement as an argument input. A\nnormal statement is bracketed tuple of terms, either constants or variables, that can be taken as\ninput. The mode declarations state not only what type of predicates can be explicitly declared by\nthe user, but also the type of predicates that can be introduced by the engine during the learning\nprocess.\n\nType Declarations\n\nThis section assigns subject types for the logic system. It is an exhaustive declaration of\nall the categories of objects in the model over the set of entities. While each type does not have\nto be defined in any sense, all model subjects must be assigned a type. The declarations have the\nform of:\n\ntype( subject )\n\nThe type declarations are unary predicate statements that assert that a subject satisfies the\nrequirements to be considered of a certain type.\n\nProcedure: The Progol Learning Engine and Analysis of Output\n\nThe progol learning engine is an iterative mechanism that searches out the most specific\nhypothetical clause to define the relationships declared as head modes. It begins by checking that\nall the assertions made in the model file are consistent, and no contradictions exist. Then, a\nsubsumption lattice is created from the background knowledge and examples declared in the file.\nThe creation of each conjecture in the subsumption lattice is as follows:\n\n1. The engine examines the first positive horn clause example found in the file, e, such that\na :- b1, b2,..., bn.\n2. It negates the clause e to create a skolemized conjunctive normal form expression, ⌐e,\nsuch that ⌐a /\\ b1 /\\ b2 /\\ ... /\\ bn.\n3. This new expression is added to the background knowledge cache.\n4. The engine then finds the first head mode declaration h that Θ-subsumes a.\n5. For each substitution in Θ:\na. If the substitution is over a constant, then the skolem variable is substituted in h.\nb. If the substitution is over a variable, then a hash table entry for the skolem\nvariable is substituted into h. Additionally, if the substitution is over an input\nvariable, then the skolem variable is cached in the set IT.\n6. Finally the head mode declaration h is added to the overall conjecture C.\n7. Next, the engine examines all the body mode declarations.\n8. For each body mode declaration b:\na. The engine attempts a substitution over all the input variables with the cached\nskolem variables in IT.\nb. For the number of possible solutions ( defined by the recall number ):\nc. If the set of substitution satisfies the predicate:\ni. if the substitution is over a constant, then the skolem variable is\nsubstituted in the clause.\nii. If the substitution is over a variable, then a hash table entry for the skolem\nvariable is substituted into the clause. Additionally, if the variable is an\noutput variable, then the skolem variable is added to the cache IT.\nd. Finally, the negated body mode declaration is added to the overall conjecture C.\n9. This process is repeated until the maximum variable depth has been reached.\n\nThe final output of these iterations is a disjunctive clause with h as its head and the\nnegated b expressions as its body atoms. For each example then, trying to find a suitable\nhypothesis involves a search of the bounded sub-lattice, constrained by the empty hypothesis and\nthe conjecture C. The progol engine builds the hypothesis by trying decreasingly general clauses\nuntil a comprehensive one is found. This search can be seen as a descent through a tree whose\nroot node is the empty conjecture and whose trunk nodes are refinements. Progol uses an A*\nstyle search to find the node with the maximum compression. The search uses a heuristic\nmeasurement to guide the learning calculated from the following data:\n\n1. ps = The number of positive examples correctly deduced by the candidate clause.\n2. ns = The number of negative examples incorrectly deduced by the candidate clause.\n3. cs = The length of candidate clause - 1\n\n4. hs = The number of additional atoms necessary to complete the cause, as calculated\nby inspecting the output and counting the number atoms needed to connect any\nskolem variables.\n5. fs = The heuristic measure given by ps - ns - cs - hs.\n\nThe search algorithm works as follows:\n\n1. Maintain two caches, open, initially populated with the empty clause, and the empty\ncache closed.\n2. s is set to the member of open that maximizes fs,\n3. s is removed from open.\n4. s is added to closed.\n5. Check the value of ns. If it is 0 and fs > 0, no possible improvements to the heuristic\nmeasure are possible therefore skip to step 8.\n6. Otherwise, descend down the tree.\n7. Add the next refinement nodes to open and remove any members common to closed.\n8. If s maximizes fs for all clauses in closed, ns = 0, and fs(s) > fs for all members of\nopen, then we have found the optimal solution and no further searching is required.\n9. If the open cache is empty, then no generalization was possible and algorithm returns\nthe original clause, otherwise the algorithm returns to step 2.\n\nThis algorithm is guaranteed to terminate and return the most comprehensive clause with\nminimal length. The program also has an internal mechanism for dealing with redundant\nexamples and background knowledge. This mechanism is not covered in this paper as it does not\npertain to the learning, but is available from the Progol documentation.\n\nWork Plan: Implementing a Cognitive Model for Design Pattern Selection.\n\nThe first step in designing the model is implementing the Progol system. The outline\nbelow details the specific steps necessary to implement the cognitive model.\n\n1. Implement Progol learning.\na. Gather examples of associations between system behavior and an optimal design\npattern implementation.\nb. Examine the examples and select a set of relevant and revealing features of the\nsystem behavior.\ni. Include features common to only a few examples.\nii. Include features common to many examples.\niii. Translate each feature into a predicate expression.\nc. Create the background knowledge by declaring predicate statements about all the\nsystem behaviors and the various features.\nd. Run the progol learning engine to generate a most specific clause for each design\npattern.\n2. Analyze the most specific clauses for each design pattern.\n\nThe following are the head and body mode declarations used in the Progol program to learn\nabout the design pattern features. They were chosen based on personal experience and the works\nof Muggleton (1995), and Budinsky, Finnie, Vlissides, and Yu (1996). The code can be found in\nits entirety in Appendix A. In the interest of avoiding repetition, a comprehensive explanation of\neach feature and the different examples can be found in the comments of the code.\n\n:- modeh(1,optimal_pattern(+prob_stat, #design_pattern))?\n:- modeb(1,distributed(+prob_stat))?\n:- modeb(1,daemon(+prob_stat))?\n:- modeb(1,tracks_state(+prob_stat))?\n:- modeb(1,tracks_value(+prob_stat))?\n:- modeb(1,tracks_ext_state(+prob_stat))?\n:- modeb(1,dep_order(+prob_stat))?\n:- modeb(1,dep_input(+prob_stat))?\n:- modeb(1,dep_impl(+prob_stat))?\n:- modeb(1,dep_hist(+prob_stat))?\n:- modeb(1,for_interp(+prob_stat))?\n:- modeb(1,for_extension(+prob_stat))?\n:- modeb(1,for_input(+prob_stat))?\n:- modeb(1,for_output(+prob_stat))?\n:- modeb(1,for_prolif(+prob_stat))?\n:- modeb(1,for_monit(+prob_stat))?\n:- modeb(1,behavior( +prob_stat, #rtbehavior ))?\n\nResults: The Conclusions Derived from the Model\n\nUsing the examples from the data collected from other's research and all available\nreference materials, the following are the conclusions of the Progol system about the relevant\nfeatures for each design pattern:\n\noptimal_pattern(A,chain_of_resp) :- distributed(A), dep_order(A),\ndep_input(A), for_output(A), behavior(A,dynamic).\n\noptimal_pattern(A,command) :- dep_input(A), for_extension(A),\nfor_input(A), for_output(A), for_prolif(A), for_monit(A),\nbehavior(A,static).\n\noptimal_pattern(A,interpreter) :- dep_order(A), dep_input(A),\nfor_input(A), behavior(A,dynamic).\n\noptimal_pattern(A,iterator) :- tracks_ext_state(A), dep_order(A),\ndep_input(A), dep_impl(A), dep_hist(A), for_input(A),\nfor_monit(A), behavior(A,dynamic).\n\noptimal_pattern(A,mediator) :- distributed(A), for_input(A),\nbehavior(A,static).\n\noptimal_pattern(A,memento) :- distributed(A), daemon(A), tracks_state(A),\ndep_order(A), dep_hist(A), for_monit(A), behavior(A,\nstatic).\n\noptimal_pattern(A,observer) :- distributed(A), tracks_value(A),\ndep_input(A), for_output(A), for_monit(A), behavior(A,\nstatic).\n\noptimal_pattern(A,state) :- behavior(A,dynamic).\n\noptimal_pattern(A,strategy) :- behavior(A,static).\n\noptimal_pattern(A,templete) :- behavior(A,dynamic).\n\noptimal_pattern(A,visitor) :- behavior(A,static).\n\nThese results closely reflect the type of assertions I anticipated before running the model.\nI used several different data sets, and tried different feature combinations before accepting the\nresults above. Using the built in Progol analysis mechanism, I generated a set of contingency\ntables that state the anticipated accuracy of the model. While all these tables state a 100% overall\naccuracy, I believe that is due to an insufficient data set and over classification.\n\nDiscussion and Conclusion: What the Model Means and Relevance to Future Research\n\nThis model represents a good start in understanding the causal relationships between\nimplementation selection and features of the solutions they endeavor to undertake. Other\nresearch I found took many different approaches to addressing design pattern selections. Some\nresearchers are beginning to study the UML representations of the different design patterns to\ngain insight into the implications of the internal mechanics of each pattern, while others continue\nto work on direct classification.\n\nWhat made this project interesting was the unique approach taken to learn about the\ndesign pattern features. As opposed to most contemporary research which takes a deductive\napproach to classification, the Progol engine allowed me to make inductive inferences about the\nclassification. This type of learning seems especially helpful in this case given my own biases\nabout each design pattern and when it should be used. By letting the data talk for itself, I learned\na lot about machine learning, as well as some more about software engineering in the process.\n\nReferences\n\nBudinsky, F., Finnie, M., Vlissides, J., & Yu, P. (1996). Automatic code generation from\ndesign patterns, IBM Systems Journal, Vol. 35, No. 2.\n\nGamme, E., Helm R., Johnson R., & Vlissides, J. ( 1977 ). Design Patterns: Elements of\nReusable Object Orientated Software. Boston, MA: Addison-Wesley.\n\nKung, D., Bhambhani, H., Shah, R., & Pancholi, G. ( 2003 ). An Expert System for\nSuggesting Design Patterns - A Methodology and a Prototype. In Software Engineering With\nComputational Intelligence. Kluwer International Series in Engineering and Computer Science.\n\nMuggleton, S. (1995). Inverse Entailment and Progol, New Generation Computing\nJournal, Vol. 13, pp. 245-286.\n\nMuggleton, S. (1997). Learning from positive data, Proceedings of the Sixth International\nWorkshop on Inductive Logic Programming, Springer-Verlag, LNAI 1314.\n\nAppendix A: Progol Code\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n% Input file for cognative modeling of design pattern selection\n% Arturo Hinojosa\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n:- set(r,100).\n:- set(h,1000).\n\n%%%%%%%%%%%%%%%%%%%%%%\n% Mode delcarations\n%%%%%%%%%%%%%%%%%%%%%%\n\n% Head Mode\n%%%%%%%%%%%%%%%%%%%%%%\n\n%%\n% States that a single output should be given when queried for\n% the optimal design pattern choice.\n%\n% Input: prob_state A problem statement definition.\n% Output: The optimal design pattern.\n%%\n:- modeh(1,optimal_pattern(+prob_stat, #design_pattern))?\n\n% Body Modes\n%%%%%%%%%%%%%%%%%%%%%%\n\n:- modeb(1,distributed(+prob_stat))?\n:- modeb(1,daemon(+prob_stat))?\n:- modeb(1,tracks_state(+prob_stat))?\n:- modeb(1,tracks_value(+prob_stat))?\n:- modeb(1,tracks_ext_state(+prob_stat))?\n:- modeb(1,dep_order(+prob_stat))?\n:- modeb(1,dep_input(+prob_stat))?\n:- modeb(1,dep_impl(+prob_stat))?\n:- modeb(1,dep_hist(+prob_stat))?\n:- modeb(1,for_interp(+prob_stat))?\n:- modeb(1,for_extension(+prob_stat))?\n:- modeb(1,for_input(+prob_stat))?\n:- modeb(1,for_output(+prob_stat))?\n:- modeb(1,for_prolif(+prob_stat))?\n:- modeb(1,for_monit(+prob_stat))?\n:- modeb(1,behavior( +prob_stat, #rtbehavior ))?\n\n%%%%%%%%%%%%%%%%%%%%%%\n% Type declarations\n%%%%%%%%%%%%%%%%%%%%%%\n\n%\n% Problem statments\n%\n% These are various software tasks to be analyzed.\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n%%\n% Provide context sensative help for UI screen.\n%\n% 1. User clicks somewhere on screen.\n% 2. Program iterates through location context to find\n% the right help output.\n% 3. Searches inorder of generality.\n%%\nprob_stat(context_help).\n\n%%\n% An event that has multiple consequences.\n%\n% 1. Given some random event:\n% 2. Program must iterate through series of subsystems\n% notifying them of the event.\n%%\nprob_stat(event_handler).\n\n%%\n% System for approving business expenses. Based on the\n% quantity and type of the expense, a hierarchiel progression\n% is made to find a certified supervisor who can approve the\n% expense.\n%\n%%\nprob_stat(exp_appr).\n\n%%\n% Store DVD lib info.\n%\n%%\nprob_stat(dvd_lib).\n\n%%\n% Spam Filter.\n%\n%%\nprob_stat(spam_filt).\n\n%%\n% Perform action triggered by external event.\n%\n% 1. Given some randomly occuring external event:\n% 2. Triggers a system responce.\n%%\nprob_stat(action_listener).\n\n%%\n% Model a transaction.\n%\n% 1. Collect information about transaction.\n% 2. Create a transaction to another process or subsystem.\n%%\nprob_stat(model_trans).\n\n%%\n% Caclulator capable of undoing transactions.\n%\n%%\nprob_stat(trak_calc).\n\n%%\n% Parse a literal text string.\n%\n% 1. Given some string input:\n% 2. Parse the string into subsections.\n% 3. Recursively parse subsections.\n% 4. Decide the most basic subsection meanings.\n%%\nprob_stat(parse_text).\n\n%%\n% Evaluate a formula.\n%\n% 1. Given some formula\n% 2. Parse the formula into hierarchy of operations.\n% 3. Evaluate each subproblem\n% 4. Output the formula value.\n%\n%%\nprob_stat(eval_formula).\n\n%%\n% Convert a roman numeral to a decimal.\n%\n%%\nprob_stat(conv_rom).\n\n%%\n% Examine the contents of a set.\n%\n% 1. Given any random set:\n% 2. Inspect all elements of the set individually.\n% 3. The set is responsible for providing a traversing scheme.\n%\n%%\nprob_stat(exam_set).\n\n%%\n% Count the members of a set.\n%\n% 1. Given any random set:\n% 2. Count the number of elements in the set.\n% 3. The set is responsible for providing a traversing scheme.\n%\n%%\nprob_stat(count_set).\n\n%%\n% Cycle through a set skipping certain elements.\n%\n%%\n\nprob_stat(disc_iter).\n\n%%\n% Collect user input from a composite dialog box.\n%\n% 1. Given control over some composite dialog box:\n% 2. Collect the user input from each field.\n% 3. Cache the user input.\n% 4. Modify fields based on other field values.\n%\n%%\nprob_stat(dialog_input).\n\n%%\n% Manage a system resource.\n%\n% 1. Monitor a system resource.\n% 2. When system changed, propogate change to all associated resources.\n% 3. Maintain system consistency.\n%\n%%\nprob_stat(man_resc).\n\n%%\n% A chat room coordinator\n%\n%%\nprob_stat(chat_host).\n\n%%\n% Implement an undo feature.\n%\n% 1. Allow the user to undo an operation.\n%\n%%\nprob_stat(make_undo).\n\n%%\n% Implement history in a web browser.\n%\n% 1. Allow a user to scroll through the history of viewed pages.\n%\n%%\nprob_stat(brow_hist).\n\n%%\n% Use a single interface to display multiple\n% potential client information.\n%\n%%\nprob_stat(sale_disp).\n\n%%\n% Display content linked to a menu\n%\n% 1. User has a list associated with individual appearance schemes.\n% 2. Each time the user changes their selection, change scheme.\n\n%\n%%\nprob_stat(disp_cont).\n\n%%\n% Display information for a highlighted file in a file manager.\n%\n% 1. User has several selection options.\n% 2. System monitors current selection and displays information about the\nselection.\n%\n%%\nprob_stat(disp_info).\n\n%%\n% Allow client to chose one of many sorting algorithms.\n%\n%%\nprob_stat(many_sort).\n\n%%\n% Maintain a remote connection port with various protocols.\n%\n% 1. System maintains an external communication resource.\n% 2. Connections take many forms, and system must handle all types.\n%\n%%\nprob_stat(comm_hand).\n\n%%\n% Create a drawing/editing tool with multiple discrete functions.\n%\n% 1. User can select different drawing tools.\n% 2. Behavior of tool varies with selection.\n%\n%%\nprob_stat(draw_tool).\n\n%%\n% Line break controller for text editor.\n%\n% 1. Text editor uses system to insert line breaks.\n% 2. System must count content width.\n% 3. Content can be text, pictures, graphs, etc.\n%\n%%\nprob_stat(line_break).\n\n%%\n% Interest calculator.\n%\n% 1. A calculator for interest for different programs.\n% 2. Programs have different interest rate and policy.\n%\n%%\nprob_stat(intr_calc).\n\n%%\n% XML parser.\n%\n% 1. Given some XML content tree:\n% 2. Inspect tree nodes.\n% 3. Cache content and inspect children nodes.\n%\n%%\nprob_stat(xml_parse).\n\n%%\n% Graphics renderer\n%\n% 1. Given some graphic input:\n% 2. Distribute input to different graphics engines.\n%\n%%\nprob_stat(render).\n\n%%\n% Compute total costs from multiple sources.\n%\n% 1. Queury different subsystems for costs.\n%.2. Output sum of all subsystem costs.\n% 3. Each subsystem has a different interface.\n%\n%%\nprob_stat(comp_cost).\n\n%%\n% Add a new discount feature to cost calculator.\n%\n% 1. Using an existing calculator system.\n% 2. After calculation, apply a variable discount.\n% 3. Discount is context specific.\n%\n%%\nprob_stat(add_disc).\n\n%%\n% Alerts registered investors of stock price change.\n%\n%%\nprob_stat(tell_inv).\n\n%%\n% Modify bank account based on standing and funds.\n%\n%%\nprob_stat(bank_acct).\n\n%%\n% A single line of communication for differenct connection\n% types and rates.\n%\n%%\nprob_stat(comm_line).\n\n%%\n% HR resource to update employee information.\n%\n%%\nprob_stat(hr_update).\n\n%%\n% Searching DVD rating archieve.\n%\n%%\nprob_stat(dvd_arch).\n\n%%\n% Resolves string matches for archieve query.\n%\n%%\nprob_stat(res_str).\n\n%%\n% Browse DVD collection.\n%\n%%\nprob_stat(dvd_brow).\n\n%%\n% Pass communication from DVD db to UI.\n%\n%%\nprob_stat(dvd_intr).\n\n%%\n% Creates a compressed back up of the DVD information.\n%\n%%\nprob_stat(dvd_back).\n\n%%\n% Updates system with new release information.\n%\n%%\nprob_stat(dvd_rel).\n\n%%\n% DVD quality rankings.\n%\n%%\nprob_stat(dvd_rank).\n\n%%\n% Variable DVD info display.\n%\n%%\nprob_stat(dvd_disp).\n\n%%\n\n% Uniform info display for different media.\n%\n%%\nprob_stat(med_disp).\n\n%%\n% Display a bunch of DVD blurbs.\n%\n%%\nprob_stat(blurbs).\n\n%%\n% Security system manager.\n%\n%%\nprob_stat(secr_man).\n\n%%\n% Message handler.\n%\n%%\nprob_stat(msg_hand).\n\n%%\n% Inventory tracking.\n%\n%%\nprob_stat(inv_track).\n\n%%\n% Exception handling.\n%\n%%\nprob_stat(exp_hand).\n\n%%\n% Graphics validation.\n%\n%%\nprob_stat(pict_val).\n\n%%\n% Get project info.\n%\n%%\nprob_stat(get_info).\n\n%%\n% Ticket master...find cheapest open seat.\n%\n%%\nprob_stat(find_open).\n\n%%\n% Electronic light switch signals.\n%\n%%\n\nprob_stat(light).\n\n%%\n% Window manager message.\n%\n%%\nprob_stat(win_msg).\n\n%%\n% Db entry.\n%\n%%\nprob_stat(db_entry).\n\n%%\n% Login result.\n%\n%%\nprob_stat(login).\n\n%%\n% XML editor.\n%\n%%\nprob_stat(xml_edit).\n\n%%\n% Syntax highligher for IDE.\n%\n%%\nprob_stat(ide_hili).\n\n%%\n% Command processor\n%\n%%\nprob_stat(com_proc).\n\n%%\n% MP3 decoder.\n%\n%%\nprob_stat(mp3_dec).\n\n%%\n% Regex matching.\n%\n%%\nprob_stat(regex_match).\n\n%%\n% Give a uniform interface without any implementation details.\n%\n%%\nprob_stat(hide_impl).\n\n%%\n\n% A stack.\n%\n%%\nprob_stat(stack).\n\n%%\n% A video service for video confrencing.\n%\n%%\nprob_stat(video).\n\n%%\n% A postfix converter.\n%\n%%\nprob_stat(postfix).\n\n%%\n% A binary search tree.\n%\n%%\nprob_stat(bst).\n\n%%\n% A dynamic tree manager.\n%\n%%\nprob_stat(dyn_tree).\n\n%%\n% A safe IO system manager.\n%\n%%\nprob_stat(safe_io).\n\n%%\n% A thread manager.\n%\n%%\nprob_stat(threads).\n\n%%\n% Save a file.\n%\n%%\nprob_stat(save_file).\n\n%%\n% Account history.\n%\n%%\nprob_stat(acct_hist).\n\n%%\n% Modular arithmatic.\n%\n%%\n\nprob_stat(mod_math).\n\n%%\n% Alarm system.\n%\n%%\nprob_stat(alarm).\n\n%%\n% IO driver.\n%\n%%\nprob_stat(io_driver).\n\n%%\n% Message router.\n%\n%%\nprob_stat(msg_rout).\n\n%%\n% CMOS logic circuit,\n%\n%%\nprob_stat(cmos).\n\n%%\n% HR db entry\n%\n%%\nprob_stat(hr_entry).\n\n%%\n% Finite state machine\n%\n%%\nprob_stat(fsm).\n\n%%\n% Traffic light\n%\n%%\nprob_stat(tr_lite).\n\n%%\n% Toll booth.\n%\n%%\nprob_stat(toll).\n\n%%\n% A CMOS logic gate.\n%\n%%\nprob_stat(cmos_gate).\n\n%%\n\n% An auto belt diagnostic program.\n%\n%%\nprob_stat(auto_belt).\n\n%%\n% A monopoly game piece.\n%\n%%\nprob_stat(mono_chip).\n\n%%\n% A monopoly game piece.\n%\n%%\nprob_stat(bed_test).\n\n%%\n% Prioritizer\n%\n%%\nprob_stat(prior).\n\n%%\n% Logrithmic math.\n%\n%%\nprob_stat(log_math).\n\n%%\n% Dynamic interpreter.\n%\n%%\nprob_stat(dyn_interp).\n\n%%\n% Bucket painting.\n%\n%%\nprob_stat(buck_pnt).\n\n%%\n% Rearrange data collection.\n%\n%%\nprob_stat(rearg).\n\n%%\n% Dispatch information.\n%\n%%\nprob_stat(dispatch).\n\n%%\n% Format a string ( -> ALL CAPS ).\n%\n%%\n\nprob_stat(form_str).\n\n% Design Patterns\n%%%%%%%%%%%%%%%%%%%%%%\n\ndesign_pattern(chain_of_resp).\ndesign_pattern(command).\ndesign_pattern(interpreter).\ndesign_pattern(iterator).\ndesign_pattern(mediator).\ndesign_pattern(memento).\ndesign_pattern(observer).\ndesign_pattern(state).\ndesign_pattern(strategy).\ndesign_pattern(template).\ndesign_pattern(visitor).\n\n%%\n% The requirements on the run-time system behavior:\n%\n% static: System behavior is determinstic during run-time.\n% dynamic: System behavior depends on run-time choices.\n%\n%%\nrtbehavior( static ).\nrtbehavior( dynamic ).\n\n%%%%%%%%%%%%%%%%%%%%%%\n% Positive Examples\n%%%%%%%%%%%%%%%%%%%%%%\n\noptimal_pattern( context_help, chain_of_resp).\noptimal_pattern( event_handler, chain_of_resp).\noptimal_pattern( exp_appr, chain_of_resp).\noptimal_pattern( dvd_lib, chain_of_resp).\noptimal_pattern( spam_filt, chain_of_resp).\noptimal_pattern( secr_man, chain_of_resp).\noptimal_pattern( msg_hand, chain_of_resp).\noptimal_pattern( inv_track, chain_of_resp).\noptimal_pattern( exp_hand, chain_of_resp).\noptimal_pattern( pict_val, chain_of_resp).\noptimal_pattern( get_info, chain_of_resp).\noptimal_pattern( find_open, chain_of_resp).\n\noptimal_pattern( action_listener, command ).\noptimal_pattern( model_trans, command ).\noptimal_pattern( trak_calc, command ).\noptimal_pattern( dvd_arch, command ).\noptimal_pattern( light, command ).\noptimal_pattern( win_msg, command ).\noptimal_pattern( db_entry, command ).\noptimal_pattern( login, command ).\noptimal_pattern( xml_edit, command ).\n\noptimal_pattern( ide_hili, command ).\noptimal_pattern( com_proc, command ).\n\noptimal_pattern( parse_text, interpreter ).\noptimal_pattern( eval_formula, interpreter ).\noptimal_pattern( conv_rom, interpreter ).\noptimal_pattern( res_str, interpreter ).\noptimal_pattern( mp3_dec, interpreter ).\noptimal_pattern( regex_match, interpreter ).\noptimal_pattern( hide_impl, interpreter ).\noptimal_pattern( postfix, interpreter ).\n\noptimal_pattern( exam_set, iterator ).\noptimal_pattern( count_set, iterator ).\noptimal_pattern( disc_iter, iterator ).\noptimal_pattern( dvd_brow, iterator ).\noptimal_pattern( stack, iterator ).\noptimal_pattern( bst, iterator ).\n\noptimal_pattern( dialog_input, mediator ).\noptimal_pattern( man_resc, mediator ).\noptimal_pattern( chat_host, mediator ).\noptimal_pattern( dvd_intr, mediator ).\noptimal_pattern( dyn_tree, mediator ).\noptimal_pattern( safe_io, mediator ).\noptimal_pattern( threads, mediator ).\n\noptimal_pattern( brow_hist, memento ).\noptimal_pattern( make_undo, memento ).\noptimal_pattern( sale_disp, memento ).\noptimal_pattern( dvd_back, memento ).\noptimal_pattern( video, memento ).\noptimal_pattern( save_file, memento ).\noptimal_pattern( acct_hist, memento ).\n\noptimal_pattern( disp_cont, observer ).\noptimal_pattern( disp_info, observer ).\noptimal_pattern( tell_inv, observer ).\noptimal_pattern( dvd_rel, observer ).\noptimal_pattern( mod_math, observer ).\noptimal_pattern( alarm, observer ).\noptimal_pattern( io_driver, observer ).\noptimal_pattern( msg_rout, observer ).\noptimal_pattern( cmos, observer ).\n\noptimal_pattern( comm_hand, state ).\noptimal_pattern( draw_tool, state ).\noptimal_pattern( bank_acct, state ).\noptimal_pattern( dvd_rank, state ).\noptimal_pattern( hr_entry, state ).\noptimal_pattern( fsm, state ).\n\noptimal_pattern( line_break, strategy ).\noptimal_pattern( intr_calc, strategy ).\noptimal_pattern( many_sort, strategy ).\noptimal_pattern( dvd_disp, strategy ).\noptimal_pattern( bed_test, strategy ).\n\noptimal_pattern( xml_parse, templete ).\noptimal_pattern( render, templete ).\noptimal_pattern( comm_liner, templete ).\noptimal_pattern( med_disp, templete ).\noptimal_pattern( prior, templete ).\n\noptimal_pattern( comp_cost, visitor ).\noptimal_pattern( add_disc, visitor ).\noptimal_pattern( hr_update, visitor ).\noptimal_pattern( blurbs, visitor ).\noptimal_pattern( buck_pnt, visitor ).\noptimal_pattern( dispatch, visitor ).\noptimal_pattern( form_str, visitor ).\n\n%%%%%%%%%%%%%%%%%%%%%%\n% Background Knowledge\n%%%%%%%%%%%%%%%%%%%%%%\n\n%%\n% Subsystem is triggered by random mouse click.\n% Subsystem must choose one of multiple help subsystems.\n% Each help subsystem passes on request to next most general\n% help subsystem.\n% Least general subsystem choosen.\n% Subsystem displays help message.\n%%\ndistributed( context_help ).\ndep_input( context_help ).\ndep_order( context_help ).\nbehavior( context_help, dynamic ).\nfor_output( context_help ).\n\n%%\n% Randomly occuring event has series of consequences.\n%\n%%\ndistributed( event_handler ).\ndep_order( event_handler ).\nbehavior( event_handler, static ).\nfor_prolif( event_handler ).\n\n%%\n% Iterative check of approving business expense.\n%\n%%\ndep_order( exp_appr ).\ndep_input( exp_appr ).\nbehavior( exp_appr, dynamic ).\nfor_output( exp_appr ).\n\n%%\n% Add DVD to inventory.\n%\n\n%%\ndistributed( dvd_lib ).\ndep_order( dvd_lib ).\nfor_input( dvd_lib ).\nfor_prolif( dvd_lib ).\nbehavior( dvd_lib, static ).\n\n%%\n% Spam Filter.\n%\n%%\ndep_order( spam_filt ).\ndep_input( spam_filt ).\nfor_interp( spam_filt ).\nfor_extension( spam_filt ).\nfor_input( spam_filt ).\nbehavior( spam_filt, dynamic ).\n\n%%\n% Must associate a static action with an input device.\n% Action executed when input triggered.\n%\n%%\ndep_input( action_listener ).\nfor_extension( action_listener ).\nfor_input( action_listener ).\nfor_output( action_listener ).\nfor_prolif( action_listener ).\nfor_monit( action_listener ).\nbehavior( action_listener, static ).\n\n%%\n% Organize various values into a set.\n%\n%%\ndep_input( model_trans ).\nfor_extension( model_trans ).\nfor_input( model_trans ).\nfor_output( model_trans ).\nfor_prolif( model_trans ).\nbehavior( model_trans, dynamic ).\n\n%%\n% Calculator that can track history.\n%\n%%\ntracks_state( trak_calc ).\ndep_order( trak_calc ).\ndep_input( trak_calc ).\ndep_hist( trak_calc ).\nfor_output( trak_calc ).\nbehavior( trak_calc, dynamic ).\n\n%%\n% Recursively parse a literal text string.\n% Breakdown into substrings.\n% Start with most general substring types.\n\n% Use specific evaulation and parser for each substring type.\n%\n%%\ndep_order( parse_text ).\ndep_input( parse_text ).\nfor_inter( parse_text ).\nfor_input( parse_text ).\nbehavior( parse_text, dynamic ).\n\n%%\n% Parse formula into operations tree.\n% Evaluate each operation.\n% Start with base operations and work way up.\n% Use specific evaluation for each operation type.\n% Output value of formula.\n%\n%%\ntracks_value( eval_formula ).\ntracks_ext_state( eval_formula ).\ndep_order( eval_formula ).\ndep_input( eval_formula ).\nfor_interp( eval_formula ).\nfor_input( eval_formula ).\nfor_output( eval_formula ).\nbehavior( eval_formula, dynamic ).\n\n%%\n% Converts a roman numeral to a decimal.\n%\n%%\ntracks_value( conv_rom ).\ntracks_ext_state( conv_rom ).\ndep_order( conv_rom ).\ndep_input( conv_rom ).\nfor_interp( conv_rom ).\nfor_input( conv_rom ).\nfor_output( conv_rom ).\nbehavior( conv_rom, dynamic ).\n\n%%\n% Examine the contents of a random set.\n% Each set implementation must be traversable via the same method.\n%\n%%\ntracks_ext_state( exam_set ).\ndep_order( exam_set ).\ndep_input( exam_set ).\ndep_impl( exam_set ).\ndep_hist( exam_set ).\nfor_input( exam_set ).\nfor_monit( exam_set ).\nbehavior( exam_set, dynamic ).\n\n%%\n% Count the elements of a random set.\n% Each set implementation must be countable via the same method.\n\n%\n%%\ntracks_value( count_set ).\ntracks_ext_state( count_set ).\ndep_order( count_set ).\ndep_input( count_set ).\ndep_impl( count_set ).\ndep_hist( count_set ).\nfor_output( count_set ).\nbehavior( count_set, dynamic ).\n\n%%\n% Iterates through a set, skipping certain\n% elements.\n%\n%%\ntracks_value( disc_iter ).\ntracks_ext_state( disc_iter ).\ndep_order( disc_iter ).\ndep_input( disc_iter ).\ndep_impl( disc_iter ).\ndep_hist( disc_iter ).\nfor_input( disc_iter ).\nfor_monit( disc_iter ).\nbehavior( disc_iter, dynamic ).\n\n%%\n% 1. Control elements of composite dialog.\n% 2. Dynamic UI behavior.\n% 3. User randomly clicks action button.\n% 4. Collects field values from all elements.\n%\n%%\ndistributed( dialog_input ).\nfor_input( dialog_input ).\nbehavior( dialog_input, static ).\n\n%%\n% 1. Monitor resources.\n% 2. Changes must be communicated between resources.\n% 3. Resource values must be consistent.\n%\n%%\ndistributed( man_resc ).\ndaemon( man_resc ).\ntracks_state( man_resc ).\ntracks_value( man_resc ).\ndep_hist( man_resc ).\nfor_prolif( man_resc ).\nfor_monit( man_resc ).\nbehavior( man_resc, dynamic ).\n\n%%\n% A chat room host, coordinates conversations.\n%\n%%\n\ndistributed( chat_host ).\ndaemon( chat_host ).\nfor_input( chat_host ).\nfor_output( chat_host ).\nfor_prolif( chat_host ).\nfor_monit( chat_host ).\nbehavior( chat_host, static ).\n\n%%\n% 1. Maintain a record of the activity history.\n% 2. Browse history.\n% 3. Change display for history movement.\n%\n%%\ndistributed( brow_hist ).\ndaemon( brow_hist ).\ntracks_state( brow_hist ).\ndep_order( brow_hist ).\ndep_hist( brow_hist ).\nfor_monit( brow_hist ).\nbehavior( brow_hist, static ).\n\n%%\n% 1. Maintain a record of the state history.\n% 2. Browse history.\n% 3. Change state for history movement.\n%\n%%\ndistributed( make_undo ).\ndaemon( make_undo ).\ntracks_state( make_undo ).\ndep_order( make_undo ).\ndep_hist( make_undo ).\nfor_monit( make_undo ).\nbehavior( make_undo, static ).\n\n%%\n% Sales display.\n%\n%%\ndistributed( sale_disp ).\ntracks_state( sale_disp ).\ndep_input( sale_disp ).\ndep_hist( sale_disp ).\nfor_output( sale_disp ).\nbehavior( sale_disp, dynamic ).\n\n%%\n% 1. Display content associated with a list member.\n% 2. List member selected randomly by user.\n%\n%%\ndistributed( disp_cont ).\ntracks_value( disp_cont ).\ndep_input( disp_cont ).\nfor_output( disp_cont ).\n\nfor_monit( disp_cont ).\nbehavior( disp_cont, static ).\n\n%%\n% 1. Display file information for user.\n% 2. User randomly selects a file.\n%\n%%\ndistributed( disp_info ).\ntracks_value( disp_info ).\ndep_input( disp_info ).\nfor_output( disp_info ).\nfor_monit( disp_info ).\nbehavior( disp_info, static ).\n\n%%\n% 1. Control communication subsystems.\n% 2. Monitor connection channel.\n% 3. Support multiple connection protocols.\n% 4. Vary subsystem behavior to fit comm type.\n% 5. Dispatch information to appropriate subsystems.\n%%\ndistributed( comm_hand ).\ndaemon( comm_hand ).\ntracks_state( comm_hand ).\ntracks_ext_state( comm_hand ).\ndep_input( comm_hand ).\ndep_impl( comm_hand ).\nfor_input( comm_hand ).\nfor_output( comm_hand ).\nfor_prolif( comm_hand ).\nfor_monit( comm_hand ).\nbehavior( comm_hand, dynamic ).\n\n%%\n% 1. Graphics tool that can vary behavior.\n% 2. Different user toolbox selections modify behavior.\n% 3. Each behavior has unique engine.\n%.4. User randomly changes behavior.\n%\n%%\ndistributed( draw_tool ).\ndaemon( draw_tool ).\ntracks_state( draw_tool ).\ndep_input( draw_tool ).\nfor_output( draw_tool ).\nbehavior( draw_tool, dynamic ).\n\n%%\n% 1. Monitor line contents for threshold value.\n% 2. Randomly occuring event triggers line break.\n%\n%%\ndaemon( line_break ).\ntracks_state( line_break ).\ntracks_value( line_break ).\n\ndep_input( line_break ).\ndep_hist( line_break ).\nfor_monit( line_break ).\nbehavior( line_break, dynamic ).\n\n%%\n% Let client choose one of many sorting algorithms.\n%\n%%\ntracks_state( many_sort ).\ntracks_value( many_sort ).\ndep_order( many_sort ).\ndep_input( many_sort ).\nfor_output( many_sort ).\nbehavior( many_sort, dynamic ).\n\n%%\n% 1. Calculates interest in various ways.\n% 2. Outputs interest.\n%\n%%\ntracks_state( intr_calc ).\ntracks_value( intr_calc ).\ndep_input( intr_calc ).\nfor_output( intr_calc ).\nbehavior( intr_calc, dynamic ).\n\n%%\n% 1. Parses XML content tree.\n% 2. Iterates down parent and children.\n% 3. Examines node and caches content.\n% 4. Different node types use different operation.\n%\n%%\ntracks_state( xml_parse ).\ntracks_ext_state( xml_parse ).\ndep_order( xml_parse ).\ndep_input( xml_parse ).\nfor_interp( xml_parse ).\nfor_input( xml_parse ).\nfor_output( xml_parse ).\nbehavior( xml_parse, dynamic ).\n\n%%\n% 1. Send relevant information to different graphics engines.\n% 2. Certain graphics types only use some engines.\n%\n%%\ntracks_state( render ).\ntracks_ext_state( render ).\ndep_order( render ).\ndep_input( render ).\nfor_monit( render ).\nfor_prolif( render ).\nbehavior( render, dynamic ).\n\n%%\n% 1. Collects cost values from various independent subsystems.\n% 2. Each subsystem has different interface.\n%\n%%\ndistributed( comp_cost ).\ntracks_value( comp_cost ).\ndep_order( comp_cost ).\ndep_input( comp_cost ).\nfor_output( comp_cost ).\nbehavior( comp_cost, dynamic ).\n\n%%\n% 1. Add a dynamic discount feature to an existing program.\n%\n%%\ndistributed( comp_cost ).\ndep_input( comp_cost ).\nfor_output( comp_cost ).\nbehavior( comp_cost, dynamic ).\n\n%%\n% Tell investors of a change in stock price.\n%\n%%\ndistributed( tell_inv ).\ndaemon( tell_inv ).\ntracks_value( tell_inv ).\ndep_input( tell_inv ).\nfor_output( tell_inv ).\nfor_monit( tell_inv ).\nfor_prolif( tell_inv ).\nbehavior( tell_inv, static ).\n\n%%\n% Dynamic bank account.\n%\n%%\ndaemon( bank_acct ).\ntracks_state( bank_acct ).\ntracks_value( bank_acct ).\ndep_input( bank_acct ).\nfor_prolif( bank_acct ).\nfor_monit( bank_acct ).\nbehavior( bank_acct, dynamic ).\n\n%%\n% Dynamic communication line.\n%\n%%\ndistributed( comm_line ).\ndaemon( comm_line ).\ntracks_state( comm_line ).\ntracks_value( comm_line ).\n\ndep_input( comm_line ).\nfor_input( comm_line ).\nfor_output( comm_line ).\nfor_prolif( comm_line ).\nfor_monit( comm_line ).\nbehavior( comm_line, dynamic ).\n\n%%\n% Update HR db info.\n%\n%%\ndistributed( hr_update ).\nfor_output( hr_update ).\nbehavior( hr_update, dynamic ).\n\n%%\n% Access DVD archieve.\n%\n%%\ndep_input( dvd_arch ).\nfor_input( dvd_arch ).\nfor_output( dvd_arch ).\nfor_prolif( dvd_arch ).\nbehavior( dvd_arch, dynamic ).\n\n%%\n% Resolve search string.\n%\n%%\ndep_order( res_str ).\ndep_input( res_str ).\nfor_interp( res_str ).\nfor_input( res_str ).\nfor_prolif( res_str ).\nbehavior( res_str, dynamic ).\n\n%%\n% Browse DVD collection.\n%\n%%\ntracks_value( dvd_brow ).\ntracks_ext_state( dvd_brow ).\ndep_order( dvd_brow ).\ndep_input( dvd_brow ).\nfor_output( dvd_brow ).\nbehavior( dvd_brow, dynamic ).\n\n%%\n% DVD interface,\n%\n%%\ndep_input( dvd_intr ).\n\nfor_input( dvd_intr ).\nfor_output( dvd_intr ).\nfor_prolif( dvd_intr ).\nfor_monit( dvd_intr ).\nbehavior( dvd_intr, static ).\n\n%%\n% Creates compressed back up\n%\n%%\ntracks_ext_state( dvd_back ).\ndep_input( dvd_back ).\nfor_input( dvd_back ).\nbehavior( dvd_back, static ).\n\n%%\n% Updates system with new release info.\n%\n%%\ndistributed( dvd_rel ).\ntracks_value( dvd_rel ).\ndep_input( dvd_rel ).\nfor_output( dvd_rel ).\nfor_prolif( dvd_rel ).\nfor_monit( dvd_rel ).\nbehavior( dvd_rel, static ).\n\n%%\n% Ranking system for DVDs\n%\n%%\ntracks_state( dvd_rank ).\ntracks_value( dvd_rank ).\nfor_output( dvd_rank ).\nbehavior( dvd_rank, static ).\n\n%%\n% Variable DVD info display.\n%\n%%\ndistributed( dvd_disp ).\ndep_input( dvd_disp ).\nfor_output( dvd_disp ).\nbehavior( dvd_disp, dynamic ).\n\n%%\n% Uniform media info display.\n%\n%%\ndistributed( med_disp ).\ndep_input( med_disp ).\nfor_output( med_disp ).\nbehavior( med_disp, dynamic ).\n\n%%\n% Display many info blurbs.\n%\n%%\ndistributed( blurbs ).\nfor_output( blurbs ).\nbehavior( blurbs, dynamic ).\n\n%%\n% Security system manager.\n%\n%%\ndistributed( secr_man ).\ndaemon( secr_man ).\ndep_order( secr_man ).\ndep_input( secr_man ).\nfor_input( secr_man ).\nfor_monit( secr_man ).\nfor_prolif( secr_man ).\nbehavior( secr_man, dynamic ).\n\n%%\n% MFC Message handler.\n%\n%%\ndistributed( msg_hand ).\ndaemon( msg_hand ).\ndep_input( msg_hand ).\nfor_input( msg_hand ).\nfor_output( msg_hand ).\nfor_prolif( msg_hand ).\nbehavior( msg_hand, dynamic ).\n\n%%\n% Inventory tracker.\n%\n%%\ndistributed( inv_track ).\ntracks_ext_state( inv_track ).\ndep_order( inv_track ).\nfor_output( inv_track ).\nfor_monitoring( inv_track ).\nbehavior( inv_track, static ).\n\n%%\n% Exception handling.\n%\n%%\ndistributed( exp_hand ).\ndaemon( exp_hand ).\ntracks_state( exp_hand ).\ndep_order( exp_hand ).\ndep_input( exp_hand ).\ndep_hist( exp_hand ).\n\nfor_output( exp_hand ).\nfor_prolif( exp_hand ).\nfor_monit( exp_hand ).\nbehavior( exp_hand, dynamic ).\n\n%%\n% Graphics validation.\n%\n%%\ndistributed( pict_val ).\ndaemon( pict_val ).\ndep_order( pict_val ).\nfor_prolif( pict_val ).\nfor_monit( pict_val ).\nbehavior( pict_val, static ).\n\n%%\n% Get info from project hierarchy.\n%\n%%\ndistributed( get_info ).\ndep_order( get_info ).\ndep_input( get_info ).\nfor_output( get_info ).\nbehavior( get_info, static ).\n\n%%\n% Finds cheapest open seat for venue.\n%\n%%\ndep_order( find_open ).\nfor_output( find_open ).\nbehavior( find_open, static ).\n\n%%\n% Light switch operation,\n%\n%%\ndep_input( light ).\nfor_input( light ).\nfor_output( light ).\nfor_prolif( light ).\nbehavior( light, dynamic ).\n\n%%\n% Window manager message.\n%\n%%\ndep_input( win_msg ).\nfor_input( win_msg ).\nfor_output( win_msg ).\nfor_prolif( win_msg ).\nbehavior( win_msg, dynamic ).\n\n%%\n% Database entry.\n%\n%%\ndep_input( db_entry ).\nfor_input( db_entry ).\nfor_output( db_entry ).\nfor_prolif( db_entry ).\nbehavior( db_entry, dynamic ).\n\n%%\n% Login result.\n%\n%%\ndep_input( login ).\nfor_input( login ).\nbehavior( login, dynamic ).\n\n%%\n% XML editor.\n%\n%%\ntracks_state( xml_edit ).\ntracks_ext_state( xml_edit ).\nfor_interp( xml_edit ).\nfor_extension( xml_edit ).\nfor_input( xml_edit ).\nbehavior( xml_edit, dynamic ).\n\n%%\n% IDE text high lighter.\n%\n%%\ndaemon( ide_hili ).\ntracks_values( ide_hili ).\ndep_input( ide_hili ).\nfor_interp( ide_hili ).\nfor_extension( ide_hili ).\nbehavior( ide_hili, dynamic ).\n\n%%\n% Command processor.\n%\n%%\ndep_input( com_proc ).\nfor_interp( com_proc ).\nfor_extension( com_proc ).\nfor_input( com_proc ).\nbehavior( com_proc, dynamic ).\n\n%%\n% MP3 Decoder\n%\n%%\ndep_order( mp3_dec ).\ndep_input( mp3_dec ).\nfor_interp( mp3_dec ).\nfor_input( mp3_dec ).\nbehavior( mp3_dec, dynamic ).\n\n%%\n% Regular expression matching.\n%\n%%\ntracks_value( regex_match ).\ndep_order( regex_match ).\ndep_input( regex_match ).\nfor_interp( regex_match ).\nfor_input( regex_match ).\nbehavior( regex_match, dynamic ).\n\n%%\n% Hide implementation from clients.\n%\n%%\ntracks_value( hide_impl ).\ntracks_state( hide_impl ).\ntracks_ext_state( hide_impl ).\ndep_impl( hide_impl ).\nfor_extension( hide_impl ).\nfor_input( hide_impl ).\nfor_output( hide_impl ).\nfor_prolif( hide_impl ).\nbehavior( hide_impl, dynamic ).\n\n%%\n% A stack.\n%\n%%\ntracks_state( stack ).\ndep_order( stack ).\ndep_hist( stack ).\ndep_impl( stack ).\nfor_output( stack ).\nbehavior( stack, dynamic ).\n\n%%\n% A video confrencer with memory.\n%\n%%\ndistributed( video ).\ntracks_ext_state( video ).\ndep_hist( video ).\nfor_input( video ).\nfor_output( video ).\n\nfor_monit( video ).\nbehavior( video, static ).\n\n%%\n% A postfix converter.\n%\n%%\ndep_order( postfix ).\ndep_input( postfix ).\nfor_interp( postfix ).\nfor_input( postfix ).\nfor_prolif( postfix ).\nbehavior( postfix, dynamic ).\n\n%%\n% A binary search tree.\n%\n%%\ntracks_state( bst ).\ndep_order( bst ).\ndep_input( bst ).\ndep_hist( bst ).\nfor_input( bst ).\nfor_output( bst ).\nfor_monit( bst ).\nbehavior( bst, dynamic ).\n\n%%\n% A dynamic tree manager.\n%\n%%\ndistributed( dyn_tree ).\ntracks_state( dyn_tree ).\ndep_input( dyn_tree ).\nfor_monit( dyn_tree ).\nbehavior( dyn_tree, dynamic ).\n\n%%\n% A safe IO manager.\n%\n%%\ndistributed( safe_io ).\ndaemon( safe_io ).\ntracks_state( safe_io ).\ndep_input( safe_io ).\ndep_hist( safe_io ).\nfor_prolif( safe_io ).\nfor_monit( safe_io ).\nbehavior( safe_io, dynamic ).\n\n%%\n\n% A thread manager.\n%\n%%\ndistributed( threads ).\ndaemon( threads ).\ntracks_state( threads ).\ndep_order( threads ).\ndep_hist( threads ).\nfor_prolif( threads ).\nfor_monit( threads ).\nbehavior( threads, dynamic ).\n\n%%\n% Save a file.\n%\n%%\ntracks_ext_state( save_file ).\ndep_hist( save_file ).\nfor_input( save_file ).\nbehavior( save_file, static ).\n\n%%\n% Account History\n%\n%%\ntracks_ext_state( acct_hist ).\ndep_order( acct_hist ).\ndep_hist( acct_hist ).\nfor_input( acct_hist ).\nfor_monit( acct_hist ).\nbehavior( acct_hist, static ).\n\n%%\n% Modular Math\n%\n%%\ntracks_value( disp_cont ).\ndep_input( disp_cont ).\nfor_output( disp_cont ).\nbehavior( disp_cont, dynamic ).\n\n%%\n% Alarm system\n%\n%%\ndistributed( alarm ).\ndaemon( alarm ).\ntracks_state( alarm ).\ndep_input( alarm ).\nfor_output( alarm ).\nfor_monit( alarm ).\nfor_prolif( alarm ).\nbehavior( alarm, dynamic ).\n\n%%\n% IO Monitor\n%\n%%\ndistributed( io_driver ).\ndaemon( io_driver ).\ntracks_state( io_driver ).\ntracks_ext_state( io_driver ).\ndep_input( io_driver ).\nfor_input( io_driver ).\nfor_output( io_driver ).\nfor_monit( io_driver ).\nfor_prolif( io_driver ).\nbehavior( io_driver, dynamic ).\n\n%%\n% Message router.\n%\n%%\ndistributed( msg_rout ).\ndaemon( msg_rout ).\ndep_input( msg_rout ).\nfor_input( msg_rout ).\nfor_output( msg_rout ).\nfor_monit( msg_rout ).\nfor_prolif( msg_rout ).\nbehavior( msg_rout, dynamic ).\n\n%%\n% CMOS logic ciruict.\n%\n%%\ndistributed( cmos ).\ndaemon( cmos ).\ntracks_state( cmos ).\ntracks_value( cmos ).\ndep_order( cmos ).\ndep_input( cmos ).\ndep_hist( cmos ).\nfor_input( cmos ).\nfor_output( cmos ).\nfor_monit( cmos ).\nfor_prolif( cmos ).\nbehavior( cmos, dynamic ).\n\n%%\n% HR db entry.\n%\n%%\ntracks_state( hr_entry ).\ndep_input( hr_entry ).\nfor_input( hr_entry ).\nfor_output( hr_entry ).\nfor_prolif( hr_entry ).\n\nfor_monit( hr_entry ).\nbehavior( hr_entry, dynamic ).\n\n%%\n% A FSM\n%\n%%\ndistributed( fsm ).\ntracks_state( fsm ).\ndep_input( fsm ).\nfor_input( fsm ).\nfor_monit( fsm ).\nbehavior( fsm, static ).\n\n%%\n% Bed tester for a couple.\n%\n%%\ntracks_state( bed_test ).\ntracks_value( bed_test ).\ndep_input( bed_test ).\nfor_output( bed_test ).\nbehavior( bed_test, dynamic ).\n\n%%\n% Prioritizer\n%\n%%\ntracks_ext_state( prior ).\ndep_order( prior ).\ndep_input( prior ).\nfor_output( prior ).\nbehavior( prior, dynamic ).\n\n%%\n% Bucket painting.\n%\n%%\ndistributed( buck_pnt ).\nfor_output( buck_pnt ).\nbehavior( buck_pnt, static ).\n\n%%\n% Dispatch\n%\n%%\ndistributed( dispatch ).\ndaemon( dispatch ).\ndep_input( dispatch ).\nfor_prolif( dispatch ).\nbehavior( dispatch, static ).\n\n%%\n% Format string.\n%\n%%\ndep_input( form_str ).\nfor_prolif( form_str ).\nbehavior( form_str, static )."
    }
  ]
}