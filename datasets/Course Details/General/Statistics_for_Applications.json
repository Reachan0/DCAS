{
  "course_name": "Statistics for Applications",
  "course_description": "This course offers a broad treatment of statistics, concentrating on specific statistical techniques used in science and industry. Topics include: hypothesis testing and estimation, confidence intervals, chi-square tests, nonparametric statistics, analysis of variance, regression, and correlation.\nOCW offers an earlier version of this course, from Fall 2003. This newer version focuses less on estimation theory and more on multiple linear regression models. In addition, a number of Matlab examples are included here.",
  "topics": [
    "Mathematics",
    "Applied Mathematics",
    "Discrete Mathematics",
    "Probability and Statistics",
    "Mathematics",
    "Applied Mathematics",
    "Discrete Mathematics",
    "Probability and Statistics"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nPrerequisites\n\nProbability and Random Variables (18.440\n)\nor\nProbabilistic Systems Analysis (6.041)\n\nTopics\n\nMaximum Likelihood Estimators\n\nProperties\n\nFisher Information\n\nAsymptotic Variance of MLE\n\nParameters of Normal Distribution\n\nChi-squared and t-Distribution\n\nDistribution of the Estimates of Parameters of Normal Distribution\n\nConfidence Intervals\n\nTesting Hypotheses\n\nt-Tests and F-Tests\n\nBayes Tests\n\nMost Powerful Tests (Including Randomized)\n\nGoodness-of-fit Tests\n\nSimple Discrete\n\nContinuous\n\nComposite Goodness-of-fit Tests\n\nIndependence and Homogeneity Tests\n\nKolmogorov-Smirnov Test\n\nLinear Regression\n\nEstimating Parameters\n\nJoint Distribution of Estimates\n\nTesting Hypotheses about Parameters\n\nConfidence and Prediction Intervals\n\nJoint Confidence Sets\n\nMultiple Regression, Analyses of Variance and Covariance\n\nDistribution of Estimates\n\nTesting General Linear Hypotheses\n\nGrading\n\nACTIVITIES\n\nweightS\n\nTen Problem Sets\n\n10 points each\n\nTwo Midterm Exams\n\n150 points each\n\nText\n\nDeGroot, Morris H., and Mark J. Schervish.\nProbability and Statistics\n. 3rd ed. Boston, MA: Addison-Wesley, 2002.\n\nCalendar\n\nThe calendar below provides information on the course's lecture (L) and exam (E) sessions.\n\nSES #\n\nTOPICS\n\nKEY DATES\n\nL1\n\nOverview of some Probability Distributions\n\nL2\n\nMaximum Likelihood Estimators\n\nL3\n\nProperties of Maximum Likelihood Estimators\n\nProblem set 1 due\n\nL4\n\nMultivariate Normal Distribution and CLT\n\nL5\n\nConfidence Intervals for Parameters of Normal Distribution\n\nProblem set 2 due\n\nL6\n\nGamma, Chi-squared, Student T and Fisher F Distributions\n\nProblem set 3 due\n\nL7-L8\n\nTesting Hypotheses about Parameters of Normal Distribution, t-Tests and F-Tests\n\nProblem set 4 due in Ses #L8\n\nL9\n\nTesting Simple Hypotheses\n\nBayes Decision Rules\n\nProblem set 5 due\n\nE1\n\nExam 1\n\nL10\n\nMost Powerful Test for Two Simple Hypotheses\n\nL11\n\nChi-squared Goodness-of-fit Test\n\nL12\n\nChi-squared Goodness-of-fit Test for Composite Hypotheses\n\nL13\n\nTests of Independence and Homogeneity\n\nProblem set 6 due\n\nL14\n\nKolmogorov-Smirnov Test\n\nL15-L16\n\nSimple Linear Regression\n\nProblem set 7 due\n\nL17-L18\n\nMultiple Linear Regression\n\nProblem set 8 due\n\nL19-L20\n\nGeneral Linear Constraints in Multiple Linear Regression\n\nAnalysis of Variance and Covariance\n\nProblem set 9 due\n\nProblem set 10 due in Ses #L20\n\nE2\n\nExam 2\n\nL21\n\nClassification Problem, AdaBoost Algorithm\n\nL22\n\nReview",
  "files": [
    {
      "category": "Resource",
      "title": "home1.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-443-statistics-for-applications-fall-2006/8bc67cbaee527be81d0184a814f2da3e_home1.pdf",
      "content": "!\n18.443. Pset 1. Due Wednesday, September 13th.\n(1) Prove that\nn k\nn-k\nk\n-\nlim\n1 -\n=\ne\n.\nn!1 k\nn\nn\nk!\n(2) Compute EX, EX2 and Var(X) for N(μ, λ2), B(p), E(), (), U(0, ).\n(3) Generate a sample X of size 100 from N(μ = 5, λ2 = 4). Compute\nsample mean and sample standard deviation of this sample using Matlab\nfunctions 'mean(X)' and 'std(X)' or 'std(X,1)'. What is the difference be\ntween 'std' and 'std( ,1)' (read Matlab help)? Plot on the same graph an\nempirical c.d.f. of your data using 'cdfplot' function and a normal c.d.f. with\nestimated mean and standard deviation. Print out the graph and write a\nsequence of all Matlab commands in this exercise."
    },
    {
      "category": "Resource",
      "title": "home2.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-443-statistics-for-applications-fall-2006/c881009470d4b8d07a400ec27b7ca746_home2.pdf",
      "content": "(\n18.443. Pset 2. Due Wednesday, Sep 20.\n(1) (Similar to example in lecture 2.) Take a height of men data subset\n(second column in body men.mat file on the class website). Use 'dfitool' to\nfit normal and log-normal distributions (or, if necessary, gamma). If you\nthink that a fit looks good (or not) perform a chi-squared test to see if the\ntest agrees with you.\n(2) Find MLE of a parameter for Poisson distribution (). Compute\nFisher information and state asymptotic normality property of MLE.\n(3) Prove that if X1, . . . , Xn U[0, ] then MLE ˆ = max(X1, . . . , Xn)\nis consistent, i.e. ˆ ! in probability. Find a constant c such that cˆ is\nunbiased estimate of .\n(4) Consider a parametric family of distributions with the p.d.f. given by\ne-x , when x ,\nf(x|) =\n0,\nwhen x < ,\nand where -∼ < < ∼. Find MLE .\nˆ\n(6) For multivariate parameter = (1, . . . , k) prove that Fisher infor\nmation matrix\nI() := Cov(→l(X|)) = -EHess(l(X|))."
    },
    {
      "category": "Resource",
      "title": "home3.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-443-statistics-for-applications-fall-2006/b31027e0924f20e5a2c79e20d15911ab_home3.pdf",
      "content": "!\n\n!\n18.443. Pset 3. Due Wednesday, Sep 27.\n1. page 403, no. 3(b).\n2. page 403, no. 4.\n3. (Solve this problem by hand without Matlab, but at each linear algebra\nstep write a Matlab function that would do it, like 'sqrtm', 'eig', 'inv', etc.)\nSuppose that a vector X has normal distribution N(0, ) with covariance\n=\n.\n(a) Write the joint density f(x1, x2) of X.\n(b) Find a 2 × 2 matrix A such that for i.i.d. standard normal vector g,\nthe distribution of Ag is N(0, ).\n(c) What is the distribution of Y = MX where\n-2\nM =\n?\n-0.5\nDoes Y have a density on R2?\n4. page 415, no. 7 (also find the confidence interval for 2.) Do it by\nhand and then use 'normfit' to check your answers.\n5. Given a sample of size n = 15 from normal distribution what is the\nprobability that the interval\nh\nˆ\nˆ\ni\n\nX -p\n, X + 2 p\nn - 1\nn - 1\nwill cover unknown parameter μ?"
    },
    {
      "category": "Resource",
      "title": "home4.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-443-statistics-for-applications-fall-2006/1d867f338d39d830fd8c1256a1f3c5ba_home4.pdf",
      "content": "18.443. Pset 4. Due Wednesday, Oct. 4.\n1. In the confidence interval for variance of normal distribution we find\nthe constants c1, c2 such that\n(0, c1) = 2\n(c2, 1) = 1 - .\nn-1\nn-1\nProve that\nc1\nlim\n= 1.\nn!1 n\n2. page 409, no. 3.\n3. page 513, no. 5.\n4. In pset 3, problem 3 (c), a random variable Y has what density on\nwhat subspace?\n5. Consider positive numbers a1, . . . , an > 0 and consider a nonnegative\ndefinite covariance matrix with entries ij = paiaj , i.e.\n⎞\npa1a1 · · · pa1an\nB\n⎜\n=\n..\n..\n..\n.\nB\n⎜\n@\n.\n.\n.\n⎝\npana1 · · · panan\nNormal distribution N(0, ) has what density on what subspace? Hint: Find\nan easy obvious choice of a matrix A such that = AAT . This matrix does\nnot have to be square!"
    },
    {
      "category": "Resource",
      "title": "home5.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-443-statistics-for-applications-fall-2006/ba28016dac1943fb288f5b00bb4f1797_home5.pdf",
      "content": "18.443. Pset\nDue Wednesday, Oct. 11.\n1.(50 points) Consider the third column - heart rate - in the normal body\ntemperature dataset normtemp.mat. Let X1, . . . , X65 be a sample of heart\nrate of men corresponding to rows 1 through 65. Let Y1, . . . , Y50 be a random\nsubset of size 50 of heart rate of women from rows 66 through 130. The\nreason you take a random subset is because the values are arranged in an\nincreasing order in the dataset. In other words, you can randomly permute\nthe values for women and then take the first fifty. For both samples, find\na normal fit and test it using chi-squared test. Then, perform statistical\nanalyses of both samples assuming that their distributions are normal. Find\nconfidence intervals for the mean and variance of the distributions of both\nsamples. Test if the means are equal using paired t-test, and t-tests for two\nsamples with and without the assumption that the variances are equal. Test\nthe hypothesis that the variances are equal using F-test. Write each step\nof your work, all formulas, threshold constants, p-values, etc. Use textbook\ntables to find the thresholds and p-values in each test.\n5."
    },
    {
      "category": "Resource",
      "title": "home6.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-443-statistics-for-applications-fall-2006/0f260a8953a728ce64f7e055d37b19e7_home6.pdf",
      "content": "18.443. Pset 6. Due Wednesday, Nov. 1.\n(1) page 470, no. 7.\n(2) Suppose you are given five observations from Poisson distribution\n(). Find the most powerful test for\nH0 : = 2 vs. H1 : = 1\nwith level of significance = 0.05.\n(3) page 541, no. 4.\n(4) page 542, no. 9."
    },
    {
      "category": "Resource",
      "title": "home7.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-443-statistics-for-applications-fall-2006/35c38799295780c95aef3106297943de_home7.pdf",
      "content": "18.443. Pset 7. Due Wednesday, Nov. 8.\n(1) page 548, no. 2.\n(2) page 549, no. 3.\n(3) page 549, no. 6.\n(4) page 555, no. 5.\n(5) page 565, no. 3."
    },
    {
      "category": "Resource",
      "title": "home8.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-443-statistics-for-applications-fall-2006/fd9b9957848954802ccf116f0069a0ed_home8.pdf",
      "content": "18.443. Pset 8. Due Wednesday, Nov. 15.\n(1) page 574, no. 5.\n(2) page 574, no. 7. Solve this problem using Matlab. Briefly explain the\noutput of 'kstest'.\n(3) page 617, no. 7.\n(4) page 617, no. 8. (MSE means variance)"
    },
    {
      "category": "Resource",
      "title": "home9.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-443-statistics-for-applications-fall-2006/1ca9d355ad6463b02034f39cc5ecb417_home9.pdf",
      "content": "18.443. Pset 9. Due Wednesday, Nov. 22.\n(1) page 636, no. 5.\n(2) page 637, no. 9.\n(3) page 637, no. 17.\n(4) page 663, nos. 7 and 8. (You can use Matlab for computations if you\nwrite down all the commands you use.)"
    },
    {
      "category": "Resource",
      "title": "home10.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-443-statistics-for-applications-fall-2006/4782e779dfa4b02117f8973ee066955e_home10.pdf",
      "content": "18.443. Pset 10. Due Wednesday, Nov. 29.\n(1) Consider OECD Economic Development dataset. Take logarithm of\nper capita income as a response variable Y and percent of labor force in\nagriculture and industry as two explanatory variables.\nlog(income) = 1 + 2 (% in agr.) + 3 (% in ind.) + \".\n(a) Without using Matlab 'regress' function, compute estimates of pa\nrameters ˆ and ˆα2 and test the hypothesis\nH0 : 2 = 3 = 0\nat the level of significance = 0.05 and compute a p-value.\n(b) Construct a 95% confidence interval for parameter 1.\nWrite all steps of your work, formulas, Matlab commands and outputs."
    },
    {
      "category": "Resource",
      "title": "practice1.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-443-statistics-for-applications-fall-2006/bd8a23261c1afc75b6fd120476c6180e_practice1.pdf",
      "content": "B\n@\np\np\np\np\nB\np\np\np\n18.443. Practice test 1.\nConsider the family of distributions with p.d.f.\nf(x|) = x-1 , for 0 < x < 1, and > 0.\n(1)\nConsider an i.i.d. sample X1, . . . , Xn from this distribution. As always, the\nunderlying parameter for this sample is unknown. In problems (1) and (2)\nthe distribution is given by equation (1) above.\n(1) Find the MLE ˆ of .\n(2) Compute Fisher information I() and state asymptotic normality of\nMLE ˆ. If n = 100, find c such that\nPr(-c ˆ- c) 0.95.\n(3) Suppose that a covariance matrix\n⎞\n⎞\n⎞\n-\n-\nB\n⎜\np\nB\n⎜\nB\n⎜\n=\n⎜\n⎜ .\n@\n⎝\n⎝\n@\n⎝\nDistribution N(0, ) has what density in what basis?\n(4) Suppose that a sample X1, . . . , X15 N(μx, αx\n2), where μx and αx\n2 are\nunknown, has sample mean and sample variance\nμˆx = X = 2.4, αˆx\n2 = X 2 - X 2 = 0.55.\nFind 95% confidence intervals for μ and α2 .\n(5) In addition to the sample from problem (4) suppose that we are given\na sample Y1, . . . , Y10 N(μy, αx\n2) from a distribution with the same variance\nas Xs, i.e. αx\n2 = αy\n2 , but possibly different mean μy. Suppose that\nμˆy = Y = 2.8, αˆy\n2 = Y 2 - Y 2 = 0.37.\nFind a 90% confidence interval for μx - μy.\n(6) For samples in problems (4) and (5), perform the t-test of the hypoth\nesis that μx μy under the assumption of equal variances. Test whether the\nvariances are equal using the F-test. In both tests, find a p-value and use\nlevel of significance = 0.05.\n(7) If X1, . . . , X5 are i.i.d. exponential E(), what is the distribution of\nX1 + . . . + X5. If X (, ) what is the distribution of cX where c > 0 is\na constant.\n(8) page 469, no. 2."
    },
    {
      "category": "Resource",
      "title": "ptest2_443.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-443-statistics-for-applications-fall-2006/a42644e735a7c7de5e4fa62032b6804d_ptest2_443.pdf",
      "content": "18.443. Practice test 2.\n(1) Given a sample 5, 1, 4, 1, 2, 3 from Poisson distribution (), construct\nthe most powerful test for\nH0 : = 1 vs. H1 : = 2,\nwith level of significance = 0.05. Test H0.\n(2) p. 561, no. 1.\n(3) p. 574, no. 4.\n(4) Suppose that in the multiple linear regression model Y = X +λ with\n20 observations, we have\n⎞\nB\n⎜\nαˆ2 = 1.5, ˆ = (1, 2, 1)T , (XT X)-1 = @ 1\n1 ⎝ .\nTest the hypothesis\nH0 : 1 = 4 - 23 and 2 = 0.5 + 3\nat the level of significance = 0.05.\n(5) p. 672, no. 5.\n(6) In two-way analysis of variance, how can we estimate the parameters\nμij and test the hypothesis\nH0 : all μij are equal?\n(7) In two-way analysis of variance, write parameters μ, i, j , βij in terms\nof parameters μij ."
    },
    {
      "category": "Lecture Notes",
      "title": "lecture1.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-443-statistics-for-applications-fall-2006/46afee1cbe70839c128ef68b2a6b1e16_lecture1.pdf",
      "content": "X\nZ 1\nLecture 1\nOverview of some probability\ndistributions.\nIn this lecture we will review several common distributions that will be used often throughtout\nthe class. Each distribution is usually described by its probability function (p.f.) in the case\nof discrete distributions or probability density function (p.d.f.) in the case of continuous\ndistributions. Let us recall basic definitions associated with these two cases.\nDiscrete distributions.\nSuppose that a set X consists of a countable or finite number of points,\nX = {a1, a2, a3, · · ·}.\nThen a probability distribution P on X can be defined via a function p(x) on X with the\nfollowing properties:\n1. 0 ≈ p(ai) ≈ 1,\n2. P1\ni=1 p(ai) = 1.\nA function p(x) is called the probability function. If X is a random variable with distribution\nP then p(ai) = P(X = ai) - a probability that X takes value ai. Given a function ' : X ! R,\nthe expectation of '(X) is defined by\nE'(X) =\n'(ai)p(ai).\ni=1\n(Absolutely) continuous distributions.\nContinuous distribution P on R is defined via a probability density function (p.d.f.) p(x)\non R such that\np(X) ∼ 0 and\np(X)dx = 1.\n-1\nIf a random variable X has distribution P then the probability that X takes a value in the\ninterval [a, b] is given by\nZ b\nP(X inf [a, b]) =\np(x)dx.\na\n\nZ 1\n\nClearly, in this case for any a inf R we have P(X = a) = 0. Given a function ' : X ! R, the\nexpectation of '(X) is defined by\nE'(X) =\n'(x)p(x)dx.\n-1\nNotation. The fact that a random variable X has distribution P will be denoted by\nX P.\nNormal (Gaussian) Distribution N(, π2). Normal distribution is a continuous dis\ntribution on R with p.d.f.\np(x) = ≤\nφπ\ne- (x-)2\nfor x inf (-→, →).\nHere -→ < <\n, π > 0 are the parameters of the distribution. Let us recall some\n→\nproperties of a normal distribution. If a random variable X has a normal distribution N(, π2)\nthen the r.v.\nY = X - N(0, 1)\nπ\nhas a standard normal distribution N(0, 1). To see this, we can write,\nP\nX - inf [a, b]\n\n= P(X inf [aπ + , bπ + ]) =\nZ\naα\nbα\n+\n+\n\n≤\nφπ\ne- (x-)2\ndx\nπ\nZ b\ny\n=\ne- dy,\na\n≤\n2φ\nwhere in the last integral we made a change of variables y = (x - )/π. This, of course,\nmeans that Y N(0, 1). The expectation of Y is\ny\nZ 1\nEY =\ny ≤\n2φe- 2 dy = 0\n-1\nsince the integrand is an odd function. To compute the second moment EY 2 , let us first note\nthat since p1\n2 e- y is a probability density function, it integrates to 1, i.e.\ny\nZ 1\n≤\n2φ\n1 =\ne- dy.\n-1\nIf we integrate this by parts, we get,\ny\ny\ny\n=\nZ 1\n1 e- 2\ndy =\n1 ye- 2\nZ 1\ny\n(-y)e- 2\ndy\n≤\n2φ\n≤\n2φ\n-1 -\n≤\n2φ\n-1\n-1\nZ 1\n.\n= 0 +\ny 2 ≤\n2φe- y dy = EY 2\n-1\nThus, the second moment EY 2 = 1. The variance of Y is\nVar(Y ) = EY 2 - (EY )2 = 1 - 0 = 1.\n\nIt is now easy to compute the mean and the variance of X = + πY N(, π2),\nEX = + πEY = , EX2 = E(2 + 2πY + π2Y 2) = 2 + π2 ,\nVar(X) = EX2 - (EX)2 = 2 + π2 - 2 = π2 .\nThus, parameter is a mean and parameter π2 is a variance of a normal distribution. Let us\nrecall (without giving a proof) that if we have several, say n, independent random variables\nXi, 1 ≈ i ≈ n, such that Xi N(i, π2) then their sum will also have a normal distribution\ni\nX1 + . . . + Xn N(1 + . . . + n, π1\n2 + . . . + π2 ).\nn\nNormal distribution appears in one of the most important results that one learns in probabil\nity class, namely, a Central Limit Theorem (CLT), which states the following. If X1, . . . , Xn\nis an i.i.d. sample such that π2 = Var(X) <\n, then\n→\nn\n1 X\n≤n\n(Xi - EXi) = ≤n(X - EX1) !d N(0, π2)\ni=1\nconverges in distribution to a normal distribution with zero mean and variance π2 , where\nconvergence in distribution means that for any interval [a, b],\nx\nP\n≤n(X n - EX1) inf [a, b]\n\n!\nZ\na\nb\n≤\nφπ e- 22 dx.\nThis result can be generalized for a sequence of random variables with different distributions\nand it basically says that the sum of many independent random variables/factors approxi\nmately looks like a normal distribution as long as each factor has a small impact on the total\nsum. A consequence of this phenomenon is that a normal distribution gives a good approxi\nmation for many random objects that by nature are affected by a sum of many independent\nfactors, for example, person's height or weight, fluctuations of a stock's price, etc.\nBernoulli Distribution B(p). This distribution describes a random variable that can\ntake only two possible values, i.e. X = {0, 1}. The distribution is described by a probability\nfunction\np(1) = P(X = 1) = p, p(0) = P(X = 0) = 1 - p for some p inf [0, 1].\nIt is easy to check that\nEX = p, Var(X) = p(1 - p).\nBinomial Distribution B(n, p). This distribution describes a random variable X that\nis a number of successes in n trials with probability of success p. In other words, X is a\nsum of n independent Bernoulli r.v. Therefore, X takes values in X = {0, 1, . . . , n} and the\ndistribution is given by a probability function\np(k) = P(X = k) = n p k(1 - p)n-k .\nk\n\nZ 1\nIt is easy to check that\nEX = np, Var(X) = np(1 - p).\nExponential Distribution E(). This is a continuous distribution with p.d.f.\ne-x x ∼ 0,\np(x) =\nx < 0.\nHere, > 0 is the parameter of the distribution. Again, it is a simple calculus exersice to\ncheck that\nEX =\n, Var(X) =\n.\n\nThis distribution has the following nice property. If a random variable X E() then\nprobability that X exceeds level t for some t > 0 is\nP(X ∼ t) = P(X inf [t,\n)) =\ne-xdx = e-t .\n→\nt\nGiven another s > 0, the conditional probability that X will exceed level t + s given that it\nwill exceed level t can be computed as follows:\nP(X ∼ t + s X ∼ t)\n= P(X ∼ t + s, X ∼ t) = P(X ∼ t + s)\n|\nP(X ∼ t)\nP(X ∼ t)\n= e-(t+s)/e-t = e-s = P(X ∼ s),\ni.e.\nP(X ∼ t + s|X ∼ t) = P(X ∼ s).\nIf X represent a lifetime of some object in some random conditions, then the above property\nmeans that the chance that X will \"live\" longer then t + s given that it will \"live\" longer\nthan t is the same as the chance that X will live longer than t in the first place. Or, in other\nwords, if X is \"alive\" at time t then it is \"like new\". Therefore, some natural examples that\ncan be described by exponential distribution are the lifetime of high quality products (or,\npossibly, soldiers in combat).\nPoisson Distribution (). This is a discrete distribution with\nX = {0, 1, 2, 3, . . .},\nk\np(k) = P(X = k) =\ne- for k = 0, 1, 2, , . . .\nk!\nIt is an exercise to show that\nEX = , Var(X) = .\nPoisson distribution could be used to describe the following random objects: the number\nof stars in a random area of the space; number of misprints in a typed page; number of\nwrong connections to your phone number; distribution of bacteria on some surface or weed\nin the field. All these examples share some common properties that give rise to a Poisson\ndistribution. Suppose that we count a number of random objects in a certain region T and\nthis counting process has the following properties:\n\nX\n1. Average number of objects in any region S ≥ T is proportional to the size of S,\ni.e. ECount(S) = S . Here S denotes the size of S, i.e. length, area, volume, etc.\n| |\n| |\nParameter > 0 represents the intensity of the process.\n2. Counts on disjoint regions are independent.\n3. Chance to observe more than one object in a small region is very small, i.e. P(Count(S) ∼\n2) becomes small when the size S gets small.\n| |\nWe will show that under these assumptions will imply that the number Count(T ) of objects\nin the region T has Poisson distribution ( T ) with parameter T .\n| |\n| |\nT\nT\nn\nX1\nX2\n. . . . . . .\nXn\n-Counts on small subintervals\nFigure 1.1: Poisson Distribution\nFor simplicity, let us assume that the region T is an interval [0, T ] of length T. Let us\nsplit this interval into a large number n of small equal subintervals of length T/n and denote\nby Xi the number of random objects in the ith subinterval, i = 1, . . . , n. By the first property\nabove,\nT\nEXi =\n.\nn\nOn the other hand, by definition of expectation\nEXi =\nkP(Xi = k) = 0 + P(Xi = 1) + αn,\nk0\nwhere αn = P\nk2 kP(Xi = k), and by the last property above we assume that αn becomes\nsmall with n, since the probability to observe more that two objects on the interval of size T/n\nbecomes small as n becomes large. Combining two equations above gives, P(Xi = 1) T .\nn\nAlso, since by the last property the probability that any count Xi is ∼ 2 is small, i.e.\nT\nP(at least one Xi ∼ 2) ≈ no\n! 0 as n ! →,\nn\nCount(T ) = X1 +\n+ Xn has approximately binomial distribution B(n, T /n) and we\n· · ·\n| |\ncan write\nn T k\nT n-k\nP(Count(T ) = X1 + · · · + Xn = k)\nk\nn\n1 - n\n(T )k\ne-T .\n!\nk!\nThe last limit is a simple calculus exercise and this is also a famous Poisson approximation\nof binomial distribution taught in every probability class.\n\nUniform Distribution U[0, λ]. This distribution has probability density function\np(x) =\nλ ,\nx inf [0, λ],\n0, otherwise.\nMatlab review of probability distributions.\nMatlab Help/Statistics Toolbox/Probability Distributions.\nEach distribution in Matlab has a name, for example, normal distribution has a name\n'norm'. Adding a suffix defines a function associated with this distribution. For example,\n'normrnd' generates random numbers from distribution 'norm', 'normpdf' gives p.d.f., 'norm\ncdf' gives c.d.f., 'normfit' fits the normal distribution for a given dataset (we will look at\nthis last type of functions when we discuss Maximum Likelihood Estimators). Please, look\nat each function for its syntax, input, output, etc. Type 'help normrnd' to quickly see how\nthe normal random number generator works. Also, there is a graphic user interface tools like\n'disttool' (to run it just type disttool in the main Matlab window) that allows you to play\nwith different distributions, or 'randtool' that generates and visualizes random samples from\ndifferent distributions."
    },
    {
      "category": "Lecture Notes",
      "title": "lecture2.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-443-statistics-for-applications-fall-2006/bd79a856f16cfa336fd4c0cf13ea2611_lecture2.pdf",
      "content": "Lecture 2\nMaximum Likelihood Estimators.\nMatlab example. As a motivation, let us look at one Matlab example. Let us generate\na random sample of size 100 from beta distribution Beta(5, 2). We will learn the definition\nof beta distribution later, at this point we only need to know that this isi a continuous\ndistribution on the interval [0, 1]. This can be done by typing 'X=betarnd(5,2,100,1)'. Let us\nfit different distributions by using a distribution fitting tool 'dfittool'. We try to fit normal\ndistribution and beta distribution to this sample and the results are displayed in figure 2.1.\n0.2\n0.5\n0.1\n100 samples ~ Beta(5,2)\nNormal fit\nBeta fit\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nData\nData\nFigure 2.1: Fitting a random sample of size 100 from Beta(5, 2). (a) Histogram of the data\nand p.d.f.s of fitted normal (solid line) and beta (dashed line) distributions; (b) Empirical\nc.d.f. and c.d.f.s of fitted normal and beta distributions.\nBesides the graphs, the distribution fitting tool outputs the following information:\nDistribution:\nNormal\nLog likelihood:\n55.2571\n100 samples ~ Beta(5,2)\nNormal fit\nBeta fit\n0.8\n2.5\n0.9\n0.7\n0.6\nCumulative probability\nDensity\n0.5\n0.4\n0.3\n1.5\n\nDomain:\n-Inf < y < Inf\nMean:\n0.742119\nVariance:\n0.0195845\nParameter\nEstimate\nStd. Err.\nmu\n0.742119\n0.0139945\nsigma\n0.139945\n0.00997064\nEstimated covariance of parameter estimates:\nmu\nsigma\nmu\n0.000195845\n6.01523e-020\nsigma 6.01523e-020\n9.94136e-005\nDistribution:\nBeta\nLog likelihood:\n63.8445\nDomain:\n0 < y < 1\nMean:\n0.741371\nVariance:\n0.0184152\nParameter\nEstimate\nStd. Err.\na\n6.97783\n1.08827\nb\n2.43424\n0.378351\nEstimated covariance of parameter estimates:\na\nb\na\n1.18433 0.370094\nb 0.370094 0.143149\nThe value 'Log likelihood' indicates that the tool uses the maximum likelihood estimators\nto fit the distribution, which will be the topic of the next few lectures. Notice the 'Parameter\nestimates' - given the data 'dfittool' estimates the unknown parameters of the distribution\nand then graphs the p.d.f. or c.d.f. corresponding to these parameters.\nSince the data was generated from beta distribution, it is not surprising that beta\ndistribution fit seems better than normal distribution fit, which is particularly clear from\nfigure 2.1 (b), that compares how estimated c.d.f. fits the empirical c.d.f. Empirical c.d.f. is\ndefined as\nn\n1 X\nFn(x) =\nI(Xi x)\nn i=1\nwhere I(Xn x) is the indicator that Xi is x. In other words, Fn(x) is the proportion of\nobservations below level x.\nOne can ask several questions about this example:\n1. How to estimate the unknown parameters of a distribution given the data from this\ndistribution?\n\n2. How good are these estimates, are they close to the actual 'true' parameters?\n3. Does the data come from a particular type of distribution, for example, normal or\nbeta distribution?\nIn the next few lectures we will study the first two questions and we will assume that we\nknow what type of distribution the sample comes from, so we only do not know the parameters\nof the distribution. In the context of the above example, we would be told that the data\ncomes from beta distribution, but the parameters (5, 2) would be unknown. Of course, in\ngeneral we might not know what kind of distribution the data comes from - we will study\nthis type of questions later when we look at the so called goodness-of-fit hypotheses tests.\nIn particular, we will see graphs like 2.1 (b) again when we study the Kolmogorov-Smirnov\ngoodness-of-fit test.\nExample. We consider a dataset of various body measurements from [1] (dataset can be\ndowloaded from journal's website), including weight, height, waist girth, abdomen girth, etc.\nFirst, we use Matlab fitting tool to fit weight and waist girth of men and women (separately)\nwith lognormal distribution, see figure 2.2 (a) and (b). Wikipedia article about normal dis\ntribution gives a reference to a 1932 book \"Problems of Relative Growth\" by Julian Huxley\nfor the explanation why the sizes of full-grown animals are approximately log-normal. One\nshort explanation is consistency between linear and volume dimensions - if linear dimensions\nare lognormal and volume dimensions are proportional to cube of linear dimensions then\nthey also are lognormal. Assumption that sizes are normal would violate this consistency,\nsince the cube of normal is not normal. We observe, hovewer, that the fit of women's waist\nwith lognormal is not very accurate. Later in the class we will learn several statistical tests\nto decide if the data comes from a certain distribution or a family of distributions, but here\nis a preview of what's to come. Chi-squared goodness-of-fit test rejects the hypothesis that\nthe distribution of logarithms of women's waists is normal:\n[h,p,stats]=chi2gof(log_women_waist)\nh = 1, p = 5.2297e-004\nstats = chi2stat: 22.0027\ndf: 5\nedges: [1x9 double]\nO: [21 44 67 60 28 18 12 10]\nE: [1x8 double]\nand so does Lilliefor's test (adjusted Kolmogorov-Smirnov test):\n[h,p,stats]=lillietest(log_women_waist)\nh = 1, p = 0, stats = 0.0841.\nThe same tests accept the hypotheses that other variables have lognormal distribution. Au\nthor's in [1] suggest that we can fit women's waist with Gamma distribution. Since Gamma\n\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nwomen's weight\nlognormal fit women\nmen's weight\nlognormal fit men\n0.9\n0.9\n0.8\n0.8\nCumulative probability\nwomen's waist\nlognormal fit women\nmen's waist\nlognormal fit men\nCumulative probability\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\nData\nData\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nCumulative probability\nwomen's waist (shifted)\nGamma fit\nnormal fit\nData\nFigure 2.2: Fitting weight (upper left) and waist girth (upper right) with lognormal distribution.\nLower left: fitting women's waist with shifted Gamma and normal distributions.\ndoes not have a translation (shift) parameter, when we fit Gamma distribution we can either\nadd to it a shift parameter or instead shift all data to start at zero. In figure 2.2 (c) we fit\nGamma and, for the sake of illustration, normal distribution, to women's waist sample. As\nwe can see, Gamma fits the data better than lognormal and much better than normal. To\nfind the parameters of fitted Gamma distribution we use Matlab 'gamfit' function:\nparam=gamfit(women_waist_shift)\nparam = 2.8700\n4.4960.\nChi-squared goodness-of-fit test for a specific (fitted) Gamma distribution:\n[h,p,stats]=chi2gof(women_waist_shift,'cdf',@(z)gamcdf(z,param(1),param(2)))\n\nX\nh = 0, p = 0.9289, stats = chi2stat: 2.4763, df: 7\naccepts the hypothesis that the sample has Gamma distribution (2.87, 4.496). This test is\nnot 'accurate' in some sense, which will be explained later. One can also check that Gamma\ndistribution fits well other variables - men's waist girth, weight of men and weight of women.\nLet us consider a family of distributions P indexed by a parameter (which could be a\nvector of parameters) φ that belongs to a set . For example, we could consider a family of\nnormal distributions N(, α2) in which case the parameter would be φ = (, α2) - the mean\nand variance of the distribution. Let f(X φ) be either a probability function (in case of\n|\ndiscrete distribution) or a probability density function (continuous case) of the distribution\nP. Suppose we are given an i.i.d. sample X1, . . . , Xn with unknown distribution P from this\nfamily, i.e. parameter φ is unknown. A likelihood function is defined by\n'(φ) = f(X1|φ) × . . . × f(Xn|φ).\nWe think of the sample X1, . . . , Xn as given numbers and we think of ' as a function of\nthe parameter φ only. The likelihood function has a clear interpretation. For example, if our\ndistributions are discrete then the probability function\nf(x φ) = P(X = x)\n|\nis the probability to observe a point x and the likelihood function\n'(φ) = f(X1|φ) × . . . × f(Xn|φ) = P(X1) × . . . × P(Xn) = P(X1, . . . , Xn)\nis the probability to observe the sample X1, . . . , Xn when the parameters of the distribution\nare equal to φ. In the continuous case the likelihood function '(φ) is the probability density\nfunction of the vector (X1, . . . , Xn).\nDefinition: (Maximum Likelihood Estimators.) Suppose that there exists a parameter\nφˆ that maximizes the likelihood function '(φ) on the set of possible parameters , i.e.\n'(φˆ) = max '(φ).\nThen φˆ is called the Maximum Likelihood Estimator (MLE).\nWhen finding the MLE it sometimes easier to maximize the log-likelihood function since\n'(φ) ! maximize ≥ log '(φ) ! maximize\nmaximizing ' is equivalent to maximizing log '. Log-likelihood function can be written as\nn\nlog '(φ) =\nlog f(Xi φ).\n|\ni=1\nLet us give several examples of computing the MLE.\n\nExample 1. Bernoulli distribution B(p).\nX = {0, 1}, P(X = 1) = p, P(X = 0) = 1 - p, p inf [0, 1].\nProbability function in this case is given by\nf(x p) =\np,\nx = 1 = p x(1 - p)1-x .\n|\n1 - p, x = 0\nLikelihood function is\n'(p)\n= f(X1 p)f(X2 p) . . . f(Xn p)\n|\n|\n|\n# of 1's(1 - p)# of 0's\nX1+...+Xn (1 - p)n-(X1+...+Xn)\n= p\n= p\nand the log-likelihood function is\nlog '(p) = (X1 + . . . + Xn) log p + (n - (X1 + . . . + Xn)) log(1 - p).\nTo maximize this over p inf [0, 1] let us find the critical point (log '(p))0 = 0,\n(X1 + . . . + Xn) p - (n - (X1 + . . . + Xn))1 - p = 0.\nSolving this for p gives,\nX1 + . . . + Xn\n\np =\n= X\nn\n\nand, therefore, the proportion of successes pˆ = X in the sample is the MLEstimator of the\nunknown true probability of success, which is a very natural and intuitive estimator. For\nexample, by law of large numbers, we know that\n\n= p\nX ! EX1\nin probability (we will recall this definition in the next lecture), which means that our\nestimate will approximate the unknown parameter p well when we get more and more data.\nRemark. In each example, once we compute the estimate of parameters, we can try to\nprove directly, using the explicit form of the estimate, that it approximates well the unkown\nparameters, as we did in Example 1. However, in the next lecture we will describe in a general\nsetting that MLE has 'good properties'.\nExample 2. Normal distribution N(, α2). The p.d.f. of normal distribution is\n(X-)2\nf(X|(, α2)) = ≤\n2α e-\n.\nand, therefore, likelihood function is\nn\n'(, α2) =\nY\ne- (Xi-)2\n.\n≤\n2α\ni=1\n\nX\n\nX\nX\nX\nX\nX\n\nY\nand log-likelihood function is\nn\n(Xi - )2\n2α2\nlog '(, α2) =\nlog ≤\n2 - log α -\ni=1\nn\n= n log ≤\n- n log α -\n(Xi - )2 .\n2α2\nWe want to maximize the log-likelihood with respect to -→ < < → and α2 > 0. First,\nobviously, for any α we need to minimize P(Xi - )2 over . The critical point condition is\nn\nn\nX\ni=1\nd\n(Xi - )2 = -2\n(Xi - ) = 0\nd i=1\ni=1\n\nand solving this for we get that ˆ = X. We can plug this estimate in the log-likelihood\nand it remains to maximize\nn\nX )2\nn log ≤\n- n log α -\n(Xi -\n2α2\ni=1\nover α. The critical point condition reads,\nn\n(Xi - X )2 = 0\n-α + α3\nand solving this for α we obtain that the MLE of α2 is\nn\nαˆ2\nX )2\n=\n(Xi -\nn\n.\ni=1\nThe normal distribution fit in figure 2.1 corresponds to these parameters (ˆ, αˆ2).\nExercise. Generate a normal sample in Matlab and fit it with a normal distribution\nusing 'dfittool'. Then plot a p.d.f. or c.d.f. corresponding to MLE above and compare this\nwith 'dfittool'.\nLet us give one more example of MLE.\nUniform distribution U[0, φ] on the interval [0, φ]. This distribution has p.d.f.\n1 , 0 x φ,\notherwise.\nf(x φ) =\n|\n0,\nThe likelihood function\nn\n'(φ) =\ni=1\nf(Xi|φ) =\n=\nφn I(X1, . . . , Xn inf [0, φ])\nφn I(max(X1, . . . , Xn) φ).\n\nHere the indicator function I(A) equals to 1 if event A happens and 0 otherwise. What the\nindicator above means is that the likelihood will be equal to 0 if at least one of the factors is\n0 and this will happen if at least one observation Xi will fall outside of the 'allowed' interval\n[0, φ]. Another way to say it is that the maximum among observations will exceed φ, i.e.\n'(φ) = 0 if φ < max(X1, . . . , Xn),\nand\n'(φ) =\nif φ max(X1, . . . , Xn).\nφn\nTherefore, looking at the figure 2.3 we see that φˆ = max(X1, . . . , Xn) is the MLE.\n'(φ)\n1.8\n1.6\n1.4\n1.2\n0.8\n0.6\n0.4\n0.2\n0.5\n1.5\n2.5\nmax(X1, . . . , Xn)\nφ\nFigure 2.3: MLE for the uniform distribution.\nSometimes it is not so easy to find the maximum of the likelihood function as in the\nexamples above and one might have to do it numerically. Also, MLE does not always exist.\nHere is an example: let us consider uniform distribution U[0, φ) and define the density by\n1 , 0 x < φ,\nf(x|φ) =\n\n,\notherwise.\nThe difference is that we 'excluded' the point φ by setting f(φ φ) = 0. Then the likelihood\n|\nfunction is\nn\nY\n'(φ) =\nf(Xi φ) =\nI(max(X1, . . . , Xn) < φ)\nφn\n|\ni=1\n\nand the maximum at the point φˆ = max(X1, . . . , Xn) is not achieved. Of course, this is an\nartificial example that shows that sometimes one needs to be careful.\nReferences:\n[1] Grete Heinz, Louis J. Peterson, Roger W. Johnson, Carter J. Kerk, (2003) \"Exploring\nRelationships in Body Dimensions\". Journal of Statistics Education, Volume 11, Number 2."
    },
    {
      "category": "Lecture Notes",
      "title": "lecture3.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-443-statistics-for-applications-fall-2006/03b407da8a94b3fe22d987453807ca46_lecture3.pdf",
      "content": "Lecture 3\nProperties of MLE: consistency,\nasymptotic normality.\nFisher information.\nIn this section we will try to understand why MLEs are 'good'.\nLet us recall two facts from probability that we be used often throughout this course.\n- Law of Large Numbers (LLN):\nIf the distribution of the i.i.d. sample X1, . . . , Xn is such that X1 has finite expectation,\ni.e. EX1 <\n, then the sample average\n|\n|\n→\n\nX1 + . . . + Xn\nXn =\n! EX1\nn\nconverges to its expectation in probability , which means that for any arbitrarily small\nα > 0,\n\nP(|Xn - EX1| > θ) ! 0 as n ! →.\nNote. Whenever we will use the LLN below we will simply say that the average\nconverges to its expectation and will not mention in what sense. More mathematically\ninclined students are welcome to carry out these steps more rigorously, especially when\nwe use LLN in combination with the Central Limit Theorem.\n- Central Limit Theorem (CLT):\nIf the distribution of the i.i.d. sample X1, . . . , Xn is such that X1 has finite expectation\nand variance, i.e. |EX1| < → and π2 = Var(X) < →, then\n≥n(X n - EX1) !d N(0, π2)\nconverges in distribution to normal distribution with zero mean and variance π2 , which\nmeans that for any interval [a, b],\nx\nP\n≥n(X n - EX1) inf [a, b]\n\n!\nZ\na\nb\n≥\n∂π e- 22 dx.\n\nIn other words, the random variable ≥n(X n - EX1) will behave like a random variable\nfrom normal distribution when n gets large.\nExercise. Illustrate CLT by generating 100 Bernoulli random varibles B(p) (or one\nBinomial r.v. B(100, p)) and then computing ≥n(X n - EX1). Repeat this many times\nand use 'dfittool' to see that this random quantity will be well approximated by normal\ndistribution.\nWe will prove that MLE satisfies (usually) the following two properties called consistency\nand asymptotic normality.\n1. Consistency. We say that an estimate φˆ is consistent if φˆ ! φ0 in probability as\nn ! →, where φ0 is the 'true' unknown parameter of the distribution of the sample.\n2. Asymptotic Normality. We say that φˆ is asymptotically normal if\n≥n(φˆ- φ0) !d N(0, π\n0 )\nwhere π\n0 is called the asymptotic variance of the estimate φˆ. Asymptotic normality\nsays that the estimator not only converges to the unknown parameter, but it converges\nfast enough, at a rate 1/≥n.\nConsistency of MLE.\nTo make our discussion as simple as possible, let us assume that a likelihood function\nis smooth and behaves in a nice way like shown in figure 3.1, i.e. its maximum is achieved\nat a unique point φ.\nˆ\n0.8\n0.6\n0.4\n0.2\n-0.2\n-0.4\n-0.6\n-0.8\n-1\n'(φ)\n'(φ)\n0.5\n1.5\n2.5\n3.5\nφˆ\nφ\nFigure 3.1: Maximum Likelihood Estimator (MLE)\nSuppose that the data X1, . . . , Xn is generated from a distribution with unknown pa\nrameter φ0 and φˆ is a MLE. Why φˆ converges to the unknown parameter φ0? This is not\nimmediately obvious and in this section we will give a sketch of why this happens.\n\nZ\nZ\nZ\nFirst of all, MLE φˆ is the maximizer of\nn\n1 X\nLn(φ) = n\nlog f(Xi|φ)\ni=1\nwhich is a log-likelihood function normalized by n\n1 (of course, this does not affect maxi\nmization). Notice that function Ln(φ) depends on data. Let us consider a function l(X φ) =\n|\nlog f(X φ) and define\n|\nL(φ) = E0 l(X φ),\n|\nwhere E0 denotes the expectation with respect to the true uknown parameter φ0 of the\nsample X1, . . . , Xn. If we deal with continuous distributions then\nL(φ) =\n(log f(x φ))f(x φ0)dx.\n|\n|\nBy law of large numbers, for any φ,\nLn(φ) ! E0 l(X|φ) = L(φ).\nNote that L(φ) does not depend on the sample, it only depends on φ. We will need the\nfollowing\nLemma. We have that for any φ,\nL(φ) ≡ L(φ0).\nMoreover, the inequality is strict, L(φ) < L(φ0), unless\nP0 (f(X φ) = f(X φ0)) = 1.\n|\n|\nwhich means that P = P0 .\nProof. Let us consider the difference\nL(φ) - L(φ0) = E0 (log f(X|φ) - log f(X|φ0)) = E0 log f\nf\n(\n(\nX\nX|\nφ\nφ\n)\n) .\n|\nSince log t ≡ t - 1, we can write\nlog\n=\nf(x φ0)dx\nE0\nf\nf\n(\n(\nX\nX\n|\n|\nφ\nφ\n)\n) ≡ E0\n\nf\nf\n(\n(\nX\nX\n|\n|\nφ\nφ\n)\n) - 1\n\nZ\nf\nf\n(\n(\nx\nx\n|\n|\nφ\nφ\n)\n) - 1\n\n|\n=\nf(x|φ)dx -\nf(x|φ0)dx = 1 - 1 = 0.\nBoth integrals are equal to 1 because we are integrating the probability density functions.\nThis proves that L(φ) - L(φ0) ≡ 0. The second statement of Lemma is also clear.\n\nWe will use this Lemma to sketch the consistency of the MLE.\nTheorem: Under some regularity conditions on the family of distributions, MLE φˆ is\nconsistent, i.e. φˆ ! φ0 as n ! →.\nThe statement of this Theorem is not very precise but but rather than proving a rigorous\nmathematical statement our goal here is to illustrate the main idea. Mathematically inclined\nstudents are welcome to come up with some precise statement.\nφ\nˆφ\nφ0\nLn(φ)\nL(φ)\nFigure 3.2: Illustration to Theorem.\nProof. We have the following facts:\n1. φˆ is the maximizer of Ln(φ) (by definition).\n2. φ0 is the maximizer of L(φ) (by Lemma).\n3. 8φ we have Ln(φ) ! L(φ) by LLN.\nThis situation is illustrated in figure 3.2. Therefore, since two functions Ln and L are\ngetting closer, the points of maximum should also get closer which exactly means that\nφˆ ! φ0.\nAsymptotic normality of MLE. Fisher information.\nWe want to show the asymptotic normality of MLE, i.e. to show that\n≥n(φˆ- φ0) !d N(0, π2\n) for some π2\nMLE\nMLE\nand compute π2\nMLE . This asymptotic variance in some sense measures the quality of MLE.\nFirst, we need to introduce the notion called Fisher Information.\nLet us recall that above we defined the function l(X φ) = log f(X φ). To simplify the\n|\n|\nnotations we will denote by l0(X φ), l00(X φ), etc. the derivatives of l(X φ) with respect to\n|\n|\n|\nφ.\nDefinition. (Fisher information.) Fisher information of a random variable X with\ndistribution P0 from the family {P : φ inf } is defined by\n@\n\nI(φ0) = E0 (l0(X|φ0))2 E0 @φ log f(X|φ)\n=0\n.\n\nZ\nRemark. Let us give a very informal interpretation of Fisher information. The derivative\nl0(X|φ0) = (log f(X|φ0))0 = f\nf\n(\n(\nX\nX\n|\n|\nφ\nφ\n)\n)\ncan be interpreted as a measure of how quickly the distribution density or p.f. will change\nwhen we slightly change the parameter φ near φ0. When we square this and take expectation,\ni.e. average over X, we get an averaged version of this measure. So if Fisher information is\nlarge, this means that the distribution will change quickly when we move the parameter, so\nthe distribution with parameter φ0 is 'quite different' and 'can be well distinguished' from\nthe distributions with parameters not so close to φ0. This means that we should be able to\nestimate φ0 well based on the data. On the other hand, if Fisher information is small, this\nmeans that the distribution is 'very similar' to distributions with parameter not so close\nto φ0 and, thus, more difficult to distinguish, so our estimation will be worse. We will see\nprecisely this behavior in Theorem below.\nNext lemma gives another often convenient way to compute Fisher information.\nLemma. We have,\n@2\nE0 l00(X|φ0) E0 @φ2 log f(X|φ0) = -I(φ0).\nProof. First of all, we have\nl0(X|φ) = (log f(X|φ))0 = f\nf\n(\n(\nX\nX|\nφ\nφ\n)\n)\n|\nand\n(log f(X|φ))00 = f\nf\n(\n(\nX\nX\n|\n|\nφ\nφ\n)\n) - (f\nf\n(\n(\nX\nX\n|φ\n|φ\n))\n)\n.\nAlso, since p.d.f. integrates to 1,\nf(x φ)dx = 1,\n|\nif we take derivatives of this equation with respect to φ (and interchange derivative and\nintegral, which can usually be done) we will get,\nZ @\nZ @2\nZ\n@φ f(x|φ)dx = 0 and\n@φ2 f(x|φ)dx =\nf 00(x|φ)dx = 0.\nTo finish the proof we write the following computation\n@2\nZ\nE0 l00(X φ0)\n= E0\nlog f(X φ0) =\n(log f(x φ0))00f(x φ0)dx\n|\n@φ2\n|\n|\n|\nf (x\n=\nZ\nZ\nf\nf\n(\n(\nx\nx\n|\n|\nφ\nφ\n)\n) - f\n(x|\n|\nφ\nφ\n)\n)\nf(x|φ0)dx\n=\nf 00(x|φ0)dx - E0 (l0(X|φ0))2 = 0 - I(φ0 = -I(φ0).\n\nWe are now ready to prove the main result of this section.\nTheorem. (Asymptotic normality of MLE.) We have,\n≥n(φˆ- φ0) ! N\n\n0,\n.\nI(φ0)\nAs we can see, the asymptotic variance/dispersion of the estimate around true parameter\nwill be smaller when Fisher information is larger.\nProof. Since MLE φˆ is maximizer of Ln(φ) = n\n1 P\ni\nn\n=1 log f(Xi|φ), we have\nL0 (φˆ) = 0.\nn\nLet us use the Mean Value Theorem\nf(a) - f(b)\na - b\n= f 0(c) or f(a) = f(b) + f 0(c)(a - b) for c inf [a, b]\nwith f(φ) = L0\nn(φ), a = φˆ and b = φ0. Then we can write,\n0 = L0 (φˆ) = L0\nn(φ0) + L00(φˆ1)(φˆ- φ0)\nn\nn\nfor some ˆ\nφ, φ0]. From here we get that\nφ1 inf [ˆ\nˆ\nn\nn\nφ - φ0 = - L0\n(\n(φ\nˆ\n0) and ≥n(φˆ- φ0) = -\n≥nL\n(\nˆ\n(φ0) .\n(3.0.1)\nL00n φ1)\nLn00 φ1)\nSince by Lemma in the previous section we know that φ0 is the maximizer of L(φ), we have\nL0(φ0) = E0 l0(X φ0) = 0.\n(3.0.2)\n|\nTherefore, the numerator in (3.0.1)\nn\n≥nLn\n0 (φ0)\n= ≥n\n\nn\n1 X\nl0(Xi|φ0) - 0\n\n(3.0.3)\ni=1\nn\n1 X\n\n= ≥n n i=1\nl0(Xi|φ0) - E0 l0(X1|φ0) ! N 0, Var0 (l0(X1|φ0))\nconverges in distribution by Central Limit Theorem.\nNext, let us consider the denominator in (3.0.1). First of all, we have that for all φ,\n1 X\nL00\nn(φ) =\nl00(Xi|φ) ! E0 l00(X1|φ) by LLN.\n(3.0.4)\nn\nAlso, since φˆ1 inf [ˆ\nφ ! φ0, we have ˆ\nφ, φ0] and by consistency result of previous section, ˆ\nφ1 ! φ0.\nUsing this together with (10.0.3) we get\nL00\nn(φˆ1) ! E0 l00(X1|φ0) = -I(φ0) by Lemma above.\n\nCombining this with (3.0.3) we get\n≥nL0\nn(φ0)\nd N\n\n0, Var0 (l0(X1|φ0))\n.\n-\nL00n(φˆ1)\n!\n(I(φ0))2\nFinally, the variance,\nVar0 (l0(X1|φ0)) = E0 (l0(X|φ0))2 - (E0 l0(x|φ0))2 = I(φ0) - 0\nwhere in the last equality we used the definition of Fisher information and (3.0.2).\nLet us compute Fisher information for some particular distributions.\nExample 1. The family of Bernoulli distributions B(p) has p.f.\nf(x|p) = p x(1 - p)1-x\nand taking the logarithm\nlog f(x|p) = x log p + (1 - x) log(1 - p).\nThe second derivative with respect to parameter p is\n@ log f(x p) = x\n1 - x, @2\nlog f(x p) = - x\n1 - x .\n@p\n|\np - 1 - p\n@p2\n|\np - (1 - p)2\nThen the Fisher information can be computed as\nI(p) = -E @2\nlog f(X p) = EX + 1 - EX = p +\n1 - p =\n.\n@p2\n|\np2\n(1 - p)2\np2\n(1 - p)2\np(1 - p)\n\nThe MLE of p is ˆp = X and the asymptotic normality result states that\n≥n(ˆp - p0) ! N(0, p0(1 - p0))\nwhich, of course, also follows directly from the CLT.\nExample. The family of exponential distributions E() has p.d.f.\nf(x ) =\ne-x ,\nx ∀ 0\n|\n0,\nx < 0\nand, therefore,\n@2\nlog f(x|) = log - x ≤ @2 log f(x|) = -2 .\nThis does not depend on X and we get\n@2\nI() = -E\nlog f(X|) =\n.\n@2\nTherefore, the MLE ˆ = 1/X is asymptotically normal and\n≥n(ˆ - 0) ! N(0, 2)."
    },
    {
      "category": "Lecture Notes",
      "title": "lecture4.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-443-statistics-for-applications-fall-2006/da928274a46632ce0d35d2012a1c8e74_lecture4.pdf",
      "content": "Lecture 4\nMultivariate normal distribution and\nmultivariate CLT.\nWe start with several simple observations. If X = (x1, . . . , xk)T is a k × 1 random vector\nthen its expectation is\nEX = (Ex1, . . . , Exk)T\nand its covariance matrix is\nCov(X) = E(X - EX)(X - EX)T .\nNotice that a covariance matrix is always symmetric\nCov(X)T = Cov(X)\nand nonnegative definite, i.e. for any k × 1 vector a,\na T Cov(X)a = Ea T (X - EX)(X - EX)T a T = E|a T (X - EX)|2 0.\nWe will often use that for any vector X its squared length can be written as |X|2 = XT X. If\nwe multiply a random k × 1 vector X by a n × k matrix A then the covariance of Y = AX\nis a n × n matrix\nCov(Y ) = EA(X - EX)(X - EX)T AT = ACov(X)AT .\nMultivariate normal distribution. Let us consider a k × 1 vector g = (g1, . . . , gk)T\nof i.i.d. standard normal random variables. The covariance of g is, obviously, a k × k identity\nmatrix, Cov(g) = I. Given a n × k matrix A, the covariance of Ag is a n × n matrix\n:= Cov(Ag) = AIAT = AAT .\nDefinition. The distribution of a vector Ag is called a (multivariate) normal distribution\nwith covariance and is denoted N(0, ).\nOne can also shift this disrtibution, the distribution of Ag + a is called a normal distri\nbution with mean a and covariance and is denoted N(a, ). There is one potential problem\n\np\np\nwith the above definition - we assume that the distribution depends only on covariance ma\ntrix and does not depend on the construction, i.e. the choice of g and a matrix A. For\nexample, if we take a m × 1 vector g→ of i.i.d. standard normal random variables and a n × m\nmatrix B then the covariance of Bg→ is a n × n matrix\nCov(Bg→) = BBT .\nIt is possible that = AAT = BBT so both constructions should give a normal distribution\nN(0, ). This is, indeed, true - the distribution of Ag and Bg→ is the same, so the definition\nof normal distribution N(0, ) does not depend on the construction. It is not very difficult\nto prove that Ag and Bg→ have the same distribution, but we will only show the simplest\ncase.\nInvertible case. Suppose that A and B are both square n × n invertible matrices. In\nthis case, vectors Ag and Bg→ have density which we will now compute. Since the density of\ng is\nY 1\n\n1 n\n\n→\n2 exp -2 xi\n= →\nexp -2|x|\n,\nin\nfor any set\ninf Rn we can write\nZ\n1 n\n\nP(Ag inf\n) = P(g inf A-1\n) =\nA-1\n\n→\nexp -2|x| 2 dx.\nLet us now make the change of variables y = Ax or x = A-1y. Then\nZ 1 n\n\nP(Ag inf\n) =\n\n→\nexp -2|A-1 y|2\ndet(A) dy.\n|\n|\nBut since\ndet() = det(AAT ) = det(A) det(AT ) = det(A)2\nwe have det(A) =\ndet(). Also\n|\n|\n|A-1 y|2 = (A-1 y)T (A-1 y) = y T (AT )-1A-1 y = y T (AAT )-1 y = y T -1 y.\nTherefore, we get\nZ 1 n\n\nP(Ag inf\n) =\n\n→\ndet()\nexp -2y T -1 y dy.\nThis means that a vector Ag has the density\n\np\nexp\ndet()\n-2y T -1 y\nwhich depends only on and not on A. This means that Ag and Bg→ have the same distri\nbutions.\n\nIt is not difficult to show that in a general case the distribution of Ag depends only\non the covariance , but we will omit this here. Many times in these lectures whenever we\nwant to represent a normal distribution N(0, ) constructively, we will find a matrix A (not\nnecessarily square) such that = AAT and use the fact that a vector Ag for i.i.d. vector g\nhas normal distribution N(0, ). One way to find such A is to take a matrix square-root of\n. Since is a symmetric nonnegative definite matrix, its eigenvalue decomposition is\n= QDQT\nfor an orthogonal matrix Q and a diagonal matrix D with eigenvalues 1, . . . , n of on\nthe diagonal. In Matlab, '[Q,D]=eig(Sigma);' will produce this decomposition. Then if D1/2\nrepresents a diagonal matrix with i\n1/2 on the diagonal then one can take\nA = QD1/2 or A = QD1/2QT .\nIt is easy to check that in both cases AAT = QDQT = . In Matlab QD1/2QT is given by\n'sqrtm(Sigma)'. Let us take, for example, a vector X = QD1/2g for i.i.d. standard normal\nvector g which by definition has normal distribution N(0, ). If q1, . . . , qn are the column\nvectors of Q then\nX = QD1/2 g = (1\n/2 g1)q1 + . . . + (1\nn\n/2 gn)qn.\nTherefore, in the orthonormal coordinate basis q1, . . . , qn a random vector X has coordi\nnates 1\n1/2 g1, . . . , n\n1/2 gn. These coordinates are independent with normal distributions with\nvariances 1, . . . , n correspondingly. When det = 0, i.e. is not invertible, some of its\neigenvalues will be zero, say, k+1 = . . . = n = 0. Then the random vector will be concen\ntrated on the subspace spanned by vectors q1, . . . , qk but it will not have density on the entire\nspace Rn . On the subspace spanned by vectors q1, . . . , qk a vector X will have a density\nk\nY\n\nxi\nf(x1, . . . , xk) =\nexp\n.\n→2i\n-2i\ni=1\nLinear transformation of a normal random vector.\nSuppose that Y is a n × 1 random vector with normal distribution N(0, ). Then given\na m × n matrix M, a m × 1 vector MY will also have normal distribution N(0, MM T ). To\nshow this, find any matrix A and i.i.d. standard normal vector g such that Ag has normal\ndistribution N(0, ). Then, by definition, M(Ag) = (MA)g also has normal distribution\nwith covariance\n(MA)(MA)T = MAAT M T = MM T .\nOrthogonal transformation of an i.i.d. standard normal sample.\nThroughout the lectures we will often use the following simple fact. Consider a vector\nX = (X1, . . . , Xn)T of i.i.d. random variables with standard normal distribution N(0, 1). If\nV is an orthogonal n × n matrix then the vector Y := V X also consists of i.i.d. random\n\nZ\n\nvariables Y1, . . . , Yn with standard normal distribution. A matrix V is orthogonal when one\nof the following equivalent properties hold:\n1. V -1 = V T .\n2. The rows of V form an orthonormal basis in Rn .\n3. The columns of V form an orthonormal basis in Rn .\n4. For any x inf Rn we have |V x| = |x|, i.e. V preserves the lengths of vectors.\nBelow we will use that det(V ) = 1. Basically, orthogonal transformations represent linear\n|\n|\ntransformations that preserve distances between points, such as rotations and reflections.\nThe joint p.d.f of a vector X is given by\nn\ni\nf(x) = f(x1, . . . , xn) =\nY\n→1\ne-x2/2 =\n(\n→\n)n e-|x|2/2 ,\ni=1\nwhere |x|2 = x1\n2 + . . . + x2\nn. To find the p.d.f. of a vector Y = V X, which is an linear\ntransformation of X, we can use the change of density formula from probability or the\nchange of variables formula from calculus as follows. For any set\n≥ Rn ,\nP(Y inf\n) = P(V X inf\n) = P(X inf V -1\n)\nZ\nZ f(V -1y)\n=\nf(x)dx =\ndy.\nV -1\n\ndet(V )\n|\n|\nwhere we made the change of variables y = V x. We know that det(V ) = 1 and, since\n|V -1y| = |y|, we have\n|\n|\nf(V -1 y) = (\n→\n)n e-|V -1y|2/2 = (\n→\n)n e-|y|2/2 = f(y).\nTherefore, we finally get that\nP(Y inf\n) =\nf(y)dy\nwhich proves that a vector Y has the same joint p.d.f. as X.\nMultivariate CLT.\nWe will state a multivariate Central Limit Theorem without a proof. Suppose that\nX = (x1, . . . , xk)T is a random vector with covariance . We assumed that Exi\n2 < 1. If\nX1, X2, . . . is a sequence of i.i.d. copies of X then\nn\nSn := →1\nn\nX\n(Xi - EXi) ! d N(0, ),\ni=1\nwhere convergence in distribution !d means that for any set\ninf Rk ,\nlim P(Sn inf\n) = P(Y inf\n)\nn!1\nfor a random vector Y with normal distribution N(0, )."
    }
  ]
}