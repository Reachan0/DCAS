{
  "course_name": "Econometrics",
  "course_description": "No description found.",
  "topics": [
    "Mathematics",
    "Probability and Statistics",
    "Social Science",
    "Economics",
    "Econometrics",
    "Mathematics",
    "Probability and Statistics",
    "Social Science",
    "Economics",
    "Econometrics"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nRecitations: 1 session / week, 1 hour / session\n\nDescription\n\nThis course covers the statistical tools needed to understand empirical economic research and to plan and execute independent research projects. Topics include statistical inference, regression, generalized least squares, instrumental variables, simultaneous equations models, and evaluation of government policies and programs.\n\nPrerequisites\n\nThe prerequisite courses include Introduction to Statistical Methods in Economics (14.30) or equivalent. Students should be familiar with basic concepts in probability theory and statistical inference. The course includes a brief statistics review.\n\nCourse Requirements\n\nEach week there are two lectures and a weekly recitation.\n\nIn addition to the readings, there are 6 graded problem sets and ungraded review problem sets at the beginning and end of the course. The problem sets have both analytical and computer-exercise components. The statistical analysis will be done using Stata or SAS on PCs or MIT workstations. Help for new Stata users will be given in recitation.\n\nTexts\n\nWooldridge, Jeffrey M.\nIntroductory Econometrics: A Modern Approach\n. 3rd ed. Mason, OH: Thomson/South-Western, 2006. ISBN: 9780324289787.\n\nGoldberger, Arthur S.\nA Course in Econometrics\n. Cambridge, MA: Harvard University Press, 1991. ISBN: 9780674175440.\n\nDeGroot, Morris H., and Mark J. Schervish.\nProbability and Statistics\n. 3rd ed. Boston, MA: Addison-Wesley, 2001. ISBN: 9780201524888.\n\nWooldridge is the main text. The material in Goldberger is more advanced and optional. DeGroot and Schervish is a recommended text for statistics review. Each unit also has\nreadings\nfrom published journal articles.\n\nGrading\n\nACTIVITIES\n\nPERCENTAGES\n\nProblem sets (5% each)\n\n30%\n\nMidterm exam\n\n30%\n\nFinal exam\n\n40%\n\nEach review problem set is worth 1 bonus percentage point.\n\nGraded problem sets are mandatory and solutions should be submitted on time to receive credit. Stata or SAS logs should be submitted with solution sets. A grade of 50% or better on at least 5 problem sets is required in order to be eligible to take the final. Consult with classmates on problem sets if you get stuck, but written solution sets should be your own work.\n\nCourse Outline\n\nPart I\n\nA. Review of probability and statistics\n\n1. Probability and distribution\n\n2. Expectation and moments\n\nB. Review of statistical inference\n\n3. Sampling distributions and inference\n\n4. The Central Limit theorem (Asymptotic distribution of the sample mean)\n\n5. Confidence intervals\n\nC. Regression basics\n\n6. Conditional expectation functions, bivariate regression\n\n7. Sampling distribution of regression estimates; Gauss-Markov theorem\n\n8. How classical assumptions are used; asymptotic distribution of the sample slope\n\n9. Residuals, fitted values, and goodness of fit\n\nPart II\n\nD. Multivariate regression\n\n10. Regression, causality, and control; anatomy of multivariate regression coefficients\n\n11. Omitted variables formula, short vs. long regressions\n\n12a. Dummy variables and interactions; testing linear restrictions using F-tests\n\n12b. Regression analysis of natural experiments, differences-in-differences\n\nE. Inference problems - heteroscedasticity and autocorrelation\n\n13a. Heteroscedasticity, consequences of; weighted least squares; the linear probability model\n\n13b. Serial correlation in time series, consequences of; quasi-differencing; common-factor restriction; Durbin-Watson test for serial correlation\n\nF. Instrumental variables, simultaneous equations models, measurement error\n\n14a. Using IV to solve omitted-variables problems\n\n14b. Measurement error (Time-permitting)\n\n14c. Regression-discontinuity designs (Time-permitting)\n\nG. Simultaneous equation models\n\n15. Simultaneous equations models I\n\na. The use of structural models\n\nb. Simultaneous equations bias\n\nc. The identification problem\n\nd. The structure and the reduced form\n\ne. Indirect least squares\n\n16. Simultaneous equations models II\n\na. IV for the SEM\n\nb. Two-stage least squares\n\nc. Sampling variance of 2SLS estimates",
  "files": [
    {
      "category": "Resource",
      "title": "ps0_07.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-32-econometrics-spring-2007/1796f0de043c6b82905a83f4147c06d6_ps0_07.pdf",
      "content": "Cite as: Joshua Angrist, James Berry, and Emre Kocatulum, course materials for 14.32 Econometrics, Spring 2007. MIT\nOpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nEconometrics\nMIT (14.32)\n\nSpring 2007\n\nReview Problem Set\n\nThis problem set is ungraded but may be handed in for 1 bonus point.\n\nA. Using data for the most recent year available from the Bureau of Labor Statistics, compute or do the\nfollowing:\n\n1. The joint distribution of race and sex for people in the labor force and the marginal\n\ndistributions of race and sex.\n\n2. The distribution of employment status for all workers (men and women) conditional on race.\n3. Graph the conditional expectation function of earnings given age group, by sex.\n\nB. MIT is planning to subject admitted freshman to \"At Home Test Kit for Illicit Drugs\", which has been\nshown to be fairly reliable, in the sense that 90% of those using drugs test positive, while only 10% of\nthose not using drugs test positive (in your age group). Assume that 20% of the population in your age\ngroup actually uses illicit drugs. Alas, you test positive. What is the probability that you are truly and\nfairly busted?\n\nC. Assume that the probability of conception in any given month among sexually active couples not\npracticing birth control is constant at .20 per month, independent of the number of months the couple has\nbeen active. What is the expected waiting time to first birth?\n\nD. A 1981 social experiment offered tax credit vouchers and direct rebate vouchers to unemployed job\nseekers on the welfare rolls in Dayton, Ohio. The job seekers were randomly divided into 3 groups. The\ntax credit voucher group received a voucher that employers could use to reduce their federal tax liability\nif they hired a someone in this group. The direct rebate voucher group received a voucher that employers\ncould cash in with the program administrator after the job seeker was employed for 3 months. Results of\nthe experiment are reported in Table 1 in Burtless (1985), available on the course web page1.\n\n1. Construct separate t-tests comparing each of the two treatment groups with the control group.\n\nDid the treatments have statistically significant effects? If so, in what direction?\n\n2. Test whether the employment rates in the two treatment groups differ from each other.\n\n3. Construct 95% confidence intervals for the two treatment-control contrasts.\n\n4. Footnote 11 reports a chi-square statistic for independence between group (tax credit, rebate,\n\nand control) and job placement. Explain how this was constructed.\n\nE. From Wooldridge: B.3, B.5, B.6, B.10, C.1\n\n1Gary Burtless, \"Are Targeted Wage Subsidies Harmful? Evidence from a Wage Voucher Experiment,\"\nIndustrial and Labor Relations Review 39 (October 1985), 105-111."
    },
    {
      "category": "Resource",
      "title": "psy07n01.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-32-econometrics-spring-2007/300124445f4e4e08cd6a4a9fb610abf2_psy07n01.pdf",
      "content": "Cite\nCite as: Joshua Angrist, James Berry, and Emre Kocatulum, course materials for 14.32 Econometrics, Spring 2007. MIT\nOpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nProblem Set I\nMIT 14.32\n\nSpring 2007\n\nDue: Th, Feb 22\n\nA. From Wooldridge: C.2, C.3, C.4, C.5, C.8, C.10\n\nB. Additional problems\n\n1. This problem asks you to use SAS to conduct a series of sampling experiments.\n\na. Draw 500 random samples of size 6 from a random number generator for a standard normal distribution. Then\nincrease the sample size to 24. Finally, increase the sample size to 96. Plot histograms of the sampling distributions\nof (i) the sample mean and (ii) the sample variance, for each these three sample sizes. Now repeat your experiments\nfor three samples drawn from another parametric distribution of your choice (e.g., a uniform distribution). Discuss\nthe results of your experiments in light of the central limit theorem.\n\nb. Your experiments produce \"samples of sample means.\" Compute the mean and variance of the sample means\ngenerated by each experiment and compare them to the mean and variance predicted by statistical theory. Does the\nvariance of the sample means (i.e., the sampling variance) decrease with sample size at the rate predicted by the\ntheory? Does Normality matter for this?\n\n2. You are asked to conduct a social experiment to measure the effects of a Job Search Assistance program designed\nto help unemployed workers find jobs. You will do this by randomly choosing n1 experimental subjects and n2\ncontrol subjects from a pool of n1+n2=n unemployed workers who were selected at random from the population of\nnew Unemployment Insurance claimants in Massachussetts.\n\na. Find the choice of proportion treated, p=n1/n, that minimizes the sampling variance of the difference in\nemployment rates between treatment and controls. (Treat n as a known constant).\n\nb. Now assume that it costs \" dollars to collect data on anyone in your experiment and that the job search assistance\nprovided to the experimental group costs $ dollars. You can choose any sample size (n) but you must spend no\nmore than R dollars on the experiment. Again, maintaining the assumption that there is no treatment effect, solve\nfor the value of p which minimizes the variance of the treatment/control contrast given the experimenter's budget\nconstraint. Interpret your result and compare to part (a).\n\nc. Why is it useful to do exercises like (a) and (b) while assuming there is actually no treatment effect?\n\nCite as: Joshua Angrist, James Berry, and Emre Kocatulum, course materials for 14.32 Econometrics, Spring 2007. MIT\nOpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n3. Table 3 in the Woodbury and R.G. Spiegelman, paper on the reading list reports the results of two social\nexperiments examining the relationship between Unemployment Insurance (UI) and the length of time unemployed.\nIn the Employer Experiment, any UI recipient finding employment for at least 4 months received a voucher worth\n$500 to his or her employer. In the Claimant Experiment, any UI recipient finding employment for at least 4 months\nreceived $500 directly.\n\na. For each experiment, test the hypothesis that bonuses increased the proportion of UI claimants returning to work\n(i.e., ending benefits) within 11 weeks. For the employer experiment, compute the test statistic under two scenarios:\n(i) the experiment has no effect; (ii) the experiment has an effect.\n\nb. For each experiment, pick a significance level and test the hypothesis that the experiment reduced weeks of\ninsured unemployment in the benefit year using a one-tailed and two-tailed test. Which test seems to make more\nsense in this case?"
    },
    {
      "category": "Resource",
      "title": "psy07n02.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-32-econometrics-spring-2007/9865af44aed8e5f2486ed3b2f533fac6_psy07n02.pdf",
      "content": "Problem Set II\n\nMIT (14.32)\n\nSpring 2007\n\ndue: March 8, 2007\n\nA. From Wooldridge: B.10 (more on expectation), 2.4, 2.8, 2.9 (bivariate regression basics)\n\nB. Additional problems\n\n1. Consider E[Y| X] where X is a dummy variable that equals one with probability p and is zero\notherwise. Prove that the CEF and the regression of Y on X are the same in this case. Do this by\nshowing that for Bernoulli X:\n\n\" = E(Y)-$E(X) = E[Y| X=0]\n\n$ = COV(X,Y)/V(X) = (E[Y| X=1]-E[Y| X=0])\n\n2. The 14.32 web page contains a data set with observations on Log(Usual weekly earnings), Sex, Age\nand Education for respondents aged 25-29 in the March 1992 CPS.\n\na. Run PROC CONTENTS and PROC MEANS to verify the contents of the data set.\n\nb. Use PROC TTEST to compare average log wages by sex for a subsample aged 40-49. Use PROC\nREG to prove the result in question B.1 above \"by computer.\"\n\nc. PROC TTEST calculates the standard error of differences in averages under two different assumptions\nabout variances. State these assumptions in words. Which calculation corresponds to the one being used\nby PROC REG? Explain and check your answer.\n\nC. Regression application.\n\nFor this problem, you will be provided with data on the price of wine (Chateau Latour) sold at a 1986\nLondon auction. Wines sold were from vintage years 1882 to 1983. The same data set includes\ninformation on rainfall and temperature in the region in France where Chateau Latour grapes are grown.\n\nThe major determinant of wine quality is vintage (i.e., time passed since the wine was bottled.) This\nsuggests that wine prices should increase with age. Let t be the year a bottle of wine is bottled and T be\nthe year that it is sold at auction. When sold, the wine is J = T-t years old. Suppose the unknown price\nof a wine aged J is an increasing concave function f(J). The present value of wine in the year it is bottled\nis therefore\n\nv(J) = f(J)/(1 + r)J,\nCite as: Joshua Angrist, James Berry, and Emre Kocatulum, course materials for 14.32 Econometrics, Spring 2007. MIT\nOpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nwhere r is the discount (interest) rate used by investors to evaluate future prospects. f(0) is the price of\nthe wine in the year it is bottled.\n\n1. Like financial investors holding a treasury bond, vintners and wine investors can sell their wine now or\nhold it and sell it at a later date. One theory of asset pricing in financial markets is that market forces will\nset prices to equalize the present value of an asset sold on different dates. Why is this a compelling\ntheory?\n\n2. Show that equal present values at different vintages implies\n\n(1)\n\nln[f(J)] . rJ + ln[f(0)]\n\n3. Use the data for this problem set and equation (1) to estimate the discount rate used by those who buy\nand sell Chateau Latour. Test whether r equals 3 percent. Plot the data and fitted values from your\nregression. Briefly discuss the fit.\nCite as: Joshua Angrist, James Berry, and Emre Kocatulum, course materials for 14.32 Econometrics, Spring 2007. MIT\nOpenCourseWare (http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "rec_4_13.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-32-econometrics-spring-2007/241402474de1ad9da3b97c1938788dd7_rec_4_13.pdf",
      "content": "More on Differences-in-Differences\n14.32 Recitation 4/13/07\nIn lecture, we went over Card and Kreuger's estimating equation:\nyit NJi D2 NJi ∗ D2\n#\nSometimes it's useful to use a 2x2 matrix for thinking about DD. For each of Card\nand Kreuger's four state-time categories, the conditional mean of yit is:\nT 1 T 2\nDiff across time\nNJ 0\n\nNJ 1\n\nDiff. across states\n\nCausality\nThe critical assumption in CK is that in the absence of the intervention, both states\nwould have the same time trend. When using time series such as these, we can\ntest the assumption using pre-trends. That is, check whether NJ and PA had the\nsame time trend before the minimum wage was raised in NJ. Card and Kreuger\ndon'bt report this in the main paper, but they do have some additional evidence on\ntrends in the Myth and Measurement book.\nAnother DD application: Children's Test Scores:\nBanerjee, Cole, Duflo and Linden (Forthcoming, Quarterly Journal of Economics)\nreport the results of several randomized experiments in India. Randomization is\nconsiderably more \"bulletproof\" than cross-state comparisons. However, it's\nexpensive collecting your own data and providing your own interventions.\nDevelopment economists perform randomized trials a lot because\nExperiments are generally cheaper in developing countries\nIt's harder to get good data prepackaged from developing countries\nComputer-assisted learning/balsakhi experiment: schools were randomly\nassigned an extra teacher (\"balsakhi\") or were taught using computers\n(computer-assisted learning, or CAL), both, or nothing.\nCite as: James Berry, course materials for 14.32 Econometrics, Spring 2007. MIT OpenCourseWare (http://ocw.mit.edu/),\nMassachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nFor the CAL experiment, schools were randomly assigned to either \"treatment\"\n(computer) or \"control\" (no computer) group. Computer classes were taught two\nhours a week to fourth graders and focused on math skills. Students were tested\nbefore the program began (\"pretest\") and after (\"posttest\").\nData were stacked for the analysis. That is, each child had two observations, one\nfor the pretest and one for the posttest. The estimating equation for the effect of\nthe program is\ntestit treatit postit treatit ∗ postit\nwhere i, t represent child i, test t,\ntreat 1 if the child was in the treatment group, 0 otherwise\npost 1 if the test was a posttest, 0 otherwise\ntreatit ∗ postit is the interaction of the two\nWe can create a similar 2x2 matrix as before:\npost 0 post 1\nDiff across tests\ntreat 0\n\ntreat 1\n\nDiff. across treatments\n\nQuestions:\nWhat is the critical assumption?\nHow confident are we that the assumption holds?\nWas it important to have a pretest?\nCite as: James Berry, course materials for 14.32 Econometrics, Spring 2007. MIT OpenCourseWare (http://ocw.mit.edu/),\nMassachusetts Institute of Technology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "rec_4_20.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-32-econometrics-spring-2007/c68f54a5cac9d51f45f5916c123a223f_rec_4_20.pdf",
      "content": "More on FGLS and How to Test for Heteroskedasticity\nGeneralized Least Squares and Feasible GLS\nHere's a little more detail on how to perform GLS/FGLS:\nSuppose you have a standard multivariate model such as\nyi 0 1x1i 2x2i i\nwhich satisfies all of the standard assumptions except Vari|x1i, x2i = 2. Instead,\nyou are told (or you guess) that\nVari|x1i, x2i 2gx1i, x2i 20 1|x1i | 2|x2i |\nThis is definitely not the only way to model the standard errors. For example, you\ncan use interactions, higher powers of the x's, etc. There are two issues to watch\nout for:\n1. All Variances must be positive (the reason for using the exponential\nfunction above)\n2. You need to believe in your model! In most cases, there is no strong\ntheory as to what the heteroskedasticity should look like. This is why many\nempirical economists just \"punt\" and just use robust standard errors.\nIf you are confident in your model and you know 0, 1, and 2, then you can just\nuse generalized least squares. Instead of running the model\nyi 0 1x1i 2x2i i\nyou need to divide all of the variables by gx1i, x2i and regress\nyi\n\n1x1i\n\n2x2i\n\ni\ngx1i, x2i\ngx1i, x2i\ngx1i, x2i\ngx1i, x2i\ngx1i, x2i\nNow, we have eliminated the heteroskedasticity, since\ni\n\nVari 2\nVar\ngx1i, x2i\ngx1i, x2i\nThis procedure is relatively easy, but not feasible since we almost never know the\nparameters of the variance equation, 0, 1, and 2. Hence the need for Feasible\nGeneralized Least Squares.\nCite as: James Berry, course materials for 14.32 Econometrics, Spring 2007. MIT OpenCourseWare (http://ocw.mit.edu/),\nMassachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nThe general idea of FGLS is to use the estimated i from the standard OLS\nequation, then estimate 0, 1, and 2 using another OLS regression. The key\nhere is that even though OLS standard errors are wrong, the estimates are still\nconsistent. This means that we can use the estimated errors as consistent\nestimators of the actual standard errors.\nFGLS is a straightforward process, but there are more steps:\n1. Run the OLS Regression yi 0 1x1i 2x2i i to get the estimated\nresiduals i.\n2. Run the regression i\n2 gx1i, x2i, in our case we have\ni 0 1|x1i | 2|x2i |\n\nThe fitted values from this regression, call them are our estimated gx1i, x2i\n(up to a multiplicative constant, which doesn't matter).\n3. We can now use a similar formula as before\nyi\n\n1x1i\n\n2x2i\n\ni\ngx1i, x2i\ngx1i, x2i\ngx1i, x2i\ngx1i, x2i\ngx1i, x2i\nComputing the actual variance of the new residual\ni\nis tricky, but suffice to\ngx1i,x2i\nsay that in large samples\nVar\ni\ngx1i, x2i\nVar\ni\ngx1i, x2i\n\ngx1i, x2i Vari 2\nwhich is exactly the same as in GLS, as if we really knew the parameters of\ngx1i, x2i!\nHow to check for Heteroskedasticity\nHere are two ways to test for heteroskedasticity, and they use the same concepts\nas we use above: they test whether the squared estimated residuals are related to\nthe x's.\nThe Breusch-Pagan Test\n1. Run the OLS Regression yi 0 1x1i 2x2i i to get the estimated\nresiduals.\nCite as: James Berry, course materials for 14.32 Econometrics, Spring 2007. MIT OpenCourseWare (http://ocw.mit.edu/),\nMassachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n2. Run the OLS regression of the estimated residuals on the independent\nvariables, that is,\ni 0 1x1i 2x2i\n3. Construct and F-test of the joint hypothesis that 1 0 and 2 0, as\nyou would in any other OLS situation.\nThe White Test\n1. Run the OLS Regression yi 0 1x1i 2x2i i to get the estimated\nresiduals\n2. Run the OLS regression of the estimated residuals on the independent\nvariables and interactions of the independent variables, that is,\ni\n2 0 1x1i 2x2i 3x1ix2i\n3. Construct and F-test of the joint hypothesis that 1 0, 2 0 and\n3 0 , as you would in any other OLS situation.\nQ: Which of these two tests is better?\nSAS can automatically compute a variant of the White test using the SPEC option\nin the model statement. (It uses a Chi-squared LM-type test instead of an F-test to\ncheck for joint significance, but it doesn't really matter which one you use.)\nPunting with White Standard Errors\nFinally, recall that if we decide to \"punt\" and assume general heteroskedasticity\njust in case it's there, we simply use the formula\nVar ∑ x 1iui\nsx\n2 1 2\nI put x in there because you need to partial out other variables when doing a\nmultivariate regression.\nTo do this numerically in SAS, you need to use the option ACOV in the MODEL\nCite as: James Berry, course materials for 14.32 Econometrics, Spring 2007. MIT OpenCourseWare (http://ocw.mit.edu/),\nMassachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nstatement. To get the standard errors of each variable, take the square root of\neach diagonal element of the matrix that turns up in the output. Check out the\noutput on the next page using a standard earnings-schooling regression with both\nthe ACOV and SPEC options:\nproc reg dataone;\nmodel rearningsed_comp exp / acov spec;\nThis was done with Kreuger's 1984 data from problem set 5.\nCite as: James Berry, course materials for 14.32 Econometrics, Spring 2007. MIT OpenCourseWare (http://ocw.mit.edu/),\nMassachusetts Institute of Technology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "rec_4_27.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-32-econometrics-spring-2007/79b4f5053aa4b0b555869b2bd8d372b5_rec_4_27.pdf",
      "content": "Serial Correlation\n14.32 Spring 2007\nConsider the generic model\nyt xt t\nwhere all of the normal assumptions hold EXCEPT for random sampling, i.e.,\nEij = 0. This violates the Gauss-Markov assumptions, so OLS estimators will\nnot be BLUE. BUT, they are still unbiased (or consistent in large samples), a\nfeature we depend on for creating \"good-guy\" standard errors.\nHow to deal with standard errors under serial correlation?\n1. Punt: Use Newey-West standard errors, which are robust to both\nheteroskedasticity and autocorrelation. You can't do this in SAS, but the\nStata command newey will do it automatically. You know you've made it\nwhen you've got your own Stata command.\n2. Model the serial correlation (GLS or FGLS). Just as with\nheteroskedasticity, the value of this procedure for correction will only be as\ngood as the error model you use. Unlike heteroskedasticity, however, serial\ncorrelation usually involves some sort of autoregressive process, i.e.,\nt\nt ∑ pjt-j t\nj1\nA specific (and intuitive) case that we often deal with is first-order autoregression,\nAR(1)\nt t-1 t\nwhere we (conveniently) assume that t has the properties we like, i.e.,\nhomoskedasticity and no serial correlation.\nAside: Macro-economists often think of the t as \"shocks.\" What sort of processes\nare implied when 1, 1, 1 in an AR(1) model?\nCite as: James Berry, course materials for 14.32 Econometrics, Spring 2007. MIT OpenCourseWare (http://ocw.mit.edu/),\nMassachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nGLS (Quasi-Differencing)\nGLS with serial correlation involves a very similar process to the one we used with\nheteroskedasticity. We use the form of the standard errors to convert the model to\none with a well-behaved error term. In the case of AR(1), we can subtract a\nmultiple of observation t - 1 from observation t:\nyt xt t-1 t\n- yt-1 xt-1 t-1\nyt - yt-1 1 - xt - xt-1 t\ny t 1 - x t t\nAside: Prais-Winston and \"getting back\" the first observation.\nIf we know , we can convert the variables using the process above, and estimate\nusing OLS. The transformed equation now satisfies all of the G-M assumptions\nand will be BLUE (technically, you have to use the first observation for it to be\nBLUE, but it doesn't matter much). It will also be asymptotically efficient in large\nsamples.\nWithout (as is normally the case), we have to rely on FGLS.\nFGLS\nJust like with heteroskedasticity, FGLS relies on the fact that OLS estimators are\nconsistent under serial correlation. We can therefore consistently estimate using\na the estimated OLS residuals. The full process is as follows:\n1. Run the normal OLS regression and collect the residuals t.\n2. Estimate the regression\nt t-1 t\nusing OLS and get the estimate .\n3. Use the estimated p to construct y t yt - yt-1 and x t xt - xt-1\n4. Run the OLS regression\nCite as: James Berry, course materials for 14.32 Econometrics, Spring 2007. MIT OpenCourseWare (http://ocw.mit.edu/),\nMassachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\ny t 1 - x t t\n...and you're done! FGLS will be asymptotically efficient, and the standard errors\nyou get will be valid.\nIterating\nIf you are just thinking about asymptotics, the process above will be just fine. In\nsmall samples, however, you could end up with very inefficient estimates of .\nIterating helps to produce more efficient estimates.\nA. Run steps 1-4 above and collect the estimated .\nB. Use that to get the estimated residuals t from the original estimating\nequation. Note: do not simply collect residuals from the transformed\nequation, as those are estimates of t and not t!\nC. Use the new t and repeat steps 2-4 above.\nD. Repeat steps A and B as many times as you like.\nEventually, your estimates of and should converge, i.e., after some point they\nwon't change much if you iterate again. That's often a good time to stop.\nWe have seen that in SAS, the command for iterative quasi-differenced strategy\n(keeping the first observation) is\nproc autoreg dataone;\nmodel yx / iter nlag1;\nNote that changing the nlag option allows you to model a higher-order processes.\nCite as: James Berry, course materials for 14.32 Econometrics, Spring 2007. MIT OpenCourseWare (http://ocw.mit.edu/),\nMassachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nTesting for Serial Correlation\nThe above discussion suggests a very simple strategy for testing for serial\ncorrelation: check the magnitude and significance level of your estimated .\nEconomists that deal with time-series data often prefer the\nsophisticated-yet-unintuitive Durbin-Watson Statistic. It turns out that the D-W\nstatistic is more or less a transform of :\nDW ≈ 21 -\nWe can reject serially uncorrelated errors if the DW statistic is close enough to 2.\nThe annoying thing about DW is that there is a range of values for the DW statistic\nwhere we can neither reject or fail to reject. Part of the reason for this is that the\nunderlying distribution changes based on the values of the of regressors in the\noriginal equation.\nCite as: James Berry, course materials for 14.32 Econometrics, Spring 2007. MIT OpenCourseWare (http://ocw.mit.edu/),\nMassachusetts Institute of Technology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "rec_5_11.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-32-econometrics-spring-2007/f0117ee91741abab8fa97535ac15f0be_rec_5_11.pdf",
      "content": "Additional Notes on 2SLS and Simultaneous Equations\n14.32, Spring 2007\nRecitation 5/11/07\nConsider the generic model\nqd 0 1p 2x\nqs 0 1p 2z\nqs qd\nIn this system, both the supply and demand equations are exactly identified. We\ncan estimate 1 and 1 using indirect least squares or two-stage least squares.\nTo obtain the 2SLS estimate of 1 :\n1. Regress p on x and z\n2. Collect the predicted values p\n3. Regress qd 0 1p 2x p - p\nThe important thing to notice is that in the first stage of 2SLS, we regress p on all\nexogenous variables, not just the instrument x. The only explanation for this is\nmechanical, and it's quite tedious to show why. But here's a sketch of how the\nmechanics work in our simple example.\nGoing back to the derivation of instrumental variables, the IV estimator for 1 is\nCovq, z\n1 Covp, z\nNote that we have to partial out the effect of x in the demand equation before we\ncan derive the IV estimator.\nIn order for the 2SLS estimator to equal this IV estimator, we must also parital out\nthe effects of x from all variables.\nCite as: James Berry, course materials for 14.32 Econometrics, Spring 2007. MIT OpenCourseWare (http://ocw.mit.edu/),\nMassachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n1,2SLS Covq, p\nVarp\nCovq, Covp,z z\n\nvarz\nVar Covp,z z\nvarz\nCovp,z Covq, z\n\nvarz\nCovp,z z\nVarz\nvarz\nCovq, z\nCovq, z 1,IV\nEven though x is not technically an instrument, we still must include it in the first\nstage. To make matters more confusing, we have to define x as an instrument\nPROC SYSLIN:\nproc syslin dataone 2sls;\nendogenous p;\ninstruments x z;\nmodel qpx;\nIncluding only z as an \"instrument\" will give you the wrong answer! As a rule of\nthumb, don't forget to include all exogenous variables in your first stage. The\ninstrument is the excluded variable in the 2nd stage.\nCite as: James Berry, course materials for 14.32 Econometrics, Spring 2007. MIT OpenCourseWare (http://ocw.mit.edu/),\nMassachusetts Institute of Technology. Downloaded on [DD Month YYYY]."
    },
    {
      "category": "Resource",
      "title": "rec_5_4.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/14-32-econometrics-spring-2007/915e24f549e0ab99bf904fa14f3d77d5_rec_5_4.pdf",
      "content": "Implementation of IV and Two-Stage Least Squares\n14.32, Spring 2007\nReview of IV\nIn lecture we saw the schooling model\nyi Si Ai i\nwhere yi is log income of individual i, Si is schooling, and Ai is unmeasured ability.\nSince ability is unobservable, the OLS regression becomes\nyi Si ui\nwhere ui Ai i. From the omitted variables bias formula, the coefficient we\nestimate has p lim AS, where AS is the coefficient of the bivarite\nregression of ability on schooling.\nOne way to deal with this problem is to put in enough covariates so that schooling\nis no longer correlated with the unobserved error term. This works in some cases,\nbut often you can't find the appropriate covariates, e.g., how can you measure\nability? Some people have used test scores, but these results are not 100%\nconvincing.\nInstrumental variables estimation gets around the omitted variables bias problem\nby finding another variable, called an instrument, which affects the variable of\ninterest but nothing else in the regression. More formally, you find some\ninstrument z where covzi, ui 0. Then, you can back out the parameter of\ninterest using covariances:\ncovyi, zi cov Si ui, zi covSi, zi\ncovyi, zi\nyz\n\ncovSi, zi\nSz\nFrom the law of large numbers, the sample analog will give you a consistent\nestimate, i.e.,\nCite as: James Berry, course materials for 14.32 Econometrics, Spring 2007. MIT OpenCourseWare (http://ocw.mit.edu/),\nMassachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\np lim covyi, zi p lim yz\ncovSi, zi\nSz\nThe important assumptions for the instrument z are covzi, Si = 0 and\ncovzi, ui 0. In words we can state these two assumptions as:\n1. The insturment z is correlated with the endogenous variable S (there is a\nfirst stage).\n2. The instrument z only affects y through the variable S (exclusion\nrestriction).\nTwo-Stage Least Squares\nPractically, IV is usually implemented through a process called two-stage least\nsquares (2SLS). This is actually a simple type of simultaneous equations problem.\nLet's set up the relationships between y, S, z and u as\nyi Si ui\nSi zi i\nTo run 2SLS:\n1. Regress S on z\n2. Using the estimated parameters, estimate S\n3. Regress y on S\n4. The estimated is your IV estimator.\nIt turns out that through this process the estimated is numerically equivalent to\nthe IV estimator developed above, covy,z . Here's the proof (supressing some hats\ncovz,S\nand i′ s to make it easier to read):\nCite as: James Berry, course materials for 14.32 Econometrics, Spring 2007. MIT OpenCourseWare (http://ocw.mit.edu/),\nMassachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n2SLS covy, S\nvarS\ncovy, covS,z z\n\nvarz\nvar covS,z z\nvarz\ncovS,z covy, z\n\nvarz\ncovS,z\nvarz\nvarz\ncovy, z IV\ncovS, z\nComputing 2SLS Estimates\n2SLS makes computing IV estimators easy. Here's an example with SAS. As we\nhave seen in class, there are a number of instruments one can use to estimate the\nwage-schooling equation\nyi Si ui\nAn alternative (and clever) instrument for schooling was developed by Card (1995).\nHis insight was that people who live closer to colleges have a lower cost of\nattending school, and therefore higher education levels. For his instrument he\nconstructed an indicator for whether the individual lived near a four-year college.\nThe quality of this instrument has been the subject of debate, and you can make\nyour own judgement as to its own validity.\nTo compute 2SLS, we first estimate the first-stage regression:\nSi ∗ nearc4i i\nThen take the fitted values and estimate\nyi Si i\nNote that the standard errors from the second OLS estimation need to be corrected\nfor 2SLS. The basic idea of this is that once we have a consistent estimate of ,\nwe use S, not S, to compute the residuals. SAS does this automatically with PROC\nSYSLIN.\nCite as: James Berry, course materials for 14.32 Econometrics, Spring 2007. MIT OpenCourseWare (http://ocw.mit.edu/),\nMassachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n/*2SLS DEMONSTRATION*/\ndata one;\nset 'card';\n/*OLS REGRESSION*/\nproc reg data=one;\nmodel lwage=educ;\ntitle \"Simple OLS\";\n/*FIRST STAGE OF 2SLS REGRESSION*/\nproc reg data=one;\nmodel educ=nearc4;\noutput out=one\np=educhat;\ntitle \"First Stage\" ;\n/*SECOND STAGE*/\nproc reg data=one;\nmodel lwage=educhat;\ntitle \"Second Stage\";\n/*DOING IT ALL AT ONCE USING PROC SYSLIN*/\nproc syslin data=one 2sls;\nendogenous educ;\ninstruments nearc4;\nmodel lwage=educ;\ntitle \"2SLS with PROC SYSLIN\";\nrun;\nCite as: James Berry, course materials for 14.32 Econometrics, Spring 2007. MIT OpenCourseWare (http://ocw.mit.edu/),\nMassachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nSimple OLS 20:57 Thursday,\nMay 3, 2007 1\nThe REG Procedure\nModel: MODEL1\nDependent Variable: lwage\nNumber of Observations Read 3010\nNumber of Observations Used 3010\nAnalysis of Variance\nSum of Mean\nSource DF Squares Square F Value Pr > F\nModel 1 58.51536 58.51536 329.54 <.0001\nError 3008 534.12627 0.17757\nCorrected Total 3009 592.64163\nRoot MSE 0.42139 R-Square 0.0987\nDependent Mean 6.26183 Adj R-Sq 0.0984\nCoeff Var 6.72948\nParameter Estimates\nParameter Standard\nVariable DF Estimate Error t Value Pr > |t|\nIntercept 1 5.57088 0.03883 143.47 <.0001\neduc 1 0.05209 0.00287 18.15 <.0001\nCite as: James Berry, course materials for 14.32 Econometrics, Spring 2007. MIT OpenCourseWare (http://ocw.mit.edu/),\nMassachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nFirst Stage 20:57 Thursday,\nMay 3, 2007 2\nThe REG Procedure\nModel: MODEL1\nDependent Variable: educ\nNumber of Observations Read 3010\nNumber of Observations Used 3010\nAnalysis of Variance\nSum of Mean\nSource DF Squares Square F Value Pr > F\nModel 1 448.60420 448.60420 63.91 <.0001\nError 3008 21113 7.01911\nCorrected Total 3009 21562\nRoot MSE 2.64936 R-Square 0.0208\nDependent Mean 13.26346 Adj R-Sq 0.0205\nCoeff Var 19.97488\nParameter Estimates\nParameter Standard\nVariable DF Estimate Error t Value Pr > |t|\nIntercept 1 12.69801 0.08564 148.27 <.0001\nnearc4 1 0.82902 0.10370 7.99 <.0001\nCite as: James Berry, course materials for 14.32 Econometrics, Spring 2007. MIT OpenCourseWare (http://ocw.mit.edu/),\nMassachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\nSecond Stage 20:57 Thursday,\nMay 3, 2007 3\nThe REG Procedure\nModel: MODEL1\nDependent Variable: lwage\nNumber of Observations Read 3010\nNumber of Observations Used 3010\nAnalysis of Variance\nSum of Mean\nSource DF Squares Square F Value Pr > F\nModel 1 15.86603 15.86603 82.74 <.0001\nError 3008 576.77560 0.19175\nCorrected Total 3009 592.64163\nRoot MSE 0.43789 R-Square 0.0268\nDependent Mean 6.26183 Adj R-Sq 0.0264\nCoeff Var 6.99299\nParameter Estimates\nParameter Standard\nVariable Label DF Estimate Error t Value Pr > |t|\nIntercept Intercept 1 3.76747 0.27433 13.73 <.0001\neduchat Predicted Value of educ 1 0.18806 0.02067 9.10 <.0001\nCite as: James Berry, course materials for 14.32 Econometrics, Spring 2007. MIT OpenCourseWare (http://ocw.mit.edu/),\nMassachusetts Institute of Technology. Downloaded on [DD Month YYYY].\n\n2SLS with PROC SYSLIN 20:57 Thursday,\nMay 3, 2007 4\nThe SYSLIN Procedure\nTwo-Stage Least Squares Estimation\nModel lwage\nDependent Variable lwage\nAnalysis of Variance\nSum of Mean\nSource DF Squares Square F Value Pr > F\nModel 1 15.86603 15.86603 51.17 <.0001\nError 3008 932.7531 0.310091\nCorrected Total 3009 592.6416\nRoot MSE 0.55686 R-Square 0.01673\nDependent Mean 6.26183 Adj R-Sq 0.01640\nCoeff Var 8.89289\nParameter Estimates\nParameter Standard\nVariable DF Estimate Error t Value Pr > |t|\nIntercept 1 3.767472 0.348862 10.80 <.0001\neduc 1 0.188063 0.026291 7.15 <.0001\nCite as: James Berry, course materials for 14.32 Econometrics, Spring 2007. MIT OpenCourseWare (http://ocw.mit.edu/),\nMassachusetts Institute of Technology. Downloaded on [DD Month YYYY]."
    }
  ]
}