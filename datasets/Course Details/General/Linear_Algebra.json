{
  "course_name": "Linear Algebra",
  "course_description": "This course covers matrix theory and linear algebra, emphasizing topics useful in other disciplines such as physics, economics and social sciences, natural sciences, and engineering. It parallels the combination of theory and applications in Professor Strang’s textbook Introduction to Linear Algebra.\nCourse Format\nThis course has been designed for independent study. It provides everything you will need to understand the concepts covered in the course. The materials include:\n\nA complete set of Lecture Videos by Professor Gilbert Strang.\nSummary Notes for all videos along with suggested readings in Prof. Strang’s textbook Linear Algebra.\nProblem Solving Videos on every topic taught by an experienced MIT Recitation Instructor.\nProblem Sets to do on your own with Solutions to check your answers against when you’re done.\nA selection of Java® Demonstrations to illustrate key concepts.\nA full set of Exams with Solutions, including review material to help you prepare.",
  "topics": [
    "Mathematics",
    "Linear Algebra",
    "Mathematics",
    "Linear Algebra"
  ],
  "syllabus_content": "«\nPrevious\n|\nNext\n»\n\nVideo Introduction by Professor Strang\n\nLinear Algebra Course Introduction\n\nView video page\n\nDownload video\n\nDownload transcript\n\nCourse Overview\n\nThis course covers matrix theory and linear algebra, emphasizing topics useful in other disciplines. Linear algebra is a branch of mathematics that studies systems of linear equations and the properties of matrices. The concepts of linear algebra are extremely useful in physics, economics and social sciences, natural sciences, and engineering. Due to its broad range of applications, linear algebra is one of the most widely taught subjects in college-level mathematics (and increasingly in high school).\n\nPrerequisites\n\n18.02 Multivariable Calculus\nis a formal prerequisite for MIT students wishing to enroll in 18.06 Linear Algebra, but knowledge of calculus is not required to learn the subject.\n\nTo succeed in this course you will need to be comfortable with vectors, matrices, and three-dimensional coordinate systems. This material is presented in the first few lectures of 18.02 Multivariable Calculus, and again here.\n\nThe basic operations of linear algebra are those you learned in grade school - addition and multiplication to produce \"linear combinations.\" But with vectors, we move into four-dimensional space and n-dimensional space!\n\nCourse Goals\n\nAfter successfully completing the course, you will have a good understanding of the following topics and their applications:\n\nSystems of linear equations\n\nRow reduction and echelon forms\n\nMatrix operations, including inverses\n\nBlock matrices\n\nLinear dependence and independence\n\nSubspaces and bases and dimensions\n\nOrthogonal bases and orthogonal projections\n\nGram-Schmidt process\n\nLinear models and least-squares problems\n\nDeterminants and their properties\n\nCramer's Rule\n\nEigenvalues and eigenvectors\n\nDiagonalization of a matrix\n\nSymmetric matrices\n\nPositive definite matrices\n\nSimilar matrices\n\nLinear transformations\n\nSingular Value Decomposition\n\nFormat\n\nThis course, designed for independent study, has been organized to follow the sequence of topics covered in an MIT course on Linear Algebra. The content is organized into three major units:\n\nAx = b and the Four Subspaces\n\nLeast Squares, Determinants and Eigenvalues\n\nPositive Definite Matrices and Applications\n\nEach unit has been further divided into a sequence of sessions that cover an amount you might expect to complete in one sitting. Each session has a video lecture on the topic, accompanied by a lecture summary. For further study, there are suggested readings in Professor Strang's textbook (both the 4th and 5th editions):\n\nStrang, Gilbert. Introduction to Linear Algebra. 4th ed. Wellesley, MA:\nWellesley-Cambridge Press\n, February 2009. ISBN: 9780980232714\n\nStrang, Gilbert. Introduction to Linear Algebra. 5th ed. Wellesley, MA:\nWellesley-Cambridge Press\n, February 2016. ISBN: 9780980232776\n\nClick on the navigation links in the left column to display the sessions in the three units.\n\nTo help guide your learning, you will see how problem solving is taught by an experienced MIT Recitation instructor (six of the Problem Solving Videos are also available in Mandarin Chinese).\n\nFinally, within each unit you will be presented with sets of problems at strategic points, so you can test your understanding of the material.\n\nMIT expects its students to spend about 150 hours on this course. More than half of that time is spent preparing for class and doing assignments. It's difficult to estimate how long it will take you to complete the course, but you can probably expect to spend an hour or more working through each individual session.\n\nMeet the Team\n\nThis OCW Scholar course was developed by:\n\nGilbert Strang\n, Professor of Mathematics, Massachusetts Institute of Technology\n\nWith technical and writing assistance from:\n\nPhD Mathematics, \"Professor of Mathematics and Computer Science, Bridgewater State University\n\nThe Help Session Videos were developed by:\n\nMartina Balagovic\n\nLinan Chen\n\nBenjamin Harris\n\nAna Rita Pires\n\nDavid Shirokoff\n\nNikola Kamburov\n\nTo learn more about each of the TA's, visit the\nMeet the TAs\n.\n\n«\nPrevious\n|\nNext\n»",
  "files": [
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_ex1.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/170cb8d5b8ee2a2c170d611f87d0525d_MIT18_06SCF11_ex1.pdf",
      "content": "18.06SC Unit 1 Exam\n1 (24 pts.)\nThis question is about an m by n matrix A for which\n\n⎡\n⎤\n⎡\n⎤\nAx = ⎢\n⎣ 1 ⎥\n⎦ has no solutions and\nAx = ⎢\n⎣ 1 ⎥\n⎦ has exactly one solution.\n(a) Give all possible information about m and n and the rank r of A.\n(b) Find all solutions to Ax = 0 and explain your answer.\n(c) Write down an example of a matrix A that fits the description in\npart (a).\n\n2 (24 pts.)\nThe 3 by 3 matrix A reduces to the identity matrix I by the following three\nrow operations (in order):\nE21 :\nSubtract 4 (row 1) from row 2.\nE31 :\nSubtract 3 (row 1) from row 3.\nE23 :\nSubtract row 3 from row 2.\n(a) Write the inverse matrix A-1 in terms of the E's. Then compute A-1 .\n(b) What is the original matrix A ?\n(c) What is the lower triangular factor L in A = LU ?\n\n3 (28 pts.)\nThis 3 by 4 matrix depends on c:\n⎡\n⎤\n1 1 2 4\nA = ⎢\n⎣ 3 c 2 8 ⎥\n⎦\n0 0 2 2\n(a) For each c find a basis for the column space of A.\n(b) For each c find a basis for the nullspace of A.\n⎡\n⎤\n(c) For each c find the complete solution x to Ax = ⎣⎢ c ⎦⎥ .\n\n4 (24 pts.)\n(a) If A is a 3 by 5 matrix, what information do you have about the\nnullspace of A ?\n(b) Suppose row operations on A lead to this matrix R = rref(A):\n⎡\n⎤\nR = ⎢⎣ 0\n0 ⎥⎦\nWrite all known information about the columns of A.\n(c) In the vector space M of all 3 by 3 matrices (you could call this a\nmatrix space), what subspace S is spanned by all possible row reduced\nechelon forms R ?\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_ex2.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/2a49ca623b8057f0b8dbcccc252db815_MIT18_06SCF11_ex2.pdf",
      "content": "18.06SC Unit 2 Exam\n1 (24 pts.)\nSuppose q1, q2, q3 are orthonormal vectors in R3 . Find all possible values\nfor these 3 by 3 determinants and explain your thinking in 1 sentence each.\n(a) det q1 q2 q3\n=\n(b) det q1 + q2\nq2 + q3\nq3 + q1\n=\n(c) det q1 q2 q3\ntimes det q2 q3 q1\n=\n\n2 (24 pts.)\nSuppose we take measurements at the 21 equally spaced times t = -10, -9, . . . , 9, 10.\nAll measurements are bi = 0 except that b11 = 1 at the middle time t = 0.\n(a) Using least squares, what are the best Cb and Db to fit those 21 points\nby a straight line C + Dt ?\n(b) You are projecting the vector b onto what subspace ? (Give a basis.)\nFind a nonzero vector perpendicular to that subspace.\n\n3 (9 + 12 + 9 pts.)\nThe Gram-Schmidt method produces orthonormal vectors q1, q2, q3\nfrom independent vectors a1, a2, a3 in R5 . Put those vectors into the columns\nof 5 by 3 matrices Q and A.\n(a) Give formulas using Q and A for the projection matrices PQ and PA\nonto the column spaces of Q and A.\n(b) Is PQ = PA and why ? What is PQ times Q ? What is det PQ ?\n(c) Suppose a4 is a new vector and a1, a2, a3, a4 are independent. Which of\nthese (if any) is the new Gram-Schmidt vector q4 ? (PA and PQ from\nabove)\nT\nT\nT\nT\na a2\na a3\na a2\na a3\n∥PQa4∥\n∥ norm of that vector ∥\n∥a4 -PAa4∥\nT\na a1\na4 -\na1 -\na2 -\na3\na4 -PAa4\nPQa4\nT\na a1\n1.\n2.\n3.\n\n4 (22 pts.)\nSuppose a 4 by 4 matrix has the same entry × throughout its first row and\ncolumn. The other 9 numbers could be anything like 1, 5, 7, 2, 3, 99, π, e, 4.\n\n×\n×\n×\n×\n\nA =\n\n×\nany numbers\n\n×\nany numbers\n\n×\nany numbers\n(a) The determinant of A is a polynomial in ×.\nWhat is the largest\npossible degree of that polynomial ? Explain your answer.\n(b) If those 9 numbers give the identity matrix I, what is det A ? Which\nvalues of × give det A = 0 ?\n\n×\n×\n×\n×\n\n× 1\nA =\n\n× 0\n\n× 0\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_ex3.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/067b49baff2de3b7133794b52b824d36_MIT18_06SCF11_ex3.pdf",
      "content": "B\n18.06SC Unit 3 Exam\n}D\n\n~k5~§{~}y\n¥§~}\nA\n~{~¤\nn\n\n¦}{!\n\"!#%$\n&(')$\n#*\"+,\n-~¤=\n\n.!/103254\n~76~{98:8¤\n~{}{}y{8\n\n¥§~;8y{\n¢y{\nA\nxy¤¦Q=<\n}?>\n@<Q¤\n~\nA¥\n;8\n\nB<QQ~¤\n2@C\nQ{¦D%EQy¢yF=\nyQ\n{}y¥¦¢yGF=¦}y\n=\n\n<QQ~¤\nk\nC=C\nQ{y\nQ&\nQQ}¥\n~¤\n8Q¤¦¥\n{\nH\n~}yy-;<QyHEy-8}Q}{e\nB\n\n-1\n-1\n\n-1\n-1\n\nH =\n\n-1\n\nH\n= HT\n2 1\n\n-1\n-1\n\ny§yG9<QyHE~¤\ny{\nJ B\n~}y λ = 0, 1, 2, 3\nLK\n¦}y B\n~{$\nh¢=8\n\ns\n-8G;F8\n¥§~;8y{vMK\n\ny C = (B + I)-1\n~{\nh¢=8\n\ns/¥§~};8y{v\ntkONJ{\n=<$\nye¤9¦{P¦Ly{\nQ\n~k\n2H4\n~{98Q8¤\n~{}{y{5\n¥\n~98y{£¢ B\n~¢\nC\ny¤\nQ@<i?>\n~}~yy{D¦QRQ B\n~¢\nC\n\n¤\n\ny\n\ny\n\n.\n\n{\n¥\ny\n\n¢\n~\nk\n\n.\n\ny\n\nk\nI\nl\n\nv\n\ny\nC\n\n{\nC\ny\n\ny\nC\n\n.\ny\nC\nk\np\n\nC\n\nE\n¥\nI\n\n¡\nC\n¤\n~\n\nA\nv\n>\n\n~\n\nv\n\ny\n\ny\ny\nk\n{\n\n¥\ny\nC\n\n>\n>\n\nC\n\ny\n\nC\n¤\n~\nA\nk\n\n~\nE\n\ny\ny\n>\n\n}DxD\n\n~kM¦¢8\nyy$y-;<QyHE~¤¦y{\n\nA\n~¢~zyG9<Qy\ny-8\nQ\n~%\nS\n\n-1\n\nA =\n\nk\nM\n\n¦\nA1001 = A\nPb{ A1000 = I\nM¦¢\ny\n}yy¢=\n\n<Q~¤y}¦y{\n\neAt\ntk§l\n¥§~}\nATA\nQ\n{}~¥\nA\n\n-2\n-4\n\nATA =\n\n-2\n\n.\n\n-4\n\n~HA\n-9<yHE~¤\ny{JJ ATA\n~}y\nQ{\nD;Ey\n|y}?>\ny-<~D;Ey\nQ\nQ¥\n}y\ny¥\n\n¦\nQQ\n~{\nyv\ny{ ATA\nQy8\ny8{}~¥\n-;<QyHEQyG8}Q}{J~{ A\n\nD\n}DxD\n\nC@C\nQ{ye\nn\n6HA n\n~% A\n~{ n\n\nQQ¥§~¤y-9<yHEQyG8}{ q1, . . . , qn\n~¢\nn\nQ{¦D%EQyyG9<QyHE~¤\ny{ λ1, . . . , λn\nv£l\n{ Aqj = λjqj\n~Qk\n~i~}y\n-;<QyHE~¤\ny{4~¢\nG9<QyHEQy-8\nQ{i\nA-1\n)'\n+@$\n)\"\n$\n@\n+&19)&\n&,+\nk§HA7EQyG8}Q b\nQ¥(6=\n~D\n\n=\ny$y-;<QyHEy-8}Q}{\nb = c1q1 + c2q2 + · · · + cnqn .\n~\n=;8rOQ}¥i¤\n3Q c1\n{¦=<\nQ\nB<QQ~¤9\n§\n=\nq\ntk§l\n{}Q¤¦D¦Q\nAx = b\n¦{\n~¤\n{\nQ¥\n6=¦~\nQ\nJ\ny$y-9<yHEQyG8}{\nA-1b = d1q1 + d2q2 + · · · + dnqn .\n~\n{~3=98r\nQ¥i¤\nQ d1\n5Q8t~§{}ye\nc\nEQy7%\nQ\n=¦¢\n~{\ny\n~\nk\n.\n\ny\n¥\n\nC\n\nv\n\nK\n\ny\ny\ny\n>\n&\n&\n\n{\n~\n\nI\nK\n\n{\n~\n\n~\n\nA\n\ny\n\n{\n>\n\ny\n\n~\n\nI\nK\n\n~\n>\n\ny\n\n{\ny\nA\n¢\n\nC\n\nv\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_final_ex.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/245582e7bf822b1a99289385d4b1fe1c_MIT18_06SCF11_final_ex.pdf",
      "content": "18.06SC Final Exam\nA\n¢D\n\nG\nAA\n\n3~W¡ A\n\ny# (c) Ax = 0\n\n¤a\\vo~\n\naa\n\nW\n⎡\n⎡\n-2\nx1 =\n666 1\n⎢⎢⎢\n(c)\nx2 =\n666 -1\n⎢⎢⎢\n⎢\n⎢\n6 1 ⎢\n0 ⎢\n⎢\n⎢\n⎣\n⎣\n!\nWq#\"k«\n«$~\n\nA\n\ny\n\n*)2¡(c)3(c)\n\na\nW ,+W« R\n|\n%o\n&'\n( (c)\n!\nq\n\n.-\n\n(c)\n\n(c)\n\n«\n\nW *+\na¬a/+W30+3 (c)«\nxa¡31 C(A)\nN(A)\n\n|\nC(AT) N(AT)\nW3\n\n43W# W3%\n6+«\n\nW\n( (c)78+W%\nW5«\nW¡+\n\n¡\n3:9;( (c)\n\nW¡0|\nv\n\nB\nF\n#C\n(B\n\nAA\n*\n!\nq.-\n\n(c)1\n\n3¡+5y\n\ny*36\n\n%W3a\nn«\n¤\n\nU\n')\n\n=~=7\n¤\n\na, b, c, d, e, f\n|\nW3\nW3a¬(c)13\n+\nWn (c)o\n\n+W«o3a\n\n+W\n\n3W¡|\n\na\n\n3$w¢2W¡(c) \\a\n\n«\n\nW d|\n-\n\n(c)(\n\n3W¡5+\nU =\na b c\n0 d e\n0 0 f\n⎡\n⎢⎢⎢⎣\n|\n!\nq\n\n3~W¡%\n\n0Wa¬3«\n\n+ U\n\n%W 3W W\n+N\n«\n¤\n\nA\n|\n\n*)j\n\nA\n\na¬\n3~¤\n\n6%W3a\n|\n!\nzq\n\n6a\n\n)\n\nU\n\n8=<=\n\n~5\n\n%W3a\n\na¬3 W«\n\nW\no«\n5«¤\n\n\\\n\nA = UV T\n|\n0(¡\n+\nW\n\nC\nC\n#C\n(E\n\nAA\n*\n\n+\n!\nq A\n(c) B\n\n«¤\n\n)\n\n«\n\n3«$~5+¡\n)\n|\nW3T¤4\n!\n!\nq~W3P\n\n0W«\n\n¤ + A\n!\nq\n\n3~W¡ B = A2\n|#\"P*)\n\n%|\n!\nzq&\n+ A\n\nm\n\nn\n+\n\n+\n\na\n\nx«\n¤\n\n⎥\nA B\n⎦\n(c)*\n\nW¡P\nW«%$\n\na\n\nW3W fw\nr\n\n)\n\nk\n\nP(c)\n\n«\n\nW +\n\n¡ 3a\na¬\n'$\n(\n3a\na¬ + A\n(\n3a\na¬\n+ ⎥\nA A\n⎦\n\nD\n\nAfB\n*\n\nA\n\n¥\n\nAx\n\n3WP¡\n!\n\n)\n\nC\nD\n(E\n\n8(y1«¤\n(c)\n\nx\n\n&3W\n¤W¤q\n|\n!\nWq\n\nz\nW3\n\n4\\\n\n+ A\n$\n\nWa\n3«\n!\nq\n\n*)\n\nATAx\n\na¬o 3W¡\n!\n\n)\n\nx = 0\nq\n\na\n\n%\n\n(¢¤\n\n&\n+ ATAx = 0\n\n3a\nxTATAx = 0\n(c)#\n\n!\n\"\n$9q Ax = 0\n|\n!\nzq\n\n)s *)±\n\nATA\n\n3¡\n\na¬|*\n\na\n\n)\n\nB = (ATA)-1AT\n%W\nwh\n(c)(c)\n3W¡#+\n!\n)\n\n(c)+\nq\n$\n|\n\n(\nr\nLv9wm\n(c)(c)\n\nA\n\nA\nB\n\n3W¡5+ A\n!\n\nq\n|\n\nE\n\nA\n\n&\n+ A\n\n(y7'\ny\n\nf«\n«\n\nW\n\n3WT(c) (\n\n¤\nP\n\nAqi = iqi\n)\n\nE\n#E\n\nW\n\n%\n(c)\\W¡\n\nW W¡«ad\n\n%\nqi\n|\nW\nW 3a\nW 3W\n\n|\n~W x = c1q1 + c2q2 + c3q3\n!\nWq\nW«3¤ x x\n(c)\\a\n¡ xTAx\n\n(¡«\n\n+<\n\nc\n\nP (c)\n\n|\nT\n!\nq W\n\n1¤\n\n+ xTAx\n\n!\nqP(c)\n\n(c)(c)\n\nx x\n\n¡\n\n!\n%(\n¡\nq\nT\n)\n\nc\n\n)\n3a\n(c)\\«\n\n¤\n\na\n1%W\n\n~W\n\na¬\n$\nW3,z ¡3«\n\n< 2 < . . . < n\n|\nW 6a\n\nk\n\no\n\nxTAx/xTx\n\n«\n\n«o3«\n)\n\nx\n\n|\n\nF\n¢DD\n\nD\n\nAfB\n\n*\n!\nq.-\n\n(c)\na\n\nz\n«$\n\nW w\n+\n\na\n\nz¡a\n\n(c)~ (c)\n3W\nw\n¤W¡ v\n(c) u\n\n~ (c)\n\nu\n|\n3a\n\n!\nq.-Wk\n\n1vuw:a\n3«\n«\n\nA =\n⎥\n⎦\n\n(c)\n(\nQ\n!\nW¢\n\nW«\naWa\n3«\n¤q\nu\nv\n(c) R\n!\nv\n\\v13~\n\n%W3a\nxqN1\n\nA = QR\n|\n!\n\nk+ Q\nW a\n\n6%\nA = QR\n( (c)\n\nW \\«\n\nP\nW ¤\nzq&h «\n\n0¡\n\na\n\n1 (c)\nu\n(c) v\n|\nput\n\nG\n¢D#C\n\nD\n\nAA\n*\n!\nq.-\n\n(c)#\n\n%W '3a¬3\n+\n⎡\n⎡\n⎢\n⎢\nC =\n6 1\n0 ⎢⎢\n⎢\n(c) C2 =\n6 0\n1 ⎢⎢\n⎢\n.\n6 0\n0 ⎢\n6 1\n0 ⎢\n⎢\n⎢\n⎣\n⎣\nqr\n!\n\nW¡ ¡\n\n~«%3x\n\nW\n«\n\n|\n\n3W¡ C-1\n(c) (C2)-1\n$\n!\nzq.-\n\n(c)(\n\n(c)¡«\n\n¤+ C\n(c) C + I\n(c) C + 2I\n|\np9v\n\nH\n¢D#C\n\nD\n\nAA\n*\n\n¡\nx %3a\n«\n\nA\n\n(c)~ (c)\nWa¬3«\n|\n$\n!\nWq\"P*)\n(c)\nW3\n( (c)\n\n~¢\na¬z¢\n3¡¡Wa\n\n⎤x\n¤ Ax = b\n$\n\nx\n\n%\n\nW¡%¡\n%\n\n3W*«\n+W¡«o3a\n\n!\na\n¡¤k k 3«$~¤q\n+W ⎤x\n(c)\na\n¡\n+W p = A⎤x\n|\n!\nqr\n\n¡\n\nW p\n\n$)\n\n+3 (c)«\nxa3\n\n¤(c)\n)\n\nA\nr\n\n*¡W\n3W W e = b - p\n\n)\n\n+3 (c)«\nxa3\n$\n!\nzq.-\n\n(c)\n\n'L«\n\nf(c)T\n\n&¡¡\n\n«\n\nP\nW (\n\n&Wa¬3«\nT\n+ A\n\nA =\n0 -1\n0 -3\n⎡\n⎢⎢⎢⎢⎢⎢⎣\n.\npz\n\nI\nC\nD\n\nD\n\nAA\n*\nr\n\n3¡\n\nW\n\nW3\n\n«\n\n)\n\ny\n\n«\n\n(c)\n\n%\n\nWW a\n\nv\n\nW #\n\n(c)\n\nW \\\n\n(c)\n\n%\n\na\n\n|\n%W ad\n*3@p\nWW a\n*)\n⎡\n⎡\nA1 =\n⎥\n⎦\nA2 =\n⎡\nA3 =\n66 1\n0 ⎢⎢\nAn =\n0 ⎢⎢\n⎢\n⎣\n⎢⎣\n666 0\n3 ·\n⎢⎢⎢\n⎣\n0 ·\n·\n!\nWq\n\nP¡\n\n(c)¡«\n\n¤+ A2\n(c) A3\n$\n!\n\n(c)¡«\n\n+ An\n\nDn\n|\n\n+\nW\n+<*)sp* (c)\nWa\n3«\nLp\nqr\n\n(\n\n3«$~¡ a\n(c) b\n\n(\n\n3¡\n\n+\n\nDn\n\n(c)(\n\n«o3a\n0+W\n()\nDn = a Dn-1 + b Dn-2 .\n!\n\nf3\n\nW ()\n\n¤«\n\nzqr\n⎡\n⎡2\n⎡\nDn\na\nb\nDn-1\n⎣ = 4\n⎣4\n⎣ .\nDn-1\nDn-2\n-\n\nW 3a¬3+\n\n«\n\n*);+\n\n¢(c)5\n\nP(c)«\n\n¤ Dn\nW«0\n%\n%\n!\n&\n+\nW3(c)\n\n(c)\n\n(c) a\n(c) b\n\n)\nW3a\n(c)\\\n)\n\n!\nzq\nW*)$\n\n(\n*)\nW3\n¡\n\n+\n\na\n(c) b\nq\nW p\n\n46( (c) D5\n|\nW\n-\npu\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_ex1s.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/ae27336edabea99bcde8649185987f5b_MIT18_06SCF11_ex1s.pdf",
      "content": "A\ny\n\n=\n\n;\n\n&\n\n;\n\n.\n§\n\n.\n\n!\n&\n\n.\n\n.\n\n.\n\n.\n\nx\n\nU\n\ny\n±\n\n.\n\n.\n\n§\n\nx\n.\n\n°\n.\n\n}\n18.06SC Unit 1 Exam Solutions\nB:D\n\n3n»\nQ§\nsQ» s¡§\nm\n\nn\n2 ¡\nA\nQ\nAx =\n\n:\n\n\"!$#\n%=\n¡§°\nAx =\n\n:&'\n)(\n!+*,=\n\n$=\n\n-!$#\n\nQx0/132Q\n±3±54Q6±37\nQ 2 8\n§\nQ» m\n§°\nn\n§°\n8$«¡§\nr\nA\nx09:\n§°b¡±\n±_ Q±\n» 8\n§ « Ax = 0\n§°;&'\n7;>=<*\"\"#@5 7:=\n>=\nx@?\n°A\n§S§SB2@4±\nL2\n¡ «63\nA\n:¡ DC «\n8f°4 8\n§E3§\n:(c)\nx\nFHGJILKNMPOQGJR\"S\nQx\nAx =\n\n:\nGJR5T\nQ±\n»\nQ§\n=⇒N(A) = {0}\n\nr = n\nJ±\n\n3§\n\nU m = 3\nAx ∈R3\n\nAx =\n\n:\n§\n±\n»\nQ§\n=⇒C(A) = R3\n:\n\nr\n\n< m\n\n¦ VW4+Q X\nX3 83>Y\nm = 3\nr\n¡§°\nm = 3\n= n = 1\nr = n = 2\n:x;Z[3§ N(A) = {0}\n+>» Ax =\n\n: 1\nQ±\n»\nQ§:x\\U: 8]3¦\n»§»3Q±3»\n3Q§V\n\n;\nAx = 0\nU\"X^3_±\n±3\nx = 0\n.Q`\n§\n+a3 x =\nh\ni\nQ x =\n\"\n#\n°a4§°\nb\nQ§^\nn = 1\nQ n = 2\nx\nA\n»±\nD\n\nQ\n\nQ\n\n2 ¡§\n\nc\n63±3\nx\n\nB\nB:D\n\nc 3\n2 « A\n°»\n\n°§ 3 V.2\n¡ 3 I\n. 8\nQ±3±\n\nb 8\n¡ 83Q§\nQ °«x\\Y\nE21\n» «\n4 (\nw )\nQ2\nr}n\nE31\n» «\n3 (\nw )\nQ2\nE23\n» «\nJ r\nQ2\n\n}n\nQxW?\n3§c2 -2 « A-1\n§¦ 2\n\n8 E\n&\n;(\n\nA-1\nx@?\n:¡\n$QXb\n§±+2\n¡ 3\nA\nx@?\n:¡\n$±\n«6\n§b»±\n\nQ L\n3§\nA = LU\nFHGJILKNMPOQGJR\"S\nQx\n_44±X\n¡ 83Q§\nI\nU)>\n¡ A-1 = E23E31E21\n\n-→\n\n-4\n\n-→\n\n=\n\n-\n\n-3\n-→-1\n-1\n-3\n\nA-1\n\n:x\n_44±X\n813§c2\n+«¡\nQ§\n\\2Q\nQ° « I\nU)\nA = E-1\n21 E-1\n31 E-1\n\n-\n\n-\n\n-→\n\n→\n\n→\n\n= A\n\na\n\n-1\n-1\n\n-3\n=\n\nx\nL =\n\n=\n\ny\n\n§\n.\n\n§\nY\nZ\n\nY\nZ\n\nY\nZ\n\n.\n\n=\n\n#\n\n&\n\n.\n\n.\n\n.\n\nY\n.\n\n§\n\nY\n`\n\n.\n\nC\nB\n\n3 3\n\n¡ «63\n°a4§°Q§\nc\nA =\n\nc\n\nQx\n\nc\n:§°b\n81±\n»2\n§64:\n\nA\nx\n\nc\n:§°b\n8$§»±\n±34:\n\nA\nx\n\nc\n:§°\n812@4±3 Q±3» 83Q§\nx\nAx =\n\nc\n\nFHGJILKNMPOQGJR\"S\nQxV ±32@3§:¡\nQ§Wb\nX2\n\nc -3\n-4\n-4\n\nV\n@\naY\n\nc = 3 c -3\n\n432Q §°\nU =\n\nc -3\n-4\n-4\n\n-→R =\n\n:6\nQ C(A)\n7C:(c) s 1Q±3»2\n§\nA\n\n3 ,\n\nc ,\n\nc = 3 c -3 = 0\n§°\nU =\n\n-4\n-4\n\n-→R =\n\n« 81C J§°V\n°\nQ±3»2\n§\nA\ns\n:6\nQ C(A)\n\n,\n\ny\nY\n.\nG\nT\nC\n\n.\n\nG\nT\nC\n\n.\n\nG\nT\nC\n\n.\n\nU\n\nY\n\nU\n\nY\n\n¢\n\n.\n\nU\n\nU\n.\n\n.\n\nU\n\nU\n\n¬\n\n:x\n8$4a>\n±_Q±3» 83Q§b\n32Q\nc = 3\n\n-\n\nN(A) =\nx4\n\n-1\n\n8$4a>\n±_Q±3» 83Q§b\n32Q\nc = 3\n\n-1\n-2\n\nN(A) =\nx2\n+ x4\n\n-1\n\nxV\n§64+> 8\nHU\n3Q§\n\nxp =\n\nQ§74 »±\nQ±\n» 8\n§\n> J§\n«x\n\nQ\n1Q2W4±3 « Q±\n» 8\n§\nc = 3\n-2\n\n+ x4\n\n-1\n\n-1\n-2\n\nQ\n1Q2W4±3 « Q±\n» 8\n§\n\nc = 3\n+ x2\n+ x4\n\n-1\n\nD\nB:D\n\nx\nA\n\n¡ «63-U:¡\n¡ 83Q§\n°\nQ»E:\n\nQ»\n\n§»±\n±34:$\nA\nx\n»44Q AS\n« 8\n§Q§\nA\n¡°\n«\n832 ¡\nR =\nμ (A)\n\nR = 0\n\n63 $±\n±§\n;\nQ 2 8\n§\nQ» 1Q±3»2\n§\nA\nx\n§L\nQa\n«Q\nM\n±\n\n2 «X\nQ»\n»±\n,¡±\n± 8\n2 «.64:x\\U:¡ »4 S\n64:§§°\n\n±3±4Q6±3\n°»°\na±\nQ§\nQ 2\nR\nFHGJILKNMPOQGJR\"S\nQx\nN(A)\n:\n\n§\n§\nM\nIT\\M 2\n§°b¡\n:x\n\nA\nQ±\n»2\n§wU¢U¬.\nA\nQ2\n:¡\nC(A)\n≈\n\nQ±3»2\n§}]3 4 × (\nQ±3»2\n§Nw )\n±\n»2\n§V\n\nx\nA =\n\na\nb\nc\nd\ne\n\n$\nJ\n\n44+\n\n§bQ»±\n\n¡\n\nf\n\n:\n\n>±3Q§\n\nQ2\n\n,\n\n,\n\n,\n\n,\n\n,\n\n.\n\n§\n\n.\n\nZ\n±\n\nY\n?\n\n§\n§\n\n.\n\n±\n.\n\n°\n\n.\n\n°\n\n.\nx\n\n.\n\n.\nx\n`\n\n.\nA\nx\n`\n`\n\n`\n\n.\n\nU\n\n»\n.\n\nx\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_ex2s.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/c224c7a55a1acaf364d0ff4f01b057b2_MIT18_06SCF11_ex2s.pdf",
      "content": "18.06SC Unit 2 Exam Solutions\n1 (24 pts.)\nSuppose q1, q2, q3 are orthonormal vectors in R3 . Find all possible values\nfor these 3 by 3 determinants and explain your thinking in 1 sentence each.\n(a) det q1 q2 q3\n=\n(b) det q1 + q2\nq2 + q3\nq3 + q1\n=\n(c) det q1 q2 q3\ntimes det q2 q3 q1\n=\nSolution.\n(a) The determinant of any square matrix with orthonormal columns (\"orthogonal matrix\")\nis ±1.\n(b) Here are two ways you could do this:\n(1) The determinant is linear in each column:\ndet q1 + q2\nq2 + q3\nq3 + q1\n= det q1\nq2 + q3\nq3 + q1 + det q2\nq2 + q3\nq3 + q1\n= det q1\nq2 + q3\nq3 + det q2\nq3\nq3 + q1\n= det q1\nq2\nq3 + det q2\nq3\nq1\nBoth of these determinants are equal (see (c)), so the total determinant is ±2.\n\n(2) You could also use row reduction. Here's what happens:\ndet q1 + q2\nq2 + q3\nq3 + q1\n= det q1 + q2\n-q1 + q3\nq3 + q1\n= det q1 + q2\n-q1 + q3\n2q3\n= 2 det q1 + q2\n-q1 + q3\nq3\n= 2 det q1 + q2\n-q1\nq3\n= 2 det q2\n-q1\nq3\n= 2 det q1\nq2\nq3\nAgain, whatever det q1 q2 q3\nis, this determinant will be twice that, or ±2.\n(c) The second matrix is an even permutation of the columns of the first matrix (swap\nq1/q2 then swap q2/q3), so it has the same determinant as the first matrix. Whether\nthe first matrix has determinant +1 or -1, the product will be +1.\n\n2 (24 pts.)\nSuppose we take measurements at the 21 equally spaced times t = -10, -9, . . . , 9, 10.\nAll measurements are bi = 0 except that b11 = 1 at the middle time t = 0.\n(a) Using least squares, what are the best Cb and Db to fit those 21 points\nby a straight line C + Dt ?\n(b) You are projecting the vector b onto what subspace ? (Give a basis.)\nFind a nonzero vector perpendicular to that subspace.\nSolution.\n(a) If the line went exactly through the 21 points, then the 21 equations\n\n1 -10\n\n-9\n\n.\n.\n\n.\n.\n.\n\n=\n.\n.\n.\n\nC\n.\n\nD\n\n.\n.\n\n.\n..\n.\n\n.\n\n.\n\n.\n\nwould be exactly solvable. Since we can't solve this equation Ax = b exactly, we look\nfor a least-squares solution ATAxˆ = ATb.\n\nCb\n=\n\nDb\nSo the line of best fit is the horizontal line Cb = 1 D = 0.\n21 , b\nh\niT h\niT\n(b) We are projecting b onto the column space of A above (basis:\n1 . . .\n,\n-10 . . .\n).\nThere are lots of vectors perpendicular to this subspace; one is the error vector e =\n1 h\niT\nb -PAb = 21 (ten -1's)\n20 (ten -1's)\n.\n\n3 (9 + 12 + 9 pts.)\nThe Gram-Schmidt method produces orthonormal vectors q1, q2, q3\nfrom independent vectors a1, a2, a3 in R5 . Put those vectors into the columns\nof 5 by 3 matrices Q and A.\n(a) Give formulas using Q and A for the projection matrices PQ and PA\nonto the column spaces of Q and A.\n(b) Is PQ = PA and why ? What is PQ times Q ? What is det PQ ?\n(c) Suppose a4 is a new vector and a1, a2, a3, a4 are independent. Which of\nthese (if any) is the new Gram-Schmidt vector q4 ? (PA and PQ from\nabove)\nT\nT\nT\nT\na a2\na a3\na a2\na a3\n∥PQa4∥\n∥ norm of that vector ∥\n∥a4 -PAa4∥\nSolution.\n(a) PA = A(ATA)-1AT and PQ = Q(QTQ)-1QT = QQT .\n(b) PA = PQ because both projections project onto the same subspace. (Some people did\nthis the hard way, by substituting A = QR into the projection formula and simplifying.\nThat also works.) The determinant is zero, because PQ is singular (like all non-identity\nprojections): all vectors orthogonal to the column space of Q are projected to 0.\n(c) Answer: choice 3. (Choice 2 is tempting, and would be correct if the ai were replaced\nby the qi. But the ai are not orthogonal!)\nT\na a1\na4 -\na1 -\na2 -\na3\nPQa4\na4 -PAa4\nT\na a1\n1.\n2.\n3.\n\n4 (22 pts.)\nSuppose a 4 by 4 matrix has the same entry × throughout its first row and\ncolumn. The other 9 numbers could be anything like 1, 5, 7, 2, 3, 99, π, e, 4.\n\n×\n×\n×\n×\n\n×\nany numbers\nA =\n\n×\nany numbers\n\n×\nany numbers\n(a) The determinant of A is a polynomial in ×.\nWhat is the largest\npossible degree of that polynomial ? Explain your answer.\n(b) If those 9 numbers give the identity matrix I, what is det A ? Which\nvalues of × give det A = 0 ?\n\n×\n×\n×\n×\n\nA =\n\n× 1\n\n× 0\n\n× 0\nSolution.\n(a) Every term in the big formula for det(A) takes one entry from each row and column,\nso we can choose at most two ×'s and the determinant has degree 2.\n(b) You can find this by cofactor expansion; here's another way:\ndet(A) = × det\n\n×\n×\n×\n1 -3×\n\n= × det\n\n×\n×\n×\n\n= ×(1-3×) det\n\n1 0\n\n= ×(1 -3×).\nThis is zero when × = 0 or × = 1\n3 .\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_ex3s.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/92b86aaa4f5f4cce178c69faeb52039e_MIT18_06SCF11_ex3s.pdf",
      "content": "18.06SC Unit 3 Exam Solutions\n1 (34 pts.) (a) If a square matrix A has all n of its singular values equal to 1 in the\nSVD, what basic classes of matrices does A belong to ? (Singular,\nsymmetric, orthogonal, positive definite or semidefinite, diagonal)\n(b) Suppose the (orthonormal) columns of H are eigenvectors of B:\n⎡\n⎤\n1 -1 -1\n⎢\n⎥\n⎢\n⎥\n1 ⎢ 1 -1 -1\n1 ⎥\nH =\n⎢\n⎥\nH-1 = HT\n2 ⎢\n⎥\n⎢ 1\n1 ⎥\n⎣\n⎦\n1 -1\n1 -1\nThe eigenvalues of B are λ = 0, 1, 2, 3. Write B as the product of 3\nspecific matrices. Write C = (B + I)-1 as the product of 3 matrices.\n(c) Using the list in question (a), which basic classes of matrices do B and\nC belong to ? (Separate question for B and C)\n\nSolution.\n(a) If σ = I then A = UV T = product of orthogonal matrices = orthogonal matrix.\n2nd proof: All σi = 1 implies ATA = I. So A is orthogonal.\n⎡\n⎤\n0 -1\n⎣\n⎦\n(A is never singular, and it won't always be symmetric -- take U =\nand\nV = I, for example. This also shows it can't be diagonal, or positive definite or\nsemidefinite.)\n⎡\n⎤\n⎢\n⎥\n⎢\n⎥\n⎢\n⎥\n⎢\n⎥\n(b) B = HΛH-1 with Λ = ⎢\n⎥\n⎢\n⎥\n⎣\n⎦\n⎡\n⎤\n⎢\n⎥\n⎢\n⎥\n⎢\n1/2\n⎥\n⎢\n⎥\n(B+I)-1 = H(Λ+I)-1H-1 with (same eigenvectors) (Λ+I)-1 = ⎢\n⎥\n⎢\n1/3\n⎥\n⎣\n⎦\n(c) B is singular, symmetric, positive semidefinite.\nC is symmetric positive definite.\n1/4\n\n2 (33 pts.) (a) Find three eigenvalues of A, and an eigenvector matrix S:\n\n⎡\n⎤\n-1\n⎢\n⎥\nA = ⎢⎢ 0\n5 ⎥⎥\n⎣\n⎦\n\n(b) Explain why A1001 = A. Is A1000 = I ? Find the three diagonal entries\nof eAt .\n(c) The matrix ATA (for the same A) is\n⎡\n⎤\n-2\n-4\n⎢\n⎥\n⎢\n⎥\nATA = ⎢ -2\n8 ⎥ .\n⎣\n⎦\n-4\n\nHow many eigenvalues of ATA are positive ? zero ? negative ? (Don't\ncompute them but explain your answer.) Does ATA have the same\neigenvectors as A ?\n\nSolution.\n(a) The eigenvalues are -1, 0, 1 since A is triangular.\n\n⎡\n⎤\n\n⎡\n⎤\n\n⎡\n⎤\n\nλ = -1 has x =\n⎢\n⎥\n⎢0 ⎥\nλ = 0 has x =\n⎢\n⎥\n⎢1 ⎥\nλ = 1 has x =\n⎢\n⎥\n\n⎢\n⎥\n\n⎢\n⎥\n\n⎣\n⎦\n⎢5 .\n\n⎢\n⎣\n⎦\n⎣\n⎦\n⎥⎥\nThose vectors x are the columns of S (upper triangular!).\n(b) A =\nΛS-1 and A1001 = SΛ1001S-1. Notice Λ 1001 = Λ, A1000 = I (A is singular)\n(01000 = 0 = 1).\neAt has e-1t , e0t = 1, et on its diagonal. Proof using series:\n\ninf\n0 (At)n/n! has triangular matrices so the diagonal has (\n\n-t)n/n! = e-t,\n0n/n! =\n\n1, tn/n! = et .\n\nProof using SΛS-1:\n\n1 ×\n×\ne-t\n1 ×\n×\n\nAt\ne\n= SeΛt\n-1\n⎡\n⎤⎡\n⎤⎡\n⎤\nS\n=\n⎢⎢0\n×\n⎥⎥\n⎢⎢\n⎥⎥\n⎢⎢\n× ⎥⎥\n.\n⎢\n⎥⎢\n⎥⎢\n⎥\n⎣\n⎦\n\n⎣\net\n⎦⎣\n⎦\n\n(c) ATA has 2 positive eigenvalues (it has rank 2, its eigenvalues can never be negative).\nOne eigenvalue is zero because ATA is singular. And 3 -2 = 1.\n(Or: ATA is symmetric, so the eigenvalues have the same signs as the pivots.\nDo elimination: the pivots are 1, 0, and 42 -16 = 26.)\n\n-\n\n3 (33 pts.) Suppose the n by n matrix A has n orthonormal eigenvectors q1, . . . , qn and\nn positive eigenvalues λ1, . . . , λn. Thus Aq\n\nj = λj qj.\n(a) What are the eigenvalues and eigenvectors of A-1 ? Prove that your\nanswer is correct.\n(b) Any vector b is a combination of the eigenvectors:\nb = c1q1 + c2q2 + · · · + cnqn .\nWhat is a quick formula for c1 using orthogonality of the q's ?\n(c) The solution to Ax = b is also a combination of the eigenvectors:\nA-1b = d1q1 + d2q2 + · · · + dnqn .\nWhat is a quick formula for d1 ? You c an u se t he c 's even if you didn't\nanswer part (b).\n\nSolution.\n(a) A-1 has eigenvalues\nwith the same eigenvectors\nλj\nAqj =\n\nλjqj -→ qj = λ A-1\nj\nqj -→ A-1qj =\nqj .\nλj\n(b) Multiply b = c1q\nT\n1 + · · · + cnqn by q1 .\nqTb\nOrthogonality gives qT\n1 b = c1qT\n1 q1 so c\n1 =\n= qTb.\nqT\n1 q1\n(c) Multiplying b by A-1 will multiply each qi by\n(part (a)). So ci becomes\nλi\n\nc\nT\nT\nd =\nq1 b\nq1 b\n\n=\nor\n\n.\nλ1\nλ1qT\n1 q1\nλ1\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_final_exs.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/03fa86c672cab6cce168955824c92f6e_MIT18_06SCF11_final_exs.pdf",
      "content": "18.06SC Final Exam Solutions\n\n1 (4+7=11 pts.)\nSuppose A is 3 by 4, and Ax = 0 has exactly 2 special solutions:\n\n-2\n\nx1 =\n\nand\nx2 =\n\n-1\n\n(a) Remembering that A is 3 by 4, find its row reduced echelon form R.\n(b) Find the dimensions of all four fundamental subspaces C(A), N(A),\nC(AT), N(AT).\nYou have enough information to find bases for one or more of these\nsubspaces--find those bases.\n\nSolution.\n(a) Each special solution tells us the solution to Rx = 0 when we set one free variable = 1\nand the others = 0. Here, the third and fourth variables must be the two free variables,\n\n∗\n∗\nand the other two are the pivots: R =\n∗\n∗\n\n-1\nNow multiply out Rx1 = 0 and Rx2 = 0 to find the ∗'s: R =\n-1\n\n(The ∗'s are just the negatives of the special solutions' pivot entries.)\n(b) We know the nullspace N(A) has n -r = 4 -2 = 2 dimensions: the special solutions\nx1, x2 form a basis.\nThe row space C(AT) has r = 2 dimensions. It's orthogonal to N(A), so just pick\ntwo linearly-independent vectors orthogonal to x1 and x2 to form a basis: for example,\n\nx3 =\n\n, x4 =\n\n.\n-1\n-1\n\n(Or: C(AT) = C(RT) is just the row space of R, so the first two rows are a basis.\nSame thing!)\nThe column space C(A) has r = 2 dimensions (same as C(AT)). We can't write down\na basis because we don't know what A is, but we can say that the first two columns of\nA are a basis.\nThe left nullspace N(AT) has m -r = 1 dimension; it's orthogonal to C(A), so any\nvector orthogonal to the first two columns of A (whatever they are) will be a basis.\n\n2 (6+3+2=11 pts.)\n(a) Find the inverse of a 3 by 3 upper triangular matrix U, with\nnonzero entries a, b, c, d, e, f. You could use cofactors and the formula\nfor the inverse. Or possibly Gauss-Jordan elimination.\n\na\nb\nc\nFind the inverse of U =\nd\ne\n.\n\nf\n(b) Suppose the columns of U are eigenvectors of a matrix A. Show that\nA is also upper triangular.\n(c) Explain why this U cannot be the same matrix as the first factor in\nthe Singular Value Decomposition A = UΣV T .\n\n(a)\n\nSolution.\nBy elimination: (We keep track of the elimination matrix E on one side, and the\nproduct EU on the other. When EU = I, then E = U-1.)\n\na\nb\nc\nb/a\nc/a\n1/a\n\nd\ne\ne/d\n1/d\n\nf\n1/f\n\n1/a\n-b/ad\n(be -cd)/adf\n\n1/d\n-e/df\n=\nh\nI\nU-1\ni\n\n1/f\nBy cofactors: (Take the minor, then \"checkerboard\" the signs to get the cofactor matrix,\nthen transpose and divide by det(U) = adf.)\n\na\nb\nc\ndf\ndf\ndf\n-bf\nbe -cd\n\nd\ne\n\nbf\naf\n\n-bf\naf\naf\n-ae\n\nf\nbe -cd\nae\nad\nbe -cd\n-ae\nad\nad\n\n1/a\n-b/ad\n(be -cd)/adf\n\nU-1\n\n1/d\n-e/df =\n\n1/f\n(b) We have a complete set of eigenvectors for A, so we can diagonalize: A = UΛU-1 . We\nknow U is upper-triangular, and so is the diagonal matrix Λ, and we've just shown\nthat U-1 is upper-triangular too. So their product A is also upper-triangular.\nT\n(c) The columns aren't orthogonal!\n(For example, the product u1 u2 of the first two\ncolumns is ab + 0d + 0 · 0 = ab, which is nonzero because we're assuming all the\nentries are nonzero.)\n\nh\ni\nh\ni\n3 (3+3+5=11 pts.)\n(a) A and B are any matrices with the same number of rows.\nWhat can you say (and explain why it is true) about the comparison\nof\nrank of A\nrank of the block matrix\nA\nB\n(b) Suppose B = A2 . How do those ranks compare ? Explain your reason\ning.\n(c) If A is m by n of rank r, what are the dimensions of these nullspaces ?\nNullspace of A\nNullspace of\nA\nA\nSolution.\n(a) All you can say is that rank A ≤ rank [A B]. (A can have any number r of pivot\ncolumns, and these will all be pivot columns for [A B]; but there could be more pivot\ncolumns among the columns of B.)\n(b) Now rank A = rank [A A2]. (Every column of A2 is a linear combination of columns\nof A. For instance, if we call A's first column a1, then Aa1 is the first column of A2 .\nSo there are no new pivot columns in the A2 part of [A A2].)\n(c) The nullspace N(A) has dimension n - r, as always. Since [A A] only has r pivot\ncolumns -- the n columns we added are all duplicates -- [A A] is an m-by-2n matrix\nof rank r, and its nullspace N ([A A]) has dimension 2n -r.\n\n4 (3+4+5=12 pts.)\nSuppose A is a 5 by 3 matrix and Ax is never zero (except when\nx is the zero vector).\n(a) What can you say about the columns of A ?\n(b) Show that ATAx is also never zero (except when x = 0) by explaining\nthis key step:\nIf ATAx = 0 then obviously xTATAx = 0 and then (WHY ?) Ax = 0.\n(c) We now know that ATA is invertible. Explain why B = (ATA)-1AT\nis a one-sided inverse of A (which side of A ?). B is NOT a 2-sided\ninverse of A (explain why not).\nSolution.\n(a) N(A) = 0 so A has full column rank r = n = 3: the columns are linearly independent.\n(b) xTATAx = (Ax)T Ax is the squared length of Ax. The only way it can be zero is if\nAx has zero length (meaning Ax = 0).\n(c) B is a left inverse of A, since BA = (ATA)-1ATA = I is the (3-by-3) identity matrix.\nB is not a right inverse of A, because AB is a 5-by-5 matrix but can only have rank 3.\n(In fact, BA = A(ATA)-1AT is the projection onto the (3-dimensional) column space\nof A.)\n\n5 (5+5=10 pts.)\nIf A is 3 by 3 symmetric positive definite, then Aqi = λiqi with\npositive eigenvalues and orthonormal eigenvectors qi.\nSuppose x = c1q1 + c2q2 + c3q3.\n(a) Compute xTx and also xTAx in terms of the c's and λ's.\n(b) Looking at the ratio of xTAx in part (a) divided by xTx in part (a),\nwhat c's would make that ratio as large as possible ? You can assume\nλ1 < λ2 < . . . < λn. Conclusion: the ratio xTAx/xTx is a maximum\nwhen x is\n.\nSolution.\n(a)\nx T x\n=\n(c1q1\nT + c2q2\nT + c3q3\nT)(c1q1 + c2q2 + c3q3)\n2 T\nT\nT\n2 T\n=\nc1q1 q1 + c1c2q1 q2 + · · · + c3c2q3 q2 + c3q3 q3\n=\nc1 + c2 + c3.\nx TAx\n=\n(c1q1\nT + c2q2\nT + c3q3\nT)(c1Aq1 + c2Aq2 + c3Aq3)\n=\n(c1q1\nT + c2q2\nT + c3q3\nT)(c1λ1q1 + c2λ2q2 + c3λ3q3)\n=\nc1\n2λ1q1\nT q1 + c1c2λ2q1\nT q2 + · · · + c3c2λ2q3\nT q2 + c3\n2λ3q3\nT q3\n=\nc1\n2λ1 + c2\n2λ2 + c3\n2λ3.\n(b) We maximize (c2\n1λ1 + c2\n2λ2 + c2\n3λ3)/(c2\n1 + c2\n2 + c2\n3) when c1 = c2 = 0, so x = c3q3 is a\nmultiple of the eigenvector q3 with the largest eigenvalue λ3.\n(Also notice that the maximum value of this \"Rayleigh quotient\" xTAx/xTx is the\nlargest eigenvalue itself. This is another way of finding eigenvectors: maximize xTAx/xTx\nnumerically.)\n\n6 (4+4+4=12 pts.)\n(a) Find a linear combination w of the linearly independent vec\ntors v and u that is perpendicular to u.\n(b) For the 2-column matrix A =\nh\nu\nv\ni\n, find Q (orthonormal columns)\nand R (2 by 2 upper triangular) so that A = QR.\n(c) In terms of Q only, using A = QR, find the projection matrix P onto\nthe plane spanned by u and v.\nSolution.\n(a) You could just write down w = 0u+0v = 0 -- that's perpendicular to everything! But\na more useful choice is to subtract offjust enough u so that w = v-cu is perpendicular\nto u. That means 0 = wTu = vTu -cuTu, so c = (vTu)/(uTu) and\nT\nv u\nw = v -(\n)u.\nuTu\n(b) We already know u and w are orthogonal; just normalize them! Take q1 = u/|u| and\n\n|u|\nq2 = w/|w|. Then solve for the columns r1, r2 of R: Qr1 = u so r1 =\n, and\n\nc|u|\nQr2 = v so r2 =\n. (Where c = (vTu)/(uTu) as before.)\n|w|\nThen Q = [q1 q2] and R = [r1 r2].\n(c) P = A(ATA)-1AT = (QR)(RTQTQR)-1(RTQT) = (QR)(RTQT) = QQT .\n\n7 (4+3+4=11 pts.)\n(a) Find the eigenvalues of\n\nC =\n\nand\nC2 =\n\n.\n\n(b) Those are both permutation matrices. What are their inverses C-1\nand (C2)-1 ?\n(c) Find the determinants of C and C + I and C + 2I.\n\nSolution.\n(a) Take the determinant of C -λI (I expanded by cofactors): λ4 -1 = 0. The roots of\nthis \"characteristic equation\" are the eigenvalues: +1, -1, i, -i.\nThe eigenvalues of C2 are just λ2 = ±1 (two of each).\n(Here's a \"guessing\" approach. Since C4 = I, all the eigenvalues λ4 of C4 are 1: so\nλ = 1, -1, i, -i are the only possibilities. Just check to see which ones work. Then the\neigenvalues of C2 must be ±1.)\n(b) For any permutation matrix, C-1 = CT: so\n\nC-1 =\n\nand (C2)-1 = C2 is itself.\n(c) The determinant of C is the product of its eigenvalues: 1(-1)i(-i) = -1.\nAdd 1 to every eigenvalue to get the eigenvalues of C +I (if C = SΛS-1, then C +I =\nS(Λ + I)S-1): 2(0)(1 + i)(1 -i) =\n(Or let λ = -1 in the characteristic equation det(C -λI).)\nAdd 2 to get the eigenvalues of C + 2I (or let λ = -2): 3(1)(2 + i)(2 -i) = 15.\n0.\n\n(a) How do you find the best least squares solution xb to Ax = b ? By\n\n8 (4+3+4=11 pts.)\nSuppose a rectangular matrix A has independent columns.\ntaking those steps, give me a formula (letters not numbers) for xb and\nalso for p = Axb.\n(b) The projection p is in which fundamental subspace associated with A ?\nThe error vector e = b -p is in which fundamental subspace ?\n(c) Find by any method the projection matrix P onto the column space\nof A:\n\nA =\n\n.\n-1\n\n-3\nSolution.\n(a)\nAx\n=\nb\nLeast-squares \"solution\":\nATAxˆ\n=\nATb\nATA is invertible:\nxˆ\n=\n(ATA)-1ATb\nand p = Axˆ is:\nAxˆ\n=\nA(ATA)-1ATb\n(b) p = Axˆ is a linear combination of columns of A, so it's in the column space C(A). The\nerror e = b -p is orthogonal to this space, so it's in the left nullspace N(AT).\n\n1/10\n(c) I used P = A(ATA)-1AT . Since ATA =\n, its inverse is\n=\n1 I,\n\n1/10\nand\n\nP = 1\n\n9 (3+4+4=11 pts.)\nThis question is about the matrices with 3's on the main diagonal,\n2's on the diagonal above, 1's on the diagonal below.\n\nA1 =\nh\ni\nA2 =\n\nA3 = 1\nAn =\n·\n\n·\n·\n(a) What are the determinants of A2 and A3 ?\n(b) The determinant of An is Dn. Use cofactors of row 1 and column 1 to\nfind the numbers a and b in the recursive formula for Dn:\n(∗)\nDn = a Dn-1 + b Dn-2 .\n(c) This equation (∗) is the same as\n\nDn\na\nb\nDn-1\n\n=\n\n.\nDn-1\nDn-2\n>From the eigenvalues of that matrix, how fast do the determinants\nDn grow ? (If you didn't find a and b, say how you would answer part\n(c) for any a and b ) For 1 point, find D5.\n\nSolution.\n(a) det(A2) = 3 · 3 -1 · 2 = 7 and det(A3) = 3 det(A2) -2 · 1 · 3 = 15.\n(b) Dn = 3Dn-1 + (-2)Dn-2. (Show your work.)\n(c) The trace of that matrix A is a = 3, and the determinant is -b = 2. So the character\nistic equation of A is λ2 -aλ -b = 0, which has roots (the eigenvalues of A)\na ±\np\na2 -4(-b)\n3 ± 1\nλ± =\n=\n= 1 or 2.\nDn grows at the same rate as the largest eigenvalue of An , λn = 2n .\n+\nThe final point: D5 = 3D4 + 2D3 = 3(3D3 + 2D2) + 2D3 = 11D3 + 6D2 = 207.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "ZoomNotes for Linear Algebra",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/c501620f128ab205bc267770934d707a_MIT18_06SCF11_ZoomNotes.pdf",
      "content": "ZOOMNOTES FOR\nLINEAR ALGEBRA\nGILBERT STRANG\nMassachusetts Institute of Technology\nWELLESLEY - CAMBRIDGE PRESS\nBox 812060 Wellesley MA 02482\n\nZoomNotes for Linear Algebra\nCopyright (c)2021 by Gilbert Strang\nISBN 978-1-7331466-4-7\nLATEX typesetting by Ashley C. Fernandes\nPrinted in the United States of America\n9 8 7 6 5 4 3 2 1\nTexts from Wellesley - Cambridge Press\nLinear Algebra for Everyone, 2020, Gilbert Strang\nISBN 978-1-7331466-3-0\nLinear Algebra and Learning from Data, 2019, Gilbert Strang\nISBN 978-0-6921963-8-0\nIntroduction to Linear Algebra, 5th Ed., 2016, Gilbert Strang\nISBN 978-0-9802327-7-6\nComputational Science and Engineering, Gilbert Strang\nISBN 978-0-9614088-1-7\nDifferential Equations and Linear Algebra, Gilbert Strang\nISBN 978-0-9802327-9-0\nWavelets and Filter Banks, Gilbert Strang and Truong Nguyen\nISBN 978-0-9614088-7-9\nIntroduction to Applied Mathematics, Gilbert Strang\nISBN 978-0-9614088-0-0\nCalculus Third Edition, Gilbert Strang\nISBN 978-0-9802327-5-2\nAlgorithms for Global Positioning, Kai Borre & Gilbert Strang\nISBN 978-0-9802327-3-8\nEssays in Linear Algebra, Gilbert Strang\nISBN 978-0-9802327-6-9\nAn Analysis of the Finite Element Method, 2008 edition, Gilbert Strang and George Fix\nISBN 978-0-9802327-0-7\nWellesley - Cambridge Press\nBox 812060, Wellesley MA 02482 USA\nwww.wellesleycambridge.com\nGilbert Strang's page : math.mit.edu/∼gs\nFor orders : math.mit.edu/weborder.php\nOutside US/Canada : www.cambridge.org\nSelect books, India : www.wellesleypublishers.com\nThe textbook websites are math.mit.edu/linearalgebra and math.mit.edu/everyone.\nThose sites link to 18.06 course materials and video lectures on YouTube and OCW.\nSolution Manuals can be printed from those sites and math.mit.edu/learningfromdata.\nLinear Algebra is included in MIT's OpenCourseWare site ocw.mit.edu/courses.\nThis provides video lectures of the full linear algebra courses 18.06 and 18.06 SC and 18.065.\n\nZoomNotes for Linear Algebra : Gilbert Strang\nPreface\nTextbooks, ZoomNotes, and Video Lectures\nThree Great Factorizations : LU and QR and SVD\nPart 1 : Basic Ideas of Linear Algebra\nPart 2 : Solving Linear Equations Ax = b : A is n by n\nPart 3 : Vector Spaces and Subspaces, Basis and Dimension\nPart 4 : Orthogonal Matrices and Least Squares\nPart 5 : Determinant of a Square Matrix\nPart 6 : Eigenvalues and Eigenvectors : Ax = λx and Anx = λnx\nPart 7 : Singular Values and Vectors : Av =σu and A=UΣV T\nPart 8 : Linear Transformations and Their Matrices\nPart 9 : Complex Numbers and the Fourier Matrix\nPart 10 : Learning from Data : Minimize Loss by Gradient Descent\nPart 11 : Basic Statistics : Mean, Variance, Covariance\niii\n\nPreface\nThe title \"ZoomNotes\" indicates that these pages were created in 2020 and 2021. But they are not\nlimited to online lectures. I hope these notes will help instructors and students to see linear algebra\nin an organized way, from vectors to matrices to subspaces to bases. \"Linear independence\" is a\ncrucial idea for this subject, so it comes early--for vectors of integers.\nI hope that faculty who are planning a linear algebra course and students who are reading for\nthemselves will see these notes.\nA happy part of linear algebra is the wonderful variety of matrices--diagonal, triangular,\nsymmetric, orthogonal, and many more. The organizing principles have become matrix factoriza-\ntions like A = LU (lower triangular times upper triangular). The idea of elimination--to simplify\nthe equations Ax = b by introducing zeros in the matrix--appears early as it must. Please don't\nspend forever on those computations. Linear algebra has so many more good ideas.\nThe reader may know my video lectures on OpenCourseWare : Math 18.06 is on ocw.mit.edu and\non Youtube/mitocw. I am so grateful that those have been helpful. Now I have realized that\nlecture notes can help in a different way. You will quickly gain a picture of the whole course--\nthe structure of the subject, the key topics in a natural order, the connecting ideas that make linear\nalgebra so beautiful. This structure is the basis of two textbooks from Wellesley-Cambridge Press :\nIntroduction to Linear Algebra\nLinear Algebra for Everyone\nI don't try to teach every topic in those books. I do try to reach eigenvalues and singular values !\nA basis of eigenvectors for square matrices--and of singular vectors for all matrices--takes you\nto the heart of a matrix in a way that elimination cannot do.\nThe last chapters of these notes extend to a third book and a second math course 18.065\nwith videos on OpenCourseWare:\nLinear Algebra and Learning from Data\n(Wellesley-Cambridge Press 2019)\nThis is \"Deep Learning\" and it is not entirely linear. It creates a learning function F (x, v)\nfrom training data v (like images of handwritten numbers) and matrix weights x. The piecewise\nlinear \"ReLU function\" plays a mysterious but crucial part in F . Then F (x, vnew) can come close\nto new data that the system has never seen.\nThe learning function F (x, v) grows out of linear algebra and optimization and statistics and\nhigh performance computing. Our aim is to understand (in part) why it succeeds.\nAbove all, I hope these ZoomNotes help you to teach linear algebra and learn linear algebra.\nThis subject is used in so many valuable ways. And it rests on ideas that everyone can understand.\nThank you.\nGilbert Strang\n\nTextbooks, ZoomNotes, and Video Lectures\nIntroduction to Linear Algebra, 5th Ed. (2016)\nLinear Algebra and Learning from Data (2019)\nLinear Algebra for Everyone (2020)\nDifferential Equations and Linear Algebra (2014)\nZoomNotes for Linear Algebra (2021)\nmath.mit.edu/linearalgebra\nmath.mit.edu/learningfromdata\nmath.mit.edu/everyone\nmath.mit.edu/dela\nVideo Lectures\nOpenCourseWare ocw.mit.edu/courses\nyoutube/mitocw\nMath 18.06 and 18.06SC\nLinear Algebra at MIT\n(added to 18.06)\nA 2020 Vision of Linear Algebra\nMath 18.065\nLinear Algebra and Learning from Data\nMath 18.085 and 18.086\nComputational Science and Engineering\nStrang and Moler\nDifferential Equations and Linear Algebra\nInterview with Lex Fridman https://www.youtube.com/watch?v=lEZPfmGCEk0\nWellesley - Cambridge Press\nBox 812060, Wellesley MA 02482 USA\nwww.wellesleycambridge.com\nGilbert Strang's page : math.mit.edu/∼gs\nOutside US/Canada : www.cambridge.org\nOrders : math.mit.edu/weborder.php\nSelect books, India : www.wellesleypublishers.com\n\nThree Great Factorizations : LU, QR, SVD\nOrthogonal matrix\nQTQ = I\nSquare QQT = I\nQ =\n\nq1\nq2\n· · ·\nqn\n\nOrthogonal basis\nTriangular matrix\nRij = 0 for i > j\nRjj = 0 on diagonal\nR =\n\nr11\nr12\n·\nr1n\nr22\n·\nr2n\n·\n·\nrnn\n\nTriangular basis\n1. A = LU = (lower triangular) (upper triangular): Elimination\n2. A = QR = (orthogonal) (upper triangular): Gram-Schmidt\n3. A = UΣV T = (orthogonal)(diagonal)(orthogonal): Singular values\nChapters 2, 4, 7\nrow\nspace\nof A\ncolumn\nspace\nof A\nU and R and V\ninput basis\nL and Q and U\noutput basis\n\nPart 1\nBasic Ideas of Linear Algebra\n1.1\nLinear Combinations of Vectors\n1.2\nDot Products v · w and Lengths ||v|| and Angles θ\n1.3\nMatrices Multiplying Vectors\n1.4\nColumn Space and Row Space of A\n1.5\nDependent and Independent Columns\n1.6\nMatrix-Matrix Multiplication AB\n1.7\nFactoring A into CR : Column rank = r = Row rank\n1.8\nRank one matrices\nA = (1 column) times (1 row)\n\nPart 1 : Basic Ideas of Linear Algebra\n1.1\nLinear Combinations of Vectors\nA 3-dimensional vector v =\n\nv1\nv2\nv3\n\nhas 3 components v1, v2, v3 as in v =\n\nv gives a point in 3-dimensional space R3. Think of an arrow from (0, 0, 0) to (2, 4, 1).\nWe add vectors v + w. We multiply them by numbers like c = 4 and d = 0 (called scalars)\n\n+\n\n-2\n\n=\n\n=\n\n-2\n\n=\n\n= zero vector\nLinear combinations\n2v -3w\nand\ncv + dw\nand\nw -2z + u\n\n-3\n\n=\n\n-2\n\n+ 1\n\n=\n\nAllow every c, d or all c, d, e\nAll combinations of v and w usually (!) fill a plane in R3\nAll c\n\n+ d\n\nfill a plane\nAll c\n\n+ d\n\n+ e\n\nfill 3D space R3\nSometimes a combination gives the zero vector. Then the vectors are dependent.\nAll c\n\n+ d\n\nonly fill a line.They are all multiples of\n\n.This includes\n\n-3\n-4\n-5\n\nAll c\n\n+ d\n\n+ e\n\nonly fill a plane and not 3D space.\nThat third vector is nothing new.\n\nis 2\n\n-\n\n.\n\nZoomNotes for Linear Algebra\n1.2\nDot Products v · w and Lengths ||v|| and Angles θ\nDot product v · w =\n\n·\n\n=\n3 × 2\n4 × 0\n5 × 1\nadd\n= 11\nv · w = w · v\n\na\nb\n\n·\n\nc\nd\n\n= ac + bd\nLength squared of v=\n\nis ||v||2 = 32 + 42 = 9 + 16. This is Pythagoras c2 = a2 + b2\nLength squared of v is ||v||2 = v · v =\n\n·\n\n= 9 + 16 + 25 (Pythagoras in 3D)\nLength squared of v + w is (v + w) · (v + w) = v · v + v · w + w · v + w · w\n||v + w||2 = ||v||2 + ||w||2 + 2v · w\n||v + w|| ≤||v|| + ||w||\nv =\n\nw =\n\n-1\n\nv + w =\n\nLength squared\nof v + w is\n42 + 42 + 42\n48 is 50 + 2 + 2 v · w\nTriangle has\nedges v, w, v -w\n||v -w||2 = ||v||2 + ||w||2 -2v · w\nw\nv -w\nv\nThe dot product v · w reveals the angle θ between v and w\n| cos θ| ≤1 is one way to see the Schwarz inequality\nv · w = ||v|| ||w|| cos θ\n|v · w| ≤||v|| ||w||\nThe angle between v=\n\n-1\n\nand w=\n\n-1\n\nis θ = 90*\nbecause\nv · w = 0 : Perpendicular\nThe angle between v=\n\nand w=\n\nis θ =45*because v · w=1 and ||v|| ||w||=\n√\n2.\n\nPart 1 :\nBasic Ideas of Linear Algebra\n1.3\nMatrices Multiplying Vectors\nThere is a row way to multiply Ax and also a column way to compute the vector Ax\nRow way = Dot product of vector x with each row of A\nAx =\nv1\nv2\n\n=\n2v1 + 5v2\n3v1 + 7v2\n\n=\n\nColumn way = Ax is a combination of the columns of A\nAx=\n\nv1\nv2\n\n= v1\n\ncolumn\n\n+ v2\n\ncolumn\n\n=\n\n+\n\n=\n\nWhich way to choose? Dot products with rows or combination of columns ?\nFor computing with numbers, I use the row way : dot products\nFor understanding with vectors, I use the column way : combine columns\nSame result Ax from the same multiply-adds. Just in a different order\nC(A) = Column space of A = all combinations of the columns = all outputs Ax\nThe identity matrix has Ix = x for every x\n\nx1\nx2\nx3\n\n=\n\nx1\nx2\nx3\n\nThe column space of the 3 by 3 identity matrix I is the whole space R3.\nIf all columns are multiples of column 1 (not zero), the column space C(A) is a line.\nLine containing all cu\ncu = -u/2\nu\nPlane from\nall cu + dv\nu\nv\n\nZoomNotes for Linear Algebra\n1.4\nColumn Space and Row Space of A\nThe column space of A contains all linear combinations of the columns of A\nAll the vectors Ax (for all x) fill the column space C(A) : line or plane or . . .\nIf v is in C(A) so is every c v. [Reason : v = Ax gives cv = A(c x)]\nIf v1 and v2 are in C(A) so is v1 + v2 [v1 = Ax1 and v2 = Ax2 give v1 + v2 =\nA(x1 + x2)]\nThe column spaces of\n\nand\n\nand\n\nare the whole R2\nThe column spaces of\n\nand\n\nare lines inside 2-dimensional space\nThe column space of Z =\n\nhas C(Z) = only one point\n\n.\nThe row space of A contains all combinations of the rows of A\nTo stay with column vectors, transpose A to make its rows into columns of AT\nThen the row space of A is the column space of AT (A transpose)\nThe column space of A =\n\n1 2 3\n3 6 9\n\nis an infinite line in the direction of\n\nThe row and column spaces of A=\n\n1 2 3\n1 3 4\n1 4 5\n\nare infinite planes. Not all of R3\nThe row and column spaces of A =\n\n1 2 3\n0 4 5\n0 0 6\n\nare the whole R3.\nA=\n\n1 2 5 1\n3 4 6 7\n\nhas column space = R2\nhas row space = 2D plane in R4\n\nPart 1 :\nBasic Ideas of Linear Algebra\n1.5\nDependent and Independent Columns\nThe columns of A are \"dependent\" if one column is a combination of the other columns\nAnother way to describe dependence: Ax = 0 for some vector x (other than x = 0)\nA1 =\n\nand A2 =\n\nand A3 =\na\nb\nc\nd\ne\nf\n\nhave dependent columns\nReasons : Column 2 of A1 = 2 (Column 1)\nA2 times x =\n\ngives\n\nA3 has 3 columns in 2-dimensional space. Three vectors in a plane: Dependent !\nThe columns of A are \"independent\" if no column is a combination of the other columns\nAnother way to say it : Ax = 0 only when x = 0\nA4 =\n\nand A5 =\n\nand A6 = I have independent columns\nWhat about the rows of A1 to A6 ? A1, A2, A4 have dependent rows. Possibly also A3.\nFor any square matrix : Columns are independent if and only if rows are independent.\nA key idea\nNumber of independent rows =\nis coming\nNumber of independent columns\n\nZoomNotes for Linear Algebra\n1.6\nMatrix-Matrix Multiplication AB\nThere are 4 ways to multiply matrices. The first way is usually best for hand computation. The\nother three ways produce whole vectors instead of just one number at a time.\n1. (Row i of A) · (Column j of B) produces one number: row i, column j of AB\n\n=\n\n·\n·\n·\n\nbecause\n\n= 17\nDot product\n2. (Matrix A) (Column j of B) produces column j of AB : Combine columns of A\n\n=\n\n·\n·\n\nbecause 5\n\n+ 6\n\n=\n\nThis is the best way for understanding: Linear combinations.\n\"Good level\"\n3. (Row i of A) (Matrix B) produces row i of AB : Combine rows of B\n\n=\n\n·\n·\n\nbecause 1\n+ 2\n=\n4. (Column k of A) (Row k of B) produces a simple matrix : Add these simple matrices !\n\n=\n\nand\n\n=\n\nNOW\nADD\n\n=AB\nDot products in 1 are \"inner products\". Column-row products in 4 are \"outer products\".\nAll four ways use the same mnp multiplications if A is m by n and B is n by p.\nIf A and B are square n by n matrices then AB uses n3 multiply-adds in 1, 2, 3, 4.\nAssociative Law\nA times BC = AB times C\nMost important rule !\nBlock multiplication\nBlock sizes must fit\n\nA\nB\nC\nD\n\nE\nF\n\n=\n\nAE + BF\nCE + DF\n\nPart 1 :\nBasic Ideas of Linear Algebra\n1.7\nFactoring A into CR : Column rank = r = Row rank\nStep 1 C contains the first r independent columns of A (delete dependent columns of A)\n1. If column 1 of A is not zero, put it into C\n2. If column 2 of A is not a multiple of column 1 of A, put it into C\n3. If column 3 of A is not a combination of columns 1 and 2 of A, put it into C\nn. If column n of A is not a combination of the first n -1 columns, put it into C\nStep 2\nColumn j of CR expresses column j of A as a combination of the columns of C\nExample\nA =\n\nColumns 1 and 2 of A go directly into C\nColumn 3 = 2 (Column 1) + 1 (Column 2)\nNot in C\n↓\nA =\n\n=\n\n= CR\n2 columns in C\n2 rows in R\nThese matrices A, C, R all have column rank 2 (2 independent columns)\nBy the theorem A, C, R also have row rank 2 (2 independent rows)\nFirst great theorem\nEvery matrix has column rank = row rank\nDimension r of the column space = Dimension r of the row space = Rank of matrix A\nA = (m by n) = CR = (m by r) (r by n)\n\nZoomNotes for Linear Algebra\n1.8\nRank one matrices\nA = (1 column) times (1 row)\nRank one\nExample\nA =\n\n=\n\n= CR\nSuppose all columns of A are multiples of one column.\nThen all rows of A are multiples of one row. Rank = 1.\nRow space is a line\nColumn space is a line\nIf all columns of A are multiples of column 1, it goes into C.\nIf all rows of A are multiples of row 1, that row (divided by a11) goes into R.\nEvery rank 1 matrix factors into one column times one row.\nEvery rank r matrix is the sum of r rank one matrices.\nThis comes from column times row multiplication of C times R.\n∗\nIf A starts with a row or column of zeros, look at row 2 or column 2\n∗∗Rank 1 matrices are the building blocks of all matrices\nA =\n\n=\n\nAll the key factorizations of linear algebra add columns times rows\nA = CR\nA = LU\nA = QR\nS = QΛQT\nA = UΣV T\nThose 5 factorizations are described in Parts 1+3, 2, 4, 6, 7 of these ZoomNotes\n\nPart 2\nSolving Linear Equations\nAx = b : A is n by n\n2.1\nInverse Matrices A-1 and Solutions x = A-1b\n2.2\nTriangular Matrix and Back Substitution for Ux=c\n2.3\nElimination : Square Matrix A to Triangular U\n2.4\nRow Exchanges for Nonzero Pivots : Permutation P\n2.5\nElimination with No Row Exchanges : Why is A = LU ?\n2.6\nTransposes / Symmetric Matrices / Dot Products\n\nPart 2 : Solving Linear Equations\nAx = b : A is n by n\n2.1\nInverse Matrices A-1 and Solutions x = A-1b\nThe inverse of a square matrix A has A-1A = I and AA-1 = I\n2 by 2\nA-1 =\n\n-1\n= 1\n\n-1\n-3\n\na\nb\nc\nd\n-1\n=\nad -bc\n\nd\n-b\n-c\na\n\nA has no inverse if ad -bc = 0\nA =\n\nhas no inverse matrix\nhas dependent rows\nhas dependent columns\n1. Invertible ⇔Rows are independent ⇔Columns are independent\n2. No zeros on the main diagonal ⇔Triangular matrix is invertible\n3. If BA = I and AC = I then B = B(AC) = (BA)C = C\n4. Invertible ⇔The only solution to Ax = b is x = A-1b\n5. Invertible ⇔determinant is not zero ⇔A-1 = [cofactor matrix]T/ det A\n6. Inverse of AB = B-1 times A-1 (need both inverses)\nABB-1A-1=I\n7. Computing A-1 is not efficient for Ax\n=\nb.\nUse 2.3 :\nelimination.\n\nPart 2 :\nSolving Linear Equations Ax = b : A is n by n\n2.2\nTriangular Matrix and Back Substitution for Ux=c\nSolve Ux =\n\nx1\nx2\nx3\n\n=\n\n= c without finding U -1\nUpper triangular U / Pivots 2, 5, 7 are not zero / Go from bottom to top\nBack substitution\nThe last equation 7x3 = 14 gives x3 = 2\nWork upwards\nThe next equation 5x2 + 6(2) = 17 gives x2 = 1\nUpwards again\nThe first equation 2x1 + 3(1) + 4(2) = 19 gives x1 = 4\nConclusion\nThe only solution to this example is x = (4, 1, 2)\nSpecial note\nTo solve for x3, x2, x1 we divided by the pivots 7, 5, 2\nA zero pivot in U produces dependent rows, dependent columns, no U -1\nInverse of this\ndifference matrix\n= sum matrix\n\n-1\n-1\n-1\n\n-1\n=\n\nCalculus : Inverse of derivative is integral\nZ x\ndf\ndx dx = f(x) -f(0)\n\nZoomNotes for Linear Algebra\n2.3\nElimination : Square Matrix A to Triangular U\nA =\n\n→\n\n→\n\n→\n\n= U\nOne elimination step subtracts lij times row j from row i (i > j)\nEach step produces a zero below the diagonal of U : l21 = 2, l31 = l32 = 1\nTo invert elimination, add\nlij times row j back to row i\n\n-l\n\n-1\n=\n\nl\n\nA = LU = (Lower triangular L) times (Upper triangular U)\nThis A needs 3 elimination steps to a beautiful result\nA =\n\n=\n\n= L times U\nElimination produced no zeros on the diagonal and created 3 zeros in U\nFor Ax = b\nAdd extra column b\nElimination and back substitution\nA b\n=\n\n→\nU c\n=\n\nbacksub\n-----→\nin 2.2\nx =\n\nPart 2 :\nSolving Linear Equations Ax = b : A is n by n\n2.4\nRow Exchanges for Nonzero Pivots : Permutation P\nIf a diagonal pivot is zero or small : Look below it for a better pivot\nExchange rows\nA =\n\ngoes to PA =\n\n=\n\nNonzero pivots\n3 and 2\nPermutation matrix P = Rows of I in any order\nThere are n ! row orders and n ! permutations of size n (this includes P = I)\nThe inverse of P is the transpose of P\nExchange rows with columns\n\n= I\nCan you find all six 3 by 3 permutations ? Is every P1P2 = P2P1 ?\nIf A is invertible then some P A has no zero pivots and\nP A = LU\nReverse\nthe order\nby P\n\nCircular\nshift\nby P\n\nZoomNotes for Linear Algebra\n2.5\nElimination with No Row Exchanges : Why is A = LU ?\nReason : Each step removes a column of L times a row of U\nRemove\n\n1 (row 1)\nl21 (row 1)\nl31 (row 1)\nl41 (row 1)\n\nfrom A to leave A2 =\n\n×\n×\n×\n×\n×\n×\n×\n×\n×\n\nWe removed a rank-one matrix : column times row. It was the column l1 =\n(1, l21, l31, l41) times row 1 of A--the first pivot row u1.\nWe face a similar problem for A2. We take a similar step to A3 :\nRemove\n\n0 (row 2 of A2)\n1 (row 2 of A2)\nl32 (row 2 of A2)\nl42 (row 2 of A2)\n\nfrom A2 to leave A3 =\n\n×\n×\n×\n×\n\nRow 2 of A2 was the second pivot row = second row u2 of U. We removed\na column l2 = (0, 1, l32, l42) times u2. Continuing this way, every step removes\na column lj times a pivot row uj of U. Now put those pieces back :\nA = l1u1 + l2u2 + · · · + lnun =\n\nl1 · · · ln\n\nu1...\nun\n\n= LU\nThat last step was column-row multiplication (see 1.6) of L times U.\nColumn k of L and row k of U begin with k -1 zeros. Then L is lower triangular\nand U is upper triangular. Here are the separate elimination matrices--inverted and\nin reverse order to bring back the original A :\nL32L31L21 A = U\nand\nA = L-1\n21 L-1\n31 L-1\n32 U = LU\n\nPart 2 :\nSolving Linear Equations Ax = b : A is n by n\n2.6\nTransposes / Symmetric Matrices / Dot Products\nTranspose of A =\n\nis AT =\n\n(AT)ij = Aji\nRules for the\nsum and product\nTranspose of A + B is AT + BT\nTranspose of AB is\nBTAT\nA symmetric matrix has ST = S\nThis means that every sij = sji\nThe matrices ATA and AAT are symmetric / usually different\n\n=\n\n=\n\n4 + 9 = 13\n6 =\nS = LU is improved to symmetric S = LDLT (pivots in U go into D)\nS =\n\n=\n\n=\n\n= LDU T\nDot product\nWork\n= Movements · Forces = xTf\nInner product\nHeat\n= Voltage drops · Currents = eTy\nx· y = xTy\nIncome = Quantities · Prices = qTp\n\nPart 3\nVector Spaces and Subspaces\nBasis and Dimension\n3.1\nVector Spaces and Four Fundamental Subspaces\n3.2\nBasis and Dimension of a Vector Space S\n3.3\nColumn Space and Row Space : Bases by Elimination\n3.4\nAx = 0 and Ax = b :\nxnullspace and xparticular\n3.5\nFour Fundamental Subspaces C(A), C(AT), N(A), N(AT)\n3.6\nGraphs, Incidence Matrices, and Kirchhoff's Laws\n3.7\nEvery Matrix A Has a Pseudoinverse A+\n+\n+\n\nPart 3 : Vector Spaces and Subspaces\nBasis and Dimension\n3.1\nVector Spaces and Four Fundamental Subspaces\nVector space : Linear combinations of vectors in S must stay in S\nS =Rn or Subspace of Rn, S = matrices Rm×n or functions ax+b\nNot vector spaces Half-line x\n≥\n0, invertible matrices, singular matrices\nSubspaces\nAll of R3, planes or lines through (0, 0, 0), one point (0, 0, 0)\nFour Subspaces\nColumn space C(A) = all vectors Ax\nRow space C(AT)\n= all vectors ATy\nColumn space = \"range\"\nNullspace N(A)\n= all x with Ax = 0\nNullspace = \"kernel\"\nLeft nullspace N(AT) = all y with ATy = 0\nAny set of vectors spans a vector space. It contains all their combinations\n\nZoomNotes for Linear Algebra\n3.2\nBasis and Dimension of a Vector Space S\nBasis = A set of independent vectors that span the space S\nEvery vector in S is a unique combination of those basis vectors\nDimension of S = The number of vectors in any basis for S\nAll bases contain the same number of vectors\nThe column space of A =\n\nis the x-y plane in R3\nThe first two columns are a basis for C(A)\nAnother basis for C(A) consists of the vectors\n\nand\n\nInfinitely many bases\nAlways 2 vectors\nDimension of C(A) = 2\nThe nullspace of this A is the z-axis in R3 : N(A) = all\n\nz\n\nEvery basis for that nullspace N(A) contains one vector like\n\nThe dimension of N(A) is 1. Notice 2 + 1 = dimension of R3\nMatrix spaces\nThe vector space of 3 by 3 matrices has dimension 9\nThe subspace of upper triangular matrices has dimension 6\nIf v1, . . . , vm\nand w1, . . . , wn\nare bases then\nm must equal n\nProof\nW =V A\n\nw1 · · · wn\n\n=\n\nv1 · · · vm\n\na11 · a1n\n·\n·\n·\nam1 · amn\n\nIf m < n then Ax = 0 would have a nonzero solution x\nThen Wx = V Ax = 0. W would have dependent columns !\n\nPart 3 :\nVector Spaces and Subspaces, Basis and Dimension\n3.3\nColumn Space and Row Space : Bases by Elimination\nEvery pivot = 1\nEliminate below\nEliminate above\n\n→\n\n→\n\n→\n\n→R0 = I\nR0 = \"reduced row echelon form\" = rref(A) =\n\nr rows start with 1\nm -r rows of zeros\n\nr = rank\nA has r independent columns and r independent rows\nA =\n\n→\n\n→\n\n→\n\n→\n\n=\nI\n\n= R0\nI locates r independent columns\nP permutes the columns if needed\nR0 =\n\nI\nF\n\nP =\n\nr rows with I\nm-r zero rows\n\nBasis for the column space\nC = First r independent columns of A\nBasis for the row space\nR =\nI\nF\nP = r rows and A = CR\nA =\n\n→\n\n→\n\n=\nI\nF\n= R0 = R\nA = CR is\n\n=\n\nRow rank = 2\nColumn rank = 2\n\nZoomNotes for Linear Algebra\n3.4\nAx = 0 and Ax = b :\nxnullspace and xparticular\nA =\n\n→\n\n→\n\n= R\n2 columns of I\n4 -2 columns of F\nBasis for nullspace\nSpecial solutions\nAs1 = 0 and As2 = 0\ns1=\n\n-2\n\ns2=\n\n-3\n-1\n\n2, 3, 1 in R\n←1 and 0\n-2, -3, -1 in S\n←0 and 1\n\"Special\"\nDependent columns 2 and 4 = combination of independent columns 1 and 3\nElimination from A to R reveals the n -r special solutions\nA x = 0 and R x = 0\nR x=\nI\nF\nP x = 0\nS =\ns1 · · · sn-r\n\n=P T\n\n-F\nIn-r\n\nPP T = I leads to RS = 0\nr equations\nn -r solutions\nComplete solution to Ax = b\nx = xnullspace + xparticular = above ↑+ below ↓\nA\nb\n=\nb1\nb2\n\n→\nd1\nd2\n\n=\nR\nd\nxparticular =\n\nd1\nd2\n\nAnew bnew\n\n=\n\nb1\nb2\nb3\n\n→\n\nd1\nd2\nd3\n\nNo solution if d3 = 0\nNo solution if b1 + b2 = b3\nElimination must give 0=0\n\nPart 3 :\nVector Spaces and Subspaces, Basis and Dimension\n3.5\nFour Fundamental Subspaces C(A), C(AT), N(A), N(AT)\nC(AT)\nC(A)\nRn\nRm\nN(A)\ndimension n -r\nN(AT)\ndimension m -r\nrow space\nall ATy\ndimension r\ncolumn space\nall Ax\ndimension r\nnullspace\nAx = 0\nleft nullspace\nATy = 0\nThe big picture\nFundamental Theorem of Linear Algebra, Part 1\nThe column space and row space both have dimension r.\nThe nullspaces have dimensions n -r and m -r.\nThis tells us the Counting Theorem : How many solutions to Ax = 0 ?\nm equations, n unknowns, rank r ⇒Ax = 0 has n -r independent solutions\nAt least n -m solutions. More for dependent equations (then r < m)\nThere is always a nonzero solution x to Ax = 0 if n > m\n\nZoomNotes for Linear Algebra\n3.6\nGraphs, Incidence Matrices, and Kirchhoff's Laws\n\"Graph\"\nx2\nx1\nx3\nx4\nb1\nb2\nb3\nb4\nb5\nnodes\nx1\nx2\nx3\nx4\nedges\nA =\n\n-1\n-1\n-1\n-1\n-1\n\nThis graph has 5 edges and 4 nodes. A is its 5 by 4 incidence matrix.\nb = b1 to b5 = currents.\nx = x1 to x4 = voltages\nEdges 1, 2, 3 form a loop in the graph\nDependent rows 1, 2, 3\nEdges 1, 2, 4 form a tree. Trees have no loops !\nIndependent rows 1, 2, 4\nThe incidence matrix A comes from a connected graph with n nodes and m edges.\nThe row space and column space have dimensions r = n -1. The nullspaces of A\nand AT have dimensions 1 and m -n + 1 :\nN(A) The constant vectors (c, c, . . . , c) make up the nullspace of A : dim = 1.\nC(AT) The edges of any spanning tree give r independent rows of A : r = n -1.\nC(A) Voltage Law: The components of Ax add to zero around all loops: dim=n-1.\nN(AT) Current Law: ATy = (flow in)-(flow out) = 0 is solved by loop currents.\nThere are m -r = m -n + 1 independent small loops in the graph.\nCurrrent law ATy = 0 at each node is fundamental to applied mathematics\n\nPart 3 :\nVector Spaces and Subspaces, Basis and Dimension\n3.7\nEvery A Has a Pseudoinverse A+\n+\n+\n2 0\n0 0\n+\n=\n2 0\n0 0\n\nA is invertible if and only if m = n = r (rank). Then A+ = A-1\nA has a left inverse A+ = (ATA)-1AT when r = n : A+A = In\nA has a right inverse A+ = AT(AAT)-1 when r = m : AA+ = Im\nA=CR has a pseudoinverse A+ =R+C+. Reverse the 4 subspaces\nA Row space to column space\nA+ Column space to row space\nAA+b : project b onto column space\nA+Ax : project x onto row space\nrow\nspace of A\ncolumn space of A+\ncolumn\nspace of A\nA\nA\nrow space of A+\nA+p = x+\nA+b = x+\nA+e = 0\nx+\np = Ax+\n= AA+b\nb\ne\nnullspace\nof A\nAAA\nnullspace of AT\nAAA\n= nullspace of A+\nA+A=\nI\nrow space\nnullspace\nAA+ =\nI\nC(A)\nN(AT)\nPseudoinverse A+\nA+ = V Σ+U T is computed from A = UΣV T : Σ+ has 1/σ's and 0's\n\nZoomNotes for Linear Algebra\nExample\nA =\n\n= uvT =\n\n= CR\nThe pseudoinverse A+ has the same rank r = 1\nRow space of A = line in R3\nColumn space of A = line in R2\nReverse the column space and row space : v and u\nA+ =\nvuT\n||v||2 ||u||2 = 1\n\n= 1\n\nCheck\nA+A= 1\n\n= projection onto\nrow space of A\nAA+ = 1\n\n=\nprojection onto\ncolumn space of A\n\nPart 4\nOrthogonal Matrices\nand Least Squares\n4.1\nOrthogonality of the Four Subspaces\n4.2\nProjections onto Subspaces\n4.3\nLeast Squares Approximations (Regression) :ATAbx=ATb\n4.4\nOrthogonal Matrices and Gram-Schmidt\n\nPart 4 : Orthogonal Matrices\nand Least Squares\n4.1\nOrthogonality of the Four Subspaces\nVectors x and y are orthogonal if xTy = 0 (Complex vectors : xTy = 0)\nThen ||x + y||2 = ||x||2 + ||y||2 = ||x -y||2 (Right triangles : Pythagoras)\nOrthogonal subspaces : Every v in V is orthogonal to every w in W\nTwo walls of a room are not orthogonal. Meeting line is in both subspaces !\nThe row space and the nullspace of any matrix are orthogonal\nThe column space C(A) and nullspace N(AT) : Also orthogonal\nClear from Ax = 0\nAx =\n\"\nrow 1\n---\nrow m\n#\nx\n\n=\n\"\n0·0\n#\nAll rows are orthogonal to x ⇒whole row space is orthogonal to x\nBig Picture of Linear Algebra : Two pairs of orthogonal subspaces\nMore : r + (n -r) = full dimension n so every x equals xrow + xnull\n\nPart 4 :\nOrthogonal Matrices and Least Squares\n4.2\nProjections onto Subspaces\n■■■■■■■\n■■■■■■■\np = bx a = aTb\naTa a\nS\nb\np\ne\np = Abx\n= A(ATA)-1ATb\n= P b\nb is projected onto line through a and onto column space of A\nError vector e = b -p is orthogonal to the line and subspace\nProjection matrix P\nPline = aaT\naTa\nPsubspace = A(ATA)-1AT\nNotice P 2 = P and P T = P\nSecond projection : p doesn't move!\nProject b =\n\nLine :\nPlane : Column spaces of a =\n\nand A =\n\npline = a aTb\naTa =\n\n3 =\n\nError e = b -p =\n\n-2\n-2\n\npplane\nSolve ATbx = ATb\n\nbx =\n\ngives bx =\n\n-3\n\nProjection p = Abx =\n\n-1\n\nError e = b -p =\n\n-\n\n-1\n\n=\n\n-2\n\nWhat is the 3 by 3 projection matrix P =A(ATA)-1AT ? Then p=P b\n\nZoomNotes for Linear Algebra\n4.3\nLeast Squares Approximations (Regression) :ATAbx=ATb\nIf Ax = b has no solution then minimize E = ||b -Ax||2 = xTATAx -2xTATb + bTb\nCalculus\nPartial derivatives ∂E/∂x of that error E are zero\nLinear algebra Ax is in the column space of A\nBest Abx=projection of b on C(A)\n\"Normal equations\"\nATAbx = ATb and then the projection is p = Abx\nKey example\nClosest straight line y = C + Dt to m points (ti, bi)\nm > 2\nm equations\n2 unknowns\nNo solution\nC + Dt1 = b1\n· · · · · · · · · · · ·\nC + Dtm = bm\nA =\n\nt1\n·\n·\n·\n·\ntm\n\nx =\n\nC\nD\n\nb =\n\nb1\n·\n·\nbm\n\ne\nb - p\n(1, -2, 1)\nbest\nb = C + Dt = 5 - 3t\nATAbx = ATb\n\"\nm\nP ti\nP ti\nP t2\ni\n#\nC\nD\n\n=\n\n-3\n\n=\n\n=\n\" P bi\nP tibi\n#\n\nPart 4 :\nOrthogonal Matrices and Least Squares\n4.4\nOrthogonal Matrices and Gram-Schmidt\nOrthogonal\ncolumns\nQ =\n\nq1\n· · ·\nqn\n\nqT\ni qi = 1 unit vectors\nqT\ni qj = 0 orthogonal\nQTQ = I\nImportant case = Square matrix Then QQT =I\nQT =Q-1 \"Orthogonal matrix\"\nQ = 1\n\n-1\n-1\n\nhas\nQTQ = I\nQQT = I\nQ = 1\n\n-1\n-1\n-1\n\nNow QT = Q-1\nOrthogonal matrix\nQ1 times Q2 is orthogonal because (Q1Q2)-1 = Q-1\n2 Q-1\n= QT\n2 QT\n1 = (Q1Q2)T\nv = c1q1 + · · · + cnqn leads to ck = qT\nk v\nv = Qc leads to c = QTv\nGram-Schmidt\nStart with independent a, b, c\nCreate orthogonal vectors q1, q2, q3\nq1 = a\n||a||\nQ2 =b -(qT\n1 b)q1\nq2 = Q2\n||Q2||\nQ3 =c -(qT\n1 c)q1 -(qT\n2 c)q2\nq3 = Q3\n||Q3||\nGram-Schmidt A = QR\nA = (orthogonal)(triangular)\nA=\n\"\ncos θ a12\nsin θ a22\n#\n=\n\"\ncos θ -sin θ\nsin θ\ncos θ\n#\"\n1 r12\n0 r22\n#\n= QR\n\nPart 5\nDeterminant of a Square Matrix\n5.1\n3 by 3 and n by n Determinants\n5.2\nCofactors and the Formula for A-1\n5.3\nDet AB = (Det A) (Det B) and Cramer's Rule\n5.4\nVolume of Box = | Determinant of Edge Matrix E |\n\nPart 5 : Determinant of a Square Matrix\n5.1\n3 by 3 and n by n Determinants\n\ndet = +1\n-1\n+1\n-1\n+1\n-1\nEven permutations have det P =+1 Odd permutations have det P =-1\nThree defining properties\nRow exchange reverses sign of det\ndet is linear in each row separately\ndet I = 1\n\na\nq\nz\n\nb\np\nz\n\nb\nr\nx\n\nc\nq\nx\n\nc\np\ny\n\na\nr\ny\n\ndet = +aqz\n-bpz\n+brx\n-cqx\n+cpy\n-ary\nLinearity separates det A into n! = 3! = 6 simple determinants\ndet =\na\nb\nc\na\nb\np\nq\nr\np\nq\nx\ny\nz\nx\ny\n\n-\n-\n-\n+\n+\n+\nCombine 6 simple determinants into det A\n+ aqz + brx + cpy -ary -bpz -cqx\nEach term takes 1 number from each row and each column\nBIG FORMULA = Sum over all n! orders P = (j, k, . . . , z) of the columns\ndet A = P (det P ) a1j a2k . . . anz as in +a11a22-a12a21\n\nZoomNotes for Linear Algebra\n5.2\nCofactors and the Formula for A-1\n3 by 3 determinant :\n2 terms start with a and with b and with c\nCofactor formula\ndet A=a (qz -ry) + b (rx -pz) + c (py -qx)\nn factors a, b, c\nn cofactors = determinants of size n -1\nRemove row i and column j from A\nCofactor Cij =det times (-1)i+j\nCofactors along row 1\ndet A = a11C11 + · · · + a1nC1n\nInverse formula\nA-1 = (transpose of C)/(determinant of A)\nEvery entry of A-1 = cofactor\ndet A\n= det of size n -1\ndet of size n\nn = 2\nA=\n\na\nb\nc\nd\n\nCofactors C =\n\nd\n-c\n-b\na\n\nA-1 =\nCT\nad -bc\nn = 3\nA =\n\na\nb\nc\np\nq\nr\nx\ny\nz\n\nC =\n\nqz -ry\nrx -pz\npy -qx\nbz -cy\naz -cx\nqx -py\nbr -cq\ncp -ar\naq -bp\n\nACT=\n\ndet A\ndet A\ndet A\n\n=(det A)I\nThis explains A-1 =\nCT\ndet A\n\nPart 5 :\nDeterminant of a Square Matrix\n5.3\nDet AB = (Det A) (Det B) and Cramer's Rule\ndet A = det AT\ndet AB = (det A) (det B)\ndet A-1 =\ndet A\nOrthogonal matrix\ndet Q = ±1 because QTQ = I gives (det Q)2 = 1\nTriangular matrix\ndet U = u11u22 · · · unn\ndet A = det LU = (det L) (det U) = product of the pivots uii\nCramer's Rule to Solve Ax = b\nStart from\n\nA\n\nx1\nx2\nx3\n\n=\n\nb1\na12\na13\nb2\na22\na23\nb3\na32\na33\n\n= B1\nUse\n(det A) (x1) = (det B1) to find x1\nx1 = det B1\ndet A\nSame\nidea\n\nA\n\nx1\nx2\nx3\n\n=\n\na1\nb\na3\n\n=B2\nx2 = det B2\ndet A\nCramer's Rule is usually not efficient ! Too many determinants\nx1\nx2\n\n=\n\nB1 =\n\nB2 =\n\nx1 = det B1\ndet A = 4\nx2 = 2\n\nZoomNotes for Linear Algebra\n5.4\nVolume of Box = | Determinant of Edge Matrix E |\n(0, 0)\n(c, d)\n(a, b)\n(a+c, b+d)\n(v, w)\nw\nu\nRotate to align\nArea = uw\nEdge\nmatrix E =\n\na\nb\nc\nd\n\n= QR = (orthogonal Q) (triangular R)\nR =\n\nu\nv\nw\n\nTo Prove : Area of a parallelogram is |det E |=|ad -bc |=|det R |=uw\n2 D area\nGram-Schmidt in 4.4 gives E = QR = (orthogonal) (triangular)\nOrthogonal Q : Rotates the shape = No change in area !\nTriangular R : u = base, w = height, uw = area=|det R |=|det E |\n3D volume Edges of box = Rows of E\nVolume of box =|det E |=|det R |\nOrthogonal Q : No volume change\nRotate box to see volume = r11r22r33\nIf the box is a unit cube : E = identity matrix and volume = 1\nAny shape\nMultiply all points by A\nVolume multiplies by det A\n\nPart 6\nEigenvalues and Eigenvectors :\nAx = λx and Anx = λnx\n6.1\nEigenvalues λ and Eigenvectors x : Ax = λx\n6.2\nDiagonalizing a Matrix : X-1AX = Λ = eigenvalues\n6.3\nSymmetric Positive Definite Matrices : Five Tests\n6.4\nLinear Differential Equations du\ndt = Au\n6.5\nMatrices in Engineering : Second differences\n\nPart 6 : Eigenvalues and Eigenvectors :\nAx = λx and Anx = λnx\n6.1\nEigenvalues λ and Eigenvectors x : Ax = λx\nAx is on the same line as x / Ax = λx means (A -λI)x = 0\nThen A2x = A(λx) = λ(Ax) = λ2x\nAnx = λnx\nA-1x = 1\nλ x\nDeterminant of A -λI = 0\nSolutions λ1 to λn : A has n eigenvalues\nA =\n\n.8\n.3\n.2\n.7\n\nA -λI =\n\n.8 -λ\n.3\n.2\n.7 -λ\n\ndet(A -λI) = λ2 -1.5λ + .56 -.06 = (λ -1)\nλ -1\n\nEigenvector x2\nfor λ2 = 1\nA -1\n2I\n\nx2 =\n.3\n.3\n.2\n.2\n\nx2\n\n=\n\ngives x2 =\n\n-1\n\nEigenvector x1\nfor λ1 = 1\n(A -I)x1 =\n\n-.2\n.3\n.2\n-.3\n\nx1\n\n=\n\ngives x1 =\n\n0.6\n0.4\n\nWhat is A10\n\n? Separate into eigenvectors / Follow each eigenvector\n\n=\n0.6\n0.4\n\n+\n\n0.4\n-0.4\n\nA10\n\n= 110\n0.6\n0.4\n\n+\n0.4\n-0.4\n\nUseful\nSum of λ's = λ1 + · · · + λn = trace of A = a11 + a22 + · · · + ann\nfacts\nProduct of λ's = (λ1) · · · (λn) = determinant of A\nEigenvalues of A + B and AB are usually not λ(A) + λ(B) and λ(A)λ(B)\n\nPart 6 :\nEigenvalues and Eigenvectors : Ax = λx and Anx = λnx\n6.2\nDiagonalizing a Matrix : X-1AX = Λ = eigenvalues\nKey idea / Follow each eigenvector separately / n simple problems\nEigenvector matrix X\nAssume independent x's\nThen X is invertible\nAX =A\n\nx1\n· · ·\nxn\n\n=\n\nλ1x1\n· · ·\nλnxn\n\nAX = XΛ\nX-1AX = Λ\nA = XΛX-1\n\nλ1x1\n· · ·\nλnxn\n\n=\n\nx1\n· · ·\nxn\n\nλ1\n...\nλn\n\nAk becomes easy\nAk = (XΛX-1) (XΛX-1) · · · (XΛX-1)\nSame eigenvectors in X\nAk = XΛkX-1\nΛk = (eigenvalues)k\n\n=XΛ4X-1=\n\n1 -1\n\n=\n\n1 -1\n\n=\n\n2 Question : When does Ak →zero matrix ?\nAnswer : All |λi| < 1\n3 Some matrices are not diagonalizable\nThey don't have n independent vectors\nA=\n\nhas λ=3 and 3\nThat A has double eigenvalue, single eigenvector\nOnly one x=\n\nAll the \"similar matrices\" BAB-1 have the same eigenvalues as A\nIf Ax = λx then (BAB-1) (Bx) = BAx = Bλx = λ(Bx)\n\nZoomNotes for Linear Algebra\n6.3\nSymmetric Positive Definite Matrices : Five Tests\nIf S = ST\nEigenvalues λ are real\nEigenvectors x are orthogonal\nS =\n\n=ST has S\n\n=9\n\nand S\n-1\n\n=\n-1\n\nNotice\nx1·x2 =0\n\n·\n-1\n\n=0\nq= x\n||x|| = eigenvectors\nlength = 1\nEigenvector matrix Q is an\northogonal matrix : QT = Q-1\nS =QΛQ-1=QΛQT\nSpectral theorem\nS =\n\n= 1\n√\n\n-1\n\n-1\n√\n2 = QΛQT\nPositive definite matrices are the best. How to test S for λi > 0 ?\nTest 1\nCompute the eigenvalues of S : All eigenvalues positive\nTest 2\nThe energy xTSx is positive for every vector x = 0\nTest 3\nThe pivots in elimination on S are all positive\nTest 4\nThe upper left determinants of S are all positive\nTest 5\nS = ATA for some matrix A with independent columns\nPositive semidefinite matrices can be singular: Test 5 is S = any ATA\nEigenvalues and energy and pivots and determinants of S can be zero\n\nb\nb\n\nPositive definite if b2 < 8\nSemidefinite if b2 ≤8\nSecond difference matrix\nPositive definite in 6.5\nS =\n\n-1\n-1\n-1\n-1\n\nPositive semidefinite\nSx = 0 for x = (1, 1, 1)\nS =\n\n-1\n-1\n-1\n-1\n\nPart 6 :\nEigenvalues and Eigenvectors : Ax = λx and Anx = λnx\n6.4\nLinear Differential Equations du\ndt = Au\nn = 1\ndu\ndt = au is solved by u(t) = Ceat = u(0)eat\nn ≥1\ndu\ndt = Au is solved by eigenvectors as in u(t) = c1eλ1tx1\nThe key is constant matrix A ⇔exponential solution eλtx when Ax = λx\nCheck : If u = eλtx then du\ndt = λeλtx = Aeλtx = Au as required\nA=\n\nhas λ1=9\nu1=e9t\n\ndu1\ndt =9e9t\n\n=e9tA\n\n=Au1\nInitial condition\nu(0) at t = 0\nSplit u(0) into\neigenvectors x\nu(0) = c1x1 + · · · + cnxn\nEach eigenvector\ngoes its own way\nCombine\nsolutions\nu(t) = c1eλ1tx1 + · · · + cneλntxn\nSpecial case\nλ1 = λ2 with\none eigenvector x\nu(t) = c1eλ1tx1 + c2teλ1tx1\nStability\nu(t) →0 if all eigenvalues λ = a + ib have real part a < 0\nWeak stability\nu(t) →steady state if one λ moves up to λ = 0\nMatrix\nExponential\neAt is the\nsolution matrix\neAt u(0) = u(t)\neAtx = eλtx\nExponential\nSeries eAt\nex = 1 + x + x2\n2 + · · · + xn\nn! + · · ·\neA = I + A + A2\n2 + · · · + An\nn! + · · ·\n\nZoomNotes for Linear Algebra\n6.5\nMatrices in Engineering : Second differences\nCentered\ndifference\ndu\ndx ≈u (x+h)-u (x-h)\n2h\nSecond\ndifference\nd2u\ndx2 ≈u (x+h)-2u (x)+u (x-h)\nh2\nSecond\ndifferences\nwith u0 = u4 = 0\n-d2u\ndx2 ≈1\nh2\n\n-1\n-1\n-1\n-1\n\nu1\nu2\nu3\n\n= 1\nh2Ku\nEigenvalues 2 -\n√\n2, 2, 2 +\n√\nPivots 2\n1, 3\n2, 4\nDeterminants 2, 3, 4\nEnergy\nxTKx\nx1 x2 x3\n\nK\n\nx1\nx2\nx3\n\n=\n2(x2\n1 -x1x2 + x2\n2 -x2x3 + x2\n3) > 0\nK is positive definite\nu0 = 0\nmass\nmass\nmass\nu4 = 0\nu0 = 0\n-Ku = m d2u\ndt2\npulling up\npulling down\nu4 = 0\nSteady state\nSpring forces\nBalance gravity\nKu = g\nDisplacements u\nOscillation\nSprings pull\nMasses move\nNewton's Law F = ma\nUse eigenvalues of K\n\nPart 7\nSingular Values and Vectors :\nAv =σu and A=UΣV T\n7.1\nSingular Vectors in U, V and Singular Values in Σ\n7.2\nReduced SVD / Full SVD / Construct UΣV T from ATA\n7.3\nThe Geometry of the SVD : Rotate - Stretch - Rotate\n7.4\nAk is Closest to A : Principal Component Analysis PCA\n7.5\nComputing Eigenvalues of S and Singular Values of A\n7.6\nCompressing Images by the SVD\n7.7\nThe Victory of Orthogonality\n\nPart 7 : Singular Values and Vectors :\nAv =σu and A=UΣV T\n7.1\nSingular Vectors in U, V and Singular Values in Σ\nAn example shows orthogonal vectors v going into orthogonal vectors u\nAv1 =\n\n=\n\nand\nAv2 =\n\n-1\n\n=\n\n-3\n\nv1 =\n\nis orthogonal to v2 =\n\n-1\n\nis orthogonal to\n\n-3\n\nDivide both inputs v by\n√\nDivide both outputs u by\n√\nMatrix form\nAV = UΣ\n\nv1\nv2\n\n=\n\nu1\nu2\n\n√\n√\n\nV and U = orthogonal matrices\nV TV =I\nU TU =I\nA = UΣV T\nv1, v2 = orthogonal basis for the row space = inputs\nu1, u2 = orthogonal basis for the column space = outputs\nσ1 = 3\n√\n5 and σ2 =\n√\n5 are the singular values of this A\n\nPart 7 :\nSingular Values and Vectors : Av =σu and A=UΣV T\n7.2\nReduced SVD / Full SVD / Construct UΣV T from ATA\nReduced SVD : Stop at ur and vr\nFull SVD : Go on to um and vn\nA = U rΣrV T\nr =\n\nu1 to ur\ncolumn space\nm × r\n\nσ1\n...\nr by r\nσr\n\nvT\nrow\nto\nspace\nvT\nr\nr × n\n\nA = UΣV T =\n\nu1 to um\ncolumns\nm × m\n\nσ1\n·\nσr\nm by n 0 0\n\nvT\nrow space\nvT\nr\nn × n\nvT\nn\nnullspace\n\nKey ideas\nATA = V ΣTUTUΣV T = V ΣTΣV T\nAAT = UΣΣTU T\nEigenvectors ! ATAv = σ2v and AATu = σ2u\nn v's and m u's\nThe u's are chosen so that Avk = σkuk\nσ1 ≥· · · ≥σr > 0\nk≤r vk and σ2\nk from ATA\nuk =Avk\nσk\nuT\nj uk =\nAvj\nσj\nTAvk\nσk\n= σk\nσj\nvT\nj vk =0\nSquareA\nhas |λ| ≤σ1\n|λ| ||x||=||Ax||=||UΣV Tx||=||ΣV Tx||≤σ1 ||V Tx||=σ1 ||x||\nA =\n\nλ = 0, 0, 0\nσ = 8, 1, (0) A = u1σ1vT\n1 + u2σ2vT\nA has rank r = 2\n2 singular values\nA =\n\nλ = 5, 3\nσ = 3\n√\n5,\n√\nu1σ1vT\n+\nu2σ2vT\n= 3\n\n+ 1\n\n3 -3\n-1\n\n=A\n\nZoomNotes for Linear Algebra\n7.3\nThe Geometry of the SVD : Rotate - Stretch - Rotate\nv2\nv1\nRotate by V T\nStretch by Σ\nRotate by U\nV T\nx\nAx\nΣ\nσ1\nσ2\nU\nσ1u1\nσ2u2\nA\nA = (Orthogonal) (Diagonal) (Orthogonal)\nA =\n\na\nb\nc\nd\n\n=\n\ncos θ\n-sin θ\nsin θ\ncos θ\n\nσ1\nσ2\n\ncos φ\nsin φ\n-sin φ\ncos φ\n\nFour numbers a, b, c, d in A produce four numbers θ, σ1, σ2, φ in the SVD\n3 × 3 : Nine numbers in A produce which 9 numbers for UΣV T ?\nn × n : An orthogonal matrix comes from 1\n2n(n -1) simple rotations\nInputs x = unit circle\nOutputs Ax = stretched ellipse\nRadius vectors v1 and v2\nAxis vectors σ1u1 and σ2u2\n\nPart 7 :\nSingular Values and Vectors : Av =σu and A=UΣV T\n7.4\nAk is Closest to A : Principal Component Analysis PCA\nSVD\nA = UΣV T\n= u1σ1vT\n1 + · · · + urσrvT\nr\nA has rank r\nAk = UkΣkV T\nk = u1σ1vT\n1 + · · · + ukσkvT\nk\nany k ≤r\nGreat fact\nThis Ak from the SVD is the closest rank k matrix to A\n\"Eckart-Young\"\n||A -Ak|| ≤||A -B|| if B has rank k\nMatrix norms\n||A||l2 norm = σ1\n||A||Frobenius =\np\nσ2\n1 + · · · + σ2\nr\nA0 = matrix of data\nA = subtract row average from each row of A0\nS = AAT\nn -1 = sample covariance matrix is symmetric positive definite\n×\n×\n×\n×\n×\n×\n×\n×\n× ×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n××\n×\n×\n×\n×\n× ×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n××\n×\n× ×\n×\n×\n×\n×\n× ×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n××\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\nLine closest to data is u1 : The key to PCA\nStraight line fit using perpendicular distances\nu1 = eigenvector of S\n= first principal component\n= singular vector of A= captures most variance\nTotal variance of the data = Add the eigenvalues of S = σ2\n1 + · · · + σ2\nr\n\nZoomNotes for Linear Algebra\n7.5\nComputing Eigenvalues of S and Singular Values of A\nStep 1\nProduce zeros in the matrix\nS →Q-1SQ = S0\nSame λ's\nQ, Q1, Q2 = orthogonal matrix\nA →Q-1\n1 AQ2 = A0\nSame σ's\nNew S0 has only 3 nonzero diagonals\nA0 has only 2 nonzero diagonals\nStep 2\n\"QR method\" uses Gram-Schmidt to orthogonalize columns\nS = (Orthogonal Q) (Upper triangular R) at every step\nFactor S0 =Q0R0\nReverse S1 =R0Q0\nRepeat S1 =Q1R1 and S2 =R1Q1\nAmazing : The off-diagonal entries get small : Watch sin θ →-sin3 θ\nSk = QkRk\n\ncos θ\nsin θ\nsin θ\n\n=\n\ncos θ\n-sin θ\nsin θ\ncos θ\n\nsin θ cos θ\n-sin2 θ\n\nSk+1 = RkQk\n\nsin θ cos θ\n-sin2 θ\n\ncos θ\n-sin θ\nsin θ\ncos θ\n\n=\n\n∗\n∗\n-sin3 θ\n∗\n\nSk approaches Λ : The eigenvalues λ begin to appear on the diagonal\nSimilar idea for SVD=Golub-Kahan algorithm :σ's appear on the diagonal\n\nPart 7 :\nSingular Values and Vectors : Av =σu and A=UΣV T\n7.6\nCompressing Images by the SVD\nFlag with\n3 stripes\n\nB B B B B B\nB B B B B B\nW W W W W W\nW W W W W W\nR R R R R R\nR R R R R R\n\n=\n\nB\nB\nW\nW\nR\nR\n\n1 1 1 1 1 1\nRank one matrix\nGreat compression\nTriangular flag\n\nThis has all σ > 1\nPoor compression\nCompress photographs\nhttps://timbaumann.info/svd-image-compression-demo/\nUncompressed image\n= 600 × 600 = 360, 000 pixels\nCompressed image UΣV T\n= 600 × 100 + 100 + 100 × 600 = 120, 000\nSerious compression UΣV T = 600 × 20 + 20 + 20 × 600 = 24, 020\nCompression is highly developed\nSee Wikipedia for Eigenfaces\n\nZoomNotes for Linear Algebra\n7.7\nThe Victory of Orthogonality\n1 Length of Qx = Length of x\n||Qx||2 = ||x||2\n(Qx)T(Qy) = xTy\n2 All powers Qn and all products Q1Q2 remain orthogonal\n3 Reflection\nH = I -2uuT = orthogonal + symmetric when uTu = 1\n4 Symmetric matrices have orthogonal eigenvectors SQ = QΛ\n5 All matrices have orthogonal singular vectors v's and u's : AV = UΣ\n6 The pseudoinverse of UΣV T is V Σ+U T\nNonzeros in Σ+ are 1\nσ\n7 Polar decomposition\nA = QS = (orthogonal) (symm positive definite)\n8 Gram-Schmidt = Highly valuable\nA=QR = (orthogonal)(triangular)\n9 Orthogonal functions for Fourier series\nf(x) = P akcos kx + bksin kx\nv1\nv2\nrows\nnull\nv3\nA\ncolumns\nu2\nu1\nAv1 = σ1u1\nAv2 = σ2u2\nBig picture of the SVD\nOrthogonal rows →Orthogonal columns\n\nPart 8\nLinear Transformations\nand Their Matrices\n8.1\nExamples of Linear Transformations\n8.2\nDerivative Matrix D and Integral Matrix D+\nD+\nD+\n8.3\nBasis for V and Basis for Y ⇒Matrix for T : V→Y\n\nPart 8 : Linear Transformations\nand Their Matrices\n8.1\nExamples of Linear Transformations\nV and Y are vector spaces (the vectors can be matrices or functions !)\nT is a linear transformation from V to Y (inputs to outputs)\nTest for linearity\nT (cv + dw) = c T (v) + d T (w)\nfor all v, w in V\nExample 1\nV = x-y plane\nRotate the plane R2 by a fixed angle θ\nStraight lines rotate into straight lines (required by linearity)\nCenter point 0=(0, 0) stays put T(0+0)=T(0)+T(0) requires T(0)=0\nThis T has an inverse T -1 : Rotate by -θ is another linear transformation\nExample 2\nInput space V = all 3 by 3 matrices = output space Y\nT sets all off-diagonal entries to zero\nT (matrix) = (diagonal matrix)\nT 2 will be the same as T :\nT is like a projection on matrix space\nMultiply transformations T2T1\nOutput space for T1 = Input space for T2\nT2T1 obeys the same rule as matrix multiplication\nT2(T1x) = (T2T1)x\nExample 3\nV = all functions a + bx + cx2\nY = all functions d + ex\nT (a + bx + cx2) = derivative of the input function = output b + 2cx\n\"Derivative\" is a linear transformation ! Otherwise calculus would fail\n\"Integral\" is also a linear transformation on a space of functions\n\nPart 8 :\nLinear Transformations and Their Matrices\n8.2\nDerivative Matrix D and Integral Matrix D+\nD+\nD+\nChoose basis 1, x, x2 for input space V : Quadratic functions\nChoose basis 1, x for output space Y : Linear functions\nApply derivative transformation to the input basis v1=1, v2 =x, v3=x2\nExpress outputs T(v1) = 0, T(v2) = 1, T(v3) = 2x in the output basis\nT(v1) = 0\nT(v2) = dx\ndx = 1 = u1\nT(v3) = d\ndx(x2) = 2x = 2u2\nThe columns of D show those derivatives with respect to the bases\nD =\n\n= matrix form of the derivative T = d\ndx\nD times\n\na\nb\nc\n\n=\n\nb\n2c\n\ntells us the outputs from the inputs a, bx, cx2\nIntegral transformation S from Y back to V\nInputs 1, x\nOutputs 1, x, x2\nS(1) = x = v2\nS(x) = 1\n2x2 = 1\n2v3\nIntegral matrix E =\n\nFundamental Theorem of Calculus : Derivative of integral of f is f\nDE =\n\n=\n\n= identity transformation on Y\nED =\n\n=\n\n= only a projection on V\nE =pseudoinverse D+of D\nNot the inverse because derivative of 1 is 0\n\nZoomNotes for Linear Algebra\n8.3\nBasis for V and Basis for Y ⇒Matrix for T : V→Y\nEvery linear transformation T : V →Y can be expressed by a matrix A\nThat matrix A depends on the basis for V and the basis for Y\nTo construct A : Apply T to the input basis vectors v1 to vn\nThen T(vj) = a1jy1 + a2jy2 + · · · + amjym gives column j of A\nInput v = c1v1 + · · · + cnvn\nOutput y = c1T(v1) + · · · + cnT(vn)\nThat output y has coefficients Ac in the output basis for Y\nMain point !\nMultiplication by A copies the linear transformation T\nBoth linear and both correct for basis ⇒both correct for every input\nChange input basis to V 1, . . . , V n\nChange output basis to Y 1, . . . , Y m\nThe matrix for the same T in these new bases is\nM = Y -1AV\nV = identity on input space : but basis change from v's to V 's\nY = identity on output space : but basis change from y's to Y 's\n\nPart 9\nComplex Numbers\nand the Fourier Matrix\n9.1\nComplex Numbers x + iy = reiθ : Unit circle r = 1\n9.2\nComplex Matrices : Hermitian S = S\nT and Unitary Q-1 = Q\nT\n9.3\nFourier Matrix F and the Discrete Fourier Transform\n9.4\nCyclic Convolution and the Convolution Rule\n9.5\nFFT : The Fast Fourier Transform\n\nZoomNotes for Linear Algebra\n✬\n✫\n✩\n✪\nR = line of all real numbers -inf< x < inf\n↔\nC = plane of all complex numbers z = x + iy\n|x| = absolute value of x\n↔\n|z| =\np\nx2 + y2 = r = absolute value (or modulus) of z\n1 and -1 solve x2 = 1\n↔\nz = 1, w, . . . , wn-1 solve zn = 1 where w = e2πi/n\nThe complex conjugate of z = x + iy is z = x -iy.\n|z|2 = x2 + y2 = zz and 1\nz =\nz\n|z|2 .\nThe polar form of z = x + iy is |z|eiθ = reiθ = r cos θ + ir sin θ. The angle has tan θ = y\nx.\nRn: vectors with n real components\n↔Cn: vectors with n complex components\nlength: ∥x∥2 = x2\n1 + · · · + x2\nn ↔length: ∥z∥2 = |z1|2 + · · · + |zn|2\ntranspose: (AT)ij = Aji ↔conjugate transpose: (AH)ij = Aji\ndot product: xTy = x1y1 + · · · + xnyn ↔inner product: uHv = u1v1 + · · · + unvn\nreason for AT: (Ax)Ty = xT(ATy) ↔reason for AH: (Au)Hv = uH(AHv)\northogonality: xTy = 0 ↔orthogonality: uHv = 0\nsymmetric matrices: S = ST ↔Hermitian matrices: S = SH\nS = QΛQ-1 = QΛQT(real Λ) ↔S = UΛU-1 = UΛU H (real Λ)\northogonal matrices: QT = Q-1 ↔unitary matrices: U H = U -1\n(Qx)T(Qy) = xTy and ∥Qx∥= ∥x∥↔(Ux)H(Uy) = xHy and ∥Uz∥= ∥z∥\n\nPart 9 : Complex Numbers\nand the Fourier Matrix\n9.1\nComplex Numbers x + iy = reiθ : Unit circle r = 1\nComplex numbers z = x + iy\nx = real part\ny = imaginary part\nMagnitude\n|z| = r =\np\nx2 + y2\nAngle\ntan θ = y\nx\nEuler's Formula\nz = reiθ = r cos θ + ir sin θ = r x\nr + ir y\nr\nComplex conjugate\nz = x -iy = re-iθ\nThen zz = x2 + y2 = r2\nAdd z1 + z2=z1 + z2\nMultiply (z1) (z2)=z1z2\nDivide 1\nz = z\n|z|2 = a -ib\na2 + b2\nReal axis\nz =1 + i=\n√\n2eiπ/4\nOn the circle |z| = |eiθ| = 1\nComplex\nplane\nz =1 -i=\n√\n2e-iπ/4\n+i\n-1\n+1\n-i\nUnit\ncircle\n|z|=1\nComplex conjugate\nπ/4\n-π/4\nAdd angles\n(reiθ) (Reiφ) = rR ei(θ+φ)\n(i)4=(eiπ/2)4 = ei2π = 1\n\nZoomNotes for Linear Algebra\n9.2\nComplex Matrices : Hermitian S = S\nT and Unitary Q-1 = Q\nT\nRule\nWhen you transpose, take complex conjugates : xT A\nT\nAutomatic for computer systems like MATLAB and Julia\nInner product = Dot product = xTy = x1y1 + · · · + xnyn\nLength squared = ||x||2 = xTx = |x1|2 + · · · + |xn|2 = ||Re x||2 + ||Im x||2\nHermitian matrix\nS =\n\n3 -3i\n3 + 3i\n\n= S\nT\nReal diagonal\nSji = Sij\nS has real eigenvalues 8, -1 and perpendicular eigenvectors\ndet(S -λI) = λ2 -7λ + 10 -\n3 + 3i\n2 = (λ -8) (λ + 1)\n(S -8I)\n\n1 + i\n\n=\n\n(S + I)\n\n1 -i\n-1\n\n=\n\nUnitary matrix\nOrthonormal columns\nQ = 1\n√\n\n1 -i\n1 + i\n-1\n\nQ\nT = Q-1\n||Qz|| = ||z||\n|λ| = 1\nThe Fourier matrix F\n√\nN\nis the most important unitary matrix\n\nPart 9 :\nComplex Numbers and the Fourier Matrix\n9.3\nFourier Matrix F and the Discrete Fourier Transform\nFourier Matrix F4 =\n\ni\ni2\ni3\ni2\ni4\ni6\ni3\ni6\ni9\n\nDFT Matrix F4 = powers of -i\nFN and FN\nN by N matrices\nReplace i = e2πi/4\nby w = e2πi/N\nFjk = wjk = e2πijk/N\nColumns k = 0 to N -1\nw\ni\n1 = w8\n-1\nw3\nw5\nw7 = 1\nw\nw = e2πi/8\nw8 = 1\n1 + w + w2 + · · · + wN-1 = 0\nFNFN = NI\nThen FN/\n√\nN is a unitary matrix. It has orthonormal columns\nN = 2\nw = eπi = -1\nF2 =\n\n-1\n\nF 2F2 =\n\n= NI\nDiscrete Fourier Transform\nf to c\nc = F -1\nN f\nInverse Fourier Transform\nc to f\nf = FNc\n\nZoomNotes for Linear Algebra\n9.4\nCyclic Convolution and the Convolution Rule\nc = (1, 2, 3)\nConvolution c ∗d = (5, 10, 19, 8, 12)\nd = (5, 0, 4)\nCyclic c ∗\n⃝d = (5 + 8, 10 + 12, 19)\n(1 + 2x + 3x2) (5 + 4x2) = 5 + 10x + 19x2 + 8x3 + 12x4\n8 12\nx3 = 1 for cyclic (5 + 8) + (10 + 12)x + 19x2\n5 10 15\n5 10 19\n8 12\nConvolution matrices\nConstant diagonals\nC =\n\nD=\n\nCD =\nDC =\n\nEigenvectors of C and D = columns of the Fourier matrix F\nEigenvalues of C = F -1c\nEigenvalues of D = F -1d\nConvolve vectors\nMultiply transforms\nConvolution Rule\nF (c ∗\n⃝d) = (F c) .∗(F d)\nx . ∗y = (x1y1, . . . , xnyn)\nKey to Signal Processing\n\nPart 9 :\nComplex Numbers and the Fourier Matrix\n9.5\nFFT : The Fast Fourier Transform\nDirect matrix multiplication of c by FN needs N 2 multiplications\nFFT factorization with many zeros : 1\n2N log2 N multiplications\nN = 210 = 1024\nlog2 N = 10\n1 million reduced to 5000\nStep 1 of the FFT : From 1024 to 512 (Cooley-Tukey)\n\"\nF1024\n#\n=\n\"\nI\nD\nI\n-D\n# \"\nF512\nF512\n# \"\nP1024\n#\nPermutation P1024 puts columns 0, 2, . . . , 1022 ahead of 1, 3, . . . , 1023\nTwo zero blocks reduce the computing time nearly by 50%\nStep 2 of the FFT : 512 to 256 (same factorization of F512)\nRecursion continues to small N : log2 N steps to Fast Transform\nEach step has N multiplications from the diagonal matrices D\nOne overall permutation = product of the P 's\nFFTW is hardwired in many computers / bases other than 2\n\nPart 10\nLearning from Data\nby Gradient Descent\n10.1\nLearning Function F (x, v0) : Data v0 and Weights x\n10.2\nCounting Flat Pieces in the Graph of F\n10.3\nMinimizing the Loss : Stochastic Gradient Descent\n10.4\nSlow Convergence with Zigzag : Add Momentum\n10.5\nConvolutional Neural Nets : CNN in 1D and 2D\n10.6\nBackpropagation : Chain Rule for ∇F\n\nPart 10 :\nLearning from Data\nby Gradient Descent\n10.1\nLearning Function F (x, v0) : Data v0 and Weights x\nTraining data = p features for N samples = N vectors v0\nEach of those N vectors enters level zero of the neural net\nLevel k from k -1 : Multiply each vector by Ak, add bk, apply ReLU\nvk = Fk(vk-1) = ReLU(Akvk-1 + bk)\nReLU applies to each component of each vector : ReLU(y) = max(y, 0)\ny\nReLU(y)\nReLU = ramp function\n= Rectified Linear Unit\nThis gives the nonlinearity that learning functions need\nLevels 0 to L\nOutput vL = FL(. . . (F2(F1(v0)))) = F(v0)\nF (x, v0) = Composition of L piecewise linear Fk : F is piecewise linear\nEach level contributes a weight matrix Ak and a vector bk to x\nvp\nv1\np=3, q=4\n(Av)q\n[(Av + b)q]+\nNeural Net\n(Av)1\n[(Av + b)1]+\nOne hidden layer\npq + 2q = 20 weights\nC[Av + b]+ = w\nInputs\nr(4, 3) = 15 linear pieces\nin the graph of w = F(v)\nReLU\nReLU\nReLU\nReLU\n\nZoomNotes for Linear Algebra\n10.2\nCounting Flat Pieces in the Graph of F\nThe weight matrices Ak and bias vectors bk produce F = learning function\nEach application of ReLU creates a fold in the graph of F\n1a\n3a\n2a\n1b\n2b\n3b\nH\nStart with 2 folds\n←r(2, 2) = 4\nAdd new fold H\nN = 3 folds in\np = 2 dimensions\n←r(2, 1) = 3\nTotal 7 regions\nThe r(2, 1)\n=\n3 pieces of the new fold H create new regions 1b, 2b, 3b.\nThen the count becomes r(3, 2) = 4+3 = 7 flat regions in the continuous piecewise\nlinear surface\nv2\n=\nF(v0).\nA fourth\nfold\nwould\ncross\nall\nthree\nexisting folds and create 4 new regions, so r (4, 2) = 7 + 4 = 11.\nThe count r of linear pieces of F will follow from the recursive formula\nr(N, p) = r(N -1, p) + r(N -1, p -1)\nTheorem\nFor v in Rp, suppose the graph of F(v) has folds along\nN hyperplanes H1, . . . , HN.\nThose come from ReLU at N neurons.\nThen the number of regions bounded by the N hyperplanes is r(N, p) :\nr(N, p) =\n\nN\n\n+\n\nN\n\n+ · · · +\n\nN\np\n\n.\nThese binomial coefficients are\n\nN\ni\n\n=\nN !\ni !(N -i) ! with 0 !=1 and\n\nN\n\n=1 and\n\nN\ni\n\n=0 for i>N.\nWith more layers : N folds from N ReLU's : still ≈r(N, p) ≈cN p pieces\n\nPart 10 :\nLearning from Data : Minimize Loss by Gradient Descent\n10.3\nMinimizing the Loss : Stochastic Gradient Descent\nThe gradient of a function F (x1, . . . , xp) is a vector ∇F = grad F\n∇F = (∂F/∂x1, . . . , ∂F/∂xn) points in the steepest direction for F(x)\nThe graph of y = F (x) = F (x1, . . . , xN) is a surface in N + 1 dimensions\nThe graph of F = x2\n1 + x2\n2 + 5 is a bowl in 2 + 1 = 3 dimensions\nMinimum of F = ||x||2 + 5 is Fmin = 5 at the point x = arg min F = 0\nWe want the minimizing point x = arg min F for a complicated F(x)\nGradient descent starts from a point x0. Go down along the gradient ∇F (x0)\nStop at a point x1 =x0 -s∇F (x0). Stepsize=learning rate=s=maybe .001\nRecompute the gradient ∇F (x1) at the new point x1\nAt every step follow the gradient ∇F (xk) to xk+1 = xk -sk∇F (xk)\nBig Problem 1\nMany unknowns x1 to xN : all weights in all L layers\nBig Problem 2\nF (x) = sum of errors in all training samples : many terms\nError\nSquare loss\n\noutput\nlayerL -known\noutput\n\nor \"Cross-entropy loss\"\nSolution 1\nUse error in only one randomly chosen sample / one v0\nSolution B Use sum of errors in only B random samples : minibatch\nStochastic gradient descent has new sampling at every step. Successful\n\nZoomNotes for Linear Algebra\n10.4\nSlow Convergence with Zigzag : Add Momentum\nTest example : Minimize F (x, y) = 1\n2(x2 + by2) with small b > 0\nGradient ∇F = (x, by). Exact search (xk+1, yk+1) = (xk, yk) -(best s)∇F\nxk = b\nb -1\nb + 1\nk\nyk =\n1 -b\n1 + b\nk\nF (xk, yk) =\n1 -b\n1 + b\n2k\nF(b, 1)\nCrucial ratio\n1 -b\n1 + b\nis near 1 for small b : Slow convergence !\nThe path zig-zags across a narrow valley : moves slowly down to (0, 0)\nHeavy ball\nAdd momentum\nDirection zk+1 remembers zk\n(x, y)k+1 = (x, y)k -szk\nzk+1 -∇F (x, y)k+1 = βzk\nOptimal s\nOptimal β\ngive fast descent :\nratio 1 -b\n1 + b changes to 1 -\n√\nb\n1 +\n√\nb\nb =\n1 -b\n1 + b\n=\n.99\n1.01\n≈.96 changes to\n0.9\n1.1\n≈.67 !\nADAM\nGk combines all earlier gradients by Gk = δGk-1 + (1 -δ)∇F (xk)\nQuestion\nWhy do the weights (matrices Ak) work well for unseen data ?\n\nPart 10 :\nLearning from Data : Minimize Loss by Gradient Descent\n10.5\nConvolutional Neural Nets : CNN in 1D and 2D\nConvolution matrix = Moving average\nA has constant diagonals\nSliding window : same weights 1\n2, 1\n2 in each window\nA =\n\n2D Convolution for Images\nWindows move across and down\nNine 3 × 3 windows in 5 × 5 square\nCenter points are marked 1 to 9\nOnly 32 = 9 weights to choose\nA convolutional filter treats all positions the same\n1. Many weights repeated--distant weights are zero\n2. 32 = 9 weights copied in every window\n3. No reason to treat positions differently--\"shift invariant\"\nRecognizing digits (like Zip codes) in MNIST : Basic test data\nMax-pooling\nReduce dimensions\nTake max from each block of outputs\nSoftmax\nConvert outputs wk to probabilities pk = ewk/ P ewk\nResidual network\nAdd skip connections that jump several layers\nBatch normalization\nReset the input variance at each new layer\n\nZoomNotes for Linear Algebra\n10.6\nBackpropagation : Chain Rule for ∇F\nF(x)= minimum\n∇F = partial derivatives\n∂errors / ∂weights=zero\nChain rule\nd\ndx\n\nF2(F1(x))\n\n=\ndF2\ndF1\n(F1(x))\ndF1\ndx (x)\n\nMultivariable chain rule\n∂w\n∂u =\n∂w\n∂v\n∂v\n∂u\n\nL layers in chain\nMultiply L matrices\n∂w\n∂v =\n\n∂w1\n∂v1\n· · ·\n∂w1\n∂vn\n· · ·\n∂wp\n∂v1\n· · ·\n∂wp\n∂vn\n\n∂v\n∂u =\n\n∂v1\n∂u1\n· · ·\n∂v1\n∂um\n· · ·\n∂vn\n∂u1\n· · ·\n∂vn\n∂um\n\np × n\nn × m\nAt each layer\nDerivatives before ReLU\nw = Av + b\n∂wi\n∂bj\n= δij = 0 or 1\n∂wi\n∂Ajk\n= δijvk\nProduct of matrices ABC\nAB first or BC first ?\nForward or back ?\nFor ∇F in deep learning, going backward is faster : Reverse mode BC first\nExample\nA=m × n\nB =n × p\nC =p × 1 vector\nDon't multiply AB !\nBackpropagation = Automatic Differentiation = the key to speed\n\nPart 11\nBasic Statistics :\nMean, Variance, Covariance\n11.1\nMean and Variance : Actual and Expected\n11.2\nProbability Distributions : Binomial, Poisson, Normal\n11.3\nCovariance Matrices and Joint Probabilities\n11.4\nThree Basic Inequalities of Statistics\n11.5\nMarkov Matrices and Markov Chains\n\nPart 11 :\nBasic Statistics :\nMean, Variance, Covariance\n11.1\nMean and Variance : Actual and Expected\nThe sample mean μ is the average of outputs from N trials\nThe expected mean m is based on probabilities p1, . . . , pn of outputs x1, . . . , xn\nExpected value m = E[x] = p1x1 + · · · + pnxn\nLaw of Large Numbers : With probability 1, sample mean →m as N →inf\nThe sample variance measures the spread around the sample mean μ\nS2 =\nN -1\n\n(x1 -μ)2 + · · · + (xN -μ)\nThe variance is the expected value of (x -m)2 based on probabilities\nσ2 = E[(x -m)2] = p1(x1 -m)2 + · · · + pn (xn -m)2\nSecond formula for this important number : σ2 = P pi x2\ni -m2\nFair coin flip\nx=0 or 1, p1=p2 = 1\n2 : Mean m= 1\nVariance σ2 = 1\n2 -1\n4 = 1\nContinuous probability distributions: Sums change to integrals\nR\np(x) dx=1\nm=\nR\nx p(x) dx\nσ2 =\nR\n(x-m)2 p(x) dx\n\nPart 11 :\nBasic Statistics : Mean, Variance, Covariance\n11.2\nProbability Distributions : Binomial, Poisson, Normal\nBinomial : pk,n = probability of k heads in n trials (coin flips)\np1,1 = p\npn,n = pn\npk,n =\nn!\nk! (n -k)! pk(1 -p)n-k\n(0 ! = 1)\nMean m in n trials = np\nVariance σ2 in n trials = np (1 -p)\nPoisson : Rare events p →0, many trials n →infKeep np=λ constant\nNo successes p0,n=(1 -p)n =\n\n1 -λ\nn\nn\n→e-λ\nk successes pk,n →λk\nk! e-λ\nPoisson mean = λ\nvariance σ2 = λ\nLimits of binomial np and np (1 -p)\nNormal distribution :\nN(m, σ2) has\np(x) =\n√\n2π σ\ne-(x -m)2/2σ2\nBell-shaped curve / Symmetric around mean / Standard N(0, 1) is\n√\n2π\ne-x2/2\nShifted and scaled\nX = x -m\nσ\nCentered and normalized\nCentral Limit Theorem for any distribution p(x)\nAverage many samples\nThe probabilities for the average X of X1 to XM approaches N(0, 1) as M →inf\nNormal p(x) for n variables Means m = (m1, . . . , mn)\nCovariance matrix V\np(x) = p(x1, . . . , xn) =\n√\n2π\nn √\ndet V\ne-(x-m)\nT V\n-1 (x-m)/2\n\nZoomNotes for Linear Algebra\n11.3\nCovariance Matrices and Joint Probabilities\nM experiments at once\nM = 2 for (age x, height y)\nMean m = (mx, my) = (average age, average height)\nJoint probabilities\npij = probability that age = i and height = j\npi =\nX\nj\npij = probability that age = i allowing all heights j\nExpected value of (x -mx)2 = σ2\n11 =\nX\npi (xi -mx)2 = usual variance\nExpected value of (x-mx)(y-my)=σ12 =\nX\ni\nX\nj\npij(xi-mx)(yj-my)\nCovariance matrix V =\nX\ni\nX\nj\npij\n\n(xi-mx)2\n(xi-mx)(yj-my)\n(xi-mx)(yj-my)\n(yj-my)2\n\nV = sum of positive semidefinite rank 1 matrices = semidefinite or definite\nV is positive definite unless age tells you the exact height (dependent case)\nV is a diagonal matrix if age and height are independent : covariance = 0\nCoin flip\nGlue 2 coins together\nV =\n\n1/4\n1/4\n1/4\n1/4\n\n= dependent case :\nsemidefinite V\nSeparate the coins\nV =\n\n1/4\n1/4\n\n= independent :\ndiagonal V\n\nPart 11 :\nBasic Statistics : Mean, Variance, Covariance\n11.4\nThree Basic Inequalities of Statistics\nMarkov's inequality when x ≥0 : No negative samples\nThe probability of x ≥a is at most E[x]\na\n= mean m\na\nSuppose a = 3 and mean m = P xipi = 0p0 + 1p1 + 2p2 + · · ·\nMarkov's inequality says probability p3 + p4 + p5+ · · · ≤m\nWrite m = p1 + 2p2 + 3(p3 + p4 + p5 + · · · ) + p4 + 2p5 + · · ·\nNo negative terms so m ≥3(p3 + p4 + p5 + · · · )\nTHIS IS MARKOV\nChebyshev's inequality\nThe probability of |x -m| ≥a is at most σ2\na2\nProof\nApply Markov's inequality to the new variable y = |x -m|2\nThe mean value E[y] for y is the variance σ2 for x\nApply Markov !\nThe probability of y ≥a2 is at most E[y]\na2\n= σ2\na2\nChernoff's inequality\nS = X1 + · · · + Xn independent random variables\nWhat is the probability that S is far from its mean S ?\nProb\nS ≥(1 + δ)S\n\n≤e-Sδ2/(2+δ)\nExponential dropoff !\nProb\nS ≤(1 -δ)S\n\n≤e-Sδ2/2\nBound for 2δ = (Bound for δ)4\nReason : A large sum S usually needs several Xi to be large / unlikely !\n\nZoomNotes for Linear Algebra\n11.5\nMarkov Matrices and Markov Chains\nMarkov matrix\nAll Mij ≥0\nAll columns add to 1\nPerron-Frobenius\nEigenvalues of M\nλmax = 1\n| other λ |< 1 if all Mij > 0\n| other λ |≤1 if all Mij ≥0\nMarkov chain\npn+1 = Mpn\nProbabilities at times n + 1 and n\nM =\n\n0.8\n0.3\n0.2\n0.7\n\nhas λ = 1 and λ = 1\nMn has λ = 1 and λ =\nn\n\nRental cars in Chicago\nRental cars in Denver\n\nn + 1\n= M\n\nin Chicago\nin Denver\n\nn\nyn+1 = Myn\nStart in Chicago y0 =\n\ny1 =\n\ny2 =\n\ny3 =\n\nStart in Denver\ny0 =\n\ny1 =\n\ny2 =\n\nyinf=\n\nSteady state from every start : Eigenvector of M for λ = 1 is\n\nOther eigenvalue λ = 1\n2 : Distance to\n\nis halved at every step\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu/\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms.\n18.06SC Linear Algebra\nFall 2011"
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_FinalRevsum.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/89e753b45eed437828821e585bed0e59_MIT18_06SCF11_FinalRevsum.pdf",
      "content": "\"\n#\n\"\n#\n\"\n#\n\"\n#\n\"\n#\n\"\n#\nFinal course review\nOnce more, we review questions from a previous exam to prepare ourselves\nfor an upcoming exam.\n1. Suppose we know that A is an m by n matrix of rank r, Ax =\nhas\nno solution, and Ax =\nhas exactly one solution.\na) What can we say about m, n and r?\nThe product Ax is a vector in three dimensions, so m = 3.\nThe fact that Ax =\nhas no solution tells us that the column\nspace is not all of R3. In addition, we know that the column space\ncontains\n, so r is not zero: 1 ≤ r < 3.\nThe fact that Ax =\nhas exactly one solution tells us that the\nnullspace of A contains only the zero vector and so n = r. Hence\n1 ≤ n < 3.\nb) Write down an example of a matrix A that fits this description.\n\nThe vector 1 must be in the column space, so we'll make it a\ncolumn of A. The simplest way to answer this question is to stop\nhere.\n\nA = 1 .\nIn this solution, n = r = 1 and m = 3.\nTo find a solution in which n = r = 2, add a second column. Make\nsure that\nis not in the column space:\n\nA = 1\n0 .\nThere are many other correct answers to this question.\n\n\"\n#\nc) Cross out all statements that are false about any matrix with the given\nproperties (which are 1 ≤ r = n, m = 3).\ni. det AT A = det AAT\nii. AT A is invertible\niii. AAT is positive definite\nOne good approach to this problem is to use our sample matrix to test\neach statement.\ni. If we leave this part to last, we can quickly answer it (false) using\nwhat we learn while answering the following two parts.\nii. The matrix AT A is invertible if r = n; i.e. if the columns of A are\nindependent.\nThe nullspace of our A contains only the zero vector, so this state-\nment is true.\nFor each of our sample matrices, ATA equals the identity and so\nis invertible.\nNote that this means det AT A = 0.\niii. We know that m = 3 and r < 3, so AAT will be a 3 by 3 ma\ntrix with rank less than 3; it can't be positive definite. (It is true\nthat for any matrix A with real valued entries, AAT is positive\nsemidefinte.)\nFor our test matrices, AAT has at least one row that's all zeros, so\n0 is an eigenvalue (and is not positive).\nNote also that det AAT = 0 and so statement (i) must be false.\n(However, if A and B are square matrices then det BA = det AB =\ndet A det B.)\nd) Prove that ATy = c has at least one solution for every right hand side\nc, and in fact has infinitely many solutions for every c.\nWe know AT is an n by m matrix with m = 3 and rank r = n < m.\nIf AT has full row rank, the equation ATy = c is always solvable.\nWe have n rows and rank r = n, so AT has full row rank. Therefore\nATy = c has a solution for every vector c.\nThe solvable system ATy = c will have infinitely many solutions if\nthe nullspace of AT has positive dimension. We know dim(N(AT)) =\nm -r > 0, so ATy = c has infinitely many solutions for every c.\n2. Suppose the columns of A are v1, v2 and v3.\na) Solve Ax = v1 -v2 + v3.\nThis is just the \"column method\" of multiplying matrices from the\nfirst lecture. Choose x =\n-1\n.\n\n\"\n#\nb) True or false: if v1 - v2 + v3 = 0, then the solution to (2a) is not\nunique. Explain your answer.\n\nTrue. Any scalar multiple of x = -1 will be a solution.\nAnother way of answering this is to note that AT has a nontrivial null\nspace, and we can always add any vector in the nullspace to a solution\nx to get a different solution.\nc) Suppose v1, v2 and v3 are orthonormal (forget about (2b)). What com\nbination of v1 and v2 is closest to v3?\nIf we imagine the right triangle out from the origin formed by av1 +\nbv2 and v3, the Pythagorean theorem tells us that 0v1 + 0v2 = 0 is the\nclosest point to v3 in the plane spanned by v1 and v2.\n3. Suppose we have the Markov matrix\n\n.2\n.4\n.3\nA = .4\n.2\n.3 .\n.4\n.4\n.4\nNote that the sum of the first two columns of A equals twice the third\ncolumn of A.\na) What are the eigenvalues of A?\nZero is an eigenvalue because the columns of A are dependent. (A is\nsingular.)\nOne is an eigenvalue because A is a Markov matrix.\nThe third eigenvalue is -.2 because the trace of A is .8.\nSo λ =\n0, 1, -.2.\nb) Let uk = Aku(0). If u(0) =\n, what is limk→inf uk?\nWe'll start by computing uk and then find the steady state. This means\nfinding a general expression of the form:\nuk = c1λ1\nk x1 + c2λ2\nk x2 + c3λ3\nk x3.\nWhen we plug in the eigenvalues we found in part (3a), this becomes\nuk = 0 + c2x2 + c3(-.2)k x3.\nWe see that as k approaches infinity, c2x2 is the only term that does\nnot go to zero.\nThe key eigenvector in any Markov process is the one with eigenvalue\none.\n\n\"\n#\n\"\n#\n\"\n#\n\nTo find x2, solve (A -1I)x2 = 0:\n\n-.8\n.4\n.3\n\n.4\n-.8\n.3 x2 = 0.\n.4\n.4\n-.6\nThe best way to solve this might be by elimination. However, because\nthe first two columns look like multiples of 4 and the third column\nlooks like a multiple of 3, we might get lucky and guess x2 =\n.\nThis gives us uinf = c2\n. We know that in a Markov process, the\nsum of the entries of uk is the same for all k. The sum of the entries of\nu(0) is 10, so c2 = 1 and uinf =\n.\n4. Find a two by two matrix that:\na) projects onto the line spanned by a =\n.\n-3\nT\naa\nThe formula for this matrix is P =\n. This gives us\nT\na a\n16/25\n-12/25\nP =\n.\n-12/25\n9/25\n(To test this answer, we can quickly check that det P = 0.)\nb) has eigenvalues λ1 = 0 and λ2 = 3 and eigenvectors x1 =\nand\nx2 =\n.\nHere the formula we need is A = SΛS-1.\n\n-1\nA =\n-1/3\n2/3\n=\n2/3\n-1/3\n-2\nA =\n.\n-1\nIf time permits, we can check this by computing the products Axi.\nc) has real entries and cannot be factored as BTB for any B.\nWe know that BTB will always be symmetric, so any asymmetric ma\ntrix has this property. For example, we could choose\n.\n\n\"\n#\n\"\n#\n\"\n#\n\nd) is not symmetric, but has orthogonal eigenvectors.\nWe know that symmetric matrices have orthogonal eigenvectors, but\nso do other types of matrices (e.g. skew symmetric and orthogonal)\nwhen we allow complex eignevectors.\nTwo possible answers are:\n(skew symmetric)\n-1\ncos θ -sin θ\n(orthogonal).\nsin θ\ncos θ\n5. Applying the least squares method to the system\n\nc\n= 4 = b\nd\ncˆ\n11/3\ngives the best fit vector\n=\n.\ndˆ\n-1\na) What is the projection p of b =\nonto the column space of\nA =\n?\nWe know that 11/3 times the first column minus 1 times the second\ncolumn is the closest point P in the column space to\n, so the\nanswer is\n\n11/3\ncˆ\nA\n=\n1 - 1 =\n8/3 .\ndˆ\n5/3\nb) Draw the straight line problem that corresponds to this system.\nPlotting the entries of the second column of A against the entries of b\nwe get the three points shown in Figure 1. The best fit line is cˆ + dtˆ .\nc) Find a different vector b = 0 ∈ R3 so that the least squares solution is\ncˆ\ndˆ\n=\n.\ncˆ\nWe know that\ndˆ\nis the projection of b onto the column space, so to\nget a zero projection we need to find a vector orthogonal to the columns.\n\n\"\n#\n(1, 4)\n-1\n-1\nFigure 1: Three data points and their \"best fit\" line 11 -t.\nb = 11 -t\n(0, 3)\n(2, 1)\nP1\nP2\nP3\nWe could get the answer b =\n-2\nby inspection, or we could use\nthe cross product of the columns to find a value for b.\nThank you for taking this course!\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_Ses1.10sum.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/62a9db9eeab190694d40afe4734068ca_MIT18_06SCF11_Ses1.10sum.pdf",
      "content": "The four fundamental subspaces\nIn this lecture we discuss the four fundamental spaces associated with a matrix\nand the relations between them.\nFour subspaces\nAny m by n matrix A determines four subspaces (possibly containing only the\nzero vector):\nColumn space, C(A)\nC(A) consists of all combinations of the columns of A and is a vector space in\nRm .\nNullspace, N(A)\nThis consists of all solutions x of the equation Ax = 0 and lies in Rn .\nRow space, C(AT)\nThe combinations of the row vectors of A form a subspace of Rn . We equate\nthis with C(AT), the column space of the transpose of A.\nLeft nullspace, N(AT )\nWe call the nullspace of AT the left nullspace of A. This is a subspace of Rm .\nBasis and Dimension\nColumn space\nThe r pivot columns form a basis for C(A)\ndim C(A) = r.\nNullspace\nThe special solutions to Ax = 0 correspond to free variables and form a basis\nfor N(A). An m by n matrix has n - r free variables:\ndim N(A) = n - r.\n\nRow space\nWe could perform row reduction on AT, but instead we make use of R, the row\nreduced echelon form of A.\n⎡\n⎤\n⎡\n⎤\n\nI\nF\nA = ⎣ 1\n1 ⎦\n⎣ 0\n0 ⎦ =\n= R\n→ · · · →\nAlthough the column spaces of A and R are different, the row space of R is the\nsame as the row space of A. The rows of R are combinations of the rows of A,\nand because reduction is reversible the rows of A are combinations of the rows\nof R.\nThe first r rows of R are the \"echelon\" basis for the row space of A:\ndim C(AT) = r.\nLeft nullspace\nThe matrix AT has m columns. We just saw that r is the rank of AT, so the\nnumber of free columns of AT must be m - r:\ndim N(AT) = m - r.\nThe left nullspace is the collection of vectors y for which ATy = 0. Equiva\nlently, yT A = 0; here y and 0 are row vectors. We say \"left nullspace\" because\nyT is on the left of A in this equation.\nTo find a basis for the left nullspace we reduce an augmented version of A:\n.\nAm×n\nIm×n\n-→\nRm×n\nEm×n\nFrom this we get the matrix E for which EA = R. (If A is a square, invertible\nmatrix then E = A-1.) In our example,\n⎡\n⎤⎡\n⎤\n⎡\n⎤\n⎣\n-1\nEA =\n1 -1\n0 ⎦⎣ 1\n1 ⎦ = ⎣ 0\n0 ⎦ = R.\n-1\nThe bottom m - r rows of E describe linear dependencies of rows of A, because\nthe bottom m - r rows of R are zero. Here m - r = 1 (one zero row in R).\nThe bottom m - r rows of E satisfy the equation yTA = 0 and form a basis\nfor the left nullspace of A.\nNew vector space\nThe collection of all 3 × 3 matrices forms a vector space; call it M. We can\nadd matrices and multiply them by scalars and there's a zero matrix (additive\nidentity). If we ignore the fact that we can multiply matrices by each other,\nthey behave just like vectors.\nSome subspaces of M include:\n\n- all upper triangular matrices\n- all symmetric matrices\n- D, all diagonal matrices\nD is the intersection of the first two spaces. Its dimension is 3; one basis for D\nis:\n⎡\n⎤⎡\n⎤⎡\n⎤\n⎣ 0\n0 ⎦ , ⎣ 0\n0 ⎦ , ⎣ 0\n0 ⎦ .\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_Ses1.11sum.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/732789d4b5c1247ccab5faf13b6f29d2_MIT18_06SCF11_Ses1.11sum.pdf",
      "content": "\"\n# \"\n# \"\n#\n\"\n# \"\n#\n\nMatrix spaces; rank 1; small world graphs\nWe've talked a lot about Rn, but we can think about vector spaces made up of\nany sort of \"vectors\" that allow addition and scalar multiplication.\nNew vector spaces\n3 by 3 matrices\nWe were looking at the space M of all 3 by 3 matrices. We identified some\nsubspaces; the symmetric 3 by 3 matrices S, the upper triangular 3 by 3 matrices\nU, and the intersection D of these two spaces - the space of diagonal 3 by 3\nmatrices.\nThe dimension of M is 9; we must choose 9 numbers to specify an element\nof M. The space M is very similar to R9. A good choice of basis is:\n,\n,\n, ...\n,\n.\nThe subspace of symmetric matrices S has dimension 6. When choosing an\nelement of S we pick three numbers on the diagonal and three in the upper\nright, which tell us what must appear in the lower left of the matrix. One basis\nfor S is the collection:\n0 0\n0 0\n1 0\n0 0\n0 0\n,\n,\n,\n,\n,\n.\nThe dimension of U is again 6; we have the same amount of freedom in\nselecting the entries of an upper triangular matrix as we did in choosing a\nsymmetric matrix. A basis for U is:\n,\n,\n,\n,\n,\n.\nThis happens to be a subset of the basis we chose for M, but there is no basis\nfor S that is a subset of the basis we chose for M.\nThe subspace D = S ∩ U of diagonal 3 by 3 matrices has dimension 3.\nBecause of the way we chose bases for U and S, a good basis for D is the inter\nsection of those bases.\nIs S ∪ U, the set of 3 by 3 matrices which are either symmetric or upper\ntriangular, a subspace of M? No. This is like taking two lines in R2 and asking\nif together they form a subspace; we have to fill in between them. If we take\nall possible sums of elements of S and elements of U we get what we call the\nsum S + U. This is a subspace of M. In fact, S + U = M. For unions and sums,\ndimensions follow this rule:\ndim S + dim U = dim S ∪ U + dim S ∩ U.\n\nDifferential equations\nAnother example of a vector space that's not Rn appears in differential equa\ntions.\nd2y\nWe can think of the solutions y to dx2 + y = 0 as the elements of a nullspace.\nSome solutions are:\ny = cos x,\ny = sin x,\nand y = eix .\nThe complete solution is:\ny = c1 cos x + c2 sin x,\nwhere c1 and c2 can be any complex numbers. This solution space is a two\ndimensional vector space with basis vectors cos x and sin x. (Even though these\ndon't \"look like\" vectors, we can build a vector space from them because they\ncan be added and multiplied by a constant.)\nRank 4 matrices\nNow let M be the space of 5 × 17 matrices. The subset of M containing all rank\n4 matrices is not a subspace, even if we include the zero matrix, because the\nsum of two rank 4 matrices may not have rank 4.\n⎤\n⎡\nIn R4, the set of all vectors v = ⎢⎣\nv1\nv2\nv3\nv4\n⎥⎦for which v1 + v2 + v3 + v4 = 0 is\na subspace. It contains the zero vector and is closed under addition and scalar\nmultiplication. It is the nullspace of the matrix A =\n1 1 1 1 . Because\nA has rank 1, the dimension of this nullspace is n - r = 3. The subspace has\nthe basis of special solutions:\n⎤\n⎡\n⎤\n⎡\n⎤\n⎡\n⎢⎢⎣\n-1\n⎥⎥⎦,\n⎢⎢⎣\n-1\n⎥⎥⎦,\n⎢⎢⎣\n-1\n⎥⎥⎦.\nThe column space of A is R1. The left nullspace contains only the zero\nvector, has dimension zero, and its basis is the empty set. The row space of A\nalso has dimension 1.\nRank one matrices\nThe rank of a matrix is the dimension of its column (or row) space. The matrix\nA =\n2 8 10\n\nhas rank 1 because each of its columns is a multiple of the first column.\n\nA =\n.\nEvery rank 1 matrix A can be written A = UVT, where U and V are column\nvectors. We'll use rank 1 matrices as building blocks for more complex matri\nces.\nSmall world graphs\nIn this class, a graph G is a collection of nodes joined by edges:\nG = {nodes, edges} .\nA typical graph appears in Figure 1. Another example of a graph is one in\nFigure 1: A graph with 5 nodes and 6 edges.\nwhich each node is a person. Two nodes are connected by an edge if the people\nare friends. We can ask how close two people are to each other in the graph -\nwhat's the smallest number of friend to friend connections joining them? The\nquestion \"what's the farthest distance between two people in the graph?\" lies\nbehind phrases like \"six degrees of separation\" and \"it's a small world\".\nAnother graph is the world wide web: its nodes are web sites and its edges\nare links.\nWe'll describe graphs in terms of matrices, which will make it easy to an\nswer questions about distances between nodes.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_Ses1.12sum.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/d71b25b942dd3dec0ede49d712deb4b0_MIT18_06SCF11_Ses1.12sum.pdf",
      "content": "Graphs, networks, incidence matrices\nWhen we use linear algebra to understand physical systems, we often find\nmore structure in the matrices and vectors than appears in the examples we\nmake up in class. There are many applications of linear algebra; for example,\nchemists might use row reduction to get a clearer picture of what elements\ngo into a complicated reaction. In this lecture we explore the linear algebra\nassociated with electrical networks.\nGraphs and networks\nA graph is a collection of nodes joined by edges; Figure 1 shows one small\ngraph.\nFigure 1: A graph with n = 4 nodes and m = 5 edges.\nWe put an arrow on each edge to indicate the positive direction for currents\nrunning through the graph.\nx\n{\nz\ny\n|\nFigure 2: The graph of Figure 1 with a direction on each edge.\nIncidence matrices\nThe incidence matrix of this directed graph has one column for each node of the\ngraph and one row for each edge of the graph:\n⎤\n⎡ -1\n0 -1\n-1\n⎢⎢⎢⎢⎣\n⎥⎥⎥⎥⎦\nA =\n.\n-1\n0 -1\nIf an edge runs from node a to node b, the row corresponding to that edge has\n-1 in column a and 1 in column b; all other entries in that row are 0. If we were\n\nstudying a larger graph we would get a larger matrix but it would be sparse;\nmost of the entries in that matrix would be 0. This is one of the ways matrices\narising from applications might have extra structure.\nNote that nodes 1, 2 and 3 and edges ¬, and (r) form a loop. The matrix\ndescribing just those nodes and edges looks like:\n⎤\n⎡\n⎣\n-1\n0 -1\n0 ⎦ .\n-1\nNote that the third row is the sum of the first two rows; loops in the graph\ncorrespond to linearly dependent rows of the matrix.\nTo find the nullspace of A, we solve Ax = 0:\n⎤\n⎡\n⎤\n⎡\nAx =\n⎢⎢⎢⎢⎣\nx2 - x1\nx3 - x2\nx3 - x1\nx4 - x1\nx4 - x3\n⎥⎥⎥⎥⎦\n=\n⎢⎢⎢⎢⎣\n⎥⎥⎥⎥⎦\n.\nIf the components xi of the vector x describe the electrical potential at the nodes\ni of the graph, then Ax is a vector describing the difference in potential across\neach edge of the graph. We see Ax = 0 when x1 = x2 = x3 = x4, so the\nnullspace has dimension 1. In terms of an electrical network, the potential\ndifference is zero on each edge if each node has the same potential. We can't tell\nwhat that potential is by observing the flow of electricity through the network,\nbut if one node of the network is grounded then its potential is zero. From that\nwe can determine the potential of all other nodes of the graph.\nThe matrix has 4 columns and a 1 dimensional nullspace, so its rank is 3.\nThe first, second and fourth columns are its pivot columns; these edges connect\nall the nodes of the graph without forming a loop - a graph with no loops is\ncalled a tree.\nThe left nullspace of A consists of the solutions y to the equation: ATy = 0.\nSince AT has 5 columns and rank 3 we know that the dimension of N(AT) is\nm - r = 2. Note that 2 is the number of loops in the graph and m is the number\nof edges. The rank r is n - 1, one less than the number of nodes. This gives us\n# loops = # edges - (# nodes - 1), or:\nnumber of nodes - number of edges + number of loops = 1.\nThis is Euler's formula for connected graphs.\nKirchhoff's law\nIn our example of an electrical network, we started with the potentials xi of\nthe nodes. The matrix A then told us something about potential differences.\nAn engineer could create a matrix C using Ohm's law and information about\n\ny1\ny4\ny3\ny2\ny5\nFigure 3: The currents in our graph.\nthe conductance of the edges and use that matrix to determine the current yi\non each edge. Kirchhoff's Current Law then says that AT y = 0, where y is\nthe vector with components y1, y2, y3, y4, y5. Vectors in the nullspace of AT\ncorrespond to collections of currents that satisfy Kirchhoff's law.\nx = x1, x2, x3, x4\nATy = 0\npotentials at nodes\nKirchhoff's Current Law\ne = Ax ↓\n↑ AT y\nx2 - x1, etc.\npotential differences\ny = Ce\n-→\nOhm's Law\ny1, y2, y3, y4, y5\ncurrents on edges\nWritten out, AT y = 0 looks like:\n⎤\n⎡\n⎤\n⎡\n⎤\n⎡\ny1\n-1\n0 -1 -1\n⎢⎢⎢⎢⎣\n⎥⎥⎥⎥⎦\n=\ny2\ny3\ny4\n⎢⎢⎣\n⎥⎥⎦\n⎢⎢⎣\n⎥⎥⎦\n-1\n.\n0 -1\ny5\nMultiplying the first row by the column vector y we get -y1 - y3 - y4 = 0.\nThis tells us that the total current flowing out of node 1 is zero - it's a balance\nequation, or a conservation law. Multiplying the second row by y tells us y1 -\ny2 = 0; the current coming into node 2 is balanced with the current going out.\nMultiplying the bottom rows, we get y2 + y3 - y5 = 0 and y4 + y5 = 0.\nWe could use the method of elimination on AT to find its column space, but\nwe already know the rank. To get a basis for N(AT) we just need to find two\nindependent vectors in this space. Looking at the equations y1 - y2 = 0 we\nmight guess y1 = y2 = 1. Then we could use the conservation laws for node 3\nto guess y3 = -1 and y5\n⎤\n⎡\n= 0. We satisfy the conservation conditions on node 4\n⎢⎢⎢⎣\n⎥⎥⎥⎦\nwith y4 = 0, giving us a basis vector\n. This vector represents one unit\n-1\n\nof current flowing around the loop joining nodes 1, 2 and 3; a multiple of this\nvector represents a different amount of current around the same loop.\nWe find a second basis vector for N(AT) by looking at the loop formed by\n1 ⎤\n⎡\n⎤\n⎡\nnodes 1, 3 and 4:\n⎢⎢⎢⎣\n-1\n⎥⎥⎥⎦ . The vector\n⎢⎢⎢⎣\n-1\n⎥⎥⎥⎦that represents a current around\nthe outer loop is also in the nullspace, but it is the sum of the first two vectors\nwe found.\nWe've almost completely covered the mathematics of simple circuits. More\ncomplex circuits might have batteries in the edges, or current sources between\nnodes. Adding current sources changes the ATy = 0 in Kirchhoff's current law\nto AT y = f. Combining the equations e = Ax, y = Ce and ATy = f gives us:\nATCAx = f.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_Ses1.13sum.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/ca8b2f24428c3ab6c74757ae5ff5056b_MIT18_06SCF11_Ses1.13sum.pdf",
      "content": "An overview of key ideas\nThis is an overview of linear algebra given at the start of a course on the math\nematics of engineering.\nLinear algebra progresses from vectors to matrices to subspaces.\nVectors\nWhat do you do with vectors? Take combinations.\nWe can multiply vectors by scalars, add, and subtract. Given vectors u, v\nand w we can form the linear combination x1u + x2v + x3w = b.\nAn example in R3 would be:\n⎡\n⎤\n⎡\n⎤\n⎡\n⎤\nu = ⎣ -1 ⎦ , v = ⎣\n1 ⎦ , w = ⎣ 0 ⎦ .\n-1\nThe collection of all multiples of u forms a line through the origin. The collec\ntion of all multiples of v forms another line. The collection of all combinations\nof u and v forms a plane. Taking all combinations of some vectors creates a\nsubspace.\nWe could continue like this, or we can use a matrix to add in all multiples\nof w.\nMatrices\nCreate a matrix A with vectors u, v and w in its columns:\n⎡\n⎤\n0 0\nA = ⎣ -1\n-1\n⎦ .\nThe product:\n⎡\n⎤⎡\n⎤\n⎡\n⎤\nx1\nx1\nAx = ⎣ -1\n0 ⎦⎣ x2 ⎦ = ⎣ -x1 + x2 ⎦\n0 -1\nx3\n-x2 + x3\nequals the sum x1u + x2v + x3w = b. The product of a matrix and a vector is\na combination of the columns of the matrix. (This particular matrix A is a dif\nference matrix because the components of Ax are differences of the components\nof that vector.)\nWhen we say x1u + x2v + x3w = b we're thinking about multiplying num\nbers by vectors; when we say Ax = b we're thinking about multiplying a\nmatrix (whose columns are u, v and w) by the numbers. The calculations are\nthe same, but our perspective has changed.\n\nFor any input vector x, the output of the operation \"multiplication by A\" is\nsome vector b:\n⎡\n⎤\n⎡\n⎤\nA ⎣ 4 ⎦ = ⎣ 3 ⎦ .\nA deeper question is to start with a vector b and ask \"for what vectors x does\nAx = b?\" In our example, this means solving three equations in three un\nknowns. Solving:\n⎡\n⎤⎡\n⎤\n⎡\n⎤\n⎡\n⎤\nx1\nx1\nb1\nAx = ⎣ -1\n0 ⎦⎣ x2 ⎦ = ⎣ x2 - x1 ⎦ = ⎣ b2 ⎦\n0 -1\nx3\nx3 - x2\nb3\nis equivalent to solving:\nx1\n= b1\nx2 - x1\n= b2\n=\nx3 - x2\nb3.\nWe see that x1 = b1 and so x2 must equal b1 + b2. In vector form, the solution\nis:\n⎡\n⎤\n⎡\n⎤\nx1\nb1\n⎣ x2 ⎦ = ⎣\nb1 + b2 ⎦ .\nx3\nb1 + b2 + b3\nBut this just says:\n⎡\n⎤⎡\n⎤\nb1\nx = ⎣ 1\n0 ⎦⎣ b2 ⎦ ,\nb3\nor x = A-1b. If the matrix A is invertible, we can multiply on both sides by\nA-1 to find the unique solution x to Ax = b. We might say that A represents a\ntransform x\nb that has an inverse transform b\nx.\n→\n\" 0 #\n\" 0 #\n→\nIn particular, if b =\nthen x =\n.\nThe second example has the same columns u and v and replaces column\nvector w:\n⎡\n⎤\nC = ⎣\n-1\n-1\n-1\n⎦ .\nThen:\n⎡\n⎤ ⎡\n⎤\n⎡\n⎤\nCx = ⎣\n-1\n-1\n-1\n⎦ ⎣\nx1\nx2\nx3\n⎦ = ⎣\nx1 - x3\nx2 - x1\nx3 - x2\n⎦\nand our system of three equations in three unknowns becomes circular.\n\nWhere before Ax = 0 implied x = 0, there are non-zero vectors x for which\nCx = 0. For any vector x with x1 = x2 = x3, Cx = 0. This is a significant\ndifference; we can't multiply both sides of Cx = 0 by an inverse to find a non\nzero solution x.\nThe system of equations encoded in Cx = b is:\n=\nx2 - x1\n= b2\nx3 - x2\n= b3.\nx1 - x3\nb1\nIf we add these three equations together, we get:\n0 = b1 +2 +b3.\nThis tells us that Cx = b has a solution x only when the components of b sum\nto 0. In a physical system, this might tell us that the system is stable as long as\nthe forces on it are balanced.\nSubspaces\nGeometrically, the columns of C lie in the same plane (they are dependent; the\ncolumns of A are independent). There are many vectors in R3 which do not lie\nin that plane. Those vectors cannot be written as a linear combination of the\ncolumns of C and so correspond to values of b for which Cx = b has no solu\ntion x. The linear combinations of the columns of C form a two dimensional\nsubspace of R3.\nThis plane of combinations of u, v and w can be described as \"all vectors\nCx\". But we know that the vectors b for which Cx = b satisfy the condition\nb1 + b2 + b3 = 0. So the plane of all combinations of u and v consists of all\nvectors whose components sum to 0.\nIf we take all combinations of:\n⎡\n⎤\n⎡\n⎤\n⎡\n⎤\nu = ⎣ -1 ⎦ , v = ⎣\n1 ⎦ , and w = ⎣ 0 ⎦\n-1\nwe get the entire space R3; the equation Ax = b has a solution for every b in\nR3. We say that u, v and w form a basis for R3.\nA basis for Rn is a collection of n independent vectors in Rn. Equivalently,\na basis is a collection of n vectors whose combinations cover the whole space.\nOr, a collection of vectors forms a basis whenever a matrix which has those\nvectors as its columns is invertible.\nA vector space is a collection of vectors that is closed under linear combina\ntions. A subspace is a vector space inside another vector space; a plane through\nthe origin in R3 is an example of a subspace. A subspace could be equal to the\nspace it's contained in; the smallest subspace contains only the zero vector.\nThe subspaces of R3 are:\n\n- the origin,\n- a line through the origin,\n- a plane through the origin,\nall of R3.\n-\nConclusion\nWhen you look at a matrix, try to see \"what is it doing?\"\nMatrices can be rectangular; we can have seven equations in three un\nknowns. Rectangular matrices are not invertible, but the symmetric, square\nmatrix AT A that often appears when studying rectangular matrices may be\ninvertible.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_Ses1.14sum.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/b547dd4153ccc350860df8f43085c2d6_MIT18_06SCF11_Ses1.14sum.pdf",
      "content": "Exam 1 review\nThis lecture is a review for the exam. The majority of the exam is on what we've\nlearned about rectangular matrices.\nSample question 1\nSuppose u, v and w are non-zero vectors in R7. They span a subspace of R7.\nWhat are the possible dimensions of that vector space?\nThe answer is 1, 2 or 3. The dimension can't be higher because a basis for\nthis subspace has at most three vectors. It can't be 0 because the vectors are\nnon-zero.\nSample question 2\nSuppose a 5 by 3 matrix R in reduced row echelon form has r = 3 pivots.\n1. What's the nullspace of R?\nSince the rank is 3 and there are 3 columns, there is no combination of the\ncolumns that equals 0 except the trivial one. N(R) = {0}.\nR\n2. Let B be the 10 by 3 matrix\n. What's the reduced row echelon form\n2R\nof B?\nR\nAnswer:\n.\n3. What is the rank of B?\nAnswer: 3.\nR\nR\n4. What is the reduced row echelon form of C =\n?\nR\nWhen we perform row reduction we get:\n\nR R\nR\nR\nR\nR\nR\n-→\n0 -R\n-→\n0 -R\n-→\n0 R\n.\nThen we might need to move some zero rows to the bottom of the matrix.\n5. What is the rank of C?\nAnswer: 6.\n6. What is the dimension of the nullspace of CT?\nm = 10 and r = 6 so dim N(CT) = 10 - 6 = 4.\n\n\"\n#\n\"\n#\n\"\n#\n\"\n#\n\"\n#\n\"\n#\nSample question 3\nSuppose we know that Ax =\nand that:\n⎡\n⎤\n⎡\n⎤\n⎡\n⎤\nx = ⎣ 0 ⎦ + c ⎣ 1 ⎦ + d ⎣ 0 ⎦\nis a complete solution.\nNote that in this problem we don't know what A is.\n1. What is the shape of the matrix A?\nAnswer: 3 by 3, because x and b both have three components.\n2. What's the dimension of the row space of A?\nFrom the complete solution we can see that the dimension of the nullspace\nof A is 2, so the rank of A must be 3 - 2 = 1.\n3. What is A?\nBecause the second and third components of the particular solution\nare zero, we see that the first column vector of A must be\n.\nKnowing that\nis in the nullspace tells us that the third column of A\nmust be 0. The fact that\nis in the nullspace tells us that the second\ncolumn must be the negative of the first. So,\n⎡\n⎤\n1 -1\nA = ⎣ 2 -2\n0 ⎦ .\n1 -1\nIf we had time, we could check that this A times x equals b.\n4. For what vectors b does Ax = b have a solution x?\nThis equation has a solution exactly when b is in the column space of A,\nso when b is a multiple of\n. This makes sense; we know that the\nrank of A is 1 and the nullspace is large.\nIn contrast, we might have had r = m or r = n.\n\n\"\n#\n\"\n#\n\"\n#\n\"\n#\n\"\n#\n\"\n#\n\"\n#\nSample question 4\nSuppose:\n⎤\n⎡\n⎤\n⎡ 1\n0 -1\n1 -1 ⎦ .\n⎣0\n⎣\n⎦\nB = CD =\nTry to answer the questions below without performing this matrix multi\nplication CD.\n1. Give a basis for the nullspace of B.\nThe matrix B is 3 by 4, so N(B) ⊆ R4. Because C =\nis invertible, the nullspace of B is the same as the nullspace of D\n-1\n=\n-1\n. Matrix D is in reduced form, so its special solutions\nform a basis for N(D) = N(B):\n⎤\n⎡\n⎤\n⎡\n⎥⎥⎦,\n⎢⎢⎣\n-2\n⎥⎥⎦\n⎢⎢⎣\n-1\n.\nThese vectors are independent, and if time permits we can multiply to\ncheck that they are in N(B).\n2. Find the complete solution to Bx =\n.\nWe can now describe any vector in the nullspace, so all we need to do is\nfind a particular solution. There are many possible particular solutions;\nthe simplest one is given below.\nOne way to solve this is to notice that C\n=\nand then find a\nvector x for which Dx =\n. Another approach is to notice that the\nfirst column of B = CD is\n. In either case, we get the complete\nsolution:\n⎤\n⎡\n⎤\n⎡\n⎤\n⎡ 1\n⎥⎥⎦+ d\n⎢⎢⎣\n-2\n⎢⎢⎣\n⎥⎥⎦+ c\n⎢⎢⎣\n-1\n⎥⎥⎦\nx =\n.\nAgain, we can check our work by multiplying.\n\nShort questions\nThere may not be true/false questions on the exam, but it's a good idea to\nreview these:\n1. Given a square matrix A whose nullspace is just {0}, what is the nullspace\nof AT?\nN(AT ) is also {0} because A is square.\n2. Do the invertible matrices form a subspace of the vector space of 5 by 5\nmatrices?\nNo. The sum of two invertible matrices may not be invertible. Also, 0 is\nnot invertible, so is not in the collection of invertible matrices.\n3. True or false: If B2 = 0, then it must be true that B = 0.\nFalse. We could have B =\n.\n4. True or false: A system Ax = b of n equations with n unknowns is solv\nable for every right hand side b if the columns of A are independent.\nTrue. A is invertible, and x = A-1b is a (unique) solution.\n5. True or false: If m = n then the row space equals the column space.\nFalse. The dimensions are equal, but the spaces are not. A good example\nto look at is B =\n.\n6. True or false: The matrices A and -A share the same four spaces.\nTrue, because whenever a vector v is in a space, so is -v.\n7. True or false: If A and B have the same four subspaces, then A is a multi\nple of B.\nA good way to approach this question is to first try to convince yourself\nthat it isn't true - look for a counterexample. If A is 3 by 3 and invertible,\nthen its row and column space are both R3 and its nullspaces are {0}. If B\nis any other invertible 3 by 3 matrix it will have the same four subspaces,\nand it may not be a multiple of A. So we answer \"false\".\nIt's good to ask how we could truthfully complete the statement \"If A\nand B have the same four subspaces, then ...\"\n8. If we exchange two rows of A, which subspaces stay the same?\nThe row space and the nullspace stay the same.\n\n\"\n#\n9. Why can't a vector v =\nbe in the nullspace of A and also be a row\nof A?\nBecause if v is the nth row of A, the nth component of the vector Av\nwould be 14, not 0. The vector v could not be a solution to Av = 0.\nIn fact, we will learn that the row space is perpendicular to the nullspace.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_Ses1.1sum.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/6913aa2f37dada8fdda334168b30339b_MIT18_06SCF11_Ses1.1sum.pdf",
      "content": "The geometry of linear equations\nThe fundamental problem of linear algebra is to solve n linear equations in n\nunknowns; for example:\n2x - y = 0\n-x + 2y = 3.\nIn this first lecture on linear algebra we view this problem in three ways.\nThe system above is two dimensional (n = 2). By adding a third variable z\nwe could expand it to three dimensions.\nRow Picture\nPlot the points that satisfy each equation. The intersection of the plots (if they\ndo intersect) represents the solution to the system of equations. Looking at\nFigure 1 we see that the solution to this system of equations is x = 1, y = 2.\n-1\nx\n-2\n-1\ny\n2x -y = 0\n-x + 2y = 3\n(1, 2)\nFigure 1: The lines 2x - y = 0 and -x + 2y = 3 intersect at the point (1, 2).\nWe plug this solution in to the original system of equations to check our\nwork:\n2 1 - 2 = 0\n·\n-1 + 2 2 = 3.\n·\nThe solution to a three dimensional system of equations is the common\npoint of intersection of three planes (if there is one).\n\nColumn Picture\nIn the column picture we rewrite the system of linear equations as a single\nequation by turning the coefficients in the columns of the system into vectors:\n\n-1\n\nx\n+ y\n=\n.\n-1\nGiven two vectors c and d and scalars x and y, the sum xc + yd is called\na linear combination of c and d. Linear combinations are important throughout\nthis course.\nGeometrically, we want to find numbers x and y so that x copies of vector\n\n-\n\nadded to y copies of vector\n-1\n\nequals the vector\n\n. As we see\nfrom Figure 2, x = 1 and y = 2, agreeing with the row picture in Figure 2.\n-1\n-1\n\n-1\n\n-1\n\n-1\n\nFigure 2: A linear combination of the column vectors equals the vector b.\nIn three dimensions, the column picture requires us to find a linear combi\nnation of three 3-dimensional vectors that equals the vector b.\nMatrix Picture\nWe write the system of equations\n2x - y = 0\n-x + 2y = 3\n\nas a single equation by using matrices and vectors:\n\n2 -1\n\nx\n\n-1\ny\n=\n.\n\n2 -1\n\nThe matrix A =\nis called the coefficient matrix. The vector\n\nx\n\n-1\nx =\nis the vector of unknowns. The values on the right hand side of the\ny\nequations form the vector b:\nAx = b.\nThe three dimensional matrix picture is very like the two dimensional one,\nexcept that the vectors and matrices increase in size.\nMatrix Multiplication\nHow do we multiply a matrix A by a vector x?\n\n= ?\nOne method is to think of the entries of x as the coefficients of a linear\ncombination of the column vectors of the matrix:\n\n= 1\n+ 2\n=\n.\nThis technique shows that Ax is a linear combination of the columns of A.\nYou may also calculate the product Ax by taking the dot product of each\nrow of A with the vector x:\n\n2 1 + 5 2\n\n·\n·\n=\n1 1 + 3 2\n=\n.\n·\n·\nLinear Independence\nIn the column and matrix pictures, the right hand side of the equation is a\nvector b. Given a matrix A, can we solve:\nAx = b\nfor every possible vector b? In other words, do the linear combinations of the\ncolumn vectors fill the xy-plane (or space, in the three dimensional case)?\nIf the answer is \"no\", we say that A is a singular matrix. In this singular\ncase its column vectors are linearly dependent; all linear combinations of those\nvectors lie on a point or line (in two dimensions) or on a point, line or plane (in\nthree dimensions). The combinations don't fill the whole space.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_Ses1.10prob.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/1036f9ef2f53425811d3ab8176b4b4d7_MIT18_06SCF11_Ses1.10prob.pdf",
      "content": "Exercises on the four fundamental subspaces\nProblem 10.1: (3.6 #11. Introduction to Linear Algebra: Strang) A is an m by\nn matrix of rank r. Suppose there are right sides b for which Ax = b has\nno solution.\na) What are all the inequalities (< or ≤) that must be true between m, n,\nand r?\nb) How do you know that ATy = 0 has solutions other than y = 0?\nProblem 10.2: (3.6 #24.) ATy = d is solvable when d is in which of the\nfour subspaces? The solution y is unique when the\ncontains\nonly the zero vector.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_Ses1.11prob.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/8bcdd1d9795d55865119bb010765a021_MIT18_06SCF11_Ses1.11prob.pdf",
      "content": "Exercises on matrix spaces; rank 1; small world graphs\nProblem 11.1: [Optional] (3.5 #41. Introduction to Linear Algebra: Strang)\nWrite the 3 by 3 identity matrix as a combination of the other five per\nmutation matrices. Then show that those five matrices are linearly inde\npendent. (Assume a combination gives c1P1 +\n+ c5P5 = 0 and check\n· · ·\nentries to prove ci is zero.) The five permutation matrices are a basis for the\nsubspace of three by three matrices with row and column sums all equal.\nProblem 11.2: (3.6 #31.) M is the space of three by three matrices. Multiply\neach matrix X in M by:\n⎡\n⎤\nA = ⎣\n-1\n-1\n-1\n⎦\n⎡\n⎤\n⎡\n⎤\nNotice that A ⎣ 1 ⎦ = ⎣ 0 ⎦.\na) Which matrices X lead to AX = 0?\nb) Which matrices have the form AX for some matrix X?\nc) Part (a) finds the \"nullspace\" of the operation AX and part (b) finds\nthe \"column space.\" What are the dimensions of those two subspaces\nof M? Why do the dimensions add to (n - r) + r = 9?\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_Ses1.12prob.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/13115844908c991f11d52f44f29ecc11_MIT18_06SCF11_Ses1.12prob.pdf",
      "content": "Exercises on graphs, networks, and incidence matrices\nProblem 12.1: (8.2 #1. Introduction to Linear Algebra: Strang) Write down\nthe four by four incidence matrix A for the square graph, shown below.\n(Hint: the first row has -1 in column 1 and +1 in column 2.) What vectors\n(x1, x2, x3, x4) are in the nullspace of A? How do you know that (1,0,0,0) is\nnot in the row space of A?\nProblem 12.2: (8.2 #7.) Continuing with the network from problem one,\nsuppose the conductance matrix is\n⎤\n⎡\nC =\n⎢⎢⎣\n⎥⎥⎦ .\nMultiply matrices to find ATCA. For f = (1, 0, -1, 0), find a solution to\nATCAx = f. Write the potentials x and currents y = -CAx on the square\ngraph (see above) for this current source f going into node 1 and out from\nnode 3.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_Ses1.1prob.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/f511d20996159f321b6704f5b2070e04_MIT18_06SCF11_Ses1.1prob.pdf",
      "content": "Exercises on the geometry of linear equations\nProblem 1.1: (1.3 #4. Introduction to Linear Algebra: Strang) Find a combi\nnation x1w1 + x2w2 + x3w3 that gives the zero vector:\n⎡\n⎤\n⎡\n⎤\n⎡\n⎤\n= ⎣ 2\n= ⎣ 5\n= ⎣ 8 ⎦ .\nw1\n⎦ w2\n⎦ w3\nThose vectors are (independent)(dependent).\nThe three vectors lie in a\n. The matrix W with those columns\nis not invertible.\n⎡\n⎤⎡\n⎤\nProblem 1.2: Multiply: ⎣ 2\n3 ⎦⎣ -2 ⎦.\nProblem 1.3: True or false: A 3 by 2 matrix A times a 2 by 3 matrix B\nequals a 3 by 3 matrix AB. If this is false, write a similar sentence which is\ncorrect.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_Ses1.2prob.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/2a96d025346d8845829251cb918caddd_MIT18_06SCF11_Ses1.2prob.pdf",
      "content": "Exercises on elimination with matrices\nProblem 2.1: In the two-by-two system of linear equations below, what\nmultiple of the first equation should be subtracted from the second equa\ntion when using the method of elimination? Convert this system of equa\ntions to matrix form, apply elimination (what are the pivots?), and use\nback substitution to find a solution. Try to check your work before look\ning up the answer.\n2x + 3y = 5\n6x + 15y = 12\nProblem 2.2: (2.3 #29. Introduction to Linear Algebra: Strang) Find the trian\ngular matrix E that reduces \"Pascal's matrix\" to a smaller Pascal:\n⎤\n⎡\n⎤\n⎡ 1\nE\n⎢⎢⎣\n⎥⎥⎦ =\n⎢⎢⎣\n⎥⎥⎦ .\nWhich matrix M (multiplying several E's) reduces Pascal all the way\nto I?\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_Ses1.3prob.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/8e9ccbcc13300a9bda7f24a684bd16b6_MIT18_06SCF11_Ses1.3prob.pdf",
      "content": "Exercises on multiplication and inverse matrices\nProblem 3.1: Add AB to AC and compare with A(B + C) :\nA =\nB =\nC =\nProblem 3.2: (2.5 #24. Introduction to Linear Algebra: Strang) Use Gauss-\nJordan elimination on [U I] to find the upper triangular U-1 :\n⎡\n⎤⎡\n⎤\n⎡\n⎤\n1 a\nb\nUU-1 = I ⎣ 0\n1 c ⎦⎣ x1 x2 x3 ⎦ = ⎣ 0\n0 ⎦ .\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_Ses1.4prob.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/0ea3c333ad8f83379b45efd8ffe88729_MIT18_06SCF11_Ses1.4prob.pdf",
      "content": "Exercises on factorization into A = LU\nProblem 4.1: What matrix E puts A into triangular form EA = U? Multi\nply by E-1 = L to factor A into LU.\n⎡\n⎤\n1 3 0\nA = ⎣2 4 0 ⎦\n2 0 1\nProblem 4.2: (2.6 #13. Introduction to Linear Algebra: Strang) Compute L\nand U for the symmetric matrix\n⎤\n⎡\nA =\n⎢⎢⎣\na\na\na\na\na\nb\nb\nb\na\nb\nc\nc\na\nb\nc\nd\n⎥⎥⎦ .\nFind four conditions on a, b, c, d to get A = LU with four pivots.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_Ses1.5prob.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/32b9793a7998e09296cd6234fe7e9017_MIT18_06SCF11_Ses1.5prob.pdf",
      "content": "Exercises on transposes, permutations, spaces\nProblem 5.1: (2.7 #13. Introduction to Linear Algebra: Strang)\na) Find a 3 by 3 permutation matrix with P3 = I (but not P = I).\nb) Find a 4 by 4 permutation Pb with Pb4 6= I.\nProblem 5.2: Suppose A is a four by four matrix. How many entries of A\ncan be chosen independently if:\na) A is symmetric?\nb) A is skew-symmetric? (AT = -A)\nProblem 5.3: (3.1 #18.) True or false (check addition or give a counterex\nample):\na) The symmetric matrices in M (with AT = A) form a subspace.\nb) The skew-symmetric matrices in M (with AT = -A) form a subspace.\nc) The unsymmetric matrices in M (with AT 6= A) form a subspace.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_Ses1.6prob.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/255ec48e518355a8ef94b086e3a5a247_MIT18_06SCF11_Ses1.6prob.pdf",
      "content": "Exercises on column space and nullspace\nProblem 6.1: (3.1 #30. Introduction to Linear Algebra: Strang) Suppose S\nand T are two subspaces of a vector space V.\na) Definition: The sum S + T contains all sums s + t of a vector s in S and\na vector t in T. Show that S + T satisfies the requirements (addition and\nscalar multiplication) for a vector space.\nb) If S and T are lines in Rm, what is the difference between S + T and\nS ∪ T? That union contains all vectors from S and T or both. Explain\nthis statement: The span of S ∪ T is S + T.\nProblem 6.2: (3.2 #18.) The plane x - 3y - z = 12 is parallel to the plane\nx - 3y - x = 0. One particular point on this plane is (12, 0, 0). All points\non the plane have the form (fill in the first components)\n⎡\n⎤\n⎡\n⎤\n⎡\n⎤\n⎡\n⎤\nx\n⎣ y ⎦ = ⎣ 0 ⎦ + y ⎣ 1 ⎦ + z ⎣ 0 ⎦ .\nz\nProblem 6.3: (3.2 #36.) How is the nullspace N(C) related to the spaces\nA\nN(A) and N(B), if C =\n?\nB\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_Ses1.7prob.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/67a97b58e4ee19bfe2ac57ab94cddff8_MIT18_06SCF11_Ses1.7prob.pdf",
      "content": "Exercises on solving Ax = 0: pivot variables, special solutions\nProblem 7.1:\na) Find the row reduced form of:\n⎡\n⎤\nA = ⎣ 0\n7 ⎦\n2 -2 11 -3\nb) What is the rank of this matrix?\nc) Find any special solutions to the equation Ax = 0.\nProblem 7.2: (3.3 #17.b Introduction to Linear Algebra: Strang) Find A1 and\nA2 so that rank(A1B) = 1 and rank(A2B) = 0 for B =\n.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_Ses1.10sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/68f4777bbcac272c9184347a9849333a_MIT18_06SCF11_Ses1.10sol.pdf",
      "content": "Exercises on the four fundamental subspaces\nProblem 10.1:\n(3.6 #11. Introduction to Linear Algebra: Strang) A is an m\nby n matrix of rank r. Suppose there are right sides b for which Ax = b\nhas no solution.\na) What are all the inequalities (< or ≤) that must be true between m, n,\nand r?\nb) How do you know that ATy = 0 has solutions other than y = 0?\nSolution:\na) The rank of a matrix is always less than or equal to the number of rows\nand columns, so r ≤ m and r ≤ n. The second statement tells us that\nthe column space is not all of Rn, so r < m.\nb) These solutions make up the left nullspace, which has dimension m -\nr > 0 (that is, there are nonzero vectors in it).\nProblem 10.2:\n(3.6 #24.) ATy = d is solvable when d is in which of the\nfour subspaces? The solution y is unique when the\ncontains\nonly the zero vector.\nSolution:\nIt is solvable when d is in the row space, which consists of\nall vectors ATy. The solution y is unique when the left nullspace contains\nonly the zero vector.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_Ses1.11sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/f203b1ebf66c210c62e5730a4b9f9f6d_MIT18_06SCF11_Ses1.11sol.pdf",
      "content": "Exercises on matrix spaces; rank 1; small world graphs\nProblem 11.1:\n[Optional] (3.5 #41. Introduction to Linear Algebra: Strang)\nWrite the 3 by 3 identity matrix as a combination of the other five permuta\ntion matrices. Then show that those five matrices are linearly independent.\n(Assume a combination gives c1P1 +\n+ c5P5 = 0 and check entries to\n· · ·\nprove ci is zero.) The five permutation matrices are a basis for the subspace\nof three by three matrices with row and column sums all equal.\nSolution:\nThe other five permutation matrices are:\n⎡\n⎤\n⎡\n⎤\n⎡\n⎤\nP21 = ⎣ 1\n⎦ , P31 = ⎣\n⎦ , P32 = ⎣\n1 ⎦ ,\n⎡\n⎤\n⎡\n⎤\nP32P21 = ⎣\n1 ⎦ and P21P32 = ⎣ 1\n⎦ .\nSince P21 + P31 + P32 is the all ones matrix and P32P21 + P21P32 is the\nmatrix with zeros on the diagonal and ones elsewhere,\nI = P21 + P31 + P32 - P32P21 - P21P32.\nFor the second part, setting c1P1 +\n+ c5P5 equal to zero gives:\n· · ·\n⎡\n⎤\nc3\nc1 + c4 c2 + c5\n⎣ c1 + c5\nc2\nc3 + c4 ⎦ = 0.\nc2 + c4 c3 + c5\nc1\nSo c1 = c2 = c3 = 0 along the diagonal, and c4 = c5 = 0 from the off-\ndiagonal entries.\nProblem 11.2:\n(3.6 #31.) M is the space of three by three matrices. Mul\ntiply each matrix X in M by:\n⎡\n⎤\n0 -1\nA = ⎣ -1\n0 ⎦\n0 -1\n⎡\n⎤\n⎡\n⎤\nNotice that A ⎣ 1 ⎦ = ⎣ 0 ⎦.\n\na) Which matrices X lead to AX = 0?\nb) Which matrices have the form AX for some matrix X?\nc) Part (a) finds the \"nullspace\" of the operation AX and part (b) finds\nthe \"column space.\" What are the dimensions of those two subspaces\nof M? Why do the dimensions add to (n - r) + r = 9?\nSolution:\na) We can use row reduction or some other method to see that the rows of\nA are dependent and that A has rank 2. Its nullspace has the basis:\n⎡\n⎤\n⎣ 1 ⎦ .\nAX = 0 precisely when the columns of X are in the nullspace of A, i.e.\nwhen they are multiples of the basis of N(A). Therefore, if AX = 0\nthen X must have the form:\n⎡\n⎤\na\nb\nc\nX = ⎣ a\nb\nc ⎦ .\na\nb\nc\nb) On the other hand, the columns of any matrix of the form AX are lin\near combinations of the columns of A. That is, they are vectors whose\ncomponents all sum to 0, so a matrix has the form AX if and only if all\nof its columns individually sum to 0:\n⎡\na\nb\nc\n⎤\nAX = B if and only if B = ⎣\nd\n-a - d\ne\n-b - e\nf\n-c - f\n⎦ .\nc) The dimension of the \"nullspace\" is 3, while the dimension of the \"col\numn space\" is 6. These add up to 9, which is the dimension of the space\nof \"inputs\" M.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_Ses1.12sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/1bbfcb718126a0a8ef8755b84922c536_MIT18_06SCF11_Ses1.12sol.pdf",
      "content": "Exercises on graphs, networks, and incidence matrices\nProblem 12.1:\n(8.2 #1. Introduction to Linear Algebra: Strang) Write down\nthe four by four incidence matrix A for the square graph, shown below.\n(Hint: the first row has -1 in column 1 and +1 in column 2.) What vectors\n(x1, x2, x3, x4) are in the nullspace of A? How do you know that (1,0,0,0) is\nnot in the row space of A?\nSolution:\nThe incidence matrix A is written as:\n⎤\n⎡ -1\n0 -1\nA =\n⎢⎢⎣\n⎥⎥⎦ .\n1 -1\n-1\nTo find the vectors in the nullspace, we solve Ax = 0 :\n⎤\n⎡\n⎤\n⎡\n⎤\n⎡\n⎤\n⎡ -1\nx1\nx2 - x1\n⎢⎢⎣\n⎢⎢⎣\n⎥⎥⎦\n⎥⎥⎦\n⎢⎢⎣\n⎥⎥⎦\n⎢⎢⎣\n⎥⎥⎦\n-1\nx2\nx3\nx3 - x2\nx3 - x4\n=\n=\n,\n1 -1\n-1\nx4\nx4 - x1\nso x1 = x2 = x3 = x4. Therefore, the nullspace consists of vectors of the\nform (a, a, a, a).\nFinally, (1,0,0,0) is not in the row space of A because it is not orthogonal\nto the nullspace.\nProblem 12.2:\n(8.2 #7.) Continuing with the network from problem one,\nsuppose the conductance matrix is\n⎤\n⎡\nC =\n⎢⎢⎣\n0 0 2 0\n0 0 0 1\n⎥⎥⎦ .\n\nMultiply matrices to find ATCA. For f = (1, 0, -1, 0), find a solution to\nATCAx = f. Write the potentials x and currents y = -CAx on the square\ngraph (see above) for this current source f going into node 1 and out from\nnode 3.\nSolution:\nFrom the previous question, we know that the incidence ma\ntrix is:\n⎤\n⎡ -1\n0 -1\n⎢⎢⎣\n⎥⎥⎦\nA =\n.\n1 -1\n-1\nMultiply to obtain ATCA:\n⎤\n⎡\n⎤\n⎡\n⎤\n⎡\n⎤\n⎡ -1\n0 -1\n-1\n2 -1\n0 -1\n⎢⎢⎣\n⎢⎢⎣\n⎥⎥⎦\n⎢⎢⎣\n⎥⎥⎦\n⎥⎥⎦ =\n⎢⎢⎣\n⎥⎥⎦\n-1\n-1\n-1\n-2\n.\n1 -1\n-2\n-2\n0 -1\n-1\n-1\n-2\nSolving the equation ATCAx = f by performing row reduction on the\naugmented matrix\n⎤\n⎡\n⎢⎢⎣\n-1\n0 -1\n1 -1\n0 -1\n-1\n⎥⎥⎦\nand choosing x3 = 0 to represent a grounded node gives:\n⎤\n⎡\n⎤\n⎡ x1\n3/4\nx =\n⎢⎢⎣\nx2\nx3\n⎥⎥⎦ =\n⎢⎢⎣\n1/4\n⎥⎥⎦ .\nx4\n1/4\n⎡\nWe know y = -CAx, so\n1 -1\n⎤\n⎡\n⎤\n⎡\n⎤\n3/4\n1/2\n⎢⎢⎣\n⎢⎢⎣\n⎥⎥⎦\n⎥⎥⎦\n⎢⎢⎣\n⎥⎥⎦\n-2\n1/4\n1/2\ny =\n=\n.\n-2\n0 -1\n1/4\n1/2\nWe draw these values on the square graph:\n1/2\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_Ses1.1sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/290a5d857d2a51bf21d4eebf05905620_MIT18_06SCF11_Ses1.1sol.pdf",
      "content": "Exercises on the geometry of linear equations\nProblem 1.1:\n(1.3 #4. Introduction to Linear Algebra: Strang) Find a com\nbination x1w1 + x2w2 + x3w3 that gives the zero vector:\n⎡\n⎤\n⎡\n⎤\n⎡\n⎤\n= ⎣ 2\n= ⎣ 5\n= ⎣ 8 ⎦ .\nw1\n⎦ w2\n⎦ w3\nThose vectors are (independent)(dependent).\nThe three vectors lie in a\n. The matrix W with those columns\nis not invertible.\nSolution:\nWe might observe that w1 + w3 - 2w2 = 0, or we might si\nmultaneously solve the system of equations:\n1x1 + 4x2 + 7x3 = 0\n2x1 + 5x2 + 8x3 = 0\n3x1 + 6x2 + 9x3 = 0\nSubtracting twice equation 1 from equation 2 gives us -3x2 - 6x3 = 0.\nSubtracting thrice equation 1 from equation 3 gives us -6x2 - 12x3 = 0,\nwhich is equivalent to the previous equation and so leads us to suspect\nthat the vectors are dependent. At this point we might guess x2 = -2 and\nx3 = 1 which would lead us to the answer we observed above:\nx1 = 1, x2 = -2, x3 = 1 and w1 - 2w2 + w3 = 0.\nThose vectors are dependent because there is a combination of the vectors\nthat gives the zero vector.\nThe three vectors lie in a plane.\n⎡\n⎤ ⎡\n⎤\n1 2 0\nProblem 1.2:\nMultiply: ⎣ 2\n⎦ ⎣ -2\n⎦.\n⎡\n⎤\n⎡\n⎤\n1 · 3 + 2 · (-2) + 0 · 1\n⎣\n-1\nSolution:\n⎣\n6 + 0 + 3\n⎦ =\n9 ⎦.\n12 - 2 + 1\n\nProblem 1.3:\nTrue or false: A 3 by 2 matrix A times a 2 by 3 matrix B\nequals a 3 by 3 matrix AB. If this is false, write a similar sentence which is\ncorrect.\nSolution:\nThe statement is true. In order to multiply two matrices, the\nnumber of columns of A must equal the number of rows of B. The product\nAB will have the same number of rows as the first matrix and the same\nnumber of columns as the second:\nA(m by n) times B(n by p) equals AB(m by p).\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_Ses1.2sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/43e6b5f1175983c7220ce471d4a93317_MIT18_06SCF11_Ses1.2sol.pdf",
      "content": "Exercises on elimination with matrices\nProblem 2.1:\nIn the two-by-two system of linear equations below, what\nmultiple of the first equation should be subtracted from the second equa\ntion when using the method of elimination? Convert this system of equa\ntions to matrix form, apply elimination (what are the pivots?), and use\nback substitution to find a solution. Try to check your work before look\ning up the answer.\n2x + 3y = 5\n6x + 15y = 12\nSolution:\nOne subtracts 3 times the first equation from the second equa\ntion in order to eliminate the 6x.\nTo convert to matrix form, use the general format Ax = b:\n\n2x + 3y = 5\nx\n6x + 15y = 12 -→\n6 15\ny\n=\n.\nWe then apply elimination on matrix A. Using the first pivot (the num\nber 2 in the upper left corner of A), we subtract three times the first row\nfrom the second row to get:\n2 3\nA =\n6 15\n-→ U =\n0 6\nwhere U is an upper triangular matrix with pivots 2 and 6. Doing the same\nto the right side b = (5, 12) gives a new equation of the form Ux = c :\n\n2 3\nx\n0 6\ny\n=\n-3\n.\nTo solve our new equation, we use back substitution:\ny = -2\n6y = -3 -→\nand\n\n=\nx = 4\n2x + 3y = 5 -→ 2x + 3 -2\n= 5 -→ 2x = 5 + 2\n2 -→\nWe know that our solution fulfills the first equation; let's make sure\nthat our values fulfill the second equation as a check on our work:\n6x + 15y = 6 13 + 15 -1\n= 78 - 30 = 12 X\nProblem 2.2:\n(2.3 #29. Introduction to Linear Algebra: Strang) Find the\ntriangular matrix E that reduces \"Pascal's matrix\" to a smaller Pascal:\n⎤\n⎡\n⎤\n⎡ 1\nE\n⎢⎢⎣\n⎥⎥⎦ =\n⎢⎢⎣\n⎥⎥⎦ .\nWhich matrix M (multiplying several E's) reduces Pascal all the way\nto I?\nSolution:\n⎤\n⎡\n⎢⎢⎣\n-1\n0 ⎥⎥⎦\nThe matrix is E =\n-1\nOne can eliminate the second column with the matrix\n-1\n⎤\n⎡\n⎢⎢⎣\n0 -1\n0 -1\n⎥⎥⎦\nand the third column with the matrix\n⎤\n⎡\n⎢⎢⎣\n0 -1\n⎥⎥⎦\n\nMultiplying these together, we get\n⎤\n⎡\n⎤\n⎡\n⎤\n⎡\n⎤\n⎡ 1\nM =\n⎢⎢⎣\n⎢⎢⎣\n⎥⎥⎦\n⎢⎢⎣\n⎥⎥⎦\n-1\n⎥⎥⎦ =\n⎢⎢⎣\n-1\n0 -1\n-1\n-2\n.\n0 -1\n-1\n-1\n-1\n-3\nSince M reduces the Pascal matrix to I, M must be the inverse matrix!\n⎥⎥⎦\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_Ses1.3sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/d2dc903ca48f3d1ffa1528abde719c94_MIT18_06SCF11_Ses1.3sol.pdf",
      "content": "Exercises on multiplication and inverse matrices\nProblem 3.1: Add AB to AC and compare with A(B + C) :\nA =\nB =\nC =\nSolution: We first add AB to AC :\n\nAB =\n=\n,\nAC =\n=\n+\n=\n.\n-→ AB + AC =\nWe then compute A(B + C) :\nB + C =\n+\n=\n\n-→ A(B + C) =\n=\n23 24\n= AB + AC.\nTherefore, AB + AC = A(B + C).\nProblem 3.2: (2.5 #24. Introduction to Linear Algebra: Strang) Use Gauss-\nJordan elimination on [U I] to find the upper triangular U-1 :\n⎡\n⎤⎡\n⎤\n⎡\n⎤\n1 a\nb\nUU-1 = I ⎣ 0\n1 c ⎦⎣ x1 x2 x3 ⎦ = ⎣ 0\n0 ⎦ .\nSolution: Row reduce [U I] to get [I U-1] as follows (here, Ri = row i)\n\n⎡\n⎤\n⎡\n⎤\n1 a\nb\n(R1 = R1 - aR2)\n0 b - ac\n1 -a\n⎣ 0\n1 c\n0 ⎦ -→ (R2 = R2 - cR2) ⎣ 0\n1 -c ⎦\n⎡\n⎤\n-→\n(R1 = R1 - (b - ac)R3)\n⎣\nac -\n-\nb\nc ⎦ = [I L-1]\n-a\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_Ses1.4sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/c0a61f86b1cb7c8bcb9d140b4e94bdae_MIT18_06SCF11_Ses1.4sol.pdf",
      "content": "Exercises on factorization into A = LU\nProblem 4.1:\nWhat matrix E puts A into triangular form EA = U? Mul\ntiply by E-1 = L to factor A into LU.\n⎡\n⎤\nA = ⎣ 2\n0 ⎦\nSolution:\nWe will perform a series of row operations to transform the\nmatrix A into an upper triangular matrix. First, we multiply the first row\nby 2 and then subtract it from the second row in order to make the first\nelement of the second row 0:\n⎡\n⎤⎡\n⎤\n⎡\n⎤\n⎣ -2\n0 ⎦⎣ 2\n0 ⎦ = ⎣ 0 -2\n0 ⎦\nNext, we multiply the first row by 2 (again) and subtract it from the third\nrow in order to make the first element of the third row 0:\n⎡\n⎤⎡\n⎤\n⎡\n⎤\n⎣\n0 ⎦⎣ 0 -2\n0 ⎦ = ⎣ 0 -2\n0 ⎦\n-2\n0 -6\nNow, we multiply the second row by 3 and subtract it from the third\nrow in order to make the second element of the third row 0:\n⎡\n⎤⎡\n⎤\n⎡\n⎤\n⎣ 0\n0 ⎦⎣ 0 -2\n0 ⎦ = ⎣ 0 -2\n0 ⎦ = U.\n0 -3\n0 -6\nWe take the three matrices we used to perform each operation and multi\nply them to get E:\n⎡\n⎤ ⎡\n⎤ ⎡\n⎤\n0 0\n1 0 0\n1 0 0\nE = ⎣ 0\n-3\n⎦ ⎣\n-2\n⎦ ⎣ -2\n⎦\n\n⎡\n⎤ ⎡\n⎤\n⎡\n⎤\n0 0\n1 0 0\n0 0\n= ⎣ 0\n-3\n⎦ ⎣ -2\n-2\n⎦ = ⎣ -2\n-3\n⎦ = E.\nTo check, we evaluate EA:\n⎡\n⎤ ⎡\n⎤\n⎡\n⎤\n0 0\n1 3 0\n3 0\n⎣ -2\n-3\n⎦ ⎣ 2\n⎦ = ⎣ 0\n-2\n⎦ = U.\nTo find E-1, use the Gauss-Jordan elimination method (or just insert the\nmultipliers 2, 2, 3 into E-1)\n⎡\n⎤\n⎡\n⎤\n0 0 1 0 0\n0 0\n1 0 0\n⎣ -2\n-3\n⎦ -→ ⎣ 0\n-3\n-4\n⎦\n⎡\n⎤\n⎡\n⎤\n1 0 0 1 0 0\n1 0 0\n-→ ⎣ 0\n⎦ -→ ⎣ 2\n⎦ = E-1\nWe can check that this is in fact the inverse of E:\n⎡\n⎤⎡\n⎤\n⎡\n⎤\nEE-1 = ⎣ -2\n0 ⎦⎣ 2\n0 ⎦ = ⎣ 0\n0 ⎦ = I.\n4 -3\nFinally, to factorize A into LU (where L = E-1):\n⎡\n⎤\n⎡\n⎤⎡\n⎤\n⎣ 2\n0 ⎦ = A = LU = ⎣ 2\n0 ⎦⎣ 0 -2\n0 ⎦ .\nProblem 4.2:\n(2.6 #13. Introduction to Linear Algebra: Strang) Compute L\nand U for the symmetric matrix\n\n⎤\n⎡\n⎢⎢⎣\na\na\na\na\na\nb\nb\nb\na\nb\nc\nc\na\nb\nc\nd\n⎥⎥⎦\nA =\n.\nFind four conditions on a, b, c, d to get A = LU with four pivots.\nSolution:\nElimination subtracts row 1 from rows 2-4, then row 2 from\nrows 3-4, and finally row 3 from row 4; the result is U. All the multipli\ners `ij are equal to 1; so L is the lower triangular matrix with 1's on the\ndiagonal and below it.\n⎤\n⎡\n⎤\n⎡ a\na\na\na\na\na\na\na\nA -→\n⎢⎢⎣\n⎥⎥⎦ -→\n⎢⎢⎣\n⎥⎥⎦\n0 b - a\nb - a\nb - a\n0 b - a\nb - a\nb - a\nb - a\n0 b - a\nc - b\nc - b\nc - b\nd - b\nc - a\nc - a\nd - a\nc - a\n⎤\n⎡\n⎤\n⎡ a\na\na\na\n⎢⎢⎣\n⎥⎥⎦ = U, L =\n⎢⎢⎣\n⎥⎥⎦\n0 b - a\nb - a\nb - a\nc - b\nc - b\n-→\n.\nd - c\nThe pivots are the nonzero entries on the diagonal of U. So there are\nfour pivots when these four conditions are satisfied: a 6= 0, b 6= a, c 6= b,\nand d = c.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_Ses1.5sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/76bdb83fe7ac8f76eccd5c7c1849c50b_MIT18_06SCF11_Ses1.5sol.pdf",
      "content": "Exercises on transposes, permutations, spaces\nProblem 5.1:\n(2.7 #13. Introduction to Linear Algebra: Strang)\na) Find a 3 by 3 permutation matrix with P3 = I (but not P = I).\nb) Find a 4 by 4 permutation Pb with Pb4 6= I.\nSolution:\na) Let P move the rows in a cycle: the first to the second, the second to the\nthird, and the third to the first. So\n⎡\n⎤\n⎡\n⎤\nP = ⎣ 1\n0 ⎦ , P2 = ⎣ 0\n1 ⎦ , and P3 = I.\nb) Let Pb be the block diagonal matrix with 1 and P on the diagonal; Pb =\n0 P . Since P3 = I, also Pb3 = I. So Pb4 = Pb 6= I.\nProblem 5.2:\nSuppose A is a four by four matrix. How many entries of\nA can be chosen independently if:\na) A is symmetric?\nb) A is skew-symmetric? (AT = -A)\nSolution:\na) The most general form of a four by four symmetric matrix is:\n⎡\n⎤\na\ne\nf\ng\nA = ⎢⎣\n⎢ e\nb\nh\ni ⎥⎦\n⎥\n.\nf\nh\nc\nj\ng\ni\nj\nd\nTherefore 10 entries can be chosen independently.\n\nb) The most general form of a four by four skew-symmetric matrix is:\n⎤\n⎡\nA =\n⎢⎢⎣\n0 -a -b -c\na\n0 -d -e\n0 - f\nb\nd\n⎥⎥⎦ .\nc\ne\nf\nTherefore 6 entries can be chosen independently.\nProblem 5.3:\n(3.1 #18.) True or false (check addition or give a counterex\nample):\na) The symmetric matrices in M (with AT = A) form a subspace.\nb) The skew-symmetric matrices in M (with AT = -A) form a subspace.\nc) The unsymmetric matrices in M (with AT 6= A) form a subspace.\nSolution:\na) True: AT = A and BT = B lead to:\n(A + B)T = AT + BT = A + B, and (cA)T = cA.\nb) True: AT = -A and BT = -B lead to:\n(A + B)T = AT + BT = -A - B = -(A + B), and (cA)T = -cA.\nc) False:\n+\n=\n.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_Ses1.6sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/f4ae8cc2b4f36574983dd4c313f106eb_MIT18_06SCF11_Ses1.6sol.pdf",
      "content": "Exercises on column space and nullspace\nProblem 6.1:\n(3.1 #30. Introduction to Linear Algebra: Strang) Suppose S\nand T are two subspaces of a vector space V.\na) Definition: The sum S + T contains all sums s + t of a vector s in S and\na vector t in T. Show that S + T satisfies the requirements (addition and\nscalar multiplication) for a vector space.\nb) If S and T are lines in Rm, what is the difference between S + T and\nS ∪ T? That union contains all vectors from S and T or both. Explain\nthis statement: The span of S ∪ T is S + T.\nSolution:\na) Let s, s0 be vectors in S, let t, t0 be vectors in T, and let c be a scalar. Then\n(s + t) + (s0 + t0) = (s + s0) + (t + t0) and c(s + t) = cs + ct.\nThus S + T is closed under addition and scalar multiplication; in other\nwords, it satisfies the two requirements for a vector space.\nb) If S and T are distinct lines, then S + T is a plane, whereas S ∪ T is only\nthe two lines. The span of S ∪ T is the set of all combinations of vectors\nin this union of two lines. In particular, it contains all sums s + t of a\nvector s in S and a vector t in T, and these sums form S + T.\nSince S + T contains both S and T, it contains S ∪ T. Further, S + T\nis a vector space. So it contains all combinations of vectors in itself; in\nparticular, it contains the span of S ∪ T. Thus the span of S ∪ T is S + T.\nProblem 6.2:\n(3.2 #18.) The plane x - 3y - z = 12 is parallel to the plane\nx - 3y - x = 0. One particular point on this plane is (12, 0, 0). All points\non the plane have the form (fill in the first components)\n⎡\n⎤\n⎡\n⎤\n⎡\n⎤\n⎡\n⎤\nx\n⎣ y ⎦ = ⎣ 0 ⎦ + y ⎣ 1 ⎦ + z ⎣ 0 ⎦ .\nz\n\nSolution:\nThe equation x = 12 + 3y + z says it all:\n⎡\n⎤⎛\n⎡\n⎤⎞\n⎡\n⎤\n⎡\n⎤\n⎡\n⎤\nx\n12 + 3y + z\n⎣ y ⎦⎝= ⎣\ny ⎦⎠ = ⎣\n0 ⎦ + y ⎣ 1 ⎦ + z ⎣ 0 ⎦ .\nz\nz\nProblem 6.3:\n(3.2 #36.) How is the nullspace N(C) related to the spaces\nA\nN(A) and N(B), if C =\n?\nB\nSolution:\nN(C) = N(A) ∩ N(B) contains all vectors that are in both\nnullspaces:\n\nAx\nCx =\n= 0\nBx\nif and only if Ax = 0 and Bx = 0.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "MIT18_06SCF11_Ses1.7sol.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/d5e9b3dac09bd41988d0cba7ed021efc_MIT18_06SCF11_Ses1.7sol.pdf",
      "content": "Exercises on solving Ax = 0: pivot variables, special solutions\nProblem 7.1:\na) Find the row reduced form of:\n⎡\n⎤\nA = ⎣ 0\n7 ⎦\n2 -2 11 -3\nb) What is the rank of this matrix?\nc) Find any special solutions to the equation Ax = 0.\nSolution:\na) To transform A into its reduced row form, we perform a series of row\noperations. Different operations are possible (same answer!). First, we\nmultiply the first row by 2 and subtract it from the third row:\n⎡\n⎤\n⎡\n⎤\n⎣ 0\n7 ⎦\n⎣ 0\n7 ⎦ .\n-→\n2 -2 11 -3\n0 -12 -3 -21\nWe then multiply the second row by 1\n4 to make the second pivot 1:\n⎡\n⎤\n⎡\n⎤\n⎣ 0\n7 ⎦ -→ ⎣ 0\n1 1/4 7/4 ⎦ .\n0 -12 -3 -21\n0 -12\n-3 -21\nMultiply the second row by 12 and add it to the third row:\n⎡\n⎤\n⎡\n⎤\n⎣ 0\n1 1/4 7/4 ⎦ -→ ⎣ 0 1 1/4 7/4 ⎦ .\n0 -12\n-3 -21\nFinally, multiply the second row by 5 and subtract it from the first row:\n⎡\n⎤\n⎡\n⎤\n1 0 23/4 1/4\n⎣ 0 1 1/4 7/4 ⎦ -→ ⎣ 0 1\n1/4 7/4 ⎦\n\nb) The matrix is of rank 2 because it has 2 pivots.\nc) The special solutions to Ax = 0 are:\n⎤\n⎡\n⎤\n⎡\n- 23/4\n⎥⎥⎦and\n⎢⎢⎣\n-1/4\n-7/4\n⎢⎢⎣\n-1/4\nProblem 7.2:\n(3.3 #17.b Introduction to Linear Algebra: Strang) Find A1\nand A2 so that rank(A1B) = 1 and rank(A2B) = 0 for B =\n.\nSolution:\n= 02.\nA less trivial example is A2 =\n1 -1\n.\n1 -1\nTake A1 = I2 and A2\n⎥⎥⎦\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.06SC Linear Algebra\nFall 2011\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    }
  ]
}