{
  "course_name": "Role of Science and Scientists in Collaborative Approaches to Environmental Policymaking",
  "course_description": "This course examines joint fact-finding within the context of adaptive and ecosystem-based management. Challenges and obstacles to collaborative approaches for deciding environmental and natural resource policy and the institutional changes within federal agencies necessary to utilize joint fact-finding as a means to link science and societal decisions are discussed and reviewed with scientists and managers. Senior-level federal policymakers also participate in these discussions.",
  "topics": [
    "Engineering",
    "Environmental Engineering",
    "Environmental Management",
    "Social Science",
    "Public Administration",
    "Environmental Policy",
    "Public Policy",
    "Science and Technology Policy",
    "Engineering",
    "Environmental Engineering",
    "Environmental Management",
    "Social Science",
    "Public Administration",
    "Environmental Policy",
    "Public Policy",
    "Science and Technology Policy"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 1 session / week, 3 hours / session\n\nThe goal of the seminar is to explore the changing role of science, research, and scientists in contemporary society. We will focus on the use of scientific information and the role of scientists in collaborative approaches to natural resources and ecosystems-based management, and environmental planning and policymaking, and the institutional and societal transformations that are necessary for science to be more effectively used in the evolving models of participatory, deliberative governance and community-based ecosystem stewardship.\n\nIncreasingly scientists and science organizations are confronting a conundrum: Why is science often ignored in important societal decisions even as the call for decisions based on sound science escalates? One reason is that decision-making is often driven by a variety of nonscientific, adversarial, and stakeholder dynamics. Thus, even though science helps inform choices, it is only one of many values and interests considered by each stakeholder. In response to this emerging challenge, science and natural resource agencies are embarking upon research that explores the problems of incorporating science into value-laden societal decisions. This research includes designing experiments that will assess the appropriateness of using the new and emerging approach of Joint Fact Finding to address some of the Nation's most contentious environmental conflicts.\n\nWe will explore development of a holistic, stakeholder-driven process, which incorporates joint fact finding with adaptive management, and that complements and works in concert with national level policy and regulations, as a more effective approach to deal with the complexity, uncertainty, and conflict inherent in natural resources decisions, ecosystems based management and environmental policymaking.\n\nThe seminar will be highly interactive and a high degree of student participation and initiative is expected. Students will read a broad selection of literature as a springboard to discuss the challenges and realities of practice with scientists, natural resource managers, activist citizens, and senior level federal policy makers. Early in the semester we will discuss options for projects and papers. Completed student projects and papers will be featured on the MIT-USGS Science Impact Collaborative (MUSIC) Web site.",
  "files": [
    {
      "category": "Resource",
      "title": "Barriers to USGS scientistsâ€™ participation in collaborative research and decision-making",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/11-375-role-of-science-and-scientists-in-collaborative-approaches-to-environmental-policymaking-spring-2006/caac4b2f96eb3276b195684b0cea8c0c_campbell.pdf",
      "content": "Barriers to USGS scientists' participation in collaborative research and decision-making\nBy: Lindsay Campbell\n24 February 2005\n11.941 The Role of Joint Fact Finding in Environmental Decision-making\nInstructors: Herman Karl, Jennie Stephens\nMIT Department of Urban Studies and Planning\n\nAbstract\nChanges in the way in which science interacts with society are driving a transformation in the\nmandate of federally sponsored science. Federal agencies like the United States Geological\nSurvey (USGS) are adapting to the call for greater public and stakeholder involvement in the\nframing of research questions, the development of socially and policy-relevant science, and\ncollaborative decision-making processes emerging from that science. In \"a radical, non-trivial\ndeparture from the way in which agencies do business,\" the USGS is researching collaborative\ndecision-making techniques like Joint Fact Finding via the Science Impact program (Susskind,\n2004). However, structural, educational, and cultural barriers complicate the nature of changing\nfrom a bureaucratic science agency to a nimble, innovative organization that promotes\ncollaborative decision-making. This paper explores disincentives to change at the individual\nlevel that operate through agency structure and culture, particularly the Research Grade\nEvaluation (RGE) process. It also describes and evaluates current efforts by the USGS to\npromote institutional change through individuals. Strategic opportunities and potential further\nmechanisms of change in the areas of education/training, evaluation/promotion, and\nprogrammatic interventions are then identified and considered.\nMethods\nThe methodology of this paper relies on a literature review of both secondary sources and\nprimary documents from the Office of Personnel Management (OPM) and the USGS, as well as\na transcript of the USGS Dialog on Science Impact held in Columbia, MO in October 2003. It\nalso synthesizes comments from visiting speakers (researchers, policymakers, and managers\nrepresenting various agencies and organizations) to the MIT-USGS Science Impact\nCollaborative course on Joint Fact Finding in Environmental Decision-making in fall of 2004.\nFinally, I conducted key informant interviews with: Laure Wallace, an employee of the USGS\nOffice of Employee Development (OED) who has worked extensively on the RGE; Ron Webb,\nmanager of doctoral recruiting and university relations for Procter and Gamble and\nrepresentatives at DuPont--two of the leading corporations in Research and Development in the\nworld; and Dan Larson, a research physicist who has worked at a large research university, a\nsmall Ivy League university, and is currently doing post-doctoral work on biomedical physics.\nWhy collaboration?\nThe relationship between science and society in the United States has undergone massive\nchanges in the last fifty years. A fundamental shift is occurring: from positivism, wherein\nscience is defined as being comprised of neutral facts and objective findings, to constructivism,\nwhere science is considered constructed knowledge with subjective judgments like all other\nforms of human knowledge (Van de Klerkhof, 2004). This paradigm shift particularly affects the\ngoals and roles of government science (both government-funded and agency-generated), which\nhas a special responsibility not only to expand the frontiers of knowledge, but also to serve the\npublic interest--to ensure that science is serving a normative function. The changing values are\nepitomized by the contrasting mandates put forth by Vannevar Bush in 1945 and Jane Lubchenco\nin 1998 (Guston, 2000). Bush's \"Science: The Endless Frontier\" report came at the end of\nWorld War II, when optimism in the ability of science to solve social problems was at an all time\nhigh during the \"golden age\" of American science. Lubchenco's address to the American\nAssociation for the Advancement of Science notes the failure of modern researchers to conduct\nsocially relevant science and calls for a \"new social contract for science\" that prioritizes society's\n\n\"most pressing problems of the day, in proportion to their importance, in exchange for public\nfunding\" (Lubchenco, 1998: 491). She argues that not only must science advance knowledge,\nbut also it must be policy-relevant in a changing world with new global and ecosystem-scale\nchallenges. The shift in the relationship between science and the federal government is still in\nprogress and is shaped at multiple levels: from the federal budget treatment of science, to science\nagency priority setting, to individual researchers acting out of interest for self, science, and\nsociety.\nOccurring in parallel to the changing science mandate is an upsurge in public desire for\nparticipation in decision-making, including scientific decision-making (Sarewitz, 1997). These\ndemands for participation and due process grew from disparate movements including the civil\nrights movement and the environmental movement, coalescing in the 1960s and 70s. Not\ncoincidentally, it was during this time of strong social movements and heightened public\nawareness that most of the country's core federal environmental regulations were created. Laws\nlike the National Environmental Protection Act (NEPA), \"never intended to be the overarching\nstructure for public involvement, [have] become the basic template for agency actions in many\ncases\" and frame the way in which agencies and citizens interact, forcing federal agencies to\nadapt (Wollondeck and Yaffee, 2000: 243). Web-based technology and the proliferation of\naccessible information have also brought into question agency legitimacy on the basis of\ntechnical wisdom alone. Agencies that were created to be technical advisors and facilitators now\nhave to be communicators and public process managers (Wollondeck and Yaffee, 2000; Jacobs,\n2001).\nThe limitations of NEPA, particularly in its ability to manage complex and controversial\nprojects, have become apparent. Despite efforts to frame NEPA in a manner that ensures public\nparticipation, the difference between meaningful participation and token participation are\nrelevant in a democratic society (Arnstein, 1969). Public input in NEPA generally consists of a\npublic comment period on the Environmental Impact Statement that the managing agency can\neither choose to take into account or not; which can hardly be considered more than token\ninvolvement and is not a means for fostering environmental democracy (Peyser, 2003; Jasanoff,\n1996). The current system is not well suited to manage and promote citizen involvement,\nmaking legal tactics common and costly. Without denying the critical role of the right to and\npower of legal action, groups of stakeholders and agencies are both realizing that the adversarial\nscience involved in a lengthy court case is a drain on resources and often counterproductive\n(Wollondeck and Yaffee, 2000). As Rich Whitley, National Stewardship and Partnership\nCoordinator of the Bureau of Land Management stated, \"the current system is broken and\ndysfunctional\" (2004). The need for functional participation spans all scales of issues, from\nregions and water basins to municipal and even site-specific concerns.\nTherefore, a second wave of change that goes beyond one-way communication towards real\ncollaborative decision-making is currently taking place. Agencies recognize the need for\ncredible science that is reputable by the highest standard, salient science that is policy-relevant,\nand legitimate science that is trusted and generated in a transparent manner to support\nparticipation and inform decision-making (Cash et al., 2003). Sheila Jasanoff notes that\ninformation exchange alone will not generate solutions to complex environmental problems,\nrather institutions of community and trust are also required to move towards collective action\n\n(1996). The Department of Interior (DOI) has placed a priority on collaboration through its\n\"Four Cs\" philosophy of \"conservation through communication, consultation and cooperation\"\nthat has now spread to include other agencies as well (DOI, 2004a; Hess, 2004). Indeed, DOI\nhas over 200 employee training courses with \"collaboration\" in the title (Whitley, 2004). The\nUSGS is taking societal relevance and collaboration seriously, through the creation of the\nScience Impact program and a broadened definition of research in the RGE that includes science\ntranslation and information dissemination (USGS, 2001; USGS, 2002). Given the growing\npressures on federal budgets, the trend toward promoting innovation since the 1993 Government\nPerformance and Results Act (GPRA), and the 1995 USGS Reduction in Force, Science Impact\nis both a core value (along with Science Excellence and Science Leadership) and a critical\nprogram to help guide the existence and growth of the Survey (Groat, 2004).\nHowever, as with all major organizational changes, there are barriers and institutional resistance\nto overcome. The Office of Personnel Management (OPM) enumerated ten common barriers to\ninstitutional change:\n1. Turf battles\n2. Employee and manager resistance to change\n3. No one \"owns\" change process\n4. Lack of incentives to change\n5. Difficulties in thinking outside the box\n6. Resources tied up in current systems\n7. No champion in top management\n8. Skepticism\n9. Lack of resources\n10. Do not see a compelling reason to change (OPM, 2000a: 4)\nThis paper focuses specifically on barriers at the individual level: workforce composition, the\nevaluation and promotion structure, and the culture of research that exists at the USGS. It is\nclear that educational, structural, and cultural barriers are interwoven and overlapping in the\ncomplex environment of an organization, but these divisions are useful to think about the\ndifferent types of challenges the USGS faces and to begin to identify what might be levers of\nchange from the perspective of Science Impact.\nEducational barriers\nDisciplinary Structure\nIn thinking about institutional change, it is intuitive to consider current employees as likely\npoints of resistance (and indeed that will be addressed in the section on cultural barriers), but it is\nperhaps more important to think about new and potential hires when charting the course of the\nfuture of an organization. A National Science Foundation project examining undergraduate\nscience courses found that they generally do not discuss policy applications or societal relevance.\nScience and engineering education is fundamentally different from liberal arts education, with\nstudents in the science disciplines often lacking the necessary training and skills in leadership\nand communication (Wallace, 2004).\n\nThe USGS research scientists of tomorrow are being taught by individuals situated in\ndepartments that are still functionally independent units. Indeed, a value of reductionism drives\nknowledge and education systems in America into a disciplinary structure (Karlqvist, 1999). For\nexample, though collaborative projects within universities (e.g. the Cornell Genomics Initiative)\nattempt to hire interdisciplinary faculty, potential hires must still pass muster at the departmental\nlevel and faculty must also be able to teach the discipline-specific undergraduate foundation\ncourses. This presents hurdles to universities going beyond \"lip service\" to interdisciplinary\nresearch unless they create a well funded, truly interdisciplinary department (e.g. Harvard\nUniversity's Department of Systems Biology) (Larson, 2004). Sung et al (2003) explored\ndisciplines as cultures, noting \"the cultural barriers are at least as great as the institutional\nbarriers. A scientific language, approach, and training style are passed from mentor to student\nwithin disciplines, like a tribal culture\" (1485). Thus, disciplines are cultural tools but are also\nsocial relationships that promote group identity (Lattuca, 2002). Chubin and Maienschein (2000)\nexpand upon this to note that while the sciences champion individual accomplishments,\noriginality, and ownership of ideas, the policy arena requires communication and \"speaking for\nsomeone else\"; the authors describe this difference as no less than a culture clash.\nTenure\nBarriers to collaboration stem not only from the fundamental way in which academic institutions\nare organized but also from the way in which academic success is measured. Success in most\nacademic science disciplines requires specialization, with a priority towards reductionist science,\nboth for graduate students and certainly for tenure-seeking faculty. In a study of\ninterdisciplinary faculty, Lisa Lattuca found that her informants were concerned about\npromotion, tenure, and rewards of interdisciplinary research (2002). Kostoff (2002) notes that\nwithout other potential rewards, most researchers will \"take the path of least resistance\" and\nfocus in one discipline or participate in collaboration that exists only on paper. Physicist Dan\nLarson discussed the impact of tenure on collaborative science research,\n\"The most important job of a young investigator is to get tenure. This is achieved by\nstarting a research program, getting funding, and producing results. However, there is\nalso this unspoken requirement that the investigator 'own those results', so to speak. The\ndepartment wants to see that this person is running his own show, not depending too\nmuch on collaborators. So if there is collaboration, a young professor has to be very\ncareful to make sure that it is on his terms, so he can state, in no uncertain terms, that it is\nhis research program. People look down on scientists if they rely too much on\ncollaboration\" (Larson, 2004).\nThe academic structure of science informs hiring options at the USGS. There is a tradeoff\nbetween hiring boundary spanners who have training in science-policy and hiring scientists who\nhave built up their credentials through single-discipline, self-directed research.\nEducational barriers to collaboration are not limited to the sciences alone. Natural resource\nmanagement education has a tendency to stress field-based work, such that often those\ngraduating with range management degrees or forestry degrees seek to work out on the grassland\nor the forest, rather than in the office or conducting stakeholder meetings. Certainly there are\nnotable exceptions--like the Yale School of Forestry and Environmental Studies or the\n\nUniversity of Michigan School of Natural Resources and Environment, to name a few--that\ntruly promote policy and leadership. Reinforcing the educational approach is the fact that people\nwho self-select into this field often have, as Karl Hess of the Fish and Wildlife Service said, \"a\ndesire to be a true hands-on manager and a love of the resource\"(Hess, 2004). Overall, Clark and\nMeidinger (1998) note, \"scientists must become more comfortable with the history and practice\nof resource management, and resource managers must become more comfortable with the history\nand practice of science\" (6).\nStructural barriers\nThe General Schedule\nIn order to understand the structure of evaluation and promotion and how it affects USGS\nscientists' involvement in collaborative decision-making processes, one must consider it in the\ncontext of the broader federal pay system, the General Schedule (GS). The GS covers 1.2\nmillion federal employees and was initially created through the Classification Act of 1949. The\nGS is a system that emphasizes \"internal equity\" as a value of utmost importance, meaning that\nemployees across the government are paid equally for equal work (OPM, 2002). The goal of the\nGS is to standardize pay and rank according to \"scope and complexity\" of work, essentially\nequating, for example, a GS-13 attorney with a GS-13 geologist, with a GS-13 manager\n(Wallace, 2004). The OPM explains the origin of this structure and how that structure no\nlonger fits with the knowledge-based modern workforce of the federal government:\n\"The fundamental nature of the Federal compensation system was established at the end\nof the 1940s, a time when over 70 percent of Federal white-collar jobs consisted of\nclerical work. Government work today is highly skilled and specialized knowledge work.\nYet in the age of the computer, the Federal Government still uses--with few\nmodifications--pay and job evaluation systems that were designed for the age of the file\nclerk.\" (OPM, 2002: 4)\nThe changing nature of work conducted by the federal government has, therefore, led to a change\nin the distribution of federal workers; making it top-heavy as an organization (see Figure 1).\nInformal observations suggest that this trend also exists in the USGS Geology Discipline, with\nits clustering of GS-13 and GS-14 research scientists (Karl, 2004). The GS structure effectively\ntreats all agencies of the government as a \"single employer\", is extremely hierarchical, and was\ndesigned for administration not innovation (OPM, 2002). At times, the structure may even\ncreate a disincentive to innovation, in that one inherently gains responsibilities as one climbs the\nhierarchy. Karl Hess of the Fish and Wildlife service reported knowing of individuals that stay\nat the GS-13 level in order to maintain some freedom, rather than take a pay increase, noting that\n\"the financial incentives aren't large enough\" (Hess, 2004). The organizational literature is\ndivided on the effects of stratification: whether it encourages specialization and good\nperformance or whether hierarchy's organizational control fosters conformity (Baron, 1984).\nThe GS structure can certainly be juxtaposed with that of smaller, less hierarchical, more team-\nbased approaches like those used by software development firms in Silicon Valley whose\nmainstay is innovation. It can also be juxtaposed larger organizations that nonetheless retain\ntheir nimbleness out of necessity, such as the United States Armed Forces (Hess, 2004).\n\nFigure 1: from (OPM, 2002: 5)\nWith its emphasis on \"internal equity\", the GS system lacks \"external equity\", wherein pay\nwithin the government is equivalent to that which one would receive doing similar work for the\nprivate sector. This presents challenges for some agencies in the recruitment and retention of\nhighly qualified individuals. The National Research Council reviewed evidence through the\nearly 1990s that showed that by the late 1980s federal white-collar employees were paid less\nthan their private-sector counterparts in most occupations and most geographic regions (NRC,\n1993). Private firms like Proctor and Gamble, which spends upwards of $5 million on research\nand development per day, stress that \"no factor has played a more important role in the success\nof R&D at P&G than its record of hiring and retaining some of the most talented people in the\nindustry. Once on board, R&D staff members are rewarded and recognized for their\ncontributions through financial compensation, promotions, freedom to influence project\nselection, and financial support for their projects\" (P&G, 2004). There are limits in the extent to\nwhich a business model can be compared to the federal government, particularly given the\ndecreasing resources for federal science research, as opposed to the growing amount of resources\nspent on R&D by Procter and Gamble in every year since World War II (P&G, 2004).\nMoreover, the literature on public service suggests that there are a number of non-monetary\nmotivators (including humanitarianism, communitarianism, patriotism, and a good Samaritan\nethic) that drive people to work in the public sector as opposed to the private sector, so external\nequity may not be the most problematic issue (Brewer, Selden, Facer, 2000).\nFinally, the government pay structure lacks \"individual equity\", wherein one's pay is inherently\nlinked to one's performance (OPM, 2002). The GS generally rewards loyalty more strongly than\nit rewards performance, with over 75% of all increases in federal pay in the year 2000 bearing no\nrelationship to individual achievement or competence. Figure 2 indicates that increases for\ninflation and locality comprise the majority of pay increases. The second highest proportion of\n\nincrease comes from Within Grade\nFigure 2: from (OPM, 2002: 21)\nIncreases, which are the natural\nprogression up the ten steps of a\ngrade that occur simply with the\npassage of time (OPM, 2002).\nBusiness literature suggests that to\nimprove performance, firms need a\nstructure of performance-based\nincentives (McKenzie and Lee,\n1998). Firms like Procter & Gamble\nand DuPont ensure that pay is linked\nto performance by conducting\nregular evaluations of deliverables\nthat support the goals of the\ncorporation. Researchers are not\nevaluated by profitability, but by\nother measures such as patents awarded. Bill Provine of DuPont commented, \"if you cant divide\nthe task into something measurable, then you need to take it down to something smaller. It's\nhard to predict that you will invent a widget to do a task, but you could break it down into\nmeasurable, definable units like 'completed literature review, met with producers, etc'\" (2004).\nCritical to the structure of these evaluations is that the goals and objectives are jointly crafted by\nsupervisors and employees to ensure that the plan functions like a contractual work plan. Pay for\nperformance is linked not only to basic salary level, but also to bonuses and promotions. Both\nProcter and Gamble and DuPont use annual reviews that rate employees relative to their peers--\nrewarding top performers and flagging low performers (Webb, 2004; Provine, 2004).\nClearly, not all business strategies translate to the government. Government lacks the simple test\nof profit increase/decrease as an evaluative measure of performance, which makes establishing\nGPRA targets for strategic performance plans extremely difficult (IAGCPMR, 1993). Despite\nthe need for improved evaluation of government performance, GPRA did not implement an\noverhaul to the GS. A particular challenge identified by Wise and Agranoff (1991) lies in the\nevaluation of public sector research. Traditionally, that evaluation has focused on the quality of\nresearch products but as federal budgets tighten, there is a need also to measure \"research\nefficiency and effectiveness\" (IAGCPMR, 1993: 9). Evaluating that effectiveness in terms of\nScience Impact is precisely the challenge facing the USGS. Despite having recognized and\ndocumented the myriad problems with the current GS as related to innovation, OPM responses\nare slow in coming, illustrating the point of that National Research Council that \"one of the\neternal verities of federal personnel policy is that it has been saddled with a considerable amount\nof inertia\" (NRC, 1993: 85).\nResearch Grade Evaluation\nThe federal pay structure is not as entirely undifferentiated as a first glance at the GS might\nsuggest. In the 1950s, several systems of evaluation were created that distinguish between the\ndifferent roles government employees serve. Though employees progress up the same steps and\ngrades, they are evaluated by different guidelines, which include the RGE that will be discussed\nin greater depth here, the Leader Grade Evaluation, and the Research-Grants Grade Evaluation,\n\namong many others. At the USGS, a distinction is made between research scientists and science\nand technology practitioners, with the RGE applied to those who spend at least half of their time\non research (USGS, 2001). A metaphor for this distinction is in the evaluation one would\nconduct for a family doctor and for a researcher at the National Institute of Health. Though both\nare \"doctors\", just as both USGS employees are \"scientists\", one would want to ask them very\ndifferent questions to evaluate whether or not they are performing well (Wallace, 2004). The\nguidelines for the Biological Resources Division of the USGS further define the application of\nRGE:\nMany different professional job series require scientific training and application of\nscientific skills. Individuals assigned to such positions are usually called \"scientists\" and\nmany have advanced degrees, e.g., Master of Science or Doctor of Philosophy. The\nRGEG is used to evaluate the grade of a professional scientific position only if the\nnature of the work performed is research. Research, as defined by the RGEG, is\nsystematic, critical, intensive investigation directed toward development of new or fuller\nscientific knowledge. It may be with or without reference to a specific application.\n(USGS, 2001, emphasis original)\nHaving different evaluative tools is one essential way in which hierarchical rank can be better\nevaluated and is an important tool for spurring organizational change by emphasizing agency\ncore values, competencies, goals, and tasks. It is also extremely challenge to design an effective\nstructure, as an Interagency Taskforce on evaluation stated, \"if one were to ask federal human\nresource practitioners to name the biggest challenges they face, designing and administering an\neffective performance appraisal system would rank at or near the top\" (Orr in IAGCPMR, 1993).\nThe RGE provides a career track for researchers that is distinct from managerial and supervisory\ntracks and introduces an element of performance-based pay through a peer review process.\nThere have also been efforts to standardize the RGE across various agencies, including USGS,\nthe USDA Forest Service and Agricultural Research Service, in order to improve interagency\ncooperation (USGS, 2001). Evaluation is conducted in the USGS at a minimum of once every\nfour years (faster if an employee is producing new work at a rapid rate and requests an expedited\nreview) and consists of a panel peer review conducted under OPM's four factors, which are:\nI.\nResearch situation or assignment\nII.\nSupervision received\nIII.\nGuidelines and originality\nIV.\nQualifications and scientific impact (USGS, 2004: 50)\nPanels are not anchored to any particular locality, but are more like academic peer review in\nwhich specialists from around the country in a scientist's distinct sub-field evaluate her body of\nwork on the above criteria. Each discipline of the USGS (geology, biology, geography, and\nwater) conducts a separate, staggered review, leading to evaluations going on across the agency\nat all times. These panels sole responsibility--though a large one--is to determine the\nappropriate title, position description, and whether scientists should remain in grade, be\npromoted, or be demoted. They do not assess performance in terms of offering awards (Wallace,\n2004; USGS, 2001).\n\nThe new guidelines make clear that the intent of the RGE is to evaluate research for its Science\nImpact, as defined by the whole continuum of promoting partnerships, conducting good research,\nand having outcomes/impacts (USGS, 2004). Whether these stated guidelines are actually\nfollowed is a question of the culture of the USGS and will be addressed in a later section. In\nessence, there is nothing in the language of the RGE that prevents Science Impact from being the\nstandard by which scientists are measured, however that are other cultural barriers and perhaps\ninterpretation barriers of the intent of the RGE that stand in the way (Wallace, 2004). Cultural\nbarriers are real barriers nonetheless, and will require further structural solutions to ensure that\nthe panel process is serving its intent. Current USGS efforts to improve the RGE and further\npossible interventions will be discussed in later sections as well.\nSupervisor Review and Awards\nIt is the supervisor's job every year, working with a classifier, to ensure that employees are being\nevaluated at the appropriate grade level and to assess their performance in that level. This means\nthat supervisors exercise the first judgment as to whether an employee should be pursuing the\nresearch track, the technology track (Equipment Development Grade Evaluation), or a\npractitioner track. Indeed, language on the panel review process noted \"The RGEP should not be\nexpected to do a supervisor's work; poor research performance should be managed through the\nperformance appraisal process at the Center or Cooperative Unit level in concert with the\nrespective servicing personnel office\" (USGS, 2001). Supervisors and center directors also work\nwith scientists to identify mentors, cyclical funding streams, and projects that will build up\nresearch credentials for their scientists (USGS, 2003). Thus, supervisors appear to represent less\nof a barrier and more of an underutilized resource as agents of change.\nA particular challenge facing supervisors is the evaluation of individuals working in a team\nenvironment. Teams are critical to interdisciplinary science, but evaluation of individuals on\nthose teams should not be reduced to whether the scientist is the first author on a refereed journal\narticle alone. However, this is a common shortcut used by supervisors and panels alike to\nquickly assess the contribution of a scientist to a project. It involves more work on the part of\nthe assessor to ascertain the real contribution of someone on the team then to the simple rubric of\nwhether they are the first author or not (Wallace, 2004). A number of techniques exist that\nwould serve to better evaluate team performance, falling roughly into the following typology:\n\nFigure 3: from (OPM, 2000b: iii)\nSupervisors can, in assessing employee performance, recommend people for rewards and offer\nconstructive and positive feedback. In a 1998 OPM study of 15 agencies that were revising their\nperformance-based awards, none of these agencies had defined \"excellence\", although they were\ngiving awards on the basis of excellence. There is very little monitoring and evaluation of the\nimpact of award programs. Furthermore, a later study of federal employees found that only 23%\nbelieve that awards have an impact on performance (OPM, 2000b). Some scholars, including W.\nEdwards Deming, have even argued that monetary rewards for individual performance actually\nhave a demoralizing and negative effect on the organization as a whole, perhaps suggesting a\nneed for a greater emphasis on team-based rewards and asserting the importance of team-based\nevaluation (IAGCPMR, 1993). Others argue that the lack of rewards for innovators and risk\ntakers stymies change (Groat, 2004). One case study from the USGS Science Impact Best\nPractices highlighted the basin modeling tools and collaborative management that were\nundertaken in Albuquerque, NM. Although the agency identified this project as one of its\ntouchstone cases of Best Practice, the manager got no recognition or positive feedback over the\ncourse of conducting the project (Posson, 2004).\nPerhaps one could argue that reward systems will not drive change, but the lack of awards and\nappropriate encouragement for those who are moving the agency in an innovative and mission-\ncritical direction are a potential missed opportunity. Respondents from Procter & Gamble and\nDuPont both highlighted the importance of awards for excellence, such as the Victor Mills\nSociety at P&G, along with other recognitions like titles and listing in corporate publications.\nThese are of particular importance for publicly reinforcing group values given the relatively\nprivate nature of wage status (Webb, 2004). Recognizing rewards and other informal\n\nrecognitions as important feedback mechanisms, this paper's focus is on evaluation and\npromotion structures and the cultures that surround them as primary incentives.\nCultural barriers\nOrganizational change does not happen easily or quickly; it is mediated by a number of factors,\nincluding the values of the employees and the culture of the organization. Lubchenco's mandate\nfor socially relevant science is not wholly consistent with the priorities and objectives of many\nresearch scientists. Scientists commonly enter the field due to an interest in self-directed,\ninquiry-driven science. Larson discussed academic science research and social relevance,\n\"My opinion is that scientists do the research in which they are interested and find de\nfacto justifications for it after the fact. Yes, we all write into the grants how our research\nis going to cure breast cancer, or halt the spread of HIV, but the fact is that every lab I\nhave ever been associated with has been a pure research lab. In fact, unless there you are\na M.D./Ph. D, the chances that you are doing clinically relevant work are low. I think a\nlot of scientists have a love/hate relationship with the topic [of societal relevance of\nresearch]. Many of them have a dim view of that sort of translational research, because it\ndoesn't involve imaginative, creative science. Maybe the first part, the original kernel of\nthe idea, was creative, but the actual reduction to practice is in most cases tedious and\ntime consuming\" (Larson, 2004)\nSimilarly, in the Science Impact Dialog, one USGS researcher noted the challenge of having to\nbe \"fairly creative in figuring out how to make science out of some of that data\", when working\non client-driven research (USGS, 2003). Scientists' individual values for pure science as\nopposed to policy relevant science should not be discounted as a relevant factor slowing change.\nUSGS has an external reputation as a credible science agency wherein research is often\nconducted in a \"pure science\" rather than a \"regulatory science\" framework. USGS has a culture\nthat values the status of the research scientist above the manager and is closer in nature to\nacademia than to other federal agencies (Wallace, 2004; Karl, 2004). In contrast with USGS,\nChris Racher of EPA Region 1 said that at the EPA, \"policymakers are better communicators and\nget promoted more\", which reinforces a hierarchy of policy over science in this regulatory\nagency; presenting essentially the opposite problem that the USGS faces as a neutral science\nagency that is trying to be policy relevant (Racher, 2004; Powell, 1999). This comparison is\noffered to clarify that simply because a public agency is involved in science, a status hierarchy of\nscience over policy does not always emerge--rather, I would argue, the relationship responds to\nthe mission of the group. Different organizations have different cultures that evolve in service to\nthe mission of the agency, but the effects of those cultures must constantly be revisited,\nparticularly in the public sector. The status of research at the USGS drives a wedge between the\nintent of the RGE guides and their use in practice in two ways: 1) individuals are sometimes\nevaluated under the wrong rubric and 2) panels often ignore the full breadth of Science Impact\nthat is supposed to be evaluated under the review (Wallace, 2004).\n\nPerson evaluated under wrong rubric\nWallace described a case of the first problem wherein an extremely innovative information\ntechnology specialist initially began his career as a research scientist but progressed to doing\ninnovative information management and web-based tool development that supported the work of\nhundreds of scientists. Despite this, he remained at the mid-range grade of GS-11 and his panel\nwas not able to recommend him for a grade increase on the basis of his research. Essentially, he\ncould not be appropriate evaluated under RGE. Despite this, the panel refused to recommend\nthat the specialist be considered for non-research grade evaluation as that would mean losing his\ntitle of \"researcher\", even though it mean a likely increase in grade and wage (Wallace, 2004).\nThis challenge points to the need for improved supervision to ensure that employees are being\nproperly evaluated. More importantly, though, it speaks symbolically to the status of research as\nan incentive and a cultural value within the agency, even eclipsing the motivation of pay.\nReview panels ignore full breadth of Science Impact\nThe second cultural barrier of panelists giving primacy to peer-reviewed publications in certain\njournals above all other criteria in the evaluation was a concern throughout the literature and\nidentified by a number of informants familiar with the USGS. The practice encourages\nreductionist science and discourages interdisciplinary, client-driven, or collaborative science in\npreferring peer-reviewed publications above all other types of research (Karl, 2004; USGS,\n2003). One USGS scientist in the Dialog on Science Impact noted, \"we're still in the old mode\nof, in the end, see how many publications you've got and that's the criteria for success. And it's\nextremely difficult to quantify impact\" (USGS, 2003). Karl Hess of the Fish and Wildlife\nService noted, \"if you're a USGS scientist who writes a policy document or book that changes\nthe world, if you're not publishing peer reviewed articles, it doesn't matter.\" USGS scientists\nface the pressure of producing peer-reviewed work, just as academic scientists do (Hess, 2004).\nA similar tradeoff exists in academia, between producing journal articles and pursuing teaching,\nan important form of societal impact. Recognition and rewards come from publishing in\nrecognized journals, not from conducting outreach or education (Jacobs, 2001). USGS scientists\noperate in this framework without the eventual possibility of the freedom of tenure.\nA case study also illustrates the point. A GS-14 research hydrologist of USGS appealed his\npanel's decision to a second panel, to the DOI, and eventually to OPM. The OPM decision\nincludes a careful weighing of a number of issues, but an excerpt from the section on Factor IV\nspeaks to this issue:\n\"....Although the appellant has authored a number of publications of considerable interest\nto other researchers emphasizing the importance of ecological considerations in\nmultipurpose water management, there is no indication that his research has as yet had a\nmajor impact on advancing the field or that it has resulted in new inventions or\ntechniques as contemplated at Degree E. The appellant's work involving network\nsimulation flow modeling and multicriteria decision making is not yet accepted as\ndefinitive within the scientific community. Information from our contacts stressed the\nlack of peer-reviewed publications by the appellant in the past ten years. They noted\nthat this lack of publications might be because the appellant has spent a large portion of\nhis time furthering his professional development through active participation in\nprofessional society and academic committees and conferences. Because of the limited\n\ndegree of published data in scientific journals that has been subjected to peer review, with\nthe conclusions accepted and proven repeatable, this aspect of Degree E cannot be\ncredited to the appellant's position....\" (OPM 2000c: 11, emphasis added)\nThe Factors include the language of science impact and are intended to measure originality,\nscience leadership, scientific validity, and societal benefit, but the shortcut of using peer\nreviewed publications as the key metric is a culturally determined priority (Wallace, 2004).\nDaniel Sarewitz offers a hypothetical alternative:\n\"What if public service were rewarded as strongly as number of publications or patents?\nIf helping a community or an organization to address a technical issues or problem was a\ncriterion for promotion, peer approval would follow. It is hard to imagine that such a\nchange would lessen public support for R&D. Moreover, positive feedback between\nsocial needs and the research agenda would begin to evolve at a grassroots level\"\n(Sarewitz, 2000: 31).\nDifferences between the disciplines\nThough the USGS is one agency, it is comprised of four disciplines that differ in their focus,\ncomposition, and culture, as well as in how they view and evaluate collaboration. A striking\ncontrast can be made between the Geology and Biology Disciplines on one hand and the water\nand geography divisions on the other. The Water Resources Discipline comprises almost half of\nthe entire survey, with 4,000 employees, of which 1500-2000 are working hydrologists, but only\n300 are research-grade scientists. In the geology staff of 1800 employees, 600 of the 700\nscientists are research grade scientists. Biology has a similar ratio as that of Geology, whereas\nGeography is comprised predominantly of practitioners (Wallace, 2004). Physically having a\nwater office in each state further promotes greater public interaction, akin to a university\nextension service model (Barrington, 2004). Simply through the employee distribution, it is\nevident how a culture of pure research can be promoted in certain divisions.\nThe culture is also driven by the way in which research is funded in the different divisions, with\ngeology receiving a much larger proportion of core, long-term funding for pure, curiosity driven\nresearch, while water relies largely upon research with reimbursable funds and cooperative\nprograms that are more client-driven (USGS, 2003; Barrington, 2004). Wallace also noted\nfunding limitations for \"systems level thinking\" when doing client-driven research (Wallace,\n2004). This impacts research scientists' ability to be promoted, particularly young scientists, as\nthe following interchange at the USGS Science Impact Dialog in Columbia, MO:\nA:\n\"As a new research-grade scientist, listening to everyone who has careers of\nprobably 10, 15, 25 years of research, how can a new research-grade scientist\ndevelop an area of expertise if she's following reimbursable funds, if she's always\nanswering a client's question?\"\nB:\n\"I'm glad you asked that question early.\"\n[LAUGHTER]\nC:\n\"Yeah, because not in this agency.\" [joking] (USGS, 2003)\n\nThe laughter of the group indicates a shared understanding of a fundamental issue within the\nUSGS. The Dialog participants noted that many scientists will \"walk away\" from a client-driven\nproject if it will not lead to peer-reviewed science; but not every unit or discipline has the option\nof doing so, nor should this practice necessarily be encouraged from the perspective of Science\nImpact (USGS, 2003). Although there is a critical interaction between resource flows and\nincentives to individual scientists, this paper continues to focus on the interventions at the\nindividual rather than the institutional level. Further research on the issue of budgeting\ninfluences on collaboration needs to be conducted. The take away message for evaluation and\nreview is that peer reviewed materials should not continue to be valued above other outputs.\nEvaluation of current USGS strategies promoting change and recommendations for further\ninterventions\nStructural Reorganization and Science Impact\nWith a total of approximately 10,000 employees, the USGS is one of the smaller agencies in the\nfederal government and should be able to promote change with greater ease than other agencies.\nUSGS is taking structural, cultural, and educational steps to facilitate this change. The agency\nrecognizes that the switch from curiosity-driven science to science with greater social relevance\nis a large transition for a 125-year-old organization to make (Wallace, 2004). The change\nbenefits from having leadership that believes in the direction, but ultimately it must be driven by\npeer learning and not just top-down mandates (Jacobs, 2001; Posson, 2004). Perhaps the largest\nand most significant trend in the USGS supporting this belief is the move towards greater\nregionalization in order to diffuse decision-making out from headquarters (Groat, 2004). The\nexisting framework of the four disciplines also provides a useful starting point for affecting\norganizational change, recognizing the different funding structures, employee compositions, and\nresearch goals of the different disciplines. Future efforts related to organizational change should\nuse the disciplines as conduits, rather than simply creating separate offices related to\ncollaboration or public involvement.\nSince the Science Impact program is still in the early stages of development, information\ngathering and listening sessions are key strategies for peer learning. A series of group and\nindividual listening sessions and discussion sessions with senior management and senior\nscientists have been held to get there input on Science Impact, including the October 2003\nDialog on Science Impact (USGS, 2003). Highlighting examples of innovation in collaborative\nscience and decision-making is the goal of the USGS Best Practices project (see an analysis of\nBest Practices by Peter Brandenburg, 2004). The Best Practices project is useful both to extract\nlessons learned from case studies on the ground, but also to affect incremental cultural change by\ncelebrating non-traditional, client-driven, and decision-relevant science. These are important\nfirst steps to understanding the status quo (both best practices and common practices via the\ndialogues). As Science Impact is incorporated as a research program into the Geography\nDivision, it needs to move beyond simply and understanding of the status quo if it truly wishes to\n\"serve the public and sustain the USGS\" (Posson, 2004). Organizational leadership, a clear\naction plan for coordinating research between the different centers, and sufficient sustained\nfunding is necessary to develop Science Impact into a robust, effective research program.\nThe MIT-USGS Science Impact Collaborative has offered comments to senior management on\nthe \"Draft Guidelines for USGS Participation in Collaborative Public Engagement Processes and\n\nNeutrality in Policy Decisions\", which could potentially serve two functions. The agency needs\na short policy statement on collaborative decision-making that makes clear the vision of senior\nmanagement. It also needs a distinct working document with detailed sets of guidelines for\nvarious employee roles (scientists, managers, etc) that establish the boundaries of advocacy\nneutrality and collaboration that is created collaboratively with employees in the different\ndisciplines.\nAnother trend specifically relevant to collaboration and boundary spanning is the greater effort\non the part of the USGS to partner with other agencies, particularly those in the DOI. This is\noccurring first in high profile, often joint federal and state funded, highly complex projects such\nas the Everglades, CALFED, and the Missouri River Basin (Groat, 2004; USGS, 2003) In the\nEverglades case, a DOI Coordinated Science Plan was created in May 2004, involving the efforts\nof dozens of scientists to craft common questions on how to restore and protect water quality,\nnatural resources on DOI lands, and endangered species of the region (DOI, 2004b). USGS\nparticipated as an agency and through its individual scientists. The Everglades case study\ninvolves a range of different collaboration mechanisms, including virtual information sharing\nthrough the South Florida Information Access website (http://sofia.usgs.gov/) that is maintained\nby the USGS. Transferring this level of collaboration down to more routine, lower profile issues\n(without federal appropriations) remains a systematic challenge to the agencies. Expanding\nbeyond just intra-DOI and intra-agency collaboration to true public collaboration is the goal of\nJoint Fact Finding processes that involve stakeholders in the framing, scoping, and conducting of\nscience.\nHiring and Education\nChange is being promoted in USGS with an emphasis on \"building from the existing human\ncapital\" of the organization (Posson, 2004). One means of doing this is by shifting existing\nemployees who are qualified and interested from various disciplines into the Science Impact\nprogram. However, significantly changing the ratios of research scientists to practitioners is not\ncurrently a priority for the Survey, with existing scientists arguing that too many managers and\nother non-scientists are already employed by the Survey (Wallace, 2004; USGS, 2003). From an\norganizational change perspective, I argue that senior management needs to evaluate its\nobjectives and hire accordingly. If USGS'goal is to increase societal relevance of both science\nand the agency at large, the USGS will need to hire different sorts of individuals than if it wishes\nto continue prioritizing inquiry driven science above science translation, tool development, and\ncollaborative decision-making. This change can occur gradually with the natural attrition of\nolder research scientists that move into retirement. In their wake, boundary spanners ought to be\nhired and promoted, with an eye towards \"hiring for attitude and training for skill\" (Groat, 2004).\nBoundary spanners are particularly needed in team leader roles, leading interdisciplinary teams\n(Hess, 2004). Strategic early appointments of exceptional boundary spanners should also be\nconsidered, given the ability of well-intentioned and similarly well-positioned individuals (even\nwithin a resistant organization) to affect change and the role for remarkable individuals at the\nearly stages of innovation (Rex, 2004; Hess, 2004).\nThis can best be achieved by increasing linkages to universities with strong interdisciplinary\nscience-policy programs. These linkages will serve both to improve recruitment, but should also\nbe thought of as a co-learning process of model sharing, with current USGS employees learning\n\nwhat is on the cusp of new academic knowledge. This approach is used by businesses and\nagencies alike. For example, DuPont and MIT have a current $35 million, 5-year long term\nresearch and development partnership. Uniquely, this partnership \"encourages the formation of\nmultidisciplinary teams from the science, technology and engineering community, and the\nbusiness, management and policy arenas\" (DuPont, 2004). Agencies and universities can\nexchange knowledge and practices related to the common struggles to foster interdisciplinary\nresearch and innovation. USGS could establish itself as a mentor organization and willing\nemployer of students doing interdisciplinary work, as suggested by Sung et al. (2003). In some\ncases, USGS could create affiliations and temporary appointments with interested universities.\n(See Anna Brown's paper on the USGS as a Boundary Spanning organization to complement\nthis discussion of individuals as Boundary Spanners).\nTraining is one tool through which current employees, including research scientists conducting\ninquiry-driven projects, can learn new approaches to public involvement. The existing course on\nJoint Fact Finding, for example, is a useful start, but thus far only 60 employees throughout the\nentire agency have taken it (Karl, 2004). This training should be expanded to reach not just team\nleaders and supervisors, but elements of collaboration should be mainstreamed into the employee\ntraining and leadership training that all employees of USGS can take. Training on Joint Fact\nFinding and other collaboration and decision-making tools and approaches being developed\nunder Science Impact should be presented to employees in a coordinated manner. The barrier of\nfear and misinformation related to procedural issues as a federal employee is also an issues,\nparticularly as related to the Federal Advisory Committee Act and its interpretation through the\nSolicitor's office. This fear can lead employees to be risk averse and wary of innovative\ncollaboration models (Hess, 2004; Karl, 2004). Thus, trainings on FACA and the guidelines\nrelated to advocacy neutrality for USGS should be incorporated into courses on collaboration.\nFinally, education does not occur solely through the one-way transfer of information, rather it is\noften facilitated through a peer-learning process. Moving from \"listening sessions\" both one-on-\none with supervisors and in groups as in the Science Impact Dialogs, to decentralized innovation\ninformation sharing across localities and disciplines should be a priority for the agency. These\nsessions could feature keynotes/ presentations from successful Boundary Spanners within the\nUSGS both for others to learn from their experiences and as a way of giving positive feedback\nthrough a nontraditional mechanism to that individual/team. These should be informal settings\nfor sharing information allowing ample time for informal networking and small group\ndiscussion, perhaps even working with professional facilitators to extract key lessons learned.\nThey should be ongoing to encourage continued innovation. Mentoring programs and cross-\ndiscipline information sharing are techniques used in the business world to encourage innovation\nand collaboration across corporate divisions, which can have both physically distinct offices and\ninherent cultural differences (Webb, 2004). Though a certain amount of dialogue internal to the\nUSGS needs to occur, the USGS senior management should also convene exploratory\ndiscussions/workshops with stakeholders in areas that seem appropriate for collaborative\ndecision making, in order to try and catalyze the process.\n\nEvaluation and promotion\nMuch of the argument put forth on structural barriers to innovation is related to the overarching\nGS pay and promotion framework. This structure remains in place for all agencies, and though\nthere are clearly still cultural barriers to effectively using the RGE, it is clear that the RGE brings\nsome element of merit-based promotion into the federal government through peer review. Given\nthat the USGS must operate within OPM constraints related to the RGE, the agency is not\nattempting to radically revise its evaluation standards. Rather, the Office of Employee\nDevelopment is taking the approach of clarifying rules and educating employees, having just\nrevised the Research and Development Evaluation Process Handbook in November 2004. They\nare focusing particularly on new supervisors, as potential levers of change (Wallace, 2004).\nI recommend that the USGS work with supervisors not only to help them understand the basics\nof RGE and who should be evaluated how, but also to partner with them to strategically devise\nways of promoting boundary spanning and collaboration as a value in their unit. Supervisors'\ninvolvement in creation of position descriptions is one critical leverage point. The\nrecommendations from the USGS Biological Resources Division reveal current thinking on\nsupervisor and scientists roles in describing their own positions, with the following hypothetical\ndivision of labor related to defining the response to OPM's four factors:\nFigure 3: from (USGS, 2001)\nThis distribution places almost all of the responsibility for description of Factor IV\n\"Qualifications and Science Impact\" on the scientist himself, whereas I argue the supervisor\ncould play a critical screening role for existence and form of science impact at this stage.\nMoreover, I would argue that a spectrum of collaboration should be built into the role and the\nofficial job descriptions of all scientists to help reduce organizational stigma. For some\nscientists, that might involve simply more science translation and communication of their work,\nfor others it might mean involvement in public peer review panels, and for some it could include\nparticipation in Joint Fact Finding efforts. Goals for the agency as a whole need to be developed\nat the organizational level, but individual scientists, teams, and their supervisors can work to\ndevelop where, when, and how their original research is used by the public. Supervisors also\n\nfacilitate the review of individuals under \"mixed positions\" who spend some of their time\ndedicated to research and other time dedicated to other activities. The RGE allows for these\nresearch/practitioners to have just the research aspect of their work evaluated through the panel\nprocess (USGS, 2004). Supervisors should be aware of this option, and use it with greater\nfrequency.\nIt is worth continuing to explore where the flexibility in the RGE system might lie.\n- Currently, scientists select their primary peer group from a list of subject areas, and that\npeer group plus one other peer group create the panel, along with a personnel office\n\"observer\" (USGS, 2001). Interdisciplinary panels with a wider array of peer groups\ncould be encouraged to get a broader perspective on science impact. This would involve\na tradeoff in specificity between engaging with the details of the science and examining a\ncandidate with critical distance regarding impact.\n- Perhaps the peer groups themselves can be revised, as they include just one category for\nsocial science but more for each other discipline. Given the role of social science in\nintegrating scientific information with policy action, greater involvement of social\nscientists in panel review is a reasonable goal (Jacobs, 2001).\n- Additionally, I would argue that either the role of the personnel officer needs to be\nincreased or every panels need to include someone whose dedicated role it is to evaluate\nresearch for its social relevance/science impact. If there is no one whose specific\nresponsibility is evaluation of science impact, the culture of the group will likely remain\nthe same, since \"when serving on the panels, it is very difficult to think outside of the\ncontext of 'these articles count'\" (Karl, 2004).\n- It is recognized that evaluating science impact is not a simple task to be put off on one\npanel member without support, therefore I recommend the further development and\nimplementation of a comprehensive program of monitoring and evaluation on the\ncustomer/client side. Current customer reviews are used on a selective basis, but in order\nto be effective they ought to be used throughout the agency (Posson, 2004). This\ninformation would be invaluable in the RGE process for comparing different\ncollaboration and outreach practices.\n- Another relatively radical recommendation would be to alter the current titling scheme to\nde-emphasize the difference between researchers and practitioners, as is done in the\nAustralian Geological Survey, though Wallace noted that this is not a very viable scheme\ngiven OPM constraints.\n- Finally, given the existence of the Senior Scientist and Senior Executive Service ranks,\nperhaps the creation of a Senior Collaborative Service (or at minimum the recruitment of\nsome truly outstanding boundary spanners to the existing services should) be encouraged.\nA Senior Collaborator role would demonstrate that this is a growth area and help push the\nfrontiers of the agency.\nThere are a number of different strategies USGS can pursue to continue to make the RGE work\nbetter at pursuing the USGS core value of science impact.\nIn order to truly push the envelope on change, the USGS might consider an OPM demonstration\nproject with an alternative human resource management and evaluative structure. Agencies can\n\napply to conduct these sorts of projects to test models within their own organizational\nenvironment by waiving existing title 5 law and regulations. An OPM Demonstration Project\nTeam exists to work with agencies to devise new projects, particularly if they have a specific\nvision in mind (OPM, 2000a). For example, the USGS could pilot a team-evaluated approach\nthat engages in collaborative decision making processes as a core part of its work (involving both\nscientists and practitioners), and using performance-based pay to the extent possible. This would\nbe one experimental way to test innovative business models and hopefully to make a compelling\ncase to the established USGS scientists on the value of an alternative evaluation and reward\nstructure.\nFinally, further research specific to the USGS needs to be conducted to fully understand barriers,\nparticularly at the cultural level. Perceptions of panelists in the RGE and variations between the\ndisciplines both require more in depth study. Collaborative research that involves scientists\nthemselves in exploring incentives and barriers to collaboration would also be useful and\nconsistent with the value of collaboration.\n\nWorks Cited\nArnstein, Sherry. \"A Ladder of Citizen Participation.\" American Institute of Planners Journal.\nJuly 1969. 35.4:216-224.\nBaron, James. \"Organizational Perspectives on Stratification.\" Annual Review of Sociology.\n1984.10:37-69.\nBarrington, Kathy. Lecture to \"The Role of Joint Fact Finding in Environmental Decision-\nmaking\", Massachusetts Institute of Technology. Cambridge, MA. 14 Oct. 2004.\nBrewer, Gene, Selden, Sally, and Facer, Rex. \"Individual Conceptions of Public Service\nMotivation.\" Public Administration Review. May/June 2000. 60.3:254-264.\nCash, D., Clark, W., Alcock, F., Dickson, N., Eckley, N., Guston, D., Jager, J. and Mitchel, R. in\npress, \"Knowledge Systems for Sustainable Development\" Proceedings of the National\nAcademy of Science. 8 July 2003. 100.14:8086-8091.\nChubin, Daryl and Maienschein, Jane. \"Staffing science policy-making.\" Science. 24 Nov.\n2000. 290.5486:1501.\nClark, Roger and Meidinger, Errol. \"Integrating Science and Policy in Natural Resource\nManagement: Lessons and Opportunities from North America.\" USDA Forest Service, Pacific\nNorthwest Research Station, General Technical Report. September 1998.\nDuPont. \"DuPont and MIT Team in Research & Development Partnership.\" 2004.\n<http://www1.dupont.com/NASApp/dupontglobal/corp/index.jsp?page=/content/US/en_US/scie\nnce/rd/collaboration/index.html> (11 Dec 2004).\nGroat, Chip. Lecture to \"The Role of Joint Fact Finding in Environmental Decision-making\",\nMassachusetts Institute of Technology. Cambridge, MA. 14 Oct. 2004.\nGuston, David. Between Politics and Science: Assuring the Integrity and Productivity of\nResearch. Cambridge, UK: Cambridge University Press, 2000.\nHenderson, Albert. \"Undermining Peer Review.\" Society. Jan/Feb 2001. 38.2:47-55.\nHess, Karl. Lecture to \"The Role of Joint Fact Finding in Environmental Decision-making\",\nMassachusetts Institute of Technology. Cambridge, MA.\nInteragency Advisory Group Committee on Performance Management and Recognition.\n\"Evaluating Team Performance.\" 31 Aug. 1993.\n<http://www.opm.gov/perform/wppdf/teameval.pdf > (1 Dec 2004).\nJacobs, Katharine. \"Connecting Science, Policy, and Decision-making: A Handbook for\nResearchers and Science Agencies.\" 2001.\n\nJasanoff, Sheila. \"The Dilemma of Environmental Democracy.\" Issues in Science and\nTechnology. Fall 1996. 13.1:63-71.\nKarl, Herman. Lecture to \"The Role of Joint Fact Finding in Environmental Decision-making\",\nMassachusetts Institute of Technology. Cambridge, MA.\nKarlqvist, Anders. \"Going Beyond Discipline.\" Policy Sciences. Dec 1999. 32.4:379.\nKostoff, Ronald. \"Overcoming Specialization.\" Bioscience. Oct. 2002. 52.10:938-942.\nLattuca, Lisa. \"Learning Interdisciplinarity: Sociocultural Perspectives on Academic Work.\"\nThe Journal of Higher Education. Nov/Dec 2002. 73.6:711-740.\nLeavitt, Mike. \"Mike Leavitt on Environmental Stewardship.\" 23 July 2004.\n<http://www.epa.gov/adminweb/leavitt/enlibra.htm> (16 Sept. 2004).\nLubchenco, Jane. \"Entering the Century of the Environment: A New Social Contract for\nScience.\" Science. 23 Jan 1998. 279.5350:491-497.\nMcKenzie, Richard and Lee, Dwight. Managing Through Incentives: How to Develop a More\nCollaborative, Productive, and Profitable Organization. New York: Oxford University Press,\n1998.\nNational Research Council. Improving the Recruitment, Retention, and Utilization of Federal\nScientists and Engineers. Washington, D.C.: National Academy Press, 1993.\nPeyser, Jen. \"Joint Fact Finding for Public Involvement in Wind-Permitting Decisions: Beyond\nNEPA.\" Cambridge, MA: unpublished paper, 2003.\nPosson, Doug. Lecture to \"The Role of Joint Fact Finding in Environmental Decision-making\",\nMassachusetts Institute of Technology. Cambridge, MA. 26 Oct. 2004.\nPowell, Mark. Science at EPA: Information in the Regulatory Process. Washington, D.C.:\nResources for the Future, 1999.\nProcter and Gamble. \"R&D's Formula for Success.\" 2004.\n<http://www.pg.com/science/rd_formula_success.jhtml> (7 Dec. 2004).\nProvine, Bill. Phone interview. 17 December 2004.\nRacher, Chris. Lecture to \"The Role of Joint Fact Finding in Environmental Decision-making\",\nMassachusetts Institute of Technology. Cambridge, MA. 16 Sept 2004.\nRex, Andrea. Lecture to \"The Role of Joint Fact Finding in Environmental Decision-making\",\nMassachusetts Institute of Technology. Cambridge, MA. 23 Sept. 2004.\n\nSarewitz, Daniel. \"Social Change and Science Policy.\" Issues in Science and Technology.\nSummer 1997. 13.4:29-33.\nSung, Nancy, Gordon, Jeffery, Rose, George, Getzoff, Elizabeth, et al. \"Educating Future\nScientists.\" Science. 12 Sept 2003. 301.5639:1485.\nSusskind, Larry. Lecture to \"The Role of Joint Fact Finding in Environmental Decision-\nmaking\", Massachusetts Institute of Technology. Cambridge, MA. 28 Sept. 2004.\nUnited States Department of Interior. \"Strengthening Citizen Stewardship and Cooperation.\" 1\nSept. 2004. < http://www.doi.gov/initiatives/conservation.html> (11 Dec. 2004).\n---. \"Science Plan in Support of Ecosystem Restoration, Preservation, and Protection in South\nFlorida.\" May 2004. <http://sofia.usgs.gov/publications/reports/doi-science-plan/DOI-\nSCIENCE-PLAN-04.pdf> (17 Sept. 2004).\nUnited States Geological Survey. \"Dialog on Science Impact.\" Discussion. 31 October 2003.\nTranscript prepared by Steven Lenard.\n---. \"Research and Development Evaluation Process Handbook.\" November 2004.\n---. \"Science Impact: Enhancing the Use of USGS Science.\" 4 April 2002.\n<http://www.ksg.harvard.edu/sed> (12 Oct 2004).\nUnited States Geological Survey Biological Resources Division. \"Research Grade Evaluation\nPanels.\" 16 April 2001. <http://biology.usgs.gov/intranet/science/Handbook.htm> (18 Nov\n2001).\nUnited States Office of Personnel Management. \"A Fresh Start for Federal Pay: the Case for\nModernization.\" April 2002. < http://www.opm.gov/strategiccomp/whtpaper.pdf> (1 Dec 2004).\n---. \"Human Resource Innovators' Tool Kit.\" 2000.\n<http://www.opm.gov/demos/HR_toolkit.pdf> (1 Dec 2004).\n---. \"Follow-up Report of Special Study of Incentive Awards.\" March 2000.\n<http://www.opm.gov/studies/incent00.pdf> (1 Dec 2004).\n---. \"Classification Appeal Decision Under section 5112 of title 5, United States Code.\" 30 Oct.\n2000. <http://www.opm.gov/classapp/decision/2000/13151401.pdf > (1 Dec 2004).\nVan de Klerkhof, Marleene. Lecture to \"The Role of Joint Fact Finding in Environmental\nDecision-making\", Massachusetts Institute of Technology. Cambridge, MA.\nWallace, Laure. Phone Interview. 3 December 2004.\n\nWebb, Ron. Phone Interview. 16 December 2004.\nWhitley, Rich. Lecture to \"The Role of Joint Fact Finding in Environmental Decision-making\",\nMassachusetts Institute of Technology. Cambridge, MA. 5 Oct. 2004.\nWollondeck, J.M. and Yaffee, S.L. Making Collaboration Work: Lessons from Innovation in\nNatural Resources Management. Washington, D.C.: Island Press, 2000."
    }
  ]
}