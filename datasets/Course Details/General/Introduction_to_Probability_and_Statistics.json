{
  "course_name": "Introduction to Probability and Statistics",
  "course_description": "This course provides an elementary introduction to probability and statistics with applications. Topics include basic combinatorics, random variables, probability distributions, Bayesian inference, hypothesis testing, confidence intervals, and linear regression.\nThese same course materials, including interactive components (online reading questions and problem checkers) are available on MIT’s Open Learning Library, which is free to use. You have the option to enroll and track your progress, or you can view and use the materials without enrolling.",
  "topics": [
    "Mathematics",
    "Discrete Mathematics",
    "Probability and Statistics",
    "Mathematics",
    "Discrete Mathematics",
    "Probability and Statistics"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 80 minutes / session\n\nStudios: 1 session / week, 50 minutes / session\n\nPrerequisites\n\n18.02 Multivariable Calculus\n\nCourse Description\n\nThis course provides an elementary introduction to probability and statistics with applications. Topics include basic combinatorics, random variables, probability distributions, Bayesian inference, hypothesis testing, confidence intervals, and linear regression.\n\nBroad Course Goals\n\nLearn the language and core concepts of probability theory.\n\nUnderstand basic principles of statistical inference (both Bayesian and frequentist).\n\nBuild a starter statistical toolbox with appreciation for both the utility and limitations of these techniques.\n\nUse software and simulation to do statistics (R).\n\nBecome an informed consumer of statistical information.\n\nPrepare for further coursework or on-the-job study.\n\nCourse Arc\n\nProbability (uncertain world, perfect knowledge of the uncertainty)\n\nCounting\n\nRandom variables, distributions, quantiles, mean variance\n\nConditional probability, Bayes' theorem, base rate fallacy\n\nJoint distributions, covariance, correlation, independence\n\nCentral limit theorem\n\nStatistics I: pure applied probability (data in an uncertain world, perfect knowledge of the uncertainty)\n\nBayesian inference with known priors, probability intervals\n\nConjugate priors\n\nStatistics II: applied probability (data in an uncertain world, imperfect knowledge of the uncertainty)\n\nBayesian inference with unknown priors\n\nFrequentist significance tests and confidence intervals\n\nResampling methods: bootstrapping\n\nLinear regression\n\nComputation, simulation, and visualization using R and applets will be used throughout the course.\n\nSpecific Learning Objectives\n\nProbability\n\nStudents completing the course will\n\nbe able to use basic counting techniques (multiplication rule, combinations, permutations) to compute probability and odds;\n\nbe able to use R to run basic simulations of probabilistic scenarios;\n\nbe able to compute conditional probabilities directly and using Bayes' theorem, and check for independence of events;\n\nbe able to set up and work with discrete random variables; in particular, to understand the Bernoulli, binomial, geometric, and Poisson distributions;\n\nbe able to work with continuous random variables. In particular, know the properties of uniform, normal, and exponential distributions;\n\nknow what expectation and variance mean and be able to compute them;\n\nunderstand the law of large numbers and the central limit theorem;\n\nbe able to compute the covariance and correlation between jointly distributed variables; and\n\nbe able to use available resources (the internet or books) to learn about and use other distributions as they arise.\n\nStatistics\n\nStudents completing the course will\n\nbe able to create and interpret scatter plots and histograms;\n\nunderstand the difference between probability and likelihood functions and be able to find the maximum likelihood estimate for a model parameter;\n\nbe able to do Bayesian updating with discrete priors to compute posterior distributions and posterior odds;\n\nbe able to do Bayesian updating with continuous priors;\n\nbe able to construct estimates and predictions using the posterior distribution;\n\nbe able to find credible intervals for parameter estimates;\n\nbe able to use null hypothesis significance testing (NHST) to test the significance of results and to understand and be able to compute the p-value for these tests\n\nbe able to use specific significance tests, including z-test, t-test (one- and two-sample), and chi-squared test;\n\nbe able to find confidence intervals for parameter estimates;\n\nbe able to use bootstrapping to estimate confidence intervals;\n\nbe able to compute and interpret simple linear regression between two variables; and\n\nbe able to set up a least squares fit of data to a model.\n\nBasic Structure of the Course\n\nWe will take an active learning approach similar in some respects to Technology Enabled Active Learning (TEAL).\n\nYou must do the reading and answer reading questions before each class.\n\nIn class we will assume you have done the reading. We will lecture and work problems assuming this.\n\nWe do not expect that you will have mastered the material on first reading. The goal is to start the process, so class will be more productive.\n\nThe reading questions will prepare you for the harder questions we will work during class and on the problem sets.\n\nThe reading questions will count toward your grade.\n\nWe meet three times a week.\n\nTuesday and Thursday will be a blend of lecture, concept questions and group problem solving.\n\nFriday will be a \"studio\" day. It will involve longer problems and the use of R. You will need to have your computer available on Fridays.\n\nWe will use \"clicker questions\" in class.\n\nParticipation on these questions will also count toward your grade.\n\nR\n\nWe will make frequent use of R for computation, simulation, and visualization.\n\nWe will teach you everything you need to know to use R as a tool in this class.\n\nYou will not be expected to do any hardcore computer programming.\n\nNote to OCW Users\n\nThe interactive components of this course - the Online Reading Questions and Problem Checkers - are available on MIT's\nOpen Learning Library\n, which is free to use. You have the option to sign up and track your progress, or you can view and use the materials without enrolling.\n\nTextbook\n\nThere will be no assigned textbook for the class. will be assigned.\n\nProblem Sets and Exams\n\nProblem sets\n\nProblem sets will be due most weeks, usually on Monday.\n\nProblem sets will have a problem checker on the\nOpen Learning Library\n(OLL). This will allow you to check many of your answers before turning in the problem sets.\n\nExams\n\nThere will be two in-class midterm exams and a comprehensive final exam.\n\nThe midterms will be designed to take one hour, but you will have the entire 80 minutes of class to finish.\n\nWe will have one R-based quiz. For this quiz, you will be allowed to use the internet in any way except to communicate with other people.\n\nGroups\n\nFor in-class problem solving you will work in groups of 3.\n\nYou will be able to choose your own group.\n\nAfter a week or so, groups will be more or less permanent.\n\nGroups should sit together at tables.\n\nCollaboration\n\nMIT has a culture of teamwork. We encourage you to work with study partners.\n\nCollaboration on homework is encouraged.\n\nYou must write your solutions yourself, in your own words.\n\nYou must list all collaborators and outside sources of information.\n\nGrading\n\nReading questions and in-class clicker questions will each count for 5% of your grade.\n\nProblem sets will count for 25% of your grade. In computing your problem set average we will drop your lowest score.\n\nThe R studios will count 5% of your grade.\n\nThe midterm exams and R quiz combined will count for 30% of your grade (12.5%, 12.5%, 5%).\n\nThe final exam will count for 30% of your grade.",
  "files": [
    {
      "category": "Resource",
      "title": "R Studio 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_studio1-instructions.pdf",
      "content": "Studio 1 Birthday Matches\n18.05, Spring 2022\nhttps://xkcd.com/523\nImage courtesy of xkcd. License: CC BY-NC.\nOverview of the studio\nSuppose n people are gathered in a room. What is the probability that at least 2 of them\nwill have the same birthday?\n- We will use an R simulation to estimate this for various n.\n- Our code will work for years of any given length. So Martians or Jovians could use\nour code.\n- We will explore how the number of trials affect the variability of our estimates.\nDownload the zip file\n- You should have downloaded the file mit18_05_s22_studio1.zip from our MITx site.\n- Create an 18.05 studio folder somewhere on your computer and unzip this file there.\nYou should see the following R files:\nmit18_05_s22_studio1.r\nmit18_05_s22_studio1-start-here.r\nmit18_05_s22_studio1-start-here-include.r\nmit18_05_s22_studio1-samplecode.r\nmit18_05_s22_studio1-test.r\nmit18_05_s22_colMatches.r\n\nStudio 1 instructions, Spring 2022\nand the following other files:\nmit18_05_s22_Hints-RStudio.pdf\nmit18_05_s22_studio1-test-answers.html\nGeneral R instructions\nIf you've never used R Studio, take a minute to read the Hints-RStudio file in this packet.\nIt is less than a page.\nIf in doubt: ask one of the staff!\nWorking on the assignment\n- Detailed instructions on each problem are below.\n- Do your work in the file mit18_05_s22_studio1.r. The structure of the file should\nbe pretty clear. (We hope!)\n- Do not ignore variables or print and cat statements we've included. They are meant\nto be used.\n- For each problem, you will be putting your code inside a wrapper function. Look\ncarefully at the arguments of that function. You must use them as described for each\nproblem.\n- The test cases give these arguments certain values for you to check your code.\n- When we grade your code, we will use other values. So if you don't use the wrapper\nfunction arguments, we will not see correct results\nGeneral administrative instructions\n1. Put your code answering the R questions in the file mit18_05_s22_studio1.r\n2. Read all instructions carefully!\n3. If code is given with a question it is meant to be used. Don't just ignore it. Likewise for\nprint or cat statements.\n4. This is really important: Before uploading the the code, read the sections (below)\ncalled Before uploading your code and Upload your code\n5. Due date\nThe goal is to upload your work by the end of class.\nIf you need extra time, you can upload your work any time before 10 PM ET the day after\nthe studio.\n6. Solutions uploaded\nSolution code will be posted on MITx at 10 PM the day after the studio.\nPrepping R Studio\n- Open R-studio\n\nStudio 1 instructions, Spring 2022\n- From the File menu find and open mit18_05_s22_studio1-start-here.r\n- From the Session menu, choose 'Set Working Directory' and pick 'To Source File\nLocation'. This will let R find the files it needs.\n- From the Code menu choose Source -note the keystroke equivalent. You should get\nthe following message\nCongratulations: your working directory is set correctly!\nand see the following plot\n-2\n-2\nθ\nIf you get an error message beginning\nError in file(filename, \"r\", encoding = encoding) :\ncannot open the connection\nit means you didn't set your working directory correctly. See the Hints-RStudio file\nfor how to do that.\n- From the File menu open mit18_05_s22_studio1.r. This is where you will work.\n- Answer the questions in the detailed instructions just below.\n- Solution code will be posted tomorrow at 10 pm\nDetailed instructions for the studio\nBefore you get started and certainly before you upload your code, read the 'General ad\nministrative instructions' and 'General R instructions' sections above.\nProblem 1. Use sections 1-5 of mit18_05_s22_studio1-samplecode.r as a tutorial to\nlearn a little more about R and to find useful code for the later problems.\nProblem 2a. This asks you to code a simulation of the birthday problem.\n\nStudio 1 instructions, Spring 2022\n- You will put your code inside the function studio1_problem_2a(). The skeleton for\nthis function is provided.\n- The arguments are:\nndays_in_year -This is the number of days in the year.\nnpeople -This is the number of people in the room.\nntrials -This is the number of times you run the simulation. Once the code is\nworking, we will want this to be large enough to get an accurate estimate.\nYour code should do the following\n1. Randomly pick npeople birthdays (repetitions allowed) from a list of ndays_in_year\nbirthdays.\n2. Check if any 2 of the chosen birthdays are the same.\n3. Repeat the previous two steps ntrials times and keep count of the number of trials\nin which at least one match is found.\n4. Report the fraction of times at least one match was found.\nAfter sourcing your code, look at studio1-test-answers.html. Find the function that was\nused to test problem 2a. Paste it into your console and run it. Compare your results to\nthose shown.\nProblem 2b. For this problem, take ndays_in_year= 365 and ntrials large enough to\nget a good simulation (say 2000 or more)\nThen, run the function studio1_problem_2a() with various values of npeople until you\nfigure out the smallest number of people it takes to have at least a 0.5 probability of a\nmatch.\nAll the function studio1_problem_2b() does is print out the value of npeople. You don't\nneed to write any code. Just set the value of npeople to the number you found.\nExtra. Set ndays_in_year= 365 and npeople = 15. Now run studio1_problem_2a()\nseveral times with ntrials = 100. Notice how the estimated probability varies. Repeat\nthis with ntrials = 500, 1000, 2000, 50.\nProblem 3. (Optional) Do this only if you have time. Use sections 6 and 7 in\nmit18_05_s22_studio1-samplecode.r to learn how to make a plot of the probability of\na shared birthday as a function of npeople. Set ndays_in_year= 365 and do a plot of\nestimated probability vs. npeople for npeople = 1 to 100.\nAfter sourcing your code, look at mit18_05_s22_studio1-test-answers.html. Copy the\nfunction shown there that was used to test problem 3 into your console and run it. Compare\nyour results to those shown.\nTesting your code\nFor each problem, we ran the problem function with certain parameters. You can see\nthe function call and the output in mit18_05_s22_studio1-test-answers.html. If you\n\nStudio 1 instructions, Spring 2022\ncall the same function with the same parameters, you should get the same results as in\nmit18_05_s22_studio1-test-answers.html - if there is randomness involved the answers\nshould be close but not identical.\nFor your convenience, the file mit18_05_s22_studio1-test.r contains all the function calls\nused to make mit18_05_s22_studio1-test-answers.html.\nBefore uploading your code\n1. Make sure all your code is in mit18_05_s22_studio1.r. Also make sure it is all inside\nthe functions for the problems.\n2. Clean the environment and plots window.\n3. Source (run) the entire file.\n4. Call each of the problem functions with the same parameters as the test file\nmit18_05_s22_studio1-test-answers.html.\n5. Make sure it runs without error and outputs just the answers asked for in the questions.\n6. Compare the output to the answers given in mit18_05_s22_studio1-test-answers.html.\nUpload your code\nUse the upload link on our MITx site to upload your code for grading.\nLeave the file name as mit18_05_s22_studio1.r. (The upload script will automatically\nadd your name and a timestamp to the file.)\nYou can upload more than once. We will grade the last file you upload.\nDue date\nDue date: The goal is to upload your work by the end of class.\nIf you need extra time, you can upload your work any time before 10 PM ET the day after\nthe studio.\nSolutions uploaded: Solution code will be posted on MITx at 10 PM the day after the\nstudio.\nHere's Johnny\nThe Tonight show in the 1970s and 80s was hosted by Johnny Carson. Here is his attempt\nto deal with the birthday question. Fashion back then was interesting.\nJohnny Carson attempt 1\nhttps://www.cornell.edu/video/the-tonight-show-with-johnny-carson-feb-6-1980-excerpt\nAttempt 2: after getting hate mail from mathematicians\n\nStudio 1 instructions, Spring 2022\nhttps://www.cornell.edu/video/the-tonight-show-with-johnny-carson-feb-7-1980-excerpt\nAttempt 3:\nhttps://www.cornell.edu/video/the-tonight-show-with-johnny-carson-feb-8-1980-excerpt\nHere is the full NY Times article\nhttps://opinionator.blogs.nytimes.com/2012/10/01/its-my-birthday-too-yeah/\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "R Studio 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_studio2-instructions.pdf",
      "content": "Studio 2 Binomial distributions; Derangements\n18.05, Spring 2022\nOverview of the studio\nThis studio works with the binomial distributions Binomial(n, p). It does both exact com\nputations and simulations.\nR introduced in this studio\nThe R needed is all introduced in mit18_05_s22_studio2-samplecode.r. We will use\n1. Functions for the binomial distribution rbinom(), dbinom(), pbinom().\nIn R terminology the r in rbinom stands for random, i.e. rbinom() generates random samples\nfrom the binomial distribution. dbinom() computes the pmf and pbinom() computes the\ncdf.\n2. choose(n,k) function (choose k things out of n).\n3. plot() for making plots\n4. We will also do arithmetic with lists. For exaple, if x is a list then x^2 squares every\nelement in the list.\n5. Finally, the sample code shows how to write code for a 'for loop'.\nDownload the zip file\n- You should have downloaded the file mit18_05_s22_studio2.zip from our MITx site.\n- Unzip it in your 18.05 studio folder.\n- You should see the following R files\nmit18_05_s22_studio2.r\nmit18_05_s22_studio2-samplecode.r\nmit18_05_s22_studio2-test.r\n\nStudio 2 instructions, Spring 2022\nand the following other files\nmit18_05_s22_studio2-test-answers.html\nPrepping R Studio\n- In R studio, open mit18_05_s22_studio2-samplecode.r and mit18_05_s22_studio2.r\n- Using the Session menu, set the working directory to source file location. (This is a\ngood habit to develop!)\n- Answer the questions in the detailed instructions just below. Your answers should be\nput in mit18_05_s22_studio2.r\n- Solution code will be posted tomorrow at 10 pm\nDetailed instructions for the studio\n- Go through mit18_05_s22_studio2-samplecode.r as a tutorial. In that file, it will\ntell you when you've done enough to code one of the problems here.\nProblem 1. This problem is on the random variable Y ∼ Binomial(n, p). We will view Y\nas giving the number of heads in n tosses of a coin with probability p of heads.\nProblem 1a. Here you will finish the code for the function\nstudio2_problem_1a(ntosses, phead, k, ntrials)\nThis function should do a simulation to estimate P(Y = k) and P(Y ≤ k).\nThe arguments to this function are:\nntosses = n = the number of tosses of the coin\npheads = p = the probability of heads\nk = the value of Y we are interested in\nntrials = number of trials to run in the simulation\nYou should use the built in R function rbinom() to generate random values for the simu\nlation.\nThe output will be the estimates of the two probabilities. The lines printing the output are\nalready in the code.\nProblem 1b. Here you will finish the code for the function\nstudio2_problem_1b(ntosses, phead, k)\nThis function should compute the exact probability for P(Y = k).\nThe arguments to this function are:\nntosses = n = the number of tosses of the coin\npheads = p = the probability of heads\nk = the value of Y we are interested in\n\nStudio 2 instructions, Spring 2022\nYou should use the built in R function choose() in your computation. That is, implement\nthe formula for the pmf. (Even though it's available, do not use the dbinom function. We\nwant you to get practice with the formula.)\nThe output will be the values of the two probabilities. The lines printing the output are\nalready in the code.\nProblem 2. This problem is based on a gambling game. A coin is tossed 10 times and the\npayoff to the player is based on the number of heads. The payoff function for k heads is\n$(k2 - 7k).\nFor example, if k = 1 the player loses $6 and if k = 10, they win $30.\nProblem 2a. Here you will finish the code for the function\nstudio2_problem_2a()\nThis function should plot the payoff function k2 - 7k vs. k. Since the game has 10 tosses,\nk should go from 0 to 10.\nThe function has no arguments. The code defines ntosses = 10 for you.\nThe sample code will show you how to make simple plots.\nProblem 2b. Here you will finish the code for the function\nstudio2_problem_2b()\nWe give you that phead = 0.6, and ntosses = 10. Your job is to decide whether or not the\ngame is a good bet. To do this, you need to decide what to compute and then compute it\nexactly. Use your results to declare the game a good or bad bet.\nFor computing binomial probabilities you can use the function dbinom() which is described\nin mit18_05_s22_studio2-samplecode.r.\nBe sure to put the exact value you compute in the variable exact_value\nThe output say what you computed (use an English sentence for this), its value and whether\nor not the game is a good bet. The lines printing the output are already in the code.\nProblem 2c. Here you will finish the code for the function\nstudio2_problem_2c(ntrials)\nIn this part you will simulate playing the game ntrials times and compute the simulated\naverage payoff.\nThe arguments to studio2_problem_2c(ntrials) function are:\nntrials = the number of times the simulation should play the game.\nAs before, we give you that phead = 0.6, and ntosses = 10. You should use rbinom() to\ngenerate random values\nThe output will give the simulated average payoff. The lines printing the output are already\nin the code.\nProblem 3. (Optional) This problem is about derangements, i.e. a permutation of n\nobjects such that none of the objects ends up in its original position.\n\nStudio 2 instructions, Spring 2022\nYou will finish the code for the function\nstudio2_problem_3(n, ntrials)\nThe arguments to this function are:\nn = the number of objects.\nntrials = the number of times the simulation should permute the objects.\nYou should use the sample() function to permute the numbers 1 to n and count the number\nof times the permutation is a derangement.\nThe output is the fraction of trials that produced derangements. The lines printing the\noutput are already in the code.\nTesting your code\nFor each problem, we ran the problem function with certain parameters. You can see\nthe function call and the output in mit18_05_s22_studio2-test-answers.html. If you\ncall the same function with the same parameters, you should get the same results as in\nmit18_05_s22_studio2-test-answers.html - if there is randomness involved the answers\nshould be close but not identical.\nFor your convenience, the file mit18_05_s22_studio2-test.r contains all the function calls\nused to make mit18_05_s22_studio2-test-answers.html.\nBefore uploading your code\n1. Make sure all your code is in mit18_05_s22_studio2.r. Also make sure it is all inside\nthe functions for the problems.\n2. Clean the environment and plots window.\n3. Source the file.\n4. Call each of the problem functions with the same parameters as the test file\nmit18_05_s22_studio2-test-answers.html.\n5. Make sure it runs without error and outputs just the answers asked for in the questions.\n6. Compare the output to the answers given in mit18_05_s22_studio2-test-answers.html.\nUpload your code\nUse the upload link on our MITx site to upload your code for grading.\nLeave the file name as mit18_05_s22_studio2.r. (The upload script will automatically\nadd your name and a timestamp to the file.)\nYou can upload more than once. We will grade the last file you upload.\n\nStudio 2 instructions, Spring 2022\nDue date\nDue date: The goal is to upload your work by the end of class.\nIf you need extra time, you can upload your work any time before 10 PM ET the day after\nthe studio.\nSolutions uploaded: Solution code will be posted on MITx at 10 PM the day after the\nstudio.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "R Studio 3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_studio3-instructions.pdf",
      "content": "Studio 3 Histograms\n18.05, Spring 2022\nHistogram of aveData\naverage data 64\nDensity\n0.6\n0.8\n1.0\n1.2\n1.4\n0.0\n1.0\n2.0\n3.0\nOverview of the studio\nThis studio is about making histograms of data. We will use the function hist() to do\nmost of the work. When we expect the histogram to match a known pdf, we will add a\ngraph of the pdf on top.\nR introduced in this studio\nThe R needed is all introduced in mit18_05_s22_studio3-samplecode.r. We will use\n1. Pseudo-random generators for various distributions: rnorm(), rexp(), runif().\n2. hist(...) for making histograms\n3. lines() for adding graphs to existing plots\nDownload the zip file\n- You should have downloaded the file mit18_05_s22_studio3.zip from our MITx site.\n- Unzip it in your 18.05 studio folder.\n\nStudio 3 instructions, Spring 2022\n- You should see the following R files\nmit18_05_s22_studio3.r\nmit18_05_s22_studio3-samplecode.r\nmit18_05_s22_studio3-test.r\nand the following other files\nmit18_05_s22_studio3-test-answers.html\nPrepping R Studio\n- In R studio, open mit18_05_s22_studio3-samplecode.r and mit18_05_s22_studio3.r\n- Using the Session menu, set the working directory to source file location. (This is a\ngood habit to develop!)\n- Answer the questions in the detailed instructions just below. Your answers should be\nput in mit18_05_s22_studio3.r\n- Solution code will be posted tomorrow at 10 pm\nDetailed instructions for the studio\n- Go through mit18_05_s22_studio3-samplecode.r as a tutorial. It is relatively short.\nProblem 1. Frequency and density histograms.\nAfter doing this problem, look at the frequency and density histograms. Note especially,\nthe scales on the vertical axis.\nProblem 1a. Here you will finish the code for the function\nstudio3_problem_1a(rate, nsamples)\nThis function should draw a frequency histogram of (simulated) exponential data.\nThe arguments to this function are:\nrate = rate parameter for an exponential distribution\nnsamples = the size of the sample to use for the histogram\nThe function should do the following.\n1. Generate nsamples from an exponential distribution with the given rate parameter.\n2. Draw a frequency histogram of the (simulated) data.\nFollow the sample code in explicitly setting up the bins for the histogram. We set bin_width\nfor you in our part of the code.\nYou should use the built in R function rexp() to generate the random sample.\nProblem 1b. Here you will finish the code for the function\nstudio3_problem_1b(rate, nsamples)\nThis is very similar to 1a.\n\nStudio 3 instructions, Spring 2022\nThe arguments to this function are:\nrate = rate parameter for an exponential distribution\nnsamples = the size of the exponential sample\nAs in 1(a), we define bin_width for you in our part of the code.\nThe function should do the following.\n1. Generate nsamples from an exponential distribution with the given rate parameter.\n2. Draw a density histogram of the (simulated) data.\n3. Add a graph of the probability density (pdf) of the underlying exponential distribution\nto the plot.\nProblem 2. In this problem we will draw density histograms of the average of many\nexponential samples. The goal is to see how the density of the averages changes as the\nnumber averaged increases. We will see graphically how it approaches a normal distribution\nas the number of terms in the average increases.\nProblem 2a. Here you will finish the code for the function\nstudio3_problem_2a(rate, nsamples)\nThe arguments to this function are:\nrate = rate parameter for an exponential distribution\nnsamples = the size of the exponential sample\nAs before, we define bin_width for you in our part of the code.\nThe function should do the following.\n1. Simulate the average of two independent samples of size nsamples from an exponential\ndistribution with the given rate parameter. That is, it should simulate sampling from a\nX1 + X2\nrandom variable Y =\n, where each X1, X2 is an exponential random variable.\n2.Draw a density histogram of the (simulated) average.\nProblem 2b. Here you will finish the code for the function\nstudio3_problem_2b(rate, nsamples, n_to_average, bin_width)\nThe arguments to this function are:\nrate = rate parameter for an exponential distribution\nnsamples = the size of the exponential sample\nn_to_average = the number of independent random variables to average\nbin_width = the bin width used in the histogram\nNote, in this function bin_width is specified by the user.\nThe function should do the following.\n1. Draw a density histogram of the (simulated) average of n_to_average independent\nexponential samples. That is, it should simulate sampling from a random variable\nX1 + X2 + ... + Xn\nY=\n,\nn\nwhere each Xj is an exponential random variables with the given rate parameter\n\nStudio 3 instructions, Spring 2022\n2. The mean and variance of the average are given in our part of the code. Use R to add\nthe graph of a normal density with this mean and variance to your plot. The graph should\nextend from 4 standard deviations below the mean to 4 above.\nBe careful, in R the arguments to dnorm are mean and standard deviation.\nTesting your code\nFor each problem, we ran the problem function with certain parameters. You can see\nthe function call and the output in mit18_05_s22_studio3-test-answers.html. If you\ncall the same function with the same parameters, you should get the same results as in\nmit18_05_s22_studio3-test-answers.html - if there is randomness involved the answers\nshould be close but not identical.\nFor your convenience, the file mit18_05_s22_studio3-test.r contains all the function calls\nused to make mit18_05_s22_studio3-test-answers.html.\nBefore uploading your code\n1. Make sure all your code is in mit18_05_s22_studio3.r. Also make sure it is all inside\nthe functions for the problems.\n2. Clean the environment and plots window.\n3. Source the file.\n4. Call each of the problem functions with the same parameters as the test file\nmit18_05_s22_studio3-test-answers.html.\n5. Make sure it runs without error and outputs just the answers asked for in the questions.\n6. Compare the output to the answers given in mit18_05_s22_studio3-test-answers.html.\nUpload your code\nUse the upload link on our MITx site to upload your code for grading.\nLeave the file name as mit18_05_s22_studio3.r. (The upload script will automatically\nadd your name and a timestamp to the file.)\nYou can upload more than once. We will grade the last file you upload.\nDue date\nDue date: The goal is to upload your work by the end of class.\nIf you need extra time, you can upload your work any time before 10 PM ET the day after\nthe studio.\nSolutions uploaded: Solution code will be posted on MITx at 10 PM the day after the\nstudio.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "R Studio 4",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_studio4-instructions.pdf",
      "content": "Studio 4 Covariance and correlation\n18.05, Spring 2022\nThis image is in the public domain.\nOverview of the studio\nThis studio explores covariance and correlation via simulation.\nR introduced in this studio\nThe R needed is introduced in mit18_05_s22_studio4-samplecode.r. We will use\nfunctions var(), cov(), cor(), sample(), hist()\nDownload the zip file\n- You should have downloaded the file mit18_05_s22_studio4.zip from our MITx site.\n- Unzip it in your 18.05 studio folder.\n- You should see the following R files\nmit18_05_s22_studio4.r\nmit18_05_s22_studio4-samplecode.r\nmit18_05_s22_studio4-test.r\nand the following other files\nmit18_05_s22_studio4-test-answers.html\nPrepping R Studio\n- In R studio, open mit18_05_s22_studio4-samplecode.r and mit18_05_s22_studio4.r\n\nStudio 4 instructions, Spring 2022\n- Using the Session menu, set the working directory to source file location. (This is a\ngood habit to develop!)\n- Answer the questions in the detailed instructions just below. Your answers should be\nput in mit18_05_s22_studio4.r\n- Solution code will be posted tomorrow at 10 pm\nDetailed instructions for the studio\n- Go through mit18_05_s22_studio4-samplecode.r as a tutorial.\nPay special attention to the sections on var(), cov(), cor() and histograms. The section\non sampling from an arbitrary discrete distribution will also help.\nProblem 1. This problem examines covariance and correlation. We will simulate a gam\nbling scenario.\nSetup\n- Axel and Barto are dealers at a casino in Las Vegas.\n- Every day during their break, Axel and Barto play roulette n_together times to\ngether, each betting one dollar on red each time.\n- Barto then plays n_Barto_alone more times alone, betting one dollar each time.\nNote: In Las Vegas the roulette wheel has 18 red, 18 black and 2 green slots. So there is\nan 18/38 chance of winning a bet on red. If you bet one dollar, you either win or lose one\ndollar.\nProblem 1a. Here you will finish the code for the function\nstudio4_problem_1a(n_together, n_Barto_alone, ntrials)\nThis function should do a simulation to estimate the means and variances of both Axel and\nBarto's daily winnings and the covariance and correlation between Axel and Barto's daily\nwinnings.\nThe arguments to this function are:\nn_together = number of games Barto and Axel play together.\nn_Barto_alone = number of games Barto plays alone after n_together games he plays\nwith Axel\nntrials = number of trials to run in one simulation\n- One trial consists of a simulation of one day of gambling.\n- Run ntrials trials.\n- Use the cat statements provided to print the sample means, variances, covariance and\ncorrelation.\n- In the cat statements, you will need to provide the variables using the names you\nchose for the various quantities.\n\nStudio 4 instructions, Spring 2022\nProblem 1b. Here you will finish the code for the function\nstudio4_problem_1b()\nRun your code from problem 1a using n_together = 10 and various values of n_Barto_alone.\nThen use the cat statements to say what happens to the sample covariance and correlation\nas the number of games Barto plays alone increases.\nYour solution should just print out the descriptions asked for. Do not include the calls to\nstudio4_problem_1a() in you submitted code.\nProblem 2. Simulated central limit theorem.\nThis is cool and shouldn't take a lot of code.\nHere you will finish the code for the function\nstudio4_problem_2(n_bets_per_trial, ntrials)\nArguments:\nn_bets_per_trial = the number of bets in each trial\nn_trials = number of trials\nFor this problem, one trial should simulate a player's winnings over n_bets_per_trial\nbets. As with Axel and Barto, the player bets on red each time. Run ntrials trials and\nplot a density histogram of the results. It is okay to let R pick the bins for your histogram.\nOne trial represents one draw from a random variable, call it X. Because X is the sum\nof many i.i.d. random variables, the central limit theorem says it should be approximately\nnormal.\nUse linearity of expectation and variance to compute the theoretical expected value and\nstandard deviation of X. Use these in dnorm() and add a plot of the approximating normal\ndistribution to your histogram.\nTesting your code\nFor each problem, we ran the problem function with certain parameters. You can see\nthe function call and the output in mit18_05_s22_studio4-test-answers.html. If you\ncall the same function with the same parameters, you should get the same results as in\nmit18_05_s22_studio4-test-answers.html - if there is randomness involved the answers\nshould be close but not identical.\nFor your convenience, the file mit18_05_s22_studio4-test.r contains all the function calls\nused to make mit18_05_s22_studio4-test-answers.html.\nBefore uploading your code\n1. Make sure all your code is in mit18_05_s22_studio4.r. Also make sure it is all inside\nthe functions for the problems.\n2. Clean the environment and plots window.\n3. Source the file.\n\nStudio 4 instructions, Spring 2022\n4. Call each of the problem functions with the same parameters as the test file\nmit18_05_s22_studio4-test-answers.html.\n5. Make sure it runs without error and outputs just the answers asked for in the questions.\n6. Compare the output to the answers given in mit18_05_s22_studio4-test-answers.html.\nUpload your code\nUse the upload link on our MITx site to upload your code for grading.\nLeave the file name as mit18_05_s22_studio4.r. (The upload script will automatically\nadd your name and a timestamp to the file.)\nYou can upload more than once. We will grade the last file you upload.\nDue date\nDue date: The goal is to upload your work by the end of class.\nIf you need extra time, you can upload your work any time before 10 PM ET the day after\nthe studio.\nSolutions uploaded: Solution code will be posted on MITx at 10 PM the day after the\nstudio.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "R Studio 5",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_studio5-instructions.pdf",
      "content": "Studio 5 Discrete Bayesian Updating\n18.05, Spring 2022\nOverview of the studio\nThis studio explores Bayesian updating of discrete priors\nR introduced in this studio\nThe R needed is introduced in mit18_05_s22_studio5-samplecode.r.\nNew functions: barplot().\nWe will use barplot() to make what is called a stacked bar plot. You should make sure\nyou understand how to read these plots. If you have any confusion, please ask one of the\ninstructors or TAs.\nDownload the zip file\n- You should have downloaded the file mit18_05_s22_studio5.zip from our MITx site.\n- Unzip it in your 18.05 studio folder.\n- You should see the following R files\nmit18_05_s22_studio5.r\nmit18_05_s22_studio5-samplecode.r\nmit18_05_s22_studio5-test.r\nand the following other files\nmit18_05_s22_studio5-test-answers.html\nPrepping R Studio\n- In R studio, open mit18_05_s22_studio5-samplecode.r and mit18_05_s22_studio5.r\n- Using the Session menu, set the working directory to source file location. (This is a\ngood habit to develop!)\n\nStudio 5 instructions, Spring 2022\n- Answer the questions in the detailed instructions just below. Your answers should be\nput in mit18_05_s22_studio5.r\n- Solution code will be posted tomorrow at 10 pm\nDetailed instructions for the studio\n- Go through mit18_05_s22_studio5-samplecode.r as a tutorial. Pay special to Ex\nample 2.\nSetup\n- We have our five Platonic dice: 4, 6, 8, 12, 20 sided.\n- There is a prior distribution of the quantity of each die.\n- One die is chosen at random and rolled repeatedly.\n- Our job is to use Bayesian updating to figure out which die was chosen.\nNote. The variable DICE = c(4,6,8,12,20) is set at the top of mit18_05_s22_studio5.r.\nThis is our list of dice for the entire studio.\nProblem 0\nThis is a warmup to make sure the concepts are clear.\nProblem 0a. Here you will finish the code for the function:\nstudio5_problem_0a()\nUse a cat or print statement to list all the hypotheses for the Bayesian updating.\nProblem 0b. Here you will finish the code for the function:\nstudio5_problem_0b()\nUse a cat or print statement to list all the possible outcomes for one roll of the chosen die.\nProblem 0c. Here you will finish the code for the function:\nstudio5_problem_0c()\nConstruct and print the full likelihood table. That is a matrix giving the likelihood of every\npossible outcome for every possible die.\nThe sample code in either example should be very helpful. Remember the sample code uses\njust 3 dice and we now have all 5 Platonic dice.\nProblem 1\nThis problem will do Bayesian updating of the probabilities for the type of die being rolled.\nProblem 1a. There is no code to write for this problem. All you need to do is read and\nrun the sample code for example 2.\nThis code shows how to choose a random die, roll it nrolls times and update the probabil\nities after each roll. It shows how to use barplot() to plot the posterior after each update.\nIt also uses barplot() to make a single stacked barplot showing the entire sequence of\nposteriors.\n\nStudio 5 instructions, Spring 2022\nProblem 1b. Here you will finish the code for the function\nstudio5_problem_1b(prior, nrolls, plot_individual_posteriors)\nArguments:\nprior = prior probilities for the type of die use to generate data\nnrolls = number of rolls to simulate\nplot_individual_posteriors = says whether or not to make individual bar charts.\nWhen nrolls is large, we don't want to plot the posterior after each roll, so we set this to\nFALSE.\nThe given code fixes random_die = 8 and then simulates nrolls rolls. You should not\nalter the die or the data.\nYou should finish the code by having it go through the rolls one at a time. For each roll, the\nprobabilities for the type of die being rolled should be updated and the resulting posterior\nshould be saved.\nIf plot_individual_posterior = TRUE, then after each update, a barplot of the posterior\nshould be drawn.\nAfter the last update, a stacked barplot of all the updates should be made.\nProblem 1c. Here you will finish the code for the function:\nstudio5_problem_1c()\nRun the function studio5_problem_1b() twice. (Both of these are conveniently in mit18_05_s22_studio5-test.r\nBoth times set nrolls = 20 and plot_individual_posteriors = FALSE.\nThe first time, set prior = c(0.2, 0.2, 0.2, 0.2, 0.2).\nThe second time, set prior = c(0.001, 0.001, 0.001, 0.001, 0.996).\nYour code for this problem should print a few sentences, comparing and contrasting the\nresulting stacked bar plots. In particular, we know the chosen die is 8-sided. How do the\ndifferent priors affect the number of rolls needed for the posteriors to become fairly certain\nof which die is being rolled?\nProblem 1d. Here you will finish the code for the function:\nstudio5_problem_1d()\nThe given code runs studio5_problem_1b() with a prior that gives the hypothesis D8 zero\nprobability.\nYour code should print out a few sentences describing what happens in the updating. Re\nmember, we know the chosen die is 8-sided.\nOptional Problem 2\nAll of Problem 2 is optional.\nSometimes data is censored in some way. In this problem, the data will be censored by\nbeing reported as 1 if the roll is a 1 and 0 if it is not. Our goal is still to use Bayesian\nupdating to guess which die is being rolled.\nOPTIONAL Problem 2a. Here you will finish the code for the function: studio5_problem_2a()\nYour code should print out the following:\n- A list of all the hypotheses for the Bayesian updating.\n\nStudio 5 instructions, Spring 2022\n- All the possible outcomes of one roll.\n- A full likelihood table for the likelihood of each outcome give each hypothesis.\nOPTIONAL Problem 2b. This problem is optional. In it you will finish code for the\nfunction\nstudio5_problem_2b(prior, nrolls)\nArguments:\nprior = prior probilities for the type of die use to generate data\nnrolls = number of rolls to simulate\nThis code will simulate the entire scenario.\n- Your code should pick a random die according to the given prior.\n- It should simulate nrolls of that die.\n- It should go through the rolls one at a time. For each roll, the probabilities for the\ntype of die being rolled should be updated and the resulting posterior should be saved.\n- After the last update, make a stacked barplot of all the updates.\n- Print out the actual die chosen and the final posterior.\nDo not plot the individual posteriors.\nSuggestion: This problem only requires a few modifications of your code for problem 1.\nYou need to modify the likelihood table so it is for censored data. Also, instead of fixing\nthe random_die = 8, you need to choose it randomly. Finally, you need to censor the data\nfor each roll.\nTesting your code\nFor each problem, we ran the problem function with certain parameters. You can see\nthe function call and the output in mit18_05_s22_studio5-test-answers.html. If you\ncall the same function with the same parameters, you should get the same results as in\nmit18_05_s22_studio5-test-answers.html - if there is randomness involved the answers\nshould be close but not identical.\nFor your convenience, the file mit18_05_s22_studio5-test.r contains all the function calls\nused to make mit18_05_s22_studio5-test-answers.html.\nBefore uploading your code\n1. Make sure all your code is in mit18_05_s22_studio5.r. Also make sure it is all inside\nthe functions for the problems.\n2. Clean the environment and plots window.\n3. Source the file.\n\nStudio 5 instructions, Spring 2022\n4. Call each of the problem functions with the same parameters as the test file\nmit18_05_s22_studio5-test-answers.html.\n5. Make sure it runs without error and outputs just the answers asked for in the questions.\n6. Compare the output to the answers given in mit18_05_s22_studio5-test-answers.html.\nUpload your code\nUse the upload link on our MITx site to upload your code for grading.\nLeave the file name as mit18_05_s22_studio5.r. (The upload script will automatically\nadd your name and a timestamp to the file.)\nYou can upload more than once. We will grade the last file you upload.\nDue date\nDue date: The goal is to upload your work by the end of class.\nIf you need extra time, you can upload your work any time before 10 PM ET the day after\nthe studio.\nSolutions uploaded: Solution code will be posted on MITx at 10 PM the day after the\nstudio.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "R Studio 6",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_studio6-instructions.pdf",
      "content": "Studio 6 Discretized Continuous Bayesian Updating\n18.05, Spring 2022\n2. Shore-sea-lighthouse\n*\n-10\n-8\n-6\n-4\n-2\nWater\nLighthouse\nShore\nObscure path\nθ\nOverview of the studio\nThis studio explores Bayesian updating of continuous priors.\nR introduced in this studio\nThe R needed is introduced in mit18_05_s22_studio6-samplecode.r.\nNew functions: dcauchy(), which.max().\nConcepts: Bayesian updating, discretization, Maximum a posteriori (MAP) estimates,\nCauchy distribution.\nDownload the zip file\n- You should have downloaded the file mit18_05_s22_studio6.zip from our MITx site.\n- Unzip it in your 18.05 studio folder.\n- It contains the following R files that you will work with\nmit18_05_s22_studio6.r,\nmit18_05_s22_studio6-samplecode.r,\nmit18_05_s22_studio6-test.r\n- The following files will help you write and test your code\nmit18_05_s22_studio6-test-answers.html\n- It also has the following files that that your code will run. You will not need to look\nat these. (Although it might be interesting)\nmit18_05_s22_studio6_problem0_draw_plot.r,\nmit18_05_s22_studio6_problem_1d.r,\nmit18_05_s22_studio6_data_frame.csv\n\nStudio 6 instructions, Spring 2022\nPrepping R Studio\n- In R studio, open mit18_05_s22_studio6-samplecode.r and mit18_05_s22_studio6.r\n- Using the Session menu, set the working directory to source file location. (This is a\ngood habit to develop!)\n- Answer the questions in the detailed instructions just below. Your answers should be\nput in mit18_05_s22_studio6.r\n- Solution code will be posted tomorrow at 10 pm\nDetailed instructions for the studio\n- Go through mit18_05_s22_studio6-samplecode.r as a tutorial.\nProblem 0\nThis problem only requires a short description of what you see.\n- Run the function studio6_problem0().\nThis will run the source file studio6_problem0_draw_plot.r, which creates two histograms:\nHistogram 1: (Simulated) data from a normal distribution Norm(10, 62).\nHistogram 2: (Simulated) averages of 9 draws from Norm(10, 62).\n- In the cat statement at the end of this function, describe the effect of averaging on the\ndistribution.\nProblem 1\nThis problem introduces the Cauchy distribution.\nProblem 1a. Look up the Cauchy distribution on Wikipedia.\nIn the cat statement at the end of the function studio6_problem_1a(), give the formula\nfor the Cauchy pdf with location theta and scale = 1.0.\nProblem 1b. Here you will finish the code for the function studio6_problem_1b().\nFor this function you should add code that\n- On one set of axes, plots the pdfs of standard normal and the Cauchy distribution with\nlocation = 0 and scale = 1.0.\n- Plot the Cauchy in blue and the normal in orange.\nProblem 1c. Here you will complete the cat statement for the function studio6_problem_1c().\nLooking at the graphs drawn in 1b, explain why we say the Cauchy distribution has fat\ntails when compared with the standard normal.\n\nStudio 6 instructions, Spring 2022\nProblem 1d. Just call the function studio6_problem1d().\nThis function mirrors problem 0, except it uses the Cauchy distribution.\nThere is nothing for you to do here, but admire the amazing variability of the natural world.\nNotice that the histogram of the average data does not get narrower. This is a feature of the\nCauchy distribution - remarkably, the distribution of the average of i.i.d. Cauchy values\nfollows the exact same Cauchy distribution.\nProblem 2: Cauchy's Lighthouse\nThe sample code in mit18_05_s22_studio6-samplecode.r, will be quite helpful for this\nproblem.\nHere is the setting for this problem.\nA lighthouse stands one kilometer offshore. Its bi-directional beam rotates at a constant\nrate, so that at any moment there is exactly one spot on the straight shoreline illuminated\nby the beam. All along the shore are kilometer markers.\nA picture of the scene is given on page one of this file.\nYou arrived during the day and didn't note the location of the path. Now it is now night\nand to get home you need to find an obscure path near the shore at the same kilometer\nmarker as the lighthouse. Your only hope for finding the path is to get a good estimate of\nthe position of the lighthouse. As you wander up and down the shore, occasionally you and\nthe light from the lighthouse coincide and you can collect the datum of the position where\nthe beam hits the shore. You manage to do this 15 times.\nWe can show (and you may assume) the position data follows a Cauchy distribution with\nunknown location = θ and scale = 1.0.\nIn this problem we will use Bayesian updating to make an estimate of the value of θ.\nProblem 2a. Here you will finish the code for the function: studio6_problem_2a(data_frame_csv)\nArguments:\ndata_frame_csv = name of the data file. (The grader will use a different data file than\nthe test scripts.)\nThe position data you collected is stored in the file mit18_05_s22_studio6_data_frame.csv.\nThe code we give you at the start of the function loads the data . (We will use different\ndata when we grade your code.)\nIt's usually a good idea to just plot data and see if anything jumps out at you. Add code\nthat plots the position data. (See mit18_05_s22_studio6-samplecode.r section 0.)\nProblem 2b. In this problem you will finish code for the function\nmit18_05_s22_studio6_problem_2b(data_frame_csv).\nArguments:\ndata_frame_csv = name of the data file. (The grader will use a different data file than\nthe test scripts.)\nThis is the one long problem in this assignment. Sections 1-4 of the sample code will be\n\nStudio 6 instructions, Spring 2022\nhelpful.\nAssume a Uniform(θmin, θmax) prior for θ. In the code given at the start of the function we\nassign values theta_min and theta_max for θmin and θmax.\nYou need to write code to do the following.\n2(i) Discretize the prior: use the value dtheta given in the function as the discretization\nstepsize.\n2(ii) Use a for loop to go through the data one value at a time and do Bayesian updating.\nSave each posterior as you go. (Before the loop, you will need to initialize a matrix to hold\nthe posteriors.)\n2(iii) On one plot graph the prior and each of the posteriors\n2(iv) The maximum a posteriori (MAP) estimate is the value of θ that maximizes the\nposterior function. Find the MAP estimate for each of the posteriors. Save these in an\narray and plot them.\n2(v) Make a separate plot showing the final posterior pdf and its MAP estimate.\n2(vi) In the cat statement at the end of the function, say where, given the final posterior,\nyou would look for the obscure path.\nOPTIONAL Problem 2c. This problem is optional. In it you will have studio6_problem_2c()\nprint out an explanation for why the data follows a Cauchy distribution.\nTesting your code\nFor each problem, we ran the problem function with certain parameters. You can see\nthe function call and the output in mit18_05_s22_studio6-test-answers.html. If you\ncall the same function with the same parameters, you should get the same results as in\nmit18_05_s22_studio6-test-answers.html - if there is randomness involved the answers\nshould be close but not identical.\nFor your convenience, the file mit18_05_s22_studio6-test.r contains all the function calls\nused to make mit18_05_s22_studio6-test-answers.html.\nBefore uploading your code\n1. Make sure all your code is in mit18_05_s22_studio6.r. Also make sure it is all inside\nthe functions for the problems.\n2. Clean the environment and plots window.\n3. Source the file.\n4. Call each of the problem functions with the same parameters as the test file\nmit18_05_s22_studio6-test-answers.html.\n5. Make sure it runs without error and outputs just the answers asked for in the questions.\n6. Compare the output to the answers given in mit18_05_s22_studio6-test-answers.html.\n\nStudio 6 instructions, Spring 2022\nUpload your code\nUse the upload link on our MITx site to upload your code for grading.\nLeave the file name as mit18_05_s22_studio6.r. (The upload script will automatically\nadd your name and a timestamp to the file.)\nYou can upload more than once. We will grade the last file you upload.\nDue date\nDue date: The goal is to upload your work by the end of class.\nIf you need extra time, you can upload your work any time before 10 PM ET the day after\nthe studio.\nSolutions uploaded: Solution code will be posted on MITx at 10 PM the day after the\nstudio.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "R Studio 7",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_studio7-instructions.pdf",
      "content": "Studio 7 Significance testing and the probability of\nhypotheses\n18.05, Spring 2022\nOverview of the studio\nIn this studio we will be frequentists. But secretly we will know the actual probability of\nthe different hypotheses. This will illustrate how frequentist values like significance and\npower are NOT probabilities that a hypothesis is true. They are probabililities assuming\na specific hypothesis is true. That is, they are likelihoods.\nMore specifically, we will simulate tossing a coin with unknown probability of heads and\ntesting whether the coin is fair or not. We'll call θ the unknown probability of heads. In\neach problem we will have the following:\n1. H0: The coin is fair. In each problem, we will give the probability of head given H0:\ntheta_H0 = 0.5\n2. HA: The coin has has a specified probability of heads theta_HA. theta_HA will be given\nas an argument to the problem functions.\n3. We will use a secret prior, named secret_prior, so we can see how various error rates\nchange with the prior.\nThe take-away is that numbers like significance give you error rates conditioned on a specific\nhypothesis. They do not give absolute error rates. That requires a prior which is not\navailable to the Frequentist.\nR introduced in this studio\nSee the sample code for examples.\nWe will use qbinom() to find rejection region boundaries (critical values). Because the\ndistribution is discrete, it is easy to make an 'off-by-one' error. The sample code shows how\nto take care to avoid this.\nWe use the %in% operator to check if one value is contained in a list.\nWe review giving the sample function a prescribed sampling distribution.\nDownload the zip file\n- You should have downloaded the file mit18_05_s22_studio7.zip from our MITx site.\n- Unzip it in your 18.05 studio folder.\n- You should see the following R files\nmit18_05_s22_studio7.r\nmit18_05_s22_studio7-samplecode.r\nmit18_05_s22_studio7-test.r\nand the following other files\n\nStudio 7 instructions, Spring 2022\nmit18_05_s22_studio7-test-answers.html\nPrepping R Studio\n- In R studio, open mit18_05_s22_studio7-samplecode.r and mit18_05_s22_studio7.r\n- Using the Session menu, set the working directory to source file location. (This is a\ngood habit to develop!)\n- Answer the questions in the detailed instructions just below. Your answers should be\nput in mit18_05_s22_studio7.r\n- Solution code will be posted tomorrow at 10 pm\nDetailed instructions for the studio\n- Go through mit18_05_s22_studio7-samplecode.r as a tutorial.\nRight-sided rejection regions\nTo avoid too much fussy coding, we will always require that theta_HA > 0.5. This means\nwe will run a right-sided significance test with significance level alpha. The code we\nprovide will enforce this.\nYou should understand why that, if theta_HA < 0.5, we would need to do a left-sided test.\nProblem 1\nProblem 1. Here you will finish the code for the function\nstudio7_problem_1(theta_HA, alpha, n_tosses)\nThe arguments to this function are:\ntheta_HA = probability of heads for an unfair coin\nalpha = the significance level for the test\nn_tosses = the number of coin tosses in one trial\nFor this problem you are omniscient. You always know the answer. It's just the poor\nexperimenters below you who don't know if they are making errors by rejecting or not\nrejecting.\nThere are two types of coins with 0.5 and theta_HA probability of heads. An experimenter\nhas one of the coins and wants to determine which type it is.\nNull hypotheses H0: the coin is fair, theta_H0 = 0.5.\nAlternative hypotheses HA: the coin has probability theta_HA of heads.\nExperiment: toss the coin n_tosses times\nTest statistic: x = number of heads\nFor this problem, your code needs to determine the rejection region, actual significance for\nthis rejection region and power∗ of this test.\n\nStudio 7 instructions, Spring 2022\nYour code should print out these values.\n∗The power is defined as P (rejection | HA), i.e. the probability of a true positive.\nProblem 2\nProblem 2. Here you will finish the code for the function\nstudio7_problem_2(theta_HA, alpha, n_tosses, n_trials, secret_prior)\nThe arguments to this function are:\ntheta_HA = probability of heads for an unfair coin\nalpha = the significance level for the test\nn_tosses = the number of coin tosses in one trial\nn_trials = the number of trials to run in the simulation\nsecret_prior = the secret prior used to pick the type of coin for each trial, i.e.\nsecret_prior = c(probability_of_H0, probability_of_HA)\nAs before H0 = the coin is fair, HA = the coin has probability theta_HA of heads.\nOne trial consists of:\n1. Choosing a type of coin using the secret_prior.\n2. Tossing it n_tosses times and counting the number of heads. The tosses should use the\nprobability appropriate to the type of coin chosen in step 1.\n3. Deciding whether or not to reject H0.\nRun n_trials trials and keep track of\nthe number of rejections, the number of type 1 errors, the number of type 2 errors\nnumber of times H0 is chosen, number of times HA is chosen\n(You can only track errors and the true state of nature because you are omniscient. In real\nlife researchers can't do this.)\nUse your results to estimate each of the following\nP (rejection | H0), P (H0 | rejection), P (rejection | HA), P (HA | rejection)\nThen print out all of the following information: (see the test answers file for what the output\nshould look like.)\n- theta_HA, alpha, n_tosses, n_trials, secret_prior\n- number of rejections\n- number of type 1 errors\n- number of type 2 errors\n- estimated P (rejection | H0)\n- estimated P (H0 | rejection)\n- estimated P (rejection | HA)\n- estimated P (HA | rejection)\n\nStudio 7 instructions, Spring 2022\n- estimated P (rejection)\nProblem 3\nIn problem 3 you will use your function for problem 2 to explore the difference between\nP(hypothesis | rejection) and P(rejection | hypothesis).\nProblem 3a. Here you will finish the code for the function\nstudio7_problem_3a(theta_HA, alpha, n_tosses, n_trials)\nThe arguments to this function are the same as in Problem 2, except they don't include\nsecret_prior.\nFor this problem, the experiement is run with only fair coins, i.e.\nsecret_prior = c(1.0, 0.0)\n.\nThe code provided specifies the secret prior and calls studio7_problem2() for you.\nFirst, call this function with various values of its arguments. Look at the results.\nYour job is to modify the cat statements at the end of this function to print a short expla\nnation of the estimated probabilities that go with the given prior.\nProblem 3b. Here you will finish the code for the function\nstudio7_problem_3b(theta_HA, alpha, n_tosses, n_trials)\nThis problem is identical to 3a, except the prior is set to only produce unfair coins, i.e.\nsecret_prior = c(0.0, 1.0)\n.\nProblem 3c. Use the function\nstudio7_problem_3c()\nto print out a few lines explaining the difference between P (H0 | rejection) and significance.\nProblem 3d. Use the function\nstudio7_problem_3d()\nto have R shout 5 times in all caps, \"THE SIGNIFICANCE IS NOT THE PROBABILITY\nOF AN ERROR GIVEN REJECTION!\"\nThen have it more calmly state once, \"It is the probability of rejection given H0. Frequentists\ndon't compute P(Error | rejection)\"\nOPTIONAL Problem 4\nOptional Problem 4. Here you will finish the code for the function\nstudio7_problem_4(theta_HA, alpha, n_tosses, prior)\nThe arguments to this function have the same meaning as in the problems above.\n\nStudio 7 instructions, Spring 2022\nIn this problem we will be open Bayesians and use the given prior to compute\nThe true value of P (H0 | rejection)\nThe true value of P (HA | rejection)\nTesting your code\nFor each problem, we ran the problem function with certain parameters. You can see\nthe function call and the output in mit18_05_s22_studio7-test-answers.html. If you\ncall the same function with the same parameters, you should get the same results as in\nmit18_05_s22_studio7-test-answers.html - if there is randomness involved the answers\nshould be close but not identical.\nFor your convenience, the file mit18_05_s22_studio7-test.r contains all the function calls\nused to make mit18_05_s22_studio7-test-answers.html.\nBefore uploading your code\n1. Make sure all your code is in mit18_05_s22_studio7.r. Also make sure it is all inside\nthe functions for the problems.\n2. Clean the environment and plots window.\n3. Source the file.\n4. Call each of the problem functions with the same parameters as the test file\nmit18_05_s22_studio7-test-answers.html.\n5. Make sure it runs without error and outputs just the answers asked for in the questions.\n6. Compare the output to the answers given in mit18_05_s22_studio7-test-answers.html.\nUpload your code\nUse the upload link on our MITx site to upload your code for grading.\nLeave the file name as mit18_05_s22_studio7.r. (The upload script will automatically\nadd your name and a timestamp to the file.)\nYou can upload more than once. We will grade the last file you upload.\nDue date\nDue date: The goal is to upload your work by the end of class.\nIf you need extra time, you can upload your work any time before 10 PM ET the day after\nthe studio.\nSolutions uploaded: Solution code will be posted on MITx at 10 PM the day after the\nstudio.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "R Studio 8",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_studio8-instructions.pdf",
      "content": "Studio 8 Simulation and computation of NHST\n18.05, Spring 2022\nHistogram of fstats\nfstats\nDensity\n0.0\n0.4\n0.8\nOverview of the studio\nIn this studio we will get some practice using R to simulate and run some common signifi\ncance tests. As usual, we will provide a lot of hints and examples, but one of the goals of\nthis studio is to get practice using R help and Google to figure out how to make R do what\nwe want.\nHere is a quick summary of the questions.\n1. Simulation of the f-statistic: draw histogram and graph\n2. z-test. Code by hand a z-test\n3. Chi-square test for independence\n4. ANOVA. using aov()\nR introduced in this studio\nSee the sample code for examples.\nF distribution: functions df, pf, etc.\nSignificance tests: t.test, chisq.test, aov\nDownload the zip file\n- You should have downloaded the file mit18_05_s22_studio8.zip from our MITx site.\n- Unzip it in your 18.05 studio folder.\n\nStudio 8 instructions, Spring 2022\n- You should see the following R files\nmit18_05_s22_studio8.r\nmit18_05_s22_studio8-samplecode.r\nmit18_05_s22_studio8-test.r\nand the following other files\nmit18_05_s22_studio8-test-answers.html\nmit18_05_s22_studio8-onesampledata.txt\nmit18_05_s22_studio8Problem3_test.tbl\nPrepping R Studio\n- In R studio, open mit18_05_s22_studio8-samplecode.r and mit18_05_s22_studio8.r\n- Using the Session menu, set the working directory to source file location. (This is a\ngood habit to develop!)\n- Answer the questions in the detailed instructions just below. Your answers should be\nput in mit18_05_s22_studio8.r\n- Solution code will be posted tomorrow at 10 pm\nDetailed instructions for the studio\n- Go through mit18_05_s22_studio8-samplecode.r as a tutorial.\nProblem 1\nProblem 1. Here you will finish the code for the function\nstudio8_problem_1 = function(n, m, mu, sigma, n_trials)\nThe arguments to this function are:\nn = number of groups\nm = number of data points in each group (same for all groups)\nmu = common mean: same for each group\nsigma = common standard deviation (same for each group)\nn_trials = number of trials to run in the simulation\nHere the goal is to do a simulation and show graphically that the f-statistic does indeed\nfollow an f-distribution.\nOne simulation will consist of:\n(i) Use the function parameters to draw m samples for each of n groups of data from a\nNorm(mu, sigma2) distribution.\n(ii) Compute the f-statistic of the data.\nDo this n_trials times and make a density histogram of the f-statistics. Overlay a graph of\nthe pdf of the f-distribution with the correct degrees of freedom.\n\nStudio 8 instructions, Spring 2022\nProblem 2\nProblem 2. Here you will finish the code for the function\nstudio8_problem_2 = function(data, mu0, known_sigma, alpha)\nThe arguments to this function are:\ndata = list of sample values\nmu0 = hypothesized mean in the null hypothesis\nknown_sigma = known value of the standard deviation\nalpha = significance for the test\nIn this problem, you are given some data drawn from a normal distribution with a known\nstandard deviation. The null hypothesis is that the mean of the normal distribution is mu0.\nThe alternative hypothesis is that the mean is not mu0.\nYour job is to run a z-test on the data using the given significance. Print out the mean\nof the data, z-statistic, p-value, and your conclusion on whether or not to reject the null\nhypothesis.\nProblem 3\nProblem 3. Here you will finish the code for the function\nstudio8_problem_3 = function(data_table_file, alpha)\nThe arguments to this function are:\ndata_table_file = data file containing a two-by-two contingency table of married\nand college data.\nalpha = significance for the test.\nIn this problem, you will run a chi-square test for independence. The data is a two-by-two\ntable (called a contingency table) from a sample of women showing the counts of their\nnumber of marriages and their college education. (When you see the table printed out, it\nshould be clear what is in it.)\nThe data for testing your code was taken from the book by Rice, example on p. 489. For\ngrading, we will make up more data.\nLet the null hypothesis be that the number of marriages is independent of the level of\neducation. Run a chi-square test for independence to test this hypothesis. You may do this\nby 'hand' or by using a built-in test in R.\nPrint out the test statistic and the p-value.\nUsing significance level alpha, print out whether or not you reject H0.\nRemember, you need to write code that can deal with different values of alpha and different\np-values.\nProblem 4\nProblem 4. Here you will finish the code for the function\nstudio8_problem_4 = function(T1, T2, T3, alpha)\nThe arguments to this function are:\n\nStudio 8 instructions, Spring 2022\nT1, T2, T3 = 3 sets of data. All the same length\nalpha = significance of the test.\nThe data sets T1, T2, T3 each represent the (simulated) levels of pain associated with\npatients receiving one of three treatments.\nIn this problem you should use the aov() function to run a one-way ANOVA with null\nhypothesis that the mean levels of pain for all 3 treatments are the same.\nTo help, we supply code to build the data into a data.frame. We then print it out so you\ncan see how it looks.\nYou should use the aov() function to run the test.\nUse the summary() command get a summary of the results of aov(). Then extract the\np-value from the summary.\nPrint out the summary and the p-value. Then, using significance level alpha, print out your\nconclusion on whether or not to reject H0\nTesting your code\nFor each problem, we ran the problem function with certain parameters. You can see\nthe function call and the output in mit18_05_s22_studio8-test-answers.html. If you\ncall the same function with the same parameters, you should get the same results as in\nmit18_05_s22_studio8-test-answers.html - if there is randomness involved the answers\nshould be close but not identical.\nFor your convenience, the file mit18_05_s22_studio8-test.r contains all the function calls\nused to make mit18_05_s22_studio8-test-answers.html.\nBefore uploading your code\n1. Make sure all your code is in mit18_05_s22_studio8.r. Also make sure it is all inside\nthe functions for the problems.\n2. Clean the environment and plots window.\n3. Source the file.\n4. Call each of the problem functions with the same parameters as the test file\nmit18_05_s22_studio8-test-answers.html.\n5. Make sure it runs without error and outputs just the answers asked for in the questions.\n6. Compare the output to the answers given in mit18_05_s22_studio8-test-answers.html.\nUpload your code\nUse the upload link on our MITx site to upload your code for grading.\nLeave the file name as mit18_05_s22_studio8.r. (The upload script will automatically\nadd your name and a timestamp to the file.)\n\nStudio 8 instructions, Spring 2022\nYou can upload more than once. We will grade the last file you upload.\nDue date\nDue date: The goal is to upload your work by the end of class.\nIf you need extra time, you can upload your work any time before 10 PM ET the day after\nthe studio.\nSolutions uploaded: Solution code will be posted on MITx at 10 PM the day after the\nstudio.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "R Studio 9",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_studio9-instructions.pdf",
      "content": "Studio 9 Simulating confidence intervals\n18.05, Spring 2022\nOverview of the studio\nThis studio explores confidence intervals using simulated data\nR introduced in this studio\nThere is no new R introduced in this studio. It makes use of familiar functions like rnorm,\nqnorm, qt, rbinom\nDownload the zip file\n- You should have downloaded the file mit18_05_s22_studio9.zip from our MITx site.\n- Unzip it in your 18.05 studio folder.\n- You should see the following R files\nmit18_05_s22_studio9.r\nmit18_05_s22_studio9-samplecode.r\nmit18_05_s22_studio9-test.r\nand the following other files\nmit18_05_s22_studio9-test-answers.html\nPrepping R Studio\n- In R studio, open mit18_05_s22_studio9-samplecode.r and mit18_05_s22_studio9.r\n- Using the Session menu, set the working directory to source file location. (This is a\ngood habit to develop!)\n- Answer the questions in the detailed instructions just below. Your answers should be\nput in mit18_05_s22_studio9.r\n- Solution code will be posted tomorrow at 10 pm\nDetailed instructions for the studio\n- Go through mit18_05_s22_studio9-samplecode.r as a tutorial.\nSummary of questions\n1a. Compute the simulated type 1 CI error rate for z-confidence intervals\n1b. Same as part a, except use t-confidence intervals\n1c. Based on a prior, find the prior and posterior probability that theta is in a given confi\ndence interval.\n\nStudio 9 instructions, Spring 2022\n2. (OPTIONAL) Simulate a poll and give the rule-or-thumb 95% confidence interval.\nProblem 1\nProblem 1. This problem will explore the meaning of c in a c-confidence interval for the\nmean. We will track the simulated type 1 confidence interval error rate. In part c, we will\nlook at the Bayesian posterior probability the parameter of interest is in a given confidence\ninterval.\nIn order to count results we will be omniscient and always know the true value of the mean\nand its prior probability.\nRecall that the type 1 confidence interval error rate is the fraction of trials where the\ntrue mean is not in the confidence interval.\nProblem 1a. Here you will finish the code for the function\nstudio9_problem_1a(theta_vals, theta_prior, sigma, n_data, confidence,\nn_trials)\nThe arguments to this function are:\ntheta_vals = possible values for the mean of the normal distribution\ntheta_prior = probabilities for choosing a θ from theta_vals\nsigma = standard deviation of the normal distribution\nn_data = the number of data values in each trial\nconfidence = the confidence level, e.g. 0.95, 0.9 etc\nn_trials = number of trials in the simulation\nOur data will be drawn from a normal distribution N(θ, σ2), where the value of θ is unknown\nand the value of σ is known. The possible values and prior probabilities of θ are given in\nthe arguments theta_vals, theta_prior.\nFor problem 1(a), we will run an experiment n_trials times and keep track of the type 1\nCI-error rate. The experiment will consist of the following steps\nStep 1. Choose a random value of theta using theta_vals and theta_prior.\nStep 2. draw n_data data points from a N(θ, σ2) distribution.\nStep 3. Create a z-confidence interval with the confidence given in the argument confidence.\nHere you will use the known value of σ.\nStep 4. Check if the true value of θ is in the interval. If it isn't we call it a type 1 CI-error.\n(We can only do this because we are omniscient and know the true value of theta.)\nRun the experiment n_trials times. Print out the last confidence interval and the fraction\nof type 1 CI-errors.\nProblem 1b. Here you will finish the code for the function\nstudio9_problem_1b(theta_vals, theta_prior, sigma, n_data, confidence,\nn_trials)\nThe arguments to this function are:\ntheta_vals = possible values for the mean of the normal distribution\ntheta_prior = probabilities for choosing a θ from theta_vals\nsigma = standard deviation of the normal distribution\n\nStudio 9 instructions, Spring 2022\nn_data = the number of data values in each trial\nconfidence = the confidence level, e.g. 0.95, 0.9 etc\nn_trials = number of trials in the simulation\nThis problem is almost identical to 1(a). The only difference is that you will compute t\nconfidence intervals. So, you will use the given value of σ to generate the data, but you\nwon't use σ when computing the t-confidence interval.\nProblem 1c. Here you will finish the code for the function\nstudio9_problem_1c(theta_vals, theta_prior, sigma, n_data, confidence,\nxbar)\nThe arguments to this function are:\ntheta_vals = possible values for the mean of the normal distribution\ntheta_prior = probabilities for choosing a θ from theta_vals\nsigma = standard deviation of the normal distribution\nn_data = the number of data values in each trial\nconfidence = the confidence level, e.g. 0.95, 0.9 etc\nxbar = the mean of the data\nHere we will put on our Bayesian hats and find the probability the true value of θ is in our\nconfidence interval.\nWe assume our data is sampled from a N(θ, σ2) distribution, where θ is unknown and σ is\nknown. We give you the data mean in the argument xbar. Using this do the following:\n(i) Use the data to update the given prior to a posterior distribution on the possible values\nof θ.\n(ii) Find the z-confidence interval with the given confidence.\n(iii) Compute both the prior and posterior probabilities that θ is in the confidence interval\ncomputed in (ii).\nPrint out, the prior and posterior distributions, the confidence interval and the prior and\nposterior probabilites found in (iii).\nProblem 2 (OPTIONAL)\nHere you will finish the code for the function\nstudio9_problem_2(true_theta, n)\nThe arguments to this function are:\ntrue_theta = the true proportion of the population who prefer Lincoln.\nn = the number of people polled.\nHere we imagine taking a poll in 1860 to find out the the fraction of Massachusetts residents\nwho support Lincoln. The argument true_theta is the true proportion supporting Lincoln.\nThe function should simulate (using true_theta) polling n people. It should then compute\nand print out the rule-of-thumb 95% confidence interval. Print this out as an estimated\nproportion plus or minus a margin of error.\n\nStudio 9 instructions, Spring 2022\nTesting your code\nFor each problem, we ran the problem function with certain parameters. You can see\nthe function call and the output in mit18_05_s22_studio9-test-answers.html. If you\ncall the same function with the same parameters, you should get the same results as in\nmit18_05_s22_studio9-test-answers.html - if there is randomness involved the answers\nshould be close but not identical.\nFor your convenience, the file mit18_05_s22_studio9-test.r contains all the function calls\nused to make mit18_05_s22_studio9-test-answers.html.\nBefore uploading your code\n1. Make sure all your code is in mit18_05_s22_studio9.r. Also make sure it is all inside\nthe functions for the problems.\n2. Clean the environment and plots window.\n3. Source the file.\n4. Call each of the problem functions with the same parameters as the test file\nmit18_05_s22_studio9-test-answers.html.\n5. Make sure it runs without error and outputs just the answers asked for in the questions.\n6. Compare the output to the answers given in mit18_05_s22_studio9-test-answers.html.\nUpload your code\nUse the upload link on our MITx site to upload your code for grading.\nLeave the file name as mit18_05_s22_studio9.r. (The upload script will automatically\nadd your name and a timestamp to the file.)\nYou can upload more than once. We will grade the last file you upload.\nDue date\nDue date: The goal is to upload your work by the end of class.\nIf you need extra time, you can upload your work any time before 10 PM ET the day after\nthe studio.\nSolutions uploaded: Solution code will be posted on MITx at 10 PM the day after the\nstudio.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "R Studio 10",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_studio10-instructions.pdf",
      "content": "Studio 10 Bootstrap confidence intervals\n18.05, Spring 2022\nOverview of the studio\nThis studio simulates bootstrap confidence intervals type I error rates. We can use the\nsimulations to estimate the true confidence (1- type I CI error rate) of bootstrip intervals\nwith a given nominal confidence. We will also compare the performance of the percentile\nand basic methods.\nHere is a little more detail on what we are hoping to do.\n1. We know that a 95% confidence intervals has the following meaning: If 1000 labs each\nran an experiment and used their data to make a 95% CI for the mean (or any parameter)\nthen we would expect about 95% of the intervals to contain the true value, i.e. there should\nbe about a 5% type I CI error rate\n2. In real life you can never actually check the error rate because you don't know the true\nvalue\n3. The bootstrap CI is this weird construction. Calling it a 95% CI is plausible, but we can\ncheck if it truly is a 95% CI by simulation, where, unlike in real life, we do know the true\nvalue of the mean (or other quantity)\nWe will do these simulations for two distributions.\n1. Standard normal distribution. This is symmetric and well behaved.\n2. Log-normal distribution. This is highly skewed and certain statistics will give the boot\nstrap some trouble.\nR introduced in this studio\nThe matrixStats package has functions colMeans2, colMedians, colSds, colQuantiles that\nare optimized for speed on matrices. This means you can generate all your bootstrap\nsamples at once and put them in a matrix. This will be much faster than doing them one\nat a time in a loop.\nNew functions: rlnorm()\nThe R needed is introduced in mit18_05_s22_studio10-samplecode.r.\nDownload the zip file\n- You should have downloaded the file mit18_05_s22_studio10.zip from our MITx\nsite.\n- Unzip it in your 18.05 studio folder.\n- You should see the following R files\nmit18_05_s22_studio10.r\nmit18_05_s22_studio10-samplecode.r\nmit18_05_s22_studio10-test.r\n\nStudio 10 instructions, Spring 2022\nand the following other files\nmit18_05_s22_studio10-test-answers.html\nPrepping R Studio\n- In R studio, open mit18_05_s22_studio10-samplecode.r and mit18_05_s22_studio10.r\n- Using the Session menu, set the working directory to source file location. (This is a\ngood habit to develop!)\n- Answer the questions in the detailed instructions just below. Your answers should be\nput in mit18_05_s22_studio10.r\n- Solution code will be posted tomorrow at 10 pm\nDetailed instructions for the studio\n- Go through mit18_05_s22_studio10-samplecode.r as a tutorial.\nSummary of questions\n1. For normal data: compute the simulated 1 CI error rate for empirical bootstrap confi\ndence intervals of the mean, median and standard deviation. Do this for both percentile\nand basic intervals.\n2a. Look up the log-normal distribution on Wikipedia\n2b. Repeat problem 1 for log-normal data\nProblem 1\nProblem 1. Here you will finish the code for the function\nstudio10_problem_1(true_mean, true_sd, n_data, n_boot, n_trials,\nconfidence)\nThe arguments to this function are:\ntrue_mean = the mean of the normal distribution used to generate the data\ntrue_sd = the standard deviation of the normal distribution used to generate the data\nn_data = number of values in the original and each bootstrap sample (original gener\nated from a normal distribution)\nn_boot = number of bootstrap samples to use in each trial\nn_trials = number of trials in the simulation\nconfidence = the bootstrap confidence level, e.g. 0.95, 0.9 etc\nYour code will simulate finding empirical bootstrap type 1 CI error rates for the mean,\nmedian and standard deviation.\nDo this by running n_trials of the following simulation\n\nStudio 10 instructions, Spring 2022\n1. Generate a normal sample of size n_data. Do this using rnorm and the given true_mean\nand true_sd.\n2. Compute the statistics in question (mean, median and standard deviation) of the\nsample.\n3. From the original sample, generate n_boot empirical bootstrap samples.\n4. Using the bootstrap samples compute the empirical percentile and basic bootstrap\nconfidence intervals with the given confidence.\n5. Using the (known) true value of the statistics in question, check for a type I CI error\nfor each statistic.\nFrom all the trials, report the type I CI error rate.\nProblem 2\nIn this problem we will explore empirical bootstrap confidence intervals for the log-normal\ndistribution. Go to\nhttps://en.wikipedia.org/w/index.php?title=Log-normal_distribution&oldid=1089459082\nLook at the graphs of the pdf and notice how asymmetric they are. For some orientation,\nread the section 'Occurrence and applications'\nProblem 2a. Here you will finish the code for the function\nstudio10_problem_2a(meanlog, sdlog)\nThe arguments to this function are:\nmeanlog = value of meanlog parameter in rlnorm\nsdlog = value of sdlog parameter in rlnorm\nGo to the Wikipedia page and find the formulas for the mean, median and standard devi\nation of the log-normal distribution in terms of the parameters to the R function rlnorm.\nYour solution should implement these formulas and print out the values for of the mean,\nmedian and standard deviation for the given meanlog and sdlog\nProblem 2b. Here you will finish the code for the function\nstudio10_problem_2b(meanlog, sdlog, n_data, n_boot, n_trials, confidence)\nThe arguments to this function are:\nmeanlog = value of meanlog parameter in rlnorm\nsdlog = value of sdlog parameter in rlnorm\nn_data = number of values in each sample (Original sample generated using\na log-normal distribution.)\nn_boot = number of bootstrap samples to use in each trial\nn_trials = number of trials to run in simulation\nconfidence = the bootstrap confidence level\nRepeat problem 1 using the log-normal instead of the normal distribution (R: rlnorm()).\nUse the given meanlog and sdlog values as the parameters to rlnorm.\n\nStudio 10 instructions, Spring 2022\nTesting your code\nFor each problem, we ran the problem function with certain parameters. You can see\nthe function call and the output in mit18_05_s22_studio10-test-answers.html. If you\ncall the same function with the same parameters, you should get the same results as in\nmit18_05_s22_studio10-test-answers.html - if there is randomness involved the an\nswers should be close but not identical.\nFor your convenience, the file mit18_05_s22_studio10-test.r contains all the function\ncalls used to make mit18_05_s22_studio10-test-answers.html.\nBefore uploading your code\n1. Make sure all your code is in mit18_05_s22_studio10.r. Also make sure it is all\ninside the functions for the problems.\n2. Clean the environment and plots window.\n3. Source the file.\n4. Call each of the problem functions with the same parameters as the test file\nmit18_05_s22_studio10-test-answers.html.\n5. Make sure it runs without error and outputs just the answers asked for in the questions.\n6. Compare the output to the answers given in mit18_05_s22_studio10-test-answers.html.\nUpload your code\nUse the upload link on our MITx site to upload your code for grading.\nLeave the file name as mit18_05_s22_studio10.r. (The upload script will automatically\nadd your name and a timestamp to the file.)\nYou can upload more than once. We will grade the last file you upload.\nDue date\nDue date: The goal is to upload your work by the end of class.\nIf you need extra time, you can upload your work any time before 10 PM ET the day after\nthe studio.\nSolutions uploaded: Solution code will be posted on MITx at 10 PM the day after the\nstudio.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Final Exam Review: in-class problems: problems",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_classfinal_pset.pdf",
      "content": "Review for final exam: in-class problems\nMIT 18.05 Spring 2022\nProbability\n- Counting\n- Sets\n- Inclusion-exclusion principle\n- Rule of product (multiplication rule)\n- Permutation and combinations\n- Basics\n- Outcome, sample space, event\n- Discrete, continuous\n- Probability function\n- Conditional probability\n- Independent events\n- Law of total probability\n- Bayes' theorem\n- Random variables\n- Discrete: general, uniform, Bernoulli, binomial, geometric\n- Continuous: general, uniform, normal, exponential\n- pmf, pdf, cdf\n- Expectation = mean = average value\n- Variance; standard deviation\n- Joint distributions\n- Joint pmf and pdf\n- Independent random variables\n- Covariance and correlation\n- Central limit theorem\nStatistics\n- Maximum likelihood\n- Least squares\n- Bayesian inference\n- Discrete sets of hypotheses\n- Continuous ranges of hypotheses\n- Beta distributions\n- Conjugate priors\n- Choosing priors\n- Probability intervals\n- Frequentist inference\n- NHST: rejection regions, significance\n- NHST: p-values\n- z, t, χ2\n- NHST: type I and type II error\n- NHST: power\n- Confidence intervals\n- Bootstrap confidence intervals\n\n- Empirical bootstrap confidence intervals\n- Parametric bootstrap confidence intervals\n- Linear regression\nProblem 1. Basketball\nSuppose that against a certain opponent the number of points the MIT basketball team\nscores is normally distributed with unknown mean θ and unknown variance, σ2.\nSuppose that over the course of the last 10 games between the two teams MIT scored the\nfollowing points:\n59, 62, 59, 74, 70, 61, 62, 66, 62, 75\n(a) Compute a 95% t-confidence interval for θ. Does 95% confidence mean that the\nprobability θ is in the interval you just found is 95%?\n(b) Now suppose that you learn that σ2 = 25. Compute a 95% z-confidence interval for\nθ. How does this compare to the interval in (a)?\n(c) Let X be the number of points scored in a game. Suppose that your friend is a\nconfirmed Bayesian with a priori belief θ ∼ N(60, 16) and that X ∼ N(θ, 25). He computes\na 95% probability interval for θ, given the data in part (a). How does this interval compare\nto the intervals in (a) and (b)?\n(d) Which of the three intervals constructed above do you prefer? Why?\nProblem 2. Confidence interval 2\nThe volume in a set of wine bottles is known to follow a N(μ, 25) distribution. You take a\nsample of the bottles and measure their volumes. How many bottles do you have to sample\nto have a 95% confidence interval for μ with width 1?\nProblem 3. Polling confidence intervals\nYou do a poll to see what fraction p of the population supports candidate A over candidate\nB.\n(a) How many people do you need to poll to know p to within 1% with 95% confidence.\n(b) Let p be the fraction of the population who prefer candidate A. If you poll 400 people,\nhow many have to prefer candidate A so that the 90% confidence interval is entirely above\np = 0.5.\nProblem 4. Confidence intervals 3\nSuppose you made 40 confidence intervals with confidence level 95%. About how many of\nthem would you expect to be \"wrong'? That is, how many would not actually contain the\nparameter being estimated? Should you be surprised if 10 of them are wrong?\nProblem 5. (Confidence intervals)\nA statistician chooses 20 randomly selected class days and counts the number of students\npresent in 18.05. They find a standard deviation of 4.06 students If the number of students\npresent is normally distributed, find the 95% confidence interval for the population standard\ndeviation of the number of students in attendance.\n\nProblem 6. Linear regression (least squares)\n(a) Set up fitting the least squares line through the points (1, 1), (2, 1), and (3, 3).\nProblem 7. Empirical bootstrap\nSuppose we had 100 data points x1, ... x100 with sample median q0.5 = 3.3.\n(a) Outline the steps needed to generate an empirical percentile bootstrap 90% confidence\ninterval for the median q0.5.\n(b) Suppose now that the sorted list in the previous problem consists of 200 empirical\nbootstrap medians computed from resamples of size 100 drawn from the original data. Use\nthe list to construct a 90% precentile CI for q0.5.\nProblem 8. Parametric bootstrap\nSuppose we have a sample of size 100 drawn from a geom(p) distribution with unknown\np. The MLE estimate for p is given by by p = 1/x. Assume for our data x = 3.30, so\np = 1/x = 0.30303.\n(a) Outline the steps needed to generate a parametric basic bootstrap 90% confidence\ninterval.\n(b) Suppose the following sorted list consists of 200 bootstrap means computed from a\nsample of size 100 drawn from a geometric(0.30303) distribution. Use the list to construct\na 90% basic CI for p.\n2.68 2.77 2.79 2.81 2.82 2.84 2.84 2.85 2.88 2.89\n2.91 2.91 2.91 2.92 2.94 2.94 2.95 2.97 2.97 2.99\n3.00 3.00 3.01 3.01 3.01 3.03 3.04 3.04 3.04 3.04\n3.04 3.05 3.06 3.06 3.07 3.07 3.07 3.08 3.08 3.08\n3.08 3.09 3.09 3.10 3.11 3.11 3.12 3.13 3.13 3.13\n3.13 3.15 3.15 3.15 3.16 3.16 3.16 3.16 3.17 3.17\n3.17 3.18 3.20 3.20 3.20 3.21 3.21 3.22 3.23 3.23\n3.23 3.23 3.23 3.24 3.24 3.24 3.24 3.25 3.25 3.25\n3.25 3.25 3.25 3.26 3.26 3.26 3.26 3.27 3.27 3.27\n3.28 3.29 3.29 3.30 3.30 3.30 3.30 3.30 3.30 3.31\n3.31 3.32 3.32 3.34 3.34 3.34 3.34 3.35 3.35 3.35\n3.35 3.35 3.36 3.36 3.37 3.37 3.37 3.37 3.37 3.37\n3.38 3.38 3.39 3.39 3.40 3.40 3.40 3.40 3.41 3.42\n3.42 3.42 3.43 3.43 3.43 3.43 3.44 3.44 3.44 3.44\n3.44 3.45 3.45 3.45 3.45 3.45 3.45 3.45 3.46 3.46\n3.46 3.46 3.47 3.47 3.49 3.49 3.49 3.49 3.49 3.50\n3.50 3.50 3.52 3.52 3.52 3.52 3.53 3.54 3.54 3.54\n3.55 3.56 3.57 3.58 3.59 3.59 3.60 3.61 3.61 3.61\n3.62 3.63 3.65 3.65 3.67 3.67 3.68 3.70 3.72 3.72\n3.73 3.73 3.74 3.76 3.78 3.79 3.80 3.86 3.89 3.91\nProblem 9. (NHST chi-square)\nA study of recidivism (repeat offenses) of juvenile offenders used an experimental design with\nrandom assignment of juveniles to experimental intervention (Family Group Counseling) or\n\ncontrol group (diversion programs). 70 out of 200 people in the control group re-offended\nand 30 out of 200 people in the experimental group re-offended.\nUse a chi-square significance test to test whether the recidivism rates within 6 months for\nthe two experimental groups are significantly different at a significance level of 0.05.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Final Exam Review: in-class problems: solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_classfinal_pset_sol.pdf",
      "content": "Review for final exam: in-class solutions\nMIT 18.05 Spring 2022\nProblem 1. Basketball\nSuppose that against a certain opponent the number of points the MIT basketball team scores\nis normally distributed with unknown mean θ and unknown variance, σ2.\nSuppose that over the course of the last 10 games between the two teams MIT scored the\nfollowing points:\n59, 62, 59, 74, 70, 61, 62, 66, 62, 75\n(a) Compute a 95% t-confidence interval for θ. Does 95% confidence mean that the proba\nbility θ is in the interval you just found is 95%?\nSolution: We compute the data mean and variance x = 65, s2 = 35.778. The number of\ndegrees of freedom is 9. We look up the critical value t9,0.025 = 2.262 in the t-table The\n95% confidence interval is\nx - t9,0.025s x + t9,0.025s\n[\n,\n] = [65 - 2.262\n√\n3.5778, 65 + 2.262\n√\n3.5778] = [60.721, 69.279]\n√n\n√n\nOn the exam you will be expected to be able to use the t-table. We won't ask you to\ncompute by hand the mean and variance of 10 numbers.\n95% confidence means that in 95% of experiments the random interval will contain the\ntrue θ. It is not the probability that θ is in the given interval. That depends on the prior\ndistribution for θ, which we don't know.\n(b) Now suppose that you learn that σ2 = 25. Compute a 95% z-confidence interval for θ.\nHow does this compare to the interval in (a)?\nSolution: We can look in the z-table or simply remember that z0.025 = 1.96. The 95%\nconfidence interval is\n⋅5\n⋅5\nx - z0.025\n√n\nσ x + z0.025\n√n\nσ ] = [65 - 1.96\n[\n,\n√\n10 , 65 + 1.96\n[61.901, 68.099]\n√\n10 ] =\nThis is a narrower interval than in part (a). There are two reasons for this, first the true\nvariance 25 is smaller than the sample variance 35.8 and second, the normal distribution\nhas narrower tails than the t distribution.\n(c) Let X be the number of points scored in a game. Suppose that your friend is a confirmed\nBayesian with a priori belief θ ∼ N(60, 16) and that X ∼ N(θ, 25). He computes a 95%\nprobability interval for θ, given the data in part (a). How does this interval compare to the\nintervals in (a) and (b)?\nSolution: We use the normal-normal update formulas to find the posterior pdf for θ.\na60 + b65\na=\nb=\n=\n= 64.3,\nσ2\n= 2.16.\nμpost\npost =\n16,\n25,\na+ b\na+ b\nThe posterior pdf is f(θ|data) = N(64.3, 2.16). The posterior 95% probability interval for θ\nis\n[64.3 - z0.025\n√\n2.16, 64.3 + z0.025\n√\n2.16] = [61.442, 67.206]\n\n(d) Which of the three intervals constructed above do you prefer? Why?\nSolution: There's no one correct answer; each method has its own advantages and disad\nvantages. In this problem they all give similar answers.\nProblem 2. Confidence interval 2\nThe volume in a set of wine bottles is known to follow a N(μ, 25) distribution. You take a\nsample of the bottles and measure their volumes. How many bottles do you have to sample\nto have a 95% confidence interval for μ with width 1?\nSolution: Suppose we have taken data x1, ... , xn with mean x. The 95% confidence interval\nfor the mean is x ± z0.025 √σ\nn. This has width 2 z0.025 √σ\nn . Setting the width equal to 1 and\nsubstitituting values z0.025 = 1.96 and σ = 5 we get\n2 ⋅ 1.96√5\nn = 1 ⇒ √n = 19.6.\nSo, n = (19.6)2 = 384. .\nIf we use our rule of thumb that z0.025 = 2 we have √n/10 = 2 ⇒ n = 400.\nProblem 3. Polling confidence intervals\nYou do a poll to see what fraction p of the population supports candidate A over candidate\nB.\n(a) How many people do you need to poll to know p to within 1% with 95% confidence.\nSolution: The rule-of-thumb is that a 95% confidence interval is\nTo be within\nx ± 1/√n.\n1% we need\n√1\nn = 0.01 ⇒ n = 10000.\nUsing z0.025 = 1.96 instead the 95% confidence interval is\nx ± z0.025\n\n2√n .\nTo be within 1% we need\nz0.025 = 0.01 ⇒ n = 9604.\n2√n\nNote, we are still using the standard Bernoulli approximation σ ≤ 1/2.\n(b) Let p be the fraction of the population who prefer candidate A. If you poll 400 people,\nhow many have to prefer candidate A so that the 90% confidence interval is entirely above\np = 0.5.\nSolution: The 90% confidence interval is x ± z0.05 ⋅ 2√1\nn . Since z0.05 = 1.64 and n = 400\nour confidence interval is\nx± 1.64 ⋅\n= x± 0.041\nIf this is entirely above 0.5 we have x - 0.041 > 0.5, so x > 0.541. Let T be the number\nT\nout of 400 who prefer A. We have x = 400 > 0.541, so T > 216 .\n\nProblem 4. Confidence intervals 3\nSuppose you made 40 confidence intervals with confidence level 95%. About how many of\nthem would you expect to be \"wrong'? That is, how many would not actually contain the\nparameter being estimated? Should you be surprised if 10 of them are wrong?\nSolution: A 95% confidence means about 5% = 1/20 will be wrong. You'd expect about\n2 to be wrong.\nWith a probability p = 0.05 of being wrong, the number wrong follows a Binomial(40, p)\ndistribution. This has expected value 2, and standard deviation √40(0.05)(0.95) = 1.38. 10\nwrong is (10-2)/1.38 = 5.8 standard deviations from the mean. This would be surprising.\nProblem 5. (Confidence intervals)\nA statistician chooses 20 randomly selected class days and counts the number of students\npresent in 18.05. They find a standard deviation of 4.06 students If the number of students\npresent is normally distributed, find the 95% confidence interval for the population standard\ndeviation of the number of students in attendance.\nSolution: We have n = 20 and s2 = 4.062. If we fix a hypothesis for σ2 we know\n(n - 1)s2\n∼ χ2\nn-1\nσ2\nWe used R to find the critical values. (Or use the χ2 table.)\nc025 = qchisq(0.975,19) = 32.852\nc975 = qchisq(0.025,19) = 8.907\nThe 95% confidence interval for σ2 is\n[(n-1) ⋅s2 (n-1) ⋅s2\n] = [19 ⋅4.062 19 ⋅4.062\n,\n,\n] = [9.53, 35.16]\nc0.025\nc0.975\n32.852\n8.907\nWe can take square roots to find the 95% confidence interval for σ\n[3.09, 5.93]\nProblem 6. Linear regression (least squares)\n(a) Set up fitting the least squares line through the points (1, 1), (2, 1), and (3, 3).\n(a) Solution: The model is yi = a + bxi + εi, where εi is random error. We assume the\nerrors are independent with mean 0 and the same variance for each i (homoscedastic).\nThe total error squared is\nE2 = ∑(yi-a-bxi)2 = (1 -a-b)2 + (1 -a-2b)2 + (3 -a-3b)2\nThe least squares fit is given by the values of a and b which minimize E2. We solve for\nthem by setting the partial derivatives of E2 with respect to a and b to 0. In R we found\nthat a = -1/3, b = 1.\nProblem 7. Empirical bootstrap\nSuppose we had 100 data points x1, ... x100 with sample median q0.5 = 3.3.\n\n(a) Outline the steps needed to generate an empirical percentile bootstrap 90% confidence\ninterval for the median q0.5.\nSolution: For the percentile bootstrap, we don't have to pivot, so the algebra is a little\nshorter.\nStep 1. We have the point estimate q0.5 ≈\n= 3.3.\nq0.5\nStep 2. Use the computer to generate many (say 10000) size 100 resamples of the original\ndata.\nStep 3. For each bootstrap sample compute and save the bootstrap median q∗\n0.5.\nStep 4. Find the quantiles c0.05 and c0.95. (Remember c0.05 is the 5th percentile in the list\nof bootstrap medians, etc.)\nStep 5. The 90% percentile bootstrap confidence interval for q0.5 is\n[c0.05, c0.95]\n(b) Suppose now that the sorted list in the previous problem consists of 200 empirical\nbootstrap medians computed from resamples of size 100 drawn from the original data. Use\nthe list to construct a 90% precentile CI for q0.5.\nSolution: The list covers steps 1-3 in part (a). Since it is sorted, step 4 is straightforward.\nThe 5th and 95th percentiles for q∗\n0.5 are\n2.89,\n3.72\n(Here we just took the 10th and 190th values. We could have interpolated between the\n9th and 10th, and 190th and 191st entries, but this would not change our answer to two\ndecimal places.)\nThe above interval is our empirical percentile bootstrap confidence interval for the median.\nProblem 8. Parametric bootstrap\nSuppose we have a sample of size 100 drawn from a geom(p) distribution with unknown\np. The MLE estimate for p is given by by p = 1/x. Assume for our data x = 3.30, so\np = 1/x = 0.30303.\n(a) Outline the steps needed to generate a parametric basic bootstrap 90% confidence interval.\nSolution: Step 1. We have the point estimate p ≈ p = 0.30303.\nStep 2. Use the computer to generate many (say 10000) size 100 samples. (These are called\nthe bootstrap samples.)\nStep 3. For each sample compute p∗ = 1/x∗ and δ∗ = p∗-p.\nStep 4. Sort the δ∗ and find the critical values δ0.95 and δ0.05. (Remember δ0.95 is the 5th\npercentile etc.)\nStep 5. The 90% bootstrap confidence interval for p is\n[\n\n]\np - δ0.05, p - δ0.95\n\n(b) Suppose the following sorted list consists of 200 bootstrap means computed from a sample\nof size 100 drawn from a geometric(0.30303) distribution. Use the list to construct a 90%\nbasic CI for p.\n2.68 2.77 2.79 2.81 2.82 2.84 2.84 2.85 2.88 2.89\n2.91 2.91 2.91 2.92 2.94 2.94 2.95 2.97 2.97 2.99\n3.00 3.00 3.01 3.01 3.01 3.03 3.04 3.04 3.04 3.04\n3.04 3.05 3.06 3.06 3.07 3.07 3.07 3.08 3.08 3.08\n3.08 3.09 3.09 3.10 3.11 3.11 3.12 3.13 3.13 3.13\n3.13 3.15 3.15 3.15 3.16 3.16 3.16 3.16 3.17 3.17\n3.17 3.18 3.20 3.20 3.20 3.21 3.21 3.22 3.23 3.23\n3.23 3.23 3.23 3.24 3.24 3.24 3.24 3.25 3.25 3.25\n3.25 3.25 3.25 3.26 3.26 3.26 3.26 3.27 3.27 3.27\n3.28 3.29 3.29 3.30 3.30 3.30 3.30 3.30 3.30 3.31\n3.31 3.32 3.32 3.34 3.34 3.34 3.34 3.35 3.35 3.35\n3.35 3.35 3.36 3.36 3.37 3.37 3.37 3.37 3.37 3.37\n3.38 3.38 3.39 3.39 3.40 3.40 3.40 3.40 3.41 3.42\n3.42 3.42 3.43 3.43 3.43 3.43 3.44 3.44 3.44 3.44\n3.44 3.45 3.45 3.45 3.45 3.45 3.45 3.45 3.46 3.46\n3.46 3.46 3.47 3.47 3.49 3.49 3.49 3.49 3.49 3.50\n3.50 3.50 3.52 3.52 3.52 3.52 3.53 3.54 3.54 3.54\n3.55 3.56 3.57 3.58 3.59 3.59 3.60 3.61 3.61 3.61\n3.62 3.63 3.65 3.65 3.67 3.67 3.68 3.70 3.72 3.72\n3.73 3.73 3.74 3.76 3.78 3.79 3.80 3.86 3.89 3.91\nSolution: The basic interval requires an algebraic pivot, so it's tricky to keep the sides\nstraight here. We work slowly and carefully:\nThe 5th and 95th percentiles for x∗ are the 10th and 190th entries\n2.89,\n3.72\n(Here again there is some ambiguity on which entries to use. We will accept using the 11th\nor the 191st entries or some interpolation between these entries.)\nSo the 5th and 95th percentiles for p∗ are\n1/3.72 = 0.26882,\n1/2.89 = 0.34602\nSo the 5th and 95th percentiles for δ∗ = p∗-p are\n-0.034213,\n0.042990\nThese are also the 0.95 and 0.05 critical values.\nSo the 90% basic CI for p is\n[0.30303 - 0.042990, 0.30303 + 0.034213] = [0.26004, 0.33724]\nProblem 9. (NHST chi-square)\nA study of recidivism (repeat offenses) of juvenile offenders used an experimental design with\n\nrandom assignment of juveniles to experimental intervention (Family Group Counseling) or\ncontrol group (diversion programs). 70 out of 200 people in the control group re-offended\nand 30 out of 200 people in the experimental group re-offended.\nUse a chi-square significance test to test whether the recidivism rates within 6 months for\nthe two experimental groups are significantly different at a significance level of 0.05.\nSolution: We will use a chi-square test for homogeneity. Remember we need to use all the\ndata!. For hypotheses we have:\nH0: the re-offense rate is the same for both groups.\nHA: the rates are different.\nHere is the table of counts. The computation of the expected counts is explained below.\nControl group\nExperimental group\nRe-offend\nDon't re-offend\nobserved\nexpected\nobserved\nexpected\nThe expected counts are computed as follows. Under H0 the re-offense rates are the same,\nsay θ. To find the expected counts we find the MLE of θ using the combined data:\ntotal re-offend\nθ =\n=\ntotal subjects\n400.\nThen, for example, the expected number of re-offenders in the control group is 200 ⋅θ = 50.\nThe other expected counts are computed in the same way.\nThe chi-square test statistic is\n= ∑(observed - expected)2\n50 + 202\n150 ≈8 + 2.67 + 8 + 2.67 ≈21.33.\nX2\nexpected\n=\n150 + 20\n+ 202\nFinally, we need the degrees of freedom: df = 1 because this is a two-by-two table and\n(2 -1) ⋅(2 -1) = 1. (Or because we can freely fill in the count in one cell and still be\nconsistent with the marginal counts 200, 200, 100, 300, 400 used to compute the expected\ncounts.)\nFrom the χ2 table: p = P(X2 > 21.33|df = 1) < 0.01.\nConclusion: we reject H0 in favor of HA. The experimental intervention appears to be\neffective.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Final Exam Review: In-class: Problem Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_examf_rev_pset_sol.pdf",
      "content": "Review for final exam: in-class solutions\nMIT 18.05 Spring 2022\nProblem 1. Basketball\nSuppose that against a certain opponent the number of points the MIT basketball team scores\nis normally distributed with unknown mean θ and unknown variance, σ2.\nSuppose that over the course of the last 10 games between the two teams MIT scored the\nfollowing points:\n59, 62, 59, 74, 70, 61, 62, 66, 62, 75\n(a) Compute a 95% t-confidence interval for θ. Does 95% confidence mean that the proba\nbility θ is in the interval you just found is 95%?\nSolution: We compute the data mean and variance x = 65, s2 = 35.778. The number of\ndegrees of freedom is 9. We look up the critical value t9,0.025 = 2.262 in the t-table The\n95% confidence interval is\nx - t9,0.025s x + t9,0.025s\n[\n,\n] = [65 - 2.262\n√\n3.5778, 65 + 2.262\n√\n3.5778] = [60.721, 69.279]\n√n\n√n\nOn the exam you will be expected to be able to use the t-table. We won't ask you to\ncompute by hand the mean and variance of 10 numbers.\n95% confidence means that in 95% of experiments the random interval will contain the\ntrue θ. It is not the probability that θ is in the given interval. That depends on the prior\ndistribution for θ, which we don't know.\n(b) Now suppose that you learn that σ2 = 25. Compute a 95% z-confidence interval for θ.\nHow does this compare to the interval in (a)?\nSolution: We can look in the z-table or simply remember that z0.025 = 1.96. The 95%\nconfidence interval is\n⋅5\n⋅5\nx - z0.025\n√n\nσ x + z0.025\n√n\nσ ] = [65 - 1.96\n[\n,\n√\n10 , 65 + 1.96\n[61.901, 68.099]\n√\n10 ] =\nThis is a narrower interval than in part (a). There are two reasons for this, first the true\nvariance 25 is smaller than the sample variance 35.8 and second, the normal distribution\nhas narrower tails than the t distribution.\n(c) Let X be the number of points scored in a game. Suppose that your friend is a confirmed\nBayesian with a priori belief θ ∼ N(60, 16) and that X ∼ N(θ, 25). He computes a 95%\nprobability interval for θ, given the data in part (a). How does this interval compare to the\nintervals in (a) and (b)?\nSolution: We use the normal-normal update formulas to find the posterior pdf for θ.\na60 + b65\na=\nb=\n=\n= 64.3,\nσ2\n= 2.16.\nμpost\npost =\n16,\n25,\na+ b\na+ b\nThe posterior pdf is f(θ|data) = N(64.3, 2.16). The posterior 95% probability interval for θ\nis\n[64.3 - z0.025\n√\n2.16, 64.3 + z0.025\n√\n2.16] = [61.442, 67.206]\n\n(d) Which of the three intervals constructed above do you prefer? Why?\nSolution: There's no one correct answer; each method has its own advantages and disad\nvantages. In this problem they all give similar answers.\nProblem 2. Confidence interval 2\nThe volume in a set of wine bottles is known to follow a N(μ, 25) distribution. You take a\nsample of the bottles and measure their volumes. How many bottles do you have to sample\nto have a 95% confidence interval for μ with width 1?\nSolution: Suppose we have taken data x1, ... , xn with mean x. The 95% confidence interval\nfor the mean is x ± z0.025 √σ\nn. This has width 2 z0.025 √σ\nn . Setting the width equal to 1 and\nsubstitituting values z0.025 = 1.96 and σ = 5 we get\n2 ⋅ 1.96√5\nn = 1 ⇒ √n = 19.6.\nSo, n = (19.6)2 = 384. .\nIf we use our rule of thumb that z0.025 = 2 we have √n/10 = 2 ⇒ n = 400.\nProblem 3. Polling confidence intervals\nYou do a poll to see what fraction p of the population supports candidate A over candidate\nB.\n(a) How many people do you need to poll to know p to within 1% with 95% confidence.\nSolution: The rule-of-thumb is that a 95% confidence interval is\nTo be within\nx ± 1/√n.\n1% we need\n√1\nn = 0.01 ⇒ n = 10000.\nUsing z0.025 = 1.96 instead the 95% confidence interval is\nx ± z0.025\n\n2√n .\nTo be within 1% we need\nz0.025 = 0.01 ⇒ n = 9604.\n2√n\nNote, we are still using the standard Bernoulli approximation σ ≤ 1/2.\n(b) Let p be the fraction of the population who prefer candidate A. If you poll 400 people,\nhow many have to prefer candidate A so that the 90% confidence interval is entirely above\np = 0.5.\nSolution: The 90% confidence interval is x ± z0.05 ⋅ 2√1\nn . Since z0.05 = 1.64 and n = 400\nour confidence interval is\nx± 1.64 ⋅\n= x± 0.041\nIf this is entirely above 0.5 we have x - 0.041 > 0.5, so x > 0.541. Let T be the number\nT\nout of 400 who prefer A. We have x = 400 > 0.541, so T > 216 .\n\nProblem 4. Confidence intervals 3\nSuppose you made 40 confidence intervals with confidence level 95%. About how many of\nthem would you expect to be \"wrong'? That is, how many would not actually contain the\nparameter being estimated? Should you be surprised if 10 of them are wrong?\nSolution: A 95% confidence means about 5% = 1/20 will be wrong. You'd expect about\n2 to be wrong.\nWith a probability p = 0.05 of being wrong, the number wrong follows a Binomial(40, p)\ndistribution. This has expected value 2, and standard deviation √40(0.05)(0.95) = 1.38. 10\nwrong is (10-2)/1.38 = 5.8 standard deviations from the mean. This would be surprising.\nProblem 5. (Confidence intervals)\nA statistician chooses 20 randomly selected class days and counts the number of students\npresent in 18.05. They find a standard deviation of 4.06 students If the number of students\npresent is normally distributed, find the 95% confidence interval for the population standard\ndeviation of the number of students in attendance.\nSolution: We have n = 20 and s2 = 4.062. If we fix a hypothesis for σ2 we know\n(n - 1)s2\n∼ χ2\nn-1\nσ2\nWe used R to find the critical values. (Or use the χ2 table.)\nc025 = qchisq(0.975,19) = 32.852\nc975 = qchisq(0.025,19) = 8.907\nThe 95% confidence interval for σ2 is\n[(n-1) ⋅s2 (n-1) ⋅s2\n] = [19 ⋅4.062 19 ⋅4.062\n,\n,\n] = [9.53, 35.16]\nc0.025\nc0.975\n32.852\n8.907\nWe can take square roots to find the 95% confidence interval for σ\n[3.09, 5.93]\nProblem 6. Linear regression (least squares)\n(a) Set up fitting the least squares line through the points (1, 1), (2, 1), and (3, 3).\n(a) Solution: The model is yi = a + bxi + εi, where εi is random error. We assume the\nerrors are independent with mean 0 and the same variance for each i (homoscedastic).\nThe total error squared is\nE2 = ∑(yi-a-bxi)2 = (1 -a-b)2 + (1 -a-2b)2 + (3 -a-3b)2\nThe least squares fit is given by the values of a and b which minimize E2. We solve for\nthem by setting the partial derivatives of E2 with respect to a and b to 0. In R we found\nthat a = -1/3, b = 1.\nProblem 7. Empirical bootstrap\nSuppose we had 100 data points x1, ... x100 with sample median q0.5 = 3.3.\n\n(a) Outline the steps needed to generate an empirical percentile bootstrap 90% confidence\ninterval for the median q0.5.\nSolution: For the percentile bootstrap, we don't have to pivot, so the algebra is a little\nshorter.\nStep 1. We have the point estimate q0.5 ≈\n= 3.3.\nq0.5\nStep 2. Use the computer to generate many (say 10000) size 100 resamples of the original\ndata.\nStep 3. For each bootstrap sample compute and save the bootstrap median q∗\n0.5.\nStep 4. Find the quantiles c0.05 and c0.95. (Remember c0.05 is the 5th percentile in the list\nof bootstrap medians, etc.)\nStep 5. The 90% percentile bootstrap confidence interval for q0.5 is\n[c0.05, c0.95]\n(b) Suppose now that the sorted list in the previous problem consists of 200 empirical\nbootstrap medians computed from resamples of size 100 drawn from the original data. Use\nthe list to construct a 90% precentile CI for q0.5.\nSolution: The list covers steps 1-3 in part (a). Since it is sorted, step 4 is straightforward.\nThe 5th and 95th percentiles for q∗\n0.5 are\n2.89,\n3.72\n(Here we just took the 10th and 190th values. We could have interpolated between the\n9th and 10th, and 190th and 191st entries, but this would not change our answer to two\ndecimal places.)\nThe above interval is our empirical percentile bootstrap confidence interval for the median.\nProblem 8. Parametric bootstrap\nSuppose we have a sample of size 100 drawn from a geom(p) distribution with unknown\np. The MLE estimate for p is given by by p = 1/x. Assume for our data x = 3.30, so\np = 1/x = 0.30303.\n(a) Outline the steps needed to generate a parametric basic bootstrap 90% confidence interval.\nSolution: Step 1. We have the point estimate p ≈ p = 0.30303.\nStep 2. Use the computer to generate many (say 10000) size 100 samples. (These are called\nthe bootstrap samples.)\nStep 3. For each sample compute p∗ = 1/x∗ and δ∗ = p∗-p.\nStep 4. Sort the δ∗ and find the critical values δ0.95 and δ0.05. (Remember δ0.95 is the 5th\npercentile etc.)\nStep 5. The 90% bootstrap confidence interval for p is\n[\n\n]\np - δ0.05, p - δ0.95\n\n(b) Suppose the following sorted list consists of 200 bootstrap means computed from a sample\nof size 100 drawn from a geometric(0.30303) distribution. Use the list to construct a 90%\nbasic CI for p.\n2.68 2.77 2.79 2.81 2.82 2.84 2.84 2.85 2.88 2.89\n2.91 2.91 2.91 2.92 2.94 2.94 2.95 2.97 2.97 2.99\n3.00 3.00 3.01 3.01 3.01 3.03 3.04 3.04 3.04 3.04\n3.04 3.05 3.06 3.06 3.07 3.07 3.07 3.08 3.08 3.08\n3.08 3.09 3.09 3.10 3.11 3.11 3.12 3.13 3.13 3.13\n3.13 3.15 3.15 3.15 3.16 3.16 3.16 3.16 3.17 3.17\n3.17 3.18 3.20 3.20 3.20 3.21 3.21 3.22 3.23 3.23\n3.23 3.23 3.23 3.24 3.24 3.24 3.24 3.25 3.25 3.25\n3.25 3.25 3.25 3.26 3.26 3.26 3.26 3.27 3.27 3.27\n3.28 3.29 3.29 3.30 3.30 3.30 3.30 3.30 3.30 3.31\n3.31 3.32 3.32 3.34 3.34 3.34 3.34 3.35 3.35 3.35\n3.35 3.35 3.36 3.36 3.37 3.37 3.37 3.37 3.37 3.37\n3.38 3.38 3.39 3.39 3.40 3.40 3.40 3.40 3.41 3.42\n3.42 3.42 3.43 3.43 3.43 3.43 3.44 3.44 3.44 3.44\n3.44 3.45 3.45 3.45 3.45 3.45 3.45 3.45 3.46 3.46\n3.46 3.46 3.47 3.47 3.49 3.49 3.49 3.49 3.49 3.50\n3.50 3.50 3.52 3.52 3.52 3.52 3.53 3.54 3.54 3.54\n3.55 3.56 3.57 3.58 3.59 3.59 3.60 3.61 3.61 3.61\n3.62 3.63 3.65 3.65 3.67 3.67 3.68 3.70 3.72 3.72\n3.73 3.73 3.74 3.76 3.78 3.79 3.80 3.86 3.89 3.91\nSolution: The basic interval requires an algebraic pivot, so it's tricky to keep the sides\nstraight here. We work slowly and carefully:\nThe 5th and 95th percentiles for x∗ are the 10th and 190th entries\n2.89,\n3.72\n(Here again there is some ambiguity on which entries to use. We will accept using the 11th\nor the 191st entries or some interpolation between these entries.)\nSo the 5th and 95th percentiles for p∗ are\n1/3.72 = 0.26882,\n1/2.89 = 0.34602\nSo the 5th and 95th percentiles for δ∗ = p∗-p are\n-0.034213,\n0.042990\nThese are also the 0.95 and 0.05 critical values.\nSo the 90% basic CI for p is\n[0.30303 - 0.042990, 0.30303 + 0.034213] = [0.26004, 0.33724]\nProblem 9. (NHST chi-square)\nA study of recidivism (repeat offenses) of juvenile offenders used an experimental design with\n\nrandom assignment of juveniles to experimental intervention (Family Group Counseling) or\ncontrol group (diversion programs). 70 out of 200 people in the control group re-offended\nand 30 out of 200 people in the experimental group re-offended.\nUse a chi-square significance test to test whether the recidivism rates within 6 months for\nthe two experimental groups are significantly different at a significance level of 0.05.\nSolution: We will use a chi-square test for homogeneity. Remember we need to use all the\ndata!. For hypotheses we have:\nH0: the re-offense rate is the same for both groups.\nHA: the rates are different.\nHere is the table of counts. The computation of the expected counts is explained below.\nControl group\nExperimental group\nRe-offend\nDon't re-offend\nobserved\nexpected\nobserved\nexpected\nThe expected counts are computed as follows. Under H0 the re-offense rates are the same,\nsay θ. To find the expected counts we find the MLE of θ using the combined data:\ntotal re-offend\nθ =\n=\ntotal subjects\n400.\nThen, for example, the expected number of re-offenders in the control group is 200 ⋅θ = 50.\nThe other expected counts are computed in the same way.\nThe chi-square test statistic is\n= ∑(observed - expected)2\n50 + 202\n150 ≈8 + 2.67 + 8 + 2.67 ≈21.33.\nX2\nexpected\n=\n150 + 20\n+ 202\nFinally, we need the degrees of freedom: df = 1 because this is a two-by-two table and\n(2 -1) ⋅(2 -1) = 1. (Or because we can freely fill in the count in one cell and still be\nconsistent with the marginal counts 200, 200, 100, 300, 400 used to compute the expected\ncounts.)\nFrom the χ2 table: p = P(X2 > 21.33|df = 1) < 0.01.\nConclusion: we reject H0 in favor of HA. The experimental intervention appears to be\neffective.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Final Exam Review: In-class: Problems",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_examf_rev_pset.pdf",
      "content": "Review for final exam: in-class problems\nMIT 18.05 Spring 2022\nProbability\n- Counting\n- Sets\n- Inclusion-exclusion principle\n- Rule of product (multiplication rule)\n- Permutation and combinations\n- Basics\n- Outcome, sample space, event\n- Discrete, continuous\n- Probability function\n- Conditional probability\n- Independent events\n- Law of total probability\n- Bayes' theorem\n- Random variables\n- Discrete: general, uniform, Bernoulli, binomial, geometric\n- Continuous: general, uniform, normal, exponential\n- pmf, pdf, cdf\n- Expectation = mean = average value\n- Variance; standard deviation\n- Joint distributions\n- Joint pmf and pdf\n- Independent random variables\n- Covariance and correlation\n- Central limit theorem\nStatistics\n- Maximum likelihood\n- Least squares\n- Bayesian inference\n- Discrete sets of hypotheses\n- Continuous ranges of hypotheses\n- Beta distributions\n- Conjugate priors\n- Choosing priors\n- Probability intervals\n- Frequentist inference\n- NHST: rejection regions, significance\n- NHST: p-values\n- z, t, χ2\n- NHST: type I and type II error\n- NHST: power\n- Confidence intervals\n- Bootstrap confidence intervals\n\n- Empirical bootstrap confidence intervals\n- Parametric bootstrap confidence intervals\n- Linear regression\nProblem 1. Basketball\nSuppose that against a certain opponent the number of points the MIT basketball team\nscores is normally distributed with unknown mean θ and unknown variance, σ2.\nSuppose that over the course of the last 10 games between the two teams MIT scored the\nfollowing points:\n59, 62, 59, 74, 70, 61, 62, 66, 62, 75\n(a) Compute a 95% t-confidence interval for θ. Does 95% confidence mean that the\nprobability θ is in the interval you just found is 95%?\n(b) Now suppose that you learn that σ2 = 25. Compute a 95% z-confidence interval for\nθ. How does this compare to the interval in (a)?\n(c) Let X be the number of points scored in a game. Suppose that your friend is a\nconfirmed Bayesian with a priori belief θ ∼ N(60, 16) and that X ∼ N(θ, 25). He computes\na 95% probability interval for θ, given the data in part (a). How does this interval compare\nto the intervals in (a) and (b)?\n(d) Which of the three intervals constructed above do you prefer? Why?\nProblem 2. Confidence interval 2\nThe volume in a set of wine bottles is known to follow a N(μ, 25) distribution. You take a\nsample of the bottles and measure their volumes. How many bottles do you have to sample\nto have a 95% confidence interval for μ with width 1?\nProblem 3. Polling confidence intervals\nYou do a poll to see what fraction p of the population supports candidate A over candidate\nB.\n(a) How many people do you need to poll to know p to within 1% with 95% confidence.\n(b) Let p be the fraction of the population who prefer candidate A. If you poll 400 people,\nhow many have to prefer candidate A so that the 90% confidence interval is entirely above\np = 0.5.\nProblem 4. Confidence intervals 3\nSuppose you made 40 confidence intervals with confidence level 95%. About how many of\nthem would you expect to be \"wrong'? That is, how many would not actually contain the\nparameter being estimated? Should you be surprised if 10 of them are wrong?\nProblem 5. (Confidence intervals)\nA statistician chooses 20 randomly selected class days and counts the number of students\npresent in 18.05. They find a standard deviation of 4.06 students If the number of students\npresent is normally distributed, find the 95% confidence interval for the population standard\ndeviation of the number of students in attendance.\n\nProblem 6. Linear regression (least squares)\n(a) Set up fitting the least squares line through the points (1, 1), (2, 1), and (3, 3).\nProblem 7. Empirical bootstrap\nSuppose we had 100 data points x1, ... x100 with sample median q0.5 = 3.3.\n(a) Outline the steps needed to generate an empirical percentile bootstrap 90% confidence\ninterval for the median q0.5.\n(b) Suppose now that the sorted list in the previous problem consists of 200 empirical\nbootstrap medians computed from resamples of size 100 drawn from the original data. Use\nthe list to construct a 90% precentile CI for q0.5.\nProblem 8. Parametric bootstrap\nSuppose we have a sample of size 100 drawn from a geom(p) distribution with unknown\np. The MLE estimate for p is given by by p = 1/x. Assume for our data x = 3.30, so\np = 1/x = 0.30303.\n(a) Outline the steps needed to generate a parametric basic bootstrap 90% confidence\ninterval.\n(b) Suppose the following sorted list consists of 200 bootstrap means computed from a\nsample of size 100 drawn from a geometric(0.30303) distribution. Use the list to construct\na 90% basic CI for p.\n2.68 2.77 2.79 2.81 2.82 2.84 2.84 2.85 2.88 2.89\n2.91 2.91 2.91 2.92 2.94 2.94 2.95 2.97 2.97 2.99\n3.00 3.00 3.01 3.01 3.01 3.03 3.04 3.04 3.04 3.04\n3.04 3.05 3.06 3.06 3.07 3.07 3.07 3.08 3.08 3.08\n3.08 3.09 3.09 3.10 3.11 3.11 3.12 3.13 3.13 3.13\n3.13 3.15 3.15 3.15 3.16 3.16 3.16 3.16 3.17 3.17\n3.17 3.18 3.20 3.20 3.20 3.21 3.21 3.22 3.23 3.23\n3.23 3.23 3.23 3.24 3.24 3.24 3.24 3.25 3.25 3.25\n3.25 3.25 3.25 3.26 3.26 3.26 3.26 3.27 3.27 3.27\n3.28 3.29 3.29 3.30 3.30 3.30 3.30 3.30 3.30 3.31\n3.31 3.32 3.32 3.34 3.34 3.34 3.34 3.35 3.35 3.35\n3.35 3.35 3.36 3.36 3.37 3.37 3.37 3.37 3.37 3.37\n3.38 3.38 3.39 3.39 3.40 3.40 3.40 3.40 3.41 3.42\n3.42 3.42 3.43 3.43 3.43 3.43 3.44 3.44 3.44 3.44\n3.44 3.45 3.45 3.45 3.45 3.45 3.45 3.45 3.46 3.46\n3.46 3.46 3.47 3.47 3.49 3.49 3.49 3.49 3.49 3.50\n3.50 3.50 3.52 3.52 3.52 3.52 3.53 3.54 3.54 3.54\n3.55 3.56 3.57 3.58 3.59 3.59 3.60 3.61 3.61 3.61\n3.62 3.63 3.65 3.65 3.67 3.67 3.68 3.70 3.72 3.72\n3.73 3.73 3.74 3.76 3.78 3.79 3.80 3.86 3.89 3.91\nProblem 9. (NHST chi-square)\nA study of recidivism (repeat offenses) of juvenile offenders used an experimental design with\nrandom assignment of juveniles to experimental intervention (Family Group Counseling) or\n\ncontrol group (diversion programs). 70 out of 200 people in the control group re-offended\nand 30 out of 200 people in the experimental group re-offended.\nUse a chi-square significance test to test whether the recidivism rates within 6 months for\nthe two experimental groups are significantly different at a significance level of 0.05.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Normal Probability Table",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_exam1_sn_table.pdf",
      "content": "18.05 Simple Normal Table\nSince we generally have access to computers there is no need for the comprehensive tables\nof old. This table is designed to be complete enough and easy to use for exams.\nStandard normal table of left tail probabilities.\nz\nz\nz\nz\nΦ(z)\nΦ(z)\nΦ(z)\nΦ(z)\n-4.00\n0.0000\n-2.00\n0.0228\n0.00\n0.5000\n2.00\n0.9772\n-3.95\n0.0000\n-1.95\n0.0256\n0.05\n0.5199\n2.05\n0.9798\n-3.90\n0.0000\n-1.90\n0.0287\n0.10\n0.5398\n2.10\n0.9821\n-3.85\n0.0001\n-1.85\n0.0322\n0.15\n0.5596\n2.15\n0.9842\n-3.80\n0.0001\n-1.80\n0.0359\n0.20\n0.5793\n2.20\n0.9861\n-3.75\n0.0001\n-1.75\n0.0401\n0.25\n0.5987\n2.25\n0.9878\n-3.70\n0.0001\n-1.70\n0.0446\n0.30\n0.6179\n2.30\n0.9893\n-3.65\n0.0001\n-1.65\n0.0495\n0.35\n0.6368\n2.35\n0.9906\n-3.60\n0.0002\n-1.60\n0.0548\n0.40\n0.6554\n2.40\n0.9918\n-3.55\n0.0002\n-1.55\n0.0606\n0.45\n0.6736\n2.45\n0.9929\n-3.50\n0.0002\n-1.50\n0.0668\n0.50\n0.6915\n2.50\n0.9938\n-3.45\n0.0003\n-1.45\n0.0735\n0.55\n0.7088\n2.55\n0.9946\n-3.40\n0.0003\n-1.40\n0.0808\n0.60\n0.7257\n2.60\n0.9953\n-3.35\n0.0004\n-1.35\n0.0885\n0.65\n0.7422\n2.65\n0.9960\n-3.30\n0.0005\n-1.30\n0.0968\n0.70\n0.7580\n2.70\n0.9965\n-3.25\n0.0006\n-1.25\n0.1056\n0.75\n0.7734\n2.75\n0.9970\n-3.20\n0.0007\n-1.20\n0.1151\n0.80\n0.7881\n2.80\n0.9974\n-3.15\n0.0008\n-1.15\n0.1251\n0.85\n0.8023\n2.85\n0.9978\n-3.10\n0.0010\n-1.10\n0.1357\n0.90\n0.8159\n2.90\n0.9981\n-3.05\n0.0011\n-1.05\n0.1469\n0.95\n0.8289\n2.95\n0.9984\n-3.00\n0.0013\n-1.00\n0.1587\n1.00\n0.8413\n3.00\n0.9987\n-2.95\n0.0016\n-0.95\n0.1711\n1.05\n0.8531\n3.05\n0.9989\n-2.90\n0.0019\n-0.90\n0.1841\n1.10\n0.8643\n3.10\n0.9990\n-2.85\n0.0022\n-0.85\n0.1977\n1.15\n0.8749\n3.15\n0.9992\n-2.80\n0.0026\n-0.80\n0.2119\n1.20\n0.8849\n3.20\n0.9993\n-2.75\n0.0030\n-0.75\n0.2266\n1.25\n0.8944\n3.25\n0.9994\n-2.70\n0.0035\n-0.70\n0.2420\n1.30\n0.9032\n3.30\n0.9995\n-2.65\n0.0040\n-0.65\n0.2578\n1.35\n0.9115\n3.35\n0.9996\n-2.60\n0.0047\n-0.60\n0.2743\n1.40\n0.9192\n3.40\n0.9997\n-2.55\n0.0054\n-0.55\n0.2912\n1.45\n0.9265\n3.45\n0.9997\n-2.50\n0.0062\n-0.50\n0.3085\n1.50\n0.9332\n3.50\n0.9998\n-2.45\n0.0071\n-0.45\n0.3264\n1.55\n0.9394\n3.55\n0.9998\n-2.40\n0.0082\n-0.40\n0.3446\n1.60\n0.9452\n3.60\n0.9998\n-2.35\n0.0094\n-0.35\n0.3632\n1.65\n0.9505\n3.65\n0.9999\n-2.30\n0.0107\n-0.30\n0.3821\n1.70\n0.9554\n3.70\n0.9999\n-2.25\n0.0122\n-0.25\n0.4013\n1.75\n0.9599\n3.75\n0.9999\n-2.20\n0.0139\n-0.20\n0.4207\n1.80\n0.9641\n3.80\n0.9999\n-2.15\n0.0158\n-0.15\n0.4404\n1.85\n0.9678\n3.85\n0.9999\n-2.10\n0.0179\n-0.10\n0.4602\n1.90\n0.9713\n3.90\n1.0000\n-2.05\n0.0202\n-0.05\n0.4801\n1.95\n0.9744\n3.95\n1.0000\nΦ(z) = P (Z ≤ z) for N(0, 1).\n(Use interpolation to estimate\nz values to a 3rd decimal\nplace.)\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Probability Tables",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_exam2_table.pdf",
      "content": "18.05 Tables\nSince we generally have access to computers there is no need for the comprehensive tables\nof old. These tables are designed to be complete enough and easy to use for exams.\nStandard normal table of left tail probabilities.\nz\nz\nz\nz\nΦ(z)\nΦ(z)\nΦ(z)\nΦ(z)\n-4.00\n0.0000\n-2.00\n0.0228\n0.00\n0.5000\n2.00\n0.9772\n-3.95\n0.0000\n-1.95\n0.0256\n0.05\n0.5199\n2.05\n0.9798\n-3.90\n0.0000\n-1.90\n0.0287\n0.10\n0.5398\n2.10\n0.9821\n-3.85\n0.0001\n-1.85\n0.0322\n0.15\n0.5596\n2.15\n0.9842\n-3.80\n0.0001\n-1.80\n0.0359\n0.20\n0.5793\n2.20\n0.9861\n-3.75\n0.0001\n-1.75\n0.0401\n0.25\n0.5987\n2.25\n0.9878\n-3.70\n0.0001\n-1.70\n0.0446\n0.30\n0.6179\n2.30\n0.9893\n-3.65\n0.0001\n-1.65\n0.0495\n0.35\n0.6368\n2.35\n0.9906\n-3.60\n0.0002\n-1.60\n0.0548\n0.40\n0.6554\n2.40\n0.9918\n-3.55\n0.0002\n-1.55\n0.0606\n0.45\n0.6736\n2.45\n0.9929\n-3.50\n0.0002\n-1.50\n0.0668\n0.50\n0.6915\n2.50\n0.9938\n-3.45\n0.0003\n-1.45\n0.0735\n0.55\n0.7088\n2.55\n0.9946\n-3.40\n0.0003\n-1.40\n0.0808\n0.60\n0.7257\n2.60\n0.9953\n-3.35\n0.0004\n-1.35\n0.0885\n0.65\n0.7422\n2.65\n0.9960\n-3.30\n0.0005\n-1.30\n0.0968\n0.70\n0.7580\n2.70\n0.9965\n-3.25\n0.0006\n-1.25\n0.1056\n0.75\n0.7734\n2.75\n0.9970\n-3.20\n0.0007\n-1.20\n0.1151\n0.80\n0.7881\n2.80\n0.9974\n-3.15\n0.0008\n-1.15\n0.1251\n0.85\n0.8023\n2.85\n0.9978\n-3.10\n0.0010\n-1.10\n0.1357\n0.90\n0.8159\n2.90\n0.9981\n-3.05\n0.0011\n-1.05\n0.1469\n0.95\n0.8289\n2.95\n0.9984\n-3.00\n0.0013\n-1.00\n0.1587\n1.00\n0.8413\n3.00\n0.9987\n-2.95\n0.0016\n-0.95\n0.1711\n1.05\n0.8531\n3.05\n0.9989\n-2.90\n0.0019\n-0.90\n0.1841\n1.10\n0.8643\n3.10\n0.9990\n-2.85\n0.0022\n-0.85\n0.1977\n1.15\n0.8749\n3.15\n0.9992\n-2.80\n0.0026\n-0.80\n0.2119\n1.20\n0.8849\n3.20\n0.9993\n-2.75\n0.0030\n-0.75\n0.2266\n1.25\n0.8944\n3.25\n0.9994\n-2.70\n0.0035\n-0.70\n0.2420\n1.30\n0.9032\n3.30\n0.9995\n-2.65\n0.0040\n-0.65\n0.2578\n1.35\n0.9115\n3.35\n0.9996\n-2.60\n0.0047\n-0.60\n0.2743\n1.40\n0.9192\n3.40\n0.9997\n-2.55\n0.0054\n-0.55\n0.2912\n1.45\n0.9265\n3.45\n0.9997\n-2.50\n0.0062\n-0.50\n0.3085\n1.50\n0.9332\n3.50\n0.9998\n-2.45\n0.0071\n-0.45\n0.3264\n1.55\n0.9394\n3.55\n0.9998\n-2.40\n0.0082\n-0.40\n0.3446\n1.60\n0.9452\n3.60\n0.9998\n-2.35\n0.0094\n-0.35\n0.3632\n1.65\n0.9505\n3.65\n0.9999\n-2.30\n0.0107\n-0.30\n0.3821\n1.70\n0.9554\n3.70\n0.9999\n-2.25\n0.0122\n-0.25\n0.4013\n1.75\n0.9599\n3.75\n0.9999\n-2.20\n0.0139\n-0.20\n0.4207\n1.80\n0.9641\n3.80\n0.9999\n-2.15\n0.0158\n-0.15\n0.4404\n1.85\n0.9678\n3.85\n0.9999\n-2.10\n0.0179\n-0.10\n0.4602\n1.90\n0.9713\n3.90\n1.0000\n-2.05\n0.0202\n-0.05\n0.4801\n1.95\n0.9744\n3.95\n1.0000\nΦ(z) = P (Z ≤ z) for N(0, 1).\n(Use interpolation to estimate\nz values to a 3rd decimal\nplace.)\n\nTable of Student t critical values (right-tail)\nThe table shows tdf, p = the 1 - p quantile of t(df).\nWe only give values for p ≤ 0.5. Use symmetry to find the values for p > 0.5, e.g.\nt5, 0.975 = -t5, 0.025\nIn R notation tdf, p = qt(1-p, df).\ndf\\p\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.040\n0.050\n0.100\n0.200\n0.300\n0.400\n0.500\n63.66\n31.82\n21.20\n15.89\n12.71\n10.58\n7.92\n6.31\n3.08\n1.38\n0.73\n0.32\n0.00\n9.92\n6.96\n5.64\n4.85\n4.30\n3.90\n3.32\n2.92\n1.89\n1.06\n0.62\n0.29\n0.00\n5.84\n4.54\n3.90\n3.48\n3.18\n2.95\n2.61\n2.35\n1.64\n0.98\n0.58\n0.28\n0.00\n4.60\n3.75\n3.30\n3.00\n2.78\n2.60\n2.33\n2.13\n1.53\n0.94\n0.57\n0.27\n0.00\n4.03\n3.36\n3.00\n2.76\n2.57\n2.42\n2.19\n2.02\n1.48\n0.92\n0.56\n0.27\n0.00\n3.71\n3.14\n2.83\n2.61\n2.45\n2.31\n2.10\n1.94\n1.44\n0.91\n0.55\n0.26\n0.00\n3.50\n3.00\n2.71\n2.52\n2.36\n2.24\n2.05\n1.89\n1.41\n0.90\n0.55\n0.26\n0.00\n3.36\n2.90\n2.63\n2.45\n2.31\n2.19\n2.00\n1.86\n1.40\n0.89\n0.55\n0.26\n0.00\n3.25\n2.82\n2.57\n2.40\n2.26\n2.15\n1.97\n1.83\n1.38\n0.88\n0.54\n0.26\n0.00\n3.17\n2.76\n2.53\n2.36\n2.23\n2.12\n1.95\n1.81\n1.37\n0.88\n0.54\n0.26\n0.00\n2.92\n2.58\n2.38\n2.24\n2.12\n2.02\n1.87\n1.75\n1.34\n0.86\n0.54\n0.26\n0.00\n2.90\n2.57\n2.37\n2.22\n2.11\n2.02\n1.86\n1.74\n1.33\n0.86\n0.53\n0.26\n0.00\n2.88\n2.55\n2.36\n2.21\n2.10\n2.01\n1.86\n1.73\n1.33\n0.86\n0.53\n0.26\n0.00\n2.86\n2.54\n2.35\n2.20\n2.09\n2.00\n1.85\n1.73\n1.33\n0.86\n0.53\n0.26\n0.00\n2.85\n2.53\n2.34\n2.20\n2.09\n1.99\n1.84\n1.72\n1.33\n0.86\n0.53\n0.26\n0.00\n2.83\n2.52\n2.33\n2.19\n2.08\n1.99\n1.84\n1.72\n1.32\n0.86\n0.53\n0.26\n0.00\n2.82\n2.51\n2.32\n2.18\n2.07\n1.98\n1.84\n1.72\n1.32\n0.86\n0.53\n0.26\n0.00\n2.81\n2.50\n2.31\n2.18\n2.07\n1.98\n1.83\n1.71\n1.32\n0.86\n0.53\n0.26\n0.00\n2.80\n2.49\n2.31\n2.17\n2.06\n1.97\n1.83\n1.71\n1.32\n0.86\n0.53\n0.26\n0.00\n2.79\n2.49\n2.30\n2.17\n2.06\n1.97\n1.82\n1.71\n1.32\n0.86\n0.53\n0.26\n0.00\n2.75\n2.46\n2.28\n2.15\n2.04\n1.95\n1.81\n1.70\n1.31\n0.85\n0.53\n0.26\n0.00\n2.74\n2.45\n2.27\n2.14\n2.04\n1.95\n1.81\n1.70\n1.31\n0.85\n0.53\n0.26\n0.00\n2.74\n2.45\n2.27\n2.14\n2.04\n1.95\n1.81\n1.69\n1.31\n0.85\n0.53\n0.26\n0.00\n2.73\n2.44\n2.27\n2.14\n2.03\n1.95\n1.81\n1.69\n1.31\n0.85\n0.53\n0.26\n0.00\n2.73\n2.44\n2.27\n2.14\n2.03\n1.95\n1.80\n1.69\n1.31\n0.85\n0.53\n0.26\n0.00\n2.72\n2.44\n2.26\n2.13\n2.03\n1.94\n1.80\n1.69\n1.31\n0.85\n0.53\n0.26\n0.00\n2.70\n2.42\n2.25\n2.12\n2.02\n1.94\n1.80\n1.68\n1.30\n0.85\n0.53\n0.26\n0.00\n2.70\n2.42\n2.25\n2.12\n2.02\n1.93\n1.80\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.70\n2.42\n2.25\n2.12\n2.02\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.70\n2.42\n2.24\n2.12\n2.02\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.69\n2.41\n2.24\n2.12\n2.02\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.69\n2.41\n2.24\n2.12\n2.01\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.69\n2.41\n2.24\n2.11\n2.01\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.68\n2.41\n2.24\n2.11\n2.01\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.68\n2.41\n2.24\n2.11\n2.01\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.68\n2.40\n2.24\n2.11\n2.01\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n\nTable of χ2 critical values (right-tail)\nThe table shows cdf, p = the 1 - p quantile of χ2(df).\nIn R notation cdf, p = qchisq(1-p, df).\ndf\\p\n0.010\n0.025\n0.050\n0.100\n0.200\n0.300\n0.500\n0.700\n0.800\n0.900\n0.950\n0.975\n0.990\n6.63\n5.02\n3.84\n2.71\n1.64\n1.07\n0.45\n0.15\n0.06\n0.02\n0.00\n0.00\n0.00\n9.21\n7.38\n5.99\n4.61\n3.22\n2.41\n1.39\n0.71\n0.45\n0.21\n0.10\n0.05\n0.02\n11.34\n9.35\n7.81\n6.25\n4.64\n3.66\n2.37\n1.42\n1.01\n0.58\n0.35\n0.22\n0.11\n13.28\n11.14\n9.49\n7.78\n5.99\n4.88\n3.36\n2.19\n1.65\n1.06\n0.71\n0.48\n0.30\n15.09\n12.83\n11.07\n9.24\n7.29\n6.06\n4.35\n3.00\n2.34\n1.61\n1.15\n0.83\n0.55\n16.81\n14.45\n12.59\n10.64\n8.56\n7.23\n5.35\n3.83\n3.07\n2.20\n1.64\n1.24\n0.87\n18.48\n16.01\n14.07\n12.02\n9.80\n8.38\n6.35\n4.67\n3.82\n2.83\n2.17\n1.69\n1.24\n20.09\n17.53\n15.51\n13.36\n11.03\n9.52\n7.34\n5.53\n4.59\n3.49\n2.73\n2.18\n1.65\n21.67\n19.02\n16.92\n14.68\n12.24\n10.66\n8.34\n6.39\n5.38\n4.17\n3.33\n2.70\n2.09\n23.21\n20.48\n18.31\n15.99\n13.44\n11.78\n9.34\n7.27\n6.18\n4.87\n3.94\n3.25\n2.56\n32.00\n28.85\n26.30\n23.54\n20.47\n18.42\n15.34\n12.62\n11.15\n9.31\n7.96\n6.91\n5.81\n33.41\n30.19\n27.59\n24.77\n21.61\n19.51\n16.34\n13.53\n12.00\n10.09\n8.67\n7.56\n6.41\n34.81\n31.53\n28.87\n25.99\n22.76\n20.60\n17.34\n14.44\n12.86\n10.86\n9.39\n8.23\n7.01\n36.19\n32.85\n30.14\n27.20\n23.90\n21.69\n18.34\n15.35\n13.72\n11.65\n10.12\n8.91\n7.63\n37.57\n34.17\n31.41\n28.41\n25.04\n22.77\n19.34\n16.27\n14.58\n12.44\n10.85\n9.59\n8.26\n38.93\n35.48\n32.67\n29.62\n26.17\n23.86\n20.34\n17.18\n15.44\n13.24\n11.59\n10.28\n8.90\n40.29\n36.78\n33.92\n30.81\n27.30\n24.94\n21.34\n18.10\n16.31\n14.04\n12.34\n10.98\n9.54\n41.64\n38.08\n35.17\n32.01\n28.43\n26.02\n22.34\n19.02\n17.19\n14.85\n13.09\n11.69\n10.20\n42.98\n39.36\n36.42\n33.20\n29.55\n27.10\n23.34\n19.94\n18.06\n15.66\n13.85\n12.40\n10.86\n44.31\n40.65\n37.65\n34.38\n30.68\n28.17\n24.34\n20.87\n18.94\n16.47\n14.61\n13.12\n11.52\n50.89\n46.98\n43.77\n40.26\n36.25\n33.53\n29.34\n25.51\n23.36\n20.60\n18.49\n16.79\n14.95\n52.19\n48.23\n44.99\n41.42\n37.36\n34.60\n30.34\n26.44\n24.26\n21.43\n19.28\n17.54\n15.66\n53.49\n49.48\n46.19\n42.58\n38.47\n35.66\n31.34\n27.37\n25.15\n22.27\n20.07\n18.29\n16.36\n54.78\n50.73\n47.40\n43.75\n39.57\n36.73\n32.34\n28.31\n26.04\n23.11\n20.87\n19.05\n17.07\n56.06\n51.97\n48.60\n44.90\n40.68\n37.80\n33.34\n29.24\n26.94\n23.95\n21.66\n19.81\n17.79\n57.34\n53.20\n49.80\n46.06\n41.78\n38.86\n34.34\n30.18\n27.84\n24.80\n22.47\n20.57\n18.51\n63.69\n59.34\n55.76\n51.81\n47.27\n44.16\n39.34\n34.87\n32.34\n29.05\n26.51\n24.43\n22.16\n64.95\n60.56\n56.94\n52.95\n48.36\n45.22\n40.34\n35.81\n33.25\n29.91\n27.33\n25.21\n22.91\n66.21\n61.78\n58.12\n54.09\n49.46\n46.28\n41.34\n36.75\n34.16\n30.77\n28.14\n26.00\n23.65\n67.46\n62.99\n59.30\n55.23\n50.55\n47.34\n42.34\n37.70\n35.07\n31.63\n28.96\n26.79\n24.40\n68.71\n64.20\n60.48\n56.37\n51.64\n48.40\n43.34\n38.64\n35.97\n32.49\n29.79\n27.57\n25.15\n69.96\n65.41\n61.66\n57.51\n52.73\n49.45\n44.34\n39.58\n36.88\n33.35\n30.61\n28.37\n25.90\n71.20\n66.62\n62.83\n58.64\n53.82\n50.51\n45.34\n40.53\n37.80\n34.22\n31.44\n29.16\n26.66\n72.44\n67.82\n64.00\n59.77\n54.91\n51.56\n46.34\n41.47\n38.71\n35.08\n32.27\n29.96\n27.42\n73.68\n69.02\n65.17\n60.91\n55.99\n52.62\n47.34\n42.42\n39.62\n35.95\n33.10\n30.75\n28.18\n74.92\n70.22\n66.34\n62.04\n57.08\n53.67\n48.33\n43.37\n40.53\n36.82\n33.93\n31.55\n28.94\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "R Quiz",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_rquiz-instructions.pdf",
      "content": "R Quiz Instructions\n18.05, Spring 2022\nOverview\nThe quiz format is identical to that of the R studios and practice quiz.\n- READ THE INSTRUCTIONS PDF!\n- Read the problems carefully\n- Save your work frequently\n- You may use any resources except another person\n- (This includes, paper, books, code on your computer, code on the internet ...)\n- If code is given with a question it is meant to be used. Don't just ignore it.\n- Remember to use print or cat statements to print the values asked for\n- Before uploading the code: clear your environment and source the entire file (choose\nsource from the code menu)\n- Make sure that it runs without error and outputs just the answers asked for in the\nquestions.\n- Due Friday 5/6 by 5 PM.\nDownload the zip file\n- You should have downloaded the file mit18_05_s22_RQuiz.zip from our MITx site.\n- Unzip it in your 18.05 studio folder.\n- You should see the following R files\nmit18_05_s22_RQuiz.r\nmit18_05_s22_RQuiz-test.r\nmit18_05_s22_RQuiz-samplecode.r\nand the following other files\nmit18_05_s22_RQuiz-test-answers.html\nmit18_05_s22_RQuiz_data_prob4_test.txt\nGeneral instructions\n- Using the Session menu, set the working directory to source file location.\n- Answer the questions in the detailed instructions just below. Your answers should be\nput in mit18_05_s22_RQuiz.r\n\nR quiz instructions, Spring 2022\n- For each question, you will complete the code for the corresponding function.\n- As usual, use the function's arguments and any other code given in the function.\n- Do not print out things that are not asked for.\nNote: The file mit18_05_s22_RQuiz-samplecode.r contains only a few lines of code, but\nthey may be useful to you.\nDetailed instructions for this quiz\n0. Clean your space\nProblem 1 (20 points)\nThis problem will ask you to do several different short tasks. The parts are not related.\nProblem 1a (5 points) Graphing\nHere you will finish the code for the function:\nrquiz_problem_1a(mu, sigma, w_shape, w_scale, a, b)\nArguments:\nmu = mean of the normal pdf to plot\nsigma = standard deviation of the normal pdf to plot\nw_shape = shape parameter for Weibull pdf\nw_scale = scale parameter for Weibull pdf\na, b = endpoints of the range of x for the plot\nOn the same plot for x between a and b put graphs of the pdfs of:\n(i) Norm(mu, sigma)\n(ii) Weibull distribution with shape and scale parameters w_shape and w_scale.\nThe graphs should be in different colors\nEvery plot should be a line graph (type='l')\nProblem 1b (5 points) Combination and factorials.\nHere you will finish the code for the function: rquiz_problem_1b(n, k, m)\nArguments:\nn = see instructions below\nk = see instructions below\nm = see instructions below\nThis function should compute and print the following values:\n(i) n choose k\n(ii) m factorial\n(iii) log of n choose k (for this use the function lchoose to avoid overflow)\nProblem 1c (10 points) Bayesian success.\nHere you will finish the code for the function:\nrquiz_problem_1c(theta_values, num_patients, num_cured)\n\nR quiz instructions, Spring 2022\nArguments:\ntheta_values = List of possible values of θ.\nnum_patients = The number of patients in the trial.\nnum_cured = The number of successes in the trial.\nA treatment with unknown probability θ of success is tried on num_patients patients yield\ning num_cured successes. The possible values of the unknown θ are given in theta_values.\nThat is, we only entertain a finite number of hypotheses for the value of θ.\nYour code should use the data and do each of the following:\n(i) Compute the maximum likelihood estimate (MLE) for θ. (HINT: the function which.max\nmight be useful.) Print out the MLE\n(ii) Suppose there is a flat prior, i.e. each of the possible values of θ is equally likely. Find\nthe prior predictive probability that a single patient will be cured. Print out the predictive\nprobability. (This part does not use the experimental data.)\n(iii) Use the data and a flat prior to do a Bayesian update to find the posterior probability\nfor θ. Print out the posterior.\nProblem 2 (20 points)\nThis problem is on making histograms.\nProblem 2a (10 points) Here you will finish the code for the function:\nrquiz_problem_2a = function(n_draws, k, bin_width)\nArguments:\nn_draws = Number of sample points in the histogram\nk = Number of degrees of freedom for the chi-square distribution\nbin_width = Bin width for histogram\nYour code should simulate n_draws draws from a chi-square distribution with k degrees\nof freedom. Use the results to plot a density histogram. Use a bin width of bin_width.\nFinally, plot the χ2(k) probability density function on top of it.\nProblem 2b (10 points) Here you will finish the code for the function:\nrquiz_problem_2b = function(n_trials, n_draws, k, bin_width)\nArguments:\nn_trials = Number of trials\nn_draws = Number of sample points in each trial\nk = Number of degrees of freedom for the chi-square distribution\nbin_width = Bin width for histogram\nThis problem will illustrate the central limit theorem as follows.\n- One trial will consist of drawing a sample of size n_draws from a χ2(k) distribution.\n- Simulate n_trials trials.\n- For each trial, compute the standardized mean. It should help to know that the χ2(k)\ndistribution has mean k and variance 2k.\n\nR quiz instructions, Spring 2022\n- Plot a density histogram of the n_trials standardized means. Use bin width bin_width.\n- Add a graph of the standard normal pdf to the histogram.\nProblem 3 (10 points)\nHere you will finish the code for the function\nrquiz_problem_3 = function(our_data, alpha)\nArguments:\nour_data = data from some experiment.\nalpha = Significance level for the Shapiro-Wilk test\nYou have collected data and before running a t-test, you want to check if the data comes\nfrom a normal distribution. To do this, you run a Shapiro-Wilk test for normality.\nRun the Shapiro-Wilk test. Then, print out the null hypothesis, p-value and whether or\nnot to reject the null hypothesis.\nYou need to use code to find and print the p-value and decide whether to reject or not.\nYou shouldn't just read the p-value off the screen and enter that value. That is, your code\nshould work correctly even if we change the data or significance level for the test.\nProblem 4 (Extra credit: 5 points)\nThis problem is for extra credit if you have time.\nMake sure you have set the WORKING DIRECTORY to the source file location, so R will\nfind the file. The lists are data from independent random normal trials.\nHere you will finish the code for the function\nrquiz_problem_4 = function(data_file_name, alpha)\nArguments:\ndata_file_name = data file\nalpha = Significance level for t-test\nThe given code extracts two lists, x and y from the data file.\nRun a two sample t-test with unequal variances to test if x and y are drawn from distribu\ntions with the same mean\nPrint out the p-value and whether or not to reject the null hypothesis at significance level\nalpha\nYou need to use code to find and print the p-value and decide whether to reject or not.\nYou shouldn't just read the p-value off the screen and enter that value. That is, your code\nshould work correctly even if we change the data file or significance level for the test.\nTesting your code\nFor each problem, we ran the problem function with certain parameters. You can see\nthe function call and the output in mit18_05_s22_RQuiz-test-answers.html. If you\ncall the same function with the same parameters, you should get the same results as in\n\nR quiz instructions, Spring 2022\nmit18_05_s22_RQuiz-test-answers.html - if there is randomness involved the answers\nshould be close but not identical.\nFor your convenience, the file mit18_05_s22_RQuiz-test.r contains all the function calls\nused to make mit18_05_s22_RQuiz-test-answers.html.\nBefore uploading your code\n1. Make sure all your code is in mit18_05_s22_RQuiz.r. Also make sure it is all inside\nthe functions for the problems.\n2. Clean the environment and plots window.\n3. Source the file.\n4. Call each of the problem functions with the same parameters as the test file\nmit18_05_s22_RQuiz-test-answers.html.\n5. Make sure it runs without error and outputs just the answers asked for in the questions.\n6. Compare the output to the answers given in mit18_05_s22_RQuiz-test-answers.html.\nUpload your code\nUse the upload link on our MITx site to upload your code for grading.\nLeave the file name as mit18_05_s22_RQuiz.r. (The upload script will automatically add\nyour name and a timestamp to the file.)\nYou can upload more than once. We will grade the last file you upload.\nDue date: Friday 5/6 at 5 pm\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "R Quiz Practice",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_rquizpractice-instructions.pdf",
      "content": "R Quiz Practice Instructions\n18.05, Spring 2022\nOverview\nThese are practice questions for the quiz. The format is similiar to the R studios and will\nbe identical to the R Quiz.\nThis is much longer than the actual quiz. As usual, you should read and follow all instruc\ntions.\nYou should be sure to look at problem 7, which is about looking up how to do something\non Google.\nDownload the zip file\n- You should have downloaded the file mit18_05_s22_RQuizPractice.zip from our\nMITx site.\n- Unzip it in your 18.05 studio folder.\n- You should see the following R files\nmit18_05_s22_RQuizPractice.r\nmit18_05_s22_RQuizPractice-test.r\nand the following other files\nmit18_05_s22_RQuizPractice-test-answers.html\nGeneral instructions\n- Using the Session menu, set the working directory to source file location. (This is a\ngood habit to develop!)\n- Answer the questions in the detailed instructions just below. Your answers should be\nput in mit18_05_s22_RQuizPractice.r\n- For each question, you will complete the code for the corresponding function.\n- As usual, use the the function's arguments and any other code given in the function.\n- Do not print out things that are not asked for.\n- Solution code is posted alongside the zip file.\n- For the real quiz, before uploading your code, soure the code file and run all the tests.\nDetailed instructions for this practice\n0. Clean your space\nThe two lines here will clear the environment and console every time you source the file.\n\nR quiz practice instructions, Spring 2022\nProblem 1\nProblem 1 will cover some basic R: pseudo-random sampling, plotting, basic sample statis\ntics.\nProblem 1a. Here you will finish the code for the function\nrquiz_practice_problem_1a(n_samples, mu, sigma)\nArguments:\nn_samples = size of the sample to generate\nmu = mean of the underlying normal distribution\nsigma = standard deviation of the underlying normal distribution\nThe function should generate and print a sample of size n_samples from a normal distri\nbution with mean mu and standard deviation sigma.\nProblem 1b. Here you will finish the code for the function\nrquiz_practice_problem_1b = function(n_samples, size, theta)\nArguments:\nn_samples = size of the sample to generate\nsize = number of Bernoulli trials\ntheta = probability of success in each Bernoulli trial\nThe function should generate and print a sample of size n_samples from a Binomial(size,\ntheta) distribution.\nProblem 1c. Here you will finish the code for the function\nrquiz_practice_problem_1c(sample_space, n_samples)\nArguments:\nsample_space = the list of outcomes to sample from\nn_samples = number of the sample to generate\nGenerate n_samples with replacement from the list given by sample_space.\nPrint the result.\nProblem 1d. Here you will finish the code for the function\nrquiz_practice_problem_1d(n)\nArguments:\nn: the problem asks you to generate a permutation of the numbers 1:n\nUse the sample function to generate a permutation of the numbers 1 to n.\nPrint the result\nProblem 1e. Here you will finish the code for the function\nrquiz_practice_problem_1e(w, x_1)\nArguments:\nw = angular frequency (for sin(w*x) and cos(w*x))\nx_1: plot from 0 to x_1\nPlot the graph of sin(w ∗ x) over the interval 0 to x_1\n\nR quiz practice instructions, Spring 2022\nAdd the graph of cos(w ∗ x) in a different color.\nAdd a vertical line at x = 0 and a horizontal line at y = 0.\nProblem 1f. Here you will finish the code for the function\nrquiz_practice_problem_1f(n_samples, a, b)\nArguments:\nn_samples = number of samples to generate from a Uniform(a,b) distribution.\na = left endpoint for the Uniform(a,b) distribution.\nb = right endpoint for the Uniform(a,b) distribution.\nGenerate n_samples independent values from a Uniform(a, b) distribution.\nCompute and print the sample mean, median, variance, standard deviation and first and\nthird quartiles of this data. (For this, you may need to get help on the quantile function.)\nProblem 1g. Here you will finish the code for the function\nrquiz_practice_problem_1g(n_samples, a, b)\nn_samples = number of samples to generate from a Uniform(a,b) distribution.\na = left endpoint for the Uniform(a,b) distribution.\nb = right endpoint for the Uniform(a,b) distribution.\n- Generate arrays x and y: each of them should have n_samples independent values\nfrom a Uniform(a, b) distribution.\n- Let z = (x + y)/2.\n- Compute the sample covariance and correlation of x and z.\n- Print the results.\n- Make a scatter plot of y vs x.\nProblem 1h. Here you will finish the code for the function\nrquiz_practice_problem_1h(n_samples, mu1, sigma1, mu2, sigma2, alpha, mu0,\nsigma0)\nArguments:\nn_samples = number of samples to generate from each normal distribution.\nmu1 = mean for the first normal distribution.\nsigm1 = standard deviation for the first normal distribution.\nmu2 = mean for the second normal distribution.\nsigm2 = standard deviation for the second normal distribution.\nalpha = significance level for tests.\nmu0, sigma0 are the mean and standard deviation for H0. (sigma0 is not used in the t-test\ncomparing means).\nGenerate x1 and x2: both samples of size n_samples from normal distributions.\nx_1 should be from a Norm(mu1, sigma1) and x_2 should be from a Norm(mu2, sigma2).\nPrint out all the arguments to this function.\n\nR quiz practice instructions, Spring 2022\n1h(i) Run a z-test with null hypothesis H0: x_1 is drawn from a normal distribution with\nmean mu0 and known variance sigma0.\nMake the test two-sided.\nUse signifcance level alpha.\nPrint out the z-statistic, p-value and the conclusion of the test.\n1h(ii) Run a t-test with null hypothesis H0: x_1 is drawn from a normal distribution with\nmean mu0 and unknown variance.\nMake the test two-sided.\nUse signifcance level alpha.\nYou can use the R function t.test().\nPrint the t-statistic, p-value and the conclusion of the test.\n1h(iii) Test whether x_1 and x_2 are drawn from normal distributions with the same mean.\nMake the test two-sided.\nAssume equal variances.\nUse significance level alpha.\nPrint the t-statistic, p-value and the conclusion of the test.\n1h(iv) Test whether x_1 and x_2 are drawn from normal distributions with the same\nvariance.\nUse significance level alpha.\nPrint the test statistic, p-value and the conclusion of the test.\nProblem 2\nProblem 2. Here you will finish the code for the function\nrquiz_practice_problem_2(prior, next_roll)\nArguments:\nprior = prior probability for the dice choices (5 numbers, usual order)\nnext_roll = the problem asks for the posterior predictive probability of rolling next_roll\non the next roll.\nRun a complete simulation of randomly choosing a die from our standard Platonic dice (4,\n6, 8, 12, and 20 sided) based on the given prior. Then rolling it once and updating to a\nposterior probability for the type of die chosen.\nPrint out the prior, chosen die, roll and posterior.\nCompute and print out the posterior predictive probability of rolling next_roll on the next\nroll.\nYou can use standard_likelihood_table loaded by the code given in the mit18_05_s22_RQuizPractice.r\nfile.\n\nR quiz practice instructions, Spring 2022\nProblem 3\nThe MIT Administration has decided that MIT students should be able to vote for their\nschool mascot. They are now able to vote between Tim the Beaver, Hack the Hawk, and\nPunt the Platypus.\nProblem 3a. Here you will finish the code for the function\nrquiz_practice_problem_3a(n_rats)\nArguments:\nn_rats = number of Brass Rats Tim has\nOn the morning of his platform speech, Tim the Beaver is deciding which Brass Rats to\nwear. They will wear 2 Brass Rats, one on their left hand, and one on their right hand.\nThey have n_rats Brass Rats to choose from. How many ways can Tim wear two of their\nBrass Rats? (Order matters)\nCompute and print out the number of ways Tim can wear their rings.\nProblem 3b. Here you will finish the code for the function\nrquiz_practice_problem_3b(n_ways)\nArguments:\nn_ways = minimum number of ways Tim wants to be able to wear their Brass Rats.\nHow many Brass Rats would Tim hypothetically need for the number of possibilities to be\nover n_ways? Use a for loop or any other efficient means.\nProblem 4\nContinuing problem 3. In past years fraction of students supporting Tim has been theta_tim.\nAn 18.05 student runs a poll to see if that support has dropped. They poll n_students\nMIT students.\nProblem 4a. Here you will finish the code for the function\nrquiz_practice_problem_4a(n_students, theta_tim)\nArguments:\nn_students = the number of students polled\ntheta_tim = true fraction of the student population that supports Tim\nThe number of students who support Tim follows a certain distribution. Plot this distribu\ntion.\nProblem 4b. Here you will finish the code for the function\nrquiz_practice_problem_4b(n_students, n_support_tim, theta_H0, alpha)\nArguments:\nn_students = the number of students polled.\nn_support_tim = the number of who support Tim in the poll.\ntheta_HO = For H0, the fraction of the student population that supports Tim\nalpha = significance level for the NHST\nTo test the conjecture that Tim's support has dropped below theta_H0, they run a signifi\n\nR quiz practice instructions, Spring 2022\ncance test.\nFor the test, use the data given: n_support and significance level alpha.\nPrint the null and alternative hypotheses.\nPrint the rejection region, p-value and the conclusion of the test.\nProblem 5\nHack the Hawk takes the Number 1 Bus to commute to MIT's campus. The 1 Bus is known\nto be late by Y minutes, where Y follows an exponential distribution with lamba = 0.2\n(units of 1/minutes).\nProblem 5a. Here you will finish the code for the function\nrquiz_practice_problem_5a(lambda)\nArguments:\nlambda = rate parameter for the exponential distribution\nPlot the pdf of Exp(lambda). Use the range 0 to 4/lambda.\nProblem 5b. Here you will finish the code for the function\nrquiz_practice_problem_5b(lambda, n_bus_trips)\nArguments:\nlambda = rate parameter for the exponential distribution\nn_bus_trips = the number of Hack's bus trips to simulate\nHack the Hawk has taken the bus to MIT n_bus_trips times. Simulate the lateness of the\nbus over n_bus_trips days.\n5b(i) Plot a frequency histogram of the simulated data. Use bin_width of 1.0.\n5b(i) Plot a density histogram of the simulated data. Use bin_width of 1.0. Add a plot of\nthe pdf for Exponential(lambda).\nProblem 6\nWork is stressful. To relax, Punt plays roulette at a nearby casino. She plays 10 times every\nday, for 7 days. She bets on red each time. (The roulette wheel has 18 red, 18 black, and 2\ngreen slots)\nIf she loses, her loss is $1. If she wins her gain is $1.\nProblem 6a. Here you will finish the code for the function\nrquiz_practice_problem_6a(n_bets_per_day, n_days)\nArguments:\nn_bets_per_day = number of bets Punt makes each day.\nn_days = number of days Punt plays\nCompute and print the expected value and variance of one bet.\nCompute and print out the expected value and variance one day's winnings.\nCompute and print out the expected value and variance n_days winnings.\n\nR quiz practice instructions, Spring 2022\nProblem 6b. Here you will finish the code for the function\nrquiz_practice_problem_6b(n_bets_per_day, n_days, n_trials)\nArguments:\nn_bets_per_day = number of bets Punt makes each day.\nn_days = number of days Punt plays n_trials = number of simulations of n_days of\nbetting to simulate.\n6b(i) Write R code to simulate Punt's total earnings for n_days. Run n_trials trials\nand plot the density histogram of the results. Add a graph of the normal curve which\napproximates this histogram.\n6b(ii) Explain why the normal distribution used in 6b(i) approximates the density.\nProblem 7\nThis problem is about using Google and R help to solve a problem.\nProblem 7. Here you will finish the code for the function\nrquiz_practice_problem_7(mit_times, harvard_times, alpha)\nArguments:\nmit_times = list of times for the MIT students\nharvard_times = list of times for the Harvard students\nalpha = significance level for NHST\nMIT challenged Harvard to a race to determine which school had, on average, faster runners.\nBoth schools chose some random students and excused them from finals to run the race.\nAfter the race you were called in to analyze the data. Unfortunately, the statistics professors\nat both schools were gone and the only instruction they had left was to use the Wilcoxon\nRank Sum test to see if the mean ranks were the same.\nFigure out how to do this in R and write code that expects two vectors of data mit_times\nand harvard_times containing the finishing time for the students who raced.\nYour code should run the Wilcoxon Rank Sum test and print:\n1. the test statistic,\n2. which if either school appears faster on average,\n3. the p_value of the test.\nYou may use any function you can find in R.\nTesting your code\nFor each problem, we ran the problem function with certain parameters. You can see the\nfunction call and the output in mit18_05_s22_RQuizPractice-test-answers.html. If\nyou call the same function with the same parameters, you should get the same results as in\nmit18_05_s22_RQuizPractice-test-answers.html - if there is randomness involved the\nanswers should be close but not identical.\nFor your convenience, the file mit18_05_s22_RQuizPractice-test.r contains all the func\ntion calls used to make mit18_05_s22_RQuizPractice-test-answers.html.\n\nR quiz practice instructions, Spring 2022\nBefore uploading your code\nYou won't upload this code. But you should still practice what you should do before\nuploading!\n1. Make sure all your code is in your R file. Also make sure it is all inside the functions\nfor the problems.\n2. Clean the environment and plots window.\n3. Source the file.\n4. Call each of the problem functions with the same parameters as the test file\nmit18_05_s22_RQuizPractice-test-answers.html.\n5. Make sure it runs without error and outputs just the answers asked for in the questions.\n6. Compare the output to the answers given in mit18_05_s22_RQuizPractice-test-answers.html.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Summary of NHST",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_exam2_nhst.pdf",
      "content": "Summary of NHST for 18.05\nJeremy Orloff and Jonathan Bloom\nz-test\n- Use: Compare the data mean to an hypothesized mean.\n- Data: x1, x2, ... , xn.\n- Assumptions: The data are independent normal samples:\nxi ∼ N(μ, σ2) where μ is unknown, but σ is known.\n- H0: For a specified μ0, μ = μ0.\n- HA:\nTwo-sided:\nμ = μ0\none-sided-greater:\nμ > μ0\none-sided-less:\nμ < μ0\nx - μ0\n- Test statistic: z = σ/√n\n- Null distribution: φ(z | H0) is the pdf of Z ∼ N(0, 1).\n- p-value:\nTwo-sided:\np = P(|Z| > z | H0)\n=\n2*(1-pnorm(abs(z), 0, 1))\none-sided-greater (right-sided):\np= P(Z > z | H0)\n=\n1 - pnorm(z, 0, 1)\none-sided-less (left-sided):\np= P(Z < z | H0)\n=\npnorm(z, 0, 1)\n- Critical values: zα has right-tail probability α\nP(z> zα| H0) = α ⇔ zα= qnorm(1 -α, 0, 1).\n- Rejection regions: let α be the significance.\nRight-sided rejection region:\n[zα, inf)\nLeft-sided rejection region:\n(-inf, z1-α]\nTwo-sided rejection region:\n(-inf, z1-α/2] ∪ [zα/2, inf)\nAlternate test statistic\n- Test statistic: x\n- Null distribution: φ(x |\n\nH0) is the pdf of X ∼ N(μ0, σ2/n).\n- p-value:\nTwo-sided:\np = P(|X -μ0| > |x-μ0| | H0)\n=\n\n2*(1-pnorm(abs((x - μ0), 0, σ/√n))\none-sided-greater:\np = P(X > x)\n=\n1 - pnorm(x,\nμ0,\nσ/√n)\n\none-sided-less:\np = P(X < x)\n=\npnorm(x, μ0, σ/√n)\n- Critical values: xα has right-tail probability α\nP(X> xα| H0) = α ⇔ xα= qnorm(1 - α, μ0, σ/√n).\n- Rejection regions: let α be the significance.\nRight-sided rejection region:\n[xα, inf)\nLeft-sided rejection region:\n(-inf, x1-α]\nTwo-sided rejection region:\n(-inf, x1-α/2] ∪ [xα/2, inf)\n\nSummary of NHST for 18.03, Spring 2022\nOne-sample t-test of the mean\n- Use: Compare the data mean to an hypothesized mean.\n- Data: x1, x2, ... , xn.\n- Assumptions: The data are independent normal samples:\nxi ∼ N(μ, σ2) where both μ and σ are unknown.\n- H0: For a specified μ0, μ = μ0\n- HA:\nTwo-sided:\nμ = μ0\none-sided-greater:\nμ > μ0\none-sided-less:\nμ < μ0\nx - μ0\n- Test statistic: t = s/√n ,\nn\nwhere s2 is the sample variance:\ns2 =\n∑(xi - x)2\nn - 1 i=1\n- Null distribution: φ(t | H0) is the pdf of T ∼t(n-1).\n(Student t-distribution with n - 1 degrees of freedom)\n- p-value:\nTwo-sided:\np = P(|T | > t)\n=\n2*(1-pt(abs(t), n-1))\none-sided-greater:\np = P(T > t)\n=\n1 - pt(t, n-1)\none-sided-less:\np = P(T < t)\n=\npt(t, n-1)\n- Critical values: tα has right-tail probability α\nP(T> tα| H0) = α ⇔ tα= qt(1 -α, n-1).\nRight-sided rejection region:\n[tα, inf)\n- Rejection regions: let α be the significance. Left-sided rejection region:\n(-inf, t1-α]\nTwo-sided rejection region:\n(-inf, t1-α/2] ∪ [tα/2, inf)\nTwo-sample t-test for comparing means (assuming equal variance)\n- Use: Compare the means from two groups.\n- Data: x1, x2, ... , xn and y1, y2, ... , ym.\n- Assumptions: Both groups of data are independent normal samples:\nxi ∼ N(μx, σ2)\nyj ∼ N(μy, σ2)\nwhere both μx and μy are unknown and possibly different. The variance σ is unknown,\nbut the same for both groups.\n- H0: μx = μy\n- HA:\nTwo-sided:\nμx = μy\none-sided-greater:\nμx > μy\none-sided-less:\nμx < μy\n\nSummary of NHST for 18.03, Spring 2022\nx - y\n- Test statistic: t =\n,\nsP\nwhere s2\nx and s2\ny are the sample variances and s2\nP\nis (sometimes called) the pooled\nsample variance:\n(n - 1)s2\nx + (m - 1)s2\ny\ns2\np =\n( 1\nn+ m-2\nn+ m\n1 )\n- Null distribution: φ(t | H0) is the pdf of T ∼t(n+ m-2).\n(Student t-distribution with n + m - 2 degrees of freedom.)\n- p-value:\nTwo-sided:\np = P(|T | > t)\n=\n2*(1-pt(abs(t), n+m-2))\none-sided-greater:\np = P(T > t)\n=\n1 - pt(t, n+m-2)\none-sided-less:\np = P(T < t)\n=\npt(t, n+m-2)\n- Critical values: tα has right-tail probability α\nP(t> tα| H0) = α ⇔ tα= qt(1 -α, n+ m-2).\n- Rejection regions: let α be the significance.\nRight-sided rejection region:\n[tα, inf)\nLeft-sided rejection region:\n(-inf, t1-α]\nTwo-sided rejection region:\n(-inf, t1-α/2] ∪ [tα/2, inf)\nNotes: 1. Unequal variances. There is a form of the t-test for when the variances are\nnot assumed equal. It is sometimes called Welch's t-test. In the R function t.test, there\nis an argument var.equal. Setting it to FALSE runs the unequal variances version of the\nt-test.\n2. When the data naturally comes in pairs (xi, yi), one uses the paired two-sample t-test.\nFor example, in comparing two treatments, each patient receiving treatment 1 might be\npaired with a patient receiving treatment 2 who is similar in terms of stage of disease, age,\nsex, etc.\n2 test for variance\n- Use: Compare the data variance to an hypothesized variance.\n- Data: x1, x2, ... , xn.\n- Assumptions: The data are independent normal samples:\nxi ∼ N(μ, σ2) where both μ and σ are unknown.\n- H0: For a specified σ0, σ = σ0\n- HA:\nTwo-sided:\nσ = σ0\none-sided-greater:\nσ > σ0\none-sided-less:\nσ < σ0\n(n - 1)s2\nn\n- Test statistic: X2 =\n, where s2 is the sample variance: s2 =\n∑(xi -\nσ0\nn-1\nx)2\ni=1\n\nSummary of NHST for 18.03, Spring 2022\n- Null distribution: φ(X2 | H0) is the pdf of χ2 ∼ χ2(n - 1).\n(Chi-square distribution with n - 1 degrees of freedom)\n- p-value:\nBecause the χ2 distribution is not symmetric around zero the two-sided test is a little\nawkward to write down. The idea is to look at the X2 statistic and see if it's in the\nleft or right tail of the distribution. The p-value is twice the probability in that tail.\nAn easy check for which tail it's in is: s2/σ0\n2 > 1 (right tail) or s2/σ0\n2 < 1 (left tail).\n= {2 ∗P(χ2 > X2)\nif X2 is in the right tail\nTwo-sided:\np\n2 ∗P(χ2 < X2)\nif X2 is in the left tail\n= 2*min(pchisq(X2,n-1), 1-pchisq(X2,n-1))\none-sided-greater:\np\n= P(χ2 > X2) = 1 - pchisq(X2 , n-1)\none-sided-less:\np\n= P(χ2 < X2) = pchisq(X2 , n-1)\n- Critical values: xα has right-tail probability α\nP(χ2 > xα | H0) = α ⇔ xα = qchisq(1 -α, n-1).\n- Rejection regions: let α be the significance.\nRight-sided rejection region:\n[xα, inf)\nLeft-sided rejection region:\n(-inf, x1-α]\nTwo-sided rejection region:\n(-inf, x1-α/2] ∪ [xα/2, inf)\n2 test for goodness of fit for categorical data\n- Use: Test whether discrete data fits a specific finite probability mass function.\n- Data: An observed count Oi in cell i of a table.\n- Assumptions: None\n- H0: The data was drawn from a specific discrete distribution.\n- HA: The data was drawn from a different distribution\n- Test statistic: The data consists of observed counts Oi for each cell. From the null hy\npothesis probability table we get a set of expected counts Ei. There are two statistics\nthat we can use:\nLikelihood ratio statistic G = 2 ∗∑Oi ln (O\nEi\ni )\n= ∑ (Oi - Ei)2\nPearson's chi-square statistic X2\n.\nEi\nIt is a theorem that under the null hypthesis X2 ≈ G and both are approximately\nchi-square. Before computers, X2 was used because it was easier to compute. Now,\nit is better to use G although you will still see X2 used quite often.\n\nSummary of NHST for 18.03, Spring 2022\n- Degrees of freedom df: The number of cell counts that can be freely specified. In the\ncase above, of the n cells n - 1 can be freely specified and the last must be set to\nmake the correct total. So we have df = n-1 degrees of freedom.\nIn other chi-square tests there can be more relations between the cell counts of df\nmight be different from n - 1.\n- Rule of thumb: Combine cells until the expected count in each cell is at least 5.\n- Null distribution: Assuming H0, both statistics (approximately) follow a chi-square\ndistribution with df degrees of freedom. That is both φ(G | H0) and φ(X2 | H0) have\nthe approximately same pdf as Y ∼ χ2(df).\n- p-value:\np\n=\nP(Y > G)\n=\n1 - pchisq(G, df)\np\n=\nP(Y > X2)\n=\n1 - pchisq(X2 , df)\n- Critical values: cα has right-tail probability α\nP(Y > cα| H0) = α ⇔ cα= qchisq(1 - α, df).\n- Rejection regions: let α be the significance.\nWe expect X2 to be small if the fit of the data to the hypothesized distribution is\ngood. So we only use a right-sided rejection region:\n[cα, inf).\nOne-way ANOVA (F -test for equal means)\n- Use: Compare the data means from n groups with m data points in each group.\n- Data:\n... ,\nx1,1,\nx1,2,\nx1,m\n... ,\nx2,1,\nx2,2,\nx2,m\n...\n... ,\nxn,1, xn,2,\nxn,m\n- Assumptions: Data for each group is an independent normal sample drawn from\ndistributions with (possibly) different means but the same variance:\nx1,j\n∼ N(μ1, σ2)\nx2,j\n∼ N(μ2, σ2)\n...\nxn,j\n∼ N(μn, σ2)\nThe group means μi are unknown and possibly different. The variance σ is unknown,\nbut the same for all groups.\n- H0: All the means are identical μ1 = μ2 = ... = μn.\n- HA: Not all the means are the same.\n- Test statistic: f = MSB\nwhere\nMSW ,\n\nSummary of NHST for 18.03, Spring 2022\nxi\n= mean of group i\nxi,1 + xi,2 + ... + xi,m\n=\n.\nm\nx\n= grand mean of all the data.\ns2\ni\n= sample variance of group i\nm\n=\n∑(xi,j - xi )2.\nm - 1 j=1\nMSB\n= between group variance\n= m × sample variance of group means\nn\nm\n=\n∑(xi - x)2.\nn - 1 i=1\nMSW\n= average within group variance\n= sample mean of s2\n1, ... , s2\nn\ns2\n1 + s2\n2 + ... + s2\nn\n=\nn\n- Idea: If the μi are all equal, this ratio should be near 1. If they are not equal then\nMSB should be larger while MSW should remain about the same, so f should be\nlarger. We won't give a proof of this.\n- Null distribution: φ(f | H0) is the pdf of F ∼F(n-1, n(m-1)).\nThis is the F -distribution with (n - 1) and n(m - 1) degrees of freedom. Several\nF -distributions are plotted below.\n- p-value: p = P(F > f) = 1- pf(f, n-1, n*(m-1)))\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nx\nF(3,4)\nF(10,15)\nF(30,15)\nNotes: 1. ANOVA tests whether all the means are the same. It does not test whether\nsome subset of the means are the same.\n2. There is a test where the variances are not assumed equal.\n3. There is a test where the groups don't all have the same number of samples.\nF -test for equal variances\n- Use: Compare the vaiances from two groups.\n- Data: x1, x2, ... , xn and y1, y2, ... , ym.\n- Assumptions: Both groups of data are independent normal samples:\nxi ∼ N(μx, σx\n2)\nyj ∼ N(μy, σy\n2)\n\nSummary of NHST for 18.03, Spring 2022\nwhere μx, μy, σx and σy are all unknown.\n- H0: σx = σy\n- HA:\nTwo-sided:\nσx = σy\none-sided-greater:\nσx > σy\none-sided-less:\nσx < σy\ns2\nx\n- Test statistic: f =\n,\ns2y\nwhere s2\nx and s2\ny are the sample variances of the data.\n- Null distribution: φ(f | H0) is the pdf of F ∼F(n-1, m-1).\n(F -distribution with n - 1 and m - 1 degrees of freedom.)\n- p-value:\nTwo-sided:\np\n=\n2*min(pf(f,n-1,m-1), 1-pf(f, n-1,m-1))\none-sided-greater:\np = P(F > f)\n=\n1 - pf(f, n-1, m-1)\none-sided-less:\np = P(F < f)\n=\npf(f, n-1, m-1)\n- Critical values: fα has right-tail probability α\nP(F> fα| H0) = α ⇔ fα= qf(1 -α, n-1, m-1).\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Exam 1 Review List",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_exam1_rev_list.pdf",
      "content": "In-class Exam 1 Review List\n18.05, Spring 2022\nList of topics\n1. Sets.\n2. Counting.\n3. Sample space, outcome, event, probability function.\n4. Probability: conditional probability, independence, Bayes' theorem.\n5. Discrete random variables: events, pmf, cdf.\n6. Bernoulli(p), binomial(n, p), geometric(p), uniform(n)\n7. E[X], Var(X), σ\n8. Continuous random variables: pdf, cdf.\n9. uniform(a,b), exponential(λ), normal(μ,σ2)\n10. Transforming random variables.\n11. Quantiles.\n12. Central limit theorem, law of large numbers, histograms.\n13. Joint distributions: pmf, pdf, cdf, covariance and correlation.\n0.1 Sets and counting\n- Sets:\n∅, union, intersection, complement Venn diagrams, products\n- Counting:\ninclusion-exclusion, rule of product,\n= (n\npermutations nPk, combinations nCk\nk)\n0.2 Probability\n- Sample space, outcome, event, probability function. Rule: P(A∪B) = P(A)+P(B)-\nP(A ∩ B).\nSpecial case: P(Ac) = 1 -P(A)\n(A and B disjoint ⇒ P(A∪B) = P(A) + P(B).)\n- Conditional probability, multiplication rule, trees, law of total probability, indepen\ndence\n- Bayes' theorem, base rate fallacy\n\nExam 1 review list, Spring 2022\n0.3 Random variables, expectation and variance\n- Discrete random variables: events, pmf, cdf\n- Bernoulli(p), binomial(n, p), geometric(p), uniform(n)\n- E[X], meaning, algebraic properties, E[h(X)]\n- Var(X), meaning, algebraic properties\n- Continuous random variables: pdf, cdf\n- uniform(a,b), exponential(λ), normal(μ,σ)\n- Transforming random variables\n- Quantiles\n0.4 Central limit theorem\n- Law of large numbers averages and histograms\n- Central limit theorem\n0.5 Joint distributions\n- Joint pmf, pdf, cdf.\n- Marginal pmf, pdf, cdf\n- Independence\n- Covariance and correlation.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Final Exam",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_exam_final.pdf",
      "content": "Name\n18.05 Final Exam Spring 2022\nNo calculators. You may bring one 8 × 11 cheat sheet.\nDo your work directly on the exam.\nNumber of problems\n18 concept questions, 13 problems\nTest format\nThe test is divided into two parts. The first part is a series of concept questions. You don't\nneed to show any work on this part. The second part consists of standard problems. You\nneed to show your work on these.\nScores\nShow your work\nCQ.\n/70\nFor the part II problems you must show your reasoning to get credit. If you\nneed extra paper, we can provide some. Indicate clearly that your solution\n1.\n/15\nis continued on a separate page and write your name on the extra page.\n2.\n/16\nSimplifying expressions\nUnless asked to explicitly, you don't need to simplify complicated expres\n3.\n/25\nsions. For example, you can leave\n⋅3 + 1 ⋅5 exactly as is. Likewise for\nexpressions like 20!\n18!2!.\nTables\nThere are z, t and χ2 tables at the end of the exam.\nGood luck!\n4.\n/20\n5.\n/10\n6.\n/15\n7.\n/30\n8.\n/20\n9.\n/20\n10.\n/20\n11.\n/10\n12.\n/20\n13.\n/15\n/306\n\n18.05 Final Exam Spring 2022\nPart I: Concept questions (70 points)\nThese questions are all multiple choice or short answer. You don't have to show any work.\nWork through them quickly.\nConcept 1. (4 pts.) Which of the following is a valid probability table?\n(i)\noutcome\n1.5\n2.5\n3.5\nprobability\n1/5\n1/5\n1/5\n1/5\n1/5\n1/5\n(ii)\noutcome\nred\nblue\ngreen\ncyan\nyellow\nprobability\n4/10\n2/10\n1/10\n3/10\nCircle the best choice:\n(i)\n(ii)\n(i) and (ii)\nneither (i) nor (ii)\nConcept 2. (4 pts.) Suppose P(A) + P(B) > 1. Consider the following statements.\n(i) P(A∪B) = 1.\n(ii) P(A∩B) > 0.\nWhich must be true? Circle the best choice below:\n(i)\n(ii)\n(i) and (ii)\nneither (i) nor (ii).\nConcept 3. (6 pts.) Circle True or False for each of the following.\n(a) If A and B are independent then we must have P (A ∩ B) = P (A)P (B).\nTrue\nFalse\n(b) If A and B are independent then we must have P(A∩B) = P(A)+P(B).\nTrue\nFalse\n(c) If A and B are disjoint then A and B must be independent.\nTrue\nFalse\nConcept 4. (4 pts.)\nYou believe the MBTA subway arrives late by X hours, where X\nfollows an exponential distribution with unknown parameter λ. To test your hypothesis,\nyou record the lateness of 5 subway trains and get data x1, x2, ... , x5. Which of the following\nare statistics? Circle the correct answers.\n(a) The expected value of a sample, namely 1/λ.\n(b) The sample average, x = (x1 + x2 + x3 + x4 + x5)/5.\n(c) The difference between x and 1/λ.\n(d) The sample standard deviation.\nConcept 5. (3 pts.) For each of the following, circle it if it is used in Bayesian inference.\n(a) Likelihood function\n(b) prior odds\n(c) p-value\nConcept 6. (4 pts.)\nSuppose X ∼ Bernoulli(θ), where θ is unknown. Complete the\nfollowing sentence using the words, \"discrete,\" \"continuous,\" or \"neither discrete nor con\ntinuous.\"\nThe random variable is\n, the space of hypotheses is\n.\n\n18.05 Final Exam Spring 2022\nConcept 7. (2 pts.) A casino is considering installing a new slot machine. A player who\nwins is paid $2 on a $1 bet. The manufacturer claims that the probability of winning on\nany play of the slot machine is p = 0.48. Before using the machine the casino wants to make\nsure it will make them money. So they hire you to test the slot machine. Which of the\nfollowing hypotheses would you use?\n(i) H0 ∶p= 0.48 vs HA ∶ p = 0.48\n(ii) H0 ∶p= 0.48 vs HA ∶ p > 0.48\n(iii) H0 ∶p= 0.48 vs HA ∶ p < 0.48\nCircle the best answer:\n(i)\n(ii)\n(iii)\nNot enough information.\nConcept 8. (6 pts.)\nThe following are hypotheses considered in the previous problem.\nFor each hypothesis circle all that apply.\n(a) H0 ∶p= 0.48\nSimple\nComposite\nTwo-sided\nOne-sided\n(b) HA ∶ p = 0.48\nSimple\nComposite\nTwo-sided\nOne-sided\n(c) HA ∶ p > 0.48\nSimple\nComposite\nTwo-sided\nOne-sided\nConcept 9. (5 pts.)\nWhich of the following are true about p-values? Circle all that\napply.\n(a) The p-value gives the probability of making a type 1 error.\n(b) The p-value is a measure of how extreme the observed data is.\n(c) A p-value below the significance level allows us to conclude with certainty that the null\nhypothesis is false.\n(d) The p-value is a frequentist concept.\n(e) If the null hypothesis is true, then the p-value will always be larger than the significance\nlevel.\nConcept 10. (8 pts.)\nA two-sample t-test for equal means of two populations has a\np-value of 0.08.\nCircle True or False for each of the following.\n(a) For a significance level of 0.05, the null hypothesis of equal means should be rejected.\nTrue\nFalse\n(b) A 90% confidence interval for the difference of the means for the two populations\nincludes 0.\nTrue\nFalse\n(c) A 95% confidence interval for the difference of the means for the two populations includes\n0.\nTrue\nFalse\n(d) With probability 95% the actual value of the difference of the means is within the 95%\nt-confidence interval for the difference.\nTrue\nFalse\n\n18.05 Final Exam Spring 2022\nConcept 11. (2 pts.) Data is drawn from a N(1, σ2) distribution. Let X5 be the average\nof 5 data points and X10 the average of 10 data points. Three densities, f1, f2, f3 are shown\nbelow. One is the pdf of X5, one is the pdf of X10 and one is another pdf. Circle the one\nthat is the pdf of X10.\nx\nf1(x)\nf2(x)\nf3(x)\nConcept 12. (4 pts.)\nFinish the following sentences with \"a type I error\", \"a type II\nerror\", or \"neither type of error\".\n(a) The rejection of a false null hypothesis is\n(b) The rejection of a true null hypothesis is\nConcept 13. (8 pts.) Suppose that the data x1, x2, ..., xn are drawn from independent,\nidentically distributed, random variables Xi with mean μ and standard deviation σ. Write\nx = (x1 + ⋯ + xn)/n for the sample mean. The Central Limit Theorem states that: (circle\nall that apply)\n(a) The distribution of each random variable Xi is approximately symmetric around the\naverage μ.\n(b) For large n, the distribution of the sample mean is approximately symmetric around\nthe average μ.\n(c) For large n, the average x approximately follows a normal distribution.\n(d) For large n, (x - μ)/σ approximately follows a standard normal distribution.\nConcept 14. (2 pts.)\nSuppose for a certain endeavor θ is the probability of success.\nSuppose also that our prior for θ is Beta(5,5). We collect data from 30 trials, obtaining 20\nsuccesses and 10 failures. What is our posterior pdf f(θ|x)? (Circle the best answer.)\n(i) Binomial(30, 2/3)\n(ii) Beta(25, 15)\n(iii) Beta(20, 10)\n(iv) None of the above\nConcept 15. (2 pts.) Circle True or False.\nLet s be a statistic. If the theoretical distribution of the statistic is hard to compute, then\nit is not advisable to use bootstrapping to compute confidence intervals for the statistic.\nTrue\nFalse\nConcept 16. (2 pts.) Suppose we run a two-sample t-test for equal means with significance\nlevel α = 0.05. If the data implies we should reject the null hypothesis, then the odds that\nthe two samples come from distributions with the same mean are (circle the best answer)\n\n18.05 Final Exam Spring 2022\n(a) 19/1\n(b) 1/19\n(c) 20/1\n(d) 1/20\n(e) unknown\nConcept 17. (2 pts.) For the bivariate data in the following scatter plot, is the correlation\nbetween x and y positive or negative? Circle your choice:\npositive\nnegative\nx\ny\nConcept 18. (2 pts.) Circle True or False\nLinear regression can fit curves other than lines to given data.\nTrue\nFalse\n\n18.05 Final Exam Spring 2022\nPart II: Problems (236 points)\nProblem 1. (15: 5,5,5)\nYou roll a fair six-sided die 8 times.\n(a) What is the probability that none of the 8 rolls is a six?\n(b) What is the probability that exactly one of the 8 rolls is a six?\n(c) What is the expected number of sixes in the 8 rolls?\nRemember: you can leave numerical expressions unevaluated\n\n18.05 Final Exam Spring 2022\nProblem 2. (16: 4,4,4,4)\nSuppose X is a random variable with values in [1,2] and density\nfor 1 ≤ x ≤ 2\nf(x) = {kx2\notherwise\nwhere k is a fixed constant.\n(a) What is k?\n(b) Find the cdf of X.\n(c) Find P (X < 3/2).\n(d) Find E[X]\n\n18.05 Final Exam Spring 2022\nProblem 3. (25: 5,5,5,5,5)\nSuppose random variables X and Y have units of dollars and:\nE[X] = 5,\nVar(X) = 62,\nand\nE[Y ] = 10,\nVar(Y) = 72.\nDefine W = X+ Y .\n(a) Find E[W ]. What are the units of E[W ]?\n(b) Assume X and Y are independent. Find Var(W ).\n(c) Assume Cov(X, Y) = 21. Find Var(W ). (Note: this assumption differs from that in\npart (b), so Var(W) can also be different.)\n(d) Assume Cov(X, Y) = 21. Compute Cor(X, Y ). What are the units of Cor(X, Y )?\n(X - E[X])\n(e) Define Z=\n= (X-5)/6. Find E[Z] and Var(Z).\n√Var(X)\n\n18.05 Final Exam Spring 2022\nProblem 4. (20: 10,5,5)\nYou roll two fair (6-sided) dice. If the sum of the dice is greater than 9, you win $100. If\nthe sum is 9 or less you get to roll again. On the second roll, if your sum of dice is greater\nthan 9, you win $50, otherwise you win nothing. Let X be the random variable of how\nmuch money you win by playing this game.\n(a) Construct a probability model for X, i.e. make a probability table.\n(b) What is the expected amount you will win playing the game?\n(c) What would you be willing to pay to play this game? (Justify your answer).\n\n18.05 Final Exam Spring 2022\nProblem 5. (10)\nThe Pareto distribution is used in economics modeling. To keep it simple we'll use the\nPareto distribution that takes values x ≥ 1 and has pdf\nf(x ∣ θ) = θx-θ-1\nfor x ≥ 1.\nIt's defined whenever θ > 0. Assume x1, ... , xn are n independent samples from a Pareto(θ)\ndistribution, find the maximum likelihood estimate of θ.\n\n18.05 Final Exam Spring 2022\nProblem 6. (15: 5,10)\nA certain medical condition exists in 1% of the population. A screening test for the condition\nhas a 4% false positive rate and a 0% false negative rate.\n(a) What are the odds that a random person has the condition?\n(b) Suppose a random person tests positive for the condition. What are the odds they\nhave the condition?\n\n18.05 Final Exam Spring 2022\nProblem 7. (30: 5,10,5,10 )\nThis question is about a robot, Bayz-E, who tries to figure out where it is located using\nBayesian updating. Bayz-E is randomly placed in front of one of four doors, A, B, C, or\nD. At every time step, Bayz-E scans the color of the door in front of it. The outcome\nwill be either orange of blue. However, its color scanner is not entirely accurate. Bayz-E\nscans the door's color correctly with a probability 0.7, and scans the color incorrectly with\na probability 0.3.\nNote. To avoid confusion for you, the color of the door is written above its letter. (Bayz-E\nhas not yet learned to read, though its machine learning algorithm is working on it.)\nA\norange\nB\norange\nC\nblue\nD\nblue\n(a) Bayz-E is placed in front of door A, B, C, or D at random (i.e. the probability of each\ndoor is the same). What is the prior probability it is in front of door B?\n(b) Bayz-E scans the door color and detects \"orange.\" What is the posterior probability\nit is in front of door B?\n(c) After computing the posterior probabilities, Bayz-E moves to the next door to the\nright. (if it was at door D, it moves to door A). What is the (new) prior probability it is\nnow in front of door C? (That is, after detecting orange in part (b) and after moving one\nto door to the right.)\n(d) What is the predictive probability that the color scan of this door (after the move in\npart (c)) will detect \"blue\"?\n\n18.05 Final Exam Spring 2022\nProblem 8. (20)\nAccording to the Mars website, each packet of Milk Chocolate M&M's should contain 20%\nblue, 20% brown, 20% green, 15% orange, 15% red, and 10% yellow M&M's.\nAlessandre decides to test this claim. She buys 20 packets of Milk Chocolate M&M's. Each\npacket has 50 M&M's, so Alessandre has a total of 1000 M&M's. She counts each color and\nobserves the following counts of M&M's.\nblue\nbrown\ngreen\norange\nred\nyellow\ntotal\nObserved count\nRun a hypothesis test at the 0.05 significance level to test whether the published Mars color\ndistribution is correct. Carefully state what you are testing and your conclusion.\n(Write down the full numerical expression for your test statistic. In order to use the tables\nyou will need to estimate the value of the test statistic. There is no need to compute it in\nfull precision.)\n\n18.05 Final Exam Spring 2022\nProblem 9. (20: 5,10,5)\nJerry wants to brag to his non-MIT colleagues about how smart MIT students are. To\ngive himself credibility, he decides to run a statistical test comparing the IQ scores of MIT\nstudents and Harvard students.\nHe collects IQ scores from 11 MIT students. The data has a sample mean of 115, with a\nsample standard deviation of 8.\nHe then collects IQ scores from 11 Harvard students. Their scores have a sample mean of\n110, with a sample standard deviation of 6.\n(a) Which test should he run to compare the IQ scores from the two schools? What\nassumptions will he need to make? What are the null and alternative hypotheses?\n(b) Run the test with a significance level of α = 0.05. Should Jerry reject the null\nhypothesis or not?\n(In this problem there is some arithmetic. You will want to use √100/11 ≈\n√\n9 = 3.)\n(c) Estimate the 95% confidence interval for the IQ of MIT students.\n(Your answer should be a numerical expression. There is no need to work it out all the way\nto a decimal answer.)\n\n18.05 Final Exam Spring 2022\nProblem 10. (20: 10,10)\nMIT has decided to form a new Department of Statistics and Probability. In a vote for\nthe new head of this department, suppose 50% of the MIT population supports Sarah, 20%\nsupports So Hee, and the remaining 30% is split evenly among Jerry, Jen, Alessandre and\nGabe.\n(a) A poll asks 100 random people whom they support. Estimate the probability that at\nleast 45% of those polled support Sarah.\n(b) A poll of n people reports that 53% ± 5% support Sarah at the 95% confidence level.\nWhat is the value of n?\n\n18.05 Final Exam Spring 2022\nProblem 11. (10)\nYou independently draw 100 data points from a N(μ, 1) distribution, where μ is unknown.\nSuppose you test the null hypothesis H0 ∶ μ = 0 against the alternative hypothesis HA ∶\nμ = 0 using a significance level of α = 0.05. What is the power of the test for the alternative\nHA ∶μ= 0.4?\n\n18.05 Final Exam Spring 2022\nProblem 12. (20: 5,10,5)\nBivariate data (4, 1), (-2, 1.5), (0, 0.5) is assumed to arise from the model yi = b|xi-2|+ei,\nwhere b is a constant and ei are independent random variables.\n(a) What assumptions are needed on ei so that it makes sense to do a least squares fit of\na curve y = b|x-2| to the data?\n(b) Given the above data and the assumptions from part (a), determine the least squares\nestimate for b.\n(c) Make a graph showing the data points and your least squares fit curve.\n\n18.05 Final Exam Spring 2022\nProblem 13. (15)\nData is collected on the time between trades at a stock exchange. We collect a data set of\nsize 36 with sample mean x = 7.0 and sample standard deviation s = 0.84.\nMake no assumptions about the distribution of the data. By bootstrapping, we generate\n500 bootstrap means x∗ . The smallest 50 and largest 50 are written in non-decreasing order\nbelow, e.g. the 12th smallest value is 6.672.\nUse this data to find an 90% percentile bootstrap confidence interval for μ.\n1- 10\n6.466\n6.506\n6.509\n6.515\n6.578\n6.597\n6.618\n6.635\n6.653\n6.664\n11- 20\n6.670\n6.672\n6.685\n6.696\n6.703\n6.707\n6.713\n6.721\n6.727\n6.727\n21- 30\n6.729\n6.731\n6.738\n6.738\n6.740\n6.743\n6.744\n6.745\n6.751\n6.752\n31- 40\n6.759\n6.760\n6.768\n6.774\n6.775\n6.777\n6.778\n6.780\n6.784\n6.784\n41- 50\n6.787\n6.789\n6.789\n6.790\n6.791\n6.791\n6.792\n6.796\n6.798\n6.800\n451- 460\n7.170\n7.172\n7.172\n7.175\n7.178\n7.179\n7.180\n7.181\n7.182\n7.182\n461- 470\n7.182\n7.186\n7.195\n7.202\n7.202\n7.205\n7.206\n7.210\n7.216\n7.219\n471- 480\n7.220\n7.220\n7.221\n7.222\n7.224\n7.225\n7.232\n7.232\n7.236\n7.236\n481- 490\n7.243\n7.244\n7.245\n7.251\n7.253\n7.258\n7.261\n7.263\n7.266\n7.273\n491- 500\n7.274\n7.288\n7.288\n7.291\n7.307\n7.312\n7.314\n7.316\n7.348\n7.488\n\n18.05 Final Exam Spring 2022\nExtra Paper\n\n18.05 Final Exam Spring 2022\nStandard normal table of left tail probabilities.\nz\nz\nz\nz\nΦ(z) = P (Z ≤ z) for N(0, 1).\n(Use interpolation to estimate\nz values to a 3rd decimal\nplace.)\nΦ(z)\nΦ(z)\nΦ(z)\nΦ(z)\n-4.00\n0.0000\n-2.00\n0.0228\n0.00\n0.5000\n2.00\n0.9772\n-3.95\n0.0000\n-1.95\n0.0256\n0.05\n0.5199\n2.05\n0.9798\n-3.90\n0.0000\n-1.90\n0.0287\n0.10\n0.5398\n2.10\n0.9821\n-3.85\n0.0001\n-1.85\n0.0322\n0.15\n0.5596\n2.15\n0.9842\n-3.80\n0.0001\n-1.80\n0.0359\n0.20\n0.5793\n2.20\n0.9861\n-3.75\n0.0001\n-1.75\n0.0401\n0.25\n0.5987\n2.25\n0.9878\n-3.70\n0.0001\n-1.70\n0.0446\n0.30\n0.6179\n2.30\n0.9893\n-3.65\n0.0001\n-1.65\n0.0495\n0.35\n0.6368\n2.35\n0.9906\n-3.60\n0.0002\n-1.60\n0.0548\n0.40\n0.6554\n2.40\n0.9918\n-3.55\n0.0002\n-1.55\n0.0606\n0.45\n0.6736\n2.45\n0.9929\n-3.50\n0.0002\n-1.50\n0.0668\n0.50\n0.6915\n2.50\n0.9938\n-3.45\n0.0003\n-1.45\n0.0735\n0.55\n0.7088\n2.55\n0.9946\n-3.40\n0.0003\n-1.40\n0.0808\n0.60\n0.7257\n2.60\n0.9953\n-3.35\n0.0004\n-1.35\n0.0885\n0.65\n0.7422\n2.65\n0.9960\n-3.30\n0.0005\n-1.30\n0.0968\n0.70\n0.7580\n2.70\n0.9965\n-3.25\n0.0006\n-1.25\n0.1056\n0.75\n0.7734\n2.75\n0.9970\n-3.20\n0.0007\n-1.20\n0.1151\n0.80\n0.7881\n2.80\n0.9974\n-3.15\n0.0008\n-1.15\n0.1251\n0.85\n0.8023\n2.85\n0.9978\n-3.10\n0.0010\n-1.10\n0.1357\n0.90\n0.8159\n2.90\n0.9981\n-3.05\n0.0011\n-1.05\n0.1469\n0.95\n0.8289\n2.95\n0.9984\n-3.00\n0.0013\n-1.00\n0.1587\n1.00\n0.8413\n3.00\n0.9987\n-2.95\n0.0016\n-0.95\n0.1711\n1.05\n0.8531\n3.05\n0.9989\n-2.90\n0.0019\n-0.90\n0.1841\n1.10\n0.8643\n3.10\n0.9990\n-2.85\n0.0022\n-0.85\n0.1977\n1.15\n0.8749\n3.15\n0.9992\n-2.80\n0.0026\n-0.80\n0.2119\n1.20\n0.8849\n3.20\n0.9993\n-2.75\n0.0030\n-0.75\n0.2266\n1.25\n0.8944\n3.25\n0.9994\n-2.70\n0.0035\n-0.70\n0.2420\n1.30\n0.9032\n3.30\n0.9995\n-2.65\n0.0040\n-0.65\n0.2578\n1.35\n0.9115\n3.35\n0.9996\n-2.60\n0.0047\n-0.60\n0.2743\n1.40\n0.9192\n3.40\n0.9997\n-2.55\n0.0054\n-0.55\n0.2912\n1.45\n0.9265\n3.45\n0.9997\n-2.50\n0.0062\n-0.50\n0.3085\n1.50\n0.9332\n3.50\n0.9998\n-2.45\n0.0071\n-0.45\n0.3264\n1.55\n0.9394\n3.55\n0.9998\n-2.40\n0.0082\n-0.40\n0.3446\n1.60\n0.9452\n3.60\n0.9998\n-2.35\n0.0094\n-0.35\n0.3632\n1.65\n0.9505\n3.65\n0.9999\n-2.30\n0.0107\n-0.30\n0.3821\n1.70\n0.9554\n3.70\n0.9999\n-2.25\n0.0122\n-0.25\n0.4013\n1.75\n0.9599\n3.75\n0.9999\n-2.20\n0.0139\n-0.20\n0.4207\n1.80\n0.9641\n3.80\n0.9999\n-2.15\n0.0158\n-0.15\n0.4404\n1.85\n0.9678\n3.85\n0.9999\n-2.10\n0.0179\n-0.10\n0.4602\n1.90\n0.9713\n3.90\n1.0000\n-2.05\n0.0202\n-0.05\n0.4801\n1.95\n0.9744\n3.95\n1.0000\n\n18.05 Final Exam Spring 2022\nTable of Student t critical values (right-tail)\nThe table shows tdf, p = the 1 - p quantile of t(df).\nWe only give values for p ≤ 0.5. Use symmetry to find the values for p > 0.5, e.g.\nt5, 0.975 = -t5, 0.025\nIn R notation tdf, p = qt(1-p, df).\ndf\\p\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.040\n0.050\n0.100\n0.200\n0.300\n0.400\n0.500\n63.66\n31.82\n21.20\n15.89\n12.71\n10.58\n7.92\n6.31\n3.08\n1.38\n0.73\n0.32\n0.00\n9.92\n6.96\n5.64\n4.85\n4.30\n3.90\n3.32\n2.92\n1.89\n1.06\n0.62\n0.29\n0.00\n5.84\n4.54\n3.90\n3.48\n3.18\n2.95\n2.61\n2.35\n1.64\n0.98\n0.58\n0.28\n0.00\n4.60\n3.75\n3.30\n3.00\n2.78\n2.60\n2.33\n2.13\n1.53\n0.94\n0.57\n0.27\n0.00\n4.03\n3.36\n3.00\n2.76\n2.57\n2.42\n2.19\n2.02\n1.48\n0.92\n0.56\n0.27\n0.00\n3.71\n3.14\n2.83\n2.61\n2.45\n2.31\n2.10\n1.94\n1.44\n0.91\n0.55\n0.26\n0.00\n3.50\n3.00\n2.71\n2.52\n2.36\n2.24\n2.05\n1.89\n1.41\n0.90\n0.55\n0.26\n0.00\n3.36\n2.90\n2.63\n2.45\n2.31\n2.19\n2.00\n1.86\n1.40\n0.89\n0.55\n0.26\n0.00\n3.25\n2.82\n2.57\n2.40\n2.26\n2.15\n1.97\n1.83\n1.38\n0.88\n0.54\n0.26\n0.00\n3.17\n2.76\n2.53\n2.36\n2.23\n2.12\n1.95\n1.81\n1.37\n0.88\n0.54\n0.26\n0.00\n2.92\n2.58\n2.38\n2.24\n2.12\n2.02\n1.87\n1.75\n1.34\n0.86\n0.54\n0.26\n0.00\n2.90\n2.57\n2.37\n2.22\n2.11\n2.02\n1.86\n1.74\n1.33\n0.86\n0.53\n0.26\n0.00\n2.88\n2.55\n2.36\n2.21\n2.10\n2.01\n1.86\n1.73\n1.33\n0.86\n0.53\n0.26\n0.00\n2.86\n2.54\n2.35\n2.20\n2.09\n2.00\n1.85\n1.73\n1.33\n0.86\n0.53\n0.26\n0.00\n2.85\n2.53\n2.34\n2.20\n2.09\n1.99\n1.84\n1.72\n1.33\n0.86\n0.53\n0.26\n0.00\n2.83\n2.52\n2.33\n2.19\n2.08\n1.99\n1.84\n1.72\n1.32\n0.86\n0.53\n0.26\n0.00\n2.82\n2.51\n2.32\n2.18\n2.07\n1.98\n1.84\n1.72\n1.32\n0.86\n0.53\n0.26\n0.00\n2.81\n2.50\n2.31\n2.18\n2.07\n1.98\n1.83\n1.71\n1.32\n0.86\n0.53\n0.26\n0.00\n2.80\n2.49\n2.31\n2.17\n2.06\n1.97\n1.83\n1.71\n1.32\n0.86\n0.53\n0.26\n0.00\n2.79\n2.49\n2.30\n2.17\n2.06\n1.97\n1.82\n1.71\n1.32\n0.86\n0.53\n0.26\n0.00\n2.75\n2.46\n2.28\n2.15\n2.04\n1.95\n1.81\n1.70\n1.31\n0.85\n0.53\n0.26\n0.00\n2.74\n2.45\n2.27\n2.14\n2.04\n1.95\n1.81\n1.70\n1.31\n0.85\n0.53\n0.26\n0.00\n2.74\n2.45\n2.27\n2.14\n2.04\n1.95\n1.81\n1.69\n1.31\n0.85\n0.53\n0.26\n0.00\n2.73\n2.44\n2.27\n2.14\n2.03\n1.95\n1.81\n1.69\n1.31\n0.85\n0.53\n0.26\n0.00\n2.73\n2.44\n2.27\n2.14\n2.03\n1.95\n1.80\n1.69\n1.31\n0.85\n0.53\n0.26\n0.00\n2.72\n2.44\n2.26\n2.13\n2.03\n1.94\n1.80\n1.69\n1.31\n0.85\n0.53\n0.26\n0.00\n2.70\n2.42\n2.25\n2.12\n2.02\n1.94\n1.80\n1.68\n1.30\n0.85\n0.53\n0.26\n0.00\n2.70\n2.42\n2.25\n2.12\n2.02\n1.93\n1.80\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.70\n2.42\n2.25\n2.12\n2.02\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.70\n2.42\n2.24\n2.12\n2.02\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.69\n2.41\n2.24\n2.12\n2.02\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.69\n2.41\n2.24\n2.12\n2.01\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.69\n2.41\n2.24\n2.11\n2.01\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.68\n2.41\n2.24\n2.11\n2.01\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.68\n2.41\n2.24\n2.11\n2.01\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.68\n2.40\n2.24\n2.11\n2.01\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n\n18.05 Final Exam Spring 2022\nTable of χ2 critical values (right-tail)\nThe table shows cdf, p = the 1 - p quantile of χ2(df).\nIn R notation cdf, p = qchisq(1-p, df).\ndf\\p\n0.010\n0.025\n0.050\n0.100\n0.200\n0.300\n0.500\n0.700\n0.800\n0.900\n0.950\n0.975\n0.990\n6.63\n5.02\n3.84\n2.71\n1.64\n1.07\n0.45\n0.15\n0.06\n0.02\n0.00\n0.00\n0.00\n9.21\n7.38\n5.99\n4.61\n3.22\n2.41\n1.39\n0.71\n0.45\n0.21\n0.10\n0.05\n0.02\n11.34\n9.35\n7.81\n6.25\n4.64\n3.66\n2.37\n1.42\n1.01\n0.58\n0.35\n0.22\n0.11\n13.28\n11.14\n9.49\n7.78\n5.99\n4.88\n3.36\n2.19\n1.65\n1.06\n0.71\n0.48\n0.30\n15.09\n12.83\n11.07\n9.24\n7.29\n6.06\n4.35\n3.00\n2.34\n1.61\n1.15\n0.83\n0.55\n16.81\n14.45\n12.59\n10.64\n8.56\n7.23\n5.35\n3.83\n3.07\n2.20\n1.64\n1.24\n0.87\n18.48\n16.01\n14.07\n12.02\n9.80\n8.38\n6.35\n4.67\n3.82\n2.83\n2.17\n1.69\n1.24\n20.09\n17.53\n15.51\n13.36\n11.03\n9.52\n7.34\n5.53\n4.59\n3.49\n2.73\n2.18\n1.65\n21.67\n19.02\n16.92\n14.68\n12.24\n10.66\n8.34\n6.39\n5.38\n4.17\n3.33\n2.70\n2.09\n23.21\n20.48\n18.31\n15.99\n13.44\n11.78\n9.34\n7.27\n6.18\n4.87\n3.94\n3.25\n2.56\n32.00\n28.85\n26.30\n23.54\n20.47\n18.42\n15.34\n12.62\n11.15\n9.31\n7.96\n6.91\n5.81\n33.41\n30.19\n27.59\n24.77\n21.61\n19.51\n16.34\n13.53\n12.00\n10.09\n8.67\n7.56\n6.41\n34.81\n31.53\n28.87\n25.99\n22.76\n20.60\n17.34\n14.44\n12.86\n10.86\n9.39\n8.23\n7.01\n36.19\n32.85\n30.14\n27.20\n23.90\n21.69\n18.34\n15.35\n13.72\n11.65\n10.12\n8.91\n7.63\n37.57\n34.17\n31.41\n28.41\n25.04\n22.77\n19.34\n16.27\n14.58\n12.44\n10.85\n9.59\n8.26\n38.93\n35.48\n32.67\n29.62\n26.17\n23.86\n20.34\n17.18\n15.44\n13.24\n11.59\n10.28\n8.90\n40.29\n36.78\n33.92\n30.81\n27.30\n24.94\n21.34\n18.10\n16.31\n14.04\n12.34\n10.98\n9.54\n41.64\n38.08\n35.17\n32.01\n28.43\n26.02\n22.34\n19.02\n17.19\n14.85\n13.09\n11.69\n10.20\n42.98\n39.36\n36.42\n33.20\n29.55\n27.10\n23.34\n19.94\n18.06\n15.66\n13.85\n12.40\n10.86\n44.31\n40.65\n37.65\n34.38\n30.68\n28.17\n24.34\n20.87\n18.94\n16.47\n14.61\n13.12\n11.52\n50.89\n46.98\n43.77\n40.26\n36.25\n33.53\n29.34\n25.51\n23.36\n20.60\n18.49\n16.79\n14.95\n52.19\n48.23\n44.99\n41.42\n37.36\n34.60\n30.34\n26.44\n24.26\n21.43\n19.28\n17.54\n15.66\n53.49\n49.48\n46.19\n42.58\n38.47\n35.66\n31.34\n27.37\n25.15\n22.27\n20.07\n18.29\n16.36\n54.78\n50.73\n47.40\n43.75\n39.57\n36.73\n32.34\n28.31\n26.04\n23.11\n20.87\n19.05\n17.07\n56.06\n51.97\n48.60\n44.90\n40.68\n37.80\n33.34\n29.24\n26.94\n23.95\n21.66\n19.81\n17.79\n57.34\n53.20\n49.80\n46.06\n41.78\n38.86\n34.34\n30.18\n27.84\n24.80\n22.47\n20.57\n18.51\n63.69\n59.34\n55.76\n51.81\n47.27\n44.16\n39.34\n34.87\n32.34\n29.05\n26.51\n24.43\n22.16\n64.95\n60.56\n56.94\n52.95\n48.36\n45.22\n40.34\n35.81\n33.25\n29.91\n27.33\n25.21\n22.91\n66.21\n61.78\n58.12\n54.09\n49.46\n46.28\n41.34\n36.75\n34.16\n30.77\n28.14\n26.00\n23.65\n67.46\n62.99\n59.30\n55.23\n50.55\n47.34\n42.34\n37.70\n35.07\n31.63\n28.96\n26.79\n24.40\n68.71\n64.20\n60.48\n56.37\n51.64\n48.40\n43.34\n38.64\n35.97\n32.49\n29.79\n27.57\n25.15\n69.96\n65.41\n61.66\n57.51\n52.73\n49.45\n44.34\n39.58\n36.88\n33.35\n30.61\n28.37\n25.90\n71.20\n66.62\n62.83\n58.64\n53.82\n50.51\n45.34\n40.53\n37.80\n34.22\n31.44\n29.16\n26.66\n72.44\n67.82\n64.00\n59.77\n54.91\n51.56\n46.34\n41.47\n38.71\n35.08\n32.27\n29.96\n27.42\n73.68\n69.02\n65.17\n60.91\n55.99\n52.62\n47.34\n42.42\n39.62\n35.95\n33.10\n30.75\n28.18\n74.92\n70.22\n66.34\n62.04\n57.08\n53.67\n48.33\n43.37\n40.53\n36.82\n33.93\n31.55\n28.94\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Practice Final Exam",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_prac_exam_final.pdf",
      "content": "Practice Final\n18.05, Spring 2022\nThe test is divided into two parts. The first part is a series of concept questions. You don't\nneed to show any work on this part. The second part consists of standard problems. You\nneed to show your work on these.\nConcept Problem 1. Which of the following represents a valid probability table?\n(i)\noutcomes\nprobability\n1/5\n1/5\n1/5\n1/5\n1/5\n(ii)\noutcomes\nprobability\n1/2\n1/5\n1/10\n1/10\n1/10\nCircle the best choice:\nA. (i)\nB. (ii)\nC. (i) and (ii)\nD. Not enough information\nConcept Problem 2.\nTrue or false: Setting the prior probability of a hypothesis to 0\nmeans that no amount of data will make the posterior probability of that hypothesis the\nmaximum over all hypotheses.\nCircle one: True\nFalse\nConcept Problem 3.\nTrue or false: It is okay to have a prior that depends on more\nthan one unknown parameter.\nCircle one: True\nFalse\nConcept Problem 4. Data is drawn from a normal distribution with unknown mean μ.\nWe make the following hypotheses:\nH0: μ = 1\nand\nHA: μ > 1.\nFor (i)-(iii) circle the correct answers:\n(i) Is H0 a simple or composite hypothesis?\nSimple\nComposite\n(ii) Is HA a simple or composite hypothesis?\nSimple\nComposite\n(iii) Is HA a one or two-sided?\nOne-sided\nTwo-sided\nConcept Problem 5.\nIf the original data has n points then a bootstrap sample should\nhave\nA. Fewer points than the original because there is less information in the sample than in\nthe underlying distribution.\nB. The same number of points as the original because we want the bootstrap statistic to\nmimic the statistic on the original data.\nC. Many more points than the original because we have the computing power to handle a\nlot of data.\nCircle the best answer:\nA\nB\nC.\nConcept Problem 6.\nIn 3 tosses of a coin which of following equals the event \"exactly\ntwo heads\"?\n\nPractice Final, Spring 2022\nA = {THH, HTH, HHT, HHH}\nB = {THH, HTH, HHT}\nC = {HTH, THH}\nCircle the best answer:\nA\nB\nC\nB and C\nConcept Problem 7.\nThese questions all refer to the following figure. For each one\ncircle the best answer.\nA1\nA2\nB1\nB2\nB1\nB2\nC1\nC2\nC1\nC2\nC1\nC2\nC1\nC2\nx\ny\nz\n(i) The probability x represents\nA. P (A1) B. P (A1|B2) C. P (B2|A1) D. P (C1|B2 ∩ A1).\n(ii) The probability y represents\nA. P (B2) B. P (A1|B2) C. P (B2|A1) D. P (C1|B2 ∩ A1).\n(iii) The probability z represents\nA. P (C1) B. P (B2|C1) C. P (C1|B2) D. P (C1|B2 ∩ A1).\n(iv) The circled node represents the event\nA. C1\nB. B2 ∩ C1\nC. A1 ∩ B2 ∩ C1\nD. C1|B2 ∩ A1.\nConcept Problem 8. The graphs below give the pmf for 3 random variables.\nx\n(A)\nx\n(B)\nx\n(C)\nCircle the answer that orders the graphs from smallest to biggest standard deviation.\nABC\nACB\nBAC\nBCA\nCAB\nCBA\nConcept Problem 9. Suppose you have $100 and you need $1000 by tomorrow morning.\nYour only way to get the money you need is to gamble. If you bet $k, you either win $k\nwith probability p or lose $k with probability 1 - p. Here are two strategies:\nMaximal strategy: Bet as much as you can, up to what you need, each time.\nMinimal strategy: Make a small bet, say $10, each time.\nSuppose p = 0.8.\nCircle the better strategy:\nMaximal\n2. Minimal\nConcept Problem 10.\nConsider the following joint pdf's for the random variables X\n\nPractice Final, Spring 2022\nand Y . Circle the ones where X and Y are independent and cross out the other ones.\nA. f(x, y) = 4x2y3\nB. f(x, y) = 1\n2(x3y+ xy3).\nC. f(x, y) = 6e-3x-2y\nConcept Problem 11.\nSuppose X ∼ Bernoulli(θ) where θ is unknown. Which of the\nfollowing is the correct statement?\nA. The random variable is discrete, the space of hypotheses is discrete.\nB. The random variable is discrete, the space of hypotheses is continuous.\nC. The random variable is continuous, the space of hypotheses is discrete.\nD. The random variable is continuous, the space of hypotheses is continuous.\nCircle the letter of the correct statement:\nA\nB\nC\nD\nConcept Problem 12. Let θ be the probability of heads for a bent coin. Suppose your\nprior f(θ) is Beta(6, 8). Also suppose you flip the coin 7 times, getting 2 heads and 5 tails.\nWhat is the posterior pdf f(θ|x)? Circle the best answer.\nA. Beta(2,5)\nB. Beta(3,6)\nC. Beta(6,8)\nD. Beta(8,13)\nE. Not enough information to say\nConcept Problem 13.\nSuppose the prior has been set. Let x1 and x2 be two sets of\ndata. Circle true or false for each of the following statements.\nA. If x1 and x2 have the same likelihood function then they result in the same posterior. True\nFalse\nB. If x1 and x2 result in the same posterior then they have the same likelihood function. True\nFalse\nC. If x1 and x2 have proportional likelihood functions then they result in the same True\nFalse\nposterior.\nConcept Problem 14.\nEach day Jane arrives X hours late to class, with X ∼\nuniform(0, θ). Jon models his initial belief about θ by a prior pdf f(θ). After Jane ar\nrives x hours late to the next class, Jon computes the likelihood function f(x|θ) and the\nposterior pdf f(θ|x).\nCircle the probability computations a frequentist would consider valid. Cross out the others.\nA. prior\nB. posterior\nC. likelihood\nConcept Problem 15.\nSuppose we run a two-sample t-test for equal means with\nsignificance level α = 0.05. If the data implies we should reject the null hypothesis, then\nthe odds that the two samples come from distributions with the same mean are (circle the\nbest answer)\nA. 19/1\nB. 1/19\nC. 20/1\nD. 1/20\nE. unknown\nConcept Problem 16.\nConsider the following statements about a 95% confidence\ninterval for a parameter θ.\n\n{\n{\n{\n{\nPractice Final, Spring 2022\nA. P (θ0 is in the CI | θ = θ0) ≥ 0.95\nB. P (θ0 is in the CI ) ≥ 0.95\nC. An experiment produces the CI [-1, 1.5]: P (θ is in [-1, 1.5] | θ = 0) ≥ 0.95\nCircle the letter of each correct statement and cross out the others:\nA\nB\nC\nProblem 17. (a) Let A and B be two events. Suppose that the probability that neither\nevent occurs is 3/8. What is the probability that at least one of the events occurs?\n(b) Let C and D be two events. Suppose P(C) = 0.5, P (C∩D) = 0.2 and P((C∪D)c) = 0.4.\nWhat is P (D)?\nProblem 18. An urn contains 3 orange balls and 2 blue balls. A ball is drawn. If the ball\nis orange, it is kept out of the urn and a second ball is drawn from the urn. If the ball is\nblue, then it is put back in the urn and an orange ball is added to the urn. Then a second\nball is drawn from the urn.\n(a) What is the probability that both balls drawn are orange?\n(b) If the second drawn ball is orange, what is the probability that the first drawn ball was\nblue?\nProblem 19. You roll a fair six sided die repeatedly until the sum of all numbers rolled\nis greater than 6. Let X be the number of times you roll the die. Let F be the cumulative\ndistribution function for X. Compute F (1), F (2), and F (7).\nProblem 20. A test is graded on the scale 0 to 1, with 0.55 needed to pass.\nStudent scores are modeled by the following density:\n⎧4x\nfor 0 ≤ x ≤ 1/2\n{\nf(x) =\n4 - 4x\nfor 1/2 ≤ x ≤ 1\n⎨\n{0\notherwise\n⎩\n(a) What is the probability that a random student passes the exam?\n(b) What score is the 87.5 percentile of the distribution?\nProblem 21. Suppose X is a random variable with cdf\n⎧0\nfor x < 0\n{\nF (x) =\nx(2 - x)\nfor 0 ≤ x ≤ 1\n⎨\n{\n⎩1\nfor x > 1\n(a) Find E[X].\n(b) Find P (X < 0.4).\n\nPractice Final, Spring 2022\nProblem 22.\nCompute the mean and variance of a random variable whose distribution\nis uniform on the interval [a, b].\nIt is not enough to simply state these values. You must give the details of the computation.\nProblem 23. Defaulting on a loan means failing to pay it back on time. The default rate\namong MIT students on their student loans is 1%. As a project you develop a test to predict\nwhich students will default. Your test is good but not perfect. It gives 4% false positives,\ni.e. prediciting a student will default who in fact will not. If has a 0% false negative rate,\ni.e. prediciting a student won't default who in fact will.\n(a) Solution: Suppose a random student tests positive. What is the probability that he\nwill truly default.\n(b) Solution: Someone offers to bet me the student in part (a) won't default. They want\nme to pay them $100 if the student doesn't default and they'll pay me $400 if the student\ndoes default. Is this a good bet for me to take?\nProblem 24.\nData was taken on height and weight from the entire population of 700\nmountain gorillas living in the Democratic Republic of Congo:\nht\\wt\nlight\naverage\nheavy\nshort\ntall\nLet X encode the weight, taking the values of a randomly chosen gorilla: 0, 1, 2 for light,\naverage, and heavy respectively.\nLikewise, let Y encode the height, taking values 0 and 1 for short and tall respectively.\n(a) Determine the joint pmf of X and Y and the marginal pmf's of X and of Y .\n(b) Are X and Y independent?\n(c) Find the covariance of X and Y .\nFor this part, you need a numerical (no variables) expression, but you can leave it uneval\nuated.\n(d) Find the correlation of X and Y .\nFor this part, you need a numerical (no variables) expression, but you can leave it uneval\nuated.\nProblem 25. A political poll is taken to determine the fraction p of the population that\nwould support a referendum requiring all citizens to be fluent in the language of probability\nand statistics.\n(a) Assume p = 0.5. Use the central limit theorem to estimate the probability that in a\npoll of 25 people, at least 14 people support the referendum.\nYour answer to this problem should be a decimal.\n(b) With p unknown and n the number of random people polled, let Xn be the fraction\nof the polled people who support the referendum.\nWhat is the smallest sample size n in order to have a 90% confidence that Xn is within\n0.01 of the true value of p?\n\nPractice Final, Spring 2022\nYour answer to this problem should be an integer.\nProblem 26.\nSuppose a researcher collects x1, ... , xn i.i.d. measurements of the back\nground radiation in Boston. Suppose also that these observations follow a Rayleigh distri\nbution with parameter τ, with pdf given by\nf(x) = xτe- 1\n2 τx2.\nFind the maximum likelihood estimate for τ.\nProblem 27.\nBivariate data (4, 10), (-1, 3), (0, 2) is assumed to arise from the model\nyi = b|xi - 3| + ei, where b is a constant and ei are independent random variables.\n(a) What assumptions are needed on ei so that it makes sense to do a least squares fit of\na curve y = b|x-3| to the data?\n(b) Given the above data, determine the least squares estimate for b.\nFor this problem we want you to calculate all the way to a fraction b = s\nr , where r and s\nare integers.\nProblem 28. Taxi problem Data is collected on the time between arrivals of consecutive\ntaxis at a downtown hotel. We collect a data set of size 45 with sample mean x = 5.0 and\nsample standard deviation s = 4.0.\n(a) Assume the data follows a normal random variable.\n(i) Find an 80% confidence interval for the mean μ of X.\n(ii) Find an 80% χ2-confidence interval for the variance?\n(b) Now make no assumptions about the distribution of of the data. By bootstrapping, we\ngenerate 500 values for the average inter-arrival time x∗ . The smallest and largest 150 are\nwritten in non-decreasing order on the next page.\nUse this data to find an 80% percentile bootstrap confidence interval for μ.\n(c) We suspect that the time between taxis is modeled by an exponential distribution, not\na normal distribution. In this case, are the approaches in the earlier parts justified?\n(d) When might method (b) be preferable to method (a)?\nThe 150 smallest and 150 largest values of x∗ for taxi problem are given in the following\ntable.\n\nPractice Final, Spring 2022\n1- 10\n4.466\n4.506\n4.509\n4.515\n4.578\n4.597\n4.618\n4.635\n4.653\n4.664\n11- 20\n4.670\n4.672\n4.685\n4.696\n4.703\n4.707\n4.713\n4.721\n4.727\n4.727\n21- 30\n4.729\n4.731\n4.738\n4.738\n4.740\n4.743\n4.744\n4.745\n4.751\n4.752\n31- 40\n4.759\n4.760\n4.768\n4.774\n4.775\n4.777\n4.778\n4.780\n4.784\n4.784\n41- 50\n4.787\n4.789\n4.789\n4.790\n4.791\n4.791\n4.792\n4.796\n4.800\n4.800\n51- 60\n4.800\n4.802\n4.805\n4.807\n4.808\n4.808\n4.811\n4.812\n4.812\n4.817\n61- 70\n4.818\n4.818\n4.819\n4.821\n4.821\n4.822\n4.824\n4.825\n4.826\n4.830\n71- 80\n4.830\n4.834\n4.836\n4.837\n4.837\n4.838\n4.838\n4.840\n4.840\n4.841\n81- 90\n4.841\n4.841\n4.842\n4.843\n4.844\n4.844\n4.845\n4.845\n4.846\n4.846\n91- 100\n4.847\n4.848\n4.849\n4.849\n4.850\n4.852\n4.852\n4.854\n4.855\n4.855\n101- 110\n4.856\n4.858\n4.858\n4.858\n4.862\n4.863\n4.865\n4.865\n4.866\n4.866\n111- 120\n4.867\n4.869\n4.871\n4.872\n4.876\n4.876\n4.876\n4.877\n4.877\n4.881\n121- 130\n4.882\n4.886\n4.886\n4.886\n4.888\n4.889\n4.891\n4.892\n4.892\n4.893\n131- 140\n4.895\n4.897\n4.897\n4.897\n4.898\n4.899\n4.901\n4.902\n4.902\n4.903\n141- 150\n4.903\n4.904\n4.905\n4.905\n4.905\n4.907\n4.907\n4.907\n4.907\n4.907\n351-360\n5.073\n5.074\n5.075\n5.075\n5.077\n5.077\n5.077\n5.077\n5.078\n5.079\n361-370\n5.079\n5.079\n5.080\n5.081\n5.081\n5.082\n5.083\n5.084\n5.085\n5.085\n371-380\n5.087\n5.087\n5.088\n5.091\n5.091\n5.091\n5.092\n5.092\n5.093\n5.093\n381-390\n5.094\n5.094\n5.096\n5.097\n5.100\n5.100\n5.101\n5.101\n5.102\n5.103\n391-400\n5.104\n5.104\n5.106\n5.106\n5.108\n5.108\n5.108\n5.108\n5.108\n5.110\n401-410\n5.110\n5.111\n5.112\n5.112\n5.112\n5.112\n5.113\n5.114\n5.114\n5.115\n411-420\n5.118\n5.122\n5.122\n5.123\n5.127\n5.129\n5.129\n5.132\n5.134\n5.134\n421-430\n5.134\n5.135\n5.136\n5.136\n5.137\n5.140\n5.141\n5.142\n5.142\n5.143\n431-440\n5.143\n5.145\n5.146\n5.147\n5.147\n5.148\n5.151\n5.151\n5.154\n5.155\n441-450\n5.156\n5.162\n5.163\n5.164\n5.164\n5.165\n5.166\n5.168\n5.169\n5.169\n451-460\n5.170\n5.172\n5.172\n5.175\n5.178\n5.179\n5.180\n5.181\n5.182\n5.182\n461-470\n5.182\n5.186\n5.195\n5.202\n5.202\n5.205\n5.206\n5.210\n5.216\n5.219\n471-480\n5.220\n5.220\n5.221\n5.222\n5.224\n5.225\n5.232\n5.232\n5.236\n5.236\n481-490\n5.243\n5.244\n5.245\n5.251\n5.253\n5.258\n5.261\n5.263\n5.266\n5.273\n491-500\n5.274\n5.288\n5.288\n5.291\n5.307\n5.312\n5.314\n5.316\n5.348\n5.488\nProblem 29.\nNote. In this problem the geometric(p) distribution is defined as the\ntotal number of trials to the first failure (the value includes the failure), where p is the\nprobabilitiy of success.\n(a) What sample statistic would you use to estimate p?\n(b) Describe how you would use the parametric bootstrap to estimate a 95% basic confidence\ninterval for p. You can be brief, but you should give careful step-by-step instructions.\nProblem 30. You independently draw 100 data points from a normal distribution.\n(a) Suppose you know the distribution is N(μ, 4) (4 = σ2) and you want to test the null\nhypothesis H0 ∶μ= 3 against the alternative hypothesis HA ∶μ=3.\nIf you want a significance level of α = 0.05. What is your rejection region?\nYou must clearly state what test statistic you are using.\n(b) Suppose the 100 data points have sample mean 5. What is the p-value for this data?\n\nPractice Final, Spring 2022\nShould you reject H0?\n(c) Determine the power of the test using the alternative HA ∶μ= 4.\nProblem 31. Suppose that you have molecular type with unknown atomic mass θ. You\nhave an atomic scale with normally-distributed error of mean 0 and variance 0.5.\n(a) Suppose your prior on the atomic mass is N(80, 4). If the scale reads 85, what is your\nposterior pdf for the atomic mass?\n(b) With the same prior as in part (a), compute the smallest number of measurements\nneeded so that the posterior variance is less than 0.01.\nProblem 32.\nYour friend grabs a die at random from a drawer containing two 6-sided\ndice, one 8-sided die, and one 12-sided die. She rolls the die once and reports that the result\nis 7.\n(a) Make a discrete Bayes table showing the prior, likelihood, and posterior for the type of\ndie rolled given the data.\n(b) What are your posterior odds that the die has 12 sides?\n(c) Given the data of the first roll, what is your probability that the next roll will be a 7?\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Practice Final Exam Probability Unit",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_prac_exam_final_probability.pdf",
      "content": "Review for final exam : probability unit\nMIT 18.05 Spring 2022\nSets and counting\n- Sets: ∅, union, intersection, complement Venn diagrams, products\n- Counting: inclusion-exclusion, rule of product,\n= (n\npermutations nPk, combinations nCk\nk)\nProblem 1. Consider the nucleotides A, G, C, T .\n(a) How many ways are there to make a sequence of 5 nucleotides.\n(b) How many sequences of length 5 are there where no adjacent nucleotides are the same\n(c) How many sequences of length 5 have exactly one A?\nProblem 2. (a) How many 5 card poker hands are there?\n(b) How many ways are there to get a full house (3 of one rank and 2 of another)?\n(c) What's the probability of getting a full house?\nProblem 3. (Counting)\n(a) How many arrangements of the letters in the word probability are there?\n(b) Suppose all of these arrangements are written in a list and one is chosen at random.\nWhat is the probability it begins with 'b'.\nProbability\n- Sample space, outcome, event, probability function. Rule: P(A∪B) = P(A)+P(B)-\nP(A ∩ B).\nSpecial case: P(Ac) = 1 -P(A)\n(A and B disjoint ⇒ P(A∪B) = P(A) + P(B).)\n- Conditional probability, multiplication rule, trees, law of total probability, indepen\ndence\n- Bayes' theorem, base rate fallacy\nProblem 4.\nLet E and F be two events. Suppose the probability that at least one of\nthem occurs is 2/3. What is the probability that neither E nor F occurs?\nProblem 5. Let C and D be two events with P(C) = 0.3, P (D) = 0.4, and P(Cc∩D) =\n0.2.\nWhat is P(C ∩D)?\nProblem 6.\nSuppose we have 8 teams labeled T1, ..., T8. Suppose they are ordered by\nplacing their names in a hat and drawing the names out one at a time.\n\n(a) How many ways can it happen that all the odd numbered teams are in the odd\nnumbered slots and all the even numbered teams are in the even numbered slots?\n(b) What is the probability of this happening?\nProblem 7. More cards! Suppose you want to divide a 52 card deck into four hands with\n13 cards each. What is the probability that each hand has a king?\nProblem 8. Suppose we roll a fair die twice. Let A be the event 'the sum of the rolls is\n5' and let B be the event 'at least one of the rolls is 4.'\n(a) Calculate P (A|B).\n(b) Are A and B independent?\nProblem 9.\nOn a quiz show the contestant is given a multiple choice question with 4\noptions. Suppose there is a 70% chance the contestant actually knows the answer. If they\ndon't know the answer they guess with a 25% chance of getting it right. Suppose they get\nit right. What is the probability that they were guessing?\nProblem 10. Suppose you have an urn containing 7 red and 3 blue balls. You draw three\nballs at random. On each draw, if the ball is red you set it aside and if the ball is blue you\nput it back in the urn. What is the probability that the third draw is blue?\n(If you get a blue ball it counts as a draw even though you put it back in the urn.)\nProblem 11. Suppose that P (A) = 0.4, P (B) = 0.3 and P((A∪B)C) = 0.42. Are A and\nB independent?\nProblem 12. Suppose now that events A, B and C are mutually independent with\nP(A) = 0.3,\nP(B) = 0.4,\nP(C) = 0.5.\nCompute the following: (Hint: Use a Venn diagram)\n(i) P(A∩B ∩Cc)\n(ii) P(A∩Bc ∩C)\n(iii) P(Ac ∩B ∩C)\nProblem 13. Suppose A and B are events with 0 < P(A) < 1 and 0 < P(B) < 1.\n(a) If A and B are disjoint can they be independent?\n(b) If A and B are independent can they be disjoint?\n(c) If A ⊂ B can they be independent?\nRandom variables, expectation and variance\n- Discrete random variables: events, pmf, cdf\n\n- Bernoulli(p), binomial(n, p), geometric(p), uniform(n)\n- E[X], meaning, algebraic properties, E[h(X)]\n- Var(X), meaning, algebraic properties\n- Continuous random variables: pdf, cdf\n- uniform(a,b), exponential(λ), normal(μ,σ)\n- Transforming random variables\n- Quantiles\nProblem 14. Directly from the definitions of expected value and variance, compute E[X]\nand Var(X) when X has probability mass function given by the following table:\nX\n-2\n-1\npmf\n1/15\n2/15\n3/15\n4/15\n5/15\nProblem 15. Suppose that X takes values between 0 and 1 and has probability density\nfunction 2x. Compute Var(X) and Var(X2).\nProblem 16. The pmf of X is given by the following table\nValue of X\n-1\nProbability\n1/3\n1/6\n1/2\n(a) Compute E[X].\n(b) Give the pdf of Y = X2 and use it to compute E[Y ].\n(c) Instead, compute E[X2] directly from an extended table.\n(d) Compute Var(X).\nProblem 17. Compute the expectation and variance of a Bernoulli(p) random variable.\nProblem 18. Suppose 100 people all toss a hat into a box and then proceed to randomly\npick out of a hat. What is the expected number of people to get their own hat back.\nHint: express the number of people who get their own hat as a sum of random variables\nwhose expected value is easy to compute.\npmf, pdf, cdf\nProbability Mass Functions, Probability Density Functions and Cumulative Distribution\nFunctions\nProblem 19.\nSuppose that X ∼ Bin(n, 0.5). Find the probability mass function of\nY = 2X.\n\nProblem 20. (a) Suppose that X is uniform on [0, 1]. Compute the pdf and cdf of X.\n(b) If Y = 2X+ 5, compute the pdf and cdf of Y .\nProblem 21.\n(a) Suppose that X has probability density function fX(x) = λe-λx for\nx ≥ 0. Compute the cdf, FX(x).\n(b) If Y = X2, compute the pdf and cdf of Y .\nProblem 22. Suppose that X is a random variable that takes on values 0, 2 and 3 with\nprobabilities 0.3, 0.1, 0.6 respectively. Let Y = 3(X-1)2.\n(a) What is the expectation of X?\n(b) What is the variance of X?\n(c) What is the expection of Y ?\n(d) Let FY (t) be the cumulative density function of Y . What is FY (7)?\nProblem 23.\nSuppose you roll a fair 6-sided die 25 times (independently), and you get\n$3 every time you roll a 6. Let X be the total number of dollars you win.\n(a) What is the pmf of X.\n(b) Find E[X] and Var(X).\n(c) Let Y be the total won on another 25 independent rolls. Compute and compare E[X+Y ],\nE[2X], Var(X + Y), Var(2X).\nExplain briefly why this makes sense.\nProblem 24. A continuous random variable X has PDF f(x) = x + ax2 on [0,1]\nFind a, the CDF and P(0.5 < X < 1).\nProblem 25. For each of the following say whether it can be the graph of a cdf. If it can\nbe, say whether the variable is discrete or continuous.\nx\nF (x)\n0.5\n(i)\nx\nF (x)\n0.5\n(ii)\nx\nF (x)\n0.5\n(iii)\nx\nF (x)\n0.5\n(iv)\n\nx\nF (x)\n0.5\n(v)\nx\nF (x)\n0.5\n(vi)\nx\nF (x)\n0.5\n(vii)\nx\nF (x)\n0.5\n(viii)\nProblem 26. Correlation\nFlip a coin 5 times. Use properties of covariance to compute the covariance and correlation\nbetween the number of heads on the first 3 and last 3 flips.\nDistributions with names\nProblem 27. Exponential Distribution\nSuppose that buses arrive are scheduled to arrive at a bus stop at noon but are always X\nminutes late, where X is an exponential random variable with probability density function\nfX(x) = λe-λx. Suppose that you arrive at the bus stop precisely at noon.\n(a) Compute the probability that you have to wait for more than five minutes for the bus\nto arrive.\n(b) Suppose that you have already waiting for 10 minutes. Compute the probability that\nyou have to wait an additional five minutes or more.\nProblem 28. Normal Distribution: Throughout these problems, let φ and Φ be the\npdf and cdf, respectively, of the standard normal distribution Suppose Z is a standard\nnormal random variable and let X = 3Z+ 1.\n(a) Express P(X ≤ x) in terms of Φ\n(b) Differentiate the expression from (a) with respect to x to get the pdf of X, f(x).\nRemember that Φ′(z) = φ(z) and don't forget the chain rule\n(c) Find P(-1 ≤X ≤1)\n(d) Recall that the probability that Z is within one standard deviation of its mean is\napproximately 68%. What is the probability that X is within one standard deviation of its\nmean?\nProblem 29. Transforming Normal Distributions\nSuppose Z ∼ N(0,1) and Y = eZ.\n\n(a) Find the cdf FY (a) and pdf fY (y) for Y . (For the CDF, the best you can do is write\nit in terms of Φ the standard normal cdf.)\n(b) We don't have a formula for Φ(z) so we don't have a formula for quantiles. So we have\nto write quantiles in terms of Φ-1.\n(i) Write the 0.33 quantile of Z in terms of Φ-1\n(ii) Write the 0.9 quantile of Y in terms of Φ-1.\n(iii) Find the median of Y .\nProblem 30. (Random variables derived from normal random variables)\nLet X1, X2, ...Xn be i.i.d. N(0, 1) random variables.\nLet Yn = X1\n2 + ... + Xn\n2 .\n(a) Use the formula Var(Xj) = E[Xj\n2] - E[Xj]2 to show E[Xj\n2] = 1.\n(b) Set up an integral in x for computing E[Xj\n4].\nFor 3 extra credit points, use integration by parts show E[Xj\n4] = 3.\n(If you don't do this, you can still use this result in part c.)\n(c) Deduce from parts (a) and (b) that Var(Xj\n2) = 2.\n(d) Use the Central Limit Theorem to approximate P (Y100 > 110).\nProblem 31. More Transforming Normal Distributions\n(a) Suppose Z is a standard normal random variable and let Y = aZ+ b, where a > 0 and\nb are constants.\nShow Y ∼ N(b, a2) (remember our notation for normal distributions uses mean and vari\nance).\nY -μ\n(b) Suppose Y ∼ N(μ, σ2). Show\nfollows a standard normal distribution.\nσ\nProblem 32. (Sums of normal random variables)\nLet X, Y be independent random variables where X ∼ N(2, 5) and Y ∼ N(5, 9) (we use\nthe notation N(μ, σ2)). Let W = 3X-2Y + 1.\n(a) Compute E[W ] and Var(W ).\n(b) It is known that the sum of independent normal distributions is normal. Compute\nP(W ≤ 6).\nProblem 33. Let X ∼ U(a, b). Compute E[X] and Var(X).\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Exam 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_exam01.pdf",
      "content": "Name\n18.05 Exam 1\n6 problems. Do all your work on the paper provided\nNo books or calculators.\nYou may have one side of a 8.5×11 piece of paper with any information you like on it.\nSimplifying expressions: Unless explicitly asked to do so, you don't need to simplify\ncomplicated expressions. For example, you can leave\n⋅3 + 1 ⋅5 exactly as is. Likewise\n20!\nfor expressions like 18!2!.\nTable of normal probabilities: Use the table of standard normal probabilities at the\nend of this exam to do your computations for normal variables.\nCheat Sheet\nProblem 0. (5 pts) Be sure to attach your cheat sheet to your test.\n\n18.05 Exam 1\nProblem 1. (10 pts: 4,6) Concept/Quick questions\n(a) (No explanations are necessary.)\nThe plot shows the pdf for three independent random variables X, Y , W . All use the same\nhorizontal and vertical scale.\nfX\nfY\nfW\nWhich random variable has the greatest variance?\n(b) Suppose A and B are two events and P (A) = 0.7, P (B) = 0.3 and P(A∩B) = 0.25.\nCompute each of the following\n(i) Compute P(A ∪ B)\n(ii) Compute P (A|B).\n\n18.05 Exam 1\nProblem 2. (15 pts: 10,5)\nYou create passwords as a string of 10 characters such that:\n- 5 of the characters are letters (upper and lower case, i.e. 52 characters) with repetitions\nallowed,\n- 3 are numbers { 0,1,2,3,4,5,6,7,8,9 } with repetitions allowed, and\n- 2 are distinct symbols from the list of 5 symbols: { !, @, #, $, & }.\n(a) How many passwords are there? (No need to simplify your answer.)\n(b) With all locations for symbols, letters, or numbers in your 10 character password being\nequally likely, what is the probability that the two symbols are next to each other?\n\n18.05 Exam 1\nProblem 3. (25 pts: 10,5,5,5)\nYou have 5 four-sided and 3 six-sided dice. You put them in a cup, choose one at random,\nroll the die, and report the result.\nLet D be the number of sides on the chosen die and let R be the result of the roll.\n(a) Make a joint probability table for D and R. Be sure to include the marginal probabilities\nfor D and R.\n(b) What is the probability of rolling a 3?\n(c) Compute P(D = 4|R = 3).\n(d) Are D and R independent?\n\n18.05 Exam 1\nProblem 4. (10 pts)\nA quick screening test for a certain disease has three outcomes: positive, negative and\nuncertain. Suppose it has the following percentages.\nFor someone with the disease: positive 70%, negative 10%, uncertain 20%.\nFor someone without the disease: positive 10%, negative 60%, uncertain 30%.\nSuppose also, that the prevalence of the disease in the population is 2%.\nWhat is the probability that a random person who tests positive has the disease?\n\n18.05 Exam 1\nProblem 5. (25 pts: 5,5,5,5,5)\nTwo students, Xeno and Yolanda are meeting up for lunch. They plan on a time to meet at\nnoon. Both have class before so neither will be early. Both have class that starts at 1pm,\nso they will both arrive between 0 and 1 hour late. Let X be the time in hours that Xeno\narrives late and let Y be the time in hours that Yolanda arrives late.\nAssume that the joint pdf of these random variables is f(x, y) = 5/4 -xy.\n(a) Find the two marginal pdfs.\n(b) Are X and Y independent?\n(c) Find E[X], Var(X). (For these, you need to simplify the fractions.)\n(Problem 5 continued on next page)\n\n18.05 Exam 1\n(Problem 5 continued)\n(d) Compute the covariance Cov(X, Y ) and correlation Cor(X, Y ).\nHint: By symmetry you know the mean and variance of Y are the same as those for X.\nFor this part, there is no need to simplify fractions.\n(e) Set up, but do not compute an expression computing the probability that Xeno and\nYolanda arrive within 6 minutes (0.1 hours) of each other and that Yolanda arrives after\nXeno.\nYour integral will be over a region R in the unit square. You can leave your integral in the\nform ∫∫ h(x, y) dx dy and show R in a figure elsewhere on the page. The function h(x, y)\nR\nmust be specified completely.\n\n18.05 Exam 1\nProblem 6. (10 pts)\nA company manufactures solar panels. When homeowners install the panels, the state pays\n50% of the cost. Because this subsidy is about to expire, the company wants to manufacture\nas many panels as it can in the next 20 days.\nFor a variety of reasons the number of panels it can manufacture in a day is a ran\ndom variable with each day independent of the others. Suppose the daily output fol\nlows a so-called gamma distribution. The pdf of this distribution is not that complicated\n(f(x) =\nx\n10 e-x/100), but we'll let Wikipedia tell us the mean and variance: mean =\n4! ⋅\n500, variance = 5 ⋅ 104.\nEstimate the probability that they will be able to manufacture more than 10,500 panels\nin the next 20 days.\n\n18.05 Exam 1\nStandard normal table of left tail probabilities.\nz\nz\nz\nz\nΦ(z) = P (Z ≤ z) for N(0, 1).\n(Use interpolation to estimate\nz values to a 3rd decimal\nplace.)\nΦ(z)\nΦ(z)\nΦ(z)\nΦ(z)\n-4.00\n0.0000\n-2.00\n0.0228\n0.00\n0.5000\n2.00\n0.9772\n-3.95\n0.0000\n-1.95\n0.0256\n0.05\n0.5199\n2.05\n0.9798\n-3.90\n0.0000\n-1.90\n0.0287\n0.10\n0.5398\n2.10\n0.9821\n-3.85\n0.0001\n-1.85\n0.0322\n0.15\n0.5596\n2.15\n0.9842\n-3.80\n0.0001\n-1.80\n0.0359\n0.20\n0.5793\n2.20\n0.9861\n-3.75\n0.0001\n-1.75\n0.0401\n0.25\n0.5987\n2.25\n0.9878\n-3.70\n0.0001\n-1.70\n0.0446\n0.30\n0.6179\n2.30\n0.9893\n-3.65\n0.0001\n-1.65\n0.0495\n0.35\n0.6368\n2.35\n0.9906\n-3.60\n0.0002\n-1.60\n0.0548\n0.40\n0.6554\n2.40\n0.9918\n-3.55\n0.0002\n-1.55\n0.0606\n0.45\n0.6736\n2.45\n0.9929\n-3.50\n0.0002\n-1.50\n0.0668\n0.50\n0.6915\n2.50\n0.9938\n-3.45\n0.0003\n-1.45\n0.0735\n0.55\n0.7088\n2.55\n0.9946\n-3.40\n0.0003\n-1.40\n0.0808\n0.60\n0.7257\n2.60\n0.9953\n-3.35\n0.0004\n-1.35\n0.0885\n0.65\n0.7422\n2.65\n0.9960\n-3.30\n0.0005\n-1.30\n0.0968\n0.70\n0.7580\n2.70\n0.9965\n-3.25\n0.0006\n-1.25\n0.1056\n0.75\n0.7734\n2.75\n0.9970\n-3.20\n0.0007\n-1.20\n0.1151\n0.80\n0.7881\n2.80\n0.9974\n-3.15\n0.0008\n-1.15\n0.1251\n0.85\n0.8023\n2.85\n0.9978\n-3.10\n0.0010\n-1.10\n0.1357\n0.90\n0.8159\n2.90\n0.9981\n-3.05\n0.0011\n-1.05\n0.1469\n0.95\n0.8289\n2.95\n0.9984\n-3.00\n0.0013\n-1.00\n0.1587\n1.00\n0.8413\n3.00\n0.9987\n-2.95\n0.0016\n-0.95\n0.1711\n1.05\n0.8531\n3.05\n0.9989\n-2.90\n0.0019\n-0.90\n0.1841\n1.10\n0.8643\n3.10\n0.9990\n-2.85\n0.0022\n-0.85\n0.1977\n1.15\n0.8749\n3.15\n0.9992\n-2.80\n0.0026\n-0.80\n0.2119\n1.20\n0.8849\n3.20\n0.9993\n-2.75\n0.0030\n-0.75\n0.2266\n1.25\n0.8944\n3.25\n0.9994\n-2.70\n0.0035\n-0.70\n0.2420\n1.30\n0.9032\n3.30\n0.9995\n-2.65\n0.0040\n-0.65\n0.2578\n1.35\n0.9115\n3.35\n0.9996\n-2.60\n0.0047\n-0.60\n0.2743\n1.40\n0.9192\n3.40\n0.9997\n-2.55\n0.0054\n-0.55\n0.2912\n1.45\n0.9265\n3.45\n0.9997\n-2.50\n0.0062\n-0.50\n0.3085\n1.50\n0.9332\n3.50\n0.9998\n-2.45\n0.0071\n-0.45\n0.3264\n1.55\n0.9394\n3.55\n0.9998\n-2.40\n0.0082\n-0.40\n0.3446\n1.60\n0.9452\n3.60\n0.9998\n-2.35\n0.0094\n-0.35\n0.3632\n1.65\n0.9505\n3.65\n0.9999\n-2.30\n0.0107\n-0.30\n0.3821\n1.70\n0.9554\n3.70\n0.9999\n-2.25\n0.0122\n-0.25\n0.4013\n1.75\n0.9599\n3.75\n0.9999\n-2.20\n0.0139\n-0.20\n0.4207\n1.80\n0.9641\n3.80\n0.9999\n-2.15\n0.0158\n-0.15\n0.4404\n1.85\n0.9678\n3.85\n0.9999\n-2.10\n0.0179\n-0.10\n0.4602\n1.90\n0.9713\n3.90\n1.0000\n-2.05\n0.0202\n-0.05\n0.4801\n1.95\n0.9744\n3.95\n1.0000\n\n18.05 Exam 1\nExtra paper\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Practice Exam 1 All Questions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_prac_exam01_all.pdf",
      "content": "Practice Exam 1: Long List\n18.05, Spring 2022\nThis is a big list of practice problems for Exam 1. It includes all the problems\nin other sets of practice problems and many more!\n1 Counting and Probability\nProblem 1.\nA full house in poker is a hand where three cards share one rank and two\ncards share another rank. How many ways are there to get a full-house? What is the\nprobability of getting a full-house?\nProblem 2. There are 3 arrangements of the word DAD, namely DAD, ADD, and DDA.\nHow many arrangements are there of the word PROBABILITY?\nProblem 3. (a) How many ways can you arrange the letters in the word STATISTICS?\n(e.g. SSSTTTIIAC counts a one arrangement.)\n(b) If all arrangements are equally likely, what is the probabilitiy the two 'i's are next to\neach other.\nProblem 4. In a ballroom dancing class the students are divided into group A and group\nB. There are six people in group A and seven in group B. If four As and four Bs are chosen\nand paired off, how many pairings are possible?\nProblem 5.\nSuppose you pick two cards from a deck of 52 playing cards. What is the\nprobability that they are both queens?\nProblem 6. Suppose that there are ten students in a classroom. What is the probability\nthat no two of them have a birthday in the same month?\nProblem 7.\n20 politicians are having a tea party, 6 Democrats and 14 Republicans. To\nprepare, they need to choose:\n3 people to set the table, 2 people to boil the water, 6 people to make the scones.\nEach person can only do 1 task. (Note that this doesn't add up to 20. The rest of the\npeople don't help.)\n(a) In how many different ways can they choose which people perform these tasks?\n(b) Suppose that the Democrats all hate tea. If they only give tea to 10 of the 20 people,\nwhat is the probability that they only give tea to Republicans?\n(c) If they only give tea to 10 of the 20 people, what is the probability that they give tea\nto 9 Republicans and 1 Democrat?\nProblem 8.\nLet A and B be two events. Suppose the probability that neither A or B\noccurs is 2/3. What is the probability that one or both occur?\n\nPractice Exam 1: All Questions, Spring 2022\nProblem 9. Let C and D be two events with P(C) = 0.25, P (D) = 0.45, and P(C∩D) =\n0.1. What is P(Cc ∩D)?\nProblem 10.\nYou roll a four-sided die 3 times. For this problem we'll use the sample\nspace with 64 equally likely outcomes.\n(a) Write down this sample space in set notation.\n(b) List all the outcomes in each of the following events.\n(i) A = 'Exactly 2 of the 3 rolls are fours'\n(ii) B = 'At least 2 of the 3 rolls are fours'\n(iii) C = 'Exactly 1 of the second and third rolls is a 4'\n(iv) A ∩ C\nProblem 11.\nSuppose we have 8 teams labeled T1, ..., T8. Suppose they are ordered by\nplacing their names in a hat and drawing the names out one at a time.\n(a) How many ways can it happen that all the odd numbered teams are in the odd\nnumbered slots and all the even numbered teams are in the even numbered slots?\n(b) What is the probability of this happening?\n2 Conditional Probability and Bayes' Theorem\nProblem 12.\nMore cards! Suppose you want to divide a 52 card deck into four hands\nwith 13 cards each. What is the probability that each hand has a king?\nProblem 13.\nSuppose you are taking a multiple-choice test with c choices for each\nquestion. In answering a question on this test, the probability that you know the answer is\np. If you don't know the answer, you choose one at random. What is the probability that\nyou knew the answer to a question, given that you answered it correctly?\nProblem 14.\nCorrupted by their power, the judges running the popular game show\nAmerica's Next Top Mathematician have been taking bribes from many of the contestants.\nEach episode, a given contestant is either allowed to stay on the show or is kicked off.\nIf the contestant has been bribing the judges they will be allowed to stay with probability\n1. If the contestant has not been bribing the judges, they will be allowed to stay with\nprobability 1/3.\nSuppose that 1/4 of the contestants have been bribing the judges. The same contestants\nbribe the judges in both rounds, i.e., if a contestant bribes them in the first round, they\nbribe them in the second round too (and vice versa).\n(a) If you pick a random contestant who was allowed to stay during the first episode, what\nis the probability that they were bribing the judges?\n(b) If you pick a random contestant, what is the probability that they are allowed to stay\nduring both of the first two episodes?\n\nPractice Exam 1: All Questions, Spring 2022\n(c) If you pick random contestant who was allowed to stay during the first episode, what\nis the probability that they get kicked off during the second episode?\nProblem 15. Consider the Monty Hall problem. Let's label the door with the car behind\nit a and the other two doors b and c. In the game the contestant chooses a door and then\nMonty chooses a door, so we can label each outcome as 'contestant followed by Monty', e.g\nab means the contestant chose a and Monty chose b.\n(a) Make a 3 × 3 probability table showing probabilities for all possible outcomes.\n(b) Make a probability tree showing all possible outcomes.\n(c) Suppose the contestant's strategy is to switch. List all the outcomes in the event 'the\ncontestant wins a car'. What is the probability the contestant wins?\n(d) Redo part (c) with the strategy of not switching.\nProblem 16. Two dice are rolled.\nA = 'sum of two dice equals 3'\nB = 'sum of two dice equals 7'\nC = 'at least one of the dice shows a 1'\n(a) What is P (A|C)?\n(b) What is P (B|C)?\n(c) Are A and C independent? What about B and C?\nProblem 17. There is a screening test for prostate cancer that looks at the level of PSA\n(prostate-specific antigen) in the blood. There are a number of reasons besides prostate\ncancer that a man can have elevated PSA levels. In addition, many types of prostate cancer\ndevelop so slowly that that they are never a problem. Unfortunately there is currently no\ntest to distinguish the different types and using the test is controversial because it is hard\nto quantify the accuracy rates and the harm done by false positives.\nFor this problem we'll call a positive test a true positive if it catches a dangerous type of\nprostate cancer. We'll assume the following numbers:\nRate of prostate cancer among men over 50 = 0.0005\nTrue positive rate for the test = 0.9\nFalse positive rate for the test = 0.01\nLet T be the event a man has a positive test and let D be the event a man has a dangerous\ntype of the disease. Find P (D|T ) and P(D|T c).\nProblem 18.\nA multiple choice exam has 4 choices for each question. A student has\nstudied enough so that the probability they will know the answer to a question is 0.5, the\nprobability that they will be able to eliminate one choice is 0.25, otherwise all 4 choices\nseem equally plausible. If they know the answer they will get the question right. If not they\nhave to guess from the 3 or 4 choices.\nAs the teacher you want the test to measure what the student knows. If the student answers\na question correctly what's the probability they knew the answer?\n\nPractice Exam 1: All Questions, Spring 2022\nProblem 19. Suppose you have an urn containing 7 red and 3 blue balls. You draw three\nballs at random. On each draw, if the ball is red you set it aside and if the ball is blue you\nput it back in the urn. What is the probability that the third draw is blue?\n(If you get a blue ball it counts as a draw even though you put it back in the urn.)\nProblem 20.\nSome games, like tennis or ping pong, reach a state called deuce. This\nmeans that the score is tied and a player wins the game when they get two points ahead\nof the other player. Suppose the probability that you win a point is p and this is true\nindependently for all points. If the game is at deuce what is the probability you win the\ngame?\nThis is a tricky problem, but amusing if you like puzzles.\nProblem 21. (Bayes formula)\nA student takes a multiple-choice exam. Suppose for each question they either know the\nanswer or gamble and choose an option at random. Further suppose that if they knows the\nanswer, the probability of a correct answer is 1, and if they gamble, this probability is 1/4.\nTo pass, students need to answer at least 60% of the questions correctly. The student has\n\"studied for a minimal pass,\" i.e., with probability 0.6 they know the answer to a question.\nFor a single question, given that they answers it correctly, what is the probability that they\nactually knew the answer?\n3 Independence\nProblem 22. Suppose that P (A) = 0.4, P (B) = 0.3 and P((A∪B)C) = 0.42. Are A and\nB independent?\nProblem 23. Suppose now that events A, B and C are mutually independent with\nP(A) = 0.3,\nP(B) = 0.4,\nP(C) = 0.5.\nCompute the following: (Hint: Use a Venn diagram)\n(i) P(A∩B ∩Cc)\n(ii) P(A∩Bc ∩C)\n(iii) P(Ac ∩B ∩C)\nProblem 24.\nYou roll a twenty-sided die. Determine whether the following pairs of\nevents are independent.\n(a) 'You roll an even number' and 'You roll a number less than or equal to 10'.\n(b) 'You roll an even number' and 'You roll a prime number'.\nProblem 25. Suppose A and B are events with 0 < P(A) < 1 and 0 < P(B) < 1.\n(a) If A and B are disjoint can they be independent?\n(b) If A and B are independent can they be disjoint?\n(c) If A ⊂ B can they be independent?\n\nPractice Exam 1: All Questions, Spring 2022\n4 Expectation and Variance\nProblem 26. Directly from the definitions of expected value and variance, compute E[X]\nand Var(X) when X has probability mass function given by the following table:\nX\n-2\n-1\npmf\n1/15\n2/15\n3/15\n4/15\n5/15\nProblem 27. Suppose that X takes values between 0 and 1 and has probability density\nfunction 2x. Compute Var(X) and Var(X2).\nProblem 28.\nThe random variable X takes values -1, 0, 1 with probabilities 1/8, 2/8,\n5/8 respectively.\n(a) Compute E[X].\n(b) Give the pmf of Y = X2 and use it to compute E[Y ].\n(c) Instead, compute E[X2] directly from an extended table.\n(d) Compute Var(X).\nProblem 29. Suppose X is a random variable with E[X] = 5 and Var(X) = 2. What is\nE[X2]?\nProblem 30. Compute the expectation and variance of a Bernoulli(p) random variable.\nProblem 31. Suppose 100 people all toss a hat into a box and then proceed to randomly\npick out of a hat. What is the expected number of people to get their own hat back.\nHint: express the number of people who get their own hat as a sum of random variables\nwhose expected value is easy to compute.\nProblem 32. Suppose I play a gambling game with even odds. So, I can wager b dollars\nand I either win or lose b dollars with probability p = 0.5.\nI employ the following strategy to try to guarantee that I win some money.\nI bet $1; if I lose, I double my bet to $2, if I lose I double my bet again. I continue until\nI win. Eventually I'm sure to win a bet and net $1 (run through the first few rounds and\nyou'll see why this is the net).\nIf this really worked casinos would be out of business. Our goal in this problem is to\nunderstand the flaw in the strategy.\n(a) Let X be the amount of money bet on the last game (the one I win). X takes values\n1, 2, 4, 8, .... Determine the probability mass function for X. That is, find p(2k), where k\nis in {0, 1, 2, ...}.\n(b) Compute E[X].\n(c) Use your answer in part (b) to explain why the stategy is a bad one.\nProblem 33. Suppose you roll a fair 6-sided die 100 times (independently), and you get\n$3 every time you roll a 6.\n\nPractice Exam 1: All Questions, Spring 2022\nLet X1 be the number of dollars you win on rolls 1 through 25.\nLet X2 be the number of dollars you win on rolls 26 through 50.\nLet X3 be the number of dollars you win on rolls 51 through 75.\nLet X4 be the number of dollars you win on rolls 76 throught 100.\nLet X = X1 + X2 + X3 + X4 be the total number of dollars you win over all 100 rolls.\n(a) What is the probability mass function of X?\n(b) What is the expectation and variance of X?\n(c) Let Y = 4X1. (So instead of rolling 100 times, you just roll 25 times and multiply your\nwinnings by 4.)\n(i) What are the expectation and variance of Y ?\n(ii) How do the expectation and variance of Y compare to those of X? (That is, are they\nbigger, smaller, or equal?) Explain (briefly) why this makes sense.\n5 Probability Mass Functions, Probability Density Functions\nand Cumulative Distribution Functions\nProblem 34.\nSuppose that X ∼ Bin(n, 0.5). Find the probability mass function of\nY = 2X.\nProblem 35. (a) Suppose that X is uniform on [0, 1]. Compute the pdf and cdf of X.\n(b) If Y = 2X+ 5, compute the pdf and cdf of Y .\nProblem 36.\n(a) Suppose that X has probability density function fX(x) = λe-λx for\nx ≥ 0. Compute the cdf, FX(x).\n(b) If Y = X2, compute the pdf and cdf of Y .\nProblem 37. Suppose that X is a random variable that takes on values 0, 2 and 3 with\nprobabilities 0.3, 0.1, 0.6 respectively. Let Y = 3(X-1)2.\n(a) What is the expectation of X?\n(b) What is the variance of X?\n(c) What is the expection of Y ?\n(d) Let FY (t) be the cumulative density function of Y . What is FY (7)?\nProblem 38.\nLet T be the waiting time for customers in a queue. Suppose that T is\nexponential with pdf f(t) = 2e-2t on [0, inf).\nFind the pdf of the rate at which customers are served R = 1/T .\nProblem 39. A continuous random variable X has PDF f(x) = x + ax2 on [0,1]\nFind a, the CDF and P(0.5 < X < 1).\n\n{\n{\nPractice Exam 1: All Questions, Spring 2022\nProblem 40. (PMF of a sum)\nSuppose X and Y are independent and X ∼ Bernoulli(1/2) and Y ∼ Bernoulli(1/3). De\ntermine the pmf of X + Y\nProblem 41. Let X be a discrete random variable with pmf p given by:\nx\n-2\n-1\np(x) 1/15 2/15 3/15 4/15 5/15\n(a) Let Y = X2. Find the pmf of Y .\n(b) Find the value the cdf of X at -3/2, 3/4, 7/8, 1, 1.5, 5.\n(c) Find the value the cdf of Y at -3/2, 3/4, 7/8, 1, 1.5, 5.\nProblem 42. Suppose that the cdf of X is given by:\n⎧0 for a < 0\n{\nfor 0 ≤ a < 2\nF(a) =\n⎨\nfor 2 ≤ a < 4\n{\n⎩1 for a ≥ 4.\nDetermine the pmf of X.\nProblem 43. For each of the following say whether it can be the graph of a cdf. If it can\nbe, say whether the variable is discrete or continuous.\nx\nF (x)\n0.5\n(i)\nx\nF (x)\n0.5\n(ii)\nx\nF (x)\n0.5\n(iii)\nx\nF (x)\n0.5\n(iv)\nx\nF (x)\n0.5\n(v)\nx\nF (x)\n0.5\n(vi)\nx\nF (x)\n0.5\n(vii)\nx\nF (x)\n0.5\n(viii)\nProblem 44. Suppose X has range [0,1] and has cdf\nF (x) = x2\nfor 0 ≤ x ≤ 1.\n\nPractice Exam 1: All Questions, Spring 2022\nCompute P( 1\n2 < X < 3\n4).\nProblem 45. Let X be a random variable with range [0, 1] and cdf\nF(X) = 2x2 -x4\nfor 0 ≤ x ≤ 1.\n(a) Compute P( 1\n4 ≤ X ≤ 3\n4).\n(b) What is the pdf of X?\n6 Distributions with Names\nProblem 46. Exponential Distribution\nSuppose that buses arrive are scheduled to arrive at a bus stop at noon but are always X\nminutes late, where X is an exponential random variable with probability density function\nfX(x) = λe-λx. Suppose that you arrive at the bus stop precisely at noon.\n(a) Compute the probability that you have to wait for more than five minutes for the bus\nto arrive.\n(b) Suppose that you have already waiting for 10 minutes. Compute the probability that\nyou have to wait an additional five minutes or more.\nProblem 47. Normal Distribution: Throughout these problems, let φ and Φ be the\npdf and cdf, respectively, of the standard normal distribution Suppose Z is a standard\nnormal random variable and let X = 3Z+ 1.\n(a) Express P(X ≤ x) in terms of Φ\n(b) Differentiate the expression from (a) with respect to x to get the pdf of X, f(x).\nRemember that Φ′(z) = φ(z) and don't forget the chain rule\n(c) Find P(-1 ≤X ≤1)\n(d) Recall that the probability that Z is within one standard deviation of its mean is\napproximately 68%. What is the probability that X is within one standard deviation of its\nmean?\nProblem 48. Transforming Normal Distributions\nSuppose Z ∼ N(0,1) and Y = eZ.\n(a) Find the cdf FY (a) and pdf fY (y) for Y . (For the CDF, the best you can do is write\nit in terms of Φ the standard normal cdf.)\n(b) We don't have a formula for Φ(z) so we don't have a formula for quantiles. So we have\nto write quantiles in terms of Φ-1.\n(i) Write the 0.33 quantile of Z in terms of Φ-1\n(ii) Write the 0.9 quantile of Y in terms of Φ-1.\n(iii) Find the median of Y .\nProblem 49. (Random variables derived from normal random variables)\nLet X1, X2, ...Xn be i.i.d. N(0, 1) random variables.\n\nPractice Exam 1: All Questions, Spring 2022\nLet Yn = X1\n2 + ... + Xn\n2 .\n(a) Use the formula Var(Xj) = E[Xj\n2] - E[Xj]2 to show E[Xj\n2] = 1.\n(b) Set up an integral in x for computing E[Xj\n4].\nFor 3 extra credit points, use integration by parts show E[Xj\n4] = 3.\n(If you don't do this, you can still use this result in part c.)\n(c) Deduce from parts (a) and (b) that Var(Xj\n2) = 2.\n(d) Use the Central Limit Theorem to approximate P (Y100 > 110).\nProblem 50. More Transforming Normal Distributions\n(a) Suppose Z is a standard normal random variable and let Y = aZ+ b, where a > 0 and\nb are constants.\nShow Y ∼ N(b, a2) (remember our notation for normal distributions uses mean and vari\nance).\nY -μ\n(b) Suppose Y ∼ N(μ, σ2). Show\nfollows a standard normal distribution.\nσ\nProblem 51. (Sums of normal random variables)\nLet X, Y be independent random variables where X ∼ N(2, 5) and Y ∼ N(5, 9) (we use\nthe notation N(μ, σ2)). Let W = 3X-2Y + 1.\n(a) Compute E[W ] and Var(W ).\n(b) It is known that the sum of independent normal distributions is normal. Compute\nP(W ≤ 6).\nProblem 52. Let X ∼ U(a, b). Compute E[X] and Var(X).\nProblem 53. In n + m independent Bernoulli(p) trials, let Sn be the number of successes\nin the first n trials and Tm the number of successes in the last m trials.\n(a) What is the distribution of Sn? Why?\n(b) What is the distribution of Tm? Why?\n(c) What is the distribution of Sn + Tm? Why?\n(d) Are Sn and Tm independent? Why?\nProblem 54. Compute the median for the exponential distribution with parameter λ.\nProblem 55. Pareto and the 80-20 rule.\nPareto was an economist who used the Pareto distribution to model the wealth in a society.\nFor a fixed baseline m, the Pareto density with parameter α is\nα mα\nf(x) = xα+1\nfor x ≥ m.\nAssume X is a random variable that follows such a distribution.\n\nPractice Exam 1: All Questions, Spring 2022\n(a) Compute P(X > a) (you may assume a ≥ m).\n(b) Pareto's principle is often paraphrased as the 80-20 rule. That is, 80% of the wealth\nis owned by 20% of the people. The rule is only exact for a Pareto distribution with\nα = log(5)/ log(4) = 1.16.\nSuppose α = m = 1. Compute the 0.80 quantile for the Pareto distribution.\nIn general, many phenomena follow the power law described by f(x). You can look up\n'Pareto principle' in Wikipedia to read more about this.\n7 Joint Probability, Covariance, Correlation\nProblem 56. (Another Arithmetic Puzzle)\nLet X and Y be two independent Bernoulli(0.5) random variables. Define S and T by:\nS = X+ Y\nand T = X-Y.\n(a) Find the joint and marginal pmf's for S and T .\n(b) Are S and T independent.\nProblem 57.\nData is taken on the height and shoe size of a sample of MIT students.\nHeight is coded by 3 values: 1 (short), 2 (average), 3 (tall) and shoe size is coded by 3\nvalues 1 (small), 2 (average), 3 (large). The joint counts are given in the following table.\nShoe \\ Height\n234 225\n180 453 161\n192 157\nLet X be the coded shoe size and Y the height of a random person in the sample.\n(a) Find the joint and marginal pmf of X and Y .\n(b) Are X and Y independent?\nProblem 58. Let X and Y be two continuous random variables with joint pdf\nf(x, y) = cx2y(1 + y) for 0 ≤ x ≤ 3 and 0 ≤ y ≤ 3,\nand f(x, y) = 0 otherwise.\n(a) Find the value of c.\n(b) Find the probability P(1 ≤X≤2, 0 ≤Y ≤1).\n(c) Determine the joint cdf, F (a, b), of X and Y for a and b between 0 and 3.\n(d) Find marginal cdf FX(a) for a between 0 and 3.\n(e) Find the marginal pdf fX(x) directly from f(x, y) and check that it is the derivative of\nFX(x).\n(f) Are X and Y independent?\n\nPractice Exam 1: All Questions, Spring 2022\nProblem 59. Let X and Y be two random variables and let r, s, t, and u be real numbers.\n(a) Show that Cov(X+ s, Y + u) = Cov(X, Y ).\n(b) Show that Cov(rX, tY ) = rtCov(X, Y ).\n(c) Show that Cov(rX + s, tY + u) = rtCov(X, Y ).\nProblem 60. Derive the formula for the covariance: Cov(X, Y) = E[XY ] -E[X]E[Y].\nProblem 61. (Arithmetic Puzzle)\nThe joint and marginal pmf's of X and Y are partly given in the following table.\nX\\Y\n1/6\n...\n1/4\n...\n...\n...\n...\n1/4\n1/3\n1/3\n...\n1/6 1/3\n...\n(a) Complete the table.\n(b) Are X and Y independent?\nProblem 62. (Simple Joint Probability)\nLet X and Y each have range {1,2,3,4}. The following formula gives their joint pmf\ni + j\nP(X = i, Y = j) = 80\nCompute each of the following:\n(a) P(X = Y).\n(b) P(XY = 6).\n(c) P(1 ≤X≤2, 2 < Y ≤4).\nProblem 63. Toss a fair coin 3 times. Let X = the number of heads on the first toss, Y\nthe total number of heads on the last two tosses, and F the number of heads on the first\ntwo tosses.\n(a) Give the joint probability table for X and Y . Compute Cov(X, Y ).\n(b) Give the joint probability table for X and F . Compute Cov(X, F ).\nProblem 64. Covariance and Independence\nLet X be a random variable that takes values -2, -1, 0, 1, 2; each with probability 1/5.\nLet Y = X2.\n(a) Fill out the following table giving the joint frequency function for X and Y . Be sure\nto include the marginal probabilities.\nX\nY\n-2\n-1\ntotal\ntotal\n\nPractice Exam 1: All Questions, Spring 2022\n(b) Find E[X] and E[Y ].\n(c) Show X and Y are not independent.\n(d) Show Cov(X, Y) = 0.\nThis is an example of uncorrelated but non-independent random variables. The reason\nthis can happen is that correlation only measures the linear dependence between the two\nvariables. In this case, X and Y are not at all linearly related.\nProblem 65. Continuous Joint Distributions\nSuppose X and Y are continuous random variables with joint density function f(x, y) = x+y\non the unit square [0, 1] × [0, 1].\n(a) Let F (x, y) be the joint CDF. Compute F (1, 1). Compute F (x, y).\n(b) Compute the marginal densities for X and Y .\n(c) Are X and Y independent?\n(d) Compute E[X], E[Y ], E[X2 + Y 2], Cov(X, Y ).\nProblem 66. Correlation\nFlip a coin 3 times. Use a joint pmf table to compute the covariance and correlation between\nthe number of heads on the first 2 and the number of heads on the last 2 flips.\nProblem 67. Correlation\nFlip a coin 5 times. Use properties of covariance to compute the covariance and correlation\nbetween the number of heads on the first 3 and last 3 flips.\n8 Law of Large Numbers, Central Limit Theorem\nProblem 68. (Table of normal probabilities)\nUse the table of standard normal probabilities to compute the following. (Z is the standard\nnormal.)\n(a) (i) P (Z ≤ 1.5)\n(ii) P (-1.5 < Z < 1.5)\nP (Z > -0.75).\n(b) Suppose X ∼ N(2, (0.5)2). Find (i) P(X ≤ 2)\n(ii) P(1 < X ≤ 1.75).\nProblem 69.\nSuppose X1, ... , X100 are i.i.d. with mean 1/5 and variance 1/9. Use the\ncentral limit theorem to estimate P (∑ Xi < 30).\nProblem 70. All or None\nYou have $100 and, never mind why, you must convert it to $1000. Anything less is no\ngood. Your only way to make money is to gamble for it. Your chance of winning one bet is\np.\nHere are two extreme strategies:\nMaximum strategy: bet as much as you can each time. To be smart, if you have less than\n$500 you bet it all. If you have more, you bet enough to get to $1000.\n\nPractice Exam 1: All Questions, Spring 2022\nMinimum strategy: bet $1 each time.\nIf p < 0.5 (the odds are against you) which is the better strategy?\nWhat about p > 0.5?\nProblem 71. (Central Limit Theorem)\nLet X1, X2, ... , X81 be i.i.d., each with expected value μ = E[Xi] = 5, and variance σ2 =\nVar(Xi) = 4. Approximate P(X1 + X2 + ⋯X81 > 369), using the central limit theorem.\nProblem 72. (Binomial ≈ normal)\nLet X ∼ binomial(100,1/3).\nAn 'exact' computation in R gives P (X ≤ 30) = 0.2765539. Use the central limit theorem\nto give an approximation of P(X ≤ 30)\nProblem 73. (More Central Limit Theorem)\nThe average IQ in a population is 100 with standard deviation 15 (by definition, IQ is\nnormalized so this is the case). What is the probability that a randomly selected group of\n100 people has an average IQ above 115?\nProblem 74. Hospitals (binomial, CLT, etc)\n- A certain town is served by two hospitals.\n- Larger hospital: about 45 babies born each day.\n- Smaller hospital about 15 babies born each day.\n- For a period of 1 year, each hospital recorded the days on which more than 60% of\nthe babies born were boys.\n(a) Which hospital do you think recorded more such days?\n(i) The larger hospital.\n(ii) The smaller hospital.\n(iii) About the same (that is, within 5% of each other).\n(b) Let Li (resp., Si) be the Bernoulli random variable which takes the value 1 if more\nthan 60% of the babies born in the larger (resp., smaller) hospital on the ith day were boys.\nDetermine the distribution of Li and of Si.\n(c) Let L (resp., S) be the number of days on which more than 60% of the babies born in\nthe larger (resp., smaller) hospital were boys. What type of distribution do L and S have?\nCompute the expected value and variance in each case.\n(d) Via the CLT, approximate the 0.84 quantile of L (resp., S). Would you like to revise\nyour answer to part (a)?\n(e) What is the correlation of L and S? What is the joint pmf of L and S? Visualize the\nregion corresponding to the event L > S. Express P(L > S) as a double sum.\n\nPractice Exam 1: All Questions, Spring 2022\n9 R Problems\nR will not be on the exam. However, these problems will help you understand the concepts\nwe've been studying.\nProblem 75. R simulation\nConsider X1, X2, ...all independent and with distribution N(0, 1). Let Xm be the average\nof X1, ...Xn.\n(a) Give E[Xn] and σXn exactly.\n(b) Use a R simulation to estimate E[Xn] and Var(Xn) for n = 1, 9, 100. (You should use\nthe rnorm function to simulate 1000 samples of each Xj.)\nProblem 76. R Exercise\nLet X1, X2, X3, X4, X5 be independent U(0, 1) random variables.\nLet X = X1 + X2 + X3 and Y = X3 + X4 + X5.\nUse the runif() function to simulate 1000 trials of each of these variables. Use these to\nestimate Cov(X, Y ).\nExtra Credit\nCompute this covariance exactly.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Practice Exam 1a",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_prac_exam01a.pdf",
      "content": "Exam 1 Practice Questions I, 18.05, Spring 2022\nNotes.\nNot every possible problem can be covered in 13 problems. Look at the other review\nproblems as well as the readings, psets and class problems.\nEven the first 13 problems are much longer than the actual test will be,\nProblem 1. In a ballroom dancing class the students are divided into group A and group\nB. There are six people in group A and seven in group B. If four As and four Bs are chosen\nand paired off, how many pairings are possible?\nProblem 2.\nLet A and B be two events. Suppose the probability that neither A or B\noccurs is 2/3. What is the probability that one or both occur?\nProblem 3. Suppose you are taking a multiple-choice test with c choices for each question.\nIn answering a question on this test, the probability that you know the answer is p. If you\ndon't know the answer, you choose one at random. What is the probability that you knew\nthe answer to a question, given that you answered it correctly?\nProblem 4. Two dice are rolled.\nA = 'sum of two dice equals 3'\nB = 'sum of two dice equals 7'\nC = 'at least one of the dice shows a 1'\n(a) What is P (A|C)?\n(b) What is P (B|C)?\n(c) Are A and C independent? What about B and C?\nProblem 5.\nSuppose that P (A) = 0.4, P (B) = 0.3 and P((A∪B)C) = 0.42. Are A and\nB independent?\nProblem 6.\nSuppose that X takes values between 0 and 1 and has probability density\nfunction 2x. Compute Var(X) and Var(X2).\nProblem 7. Suppose 100 people all toss a hat into a box and then proceed to randomly\npick out of a hat. What is the expected number of people to get their own hat back.\nHint: express the number of people who get their own hat as a sum of random variables\nwhose expected value is easy to compute.\nProblem 8.\nLet T be the waiting time for customers in a queue. Suppose that T is\nexponential with pdf f(t) = 2e-2t on [0, inf).\nFind the pdf of the rate at which customers are served R = 1/T .\n\n{\n{\nExam 1 Practice I, Spring 2022\nProblem 9. Suppose that the cdf of X is given by:\n⎧0 for a < 0\n{\nfor 0 ≤ a < 2\nF(a) =\n⎨\nfor 2 ≤ a < 4\n{\n⎩1 for a ≥ 4.\nDetermine the pmf of X.\nProblem 10. Exponential Distribution\nSuppose that buses arrive are scheduled to arrive at a bus stop at noon but are always X\nminutes late, where X is an exponential random variable with probability density function\nfX(x) = λe-λx. Suppose that you arrive at the bus stop precisely at noon.\n(a) Compute the probability that you have to wait for more than five minutes for the bus\nto arrive.\n(b) Suppose that you have already waiting for 10 minutes. Compute the probability that\nyou have to wait an additional five minutes or more.\nProblem 11. Let X and Y be two continuous random variables with joint pdf\nf(x, y) = cx2y(1 + y) for 0 ≤ x ≤ 3 and 0 ≤ y ≤ 3,\nand f(x, y) = 0 otherwise.\n(a) Find the value of c.\n(b) Find the probability P(1 ≤X≤2, 0 ≤Y ≤1).\n(c) Determine the joint cdf, F (a, b), of X and Y for a and b between 0 and 3.\n(d) Find marginal cdf FX(a) for a between 0 and 3.\n(e) Find the marginal pdf fX(x) directly from f(x, y) and check that it is the derivative of\nFX(x).\n(f) Are X and Y independent?\nProblem 12. (Table of normal probabilities)\nUse the table of standard normal probabilities to compute the following. (Z is the standard\nnormal.)\n(a) (i) P (Z ≤ 1.5)\n(ii) P (-1.5 < Z < 1.5)\nP (Z > -0.75).\n(b) Suppose X ∼ N(2, (0.5)2). Find (i) P(X ≤ 2)\n(ii) P(1 < X ≤ 1.75).\nProblem 13. (Central Limit Theorem)\nLet X1, X2, ... , X81 be i.i.d., each with expected value μ = E[Xi] = 5, and variance σ2 =\nVar(Xi) = 4. Approximate P(X1 + X2 + ⋯X81 > 369), using the central limit theorem.\n\nExam 1 Practice I, Spring 2022\nMore problems\nProblem 14.\nThere are 3 arrangements of the word DAD, namely DAD, ADD, and\nDDA. How many arrangements are there of the word PROBABILITY?\nProblem 15.\nA multiple choice exam has 4 choices for each question. A student has\nstudied enough so that the probability they will know the answer to a question is 0.5, the\nprobability that they will be able to eliminate one choice is 0.25, otherwise all 4 choices\nseem equally plausible. If they know the answer they will get the question right. If not they\nhave to guess from the 3 or 4 choices.\nAs the teacher you want the test to measure what the student knows. If the student answers\na question correctly what's the probability they knew the answer?\nProblem 16. Compute the expectation and variance of a Bernoulli(p) random variable.\nProblem 17. Transforming Normal Distributions\nSuppose Z ∼ N(0,1) and Y = eZ.\n(a) Find the cdf FY (a) and pdf fY (y) for Y . (For the CDF, the best you can do is write\nit in terms of Φ the standard normal cdf.)\n(b) We don't have a formula for Φ(z) so we don't have a formula for quantiles. So we have\nto write quantiles in terms of Φ-1.\n(i) Write the 0.33 quantile of Z in terms of Φ-1\n(ii) Write the 0.9 quantile of Y in terms of Φ-1.\n(iii) Find the median of Y .\nProblem 18.\nSuppose that X ∼ Bin(n, 0.5). Find the probability mass function of\nY = 2X.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Practice Exam 1b",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_prac_exam01b.pdf",
      "content": "Exam 1 Practice Questions II, 18.05, Spring 2022\nNotes.\nNot every possible question be covered in 11 problems. Look at the other review problems\nas well as the readings, psets and class problems.\nEven the first 11 problems are much longer than the actual test will be,\nProblem 1.\nA full house in poker is a hand where three cards share one rank and two\ncards share another rank. How many ways are there to get a full-house? What is the\nprobability of getting a full-house?\nProblem 2. Let C and D be two events with P(C) = 0.25, P (D) = 0.45, and P(C∩D) =\n0.1. What is P(Cc ∩D)?\nProblem 3.\nCorrupted by their power, the judges running the popular game show\nAmerica's Next Top Mathematician have been taking bribes from many of the contestants.\nEach episode, a given contestant is either allowed to stay on the show or is kicked off.\nIf the contestant has been bribing the judges they will be allowed to stay with probability\n1. If the contestant has not been bribing the judges, they will be allowed to stay with\nprobability 1/3.\nSuppose that 1/4 of the contestants have been bribing the judges. The same contestants\nbribe the judges in both rounds, i.e., if a contestant bribes them in the first round, they\nbribe them in the second round too (and vice versa).\n(a) If you pick a random contestant who was allowed to stay during the first episode, what\nis the probability that they were bribing the judges?\n(b) If you pick a random contestant, what is the probability that they are allowed to stay\nduring both of the first two episodes?\n(c) If you pick random contestant who was allowed to stay during the first episode, what\nis the probability that they get kicked off during the second episode?\nProblem 4. Suppose now that events A, B and C are mutually independent with\nP(A) = 0.3,\nP(B) = 0.4,\nP(C) = 0.5.\nCompute the following: (Hint: Use a Venn diagram)\n(i) P(A∩B ∩Cc)\n(ii) P(A∩Bc ∩C)\n(iii) P(Ac ∩B ∩C)\nProblem 5.\nSuppose X is a random variable with E[X] = 5 and Var(X) = 2. What is\nE[X2]?\nProblem 6.\n(a) Suppose that X has probability density function fX(x) = λe-λx for\nx ≥ 0. Compute the cdf, FX(x).\n(b) If Y = X2, compute the pdf and cdf of Y .\n\nExam 1 Practice II, Spring 2022\nProblem 7. For each of the following say whether it can be the graph of a cdf. If it can\nbe, say whether the variable is discrete or continuous.\nx\nF (x)\n0.5\n(i)\nx\nF (x)\n0.5\n(ii)\nx\nF (x)\n0.5\n(iii)\nx\nF (x)\n0.5\n(iv)\nx\nF (x)\n0.5\n(v)\nx\nF (x)\n0.5\n(vi)\nx\nF (x)\n0.5\n(vii)\nx\nF (x)\n0.5\n(viii)\nProblem 8. Compute the median for the exponential distribution with parameter λ.\nProblem 9. (Another Arithmetic Puzzle)\nLet X and Y be two independent Bernoulli(0.5) random variables. Define S and T by:\nS = X+ Y\nand T = X-Y.\n(a) Find the joint and marginal pmf's for S and T .\n(b) Are S and T independent.\nProblem 10. Let X and Y be two random variables and let r, s, t, and u be real numbers.\n(a) Show that Cov(X+ s, Y + u) = Cov(X, Y ).\n(b) Show that Cov(rX, tY ) = rtCov(X, Y ).\n(c) Show that Cov(rX + s, tY + u) = rtCov(X, Y ).\nProblem 11. (More Central Limit Theorem)\nThe average IQ in a population is 100 with standard deviation 15 (by definition, IQ is\nnormalized so this is the case). What is the probability that a randomly selected group of\n100 people has an average IQ above 115?\n\nExam 1 Practice II, Spring 2022\nMore problems\nProblem 12. 20 politicians are having a tea party, 6 Democrats and 14 Republicans. To\nprepare, they need to choose:\n3 people to set the table, 2 people to boil the water, 6 people to make the scones.\nEach person can only do 1 task. (Note that this doesn't add up to 20. The rest of the\npeople don't help.)\n(a) In how many different ways can they choose which people perform these tasks?\n(b) Suppose that the Democrats all hate tea. If they only give tea to 10 of the 20 people,\nwhat is the probability that they only give tea to Republicans?\n(c) If they only give tea to 10 of the 20 people, what is the probability that they give tea\nto 9 Republicans and 1 Democrat?\nProblem 13.\nMore cards! Suppose you want to divide a 52 card deck into four hands\nwith 13 cards each. What is the probability that each hand has a king?\nProblem 14. There is a screening test for prostate cancer that looks at the level of PSA\n(prostate-specific antigen) in the blood. There are a number of reasons besides prostate\ncancer that a man can have elevated PSA levels. In addition, many types of prostate cancer\ndevelop so slowly that that they are never a problem. Unfortunately there is currently no\ntest to distinguish the different types and using the test is controversial because it is hard\nto quantify the accuracy rates and the harm done by false positives.\nFor this problem we'll call a positive test a true positive if it catches a dangerous type of\nprostate cancer. We'll assume the following numbers:\nRate of prostate cancer among men over 50 = 0.0005\nTrue positive rate for the test = 0.9\nFalse positive rate for the test = 0.01\nLet T be the event a man has a positive test and let D be the event a man has a dangerous\ntype of the disease. Find P (D|T ) and P(D|T c).\nProblem 15. Let X be a discrete random variable with pmf p given by:\nx\n-2\n-1\np(x) 1/15 2/15 3/15 4/15 5/15\n(a) Let Y = X2. Find the pmf of Y .\n(b) Find the value the cdf of X at -3/2, 3/4, 7/8, 1, 1.5, 5.\n(c) Find the value the cdf of Y at -3/2, 3/4, 7/8, 1, 1.5, 5.\nProblem 16. Suppose I play a gambling game with even odds. So, I can wager b dollars\nand I either win or lose b dollars with probability p = 0.5.\nI employ the following strategy to try to guarantee that I win some money.\n\nExam 1 Practice II, Spring 2022\nI bet $1; if I lose, I double my bet to $2, if I lose I double my bet again. I continue until\nI win. Eventually I'm sure to win a bet and net $1 (run through the first few rounds and\nyou'll see why this is the net).\nIf this really worked casinos would be out of business. Our goal in this problem is to\nunderstand the flaw in the strategy.\n(a) Let X be the amount of money bet on the last game (the one I win). X takes values\n1, 2, 4, 8, .... Determine the probability mass function for X. That is, find p(2k), where k\nis in {0, 1, 2, ...}.\n(b) Compute E[X].\n(c) Use your answer in part (b) to explain why the stategy is a bad one.\nProblem 17. Normal Distribution: Throughout these problems, let φ and Φ be the\npdf and cdf, respectively, of the standard normal distribution Suppose Z is a standard\nnormal random variable and let X = 3Z+ 1.\n(a) Express P(X ≤ x) in terms of Φ\n(b) Differentiate the expression from (a) with respect to x to get the pdf of X, f(x).\nRemember that Φ′(z) = φ(z) and don't forget the chain rule\n(c) Find P(-1 ≤X ≤1)\n(d) Recall that the probability that Z is within one standard deviation of its mean is\napproximately 68%. What is the probability that X is within one standard deviation of its\nmean?\nProblem 18. Toss a fair coin 3 times. Let X = the number of heads on the first toss, Y\nthe total number of heads on the last two tosses, and F the number of heads on the first\ntwo tosses.\n(a) Give the joint probability table for X and Y . Compute Cov(X, Y ).\n(b) Give the joint probability table for X and F . Compute Cov(X, F ).\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Exam 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_exam02.pdf",
      "content": "Name\n18.05 Exam 2\n5 part I (concept) questions, 5 part II problems. Do all your work on the paper provided\nNo books or calculators.\nYou may have one side of a 8.5×11 piece of paper with any information you like on it.\nProbability tables: Use the probability tables at the end of this exam to do probability\ncomputations for normal, t and chi-square distributions.\nCheat Sheet\nProblem 0. (5 pts) Be sure to attach your cheat sheet to your test.\n\n18.05 Exam 2\nPart I: Concept Questions\nAnswer the Part I questions on directly on the exam pages.\nProblem I.1. (10 pts: 2,2,2,2,2) Determine which of the following concepts/statements\nare Frequentist and which are Bayesian. Note: a concept can be either or both. Give a\nshort explanation for each answer.\n(a) P-value\n(b) Prior distribution\n(c) Average of data\n(d) There is a 3% probability that the average weight is between 92.3 mg and 100 mg.\n(e) The odds in favor of H0 against HA are 1 to 3.\n\n18.05 Exam 2\nProblem I.2. (6 pts: 3,3) Suppose you run a significance test at significance level 0.025,\nand that the test has a power of 95%.\nFor each part of this problem, give your answer and a short explanation.\n(a) Assuming the null hypothesis, what is the probability of a type I error?\n(i) 0.025\n(ii) 0.05\n(iii) 0.95\n(iv) 0.975\n(v) cannot be determined from the information given.\n(b) Assuming the alternative hypothesis, what is the probability of a type II error?\n(i) 0.025\n(ii) 0.05\n(iii) 0.95\n(iv) 0.975\n(v) cannot be determined from the information given.\n\n18.05 Exam 2\nProblem I.3. (3 pts) The following graphs show the rejection regions and pdfs of the null\nand alternative hypotheses for two different hypothesis tests. Which graph shows the test\nwith the higher power?\nYour answer should be 'left graph' or 'right graph'. Give a short explanation.\nx\nφ(x|H0)\nφ(x|HA)\n.\nreject H0 region\ndo not reject H0 region\nx\nφ(x|H0)\nφ(x|HA)\n.\nreject H0 region\ndo not reject H0 region\nProblem I.4. (3 pts) You find a coin on the street, with some unknown probability θ of\nlanding heads when tossed. Circle the only reasonable prior for θ. (No explanation needed.)\n(i) Uniform([0, 0.5])\n(ii) Beta(2, 2)\n(iii) N(0.5, 0.25).\n\n18.05 Exam 2\nProblem I.5. (4 pts: 2,2) For each of the following: Is the prior conjugate to the given\nlikelihood? In each case, a and b are parameters for the priors.\nhypothesis\ndata\nprior\nlikelihood\n(a)\nθ ∈ [0, 1]\nx\nc1θa(1 - θ)b\n(10\nx )θx(1 - θ)10-x\n(b)\nλ ∈ [0, inf)\nx\nc1λa-1e-bλ\nλe-λx\n\n18.05 Exam 2\nPart II: Problems\nAnswer the Part II questions on the paper provided. You must show your work for\ncredit.\nProblem II.1. (12 pts.)\nThe gamma distribution with shape parameter 3 and unknown rate parameter β has range\n(0, inf) and pdf\nβ3x2\nf(x) =\ne-βx.\nSuppose the data\n1, 1, 2, 3, 5\nwas drawn independently from such a distribution. Find the maximum likelihood estimate\n(MLE) of β.\n(Note: we shouldn't expect to get integer values for the data. So either our measurements\nwere quite crude or we didn't want you to have to do arithmetic with fractions.)\n\n18.05 Exam 2\nProblem II.2. (15 pts)\nA random process produces outcomes labeled A, B and C with probabilities θ/2, θ/2, 1 - θ\nrespectively. Here θ is an unknown parameter with value between 0 and 1. You want to\nknow the value of θ.\nBefore running any experiments you have a prior pdf for θ of f(θ) = 3θ2. You then run the\nprocess five times producing data A, B, C, A, B.\nFind the posterior probability density for θ.\n\n18.05 Exam 2\nProblem II.3. (12 pts.) It is the year 2122 and a small percentage of children are born\nwith an array of superpowers. The usual super strength and ability to make bad jokes in\nthe direst of dire situations won't manifest themselves till puberty. The one superpower\nthat manifests at age 7 is known as Bayesian intelligence. They can think clearly about\nstatistics and can answer virtually any stats question.\nSo, a screening test was developed that asks 7 year-olds to compute the posterior odds that\na 7 year-old who correctly answers the screening question has superpowers. The test is\nquite accurate, but some ordinary children do answer correctly\nSuppose the odds that a random 7 year-old has superpowers is 1/100. Suppose the screening\ntest has a 100% true positive rate and a 10% false positive rate. If a randomly chosen child\ncorrectly answers the question, what are the posterior odds that they have superpowers?\n\n18.05 Exam 2\nProblem II.4. (15 pts: 3,3,3,3,3)\nYou collect data from an experiment and do a one-sided Z-test with the rejection region in\nthe right tail and significance level 0.1. You find the Z-value is 2.\n(a) Which R code computes the critical value for the rejection region?\n(i) pnorm(0.1, 0, 1)\n(ii) pnorm(0.9, 0, 1)\n(iii) pnorm(0.95, 0, 1)\n(iv) pnorm(2.0, 0, 1)\n(v) 1 - pnorm(2.0, 0, 1)\n(vi) qnorm(0.05, 0, 1)\n(vii) qnorm(0.1, 0, 1)\n(viii) qnorm(0.9, 0, 1)\n(ix) qnorm(0.95, 0, 1)\n(b) Using the tables at the end of the exam, compute this critical value.\n(c) Which R code computes the p-value for this experiment?\n(i) pnorm(0.1, 0, 1)\n(ii) pnorm(0.9, 0, 1)\n(iii) pnorm(0.95, 0, 1)\n(iv) pnorm(2.0, 0, 1)\n(v) 1 - pnorm(2.0, 0, 1)\n(vi) qnorm(0.05, 0, 1)\n(vii) qnorm(0.1, 0, 1)\n(viii) qnorm(0.9, 0, 1)\n(ix) qnorm(0.95, 0, 1)\n(d) Using the tables at the end of the exam, compute the p-value.\n(e) Should you reject the null hypothesis?\n\n18.05 Exam 2\nProblem II.5. (15 pts)\nAdult onset diabetes is known to be highly genetically determined. A study was done\ncomparing frequencies of a particular allele in a sample of diabetics and a sample of nondi\nabetics. The data are shown in the following table. (We adjusted the data to make hand\ncalculation easier.)\nDiabetic\nNormal\nBb or bb\nBB\nDo a significance test for whether the frequencies of the alleles is different in the two groups\nat a significance level of 0.05.\n\n18.05 Exam 2\nExtra Paper\n\n18.05 Exam 2\nStandard normal table of left tail probabilities.\nz\nz\nz\nz\nΦ(z) = P (Z ≤ z) for N(0, 1).\n(Use interpolation to estimate\nz values to a 3rd decimal\nplace.)\nΦ(z)\nΦ(z)\nΦ(z)\nΦ(z)\n-4.00\n0.0000\n-2.00\n0.0228\n0.00\n0.5000\n2.00\n0.9772\n-3.95\n0.0000\n-1.95\n0.0256\n0.05\n0.5199\n2.05\n0.9798\n-3.90\n0.0000\n-1.90\n0.0287\n0.10\n0.5398\n2.10\n0.9821\n-3.85\n0.0001\n-1.85\n0.0322\n0.15\n0.5596\n2.15\n0.9842\n-3.80\n0.0001\n-1.80\n0.0359\n0.20\n0.5793\n2.20\n0.9861\n-3.75\n0.0001\n-1.75\n0.0401\n0.25\n0.5987\n2.25\n0.9878\n-3.70\n0.0001\n-1.70\n0.0446\n0.30\n0.6179\n2.30\n0.9893\n-3.65\n0.0001\n-1.65\n0.0495\n0.35\n0.6368\n2.35\n0.9906\n-3.60\n0.0002\n-1.60\n0.0548\n0.40\n0.6554\n2.40\n0.9918\n-3.55\n0.0002\n-1.55\n0.0606\n0.45\n0.6736\n2.45\n0.9929\n-3.50\n0.0002\n-1.50\n0.0668\n0.50\n0.6915\n2.50\n0.9938\n-3.45\n0.0003\n-1.45\n0.0735\n0.55\n0.7088\n2.55\n0.9946\n-3.40\n0.0003\n-1.40\n0.0808\n0.60\n0.7257\n2.60\n0.9953\n-3.35\n0.0004\n-1.35\n0.0885\n0.65\n0.7422\n2.65\n0.9960\n-3.30\n0.0005\n-1.30\n0.0968\n0.70\n0.7580\n2.70\n0.9965\n-3.25\n0.0006\n-1.25\n0.1056\n0.75\n0.7734\n2.75\n0.9970\n-3.20\n0.0007\n-1.20\n0.1151\n0.80\n0.7881\n2.80\n0.9974\n-3.15\n0.0008\n-1.15\n0.1251\n0.85\n0.8023\n2.85\n0.9978\n-3.10\n0.0010\n-1.10\n0.1357\n0.90\n0.8159\n2.90\n0.9981\n-3.05\n0.0011\n-1.05\n0.1469\n0.95\n0.8289\n2.95\n0.9984\n-3.00\n0.0013\n-1.00\n0.1587\n1.00\n0.8413\n3.00\n0.9987\n-2.95\n0.0016\n-0.95\n0.1711\n1.05\n0.8531\n3.05\n0.9989\n-2.90\n0.0019\n-0.90\n0.1841\n1.10\n0.8643\n3.10\n0.9990\n-2.85\n0.0022\n-0.85\n0.1977\n1.15\n0.8749\n3.15\n0.9992\n-2.80\n0.0026\n-0.80\n0.2119\n1.20\n0.8849\n3.20\n0.9993\n-2.75\n0.0030\n-0.75\n0.2266\n1.25\n0.8944\n3.25\n0.9994\n-2.70\n0.0035\n-0.70\n0.2420\n1.30\n0.9032\n3.30\n0.9995\n-2.65\n0.0040\n-0.65\n0.2578\n1.35\n0.9115\n3.35\n0.9996\n-2.60\n0.0047\n-0.60\n0.2743\n1.40\n0.9192\n3.40\n0.9997\n-2.55\n0.0054\n-0.55\n0.2912\n1.45\n0.9265\n3.45\n0.9997\n-2.50\n0.0062\n-0.50\n0.3085\n1.50\n0.9332\n3.50\n0.9998\n-2.45\n0.0071\n-0.45\n0.3264\n1.55\n0.9394\n3.55\n0.9998\n-2.40\n0.0082\n-0.40\n0.3446\n1.60\n0.9452\n3.60\n0.9998\n-2.35\n0.0094\n-0.35\n0.3632\n1.65\n0.9505\n3.65\n0.9999\n-2.30\n0.0107\n-0.30\n0.3821\n1.70\n0.9554\n3.70\n0.9999\n-2.25\n0.0122\n-0.25\n0.4013\n1.75\n0.9599\n3.75\n0.9999\n-2.20\n0.0139\n-0.20\n0.4207\n1.80\n0.9641\n3.80\n0.9999\n-2.15\n0.0158\n-0.15\n0.4404\n1.85\n0.9678\n3.85\n0.9999\n-2.10\n0.0179\n-0.10\n0.4602\n1.90\n0.9713\n3.90\n1.0000\n-2.05\n0.0202\n-0.05\n0.4801\n1.95\n0.9744\n3.95\n1.0000\n\n18.05 Exam 2\nTable of Student t critical values (right-tail)\nThe table shows tdf, p = the 1 - p quantile of t(df).\nWe only give values for p ≤ 0.5. Use symmetry to find the values for p > 0.5, e.g.\nt5, 0.975 = -t5, 0.025\nIn R notation tdf, p = qt(1-p, df).\ndf\\p\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.040\n0.050\n0.100\n0.200\n0.300\n0.400\n0.500\n63.66\n31.82\n21.20\n15.89\n12.71\n10.58\n7.92\n6.31\n3.08\n1.38\n0.73\n0.32\n0.00\n9.92\n6.96\n5.64\n4.85\n4.30\n3.90\n3.32\n2.92\n1.89\n1.06\n0.62\n0.29\n0.00\n5.84\n4.54\n3.90\n3.48\n3.18\n2.95\n2.61\n2.35\n1.64\n0.98\n0.58\n0.28\n0.00\n4.60\n3.75\n3.30\n3.00\n2.78\n2.60\n2.33\n2.13\n1.53\n0.94\n0.57\n0.27\n0.00\n4.03\n3.36\n3.00\n2.76\n2.57\n2.42\n2.19\n2.02\n1.48\n0.92\n0.56\n0.27\n0.00\n3.71\n3.14\n2.83\n2.61\n2.45\n2.31\n2.10\n1.94\n1.44\n0.91\n0.55\n0.26\n0.00\n3.50\n3.00\n2.71\n2.52\n2.36\n2.24\n2.05\n1.89\n1.41\n0.90\n0.55\n0.26\n0.00\n3.36\n2.90\n2.63\n2.45\n2.31\n2.19\n2.00\n1.86\n1.40\n0.89\n0.55\n0.26\n0.00\n3.25\n2.82\n2.57\n2.40\n2.26\n2.15\n1.97\n1.83\n1.38\n0.88\n0.54\n0.26\n0.00\n3.17\n2.76\n2.53\n2.36\n2.23\n2.12\n1.95\n1.81\n1.37\n0.88\n0.54\n0.26\n0.00\n2.92\n2.58\n2.38\n2.24\n2.12\n2.02\n1.87\n1.75\n1.34\n0.86\n0.54\n0.26\n0.00\n2.90\n2.57\n2.37\n2.22\n2.11\n2.02\n1.86\n1.74\n1.33\n0.86\n0.53\n0.26\n0.00\n2.88\n2.55\n2.36\n2.21\n2.10\n2.01\n1.86\n1.73\n1.33\n0.86\n0.53\n0.26\n0.00\n2.86\n2.54\n2.35\n2.20\n2.09\n2.00\n1.85\n1.73\n1.33\n0.86\n0.53\n0.26\n0.00\n2.85\n2.53\n2.34\n2.20\n2.09\n1.99\n1.84\n1.72\n1.33\n0.86\n0.53\n0.26\n0.00\n2.83\n2.52\n2.33\n2.19\n2.08\n1.99\n1.84\n1.72\n1.32\n0.86\n0.53\n0.26\n0.00\n2.82\n2.51\n2.32\n2.18\n2.07\n1.98\n1.84\n1.72\n1.32\n0.86\n0.53\n0.26\n0.00\n2.81\n2.50\n2.31\n2.18\n2.07\n1.98\n1.83\n1.71\n1.32\n0.86\n0.53\n0.26\n0.00\n2.80\n2.49\n2.31\n2.17\n2.06\n1.97\n1.83\n1.71\n1.32\n0.86\n0.53\n0.26\n0.00\n2.79\n2.49\n2.30\n2.17\n2.06\n1.97\n1.82\n1.71\n1.32\n0.86\n0.53\n0.26\n0.00\n2.75\n2.46\n2.28\n2.15\n2.04\n1.95\n1.81\n1.70\n1.31\n0.85\n0.53\n0.26\n0.00\n2.74\n2.45\n2.27\n2.14\n2.04\n1.95\n1.81\n1.70\n1.31\n0.85\n0.53\n0.26\n0.00\n2.74\n2.45\n2.27\n2.14\n2.04\n1.95\n1.81\n1.69\n1.31\n0.85\n0.53\n0.26\n0.00\n2.73\n2.44\n2.27\n2.14\n2.03\n1.95\n1.81\n1.69\n1.31\n0.85\n0.53\n0.26\n0.00\n2.73\n2.44\n2.27\n2.14\n2.03\n1.95\n1.80\n1.69\n1.31\n0.85\n0.53\n0.26\n0.00\n2.72\n2.44\n2.26\n2.13\n2.03\n1.94\n1.80\n1.69\n1.31\n0.85\n0.53\n0.26\n0.00\n2.70\n2.42\n2.25\n2.12\n2.02\n1.94\n1.80\n1.68\n1.30\n0.85\n0.53\n0.26\n0.00\n2.70\n2.42\n2.25\n2.12\n2.02\n1.93\n1.80\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.70\n2.42\n2.25\n2.12\n2.02\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.70\n2.42\n2.24\n2.12\n2.02\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.69\n2.41\n2.24\n2.12\n2.02\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.69\n2.41\n2.24\n2.12\n2.01\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.69\n2.41\n2.24\n2.11\n2.01\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.68\n2.41\n2.24\n2.11\n2.01\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.68\n2.41\n2.24\n2.11\n2.01\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.68\n2.40\n2.24\n2.11\n2.01\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n\n18.05 Exam 2\nTable of χ2 critical values (right-tail)\nThe table shows cdf, p = the 1 - p quantile of χ2(df).\nIn R notation cdf, p = qchisq(1-p, df).\ndf\\p\n0.010\n0.025\n0.050\n0.100\n0.200\n0.300\n0.500\n0.700\n0.800\n0.900\n0.950\n0.975\n0.990\n6.63\n5.02\n3.84\n2.71\n1.64\n1.07\n0.45\n0.15\n0.06\n0.02\n0.00\n0.00\n0.00\n9.21\n7.38\n5.99\n4.61\n3.22\n2.41\n1.39\n0.71\n0.45\n0.21\n0.10\n0.05\n0.02\n11.34\n9.35\n7.81\n6.25\n4.64\n3.66\n2.37\n1.42\n1.01\n0.58\n0.35\n0.22\n0.11\n13.28\n11.14\n9.49\n7.78\n5.99\n4.88\n3.36\n2.19\n1.65\n1.06\n0.71\n0.48\n0.30\n15.09\n12.83\n11.07\n9.24\n7.29\n6.06\n4.35\n3.00\n2.34\n1.61\n1.15\n0.83\n0.55\n16.81\n14.45\n12.59\n10.64\n8.56\n7.23\n5.35\n3.83\n3.07\n2.20\n1.64\n1.24\n0.87\n18.48\n16.01\n14.07\n12.02\n9.80\n8.38\n6.35\n4.67\n3.82\n2.83\n2.17\n1.69\n1.24\n20.09\n17.53\n15.51\n13.36\n11.03\n9.52\n7.34\n5.53\n4.59\n3.49\n2.73\n2.18\n1.65\n21.67\n19.02\n16.92\n14.68\n12.24\n10.66\n8.34\n6.39\n5.38\n4.17\n3.33\n2.70\n2.09\n23.21\n20.48\n18.31\n15.99\n13.44\n11.78\n9.34\n7.27\n6.18\n4.87\n3.94\n3.25\n2.56\n32.00\n28.85\n26.30\n23.54\n20.47\n18.42\n15.34\n12.62\n11.15\n9.31\n7.96\n6.91\n5.81\n33.41\n30.19\n27.59\n24.77\n21.61\n19.51\n16.34\n13.53\n12.00\n10.09\n8.67\n7.56\n6.41\n34.81\n31.53\n28.87\n25.99\n22.76\n20.60\n17.34\n14.44\n12.86\n10.86\n9.39\n8.23\n7.01\n36.19\n32.85\n30.14\n27.20\n23.90\n21.69\n18.34\n15.35\n13.72\n11.65\n10.12\n8.91\n7.63\n37.57\n34.17\n31.41\n28.41\n25.04\n22.77\n19.34\n16.27\n14.58\n12.44\n10.85\n9.59\n8.26\n38.93\n35.48\n32.67\n29.62\n26.17\n23.86\n20.34\n17.18\n15.44\n13.24\n11.59\n10.28\n8.90\n40.29\n36.78\n33.92\n30.81\n27.30\n24.94\n21.34\n18.10\n16.31\n14.04\n12.34\n10.98\n9.54\n41.64\n38.08\n35.17\n32.01\n28.43\n26.02\n22.34\n19.02\n17.19\n14.85\n13.09\n11.69\n10.20\n42.98\n39.36\n36.42\n33.20\n29.55\n27.10\n23.34\n19.94\n18.06\n15.66\n13.85\n12.40\n10.86\n44.31\n40.65\n37.65\n34.38\n30.68\n28.17\n24.34\n20.87\n18.94\n16.47\n14.61\n13.12\n11.52\n50.89\n46.98\n43.77\n40.26\n36.25\n33.53\n29.34\n25.51\n23.36\n20.60\n18.49\n16.79\n14.95\n52.19\n48.23\n44.99\n41.42\n37.36\n34.60\n30.34\n26.44\n24.26\n21.43\n19.28\n17.54\n15.66\n53.49\n49.48\n46.19\n42.58\n38.47\n35.66\n31.34\n27.37\n25.15\n22.27\n20.07\n18.29\n16.36\n54.78\n50.73\n47.40\n43.75\n39.57\n36.73\n32.34\n28.31\n26.04\n23.11\n20.87\n19.05\n17.07\n56.06\n51.97\n48.60\n44.90\n40.68\n37.80\n33.34\n29.24\n26.94\n23.95\n21.66\n19.81\n17.79\n57.34\n53.20\n49.80\n46.06\n41.78\n38.86\n34.34\n30.18\n27.84\n24.80\n22.47\n20.57\n18.51\n63.69\n59.34\n55.76\n51.81\n47.27\n44.16\n39.34\n34.87\n32.34\n29.05\n26.51\n24.43\n22.16\n64.95\n60.56\n56.94\n52.95\n48.36\n45.22\n40.34\n35.81\n33.25\n29.91\n27.33\n25.21\n22.91\n66.21\n61.78\n58.12\n54.09\n49.46\n46.28\n41.34\n36.75\n34.16\n30.77\n28.14\n26.00\n23.65\n67.46\n62.99\n59.30\n55.23\n50.55\n47.34\n42.34\n37.70\n35.07\n31.63\n28.96\n26.79\n24.40\n68.71\n64.20\n60.48\n56.37\n51.64\n48.40\n43.34\n38.64\n35.97\n32.49\n29.79\n27.57\n25.15\n69.96\n65.41\n61.66\n57.51\n52.73\n49.45\n44.34\n39.58\n36.88\n33.35\n30.61\n28.37\n25.90\n71.20\n66.62\n62.83\n58.64\n53.82\n50.51\n45.34\n40.53\n37.80\n34.22\n31.44\n29.16\n26.66\n72.44\n67.82\n64.00\n59.77\n54.91\n51.56\n46.34\n41.47\n38.71\n35.08\n32.27\n29.96\n27.42\n73.68\n69.02\n65.17\n60.91\n55.99\n52.62\n47.34\n42.42\n39.62\n35.95\n33.10\n30.75\n28.18\n74.92\n70.22\n66.34\n62.04\n57.08\n53.67\n48.33\n43.37\n40.53\n36.82\n33.93\n31.55\n28.94\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Practice Exam 2a",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_prac_exam02a.pdf",
      "content": "Exam 2 Practice Questions, 18.05, Spring 2022\nNote: This is a set of practice problems for exam 2. The actual exam will be much shorter.\nWithin each section we've arranged the problems roughly in order of difficulty.\n1 Topics\n- Statistics: data, MLE\n- Bayesian inference: prior, likelihood, posterior, predictive probability, probability in\ntervals\n- Frequentist inference: NHST\n2 Using the probability tables\nYou should become familiar with the probability tables at the end of these notes.\nProblem 1.\nUse the standard normal table to find the following values. In all the\nproblems Z is a standard normal random variable.\n(a) (i) P (Z < 1.5)\n(ii) P (Z > 1.5)\n(iii) P (-1.5 < Z < 1.5)\n(iv) P (Z ≤ 1.625)\n(b) (i) The right-tail with probability α = 0.05.\n(ii) The two-sided rejection region with probability α = 0.2.\n(iii) Find the range for the middle 50% of probability.\nProblem 2. The t-tables are different. They give the right critical values corresponding\nto probabilities. To save space we only give critical values for p ≤ 0.5. You need to use\nthe symmetry of the t-distribution to get them for p < 0.5. That is, tdf, p = -tdf, 1-p, e.g.\nt5, 0.975 = -t5, 0.025.\nUse the t-table to estimate the following values. In all the problems T is a random variable\ndrawn from a t-distribution with the indicated number of degrees of freedom.\n(a) (i) P (T > 1.6), with df = 3\n(ii) P (T < 1.6) with df = 3\n(iii) P (-1.68 < T < 1.68) with df = 49\n(iv) P (-1.6 < T < 1.6) with df = 49\n(b) (i) The critical value for probability α = 0.05 for 8 degrees of freedom.\n(ii) The two-sided rejection region with probability α = 0.2 for 16 degrees of freedom.\n(iii) Find the range for the middle 50% of probability with df = 20.\nProblem 3.\nThe chi-square tables are different. They give the right critical values\ncorresponding to probabilities.\n\nExam 2 Practice 2, Spring 2022\nUse the chi-square tables table to find the following values. In all the problems X2 is\na random variable drawn from a χ2-distribution with the indicated number of degrees of\nfreedom.\n(a) (i) P(X2 > 1.6), with df = 3\n(ii) P(X2 > 20) with df = 16\n(b) (i) The right critical value for probability α = 0.05 for 8 degrees of freedom.\n(ii) The two-sided rejection region with probability α = 0.2 for 16 degrees of freedom.\n3 Data\nProblem 4. The following data is from a random sample: 5, 1, 3, 3, 8.\nCompute the sample mean, sample standard deviation and sample median.\n4 MLE\nProblem 5. (a) A coin is tossed 100 times and lands heads 62 times. Find the maximum\nlikelihood estimate for the probability θ of heads.\n(b) A coin is tossed n times and lands heads k times. Find the maximum likelihood\nestimate for the probability θ of heads.\nProblem 6. Suppose the data set y1, ... , yn is a drawn from a random sample consisting\nof i.i.d. discrete uniform distributions with range 1 to N . Find the maximum likelihood\nestimate of N.\nProblem 7.\nSuppose data x1, ... , xn is drawn from an exponential distribution exp(λ).\nFind the maximum likelihood for λ.\nProblem 8.\nSuppose x1, ... , xn is a data set drawn from a geometric(1/a) distribution.\nFind the maximum likelihood estimate of a. Here, geometric(p) means the probability of\nsuccess is p and we run trials until the first success and report the total number of trials,\nincluding the success. For example, the sequence F F F F S is 4 failures followed by a success,\nwhich produces x = 5.\nProblem 9.\nYou want to estimate the size of an MIT class that is closed to visitors.\nYou know that the students are numbered from 1 to n, where n is the number of students.\nYou call three random students out of the classroom and ask for their numbers, which turn\nout to be 1, 3, 7. Find the maximum likelihood estimate for n. (Hint: the student #'s are\ndrawn from a discrete uniform distribution.)\n\nExam 2 Practice 2, Spring 2022\n5 Bayesian updating: discrete prior, discrete likelihood\nProblem 10. Twins\n(a) Suppose 1/4 of twins are identical and 3/4 of twins are fraternal. If you are pregnant\nwith twins of the same sex, what is the probability that they are identical?\n(b) Find the posterior odds the twins are identical. Do this by multiplying the prior odds\nby the Bayes factor (likelihood ratio). Check this by computing the odds directly from your\nanswer to part (a).\nProblem 11. Dice.\nYou have a drawer full of 4, 6, 8, 12 and 20-sided dice. You suspect that they are in\nproportion 1:2:10:2:1. Your friend picks one at random and rolls it twice getting 5 both\ntimes.\n(a) What is the probability your friend picked the 8-sided die?\n(b) (i) What is the probability the next roll will be a 5?\n(ii) What is the probability the next roll will be a 15?\nProblem 12.\nSameer has two coins: one fair coin and one biased coin which lands heads with probability\n3/4. He picks one coin at random (50-50) and flips it repeatedly until he gets a tails.\nAssume that he observes 3 heads before the first tails.\n(a) What are the prior and posterior odds for the fair coin?\n(b) What are the prior and posterior predictive probabilities of heads on the next flip?\nHere prior predictive means prior to considering the data of the first four flips.\n6 Bayesian Updating: continuous prior, discrete likelihood\nProblem 13.\nPeter and Jerry disagree over whether 18.05 students prefer Bayesian or\nfrequentist statistics. They decide to pick a random sample of 10 students from the class\nand get Shelby to ask each student which they prefer. They agree to start with a prior\nf(θ) ∼ Beta(2, 2), where θ is the percent that prefer Bayesian.\n(a) Let x1 be the number of people in the sample who prefer Bayesian statistics. What is\nthe pmf of x1?\n(b) Compute the posterior distribution of θ given x1 = 6.\n(c) Use R to compute 50% and 90% probability intervals for θ. Center the intervals so\nthat the leftover probability in both tails is the same.\n(d) The maximum a posteriori (MAP) estimate of θ (the peak of the posterior) is given\nby θ = 7/12, leading Jerry to concede that a majority of students are Bayesians. In light\nof your answer to part (c) does Jerry have a strong case?\n(e) They decide to get another sample of 10 students and ask Neil to poll them. Write\ndown in detail the expression for the posterior predictive probability that the majority of\n\nExam 2 Practice 2, Spring 2022\nthe second sample prefer Bayesian statistics. The result will be an integral with several\nterms. Don't bother computing the integral.\nProblem 14. Coins\nWe have a 'bent' coin with an unknown probability θ of heads. Assume the following:\n- Prior for the value of θ: f(θ) = 2(1 - θ) on [0, 1].\n- Data: toss once and get tails.\n(a) Find the posterior pdf to this data.\n(b) Suppose you toss again and get tails. Update your posterior from part (a) using this\ndata.\n(c) On one set of axes graph the prior and the posteriors from parts (a) and (b).\nProblem 15. Take your medicine\nA lab has an experimental treatment for a disease. The treatment will cure an unknown\nfraction θ of the patients it's used on. Because it is brand new, they have no idea what θ\nis, so they use a flat prior f(θ) = 1.\nIn a small preliminary study, the treatment cured 16 out of 20 patients.\nUse this data to find the posterior pdf for θ.\nWrite an integral formula for the normalizing factor (total probability of the data), but do\nnot compute it. Call its value T and give the posterior pdf in terms of T .\n7 Bayesian Updating: normal-normal conjugate pairs\nProblem 16.\nSuppose that you have a cable whose exact length is θ. You have a ruler\nwith known error normally distributed with mean 0 and variance 10-4. Using this ruler,\nyou measure your cable, and the resulting measurement x is distributed as N(θ, 10-4).\n(a) Suppose your prior on the length of the cable is θ ∼ N(9, 1). If you then measure\nx = 10, what is your posterior pdf for θ?\n(b) With the same prior as in part (a), compute the total number of measurements needed\nso that the posterior variance of θ is less than 10-6.\n8 NHST\nProblem 17. z-test\nSuppose we have 49 data points with sample mean 6.25 and sample variance 12. We want\nto test the following hypotheses\nH0: the data is drawn from a N(4, 102) distribution.\nHA: the data is drawn from N(μ, 102) where μ = 4.\n(a) Test for significance at the α = 0.05 level. Use the tables at the end of this file to\ncompute p-values.\n\nExam 2 Practice 2, Spring 2022\n(b) Draw a picture showing the null pdf, the rejection region and the area used to compute\nthe p-value.\nProblem 18. t-test\nSuppose we have 49 data points with sample mean 6.25 and sample variance 36. We want\nto test the following hypotheses:\n(a) H0: the data is drawn from N(4, σ2), where σ is unknown.\nHA: the data is drawn from N(μ, σ2) where μ = 4.\nTest for significance at the α = 0.05 level. Use the t-table to find the p value.\n(b) Draw a picture showing the null pdf, the rejection region and the area used to compute\nthe p-value for part (a).\nProblem 19.\nThere are lots of good NHST problems in psets 7 and 8, the reading and\nin-class problems, including two-sample t test, chi-square, ANOVA, and F-test for equal\nvariance.\nProblem 20. Probability, MLE, goodness of fit\nThere was a multicenter test of the rate of success for a certain medical procedure. At each\nof the 60 centers the researchers tested 12 subjects and reported the number of successes.\n(a) Assume that θ is the probability of success for one patient and let x be the data from\none center. What is the probability mass function of x?\n(b) Assume that the probability of success θ is the same at each center and the 60 centers\nproduced data:\nx1, x2, ... , x60. Find the MLE for θ. Write your answer in terms of x\nParts (c-e) use the following table which gives counts from 60 centers, e.g. x = 2 occurred\nin 17 out of 60 centers.\nx\ncounts\nNote, the possible values of x are 0 to 12. The table shows that x > 5 never occurred. (c)\nCompute x the average number of successes over the 60 centers.\n(d) Assuming the probability of success at each center is the same, show that the MLE\nfor θ is θ = 0.1958.\n(e) Do a χ2 goodness of fit to test the assumption that the probability of success is the\nsame at each center. Find the p-value and use a significance level of 0.05.\nIn this test the number of degrees of freedom is the number of bins - 2.\n\nExam 2 Practice 2, Spring 2022\nStandard normal table of left tail probabilities.\nz\nz\nz\nz\nΦ(z) = P (Z ≤ z) for N(0, 1).\n(Use interpolation to estimate\nz values to a 3rd decimal\nplace.)\nΦ(z)\nΦ(z)\nΦ(z)\nΦ(z)\n-4.00\n0.0000\n-2.00\n0.0228\n0.00\n0.5000\n2.00\n0.9772\n-3.95\n0.0000\n-1.95\n0.0256\n0.05\n0.5199\n2.05\n0.9798\n-3.90\n0.0000\n-1.90\n0.0287\n0.10\n0.5398\n2.10\n0.9821\n-3.85\n0.0001\n-1.85\n0.0322\n0.15\n0.5596\n2.15\n0.9842\n-3.80\n0.0001\n-1.80\n0.0359\n0.20\n0.5793\n2.20\n0.9861\n-3.75\n0.0001\n-1.75\n0.0401\n0.25\n0.5987\n2.25\n0.9878\n-3.70\n0.0001\n-1.70\n0.0446\n0.30\n0.6179\n2.30\n0.9893\n-3.65\n0.0001\n-1.65\n0.0495\n0.35\n0.6368\n2.35\n0.9906\n-3.60\n0.0002\n-1.60\n0.0548\n0.40\n0.6554\n2.40\n0.9918\n-3.55\n0.0002\n-1.55\n0.0606\n0.45\n0.6736\n2.45\n0.9929\n-3.50\n0.0002\n-1.50\n0.0668\n0.50\n0.6915\n2.50\n0.9938\n-3.45\n0.0003\n-1.45\n0.0735\n0.55\n0.7088\n2.55\n0.9946\n-3.40\n0.0003\n-1.40\n0.0808\n0.60\n0.7257\n2.60\n0.9953\n-3.35\n0.0004\n-1.35\n0.0885\n0.65\n0.7422\n2.65\n0.9960\n-3.30\n0.0005\n-1.30\n0.0968\n0.70\n0.7580\n2.70\n0.9965\n-3.25\n0.0006\n-1.25\n0.1056\n0.75\n0.7734\n2.75\n0.9970\n-3.20\n0.0007\n-1.20\n0.1151\n0.80\n0.7881\n2.80\n0.9974\n-3.15\n0.0008\n-1.15\n0.1251\n0.85\n0.8023\n2.85\n0.9978\n-3.10\n0.0010\n-1.10\n0.1357\n0.90\n0.8159\n2.90\n0.9981\n-3.05\n0.0011\n-1.05\n0.1469\n0.95\n0.8289\n2.95\n0.9984\n-3.00\n0.0013\n-1.00\n0.1587\n1.00\n0.8413\n3.00\n0.9987\n-2.95\n0.0016\n-0.95\n0.1711\n1.05\n0.8531\n3.05\n0.9989\n-2.90\n0.0019\n-0.90\n0.1841\n1.10\n0.8643\n3.10\n0.9990\n-2.85\n0.0022\n-0.85\n0.1977\n1.15\n0.8749\n3.15\n0.9992\n-2.80\n0.0026\n-0.80\n0.2119\n1.20\n0.8849\n3.20\n0.9993\n-2.75\n0.0030\n-0.75\n0.2266\n1.25\n0.8944\n3.25\n0.9994\n-2.70\n0.0035\n-0.70\n0.2420\n1.30\n0.9032\n3.30\n0.9995\n-2.65\n0.0040\n-0.65\n0.2578\n1.35\n0.9115\n3.35\n0.9996\n-2.60\n0.0047\n-0.60\n0.2743\n1.40\n0.9192\n3.40\n0.9997\n-2.55\n0.0054\n-0.55\n0.2912\n1.45\n0.9265\n3.45\n0.9997\n-2.50\n0.0062\n-0.50\n0.3085\n1.50\n0.9332\n3.50\n0.9998\n-2.45\n0.0071\n-0.45\n0.3264\n1.55\n0.9394\n3.55\n0.9998\n-2.40\n0.0082\n-0.40\n0.3446\n1.60\n0.9452\n3.60\n0.9998\n-2.35\n0.0094\n-0.35\n0.3632\n1.65\n0.9505\n3.65\n0.9999\n-2.30\n0.0107\n-0.30\n0.3821\n1.70\n0.9554\n3.70\n0.9999\n-2.25\n0.0122\n-0.25\n0.4013\n1.75\n0.9599\n3.75\n0.9999\n-2.20\n0.0139\n-0.20\n0.4207\n1.80\n0.9641\n3.80\n0.9999\n-2.15\n0.0158\n-0.15\n0.4404\n1.85\n0.9678\n3.85\n0.9999\n-2.10\n0.0179\n-0.10\n0.4602\n1.90\n0.9713\n3.90\n1.0000\n-2.05\n0.0202\n-0.05\n0.4801\n1.95\n0.9744\n3.95\n1.0000\n\nExam 2 Practice 2, Spring 2022\nTable of Student t critical values (right-tail)\nThe table shows tdf, p = the 1 - p quantile of t(df).\nWe only give values for p ≤ 0.5. Use symmetry to find the values for p > 0.5, e.g.\nt5, 0.975 = -t5, 0.025\nIn R notation tdf, p = qt(1-p, df).\ndf\\p\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.040\n0.050\n0.100\n0.200\n0.300\n0.400\n0.500\n63.66\n31.82\n21.20\n15.89\n12.71\n10.58\n7.92\n6.31\n3.08\n1.38\n0.73\n0.32\n0.00\n9.92\n6.96\n5.64\n4.85\n4.30\n3.90\n3.32\n2.92\n1.89\n1.06\n0.62\n0.29\n0.00\n5.84\n4.54\n3.90\n3.48\n3.18\n2.95\n2.61\n2.35\n1.64\n0.98\n0.58\n0.28\n0.00\n4.60\n3.75\n3.30\n3.00\n2.78\n2.60\n2.33\n2.13\n1.53\n0.94\n0.57\n0.27\n0.00\n4.03\n3.36\n3.00\n2.76\n2.57\n2.42\n2.19\n2.02\n1.48\n0.92\n0.56\n0.27\n0.00\n3.71\n3.14\n2.83\n2.61\n2.45\n2.31\n2.10\n1.94\n1.44\n0.91\n0.55\n0.26\n0.00\n3.50\n3.00\n2.71\n2.52\n2.36\n2.24\n2.05\n1.89\n1.41\n0.90\n0.55\n0.26\n0.00\n3.36\n2.90\n2.63\n2.45\n2.31\n2.19\n2.00\n1.86\n1.40\n0.89\n0.55\n0.26\n0.00\n3.25\n2.82\n2.57\n2.40\n2.26\n2.15\n1.97\n1.83\n1.38\n0.88\n0.54\n0.26\n0.00\n3.17\n2.76\n2.53\n2.36\n2.23\n2.12\n1.95\n1.81\n1.37\n0.88\n0.54\n0.26\n0.00\n2.92\n2.58\n2.38\n2.24\n2.12\n2.02\n1.87\n1.75\n1.34\n0.86\n0.54\n0.26\n0.00\n2.90\n2.57\n2.37\n2.22\n2.11\n2.02\n1.86\n1.74\n1.33\n0.86\n0.53\n0.26\n0.00\n2.88\n2.55\n2.36\n2.21\n2.10\n2.01\n1.86\n1.73\n1.33\n0.86\n0.53\n0.26\n0.00\n2.86\n2.54\n2.35\n2.20\n2.09\n2.00\n1.85\n1.73\n1.33\n0.86\n0.53\n0.26\n0.00\n2.85\n2.53\n2.34\n2.20\n2.09\n1.99\n1.84\n1.72\n1.33\n0.86\n0.53\n0.26\n0.00\n2.83\n2.52\n2.33\n2.19\n2.08\n1.99\n1.84\n1.72\n1.32\n0.86\n0.53\n0.26\n0.00\n2.82\n2.51\n2.32\n2.18\n2.07\n1.98\n1.84\n1.72\n1.32\n0.86\n0.53\n0.26\n0.00\n2.81\n2.50\n2.31\n2.18\n2.07\n1.98\n1.83\n1.71\n1.32\n0.86\n0.53\n0.26\n0.00\n2.80\n2.49\n2.31\n2.17\n2.06\n1.97\n1.83\n1.71\n1.32\n0.86\n0.53\n0.26\n0.00\n2.79\n2.49\n2.30\n2.17\n2.06\n1.97\n1.82\n1.71\n1.32\n0.86\n0.53\n0.26\n0.00\n2.75\n2.46\n2.28\n2.15\n2.04\n1.95\n1.81\n1.70\n1.31\n0.85\n0.53\n0.26\n0.00\n2.74\n2.45\n2.27\n2.14\n2.04\n1.95\n1.81\n1.70\n1.31\n0.85\n0.53\n0.26\n0.00\n2.74\n2.45\n2.27\n2.14\n2.04\n1.95\n1.81\n1.69\n1.31\n0.85\n0.53\n0.26\n0.00\n2.73\n2.44\n2.27\n2.14\n2.03\n1.95\n1.81\n1.69\n1.31\n0.85\n0.53\n0.26\n0.00\n2.73\n2.44\n2.27\n2.14\n2.03\n1.95\n1.80\n1.69\n1.31\n0.85\n0.53\n0.26\n0.00\n2.72\n2.44\n2.26\n2.13\n2.03\n1.94\n1.80\n1.69\n1.31\n0.85\n0.53\n0.26\n0.00\n2.70\n2.42\n2.25\n2.12\n2.02\n1.94\n1.80\n1.68\n1.30\n0.85\n0.53\n0.26\n0.00\n2.70\n2.42\n2.25\n2.12\n2.02\n1.93\n1.80\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.70\n2.42\n2.25\n2.12\n2.02\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.70\n2.42\n2.24\n2.12\n2.02\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.69\n2.41\n2.24\n2.12\n2.02\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.69\n2.41\n2.24\n2.12\n2.01\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.69\n2.41\n2.24\n2.11\n2.01\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.68\n2.41\n2.24\n2.11\n2.01\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.68\n2.41\n2.24\n2.11\n2.01\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.68\n2.40\n2.24\n2.11\n2.01\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n\nExam 2 Practice 2, Spring 2022\nTable of χ2 critical values (right-tail)\nThe table shows cdf, p = the 1 - p quantile of χ2(df).\nIn R notation cdf, p = qchisq(1-p, df).\ndf\\p\n0.010\n0.025\n0.050\n0.100\n0.200\n0.300\n0.500\n0.700\n0.800\n0.900\n0.950\n0.975\n0.990\n6.63\n5.02\n3.84\n2.71\n1.64\n1.07\n0.45\n0.15\n0.06\n0.02\n0.00\n0.00\n0.00\n9.21\n7.38\n5.99\n4.61\n3.22\n2.41\n1.39\n0.71\n0.45\n0.21\n0.10\n0.05\n0.02\n11.34\n9.35\n7.81\n6.25\n4.64\n3.66\n2.37\n1.42\n1.01\n0.58\n0.35\n0.22\n0.11\n13.28\n11.14\n9.49\n7.78\n5.99\n4.88\n3.36\n2.19\n1.65\n1.06\n0.71\n0.48\n0.30\n15.09\n12.83\n11.07\n9.24\n7.29\n6.06\n4.35\n3.00\n2.34\n1.61\n1.15\n0.83\n0.55\n16.81\n14.45\n12.59\n10.64\n8.56\n7.23\n5.35\n3.83\n3.07\n2.20\n1.64\n1.24\n0.87\n18.48\n16.01\n14.07\n12.02\n9.80\n8.38\n6.35\n4.67\n3.82\n2.83\n2.17\n1.69\n1.24\n20.09\n17.53\n15.51\n13.36\n11.03\n9.52\n7.34\n5.53\n4.59\n3.49\n2.73\n2.18\n1.65\n21.67\n19.02\n16.92\n14.68\n12.24\n10.66\n8.34\n6.39\n5.38\n4.17\n3.33\n2.70\n2.09\n23.21\n20.48\n18.31\n15.99\n13.44\n11.78\n9.34\n7.27\n6.18\n4.87\n3.94\n3.25\n2.56\n32.00\n28.85\n26.30\n23.54\n20.47\n18.42\n15.34\n12.62\n11.15\n9.31\n7.96\n6.91\n5.81\n33.41\n30.19\n27.59\n24.77\n21.61\n19.51\n16.34\n13.53\n12.00\n10.09\n8.67\n7.56\n6.41\n34.81\n31.53\n28.87\n25.99\n22.76\n20.60\n17.34\n14.44\n12.86\n10.86\n9.39\n8.23\n7.01\n36.19\n32.85\n30.14\n27.20\n23.90\n21.69\n18.34\n15.35\n13.72\n11.65\n10.12\n8.91\n7.63\n37.57\n34.17\n31.41\n28.41\n25.04\n22.77\n19.34\n16.27\n14.58\n12.44\n10.85\n9.59\n8.26\n38.93\n35.48\n32.67\n29.62\n26.17\n23.86\n20.34\n17.18\n15.44\n13.24\n11.59\n10.28\n8.90\n40.29\n36.78\n33.92\n30.81\n27.30\n24.94\n21.34\n18.10\n16.31\n14.04\n12.34\n10.98\n9.54\n41.64\n38.08\n35.17\n32.01\n28.43\n26.02\n22.34\n19.02\n17.19\n14.85\n13.09\n11.69\n10.20\n42.98\n39.36\n36.42\n33.20\n29.55\n27.10\n23.34\n19.94\n18.06\n15.66\n13.85\n12.40\n10.86\n44.31\n40.65\n37.65\n34.38\n30.68\n28.17\n24.34\n20.87\n18.94\n16.47\n14.61\n13.12\n11.52\n50.89\n46.98\n43.77\n40.26\n36.25\n33.53\n29.34\n25.51\n23.36\n20.60\n18.49\n16.79\n14.95\n52.19\n48.23\n44.99\n41.42\n37.36\n34.60\n30.34\n26.44\n24.26\n21.43\n19.28\n17.54\n15.66\n53.49\n49.48\n46.19\n42.58\n38.47\n35.66\n31.34\n27.37\n25.15\n22.27\n20.07\n18.29\n16.36\n54.78\n50.73\n47.40\n43.75\n39.57\n36.73\n32.34\n28.31\n26.04\n23.11\n20.87\n19.05\n17.07\n56.06\n51.97\n48.60\n44.90\n40.68\n37.80\n33.34\n29.24\n26.94\n23.95\n21.66\n19.81\n17.79\n57.34\n53.20\n49.80\n46.06\n41.78\n38.86\n34.34\n30.18\n27.84\n24.80\n22.47\n20.57\n18.51\n63.69\n59.34\n55.76\n51.81\n47.27\n44.16\n39.34\n34.87\n32.34\n29.05\n26.51\n24.43\n22.16\n64.95\n60.56\n56.94\n52.95\n48.36\n45.22\n40.34\n35.81\n33.25\n29.91\n27.33\n25.21\n22.91\n66.21\n61.78\n58.12\n54.09\n49.46\n46.28\n41.34\n36.75\n34.16\n30.77\n28.14\n26.00\n23.65\n67.46\n62.99\n59.30\n55.23\n50.55\n47.34\n42.34\n37.70\n35.07\n31.63\n28.96\n26.79\n24.40\n68.71\n64.20\n60.48\n56.37\n51.64\n48.40\n43.34\n38.64\n35.97\n32.49\n29.79\n27.57\n25.15\n69.96\n65.41\n61.66\n57.51\n52.73\n49.45\n44.34\n39.58\n36.88\n33.35\n30.61\n28.37\n25.90\n71.20\n66.62\n62.83\n58.64\n53.82\n50.51\n45.34\n40.53\n37.80\n34.22\n31.44\n29.16\n26.66\n72.44\n67.82\n64.00\n59.77\n54.91\n51.56\n46.34\n41.47\n38.71\n35.08\n32.27\n29.96\n27.42\n73.68\n69.02\n65.17\n60.91\n55.99\n52.62\n47.34\n42.42\n39.62\n35.95\n33.10\n30.75\n28.18\n74.92\n70.22\n66.34\n62.04\n57.08\n53.67\n48.33\n43.37\n40.53\n36.82\n33.93\n31.55\n28.94\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Practice Exam 2b",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_prac_exam02b.pdf",
      "content": "18.05 Practice Exam 2b\nNo books or calculators. You may have one side of an 8×11 sheet of paper with any information you like on it.\n7 problems, 7 pages\nSimplifying expressions: You don't need to simplify complicated expressions. For ex\n20!\nample, you can leave\n⋅3 + 1 ⋅5 exactly as is. Likewise for expressions like\n18!2!.\nThe z, t and χ2 tables are at the end of the exam if you need them.\nProblem 1. Concept questions\n(a) A certain august journal publishes psychological research. They will only publish results\nthat are statistically significant when tested at a significance level of 0.05.\nCould all of their published results be true?\nYes\nNo\nCould all of their published results be false?\nYes\nNo\n(b) True or false: Setting the prior probability of a hypothesis to 0 means that no amount of\ndata will make the posterior probability of that hypothesis the maximum over all hypotheses.\nTrue\nFalse\n(c) A researcher collected data that fit the criteria for a two-sided Z-test. He set the\nsignificance level at 0.05. He ran 80 trials and got a z-value of 1.7. This gave a p-value of\n0.0892, so he could not reject the null hypothesis. Convinced that his alternative hypothesis\nwas correct he ran 80 more trials. The combined data from the 160 trials now had a z-value\nof 2.1. He wrote a paper carefully describing his experiments and submitted it to the journal\nin part (a).\nWill the journal publish his results?\nYes\nNo\n(d) Let θ be the probability of heads for a bent coin. Suppose your prior f(θ) is Beta(6, 8).\nAlso suppose you flip the coin 7 times, getting 2 heads and 5 tails. What is the posterior\npdf f(θ|x)?\nProblem 2. The Pareto distribution with parameter α has range [1, inf) and pdf\nα\nf(x) = xα\nSuppose the data\n5, 2, 3\nwas drawn independently from such a distribution. Find the maximum likelihood estimate\n(MLE) of α.\nProblem 3.\nYour friend grabs a die at random from a drawer containing two 4-sided dice, one 8-sided\ndie, and one 12-sided die. They roll the die once and report that the result is 5.\n\n18.05 Practice Exam 2b\n(a) Make a discrete Bayes table showing the prior, likelihood, and posterior for the type\nof die rolled given the data.\n(b) What is the prior predictive probability of rolling a 5?\n(c) What are your posterior odds that the die has 12 sides?\n(d) Given the data of the first roll, what is your probability that the next roll will be a\n7?\nProblem 4.\nEveryone knows that giraffes are tall, but how much do they weigh? Let's\nsuppose that the weight of male giraffes is normally distributed with mean 1200 kg and\nstandard deviation 200 kg.\nI volunteered at the zoo and was given the task of weighing their male giraffe Beau. Now\nweighing a giraffe is not easy and the process produces random errors following a N(0, 1002)\ndistribution. To compensate for the inaccuracy of the scale I weighed Beau three times and\ngot the following measurements:\n1250 kg, 1300 kg, 1350 kg\n.\nWhat is the posterior expected value of Beau's weight?\nProblem 5. Data is drawn from a binomial(5, θ) distribution, where θ is unknown. Here\nis the table of probabilities p(x | θ) for 3 values of θ:\nx\nθ = 0.5\n0.031\n0.156\n0.313\n0.313\n0.156\n0.031\nθ = 0.6\n0.010\n0.077\n0.230\n0.346\n0.259\n0.078\nθ = 0.8\n0.000\n0.006\n0.051\n0.205\n0.410\n0.328\nYou want to run a significance test on the value of θ. You have the following:\nNull hypothesis: θ = 0.5.\nAlternate hypotheses: θ > 0.5.\nSignificance level: α = 0.1.\n(a) Find the rejection region.\n(b) Compute the power of the test for each of the two hypotheses θ = 0.6 and θ = 0.8.\n(c) Suppose you run an experiment and the data gives x = 4. Compute the p-value of this\ndata.\nProblem 6.\nYou have data drawn from a normal distribution with a known variance of\n16. You set up the following NHST:\n- H0: data follows a N(2, 42)\n- HA: data follows a N(μ, 42) where μ = 2.\n- Test statistic: standardized sample mean z.\n- Significance level set to α = 0.05.\n\n18.05 Practice Exam 2b\nYou then collected n = 16 data points with sample mean 1.5.\n(a) Find the rejection region. Draw a graph indicating the null distribution and the rejection\nregion.\n(b) Find the z-value and add it to your picture in part (a).\n(c) Find the p-value for this data and decide whether or not to reject H0 in favor of HA.\nProblem 7.\nSomeone claims to have found a long lost work by Jane Austen. She asks\nyou to decide whether or not the book was actually written by Austen.\nYou buy a copy of Sense and Sensibility and count the frequencies of certain common words\non some randomly selected pages. You do the same thing for the 'long lost work'. You get\nthe following table of counts.\nWord\na\nan\nthis\nthat\nSense and Sensibility\nLong lost work\nUsing this data, set up and evaluate a significance test of the claim that the long lost book\nis by Jane Austen. Use a significance level of 0.1.\n\n18.05 Practice Exam 2b\nStandard normal table of left tail probabilities.\nz\nz\nz\nz\nΦ(z) = P (Z ≤ z) for N(0, 1).\n(Use interpolation to estimate\nz values to a 3rd decimal\nplace.)\nΦ(z)\nΦ(z)\nΦ(z)\nΦ(z)\n-4.00\n0.0000\n-2.00\n0.0228\n0.00\n0.5000\n2.00\n0.9772\n-3.95\n0.0000\n-1.95\n0.0256\n0.05\n0.5199\n2.05\n0.9798\n-3.90\n0.0000\n-1.90\n0.0287\n0.10\n0.5398\n2.10\n0.9821\n-3.85\n0.0001\n-1.85\n0.0322\n0.15\n0.5596\n2.15\n0.9842\n-3.80\n0.0001\n-1.80\n0.0359\n0.20\n0.5793\n2.20\n0.9861\n-3.75\n0.0001\n-1.75\n0.0401\n0.25\n0.5987\n2.25\n0.9878\n-3.70\n0.0001\n-1.70\n0.0446\n0.30\n0.6179\n2.30\n0.9893\n-3.65\n0.0001\n-1.65\n0.0495\n0.35\n0.6368\n2.35\n0.9906\n-3.60\n0.0002\n-1.60\n0.0548\n0.40\n0.6554\n2.40\n0.9918\n-3.55\n0.0002\n-1.55\n0.0606\n0.45\n0.6736\n2.45\n0.9929\n-3.50\n0.0002\n-1.50\n0.0668\n0.50\n0.6915\n2.50\n0.9938\n-3.45\n0.0003\n-1.45\n0.0735\n0.55\n0.7088\n2.55\n0.9946\n-3.40\n0.0003\n-1.40\n0.0808\n0.60\n0.7257\n2.60\n0.9953\n-3.35\n0.0004\n-1.35\n0.0885\n0.65\n0.7422\n2.65\n0.9960\n-3.30\n0.0005\n-1.30\n0.0968\n0.70\n0.7580\n2.70\n0.9965\n-3.25\n0.0006\n-1.25\n0.1056\n0.75\n0.7734\n2.75\n0.9970\n-3.20\n0.0007\n-1.20\n0.1151\n0.80\n0.7881\n2.80\n0.9974\n-3.15\n0.0008\n-1.15\n0.1251\n0.85\n0.8023\n2.85\n0.9978\n-3.10\n0.0010\n-1.10\n0.1357\n0.90\n0.8159\n2.90\n0.9981\n-3.05\n0.0011\n-1.05\n0.1469\n0.95\n0.8289\n2.95\n0.9984\n-3.00\n0.0013\n-1.00\n0.1587\n1.00\n0.8413\n3.00\n0.9987\n-2.95\n0.0016\n-0.95\n0.1711\n1.05\n0.8531\n3.05\n0.9989\n-2.90\n0.0019\n-0.90\n0.1841\n1.10\n0.8643\n3.10\n0.9990\n-2.85\n0.0022\n-0.85\n0.1977\n1.15\n0.8749\n3.15\n0.9992\n-2.80\n0.0026\n-0.80\n0.2119\n1.20\n0.8849\n3.20\n0.9993\n-2.75\n0.0030\n-0.75\n0.2266\n1.25\n0.8944\n3.25\n0.9994\n-2.70\n0.0035\n-0.70\n0.2420\n1.30\n0.9032\n3.30\n0.9995\n-2.65\n0.0040\n-0.65\n0.2578\n1.35\n0.9115\n3.35\n0.9996\n-2.60\n0.0047\n-0.60\n0.2743\n1.40\n0.9192\n3.40\n0.9997\n-2.55\n0.0054\n-0.55\n0.2912\n1.45\n0.9265\n3.45\n0.9997\n-2.50\n0.0062\n-0.50\n0.3085\n1.50\n0.9332\n3.50\n0.9998\n-2.45\n0.0071\n-0.45\n0.3264\n1.55\n0.9394\n3.55\n0.9998\n-2.40\n0.0082\n-0.40\n0.3446\n1.60\n0.9452\n3.60\n0.9998\n-2.35\n0.0094\n-0.35\n0.3632\n1.65\n0.9505\n3.65\n0.9999\n-2.30\n0.0107\n-0.30\n0.3821\n1.70\n0.9554\n3.70\n0.9999\n-2.25\n0.0122\n-0.25\n0.4013\n1.75\n0.9599\n3.75\n0.9999\n-2.20\n0.0139\n-0.20\n0.4207\n1.80\n0.9641\n3.80\n0.9999\n-2.15\n0.0158\n-0.15\n0.4404\n1.85\n0.9678\n3.85\n0.9999\n-2.10\n0.0179\n-0.10\n0.4602\n1.90\n0.9713\n3.90\n1.0000\n-2.05\n0.0202\n-0.05\n0.4801\n1.95\n0.9744\n3.95\n1.0000\n\n18.05 Practice Exam 2b\nTable of Student t critical values (right-tail)\nThe table shows tdf, p = the 1 - p quantile of t(df).\nWe only give values for p ≤ 0.5. Use symmetry to find the values for p > 0.5, e.g.\nt5, 0.975 = -t5, 0.025\nIn R notation tdf, p = qt(1-p, df).\ndf\\p\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.040\n0.050\n0.100\n0.200\n0.300\n0.400\n0.500\n63.66\n31.82\n21.20\n15.89\n12.71\n10.58\n7.92\n6.31\n3.08\n1.38\n0.73\n0.32\n0.00\n9.92\n6.96\n5.64\n4.85\n4.30\n3.90\n3.32\n2.92\n1.89\n1.06\n0.62\n0.29\n0.00\n5.84\n4.54\n3.90\n3.48\n3.18\n2.95\n2.61\n2.35\n1.64\n0.98\n0.58\n0.28\n0.00\n4.60\n3.75\n3.30\n3.00\n2.78\n2.60\n2.33\n2.13\n1.53\n0.94\n0.57\n0.27\n0.00\n4.03\n3.36\n3.00\n2.76\n2.57\n2.42\n2.19\n2.02\n1.48\n0.92\n0.56\n0.27\n0.00\n3.71\n3.14\n2.83\n2.61\n2.45\n2.31\n2.10\n1.94\n1.44\n0.91\n0.55\n0.26\n0.00\n3.50\n3.00\n2.71\n2.52\n2.36\n2.24\n2.05\n1.89\n1.41\n0.90\n0.55\n0.26\n0.00\n3.36\n2.90\n2.63\n2.45\n2.31\n2.19\n2.00\n1.86\n1.40\n0.89\n0.55\n0.26\n0.00\n3.25\n2.82\n2.57\n2.40\n2.26\n2.15\n1.97\n1.83\n1.38\n0.88\n0.54\n0.26\n0.00\n3.17\n2.76\n2.53\n2.36\n2.23\n2.12\n1.95\n1.81\n1.37\n0.88\n0.54\n0.26\n0.00\n2.92\n2.58\n2.38\n2.24\n2.12\n2.02\n1.87\n1.75\n1.34\n0.86\n0.54\n0.26\n0.00\n2.90\n2.57\n2.37\n2.22\n2.11\n2.02\n1.86\n1.74\n1.33\n0.86\n0.53\n0.26\n0.00\n2.88\n2.55\n2.36\n2.21\n2.10\n2.01\n1.86\n1.73\n1.33\n0.86\n0.53\n0.26\n0.00\n2.86\n2.54\n2.35\n2.20\n2.09\n2.00\n1.85\n1.73\n1.33\n0.86\n0.53\n0.26\n0.00\n2.85\n2.53\n2.34\n2.20\n2.09\n1.99\n1.84\n1.72\n1.33\n0.86\n0.53\n0.26\n0.00\n2.83\n2.52\n2.33\n2.19\n2.08\n1.99\n1.84\n1.72\n1.32\n0.86\n0.53\n0.26\n0.00\n2.82\n2.51\n2.32\n2.18\n2.07\n1.98\n1.84\n1.72\n1.32\n0.86\n0.53\n0.26\n0.00\n2.81\n2.50\n2.31\n2.18\n2.07\n1.98\n1.83\n1.71\n1.32\n0.86\n0.53\n0.26\n0.00\n2.80\n2.49\n2.31\n2.17\n2.06\n1.97\n1.83\n1.71\n1.32\n0.86\n0.53\n0.26\n0.00\n2.79\n2.49\n2.30\n2.17\n2.06\n1.97\n1.82\n1.71\n1.32\n0.86\n0.53\n0.26\n0.00\n2.75\n2.46\n2.28\n2.15\n2.04\n1.95\n1.81\n1.70\n1.31\n0.85\n0.53\n0.26\n0.00\n2.74\n2.45\n2.27\n2.14\n2.04\n1.95\n1.81\n1.70\n1.31\n0.85\n0.53\n0.26\n0.00\n2.74\n2.45\n2.27\n2.14\n2.04\n1.95\n1.81\n1.69\n1.31\n0.85\n0.53\n0.26\n0.00\n2.73\n2.44\n2.27\n2.14\n2.03\n1.95\n1.81\n1.69\n1.31\n0.85\n0.53\n0.26\n0.00\n2.73\n2.44\n2.27\n2.14\n2.03\n1.95\n1.80\n1.69\n1.31\n0.85\n0.53\n0.26\n0.00\n2.72\n2.44\n2.26\n2.13\n2.03\n1.94\n1.80\n1.69\n1.31\n0.85\n0.53\n0.26\n0.00\n2.70\n2.42\n2.25\n2.12\n2.02\n1.94\n1.80\n1.68\n1.30\n0.85\n0.53\n0.26\n0.00\n2.70\n2.42\n2.25\n2.12\n2.02\n1.93\n1.80\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.70\n2.42\n2.25\n2.12\n2.02\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.70\n2.42\n2.24\n2.12\n2.02\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.69\n2.41\n2.24\n2.12\n2.02\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.69\n2.41\n2.24\n2.12\n2.01\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.69\n2.41\n2.24\n2.11\n2.01\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.68\n2.41\n2.24\n2.11\n2.01\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.68\n2.41\n2.24\n2.11\n2.01\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n2.68\n2.40\n2.24\n2.11\n2.01\n1.93\n1.79\n1.68\n1.30\n0.85\n0.53\n0.25\n0.00\n\n18.05 Practice Exam 2b\nTable of χ2 critical values (right-tail)\nThe table shows cdf, p = the 1 - p quantile of χ2(df).\nIn R notation cdf, p = qchisq(1-p, df).\ndf\\p\n0.010\n0.025\n0.050\n0.100\n0.200\n0.300\n0.500\n0.700\n0.800\n0.900\n0.950\n0.975\n0.990\n6.63\n5.02\n3.84\n2.71\n1.64\n1.07\n0.45\n0.15\n0.06\n0.02\n0.00\n0.00\n0.00\n9.21\n7.38\n5.99\n4.61\n3.22\n2.41\n1.39\n0.71\n0.45\n0.21\n0.10\n0.05\n0.02\n11.34\n9.35\n7.81\n6.25\n4.64\n3.66\n2.37\n1.42\n1.01\n0.58\n0.35\n0.22\n0.11\n13.28\n11.14\n9.49\n7.78\n5.99\n4.88\n3.36\n2.19\n1.65\n1.06\n0.71\n0.48\n0.30\n15.09\n12.83\n11.07\n9.24\n7.29\n6.06\n4.35\n3.00\n2.34\n1.61\n1.15\n0.83\n0.55\n16.81\n14.45\n12.59\n10.64\n8.56\n7.23\n5.35\n3.83\n3.07\n2.20\n1.64\n1.24\n0.87\n18.48\n16.01\n14.07\n12.02\n9.80\n8.38\n6.35\n4.67\n3.82\n2.83\n2.17\n1.69\n1.24\n20.09\n17.53\n15.51\n13.36\n11.03\n9.52\n7.34\n5.53\n4.59\n3.49\n2.73\n2.18\n1.65\n21.67\n19.02\n16.92\n14.68\n12.24\n10.66\n8.34\n6.39\n5.38\n4.17\n3.33\n2.70\n2.09\n23.21\n20.48\n18.31\n15.99\n13.44\n11.78\n9.34\n7.27\n6.18\n4.87\n3.94\n3.25\n2.56\n32.00\n28.85\n26.30\n23.54\n20.47\n18.42\n15.34\n12.62\n11.15\n9.31\n7.96\n6.91\n5.81\n33.41\n30.19\n27.59\n24.77\n21.61\n19.51\n16.34\n13.53\n12.00\n10.09\n8.67\n7.56\n6.41\n34.81\n31.53\n28.87\n25.99\n22.76\n20.60\n17.34\n14.44\n12.86\n10.86\n9.39\n8.23\n7.01\n36.19\n32.85\n30.14\n27.20\n23.90\n21.69\n18.34\n15.35\n13.72\n11.65\n10.12\n8.91\n7.63\n37.57\n34.17\n31.41\n28.41\n25.04\n22.77\n19.34\n16.27\n14.58\n12.44\n10.85\n9.59\n8.26\n38.93\n35.48\n32.67\n29.62\n26.17\n23.86\n20.34\n17.18\n15.44\n13.24\n11.59\n10.28\n8.90\n40.29\n36.78\n33.92\n30.81\n27.30\n24.94\n21.34\n18.10\n16.31\n14.04\n12.34\n10.98\n9.54\n41.64\n38.08\n35.17\n32.01\n28.43\n26.02\n22.34\n19.02\n17.19\n14.85\n13.09\n11.69\n10.20\n42.98\n39.36\n36.42\n33.20\n29.55\n27.10\n23.34\n19.94\n18.06\n15.66\n13.85\n12.40\n10.86\n44.31\n40.65\n37.65\n34.38\n30.68\n28.17\n24.34\n20.87\n18.94\n16.47\n14.61\n13.12\n11.52\n50.89\n46.98\n43.77\n40.26\n36.25\n33.53\n29.34\n25.51\n23.36\n20.60\n18.49\n16.79\n14.95\n52.19\n48.23\n44.99\n41.42\n37.36\n34.60\n30.34\n26.44\n24.26\n21.43\n19.28\n17.54\n15.66\n53.49\n49.48\n46.19\n42.58\n38.47\n35.66\n31.34\n27.37\n25.15\n22.27\n20.07\n18.29\n16.36\n54.78\n50.73\n47.40\n43.75\n39.57\n36.73\n32.34\n28.31\n26.04\n23.11\n20.87\n19.05\n17.07\n56.06\n51.97\n48.60\n44.90\n40.68\n37.80\n33.34\n29.24\n26.94\n23.95\n21.66\n19.81\n17.79\n57.34\n53.20\n49.80\n46.06\n41.78\n38.86\n34.34\n30.18\n27.84\n24.80\n22.47\n20.57\n18.51\n63.69\n59.34\n55.76\n51.81\n47.27\n44.16\n39.34\n34.87\n32.34\n29.05\n26.51\n24.43\n22.16\n64.95\n60.56\n56.94\n52.95\n48.36\n45.22\n40.34\n35.81\n33.25\n29.91\n27.33\n25.21\n22.91\n66.21\n61.78\n58.12\n54.09\n49.46\n46.28\n41.34\n36.75\n34.16\n30.77\n28.14\n26.00\n23.65\n67.46\n62.99\n59.30\n55.23\n50.55\n47.34\n42.34\n37.70\n35.07\n31.63\n28.96\n26.79\n24.40\n68.71\n64.20\n60.48\n56.37\n51.64\n48.40\n43.34\n38.64\n35.97\n32.49\n29.79\n27.57\n25.15\n69.96\n65.41\n61.66\n57.51\n52.73\n49.45\n44.34\n39.58\n36.88\n33.35\n30.61\n28.37\n25.90\n71.20\n66.62\n62.83\n58.64\n53.82\n50.51\n45.34\n40.53\n37.80\n34.22\n31.44\n29.16\n26.66\n72.44\n67.82\n64.00\n59.77\n54.91\n51.56\n46.34\n41.47\n38.71\n35.08\n32.27\n29.96\n27.42\n73.68\n69.02\n65.17\n60.91\n55.99\n52.62\n47.34\n42.42\n39.62\n35.95\n33.10\n30.75\n28.18\n74.92\n70.22\n66.34\n62.04\n57.08\n53.67\n48.33\n43.37\n40.53\n36.82\n33.93\n31.55\n28.94\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Final Exam Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_exam_final_sol.pdf",
      "content": "18.05 Final Exam Spring 2022 Solutions\nPart I: Concept questions (70 points)\nThese questions are all multiple choice or short answer. You don't have to show any work.\nWork through them quickly.\nConcept 1. (4 pts.) Which of the following is a valid probability table?\n(i)\noutcome\n1.5\n2.5\n3.5\nprobability\n1/5\n1/5\n1/5\n1/5\n1/5\n1/5\n(ii)\noutcome\nred\nblue\ngreen\ncyan\nyellow\nprobability\n4/10\n2/10\n1/10\n3/10\nCircle the best choice:\n(i)\nSolution: (ii)\n(ii)\n(i) and (ii)\nneither (i) nor (ii)\nConcept 2. (4 pts.) Suppose P (A) + P (B) > 1. Consider the following statements.\n(i) P (A ∪ B) = 1.\n(ii) P (A ∩ B) > 0.\nWhich must be true? Circle the best choice below:\n(i)\n(ii)\n(i) and (ii)\nneither (i) nor (ii).\nSolution: (ii)\nConcept 3. (6 pts.) Circle True or False for each of the following.\n(a) If A and B are independent then we must have P (A ∩ B) = P (A)P (B).\n(b) If A and B are independent then we must have P (A∩B) = P (A)+P (B).\n(c) If A and B are disjoint then A and B must be independent.\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\nSolution: True False False\nConcept 4. (4 pts.) You believe the MBTA subway arrives late by X hours, where X\nfollows an exponential distribution with unknown parameter λ. To test your hypothesis, you\nrecord the lateness of 5 subway trains and get data x1, x2, ... , x5. Which of the following are\nstatistics? Circle the correct answers.\n(a) The expected value of a sample, namely 1/λ.\n(b) The sample average, x = (x1 + x2 + x3 + x4 + x5)/5.\n(c) The difference between x and 1/λ.\n(d) The sample standard deviation.\nSolution: (b) and (d)\nConcept 5. (3 pts.) For each of the following, circle it if it is used in Bayesian inference.\n(a) Likelihood function\n(b) prior odds\n(c) p-value\n\n18.05 Final Exam Spring 2022 Solutions\nSolution: Circle (a) and (b)\nConcept 6. (4 pts.) Suppose X ∼ Bernoulli(θ), where θ is unknown. Complete the follow\ning sentence using the words, \"discrete,\" \"continuous,\" or \"neither discrete nor continuous.\"\nThe random variable is discrete , the space of hypotheses is continuous .\nSolution: discrete,\ncontinuous\nConcept 7. (2 pts.) A casino is considering installing a new slot machine. A player who\nwins is paid $2 on a $1 bet. The manufacturer claims that the probability of winning on any\nplay of the slot machine is p = 0.48. Before using the machine the casino wants to make\nsure it will make them money. So they hire you to test the slot machine. Which of the\nfollowing hypotheses would you use?\n(i) H0 ∶p= 0.48 vs HA ∶ p = 0.48\n(ii) H0 ∶p= 0.48 vs HA ∶ p > 0.48\n(iii) H0 ∶p= 0.48 vs HA ∶ p < 0.48\nCircle the best answer:\n(i)\n(ii)\n(iii)\nNot enough information.\nSolution: (ii)\nConcept 8. (6 pts.) The following are hypotheses considered in the previous problem. For\neach hypothesis circle all that apply.\n(a) H0 ∶p= 0.48\nSimple\nComposite\nTwo-sided\nOne-sided\n(b) HA ∶ p = 0.48\nSimple\nComposite\nTwo-sided\nOne-sided\n(c) HA ∶ p > 0.48\nSimple\nComposite\nTwo-sided\nOne-sided\n(a) Solution: Simple\n(b) Solution: Composite Two-sided\n(c) Solution: Composite One-sided\nConcept 9. (5 pts.) Which of the following are true about p-values? Circle all that apply.\n(a) The p-value gives the probability of making a type 1 error.\n(b) The p-value is a measure of how extreme the observed data is.\n(c) A p-value below the significance level allows us to conclude with certainty that the null\nhypothesis is false.\n(d) The p-value is a frequentist concept.\n(e) If the null hypothesis is true, then the p-value will always be larger than the significance\nlevel.\nSolution: (b), (d)\nConcept 10. (8 pts.) A two-sample t-test for equal means of two populations has a p-value\nof 0.08.\n\n18.05 Final Exam Spring 2022 Solutions\nCircle True or False for each of the following.\n(a) For a significance level of 0.05, the null hypothesis of equal means should be rejected.\nTrue\nFalse\n(b) A 90% confidence interval for the difference of the means for the two populations includes\n0.\nTrue\nFalse\n(c) A 95% confidence interval for the difference of the means for the two populations includes\n0.\nTrue\nFalse\n(d) With probability 95% the actual value of the difference of the means is within the 95%\nt-confidence interval for the difference.\nTrue\nFalse\nSolution: False False True, False\nConcept 11. (2 pts.) Data is drawn from a N(1, σ2) distribution. Let X5 be the average of\n5 data points and X10 the average of 10 data points. Three densities, f1, f2, f3 are shown\nbelow. One is the pdf of X5, one is the pdf of X10 and one is another pdf. Circle the one\nthat is the pdf of X10.\nx\nf1(x)\nf2(x)\nf3(x)\nSolution: f1\nConcept 12. (4 pts.) Finish the following sentences with \"a type I error\", \"a type II\nerror\", or \"neither type of error\".\n(a) The rejection of a false null hypothesis is neither type of error\n(b) The rejection of a true null hypothesis is a type I error\nSolution: neither,\ntype I\nConcept 13. (8 pts.) Suppose that the data x1, x2, ..., xn are drawn from independent,\nidentically distributed, random variables Xi with mean μ and standard deviation σ. Write\nx = (x1 + ⋯ + xn)/n for the sample mean. The Central Limit Theorem states that: (circle\nall that apply)\n(a) The distribution of each random variable Xi is approximately symmetric around the\naverage μ.\n(b) For large n, the distribution of the sample mean is approximately symmetric\naround the average μ.\n(c) For large n, the average x approximately follows a normal distribution.\n\n18.05 Final Exam Spring 2022 Solutions\n(d) For large n, (x - μ)/σ approximately follows a standard normal distribution.\nSolution: (b), (c). (For (d), this follows N(0, 1/n), a normal with mean 0 and variance\n1/n.)\nConcept 14. (2 pts.) Suppose for a certain endeavor θ is the probability of success. Suppose\nalso that our prior for θ is Beta(5,5). We collect data from 30 trials, obtaining 20 successes\nand 10 failures. What is our posterior pdf f(θ|x)? (Circle the best answer.)\n(i) Binomial(30, 2/3)\n(ii) Beta(25, 15)\n(iii) Beta(20, 10)\n(iv) None of the above\nSolution: (ii)\nConcept 15. (2 pts.) Circle True or False.\nLet s be a statistic. If the theoretical distribution of the statistic is hard to compute, then it\nis not advisable to use bootstrapping to compute confidence intervals for the statistic.\nTrue\nFalse\nSolution: False, to the contrary\nConcept 16. (2 pts.) Suppose we run a two-sample t-test for equal means with significance\nlevel α = 0.05. If the data implies we should reject the null hypothesis, then the odds that\nthe two samples come from distributions with the same mean are (circle the best answer)\n(a) 19/1\n(b) 1/19\n(c) 20/1\n(d) 1/20\n(e) unknown\nSolution: e. Frequentist methods only give probabilities for data under an assumed hy\npothesis. They do not give probabilities or odds for hypotheses. So we don't know the odds\nfor distribution means.\nConcept 17. (2 pts.) For the bivariate data in the following scatter plot, is the correlation\nbetween x and y positive or negative? Circle your choice:\npositive\nnegative\nx\ny\nSolution: positive\nConcept 18. (2 pts.) Circle True or False\nLinear regression can fit curves other than lines to given data.\nTrue\nFalse\n\n18.05 Final Exam Spring 2022 Solutions\nSolution: True, e.g. polynomials\nPart II: Problems (236 points)\nProblem 1. (15: 5,5,5)\nYou roll a fair six-sided die 8 times.\n(a) What is the probability that none of the 8 rolls is a six?\nSolution: (5/6)8 ≈ 1/4. (Since 54 = 625 and 64 = 1296, we see that (5/6)4 is approxi\nmately 1/2. Squaring...)\n(b) What is the probability that exactly one of the 8 rolls is a six?\nSolution: This is a binomial probability\n8 ⋅ 57\n(8\n1) ⋅ (1\n6) ⋅ (5\n6) =\n≈ 2/5\n(c) What is the expected number of sixes in the 8 rolls?\nSolution: Expected values add, so Solution: 8 ⋅ 1/6 = 4/3.\nProblem 2. (16: 4,4,4,4)\nSuppose X is a random variable with values in [1,2] and density\nf(x) = {kx2\nfor 1 ≤ x ≤ 2\notherwise\nwhere k is a fixed constant.\n(a) What is k?\nSolution: ∫1\n2 x2 dx = x3/3]2\n1 = 8/3 - 1/3 = 7/3. So k = 3/7 .\n(b) Find the cdf of X.\nSolution: cdf(x) = ∫1\nx kt2 dt = (3/7)(x3/3 - 1/3) = (x3 - 1)/7 . This calculation is valid\nfor 1 ≤ x ≤ 2; the cdf is 0 for x < 1 and 1 for x > 2 .\n(c) Find P (X < 3/2).\nSolution: This is cdf(3/2) = (27/8 - 1)/7 = 19/56 .\n(d) Find E[X]\nSolution: E[X] = ∫ x⋅(3x2/7) dx = 3x4/28]1\n2 = (3 ⋅16 -3 ⋅1)/28 = 45/28 .\nProblem 3. (25: 5,5,5,5,5)\nSuppose random variables X and Y have units of dollars and:\nE[X] = 5,\nVar(X) = 62,\nand\nE[Y ] = 10,\nVar(Y) = 72.\nDefine W = X+ Y .\n\n18.05 Final Exam Spring 2022 Solutions\n(a) Find E[W ]. What are the units of E[W ]?\nSolution: The expected value of a sum of random variables is the sum of the expected\nvalues, so\nE[W] = E[X] + E[Y] = 5 + 10 = 15\nThe units are those of X and Y , namely dollars : we compute the expected value by adding\nup values of the variable (which are dollars) multiplied by (dimensionless) probabilities.\n(b) Assume X and Y are independent. Find Var(W ).\nSolution: The magic formula is Var(X + Y) = Var(X) + Var(Y ) + 2Cov(X, Y ). Indepen\ndence implies that the covariance is zero, so\nVar(W) = Var(X) + Var(Y) = 62 + 72 = 36 + 49 = 85 .\n(c) Assume Cov(X, Y) = 21. Find Var(W). (Note: this assumption differs from that in\npart (b), so Var(W ) can also be different.)\nSolution: Same formula as used in (b) gives\nVar(W) = Var(X) + Var(Y ) + 2Cov(X, Y) = 62 + 72 + 2 ⋅21 = 36 + 49 + 42 = 127 .\nThe units are square dollars, which seems like a very American concept.\n(d) Assume Cov(X, Y) = 21. Compute Cor(X, Y ). What are the units of Cor(X, Y )?\nSolution: The correlation is by definition Cor(X, Y) = Cov(X, Y )/√Var(X)Var(Y ). The\nnumerator has units ($)2. What's under the square root has units ($)4, so the denominator\nhas units ($)2, and the correlation is dimensionless . The value is\nCor(X, Y) = Cov(X, Y )/√Var(X)Var(Y) = 21/\n√\n62 ⋅72 = 21/(6 ⋅7) = 1/2 .\n(X - E[X])\n(e) Define Z=\n= (X-5)/6. Find E[Z] and Var(Z).\n√Var(X)\nSolution: This is the standardization of X, which is supposed to have mean 0 and variance 1 .\nHere's why. The linearity of expectation E[aX + b] = aE[X] + b (for constants a and b)\ngives\nE[Z] = E[X]/6 - 5/6 = 5/6 -5/6 = 0 .\nThe corresponding fact for variance is Var(aX + b) = a2Var(X), so\nVar(Z) = Var(X)/62 = 62/62 = 1 .\nProblem 4. (20: 10,5,5)\nYou roll two fair (6-sided) dice. If the sum of the dice is greater than 9, you win $100. If\nthe sum is 9 or less you get to roll again. On the second roll, if your sum of dice is greater\nthan 9, you win $50, otherwise you win nothing. Let X be the random variable of how much\nmoney you win by playing this game.\n(a) Construct a probability model for X, i.e. make a probability table.\n\n18.05 Final Exam Spring 2022 Solutions\nSolution:\nsum of dice\nprobability\n1/36\n2/36\n3/36\n4/36\n5/36\n6/36\n5/36\n4/36\n3/36\n2/36\n1/36\nSo the probability of rolling less than or equal to 9 is 30/36 = 5/6, and the probability\nof 10 or more is 6/36 = 1/6. The probabilities for various outcomes of the game can be\ncomputed from the following tree.\n1/6\nwin $100\n5/6\nreplay\n1/6\nwin $50\n5/6\nThe table of probabilities and outcomes is\noutcome\n$100\n$50\nprob.\n1/6\n5/36\n25/36\n(b) What is the expected amount you will win playing the game?\nSolution: This can be read from the last table in (a) : it's\nExpected payoff = (25/36) ⋅ $0 + (5/36) ⋅ $50 + (1/6) ⋅ $100 = $850/36 = $2311\n18 ≈ $23.61\n(c) What would you be willing to pay to play this game? (Justify your answer).\nSolution: If you pay less than $23.61 , you'll make money in the long run. But you\nneed a big stake to be able to be sure you can keep playing long enough if to win: since you\nget nothing two-thirds of the time, you have a one in ten chance of winning nothing in ten\ngames, for instance; so you clearly need to have a stake (much) more than ten times the\nprice to be even 90% sure of winning.\nSo I don't think I'd pay much more than $10 a game .\nProblem 5. (10)\nThe Pareto distribution is used in economics modeling. To keep it simple we'll use the\nPareto distribution that takes values x ≥ 1 and has pdf\nf(x ∣ θ) = θx-θ-1\nfor x ≥ 1.\nIt's defined whenever θ > 0. Assume x1, ... , xn are n independent samples from a Pareto(θ)\ndistribution, find the maximum likelihood estimate of θ.\nSolution: The likelihood for these n independent samples is the product of the densities\nat all the xi:\nθx-θ-1\nθx-θ-1 = θn(x1 ⋯xn)-θ-1\n[\n][θx-θ-1\n] ⋯ [\nn\n]\nThe log likelihood is\nn log(θ) + log(x1 ⋯ xn)(-θ - 1).\n\n18.05 Final Exam Spring 2022 Solutions\nThe MLE is the choice of θ > 0 that maximizes this function. The function approaches -inf\nas θ approaches 0 or inf (because all the xi need to be at least 1 to have a chance of being\nmodelled by Pareto; this makes log(x1 ⋯xn) > 0). So any maximum must occur when the\nderivative with respect to θ is zero. That is,\nn/θ - log(x1 ⋯ xn) = 0,\nwhich gives\nn\nn\nθ = log(x1 ⋯ xn) = ∑i log(xi) .\nProblem 6. (15: 5,10)\nA certain medical condition exists in 1% of the population. A screening test for the condition\nhas a 4% false positive rate and a 0% false negative rate.\n(a) What are the odds that a random person has the condition?\nSolution: Let's let D+ = having the condition; D- = not having it; T + = testing positive.\nP (D+)\n1/100\nThe odds of having the condition are P (D-) = 99/100 = 99.\n(b) Suppose a random person tests positive for the condition. What are the odds they have\nthe condition?\nSolution: The odds after the evidence are the prior odds multiplied by the likelihood ratio.\nThe likelihoods are\nP (T + |D+) = 1.00,\nP (T + |D-) = 0.04.\nP (T + |D+)\nSo, the likelihood ratio is\n0.04 = 25. Thus, the posterior odds are\nP (T + |D-) =\nprior odds ⋅ likelihood ratio =\n⋅25 =\n≈ 1/4.\nAfter a positive test, the odds are about 1 to 4 that the person has the condition (or\nabout a 20% chance).\nProblem 7. (30: 5,10,5,10 )\nThis question is about a robot, Bayz-E, who tries to figure out where it is located using\nBayesian updating. Bayz-E is randomly placed in front of one of four doors, A, B, C, or D.\nAt every time step, Bayz-E scans the color of the door in front of it. The outcome will be\neither orange of blue. However, its color scanner is not entirely accurate. Bayz-E scans the\ndoor's color correctly with a probability 0.7, and scans the color incorrectly with a probability\n0.3.\nNote. To avoid confusion for you, the color of the door is written above its letter. (Bayz-E\nhas not yet learned to read, though its machine learning algorithm is working on it.)\n\n18.05 Final Exam Spring 2022 Solutions\nA\norange\nB\norange\nC\nblue\nD\nblue\n(a) Bayz-E is placed in front of door A, B, C, or D at random (i.e. the probability of each\ndoor is the same). What is the prior probability it is in front of door B?\nSolution: There are four hypotheses, labeled A, B, C, D according to the door in front of\nwhich Bayz-E starts. Each of the four is equally likely, so the prior for B (or for any other\ndoor) is 1/4 .\n(b) Bayz-E scans the door color and detects \"orange.\" What is the posterior probability it\nis in front of door B?\nSolution: Time to start making tables: We'll abbreviate Bayes numerator as BN.\nhypothesis\nprior\nlikelihood\nP(orange | hypoth.)\nBN\nposterior\nA (orange)\nB (orange)\nC (blue)\nD (blue)\n1/4\n1/4\n1/4\n1/4\n7/10\n7/10\n3/10\n3/10\n7/40\n7/40\n3/40\n3/40\n7/20\n7/20\n3/20\n3/20\nsum\n1/2\nSo the posterior probability of B is P(B | orange) = 7/20 .\n(c) After computing the posterior probabilities, Bayz-E moves to the next door to the right.\n(if it was at door D, it moves to door A). What is the (new) prior probability it is now in\nfront of door C? (That is, after detecting orange in part (b) and after moving one to door\nto the right.)\nSolution: To keep things straight, we start a new table for the door Bayz-E is currently in\nfront of, i.e. after moving. The priors are found by rotating the posteriors from the table\nin part (b), e.g. the part (b) posterior probability for A becomes the prior probability for\nB etc.\nhypothesis\nprior\nlikelihood\nBN 2\nposterior 2\nA (orange)\nB (orange)\nC (blue)\nD (blue)\n3/20 -part (b) posterior for D\n7/20 -part (b) posterior for A\n7/20 -part (b) posterior for B\n3/20 -part (b) posterior for C\nsum\nP (now in front of C | orange on first scan) = 7/20 .\n(d) What is the predictive probability that the color scan of this door (after the move in part\n(c)) will detect \"blue\"?\nSolution: We continue the table started in part (c).\n\n18.05 Final Exam Spring 2022 Solutions\nhypothesis\nprior\nlikelihood\nP(2nd is blue | hypoth.)\nBN 2\nposterior 2\nA (orange)\nB (orange)\nC (blue)\nD (blue)\n3/20\n7/20\n7/20\n3/20\n3/10\n3/10\n7/10\n7/10\n9/200\n21/200\n49/200\n21/200\nsum\n100/200\nThe predictive probability is the total probability of blue, i.e. the sum of the Bayes numer\nator column:\nP (blue on second door | orange on first door) = 1/2 .\nProblem 8. (20)\nAccording to the Mars website, each packet of Milk Chocolate M&M's should contain 20%\nblue, 20% brown, 20% green, 15% orange, 15% red, and 10% yellow M&M's.\nAlessandre decides to test this claim. She buys 20 packets of Milk Chocolate M&M's. Each\npacket has 50 M&M's, so Alessandre has a total of 1000 M&M's. She counts each color\nand observes the following counts of M&M's.\nblue\nbrown\ngreen\norange\nred\nyellow\ntotal\nObserved count\nRun a hypothesis test at the 0.05 significance level to test whether the published Mars color\ndistribution is correct. Carefully state what you are testing and your conclusion.\n(Write down the full numerical expression for your test statistic. In order to use the tables\nyou will need to estimate the value of the test statistic. There is no need to compute it in\nfull precision.)\nSolution: We want to do a chi-squared test for goodness of fit : we are testing\nwhether the distribution published on the Mars website is a good fit with the observed\ncounts. H0 is that the data was drawn from the Mars' distribution and HA is that it is\ndrawn from another distribution.\nHere's a table of the data and a bit of the computation.\nblue\nbrown\ngreen\norange\nred\nyellow\nobs. count\npubl. freq.\nexp. count\n(O - E)2/E\n0.20\n0.20\n0.20\n0.15\n0.15\n= 2\n= 0.5\n= 1.125\n102/150 = 0.67\n= 1.5\n0.10\n1-0\n2002 = 4\nThe χ-squared statistic is the sum of the entries in the last row, namely\nX2 = 2 + 0.5 + 1.125 + 0.67 + 1.5 + 4 ≈ 9.8 .\nThe number of degrees of freedom is the number of colors minus 1 (because we used a known\ntotal number of M&M's), or df = 5 . Using the table at the end of the test, we see that\n9.8 corresponds to a p-value between 0.05 and 0.10. (Interpolating it is about 0.085, using\nR we get 0.081.) Since this is greater than 0.05, we do not reject the null hypothesis that\nthe published distribution is correct. We will not blog that their website gives misleading\ninformation.\n\n18.05 Final Exam Spring 2022 Solutions\nProblem 9. (20: 5,10,5)\nJerry wants to brag to his non-MIT colleagues about how smart MIT students are. To\ngive himself credibility, he decides to run a statistical test comparing the IQ scores of MIT\nstudents and Harvard students.\nHe collects IQ scores from 11 MIT students. The data has a sample mean of 115, with a\nsample standard deviation of 8.\nHe then collects IQ scores from 11 Harvard students. Their scores have a sample mean of\n110, with a sample standard deviation of 6.\n(a) Which test should he run to compare the IQ scores from the two schools? What\nassumptions will he need to make? What are the null and alternative hypotheses?\nSolution: We can run a two-sample t test for comparing means. This test assumes that the\ntwo sets of data are drawn from normal distributions (a reasonable assumption for\nthe results of IQ tests, which are designed that way) with equal variances (a reasonable\nassumption since the sample standard deviations are not far apart). The null hypothesis\nis means are equal . The alternative hypothesis is means are unequal . (Can also\nargue that HA should be one-sided.)\n(b) Run the test with a significance level of α = 0.05. Should Jerry reject the null hypothesis\nor not?\n(In this problem there is some arithmetic. You will want to use √100/11 ≈\n√\n9 = 3.)\nSolution: The t statistic is t = (x - y)/sp, where s2\np is the pooled variance\n10 ⋅64 + 10 ⋅36 ( 1\ns2\np=\n11 + 11 -2\n11 + 11\n1 ) = 11 .\nAccording to the hint sp ≈3. The difference in means is 5, so t ≈ 5/3 = 1.67 . The number\nof degrees of freedom is 11+11-2 = 20. According to the table, the right tail probability for\nthe t distribution with 20 degrees of freedom at 1.67 is between 0.05 and 0.10, (interpolating\nit's about 0.058). The two-tailed p-value is twice the right tail probability, (about 0.116 ),\nwhich is clearly greater than α = 0.05. So, we do not reject the null hypothesis, i.e.,\nthe difference between the mean IQ scores of MIT and Harvard students is not signifcant\nat significance level 0.05.\n(c) Estimate the 95% confidence interval for the IQ of MIT students.\n(Your answer should be a numerical expression. There is no need to work it out all the way\nto a decimal answer.)\nSolution: The 95% confidence intervals is\nx ± t0.025 ⋅ √s\nn = 115 ± 2.23 ⋅ √8\n.\nHere t0.025 is the right t-critical value with 10 degrees of freedom.\nProblem 10. (20: 10,10)\nMIT has decided to form a new Department of Statistics and Probability. In a vote for\nthe new head of this department, suppose 50% of the MIT population supports Sarah, 20%\n\n18.05 Final Exam Spring 2022 Solutions\nsupports So Hee, and the remaining 30% is split evenly among Jerry, Jen, Alessandre and\nGabe.\n(a) A poll asks 100 random people whom they support. Estimate the probability that at least\n45% of those polled support Sarah.\nSolution: Write S for the fraction of the sample that supports Sarah.\n2√1\nn = 20\nE[S] = 0.5,\nσS =\nSince S is an average of 100 Bernoulli(0.5) variables, the CLT says it's approximately\nnormal. Standardizing gives\n> 0.45 - 0.50\nP(S> 0.45) = P(S-0.5\n) ≈P(Z> -1) ≈ 0.84.\n1/20\n1/20\n(b) A poll of n people reports that 53% ± 5% support Sarah at the 95% confidence level.\nWhat is the value of n?\nSolution: The rule of thumb for 95% polling confidence intercals tells us that\n0.05 = √1\nn ⇒ √n= 20 ⇒ n = 400.\nIf we use the more precise critical z of 1.96 (for 95%) instead of the 2 in the rule of thumb,\nwe get n = 385 .\nProblem 11. (10)\nYou independently draw 100 data points from a N(μ, 1) distribution, where μ is unknown.\nSuppose you test the null hypothesis H0 ∶μ= 0 against the alternative hypothesis HA ∶μ=0\nusing a significance level of α = 0.05. What is the power of the test for the alternative\nHA ∶μ= 0.4?\nSolution: Power = P(reject | μ= 0.4) = 1 -P(don't reject|μ = 0.4).\nRejection region for x is\nz0.975 ⋅ √σ\nn ≤ x - μ0 ≤ z0.025 ⋅ √σ\nn.\nWe have μ0 = 0, σ = 1, n = 100, α = 0.05. So, the rejection region is\n-1.96 ⋅ 10\n1 ≤ x ≤ 1.96 ⋅ 10\n⇔ -0.196 ≤ x ≤ 0.196.\nSo, P (don't reject|μ = 0.4) = P (-0.196 ≤ x ≤ 0.196 | μ = 0.4).\nStandardizing x this becomes\nμ= 0.4) = P(-0.196 -0.4 ≤ x-0.4 ≤ 0.196 -0.4\nP (don't reject |\n| μ = 0.4)\n1/10\n1/10\n1/10\n= P (-5.96 ≤ Z ≤ -2.04).\n\n18.05 Final Exam Spring 2022 Solutions\nIn the last expression Z is a standard normal random variable. Using the standard normal\ntable, we get\nP (don't reject) = Φ(-2.04) - Φ(-5.96) ≈ 0.0207 - 0.\nSo Power = 1 - 0.0207 ≈ 0.98.\nProblem 12. (20: 5,10,5)\nBivariate data (4, 1), (-2, 1.5), (0, 0.5) is assumed to arise from the model yi = b|xi-2|+ei,\nwhere b is a constant and ei are independent random variables.\n(a) What assumptions are needed on ei so that it makes sense to do a least squares fit of a\ncurve y = b|x-2| to the data?\nSolution: We need to know that the error terms ei have mean 0. It is good for them to\nbe independent and to have the same variance (homoscedastic). It is wonderful if they are\nindependent identically distributed normal of mean zero.\n(b) Given the above data and the assumptions from part (a), determine the least squares\nestimate for b.\nSolution: The sum of the squares of the errors is\nT = ∑(yi - b|xi - 2|)2.\ni=1\nSo, setting the derivative to 0 give\ndT = ∑ -2|xi - 2|(yi - b|xi - 2|) = 0.\ndb\ni=1\nSolving we get\n∑\ni=1 yi|xi - 2|\nb∑|xi-2|2 = ∑yi|xi-2| ⇔ b=\ni=1\ni=1\n∑i=1\n|xi - 2|2 .\nPlugging in numbers we find b = 3/8.\n(c) Make a graph showing the data points and your least squares fit curve.\nSolution:\n\n18.05 Final Exam Spring 2022 Solutions\n-4\n-2\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nx\ny\nProblem 13. (15)\nData is collected on the time between trades at a stock exchange. We collect a data set of\nsize 36 with sample mean x = 7.0 and sample standard deviation s = 0.84.\nMake no assumptions about the distribution of the data. By bootstrapping, we generate 500\nbootstrap means x∗ . The smallest 50 and largest 50 are written in non-decreasing order\nbelow, e.g. the 12th smallest value is 6.672.\nUse this data to find an 90% percentile bootstrap confidence interval for μ.\nSolution: The 90% percentile bootstrap CI is [q∗\n], where q∗\n0.95 are empir\n0.05, q∗\n0.95\n0.05 and q∗\nical quantiles for x∗ .\nq∗\n= 25th element = 6.740. q∗\n0.05\n0.95\nSo the 90% CI = [6.740, 7.224] .\n1- 10\n6.466\n6.506\n6.509\n11- 20\n6.670\n6.672\n6.685\n21- 30\n6.729\n6.731\n6.738\n31- 40\n6.759\n6.760\n6.768\n41- 50\n6.787\n6.789\n6.789\n451- 460\n7.170\n7.172\n7.172\n7.175\n7.178\n7.179\n7.180\n7.181\n7.182\n7.182\n461- 470\n7.182\n7.186\n7.195\n7.202\n7.202\n7.205\n7.206\n7.210\n7.216\n7.219\n471- 480\n7.220\n7.220\n7.221\n7.222\n7.224\n7.225\n7.232\n7.232\n7.236\n7.236\n481- 490\n7.243\n7.244\n7.245\n7.251\n7.253\n7.258\n7.261\n7.263\n7.266\n7.273\n491- 500\n7.274\n7.288\n7.288\n7.291\n7.307\n7.312\n7.314\n7.316\n7.348\n7.488\n= 475th element = 7.224\n6.515\n6.578\n6.597\n6.618\n6.635\n6.653\n6.664\n6.696\n6.703\n6.707\n6.713\n6.721\n6.727\n6.727\n6.738\n6.740\n6.743\n6.744\n6.745\n6.751\n6.752\n6.774\n6.775\n6.777\n6.778\n6.780\n6.784\n6.784\n6.790\n6.791\n6.791\n6.792\n6.796\n6.798\n6.800\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Practice Final Exam Probability Unit Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_prac_exam_final_probability_sol.pdf",
      "content": "Review for final exam solutions: probability unit\nMIT 18.05 Spring 2022\nProblem 1. Consider the nucleotides A, G, C, T .\n(a) How many ways are there to make a sequence of 5 nucleotides.\nSolution: Four ways to fill each slot:\n.\n(b) How many sequences of length 5 are there where no adjacent nucleotides are the same\nSolution: Four ways to fill the first slot and 3 ways to fill each subsequsent slot: 4 ⋅ 34 .\n(c) How many sequences of length 5 have exactly one A?\nSolution: Build the sequences as follows:\nStep 1: Choose which of the 5 slots gets the A: 5 ways to place the one A.\nStep 2: 34 ways to fill the remain 4 slots.\nBy the rule of product there are 5 ⋅ 34 such sequences.\nProblem 2. (a) How many 5 card poker hands are there?\n(b) How many ways are there to get a full house (3 of one rank and 2 of another)?\n(c) What's the probability of getting a full house?\n(a) Solution: (52\n5 ).\n(b) Solution: Number of ways to get a full-house: (4\n2)(13\n1 )(4\n3)(12\n1 )\n(4\n2)(13\n1 )(3\n4)(12\n1 )\n(c) Solution:\n(52\n5 )\nProblem 3. (Counting)\n(a) How many arrangements of the letters in the word probability are there?\n(b) Suppose all of these arrangements are written in a list and one is chosen at random.\nWhat is the probability it begins with 'b'.\n(a) Solution: There are several ways to think about this. Here is one.\nThe 11 letters are p, r, o, b,b, a, i,i, l, t, y. We use the following steps to create a sequence\nof these letters.\nStep 1: Choose a position for the letter p: 11 ways to do this.\nStep 2: Choose a position for the letter r: 10 ways to do this.\nStep 3: Choose a position for the letter o: 9 ways to do this.\nStep 4: Choose two positions for the two b's: 8 choose 2 ways to do this.\nStep 5: Choose a position for the letter a: 6 ways to do this.\nStep 6: Choose two positions for the two i's: 5 choose 2 ways to do this.\nStep 7: Choose a position for the letter l: 3 ways to do this.\n\nStep 8: Choose a position for the letter t: 2 ways to do this.\nStep 9: Choose a position for the letter y: 1 ways to do this.\nMultiply these all together we get:\n11 ⋅10 ⋅9 ⋅(8 ⋅6 ⋅(5 ⋅3 ⋅2 ⋅1 =\n11!\n2)\n2)\n2! ⋅2!\n(b) Solution: Here are two ways to do this problem.\nMethod 1. Since every arrangement has equal probability of being chosen we simply have\nto count the number that start with the letter 'b'. After putting a 'b' in position 1 there\nare 10 letters: p, r, o, b, a, i,i, l, t, y, to place in the last 10 positions. We count this in the\nsame manner as part (a). That is\nChoose the position for p: 10 ways.\nChoose the positions for r,o,b,a,: 9 ⋅8 ⋅7 ⋅6 ways.\nChoose two positions for the two i's: 5 choose 2 ways.\nChoose the position for l: 3 ways.\nChoose the position for t: 2 ways.\nChoose the position for y: 1 ways.\n10!\nMultiplying this together we get 10⋅9⋅8⋅7⋅6⋅(5\n2)⋅3⋅2⋅1 = 2! arrangements start with the\n10!/2!\nletter b. Therefore the probability a random arrangement starts with b is 11!/2! ⋅ 2! = 11\nMethod 2. Suppose we build the arrangement by picking a letter for the first position,\nthen the second position etc. Since there are 11 letters, two of which are b's we have a 2/11\nchance of picking a b for the first letter.\nProblem 4. Let E and F be two events. Suppose the probability that at least one of them\noccurs is 2/3. What is the probability that neither E nor F occurs?\nSolution: We are given P(E ∪ F) = 2/3 and asked to find P ((E ∪ F )c).\nP((E ∪ F)c) = 1 -P(E ∪ F) = 1/3.\nProblem 5. Let C and D be two events with P(C) = 0.3, P (D) = 0.4, and P(Cc∩D) = 0.2.\nWhat is P(C ∩D)?\nSolution: D is the disjoint union of D ∩ C and D ∩ Cc.\nSo, P(D ∩ C) + P(D ∩ Cc) = P(D). Thus, P(D ∩ C) = P(D) -P(D ∩ Cc) = 0.4 -0.2 =\n0.2.\nProblem 6. Suppose we have 8 teams labeled T1, ..., T8. Suppose they are ordered by\nplacing their names in a hat and drawing the names out one at a time.\n(a) How many ways can it happen that all the odd numbered teams are in the odd numbered\nslots and all the even numbered teams are in the even numbered slots?\nSolution: Slots 1, 3, 5, 7 are filled by T1, T3, T5, T7 in any order: 4! ways.\nSlots 2, 4, 6, 8 are filled by T2, T4, T6, T8 in any order: 4! ways.\n\nSolution: 4! ⋅4! = 576.\n(b) What is the probability of this happening?\nSolution: There are 8! ways to fill the 8 slots in any way.\n4! ⋅4!\nSince each outcome is equally likely the probabilitiy is\n= 40320 = 0.143 = 1.43%.\n8!\nProblem 7. More cards! Suppose you want to divide a 52 card deck into four hands with\n13 cards each. What is the probability that each hand has a king?\nSolution: Let Hi be the event that the ith hand has one king. We have the conditional\nprobabilities\n(4\n1)(48\n(3\n1)(36\n(2\n1)(24\n12)\n12)\n12)\nP (H1) =\n;\nP (H2|H1) =\n;\nP (H3|H1 ∩ H2) =\n(52\n(39\n(26\n13)\n13)\n13)\nP (H4|H1 ∩ H2 ∩ H3) = 1\nP (H1 ∩ H2 ∩ H3 ∩ H4) = P (H4|H1 ∩ H2 ∩ H3) P (H3|H1 ∩ H2) P (H2|H1) P (H1)\n(2\n1)(24\n1)(36\n1)(48\n12)(3\n12)(4\n12)\n=\n.\n(26\n13)(39\n13)(52\n13)\nProblem 8. Suppose we roll a fair die twice. Let A be the event 'the sum of the rolls is 5'\nand let B be the event 'at least one of the rolls is 4.'\n(a) Calculate P (A|B).\n(b) Are A and B independent?\n(a) Solution: Sample space = S = {(1, 1), (1, 2), (1, 3), ... , (6, 6) } = {(i, j) | i, j = 1, 2, 3, 4, 5, 6 }.\n(Each outcome is equally likely, with probability 1/36.)\nA = {(1, 4), (2, 3), (3, 2), (4, 1)},\nB = {(4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6), (1, 4), (2, 4), (3, 4), (5, 4), (6, 4) }\nP(A ∩ B)\n2/36\nP (A|B) =\n=\nP(B)\n11/36 = 11.\n(b) Solution: P (A) = 4/36 = P (A|B), so they are not independent.\nProblem 9. On a quiz show the contestant is given a multiple choice question with 4\noptions. Suppose there is a 70% chance the contestant actually knows the answer. If they\ndon't know the answer they guess with a 25% chance of getting it right. Suppose they get it\nright. What is the probability that they were guessing?\nSolution: Let C be the event the contestant gets the question correct and G the event the\ncontestant guessed.\n\nThe question asks for P (G|C).\nP (C|G) P (G)\nWe'll compute this using Bayes' rule: P(G|C) =\n.\nP (C)\nWe're given:\nP(C|G) = 0.25, P(K) = 0.7.\nLaw of total prob.: P(C) = P(C|G) P(G) + P(C|Gc) P(Gc) = 0.25 ⋅0.3 + 1.0 ⋅0.7 = 0.775\n0.075\nTherefore P(G|C) = 0.775 = 0.097 = 9.7%.\nProblem 10. Suppose you have an urn containing 7 red and 3 blue balls. You draw three\nballs at random. On each draw, if the ball is red you set it aside and if the ball is blue you\nput it back in the urn. What is the probability that the third draw is blue?\n(If you get a blue ball it counts as a draw even though you put it back in the urn.)\nSolution: Here is the game tree, R1 means red on the first draw etc.\nR1\nB1\nR2\nB2\nR2\nB2\nR3\nB3 R3\nB3 R3\nB3 R3\nB3\n7/10\n3/10\n6/9\n3/9\n7/10\n3/10\n5/8\n3/8\n6/9\n3/9\n6/9\n3/9\n7/10\n3/10\nSumming the probability to all the B3 nodes we get\nP(B3) =\n⋅\n⋅8 + 7 ⋅\n⋅9 + 3 ⋅\n⋅9 + 3 ⋅\n⋅10 = 0.350.\nProblem 11. Suppose that P (A) = 0.4, P (B) = 0.3 and P((A∪B)C) = 0.42. Are A and\nB independent?\nSolution: We have P(A∪B) = 1 -0.42 = 0.58 and we know because of the inclusion-\nexclusion principle that\nP(A∪B) = P(A) + P(B) -P(A∩B).\nThus,\nP (A ∩ B) = P (A) + P (B) - P (A ∪ B) = 0.4 + 0.3 - 0.58 = 0.12 = (0.4)(0.3) = P (A)P (B).\nSo, A and B are independent.\nProblem 12. Suppose now that events A, B and C are mutually independent with\nP(A) = 0.3,\nP(B) = 0.4,\nP(C) = 0.5.\nCompute the following: (Hint: Use a Venn diagram)\n(i) P(A∩B ∩Cc)\n(ii) P(A∩Bc ∩C)\n(iii) P(Ac ∩B ∩C)\n\nSolution: By the mutual independence we have\nP(A∩B∩C) = P(A)P(B)P(C) = 0.06\nP(A∩B) = P(A)P(B) = 0.12\nP(A∩C) = P(A)P(C) = 0.15\nP(B∩C) = P(B)P(C) = 0.2\nWe show this in the following Venn diagram\n0.09\n0.14\n0.21\n0.06\n0.14\n0.09\n0.06\nA\nB\nC\nNote that, for instance, P (A ∩ B) is split into two pieces. One of the pieces is P (A ∩ B ∩ C)\nwhich we know and the other we compute as P(A∩B)-P(A∩B∩C) = 0.12-0.06 = 0.06.\nThe other intersections are similar.\nWe can read off the asked for probabilities from the diagram.\n(i) P(A∩B ∩Cc) = 0.06\n(ii) P(A∩Bc ∩C) = 0.09\n(iii) P(Ac ∩B ∩C) = 0.14.\nProblem 13. Suppose A and B are events with 0 < P(A) < 1 and 0 < P(B) < 1.\n(a) If A and B are disjoint can they be independent?\n(b) If A and B are independent can they be disjoint?\n(c) If A ⊂ B can they be independent?\nSolution: The answer to all three parts is 'No'. Each of these answers relies on the fact\nthat the probabilities of A and B are strictly between 0 and 1.\nTo show A and B are not independent we need to show either P(A ∩ B) =P(A) ⋅P(B)\nor P (A|B) = P (A).\n(a) No, they cannot be independent: A ∩ B = ∅ ⇒ P(A ∩ B) = 0 =P(A) ⋅P(B).\n(b) No, they cannot be disjoint: same reason as in part (a).\n(c) No, they cannot be independent: A⊂B ⇒ A∩B = A\n⇒ P(A ∩ B) = P(A) > P(A) ⋅P(B). The last inequality follows because P(B) < 1.\nProblem 14. Directly from the definitions of expected value and variance, compute E[X]\nand Var(X) when X has probability mass function given by the following table:\nX\n-2\n-1\npmf\n1/15\n2/15\n3/15\n4/15\n5/15\n\nSolution: We compute\nE[X] = -2 ⋅ 15 + -1 ⋅ 15 + 0 ⋅ 15 + 1 ⋅ 15 + 2 ⋅ 15 = 3.\nThus\nVar(X) = E[(X - 3\n2)2]\n= (-2 - 2\n3) ⋅ 15 + (-1 - 2\n3) ⋅ 15 + (0 - 2\n3) ⋅ 15 + (1 - 2\n3) ⋅ 15 + (2 - 2\n3) ⋅ 15\n= 9 .\nProblem 15. Suppose that X takes values between 0 and 1 and has probability density\nfunction 2x. Compute Var(X) and Var(X2).\nSolution: We will make use of the formula Var(Y ) = E[Y 2] - E[Y ]2. First we compute\nE[X] = ∫\nx ⋅ 2xdx = 3\nE[X2] = ∫\nx2 ⋅ 2xdx = 2\nE[X4] = ∫\nx4 ⋅ 2xdx = 3.\nThus,\nVar(X) = E[X2] - (E[X])\n2 = 2 - 4\n9 = 18\nand\nVar(X2) = E[X4] - (E[X2])\n2 = 3 - 1\n4 = 12.\nProblem 16. The pmf of X is given by the following table\nValue of X\n-1\nProbability\n1/3\n1/6\n1/2\n(a) Compute E[X].\n(b) Give the pdf of Y = X2 and use it to compute E[Y ].\n(c) Instead, compute E[X2] directly from an extended table.\n(d) Compute Var(X).\n(a) Solution: We have the extended table\nX values:\n-1\nprob:\n1/3\n1/6\n1/2\nX2\nSo, E[X] = -1/3 + 1/2 = 1/6.\n\n(b) Solution:\nY values:\nprob:\n1/6\n5/6\n⇒ E[Y ] = 5/6.\n(c) Solution: Using the table in part (a) E[X2] = 1 ⋅(1/3) + 0 ⋅(1/6) + 1 ⋅(1/2) = 5/6\n(same as part (b)).\n(d) Solution: Var(X) = E[X2] - E[X]2 = 5/6 - 1/36 = 29/36.\nProblem 17. Compute the expectation and variance of a Bernoulli(p) random variable.\nSolution: Make a table:\nX:\nprob:\n(1-p)\np\nX2\n1.\nFrom the table, E[X] = 0 ⋅(1 -p) + 1 ⋅p = p.\nSince X and X2 have the same table E[X2] = E[X] = p.\nTherefore, Var(X) = p-p2 = p(1 -p).\nProblem 18. Suppose 100 people all toss a hat into a box and then proceed to randomly\npick out of a hat. What is the expected number of people to get their own hat back.\nHint: express the number of people who get their own hat as a sum of random variables\nwhose expected value is easy to compute.\nSolution: Let X be the number of people who get their own hat.\nFollowing the hint: let Xj represent whether person j gets their own hat. That is, Xj = 1\nif person j gets their hat and 0 if not.\nWe have, X = ∑Xj, so E[X] = ∑E[Xj].\nj=1\nj=1\nSince person j is equally likely to get any hat, we have P (Xj = 1) = 1/100. Thus, Xj ∼\nBernoulli(1/100) ⇒ E[Xj] = 1/100 ⇒ E[X] = 1.\nProblem 19. Suppose that X ∼ Bin(n, 0.5). Find the probability mass function of Y = 2X.\nSolution: For y = 0, 2, 4, ... , 2n,\ny\n2) = ( n\nP(Y = y) = P(X =\nn\n.\ny/2) (1\n2)\nProblem 20. (a) Suppose that X is uniform on [0, 1]. Compute the pdf and cdf of X.\n(b) If Y = 2X+ 5, compute the pdf and cdf of Y .\n(a) Solution: We have fX(x) = 1 for 0 ≤ x ≤ 1. The cdf of X is\nx\nx\nFX(x) = ∫ fX(t)dt = ∫ 1dt = x.\n\n(b) Solution: Since X is between 0 and 1 we have Y is between 5 and 7. Now for 5 ≤ y ≤ 7,\nwe have\n) = FX(y-5\ny-5\nFY(y) = P(Y≤y) = P(2X+ 5 ≤y) = P(X≤ y-5\n) =\n.\nDifferentiating P(Y ≤ y) with respect to y, we get the probability density function of Y ,\nfor 5 ≤ y ≤ 7,\nfY (y) = 2.\nProblem 21. (a) Suppose that X has probability density function fX(x) = λe-λx for\nx ≥ 0. Compute the cdf, FX(x).\n(b) If Y = X2, compute the pdf and cdf of Y .\n(a) Solution: We have cdf of X,\nFX(x) = ∫\nx\nλe-λxdx = 1 - e-λx.\nNow for y ≥ 0, we have\n(b) Solution:\nFY(y) = P(Y ≤y) = P(X2 ≤y) = P(X≤ √y) = 1 - e-λ√y.\nDifferentiating FY (y) with respect to y, we have\nfY (y) = λ\n2 y- 1\n2 e-λ√y.\nProblem 22. Suppose that X is a random variable that takes on values 0, 2 and 3 with\nprobabilities 0.3, 0.1, 0.6 respectively. Let Y = 3(X-1)2.\n(a) What is the expectation of X?\n(b) What is the variance of X?\n(c) What is the expection of Y ?\n(d) Let FY (t) be the cumulative density function of Y . What is FY (7)?\n(a) Solution: We first make the probability tables\nX\nprob.\n0.3\n0.1\n0.6\nY\nSo, E[X] = 0 ⋅0.3 + 2 ⋅0.1 + 3 ⋅0.6 = 2\n(b) Solution: E[X2] = 0 ⋅0.3 + 4 ⋅0.1 + 9 ⋅0.6 = 5.8 ⇒ Var(X) = E[X2] - E[X]2 =\n5.8 -4 = 1.8.\n(c) Solution: E[Y] = 3 ⋅0.3 + 3 ⋅0.1 + 12 ⋅6 = 8.4.\n(d) Solution: From the table we see that FY(7) = P(Y ≤7) = 0.4.\n\nProblem 23. Suppose you roll a fair 6-sided die 25 times (independently), and you get $3\nevery time you roll a 6. Let X be the total number of dollars you win.\n(a) What is the pmf of X.\n(b) Find E[X] and Var(X).\n(c) Let Y be the total won on another 25 independent rolls. Compute and compare E[X+Y ],\nE[2X], Var(X + Y), Var(2X).\nExplain briefly why this makes sense.\n(a) Solution: There are a number of ways to present this. Here's one:\nX ∼3 binomial(25, 1/6), so\nk\n25-k\nP(X = 3k) = (25\nk) (6\n1) (6\n5)\n,\nfor k= 0, 1, 2, ... , 25.\n(b) Solution: X ∼3 binomial(25, 1/6).\nRecall that the mean and variance of binomial(n, p) are np and np(1 - p). So,\nE[X] = 3np= 3 ⋅25 ⋅6\n1 = 75/6, and Var(X) = 9np(1 - p) = 9 ⋅ 25(1/6)(5/6) = 125/4.\n(c) Solution: E[X + Y ] = E[X] + E[Y ] = 150/6 = 25., E[2X] = 2E[X] = 150/6 = 25.\nVar(X + Y) = Var(X) + Var(Y ) = 250/4. Var(2X) = 4Var(X) = 500/4.\nThe means of X + Y and 2X are the same, but Var(2X) > Var(X + Y).\nThis makes sense because in X + Y sometimes X and Y will be on opposite sides from the\nmean so distances to the mean will tend to cancel, However in 2X the distance to the mean\nis always doubled.\nProblem 24. A continuous random variable X has PDF f(x) = x + ax2 on [0,1]\nFind a, the CDF and P(0.5 < X < 1).\nSolution: First we find the value of a:\n2 + a\n∫ f(x) dx = 1 = ∫ x+ ax2 dx = 1\n3 ⇒ a = 3/2.\nThe CDF is FX(x) = P(X ≤ x). We break this into cases:\n(i) b < 0, so FX(b) = 0.\n(ii) 0 ≤ b ≤ 1, so FX(b) = ∫0\nbx+ 3\n2x2 dx = b\n2 + b\n3 .\n(iii) 1 < x, so FX(b) = 1.\nUsing FX we get\nP(0.5 < X< 1) = FX(1) -FX(0.5) = 1 -(0.52 + 0.53\n) = 13\n16.\nProblem 25. For each of the following say whether it can be the graph of a cdf. If it can\nbe, say whether the variable is discrete or continuous.\n\nx\nF (x)\n0.5\n(i)\nx\nF (x)\n0.5\n(ii)\nx\nF (x)\n0.5\n(iii)\nx\nF (x)\n0.5\n(iv)\nx\nF (x)\n0.5\n(v)\nx\nF (x)\n0.5\n(vi)\nx\nF (x)\n0.5\n(vii)\nx\nF (x)\n0.5\n(viii)\nSolution:\n(i) yes, discrete,\n(ii) no,\n(iii) no,\n(iv) no,\n(v) yes, continuous\n(vi) no\n(vii) yes, continuous,\n(viii) yes, continuous.\nProblem 26. Correlation\nFlip a coin 5 times. Use properties of covariance to compute the covariance and correlation\nbetween the number of heads on the first 3 and last 3 flips.\nSolution: As usual let Xi = the number of heads on the ith flip, i.e. 0 or 1.\nLet X = X1 + X2 + X3 the sum of the first 3 flips and Y = X3 + X4 + X5 the sum of the\nlast 3. Using the algebraic properties of covariance we have\nCov(X, Y) = Cov(X1 + X2 + X3, X3 + X4 + X5)\n= Cov(X1, X3) + Cov(X1, X4) + Cov(X1, X5)\n+ Cov(X2, X3) + Cov(X2, X4) + Cov(X2, X5)\n+ Cov(X3, X3) + Cov(X3, X4) + Cov(X3, X5)\nBecause the Xi are independent the only non-zero term in the above sum is Cov(X3X3) = Var(X3) = 4\nTherefore, Cov(X, Y) = 1\n4.\n\nWe get the correlation by dividing by the standard deviations. Since X is the sum of 3\nindependent Bernoulli(0.5) we have σX = √3/4\nCov(X, Y)\n1/4\nCor(X, Y) =\n=\nσXσY\n(3)/4 = 3.\nProblem 27. Exponential Distribution\nSuppose that buses arrive are scheduled to arrive at a bus stop at noon but are always X\nminutes late, where X is an exponential random variable with probability density function\nfX(x) = λe-λx. Suppose that you arrive at the bus stop precisely at noon.\n(a) Compute the probability that you have to wait for more than five minutes for the bus\nto arrive.\nSolution: We compute\nP(X ≥5) = 1 -P(X < 5) = 1 -∫\nλe-λxdx = 1 - (1 - e-5λ) = e-5λ.\n(b) Suppose that you have already waiting for 10 minutes. Compute the probability that you\nhave to wait an additional five minutes or more.\nSolution: We want P (X ≥ 15|X ≥ 10). First observe that P(X ≥ 15, X ≥ 10) = P(X ≥\n15). From similar computations in (a), we know\nP(X ≥ 15) = e-15λ\nP (X ≥ 10) = e-10λ.\nFrom the definition of conditional probability,\nP(X ≥ 15, X ≥ 10)\nP(X ≥ 15)\nP(X ≥ 15|X ≥ 10) =\n=\nP(X ≥ 10)\nP(X ≥ 10) = e-5λ\nNote: This is an illustration of the memorylessness property of the exponential distribu\ntion.\nProblem 28. Normal Distribution: Throughout these problems, let φ and Φ be the pdf\nand cdf, respectively, of the standard normal distribution Suppose Z is a standard normal\nrandom variable and let X = 3Z+ 1.\n(a) Express P(X ≤ x) in terms of Φ\nSolution: We have\n) = Φ (x-1\nFX(x) = P(X≤x) = P(3Z+ 1 ≤x) = P(Z≤ x-1\n) .\n(b) Differentiate the expression from (a) with respect to x to get the pdf of X, f(x).\nRemember that Φ′(z) = φ(z) and don't forget the chain rule\nSolution: Differentiating with respect to x, we have\nd\n3φ(x - 1\nfX(x) =\nFX(x) =\n) .\ndx\n\n2 e- x2\nSince φ(x) = (2π)- 1\n2 , we conclude\n- (x-1)2\nfX(x) =\n2⋅32 ,\n√\n2π\ne\nwhich is the probability density function of the N(1, 9) distribution. Note: The arguments\nin (a) and (b) give a proof that 3Z +1 is a normal random variable with mean 1 and variance\n9. See Problem Set 3, Question 5.\n(c) Find P(-1 ≤X ≤1)\nSolution: We have\nP(-1 ≤X≤1) = P(-2\n3 ≤Z≤0) = Φ(0) -Φ (-2\n3) ≈0.2475\n(d) Recall that the probability that Z is within one standard deviation of its mean is approx\nimately 68%. What is the probability that X is within one standard deviation of its mean?\nSolution: Since E[X] = 1, Var(X) = 9, we want P(-2 ≤ X ≤ 4). We have\nP(-2 ≤X ≤4) = P(-3 ≤3Z ≤3) = P(-1 ≤Z ≤1) ≈0.68.\nProblem 29. Transforming Normal Distributions\n= eZ\nSuppose Z ∼ N(0,1) and Y\n.\n(a) Find the cdf FY (a) and pdf fY (y) for Y . (For the CDF, the best you can do is write it\nin terms of Φ the standard normal cdf.)\nSolution: Note, Y follows what is called a log-normal distribution.\nFY(a) = P(Y ≤a) = P(eZ≤a) = P(Z≤ ln(a)) = Φ(ln(a)).\nDifferentiating using the chain rule:\nd\nd\nfy(a) = daFY (a) = daΦ(ln(a)) = aφ(ln(a)) = √\n2π a\ne-(ln(a))2/2.\n(b) We don't have a formula for Φ(z) so we don't have a formula for quantiles. So we have\nto write quantiles in terms of Φ-1.\n(i) Write the 0.33 quantile of Z in terms of Φ-1\n(ii) Write the 0.9 quantile of Y in terms of Φ-1.\n(iii) Find the median of Y .\nSolution: (i) The 0.33 quantile for Z is the value q0.33 such that P (Z ≤ q0.33) = 0.33.\nThat is, we want\nΦ(q0.33) = 0.33 ⇔ q0.33 = Φ-1(0.33) .\n(ii) We want to find q0.9 where\nq0.9 = eΦ-1(0.9) .\nFY (q0.9) = 0.9 ⇔ Φ(ln(q0.9)) = 0.9 ⇔\n\nΦ-1(0.5) = e\n(iii) As in (ii) q0.5 = e\n0 = 1 .\nProblem 30. (Random variables derived from normal random variables)\nLet X1, X2, ...Xn be i.i.d. N(0, 1) random variables.\nLet Yn = X1\n2 + ... + Xn\n2 .\n(a) Use the formula Var(Xj) = E[Xj\n2] - E[Xj]2 to show E[Xj\n2] = 1.\nSolution: Var(Xj) = 1 = E[Xj\n2] - E[Xj]2 = E[Xj\n2]. QED\n(b) Set up an integral in x for computing E[Xj\n4].\nFor 3 extra credit points, use integration by parts show E[Xj\n4] = 3.\n(If you don't do this, you can still use this result in part c.)\nSolution: E[Xj\n4] =\ninf\nx4e-x2/2 dx.\n√1\n2π\n∫\n-inf\n(Extra credit) By parts: let u = x3, v′ = xe-x2/2 ⇒ u′ = 3x2, v = -e-x2/2\ninf\nE[\n] =\n∣\ninf\n+\n3x2e-x2/2 dx]\nXj\n√1\n2π\n[x3e-x2/2\n√1\n2π\n∫\ninfty\n-inf\nThe first term is 0 and the second term is the formula for 3E[Xj\n2] = 3 (by part (a)). Thus,\nE[Xj\n4] = 3.\n(c) Deduce from parts (a) and (b) that Var(Xj\n2) = 2.\nSolution: Var(Xj\n2) = E[Xj\n4] - E[Xj\n2]2 = 3 - 1 = 2. QED\n(d) Use the Central Limit Theorem to approximate P (Y100 > 110).\nSolution: E[Y100] = E[100Xj\n2] = 100.\nVar(Y100) = 100Var(Xj) = 200.\nThe CLT says Y100 is approximately normal. Standardizing gives\nP(Y100 > 110) = P(Y100√\n-100 > √10\n200) ≈P(Z> 1/\n√\n2) = 0.24 .\nThis last value was computed using R: 1 - pnorm(1/sqrt(2),0,1).\nProblem 31. More Transforming Normal Distributions\n(a) Suppose Z is a standard normal random variable and let Y = aZ+ b, where a > 0 and\nb are constants.\nShow Y ∼ N(b, a2) (remember our notation for normal distributions uses mean and vari\nance).\nSolution: Let φ(z) and Φ(z) be the PDF and CDF of Z.\nFY (y) = P (Y ≤ y) = P (aZ + b ≤ y) = P (Z ≤ (y - b)/a) = Φ((y - b)/a).\nDifferentiating:\nd\nd\nfY(y) = dy FY(y) = dy Φ((y-b)/a) = aφ((y-b)/a) = √\n2πa\ne-(y-b)2/2a2.\nSince this is the density for N(b, a2) we have shown Y ∼ N(b, a2).\nY -μ\n(b) Suppose Y ∼ N(μ, σ2). Show\nfollows a standard normal distribution.\nσ\n\nSolution: By part (a), Y ∼ N(μ, σ2) ⇒ Y = σZ+ μ. But, this implies (Y -μ)/σ = Z∼\nN(0, 1).\nQED\nProblem 32. (Sums of normal random variables)\nLet X, Y be independent random variables where X ∼ N(2, 5) and Y ∼ N(5, 9) (we use the\nnotation N(μ, σ2)). Let W = 3X-2Y + 1.\n(a) Compute E[W ] and Var(W ).\nSolution: E[W] = 3E[X] -2E[Y] + 1 = 6 -10 + 1 = -3\nVar(W) = 9Var(X) + 4Var(Y) = 45 + 36 = 81\n(b) It is known that the sum of independent normal distributions is normal. Compute\nP(W ≤ 6).\nSolution: Since the sum of independent normal is normal part (a) shows: W ∼ N(-3, 81).\nLet Z ∼ N(0, 1). We standardize W : P(W≤6) = P(W+ 3 ≤9\n9) = P(Z≤1) ≈ 0.84.\nProblem 33. Let X ∼ U(a, b). Compute E[X] and Var(X).\nSolution: Method 1\nU(a, b) has density f(x) =\n[a, b]. So,\nb - a on\nb\nb\nb\nx2\nb2 - a2\na + b\nE[X] = ∫ xf(x) dx =\nxdx =\n∣ =\n.\nb-a ∫\n2(b-a)\n2(b-a) =\na\na\na\nb\nb\nb\nx3\nb3 - a3\nE[X2] = ∫ x2f(x) dx =\nx2 dx =\n∣ =\nb-a ∫\n3(b-a)\n3(b-a).\na\na\na\nFinding Var(X) now requires a little algebra,\nb3 - a3\nVar(X) = E[X2] - E[X]2 = 3(b - a) - (b + a)2\n4(b3 - a3) - 3(b - a)(b + a)2\nb3 - 3ab2 + 3a2b - a3\n(b - a)3\n(b - a)2\n=\n=\n=\n.\n12(b - a)\n12(b - a)\n12(b - a) =\nMethod 2\nThere is an easier way to find E[X] and Var(X).\nLet U ∼ U(a, b). Then the calculations above show E[U] = 1/2 and (E[U2] = 1/3 ⇒\nVar(U) = 1/3 - 1/4 = 1/12.\nNow, we know X = (b-a)U + a, so E[X] = (b - a)E[U] + a = (b - a)/2 + a = (b + a)/2\nand Var(X) = (b - a)2Var(U) = (b - a)2/12.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Practice Final Exam Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_prac_exam_final_sol.pdf",
      "content": "Practice Final -solutions, 18.05, Spring 2022\nConcept Problem 1. Which of the following represents a valid probability table?\n(i)\noutcomes\nprobability\n1/5\n1/5\n1/5\n1/5\n1/5\n(ii)\noutcomes\nprobability\n1/2\n1/5\n1/10\n1/10\n1/10\nCircle the best choice:\nA. (i)\nB. (ii)\nC. (i) and (ii)\nD. Not enough information\nSolution: C. (i) and (ii)\nConcept Problem 2. True or false: Setting the prior probability of a hypothesis to 0\nmeans that no amount of data will make the posterior probability of that hypothesis the\nmaximum over all hypotheses.\nCircle one: True\nFalse\nSolution: True\nConcept Problem 3. True or false: It is okay to have a prior that depends on more than\none unknown parameter.\nCircle one: True\nFalse\nSolution: True\nConcept Problem 4. Data is drawn from a normal distribution with unknown mean μ.\nWe make the following hypotheses:\nH0: μ = 1\nand\nHA: μ > 1.\nFor (i)-(iii) circle the correct answers:\n(i) Is H0 a simple or composite hypothesis?\nSimple\nComposite\n(ii) Is HA a simple or composite hypothesis?\nSimple\nComposite\n(iii) Is HA a one or two-sided?\nOne-sided\nTwo-sided\nSolution: (i) Simple\n(ii) Composite\n(iii) One-sided\nConcept Problem 5. If the original data has n points then a bootstrap sample should\nhave\nA. Fewer points than the original because there is less information in the sample than in\nthe underlying distribution.\nB. The same number of points as the original because we want the bootstrap statistic to\nmimic the statistic on the original data.\nC. Many more points than the original because we have the computing power to handle a\nlot of data.\nCircle the best answer:\nA\nB\nC.\nSolution: B.\n\nPractice Final, Spring 2022\nConcept Problem 6. In 3 tosses of a coin which of following equals the event \"exactly\ntwo heads\"?\nA = {THH, HTH, HHT, HHH}\nB = {THH, HTH, HHT}\nC = {HTH, THH}\nCircle the best answer:\nA\nB\nC\nB and C\nSolution: 2. B\nConcept Problem 7. These questions all refer to the following figure. For each one circle\nthe best answer.\nA1\nA2\nB1\nB2\nB1\nB2\nC1\nC2\nC1\nC2\nC1\nC2\nC1\nC2\nx\ny\nz\n(i) The probability x represents\nA. P (A1) B. P (A1|B2) C. P (B2|A1) D. P (C1|B2 ∩ A1).\nSolution: A. P (A1).\n(ii) The probability y represents\nA. P (B2) B. P (A1|B2) C. P (B2|A1) D. P (C1|B2 ∩ A1).\nSolution: C. P (B2|A1).\n(iii) The probability z represents\nA. P (C1) B. P (B2|C1) C. P (C1|B2) D. P (C1|B2 ∩ A1).\nSolution: D. P (C1|B2 ∩ A1).\n(iv) The circled node represents the event\nA. C1\nB. B2 ∩ C1\nC. A1 ∩ B2 ∩ C1\nD. C1|B2 ∩ A1.\nSolution: C. A1 ∩ B2 ∩ C1.\nConcept Problem 8. The graphs below give the pmf for 3 random variables.\nx\n(A)\nx\n(B)\nx\n(C)\nCircle the answer that orders the graphs from smallest to biggest standard deviation.\nABC\nACB\nBAC\nBCA\nCAB\nCBA\nSolution: BAC.\nConcept Problem 9. Suppose you have $100 and you need $1000 by tomorrow morning.\nYour only way to get the money you need is to gamble. If you bet $k, you either win $k with\nprobability p or lose $k with probability 1 - p. Here are two strategies:\n\nPractice Final, Spring 2022\nMaximal strategy: Bet as much as you can, up to what you need, each time.\nMinimal strategy: Make a small bet, say $10, each time.\nSuppose p = 0.8.\nCircle the better strategy:\nMaximal\n2. Minimal\nSolution: p = 0.8 use minimal strategy.\nIf you use the minimal strategy the law of large numbers says your average winnings per\nbet will almost certainly be the expected winnings of one bet.\nWin\n-10\np\n0.2\n0.8\nThe expected value when p = 0.8 is 6. Since this is positive you'd like to make a lot of bets\nand let the law of large numbers (practically) guarantee you will win an average of $6 per\nbet. So you use the minimal strategy.\nConcept Problem 10. Consider the following joint pdf's for the random variables X and\nY . Circle the ones where X and Y are independent and cross out the other ones.\nA. f(x, y) = 4x2y3\nB. f(x, y) = 1\n2(x3y+ xy3).\nC. f(x, y) = 6e-3x-2y\nSolution:\nA. Independent. The variables can be separated: the marginal densities are fX(x) = ax2\nand fY (y) = by3 for some constants a and b with ab = 4.\nX and Y are not independent because there is no way to factor f(x, y)\nB. Not independent.\ninto a product fX(x)fY (y).\nC. Independent. The variables can be separated: the marginal densities are fX(x) = ae-3x\nand fY (y) = be-2y for some constants a and b with ab = 6.\nConcept Problem 11. Suppose X ∼ Bernoulli(θ) where θ is unknown. Which of the\nfollowing is the correct statement?\nA. The random variable is discrete, the space of hypotheses is discrete.\nB. The random variable is discrete, the space of hypotheses is continuous.\nC. The random variable is continuous, the space of hypotheses is discrete.\nD. The random variable is continuous, the space of hypotheses is continuous.\nCircle the letter of the correct statement:\nA\nB\nC\nD\nSolution: B. A Bernoulli random variable takes values 0 or 1. So X is discrete. The pa\nrameter θ can be anywhere in the continuous range [0,1]. Therefore the space of hypotheses\nis continuous.\nConcept Problem 12. Let θ be the probability of heads for a bent coin. Suppose your\nprior f(θ) is Beta(6, 8). Also suppose you flip the coin 7 times, getting 2 heads and 5 tails.\nWhat is the posterior pdf f(θ|x)? Circle the best answer.\nA. Beta(2,5)\nB. Beta(3,6)\nC. Beta(6,8)\nD. Beta(8,13)\nE. Not enough information to say\n\nPractice Final, Spring 2022\nSolution: D. By the form of the posterior pdf we know it is Beta(8, 13).\nConcept Problem 13. Suppose the prior has been set. Let x1 and x2 be two sets of data.\nCircle true or false for each of the following statements.\nA. If x1 and x2 have the same likelihood function then they result in the same posterior. True\nFalse\nB. If x1 and x2 result in the same posterior then they have the same likelihood function. True\nFalse\nC. If x1 and x2 have proportional likelihood functions then they result in the same True\nFalse\nposterior.\nSolution: A. True,\nB. False\nC. True\nConcept Problem 14. Each day Jane arrives X hours late to class, with X ∼ uniform(0, θ).\nJon models his initial belief about θ by a prior pdf f(θ). After Jane arrives x hours late to\nthe next class, Jon computes the likelihood function f(x|θ) and the posterior pdf f(θ|x).\nCircle the probability computations a frequentist would consider valid. Cross out the others.\nA. prior\nB. posterior\nC. likelihood\nSolution: A. Not valid\nB. not valid\nC. valid\nBoth the prior and posterior measure a belief in the distribution of hypotheses about the\nvalue of θ. The frequentist does not consider them valid.\nThe likelihood f(x|θ) is perfectly acceptable to the frequentist. It represents the probability\nof data from a repeatable experiment, i.e. measuring how late Jane is each day. Conditioning\non θ is fine. This just fixes a model parameter θ. It doesn't require computing probabilities\nfor θ.\nConcept Problem 15. Suppose we run a two-sample t-test for equal means with signif\nicance level α = 0.05. If the data implies we should reject the null hypothesis, then the\nodds that the two samples come from distributions with the same mean are (circle the best\nanswer)\nA. 19/1\nB. 1/19\nC. 20/1\nD. 1/20\nE. unknown\nSolution: E. unknown. Frequentist methods only give probabilities for data under an\nassumed hypothesis. They do not give probabilities or odds for hypotheses. So we don't\nknow the odds for distribution means\nConcept Problem 16. Consider the following statements about a 95% confidence interval\nfor a parameter θ.\nA. P (θ0 is in the CI | θ = θ0) ≥ 0.95\nB. P (θ0 is in the CI ) ≥ 0.95\nC. An experiment produces the CI [-1, 1.5]: P (θ is in [-1, 1.5] | θ = 0) ≥ 0.95\nCircle the letter of each correct statement and cross out the others:\nA\nB\nC\n\nPractice Final, Spring 2022\nA. Solution: Correct, This is the definition of a confidence interval.\nB. Solution: Incorrect. Frequentist methods do not give probabilities for hypotheses.\nC. Solution: Correct. Given θ = 0 the probability θ is in [-1, 1.5] is 100%.\nProblem 17. (a) Let A and B be two events. Suppose that the probability that neither\nevent occurs is 3/8. What is the probability that at least one of the events occurs?\n(b) Let C and D be two events. Suppose P(C) = 0.5, P (C∩D) = 0.2 and P((C∪D)c) = 0.4.\nWhat is P (D)?\n(a) Solution: P ((A ∪ B)c) = 3/8 ⇒ P(A∪B) = 5/8 ..\nA\nB\nC\nD\n0.3\n0.2\n0.4\n0.2\n0.1\nFigure for part (a).\nFigure for part (b).\n(b) Solution: See the figure: P((CUD)c) = 0.4 ⇒ P((CUD) = 0.6).\nP(C∪D) = P(C) + P(D) -P(C∩D) ⇒ 0.6 = 0.5 + P(D) -0.2 ⇒ P(D) = 0.3 .\nProblem 18. An urn contains 3 orange balls and 2 blue balls. A ball is drawn. If the ball\nis orange, it is kept out of the urn and a second ball is drawn from the urn. If the ball is\nblue, then it is put back in the urn and an orange ball is added to the urn. Then a second\nball is drawn from the urn.\n(a) What is the probability that both balls drawn are orange?\n(b) If the second drawn ball is orange, what is the probability that the first drawn ball was\nblue?\nSolution: Let O1 be the event the first ball is orange and O2 that the second ball is orange.\nLikewise for B1 and B2. The following tree captures all the details of the game.\nO1\nB1\nO2\nB2\nO2\nB2\n3/5\n2/5\n2/4\n2/4\n4/6\n2/6\n(a) P(O1 ∩O2) =\n⋅\n20 = 0.3 .\n4 =\nP (O2|B1)P (B1)\n6 ⋅ 2\n8/30\n(b) P (B1|O2) =\n=\n=\nP(O2)\n⋅5 + 4 ⋅\n17/30 = 17.\nProblem 19. You roll a fair six sided die repeatedly until the sum of all numbers rolled\nis greater than 6. Let X be the number of times you roll the die. Let F be the cumulative\ndistribution function for X. Compute F (1), F (2), and F (7).\n\n{\n{\nPractice Final, Spring 2022\nSolution: F (1): Since you never get more than 6 on one roll we have F(1) = 0 .\nF(2) = P(X = 1) + P(X = 2):\nP(X = 1) = 0\nP(X = 2) = P(total on 2 dice = 7,8,9,10,11,12) = 36 = 12 .\nF (7): The smallest total on 7 rolls is 7, so F(7) = 1 .\nProblem 20. A test is graded on the scale 0 to 1, with 0.55 needed to pass.\nStudent scores are modeled by the following density:\n⎧4x\nfor 0 ≤ x ≤ 1/2\n{\nf(x) =\n4 - 4x\nfor 1/2 ≤ x ≤ 1\n⎨\n{\n⎩0\notherwise\n(a) What is the probability that a random student passes the exam?\n(b) What score is the 87.5 percentile of the distribution?\n(a) Solution: Let X = score of a random student.\nP(X≥0.55) = ∫\nf(x) dx = ∫\n4 -4xdx = 4x-2x2∣0.55 = 2 -4 × 0.55 + 2(0.55)2 = 0.405\n0.55\n0.55\n(b) Solution: Geometric method:\nWe need the shaded area in the figure to be 0.125\ny\nShaded area = area of triangle = 2\n1(1 - x)(4 - 4x) = 0.125.\nSolving for x we get\n2(1 -x)2 = 0.125 ⇒ (1 -x)2 =\nx =\nx\n4 .\n1 ⇒\nx = q0.875\nAnalytic mehtod: We want a such that F (a) = 7/8. Since f(x) is defined in two pieces we\nhave to compute F (a) in two pieces.\n1/2\nF (1/2) = ∫\n1/2\n4x dx = 2x2∣\n=\n2.\n(Which we knew geometrically already.)\nFor a ≥ 1/2 we then have\n1/2\na\nF(a) = ∫\n4xdx+ ∫ 4 -4xdx\n1/2\na\n= 2\n1 + ∫ 4 -4xdx\n1/2\n= 2\n1 + [4x - 2x2∣\na\n1/2\n= 4a - 2a2 - 1.\n\n{\n{\nPractice Final, Spring 2022\nSolving for a such that F (a) = 7/8 we get\n4 ±\n√\n4a-2a2 -1 = 7/8 ⇒ 2a2 -4a+ 15/8 = 0 ⇒ a =\n= 4, 4.\nSince 5\n4 is not in the range of X we have a = 3/4 . (The same answer as with the geometric\nmethod.)\nProblem 21. Suppose X is a random variable with cdf\n⎧0\nfor x < 0\n{\nF (x) =\nx(2 - x)\nfor 0 ≤ x ≤ 1\n⎨\n{\n⎩1\nfor x > 1\n(a) Find E[X].\n(b) Find P (X < 0.4).\n(a) Solution: f(x) = F ′(x) = 2 - 2x on [0, 1]. Therefore\nE[X] = ∫\nxf(x) dx\n= ∫ 2x - 2x2 dx\n= x2 - 2\n3x3∣\n= 3 .\n(b) Solution: P (X ≤ 0.4) = F (0.4) = 0.4(2 - 0.4) = 0.4(1.6) = 0.64 .\nProblem 22. Compute the mean and variance of a random variable whose distribution is\nuniform on the interval [a, b].\nIt is not enough to simply state these values. You must give the details of the computation.\nSolution: Let X ∼ U(a, b). The pdf of X is f(x) = b - a on the interval [a, b]. Thus,\nb\nb\nb\nx\nx2\nb2 - a2\nb + a\nE[X] = ∫ xf(x) dx = ∫\n∣ =\nb-a dx = 2(b-a)\n2(b-a) =\na\na\na\n\nPractice Final, Spring 2022\nVar(X) = ∫\nb\n(x-μ)2f(x) dx\na\n(x- a+ b\n= ∫\nb\n)\nb - a dx\na\n(x - a+b\nb\n2 )\n=\n∣\nb-a\na\n= ... algebra ...\n= 12\n1 (b - a)3\nb - a\n(b - a)2\n=\n.\nProblem 23. Defaulting on a loan means failing to pay it back on time. The default rate\namong MIT students on their student loans is 1%. As a project you develop a test to predict\nwhich students will default. Your test is good but not perfect. It gives 4% false positives,\ni.e. prediciting a student will default who in fact will not. If has a 0% false negative rate,\ni.e. prediciting a student won't default who in fact will.\n(a) Solution: Suppose a random student tests positive. What is the probability that he will\ntruly default.\n(b) Solution: Someone offers to bet me the student in part (a) won't default. They want\nme to pay them $100 if the student doesn't default and they'll pay me $400 if the student\ndoes default. Is this a good bet for me to take?\n(a) Solution: We organize the problem in a tree. Here:\nD+ = default, D- = no default\nT + = test is positive, T - = test is negative\nD+\nD-\nT+\nT-\nT+\nT-\n0.01\n0.99\n0.04\n0.96\nP (T +|D+)P (D+)\n0.01\nP (D+|T +) =\n=\nP (T +)\n0.01 + 0.99 ⋅ 0.04 =\n0.01\n4.96 ≈0.2 .\n0.0496 =\nP (D+|T +)\n1/4.96\n(b) Solution: Odds(winning) = Odds(D+|T +) = P (D-|T +) = 3.96/4.96 = 3.96.\nSince the payoff ratio 1 is greater than 1/(odds of winning), it is a good bet.\nEquivalently we can argue the\n3.96\nE[winnings] = 400 ⋅ 4.96 -100 ⋅ 4.96 = 4.96 > 0.\n\nPractice Final, Spring 2022\nA positive expected winnings means it's a good bet.\nProblem 24. Data was taken on height and weight from the entire population of 700\nmountain gorillas living in the Democratic Republic of Congo:\nht\\wt\nlight\naverage\nheavy\nshort\ntall\nLet X encode the weight, taking the values of a randomly chosen gorilla: 0, 1, 2 for light,\naverage, and heavy respectively.\nLikewise, let Y encode the height, taking values 0 and 1 for short and tall respectively.\n(a) Determine the joint pmf of X and Y and the marginal pmf's of X and of Y .\n(b) Are X and Y independent?\n(c) Find the covariance of X and Y .\nFor this part, you need a numerical (no variables) expression, but you can leave it uneval\nuated.\n(d) Find the correlation of X and Y .\nFor this part, you need a numerical (no variables) expression, but you can leave it uneval\nuated.\n(a) Solution: Probability table:\nY \\X\nmarginal for Y\n170/700\n70/700\n30/700\n270/700\nmarginal for X\n85/700\n255/700\n190/700\n260/700\n155/700\n185/700\n430/700\n(b) Solution: We check if P(X = 0, Y = 0) = P(X = 0)P(Y = 0).\n170 ? 255 270\n=\n700 700.\nCross-multiply and do a little algebra\n170 ⋅700 =\n? 255 ⋅270\n⇔\n11900 =\n?\n⇔ 11900 =\n? 68850\nSince they are not equal X and Y are not independent.\n(c) Solution:\nE[X] =\n⋅\n700 + 2 700 = 700 = 10\nE[Y ] = 700 = 70\nE[XY ] =\n⋅\n700 + 2 700 = 700 = 7\nCov(X, Y) -E[XY ] -E[X]E[Y] = 7 - 9 ⋅\n10 70 = 700\n\nPractice Final, Spring 2022\nCov(X, Y )\n(d) The definition of correlation is Cor(X, Y) =\n. So we first need to compute\nσXσY\nthe variances of X and Y .\nE[X2] = 700 + 4 ⋅\n=\n700 = 700\nThus,\n7 -81\nVar(X) = E[X2] - E[X]2 =\n100 = 700\nE[Y 2] = 70\n43 ⋅27\nVar(Y) = E[Y2] -E[Y]2 = 70 -( 43\n=\n70)\ntherefore\n113/700\nCor(X, Y) = √433/700 √43 ⋅ 27/702\nNote: We would accecpt -even encourage solutions- that left the fractions uncomputed,\ne.g. σY = √43/70 - (43/70)2.\nProblem 25. A political poll is taken to determine the fraction p of the population that\nwould support a referendum requiring all citizens to be fluent in the language of probability\nand statistics.\n(a) Assume p = 0.5. Use the central limit theorem to estimate the probability that in a poll\nof 25 people, at least 14 people support the referendum.\nYour answer to this problem should be a decimal.\nSolution: Let X ∼ binomial(25, 0.5) = the number supporting the referendum. We know\nthat\nE[X] = 12.5,\nVar(X) = 25 ⋅4 = 4 ,\nσX= 2.\nX - 12.5\nStandardizing and using the CLT we have Z=\n≈ N(0, 1) Therefore,\n5/2\n≥ 14 - 12.5\nP(X≥14) = P(X-12.5\n) ≈P(Z≥0.6) = Φ(-0.6) = 0.2743 ,\n5/2\n5/2\nwhere the last probability was looked up in the Z-table.\n(b) With p unknown and n the number of random people polled, let Xn be the fraction of\nthe polled people who support the referendum.\nWhat is the smallest sample size n in order to have a 90% confidence that Xn is within\n0.01 of the true value of p?\nYour answer to this problem should be an integer.\n\nPractice Final, Spring 2022\nSolution: The rule of thumb CI is\nx ± z0.05 ⋅ 2√1\nn.\nz0.05\nSo we want 2√n ≤ 0.01.\nFrom the table z0.05 = Φ(-0.05) = 1.65. So we want\n1.65\n2√n ≤ 0.01\n⇒\n√n ≥ 165\n⇒\nn > (82.5)2 = 6806.25\nSolution: n = 6807\nProblem 26. Suppose a researcher collects x1, ... , xn i.i.d. measurements of the background\nradiation in Boston. Suppose also that these observations follow a Rayleigh distribution with\nparameter τ , with pdf given by\n2 τx2\nf(x) = xτe-1\n.\nFind the maximum likelihood estimate for τ.\nSolution: For a fixed τ the pdf for xi is f(xi| τ) = xτe- 1\n2 τx2. Therefore the likelihood\nfunction of the data is\nf(data | τ) = x1x2 ⋯xnτn e- 1\n2 τ ∑ x2\ni .\nThe log likelihood is\nln(f(data | τ)) = ln(x1x2 ⋯ xn) + n ln(τ) - 2\n1τ∑x2\ni.\nWe find the MLE for τ by taking a derivative of the log likelihood with respect to τ and\nsetting equal to 0.\nd ln(f(data | τ))\nn\n2n\n=\n2 ∑x2\ni = 0 ⇒ n = 2 ∑x2\ni ⇒ τ=\n.\ndτ\nτ -1\nτ\n∑ x2\ni\nProblem 27. Bivariate data (4, 10), (-1, 3), (0, 2) is assumed to arise from the model\nyi = b|xi - 3| + ei, where b is a constant and ei are independent random variables.\n(a) What assumptions are needed on ei so that it makes sense to do a least squares fit of a\ncurve y = b|x-3| to the data?\n(b) Given the above data, determine the least squares estimate for b.\nFor this problem we want you to calculate all the way to a fraction b = s\nr , where r and s\nare integers.\n(a) Solution: We assume the random error terms ei are independent, have mean 0 and all\nhave the same variance (homoscedastic).\n(b) Solution:\nE[b] = sum of the squared errors\n= ∑(yi - b|xi - 3|)2\n= (10 - b)2 + (3 - 4b)2 + (2 - 3b)2\n\nPractice Final, Spring 2022\nThe least squares fit is found by setting the derivative (with respect to b) to 0,\nd E[b] = -2(10 -b) -8(3 -4b) -6(2 -3b) = 52b-56 = 0.\ndb\nTherefore the least squares estimate of b is b = 52 = 13 .\nProblem 28. Taxi problem Data is collected on the time between arrivals of consecutive\ntaxis at a downtown hotel. We collect a data set of size 45 with sample mean x = 5.0 and\nsample standard deviation s = 4.0.\n(a) Assume the data follows a normal random variable.\n(i) Find an 80% confidence interval for the mean μ of X.\n(ii) Find an 80% χ2-confidence interval for the variance?\n(b) Now make no assumptions about the distribution of of the data. By bootstrapping, we\ngenerate 500 values for the average inter-arrival time x∗ . The smallest and largest 150 are\nwritten in non-decreasing order on the next page.\nUse this data to find an 80% percentile bootstrap confidence interval for μ.\n(c) We suspect that the time between taxis is modeled by an exponential distribution, not a\nnormal distribution. In this case, are the approaches in the earlier parts justified?\n(d) When might method (b) be preferable to method (a)?\n(a) Solution: Since σ is unknown we use the Studentized mean\nx - μ\nt = s/√n ∼ t(44)\nwhich follows a t distribution with 44 degrees of freedom.\n(i) The 80% CI is x ± t0.1 √s\nn . From the t-table we get t0.1 with df = 44 is approximately\n1.3. Thus,\n80% CI = 5 ±\n⋅1.3\n√4\n(n - 1)s2\n(ii) We use the statistic\n∼ χ2(44). The 80% confidence interval for σ2 is\nσ2\n[(n - 1)s2 (n - 1)s2\n,\n] ,\nc0.9\nc0.1\nwhere c0.9 and c0.1 are the right critical values from the chi-square distribution with 44\ndegrees of freedom.\n80% CI for σ2\n= [(n - 1)s2\n,\n56.37\n(n - 1)s2\n]\n32.49\n= [44 ⋅ 16\n56.37 , 44 ⋅ 16\n32.49 ]\n(b) Solution: The 80% percentile bootstrap CI is [c∗\n0.9], where c∗\n0.9 are em\n0.1, c∗\n0.1 and c∗\npirical quantiles for x∗\n\nPractice Final, Spring 2022\nc∗\n=\n(Really it is interporlated between the 49th and 50th\n0.1 is the 50th element\n4.800.\nelement, fortunately both are 4.800)\nc∗\n0.9 is the 450th element = 5.169. (Really it is interporlated between the 449th and 450th\nelement, fortunately both are 5.169)\nSo the 80% CI = [4.800, 5.169].\n(c) Solution: The approach in (b) is fine since it makes no assumptions about the under\nx - μ\nlying distribution. The approach in (a) is more problematic since s/√n does not follow a\nStudent-t distribution. However for an exponential distribution and n = 45 the approxima\ntion is not too bad.\n(d) Solution: Method (b) is preferable if the sample mean x is not drawn from a normal\ndistribution.\nThe 150 smallest and 150 largest values of x∗ for taxi problem are given in the following table.\n1- 10\n4.466\n4.506\n4.509\n4.515\n4.578\n4.597\n4.618\n4.635\n4.653\n4.664\n11- 20\n4.670\n4.672\n4.685\n4.696\n4.703\n4.707\n4.713\n4.721\n4.727\n4.727\n21- 30\n4.729\n4.731\n4.738\n4.738\n4.740\n4.743\n4.744\n4.745\n4.751\n4.752\n31- 40\n4.759\n4.760\n4.768\n4.774\n4.775\n4.777\n4.778\n4.780\n4.784\n4.784\n41- 50\n4.787\n4.789\n4.789\n4.790\n4.791\n4.791\n4.792\n4.796\n4.800\n4.800\n51- 60\n4.800\n4.802\n4.805\n4.807\n4.808\n4.808\n4.811\n4.812\n4.812\n4.817\n61- 70\n4.818\n4.818\n4.819\n4.821\n4.821\n4.822\n4.824\n4.825\n4.826\n4.830\n71- 80\n4.830\n4.834\n4.836\n4.837\n4.837\n4.838\n4.838\n4.840\n4.840\n4.841\n81- 90\n4.841\n4.841\n4.842\n4.843\n4.844\n4.844\n4.845\n4.845\n4.846\n4.846\n91- 100\n4.847\n4.848\n4.849\n4.849\n4.850\n4.852\n4.852\n4.854\n4.855\n4.855\n101- 110\n4.856\n4.858\n4.858\n4.858\n4.862\n4.863\n4.865\n4.865\n4.866\n4.866\n111- 120\n4.867\n4.869\n4.871\n4.872\n4.876\n4.876\n4.876\n4.877\n4.877\n4.881\n121- 130\n4.882\n4.886\n4.886\n4.886\n4.888\n4.889\n4.891\n4.892\n4.892\n4.893\n131- 140\n4.895\n4.897\n4.897\n4.897\n4.898\n4.899\n4.901\n4.902\n4.902\n4.903\n141- 150\n4.903\n4.904\n4.905\n4.905\n4.905\n4.907\n4.907\n4.907\n4.907\n4.907\n351-360\n5.073\n5.074\n5.075\n5.075\n5.077\n5.077\n5.077\n5.077\n5.078\n5.079\n361-370\n5.079\n5.079\n5.080\n5.081\n5.081\n5.082\n5.083\n5.084\n5.085\n5.085\n371-380\n5.087\n5.087\n5.088\n5.091\n5.091\n5.091\n5.092\n5.092\n5.093\n5.093\n381-390\n5.094\n5.094\n5.096\n5.097\n5.100\n5.100\n5.101\n5.101\n5.102\n5.103\n391-400\n5.104\n5.104\n5.106\n5.106\n5.108\n5.108\n5.108\n5.108\n5.108\n5.110\n401-410\n5.110\n5.111\n5.112\n5.112\n5.112\n5.112\n5.113\n5.114\n5.114\n5.115\n411-420\n5.118\n5.122\n5.122\n5.123\n5.127\n5.129\n5.129\n5.132\n5.134\n5.134\n421-430\n5.134\n5.135\n5.136\n5.136\n5.137\n5.140\n5.141\n5.142\n5.142\n5.143\n431-440\n5.143\n5.145\n5.146\n5.147\n5.147\n5.148\n5.151\n5.151\n5.154\n5.155\n441-450\n5.156\n5.162\n5.163\n5.164\n5.164\n5.165\n5.166\n5.168\n5.169\n5.169\n451-460\n5.170\n5.172\n5.172\n5.175\n5.178\n5.179\n5.180\n5.181\n5.182\n5.182\n461-470\n5.182\n5.186\n5.195\n5.202\n5.202\n5.205\n5.206\n5.210\n5.216\n5.219\n471-480\n5.220\n5.220\n5.221\n5.222\n5.224\n5.225\n5.232\n5.232\n5.236\n5.236\n481-490\n5.243\n5.244\n5.245\n5.251\n5.253\n5.258\n5.261\n5.263\n5.266\n5.273\n491-500\n5.274\n5.288\n5.288\n5.291\n5.307\n5.312\n5.314\n5.316\n5.348\n5.488\n\nPractice Final, Spring 2022\nProblem 29. Note. In this problem the geometric(p) distribution is defined as the total\nnumber of trials to the first failure (the value includes the failure), where p is the probabilitiy\nof success.\n(a) What sample statistic would you use to estimate p?\n(b) Describe how you would use the parametric bootstrap to estimate a 95% basic confidence\ninterval for p. You can be brief, but you should give careful step-by-step instructions.\n(a) Solution: Since μ = 1/(1 -p), so p = 1 -1/μ, we should use the approximation\np = 1 -1/x .\n(b) Solution: Step 1. Approximate p by p = 1 -1/x.\nStep 2. Generate a bootstrap sample x∗\n1, ... , xn\n∗ from geom(p).\nStep 3. Compute p∗ = 1 -1/x∗ and δ∗ = p∗-p.\nRepeat steps 2 and 3 many times (say 104 times.\nStep 4. List all the δ∗ and find the critical values.\nLet δ∗\n= 0.025 critical value = 0.975 quantile.\n0.025\nLet δ∗\n= 0.975 critical value = 0.025 quantile.\n0.975\nStep 5. The basic bootstrap confidence interval is [p-δ∗\np-δ∗\n].\n\n0.025,\n0.975\nProblem 30. You independently draw 100 data points from a normal distribution.\n(a) Suppose you know the distribution is N(μ, 4) (4 = σ2) and you want to test the null\nhypothesis H0 ∶μ= 3 against the alternative hypothesis HA ∶μ=3.\nIf you want a significance level of α = 0.05. What is your rejection region?\nYou must clearly state what test statistic you are using.\n(b) Suppose the 100 data points have sample mean 5. What is the p-value for this data?\nShould you reject H0?\n(c) Determine the power of the test using the alternative HA ∶μ= 4.\n(a) Solution: We will use the standardized mean based on H0 as a test statistic:\nx-μ0\nx-3\nz=\n=\n= 5(x-3).\nσ/√n\n2/10\nAt α = 0.05 we reject H0 if\nz < z0.975 = -1.96 or z > z0.025 = 1.96.\n(Or we could have used x as a test statistic and got the corresponding rejection region.)\n5-3\n(b) Solution: With this data we have z= 2/10 = 10. The rejection region is two sided so\np = P(|Z| > |z|) = P(|Z| > 10) = 0.\nYes, since p < α you should reject H0.\n(c) Solution: Power = P(reject | μ= 4)\n\nPractice Final, Spring 2022\nx - 3\nOur z-statistic is z = 2/10 and we don't reject if\n-1.96 ≤ z ≤ 1.96\n⇔\n2/10 ≤ 1.96\n2.61 ≤ x ≤ 3.39\n-1.96 ≤ x-3\n⇔\nSo,\nPower = P(reject | μ= 4)\n= 1 -P(don't reject | μ= 4)\n= 1 -P(2.61 < x< 3.39 | μ= 4)\nWe standardize using the given mean μ = 4\n= 1 -P(2.61 -4 < Z< -0.61\n2/10\n2/10 )\n= 1 - P (-6.9 < Z < -3.05)\n= 1 - Φ(-3.05) + Φ(-6.9)\n= 1 -0.0011 + 0 = 0.9989 .\nThe probabilities were looked up in the z-table. We used Φ(-6.9) ≈ 0.\n(We could have used much less calculation to find that the non-rejection range is x between\n-7σx and -3σx from the mean μ = 4.)\nProblem 31. Suppose that you have molecular type with unknown atomic mass θ. You\nhave an atomic scale with normally-distributed error of mean 0 and variance 0.5.\n(a) Suppose your prior on the atomic mass is N(80, 4). If the scale reads 85, what is your\nposterior pdf for the atomic mass?\n(b) With the same prior as in part (a), compute the smallest number of measurements\nneeded so that the posterior variance is less than 0.01.\n(a) Solution: This is a normal/normal conjugate prior/likilihood update.\nHypothesis\nPrior\nLikelihood\nPosterior\nθ\nN(80, 4)\nf(x|θ) ∼ N(θ, 0.5)\n, σ2\n)\nN(μpost\npost\nWe have\na=\n=\nb=\n=\n4,\nσ2\nσ2\n0.5 = 2.\nprior\nFor the update\naμprior + bx\n=\nμpost\na + b\n80/4 + 170\n=\n=\n≈ 84.44\n1/4 + 2\nσ2\n=\npost\na + b\n=\n4 ≈ 0.4444\n1/4 + 2 =\n\nPractice Final, Spring 2022\nSo, the posterior is\nf(θ | x = 84) ∼ N(μpost, σ2\n) = N(84.44, 0.4444)\npost\n(b) Solution: In this case a = 1/4, b = n/0.5 = 2n. We know\nσ2\n=\n=\npost\na+ b\n1/4 + 2n = 8n+ 1\nNow σ2\n≤ 0.01 gives us\npost\n⇒\n400 ≤8n+ 1\n⇒\n≤n Solution: n = 50 .\n8n+ 1 ≤0.01\nProblem 32. Your friend grabs a die at random from a drawer containing two 6-sided\ndice, one 8-sided die, and one 12-sided die. She rolls the die once and reports that the result\nis 7.\n(a) Make a discrete Bayes table showing the prior, likelihood, and posterior for the type of\ndie rolled given the data.\n(b) What are your posterior odds that the die has 12 sides?\n(c) Given the data of the first roll, what is your probability that the next roll will be a 7?\n(a) Solution: Let θ represent the number of sides to the die. The data is x1 = 7\nHypothesis\nθ\nprior\np(θ)\nlikelihood\np(x1 = 7 | θ)\nBayes numer.\np(θ)p(x1 = 7 | θ)\nposterior\np(θ)p(x1 = 7 | θ)\np(θ | x1 = 7) =\np(x1 = 7)\nθ = 6\n1/2\nθ = 8\n1/4\n1/8\n1/32\n3/5\nθ = 12\n1/4\n1/12\n1/48\n2/5\np(θ = 12 | x1 = 7)\n2/5\n(b) Solution: Odds =\n3 .\np(θ = 12 | x1 = 7) = 3/5 =\n(c) We extend the table in order to compute the posterior predictive probability.\nθ\np(θ | x1 = 7)\np(x2 = 7 | θ)\np(θ | x1 = 7)p(x2 = 7 | θ)\nθ = 6\nθ = 8\n3/5\n1/8\n3/40\nθ = 12\n2/5\n1/12\n2/60\nTotal\nThe total probability p(x2 = 7 | x1 = 7) =\n13/120\n120 .\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Exam 1 Review: All Questions: Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_prac_exam01_all_sol.pdf",
      "content": "Exam 1 Practice Exam 1: Long List -solutions, 18.05, Spring\nThis is a big list of practice problems for Exam 1. It includes all the problems\nin other sets of practice problems and many more!\n1 Counting and Probability\nProblem 1. A full house in poker is a hand where three cards share one rank and two cards\nshare another rank. How many ways are there to get a full-house? What is the probability\nof getting a full-house?\nSolution: We build a full-house in stages and count the number of ways to make each\nstage:\nStage 1. Choose the rank of the pair: (13\n1 ).\nStage 2. Choose the pair from that rank, i.e. pick 2 of 4 cards: (4\n2).\nStage 3. Choose the rank of the triple (from the remaining 12 ranks): (12\n1 ).\nStage 4. Choose the triple from that rank: (4\n3).\nNumber of ways to get a full-house: (13\n1 )(4\n2)(12\n1 )(4\n3)\nNumber of ways to pick any 5 cards out of 52: (52\n5 )\n(13\n2)(12\n1 )(4\n1 )(4\n3)\nProbability of a full house:\n≈ 0.00144\n(52\n5 )\nProblem 2. There are 3 arrangements of the word DAD, namely DAD, ADD, and DDA.\nHow many arrangements are there of the word PROBABILITY?\nSolution: Sort the letters: A BB II L O P R T Y. There are 11 letters in all. We build\narrangements by starting with 11 'slots' and placing the letters in these slots, e.g\nA B I B I L O P R T Y\nCreate an arrangement in stages and count the number of possibilities at each stage:\nStage 1: Choose one of the 11 slots to put the A: (11\n1 )\nStage 2: Choose two of the remaining 10 slots to put the B's: (10\n2 )\nStage 3: Choose two of the remaining 8 slots to put the I's: (8\n2)\nStage 4: Choose one of the remaining 6 slots to put the L: (6\n1)\n\nPractice Exam 1: All Questions, Spring 2022\nStage 5: Choose one of the remaining 5 slots to put the O: (5\n1)\nStage 6: Choose one of the remaining 4 slots to put the P: (4\n1)\nStage 7: Choose one of the remaining 3 slots to put the R: (3\n1)\nStage 8: Choose one of the remaining 2 slots to put the T: (2\n1)\nStage 9: Use the last slot for the Y: (1\n1)\nNumber of arrangements:\n(11\n1 )(10\n10 ⋅9 8 ⋅7\n2 )(8\n2)(6\n1)(1\n5)(4\n1)(1\n3)(1\n2)(1\n1) = 11 ⋅\n⋅\n⋅ 6 ⋅ 5 ⋅ 4 ⋅ 3 ⋅ 2 ⋅ 1 = 9979200\nNote: choosing 11 out of 1 is so simple we could have immediately written 11 instead of\nbelaboring the issue by writing (11\n1 ). We wrote it this way to show one systematic way to\nthink about problems like this.\nProblem 3. (a) How many ways can you arrange the letters in the word STATISTICS?\n(e.g. SSSTTTIIAC counts a one arrangement.)\n(b) If all arrangements are equally likely, what is the probabilitiy the two 'i's are next to\neach other.\nSolution: (a) Create an arrangement in stages and count the number of possibilities at\neach stage:\nStage 1: Choose three of the 10 slots to put the S's: (10\n3 )\nStage 2: Choose three of the remaining 7 slots to put the T's: (7\n3)\nStage 3: Choose two of the remaining 4 slots to put the I's: (4\n2)\nStage 4: Choose one of the remaining 2 slots to put the A: (2\n1)\nStage 5: Use the last slot for the C: (1\n1)\nNumber of arrangements:\n(10\n3 )(7\n3)(2\n4)(2\n1)(1\n1) = 50400.\n(b) The are (10\n2 ) = 45 equally likely ways to place the two I's.\nThere are 9 ways to place them next to each other, i.e. in slots 1 and 2, slots 2 and 3, ...,\nslots 9 and 10.\nSo the probability the I's are adjacent is 9/45 = 0.2.\n\nPractice Exam 1: All Questions, Spring 2022\nProblem 4. In a ballroom dancing class the students are divided into group A and group\nB. There are six people in group A and seven in group B. If four As and four Bs are\nchosen and paired off, how many pairings are possible?\nSolution: Build the pairings in stages and count the ways to build each stage:\nStage 1: Choose the 4 from group A: (6\n4).\nStage 2: Choose the 4 from group B: (7\n4)\nWe need to be careful because we don't want to build the same 4 couples in multiple ways.\nLine up the 4 A's A1, A2, A3, A4\nStage 3: Choose a partner from the 4 Bs for A1: 4.\nStage 4: Choose a partner from the remaining 3 Bs for A2: 3\nStage 5: Choose a partner from the remaining 2 Bs for A3: 2\nStage 6: Pair the last B with A4: 1\nNumber of possible pairings: (6\n4)(7\n4)4!.\nNote: we could have done stages 3-6 in one go as: Stages 3-6: Arrange the 4 Bs opposite\nthe 4 As: 4! ways.\nProblem 5. Suppose you pick two cards from a deck of 52 playing cards. What is the\nprobability that they are both queens?\nSolution: Using choices (order doesn't matter):\nNumber of ways to pick 2 queens: (4\n2). Number of ways to pick 2 cards: (52\n2 ).\n(4\n2)\nAll choices of 2 cards are equally likely. So, probability of 2 queens =\n(52\n2 )\nUsing permutations (order matters):\nNumber of ways to pick the first queen: 4. No. of ways to pick the second queen: 3.\nNumber of ways to pick the first card: 52. No. of ways to pick the second card: 51.\n4⋅3\nAll arrangements of 2 cards are equally likely. So, probability of 2 queens: 52⋅51 .\nProblem 6. Suppose that there are ten students in a classroom. What is the probability\nthat no two of them have a birthday in the same month?\nSolution: We assume each month is equally likely to be a student's birthday month.\nNumber of ways ten students can have birthdays in 10 different months:\n12!\n12 ⋅11 ⋅10 ... ⋅3 = 2!\nNumber of ways 10 students can have birthday months: 1210.\n12!\nProbability no two share a birthday month:\n= 0.00387.\n2! 1210\nProblem 7. 20 politicians are having a tea party, 6 Democrats and 14 Republicans. To\nprepare, they need to choose:\n\n⋅\n⋅\n⋅\n⋅\n⋅\n⋅\n⋅\n⋅\n⋅\nPractice Exam 1: All Questions, Spring 2022\n3 people to set the table, 2 people to boil the water, 6 people to make the scones.\nEach person can only do 1 task. (Note that this doesn't add up to 20. The rest of the people\ndon't help.)\n(a) In how many different ways can they choose which people perform these tasks?\n(b) Suppose that the Democrats all hate tea. If they only give tea to 10 of the 20 people,\nwhat is the probability that they only give tea to Republicans?\n(c) If they only give tea to 10 of the 20 people, what is the probability that they give tea to\n9 Republicans and 1 Democrat?\nSolution: (a) There are (20\n3 ) ways to choose the 3 people to set the table, then (17\n2 ) ways\nto choose the 2 people to boil water, and (15\n6 ) ways to choose the people to make scones.\nSo the total number of ways to choose people for these tasks is\n(20\n20!\n17!\n15!\n20!\n3 )(17\n2 )(15\n=\n⋅\n⋅\n=\n= 775975200.\n6 )\n3! 17! 2! 15! 6! 9!\n3! 2! 6! 9!\n(b) The number of ways to choose 10 of the 20 people is (20\n10) The number of ways to choose\n10 people from the 14 Republicans is (14\nSo the probability that you only choose 10\n10).\nRepublicans is\n(14\n14!\n10)\n10! 4! ≈ 0.00542\n(20\n20!\n10) =\n10! 10!\nAlternatively, you could choose the 10 people in sequence and say that there is a 14/20\nprobability that the first person is a Republican, then a 13/19 probability that the second\none is, a 12/18 probability that third one is, etc. This gives a probability of\n17 16\n11.\n(You can check that this is the same as the other answer given above.)\n(c) You can choose 1 Democrat in (6\n1) = 6 ways, and you can choose 9 Republicans in (14\n9 )\nways, so the probability equals\n(14\n14!\n6 ⋅\n9 ) = 6 ⋅ 9! 5! = 6 ⋅14! 10! 10! .\n(20\n20!\n9! 5! 20!\n10)\n10! 10!\nProblem 8. Let A and B be two events. Suppose the probability that neither A or B occurs\nis 2/3. What is the probability that one or both occur?\nSolution: We are given P(Ac ∩ Bc) = 2/3 and asked to find P(A ∪ B).\nAc∩Bc= (A∪B)c ⇒P(A∪B) = 1 -P(Ac∩Bc) = 1/3.\nProblem 9. Let C and D be two events with P(C) = 0.25, P (D) = 0.45, and P(C ∩D) =\n0.1. What is P(Cc ∩D)?\nSolution: D is the disjoint union of D ∩ C and D ∩ Cc.\n\nPractice Exam 1: All Questions, Spring 2022\n0.35.\nSo, P(D ∩ C) + P(D ∩ Cc) = P(D)\n⇒ P(D ∩ Cc) = P(D) -P(D ∩ C) = 0.45 -0.1 =\n(We never use P(C) = 0.25.)\n0.1\nD∩Cc\n0.45-0.1\nD\nC\nProblem 10. You roll a four-sided die 3 times. For this problem we'll use the sample\nspace with 64 equally likely outcomes.\n(a) Write down this sample space in set notation.\n(b) List all the outcomes in each of the following events.\n(i) A = 'Exactly 2 of the 3 rolls are fours'\n(ii) B = 'At least 2 of the 3 rolls are fours'\n(iii) C = 'Exactly 1 of the second and third rolls is a 4'\n(iv) A ∩ C\nSolution: (a) Writing all 64 possibilities is too tedius. Here's a more compact representa\ntion\n{(i, j, k) | i, j, k are integers from 1 to 4}\n(b) (i) Here we'll just list all 9 possibilities\n{(4,4,1), (4,4,2), (4,4,3), (4,1,4), (4,2,4), (4,3,4), (1,4,4), (2,4,4), (3,4,4)}\n(ii) This is the same as (i) with the addition of (4,4,4).\n{ (4,4,1), (4,4,2), (4,4,3), (4,1,4), (4,2,4), (4,3,4), (1,4,4), (2,4,4), (3,4,4), (4,4,4)}\n(iii) This is list is a little longer. If we're systematic about it we can still just write it out.\n{(1,4,1),\n(2,4,1),\n(3,4,1),\n(4,4,1),\n(1,4,2),\n(2,4,2),\n(3,4,2),\n(4,4,2),\n(1,4,3),\n(2,4,3),\n(3,4,3),\n(4,4,3),\n(1,1,4),\n(2,1,4),\n(3,1,4),\n(4,1,4),\n(1,2,4),\n(2,2,4),\n(3,2,4),\n(4,2,4),\n(1,3,4),\n(2,3,4),\n(3,3,4),\n(4,3,4)}\n(iv) {(4,4,1), (4,4,2), (4,4,3), (4,1,4), (4,2,4), (4,3,4)}\nProblem 11. Suppose we have 8 teams labeled T1, ..., T8. Suppose they are ordered by\nplacing their names in a hat and drawing the names out one at a time.\n(a) How many ways can it happen that all the odd numbered teams are in the odd numbered\nslots and all the even numbered teams are in the even numbered slots?\nSolution: Slots 1, 3, 5, 7 are filled by T1, T3, T5, T7 in any order: 4! ways.\nSlots 2, 4, 6, 8 are filled by T2, T4, T6, T8 in any order: 4! ways.\nSolution: 4! ⋅4! = 576.\n(b) What is the probability of this happening?\nSolution: There are 8! ways to fill the 8 slots in any way.\n\nPractice Exam 1: All Questions, Spring 2022\n4! ⋅4!\nSince each outcome is equally likely the probabilitiy is\n= 40320 = 0.143 = 1.43%.\n8!\n2 Conditional Probability and Bayes' Theorem\nProblem 12. More cards! Suppose you want to divide a 52 card deck into four hands with\n13 cards each. What is the probability that each hand has a king?\nSolution: Let Hi be the event that the ith hand has one king. We have the conditional\nprobabilities\n(4\n1)(48\n(3\n1)(36\n(2\n1)(24\n12)\n12)\n12)\nP (H1) =\n;\nP (H2|H1) =\n;\nP (H3|H1 ∩ H2) =\n(52\n(39\n(26\n13)\n13)\n13)\nP (H4|H1 ∩ H2 ∩ H3) = 1\nP (H1 ∩ H2 ∩ H3 ∩ H4) = P (H4|H1 ∩ H2 ∩ H3) P (H3|H1 ∩ H2) P (H2|H1) P (H1)\n(2\n1)(24\n1)(36\n1)(48\n12)(3\n12)(4\n12)\n=\n.\n(26\n13)(39\n13)(52\n13)\nProblem 13. Suppose you are taking a multiple-choice test with c choices for each question.\nIn answering a question on this test, the probability that you know the answer is p. If you\ndon't know the answer, you choose one at random. What is the probability that you knew\nthe answer to a question, given that you answered it correctly?\nSolution: The following tree shows the setting\nGuess\nKnow\nCorrect\nWrong\nCorrect\nWrong\np\n1 -p\n1/c\n1 -1/c\nLet C be the event that you answer the question correctly. Let K be the event that you\nactually know the answer. The left circled node shows P(K∩C) = p. Both circled nodes\ntogether show P(C) = p + (1 -p)/c. So,\nP(K∩C)\np\nP(K|C) =\n=\nP(C)\np+ (1 -p)/c\nOr we could use the algebraic form of Bayes' theorem and the law of total probability: Let\nG stand for the event that you're guessing. Then we have,\nP(C|K) = 1, P(K) = p, P(C) = P(C|K)P(K) + P(C|G)P(G) = p+ (1 -p)/c. So,\nP(C|K)P(K)\np\nP(K|C) =\n=\nP(C)\np+ (1 -p)/c\n\nPractice Exam 1: All Questions, Spring 2022\nProblem 14. Corrupted by their power, the judges running the popular game show Amer\nica's Next Top Mathematician have been taking bribes from many of the contestants. Each\nepisode, a given contestant is either allowed to stay on the show or is kicked off.\nIf the contestant has been bribing the judges they will be allowed to stay with probability 1.\nIf the contestant has not been bribing the judges, they will be allowed to stay with probability\n1/3.\nSuppose that 1/4 of the contestants have been bribing the judges. The same contestants\nbribe the judges in both rounds, i.e., if a contestant bribes them in the first round, they bribe\nthem in the second round too (and vice versa).\n(a) If you pick a random contestant who was allowed to stay during the first episode, what\nis the probability that they were bribing the judges?\n(b) If you pick a random contestant, what is the probability that they are allowed to stay\nduring both of the first two episodes?\n(c) If you pick random contestant who was allowed to stay during the first episode, what is\nthe probability that they get kicked off during the second episode?\nSolution: The following tree shows the setting. Stay1 means the contestant was allowed\nto stay during the first episode and stay2 means the they were allowed to stay during the\nsecond.\nHonest\nBribe\nStay1\nLeave1\nStay1\nLeave1\nStay2\nLeave2\nStay2\nLeave2\n1/4\n3/4\n1/3\n2/3\n1/3\n2/3\nLet's name the relevant events:\nB = the contestant is bribing the judges\nH = the contestant is honest (not bribing the judges)\nS1 = the contestant was allowed to stay during the first episode\nS2 = the contestant was allowed to stay during the second episode\nL1 = the contestant was asked to leave during the first episode\nL2 = the contestant was asked to leave during the second episode\n(a) We first compute P (S1) using the law of total probability.\nP(S1) = P(S1|B)P(B) + P(S1|H)P(H) = 1 ⋅4 + 1 ⋅4 = 2.\nWe therefore have (by Bayes' rule) P(B|S1) = P(S1|B) P(B)\n⋅1/4\n1.\nP(S1) = 1 1/2 =\n(b) Using the tree we have the total probability of S2 is\nP(S2) = 4 + 3 ⋅\n⋅\n4 3 3 = 3\n\nPractice Exam 1: All Questions, Spring 2022\nP (L2 ∩ S1)\n(c) We want to compute P (L2|S1) =\n.\nP (S1)\nFrom the calculation we did in part (a), P (S1) = 1/2. For the numerator, we have (see the\ntree)\nP(L2 ∩ S1) = P(L2 ∩ S1|B)P(B) + P(L2 ∩ S1|H)P(H) = 0 ⋅4 + 2 ⋅ 4 =\n1/6\nTherefore P (L2|S1) = 1/2 = 3.\nProblem 15. Consider the Monty Hall problem. Let's label the door with the car behind\nit a and the other two doors b and c. In the game the contestant chooses a door and then\nMonty chooses a door, so we can label each outcome as 'contestant followed by Monty', e.g\nab means the contestant chose a and Monty chose b.\n(a) Make a 3 × 3 probability table showing probabilities for all possible outcomes.\n(b) Make a probability tree showing all possible outcomes.\n(c) Suppose the contestant's strategy is to switch. List all the outcomes in the event 'the\ncontestant wins a car'. What is the probability the contestant wins?\n(d) Redo part (c) with the strategy of not switching.\nSolution: (a) and (b) In the tree the first row is the contestant's choice and the second\nrow is the host's (Monty's) choice.\nViewed as a table\nViewed as a tree\nContestant\na\nb\nc\na\nHost\nb\n1/6\n1/3\nc\n1/6\n1/3\na\nb\nc\n0 1\n1 0\na\nc\na\nc\na\nc\nb\nb\nb\n(b) With this strategy the contestant wins with {bc, cb}. The probability of winning is\nP (bc) + P (cb) = 2/3. (Both the tree and the table show this.)\n(c) {ab, ac}, probability = 1/3.\nProblem 16. Two dice are rolled.\nA = 'sum of two dice equals 3'\nB = 'sum of two dice equals 7'\nC = 'at least one of the dice shows a 1'\n(a) What is P (A|C)?\n(b) What is P (B|C)?\n(c) Are A and C independent? What about B and C?\nSolution: Sample space =\nΩ = {(1, 1), (1, 2), (1, 3), ... , (6, 6) } = {(i, j) | i, j = 1, 2, 3, 4, 5, 6 }.\n\nPractice Exam 1: All Questions, Spring 2022\n(Each outcome is equally likely, with probability 1/36.)\nA = {(1, 2), (2, 1)},\nB = {(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)}\nC = {(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)}\nP(A ∩ C)\n2/36\n(a) P(A|C) =\n=\nP(C)\n11/36 = 11..\nP(B ∩ C)\n2/36\n(b) P(B|C) =\n=\nP(C)\n11/36 = 11..\n(c) P(A) = 2/36 = P(A|C), so they are not independent. Similarly, P (B) = 6/36 =\nP (B|C), so they are not independent.\nProblem 17. There is a screening test for prostate cancer that looks at the level of PSA\n(prostate-specific antigen) in the blood. There are a number of reasons besides prostate\ncancer that a man can have elevated PSA levels. In addition, many types of prostate cancer\ndevelop so slowly that that they are never a problem. Unfortunately there is currently no\ntest to distinguish the different types and using the test is controversial because it is hard to\nquantify the accuracy rates and the harm done by false positives.\nFor this problem we'll call a positive test a true positive if it catches a dangerous type of\nprostate cancer. We'll assume the following numbers:\nRate of prostate cancer among men over 50 = 0.0005\nTrue positive rate for the test = 0.9\nFalse positive rate for the test = 0.01\nLet T be the event a man has a positive test and let D be the event a man has a dangerous\ntype of the disease. Find P (D|T ) and P (D|T c).\nSolution: You should write this out in a tree! (For example, see the solution to the next\nproblem.)\nWe compute all the pieces needed to apply Bayes' rule. We're given\nP (T |D) = 0.9 ⇒ P (T c|D) = 0.1,\nP(T|Dc) = 0.01 ⇒ P(T c|Dc) = 0.99.\nP(D) = 0.0005 ⇒ P(Dc) = 1 -P(D) = 0.9995.\nWe use the law of total probability to compute P (T ):\nP(T) = P(T|D) P(D) + P(T|Dc) P(Dc) = 0.9 ⋅0.0005 + 0.01 ⋅0.9995 = 0.010445\nNow we can use Bayes' rule to answer the questions:\nP (T |D) P (D)\n0.9 × 0.0005\nP (D|T ) =\n=\n= 0.043\nP (T )\n0.010445\nP(T c|D) P(D)\n0.1 × 0.0005\nP(D|T c) =\n=\n= 5.0 × 10-5\nP (T c)\n0.989555\nProblem 18. A multiple choice exam has 4 choices for each question. A student has\nstudied enough so that the probability they will know the answer to a question is 0.5, the\nprobability that they will be able to eliminate one choice is 0.25, otherwise all 4 choices seem\n\nPractice Exam 1: All Questions, Spring 2022\nequally plausible. If they know the answer they will get the question right. If not they have\nto guess from the 3 or 4 choices.\nAs the teacher you want the test to measure what the student knows. If the student answers\na question correctly what's the probability they knew the answer?\nSolution: We show the probabilities in a tree:\nKnow\nEliminate 1\nTotal guess\nCorrect\nWrong\nCorrect\nWrong\nCorrect\nWrong\n1/2\n1/4\n1/4\n1/3\n2/3\n1/4\n3/4\nFor a given problem let C be the event the student gets the problem correct and K the\nevent the student knows the answer.\nThe question asks for P (K|C).\nWe'll compute this using Bayes' rule:\nP(C|K) P(K)\n1 ⋅1/2\nP(K|C) =\n=\n= 31 ≈ 0.774 = 77.4%\nP(C)\n1/2 + 1/12 + 1/16\nProblem 19. Suppose you have an urn containing 7 red and 3 blue balls. You draw three\nballs at random. On each draw, if the ball is red you set it aside and if the ball is blue you\nput it back in the urn. What is the probability that the third draw is blue?\n(If you get a blue ball it counts as a draw even though you put it back in the urn.)\nSolution: Here is the game tree, R1 means red on the first draw etc.\nR1\nB1\nR2\nB2\nR2\nB2\nR3\nB3 R3\nB3 R3\nB3 R3\nB3\n7/10\n3/10\n6/9\n3/9\n7/10\n3/10\n5/8\n3/8\n6/9\n3/9\n6/9\n3/9\n7/10\n3/10\nSumming the probability to all the B3 nodes we get\nP(B3) =\n⋅\n⋅8 + 7 ⋅\n⋅9 + 3 ⋅\n⋅9 + 3 ⋅\n⋅10 = 0.350.\nProblem 20. Some games, like tennis or ping pong, reach a state called deuce. This means\nthat the score is tied and a player wins the game when they get two points ahead of the other\nplayer. Suppose the probability that you win a point is p and this is true independently for\nall points. If the game is at deuce what is the probability you win the game?\nThis is a tricky problem, but amusing if you like puzzles.\nSolution: Let W be the event you win the game from deuce and L\n\ndeuce\nPractice Exam 1: All Questions, Spring 2022\n+1\n-1\nW\ndeuce\ndeuce\nL\np\n1-p\np\n1-p\np\n1-p\nw\nw\nthe event you lose. For convenience, define w = P(W).\nThe figure shows the complete game tree through 2 points. In the\nthird level we just abreviate by indicating the probability of winning\nfrom deuce.\nThe nodes marked +1 and -1, indicate whether you won or lost the\nW\nW\nfirst point.\nSumming all the paths to W we get\np2\nw= P(W) = p2 + p(1 -p)w+ (1 -p)pw = p2 + 2p(1 -p)w ⇒ w = 1 - 2p(1 - p).\nProblem 21. (Bayes formula)\nA student takes a multiple-choice exam. Suppose for each question they either know the\nanswer or gamble and choose an option at random. Further suppose that if they knows the\nanswer, the probability of a correct answer is 1, and if they gamble, this probability is 1/4.\nTo pass, students need to answer at least 60% of the questions correctly. The student has\n\"studied for a minimal pass,\" i.e., with probability 0.6 they know the answer to a question.\nFor a single question, given that they answers it correctly, what is the probability that they\nactually knew the answer?\nFor a given problem let C be the event the student gets the problem correct and K the\nevent the student knows the answer.\nThe question asks for P (K|C).\nP (C|K) P (K)\nWe'll compute this using Bayes' rule: P(K|C) =\n.\nP (C)\nWe're given:\nP(C|K) = 1, P(K) = 0.6.\nLaw of total prob.:\nP(C) = P(C|K) P(K) + P(C|Kc) P(Kc) = 1 ⋅0.6 + 0.25 ⋅0.4 = 0.7.\n0.6\nTherefore P(K|C) = 0.7 = 0.857 = 85.7%.\n3 Independence\nProblem 22. Suppose that P (A) = 0.4, P (B) = 0.3 and P((A∪B)C) = 0.42. Are A and\nB independent?\nSolution: We have P(A∪B) = 1 -0.42 = 0.58 and we know because of the inclusion-\nexclusion principle that\nP(A∪B) = P(A) + P(B) -P(A∩B).\nThus,\nP (A ∩ B) = P (A) + P (B) - P (A ∪ B) = 0.4 + 0.3 - 0.58 = 0.12 = (0.4)(0.3) = P (A)P (B).\nSo, A and B are independent.\n\nPractice Exam 1: All Questions, Spring 2022\nProblem 23. Suppose now that events A, B and C are mutually independent with\nP(A) = 0.3,\nP(B) = 0.4,\nP(C) = 0.5.\nCompute the following: (Hint: Use a Venn diagram)\n(i) P(A∩B ∩Cc)\n(ii) P(A∩Bc ∩C)\n(iii) P(Ac ∩B ∩C)\nSolution: By the mutual independence we have\nP(A∩B∩C) = P(A)P(B)P(C) = 0.06\nP(A∩B) = P(A)P(B) = 0.12\nP(A∩C) = P(A)P(C) = 0.15\nP(B∩C) = P(B)P(C) = 0.2\nWe show this in the following Venn diagram\n0.09\n0.14\n0.21\n0.06\n0.14\n0.09\n0.06\nA\nB\nC\nNote that, for instance, P (A ∩ B) is split into two pieces. One of the pieces is P (A ∩ B ∩ C)\nwhich we know and the other we compute as P(A∩B)-P(A∩B∩C) = 0.12-0.06 = 0.06.\nThe other intersections are similar.\nWe can read off the asked for probabilities from the diagram.\n(i) P(A∩B ∩Cc) = 0.06\n(ii) P(A∩Bc ∩C) = 0.09\n(iii) P(Ac ∩B ∩C) = 0.14.\nProblem 24. You roll a twenty-sided die. Determine whether the following pairs of events\nare independent.\n(a) 'You roll an even number' and 'You roll a number less than or equal to 10'.\n(b) 'You roll an even number' and 'You roll a prime number'.\nSolution: E = even numbered = {2, 4, 6, 8, 10, 12, 14, 16, 18, 20}.\nL = roll ≤ 10 = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}.\nB = roll is prime = {2, 3, 5, 7, 11, 13, 17, 19} (We use B because P is not a good choice.)\n(a) P(E) = 10/20, P(E|L) = 5/10. These are the same, so the events are independent.\n(b) P (E) = 10/20. P(E|B) = 1/8. These are not the same so the events are not indepen\ndent.\nProblem 25. Suppose A and B are events with 0 < P(A) < 1 and 0 < P(B) < 1.\n\nPractice Exam 1: All Questions, Spring 2022\n(a) If A and B are disjoint can they be independent?\n(b) If A and B are independent can they be disjoint?\n(c) If A ⊂ B can they be independent?\nSolution: The answer to all three parts is 'No'. Each of these answers relies on the fact\nthat the probabilities of A and B are strictly between 0 and 1.\nTo show A and B are not independent we need to show either P(A ∩ B) =P(A) ⋅P(B)\nor P (A|B) = P (A).\n(a) No, they cannot be independent: A ∩ B = ∅ ⇒ P(A ∩ B) = 0 =P(A) ⋅P(B).\n(b) No, they cannot be disjoint: same reason as in part (a).\n(c) No, they cannot be independent: A⊂B ⇒ A∩B = A\n⇒ P(A ∩ B) = P(A) > P(A) ⋅P(B). The last inequality follows because P(B) < 1.\n4 Expectation and Variance\nProblem 26. Directly from the definitions of expected value and variance, compute E[X]\nand Var(X) when X has probability mass function given by the following table:\nX\n-2\n-1\npmf\n1/15\n2/15\n3/15\n4/15\n5/15\nSolution: We compute\nE[X] = -2 ⋅15 + -1 ⋅15 + 0 ⋅15 + 1 ⋅15 + 2 ⋅\n3.\n15 =\nThus\nVar(X) = E[(X - 2\n3)2]\n= (-2 - 2\n⋅15 + (-1 - 2\n⋅15 + (0 - 2\n⋅15 + (1 - 2\n⋅15 + (2 - 2\n⋅\n3)\n3)\n3)\n3)\n3)\n= 9 .\nProblem 27. Suppose that X takes values between 0 and 1 and has probability density\nfunction 2x. Compute Var(X) and Var(X2).\nSolution: We will make use of the formula Var(Y ) = E[Y 2] - E[Y ]2. First we compute\nE[X] = ∫\nx ⋅ 2xdx = 3\nE[X2] = ∫\nx2 ⋅ 2xdx = 2\nE[X4] = ∫ x4 ⋅ 2xdx = 3.\n\nPractice Exam 1: All Questions, Spring 2022\nThus,\nVar(X) = E[X2] - (E[X])\n2 = 2 - 4\n9 = 18\nand\nVar(X2) = E[X4] - (E[X2])\n2 = 3 - 1\n4 = 12.\nProblem 28. The random variable X takes values -1, 0, 1 with probabilities 1/8, 2/8, 5/8\nrespectively.\n(a) Compute E[X].\n(b) Give the pmf of Y = X2 and use it to compute E[Y ].\n(c) Instead, compute E[X2] directly from an extended table.\n(d) Compute Var(X).\n(a) Solution: We have\nX values:\n-1\nprob:\n1/8\n2/8\n5/8\nX2\nSo, E[X] = -1/8 + 5/8 = 1/2.\n(b) Solution: Y values:\nprob:\n2/8\n6/8\n⇒ E[Y ] = 6/8 = 3/4.\n(c) Solution: The change of variables formula just says to use the bottom row of the table\nin part (a): E[X2] = 1 ⋅(1/8) + 0 ⋅(2/8) + 1 ⋅(5/8) = 3/4 (same as part (b)).\n(d) Solution: Var(X) = E[X2] - E[X]2 = 3/4 - 1/4 = 1/2.\nProblem 29. Suppose X is a random variable with E[X] = 5 and Var(X) = 2. What is\nE[X2]?\nSolution: Use Var(X) = E[X2] - E[X]2 ⇒ 2 = E[X2] - 25 ⇒ E[X2] = 27.\nProblem 30. Compute the expectation and variance of a Bernoulli(p) random variable.\nSolution: Make a table:\nX:\nprob:\n(1-p)\np\nX2\n1.\nFrom the table, E[X] = 0 ⋅(1 -p) + 1 ⋅p = p.\nSince X and X2 have the same table E[X2] = E[X] = p.\nTherefore, Var(X) = p-p2 = p(1 -p).\nProblem 31. Suppose 100 people all toss a hat into a box and then proceed to randomly\npick out of a hat. What is the expected number of people to get their own hat back.\n\nPractice Exam 1: All Questions, Spring 2022\nHint: express the number of people who get their own hat as a sum of random variables\nwhose expected value is easy to compute.\nSolution: Let X be the number of people who get their own hat.\nFollowing the hint: let Xj represent whether person j gets their own hat. That is, Xj = 1\nif person j gets their hat and 0 if not.\nWe have, X = ∑Xj, so E[X] = ∑E[Xj].\nj=1\nj=1\nSince person j is equally likely to get any hat, we have P (Xj = 1) = 1/100. Thus, Xj ∼\nBernoulli(1/100) ⇒ E[Xj] = 1/100 ⇒ E[X] = 1.\nProblem 32. Suppose I play a gambling game with even odds. So, I can wager b dollars\nand I either win or lose b dollars with probability p = 0.5.\nI employ the following strategy to try to guarantee that I win some money.\nI bet $1; if I lose, I double my bet to $2, if I lose I double my bet again. I continue until\nI win. Eventually I'm sure to win a bet and net $1 (run through the first few rounds and\nyou'll see why this is the net).\nIf this really worked casinos would be out of business. Our goal in this problem is to\nunderstand the flaw in the strategy.\n(a) Let X be the amount of money bet on the last game (the one I win). X takes values 1,\n2, 4, 8, .... Determine the probability mass function for X. That is, find p(2k), where k is\nin {0, 1, 2, ...}.\nSolution: It is easy to see that (e.g. look at the probability tree) P (2k) = 2k+1 .\n(b) Compute E[X].\ninf\nSolution: E[X] = ∑2k\n= ∑1\n2 = inf. Technically, E[X] is undefined in this case.\n2k+1\nk=0\n(c) Use your answer in part (b) to explain why the stategy is a bad one.\nSolution: Technically, E[X] is undefined in this case. But the value of inf tells us what\nis wrong with the scheme. Since the average last bet is infinite, I need to have an infinite\namount of money in reserve.\nThis problem and solution is often referred to as the St. Petersburg paradox\nProblem 33. Suppose you roll a fair 6-sided die 100 times (independently), and you get\n$3 every time you roll a 6.\nLet X1 be the number of dollars you win on rolls 1 through 25.\nLet X2 be the number of dollars you win on rolls 26 through 50.\nLet X3 be the number of dollars you win on rolls 51 through 75.\nLet X4 be the number of dollars you win on rolls 76 throught 100.\nLet X = X1 + X2 + X3 + X4 be the total number of dollars you win over all 100 rolls.\n(a) What is the probability mass function of X?\n\nPractice Exam 1: All Questions, Spring 2022\n(b) What is the expectation and variance of X?\n(c) Let Y = 4X1. (So instead of rolling 100 times, you just roll 25 times and multiply your\nwinnings by 4.)\n(i) What are the expectation and variance of Y ?\n(ii) How do the expectation and variance of Y compare to those of X? (That is, are they\nbigger, smaller, or equal?) Explain (briefly) why this makes sense.\nSolution: (a) There are a number of ways to present this.\nLet T be the total number of times you roll a 6 in the 100 rolls. We know T ∼ Binomial(100, 1/6).\nSince you win $3 every time you roll a 6, we have X = 3T . So, we can write\nk\n100-k\nP(X = 3k) = (100\n6) (5\n6)\nfor k= 0, 1, 2, ... , 100.\nk) (1\n,\nAlternatively we could write\nx/3\n100-x/3\nP(X = x) = (100\n6)\n(5\n6)\n,\nfor x= 0, 3, 6, ... , 300.\nx/3) (1\n(b) E[X] = E[3T] = 3E[T] = 3 ⋅100 ⋅ 6\n1 = 50,\nVar(X) = Var(3T) = 9Var(T) = 9 ⋅100 ⋅ 1\n6 ⋅ 5\n6 = 125.\n(c) (i) Let T1 be the total number of times you roll a 6 in the first 25 rolls. So, X1 = 3T1\nand Y = 12T1.\nNow, T1 ∼ Binomial(25, 1/6), so\nE[Y] = 12E[T1] = 12 ⋅25 ⋅16 = 50.\nand\nVar(Y) = 144Var(T1) = 144 ⋅25 ⋅1 ⋅6\n5 = 500.\n(ii) The expectations are the same by linearity because X and Y are the both\n3 × 100 × a Bernoulli(1/6) random variable.\nFor the variance, Var(X) = 4Var(X1) because X is the sum of 4 independent variables all\nidentical to X1. However Var(Y) = Var(4X1) = 16Var(X1). So, the variance of Y is 4\ntimes that of X. This should make some intuitive sense because X is built out of more\nindependent trials than X1.\nAnother way of thinking about it is that the difference between Y and its expectation is\nfour times the difference between X1 and its expectation. However, the difference between\nX and its expectation is the sum of such a difference for X1, X2, X3, and X4. It's probably\nthe case that some of these deviations are positive and some are negative, so the absolute\nvalue of this difference for the sum is probably less than four times the absolute value of this\ndifference for one of the variables, i.e. the deviations are likely to cancel to some extent.\n5 Probability Mass Functions, Probability Density Functions\nand Cumulative Distribution Functions\nProblem 34. Suppose that X ∼ Bin(n, 0.5). Find the probability mass function of Y = 2X.\n\nPractice Exam 1: All Questions, Spring 2022\nSolution: For y = 0, 2, 4, ... , 2n,\nn\ny\n2) = ( n\nP(Y = y) = P(X =\ny/2) (2\n1) .\nProblem 35. (a) Suppose that X is uniform on [0, 1]. Compute the pdf and cdf of X.\n(b) If Y = 2X+ 5, compute the pdf and cdf of Y .\n(a) Solution: We have fX(x) = 1 for 0 ≤ x ≤ 1. The cdf of X is\nx\nx\nFX(x) = ∫ fX(t)dt = ∫ 1dt = x.\n(b) Solution: Since X is between 0 and 1 we have Y is between 5 and 7. Now for 5 ≤ y ≤ 7,\nwe have\n) = FX(y-5\ny-5\nFY(y) = P(Y≤y) = P(2X+ 5 ≤y) = P(X≤ y-5\n) =\n.\nDifferentiating P(Y ≤ y) with respect to y, we get the probability density function of Y ,\nfor 5 ≤ y ≤ 7,\nfY (y) = 2\n1.\nProblem 36. (a) Suppose that X has probability density function fX(x) = λe-λx for\nx ≥ 0. Compute the cdf, FX(x).\n(b) If Y = X2, compute the pdf and cdf of Y .\n(a) Solution: We have cdf of X,\nFX(x) = ∫\nx\nλe-λxdx = 1 - e-λx.\nNow for y ≥ 0, we have\n(b) Solution:\nFY(y) = P(Y ≤y) = P(X2 ≤y) = P(X≤ √y) = 1 - e-λ√y.\nDifferentiating FY (y) with respect to y, we have\nfY (y) = λ\n2 y- 1\n2 e-λ√y.\nProblem 37. Suppose that X is a random variable that takes on values 0, 2 and 3 with\nprobabilities 0.3, 0.1, 0.6 respectively. Let Y = 3(X-1)2.\n(a) What is the expectation of X?\n(b) What is the variance of X?\n(c) What is the expection of Y ?\n\nPractice Exam 1: All Questions, Spring 2022\n(d) Let FY (t) be the cumulative density function of Y . What is FY (7)?\n(a) Solution: We first make the probability tables\nX\nprob.\n0.3\n0.1\n0.6\nY\nSo, E[X] = 0 ⋅0.3 + 2 ⋅0.1 + 3 ⋅0.6 = 2\n(b) Solution: E[X2] = 0 ⋅0.3 + 4 ⋅0.1 + 9 ⋅0.6 = 5.8 ⇒ Var(X) = E[X2] - E[X]2 =\n5.8 -4 = 1.8.\n(c) Solution: E[Y] = 3 ⋅0.3 + 3 ⋅0.1 + 12 ⋅6 = 8.4.\n(d) Solution: From the table we see that FY(7) = P(Y ≤7) = 0.4.\nProblem 38. Let T be the waiting time for customers in a queue. Suppose that T is\nexponential with pdf f(t) = 2e-2t on [0, inf).\nFind the pdf of the rate at which customers are served R = 1/T .\nSolution: The CDF for T is\nt\nFT(t) = P(T ≤ t) = ∫ 2e-2u du = -e-2u∣\nt\n0 = 1 - e-2t.\nNext, we find the CDF of R. R takes values in (0, inf).\nFor 0 < r,\nFR(r) = P(R≤r) = P(1/T < r) = P(T > 1/r) = 1 -FT(1/r) = e-2/r.\nWe differentiate to get fR(r) = dr\nd (e-2/r) = r\n2 e-2/r.\nProblem 39. A continuous random variable X has PDF f(x) = x + ax2 on [0,1]\nFind a, the CDF and P(0.5 < X < 1).\nSolution: First we find the value of a:\n2 + a\n∫ f(x) dx = 1 = ∫ x+ ax2 dx = 1\n3 ⇒ a = 3/2.\nThe CDF is FX(x) = P(X ≤ x). We break this into cases:\n(i) b < 0, so FX(b) = 0.\n(ii) 0 ≤ b ≤ 1, so FX(b) = ∫0\nbx+ 3\n2x2 dx = b\n2 + b\n3 .\n(iii) 1 < x, so FX(b) = 1.\nUsing FX we get\nP(0.5 < X< 1) = FX(1) -FX(0.5) = 1 -(0.52 + 0.53\n) = 13\n16.\nProblem 40. (PMF of a sum)\nSuppose X and Y are independent and X ∼ Bernoulli(1/2) and Y ∼ Bernoulli(1/3).\nDetermine the pmf of X + Y\n\n{\n{\nPractice Exam 1: All Questions, Spring 2022\nSolution: First we'll give the joint probability table:\n2/3\n1/3\nY \\X\n1/3\n1/3\n1/6\n1/6\n1/2\n1/2\nWe'll use the joint probabilities to build the probability table for the sum.\nX + Y\n(X, Y )\n(0,0)\n(0,1), (1,0)\n(1,1)\nprob.\n1/3\n1/6 + 1/3\n1/6\nprob.\n1/3\n1/2\n1/6\nProblem 41. Let X be a discrete random variable with pmf p given by:\nx\n-2\n-1\np(x) 1/15 2/15 3/15 4/15 5/15\n(a) Let Y = X2. Find the pmf of Y .\n(b) Find the value the cdf of X at -3/2, 3/4, 7/8, 1, 1.5, 5.\n(c) Find the value the cdf of Y at -3/2, 3/4, 7/8, 1, 1.5, 5.\nSolution: (a) Note: Y = 1 when X = 1 or X = -1, so\nP(Y = 1) = P(X = 1) + P(X = -1).\nValues y of Y\npmf pY (y)\n3/15\n6/15\n6/15\n(b) and (c) To distinguish the distribution functions we'll write FX and FY .\nUsing the tables in part (a) and the definition FX(a) = P (X ≤ a) etc. we get\na\n-3/2\n3/4\n7/8\n1.5\nFX(a)\n1/15\n6/15\n6/15\n10/15\n10/15\nFY (a)\n3/15\n3/15\n9/15\n9/15\nProblem 42. Suppose that the cdf of X is given by:\n⎧0 for a < 0\n{\nfor 0 ≤ a < 2\nF(a) =\n⎨\nfor 2 ≤ a < 4\n{\n⎩ 1\nfor a ≥ 4.\nDetermine the pmf of X.\nSolution: The jumps in the distribution function are at 0, 2, 4. The value of p(a) at a\njump is the height of the jump:\na\np(a)\n1/5\n1/5\n3/5\nProblem 43. For each of the following say whether it can be the graph of a cdf. If it can\nbe, say whether the variable is discrete or continuous.\n\nPractice Exam 1: All Questions, Spring 2022\nx\nF (x)\n0.5\n(i)\nx\nF (x)\n0.5\n(ii)\nx\nF (x)\n0.5\n(iii)\nx\nF (x)\n0.5\n(iv)\nx\nF (x)\n0.5\n(v)\nx\nF (x)\n0.5\n(vi)\nx\nF (x)\n0.5\n(vii)\nx\nF (x)\n0.5\n(viii)\nSolution: (i) yes, discrete,\n(ii) no,\n(iii) no,\n(iv) no,\n(v) yes, continuous\n(vi) no (vii) yes, continuous,\n(viii) yes, continuous.\nProblem 44. Suppose X has range [0,1] and has cdf\nF (x) = x2\nfor 0 ≤ x ≤ 1.\nCompute P( 1\n2 < X < 3\n4).\nSolution: P (1/2 ≤ X ≤ 3/4) = F (3/4) - F (1/2) = (3/4)2 - (1/2)2 = 5/16 .\nProblem 45. Let X be a random variable with range [0, 1] and cdf\nF (X) = 2x2 - x4\nfor 0 ≤ x ≤ 1.\n(a) Compute P( 1\n4 ≤ X ≤ 3\n4).\n(b) What is the pdf of X?\nSolution: (a) P(1/4 ≤ X ≤ 3/4) = F(3/4) -F(1/4) = 11/16 = 0.6875.\n(b) f(x) = F ′(x) = 4x - 4x3 in [0,1].\n6 Distributions with Names\nProblem 46. Exponential Distribution\nSuppose that buses arrive are scheduled to arrive at a bus stop at noon but are always X\nminutes late, where X is an exponential random variable with probability density function\nfX(x) = λe-λx. Suppose that you arrive at the bus stop precisely at noon.\n\nPractice Exam 1: All Questions, Spring 2022\n(a) Compute the probability that you have to wait for more than five minutes for the bus\nto arrive.\nSolution: We compute\nP(X ≥5) = 1 -P(X < 5) = 1 -∫\nλe-λxdx = 1 - (1 - e-5λ) = e-5λ.\n(b) Suppose that you have already waiting for 10 minutes. Compute the probability that you\nhave to wait an additional five minutes or more.\nSolution: We want P (X ≥ 15|X ≥ 10). First observe that P(X ≥ 15, X ≥ 10) = P(X ≥\n15). From similar computations in (a), we know\nP(X ≥ 15) = e-15λ\nP (X ≥ 10) = e-10λ.\nFrom the definition of conditional probability,\nP(X ≥ 15, X ≥ 10)\nP(X ≥ 15)\nP(X ≥ 15|X ≥ 10) =\n=\nP(X ≥ 10)\nP(X ≥ 10) = e-5λ\nNote: This is an illustration of the memorylessness property of the exponential distribu\ntion.\nProblem 47. Normal Distribution: Throughout these problems, let φ and Φ be the pdf\nand cdf, respectively, of the standard normal distribution Suppose Z is a standard normal\nrandom variable and let X = 3Z+ 1.\n(a) Express P(X ≤ x) in terms of Φ\nSolution: We have\n) = Φ (x-1\nFX(x) = P(X≤x) = P(3Z+ 1 ≤x) = P(Z≤ x-1\n) .\n(b) Differentiate the expression from (a) with respect to x to get the pdf of X, f(x).\nRemember that Φ′(z) = φ(z) and don't forget the chain rule\nSolution: Differentiating with respect to x, we have\nd\n3φ(x - 1\nfX(x) =\nFX(x) =\n) .\ndx\n2 e- x2\nSince φ(x) = (2π)- 1\n2 , we conclude\n- (x-1)2\n2⋅32\nfX(x) =\n,\n√\n2π\ne\nwhich is the probability density function of the N(1, 9) distribution. Note: The arguments\nin (a) and (b) give a proof that 3Z +1 is a normal random variable with mean 1 and variance\n9. See Problem Set 3, Question 5.\n(c) Find P(-1 ≤X ≤1)\n\nPractice Exam 1: All Questions, Spring 2022\nSolution: We have\nP(-1 ≤X≤1) = P(-2\n3 ≤Z≤0) = Φ(0) -Φ (-2\n3) ≈0.2475\n(d) Recall that the probability that Z is within one standard deviation of its mean is approx\nimately 68%. What is the probability that X is within one standard deviation of its mean?\nSolution: Since E[X] = 1, Var(X) = 9, we want P(-2 ≤ X ≤ 4). We have\nP(-2 ≤X ≤4) = P(-3 ≤3Z ≤3) = P(-1 ≤Z ≤1) ≈0.68.\nProblem 48. Transforming Normal Distributions\n= eZ\nSuppose Z ∼ N(0,1) and Y\n.\n(a) Find the cdf FY (a) and pdf fY (y) for Y . (For the CDF, the best you can do is write it\nin terms of Φ the standard normal cdf.)\nSolution: Note, Y follows what is called a log-normal distribution.\nFY(a) = P(Y ≤a) = P(eZ≤a) = P(Z≤ ln(a)) = Φ(ln(a)).\nDifferentiating using the chain rule:\nd\nd\nfy(a) = daFY (a) = daΦ(ln(a)) = aφ(ln(a)) = √\n2π a\ne-(ln(a))2/2.\n(b) We don't have a formula for Φ(z) so we don't have a formula for quantiles. So we have\nto write quantiles in terms of Φ-1.\n(i) Write the 0.33 quantile of Z in terms of Φ-1\n(ii) Write the 0.9 quantile of Y in terms of Φ-1.\n(iii) Find the median of Y .\nSolution: (i) The 0.33 quantile for Z is the value q0.33 such that P (Z ≤ q0.33) = 0.33.\nThat is, we want\nΦ(q0.33) = 0.33 ⇔ q0.33 = Φ-1(0.33) .\n(ii) We want to find q0.9 where\nq0.9 = eΦ-1(0.9) .\nFY (q0.9) = 0.9 ⇔ Φ(ln(q0.9)) = 0.9 ⇔\n= eΦ-1(0.5) = e0 =\n(iii) As in (ii) q0.5\n1 .\nProblem 49. (Random variables derived from normal random variables)\nLet X1, X2, ...Xn be i.i.d. N(0, 1) random variables.\nLet Yn = X1\n2 + ... + Xn\n2 .\n(a) Use the formula Var(Xj) = E[Xj\n2] - E[Xj]2 to show E[Xj\n2] = 1.\nSolution: Var(Xj) = 1 = E[Xj\n2] - E[Xj]2 = E[Xj\n2]. QED\n\nPractice Exam 1: All Questions, Spring 2022\n(b) Set up an integral in x for computing E[Xj\n4].\nFor 3 extra credit points, use integration by parts show E[Xj\n4] = 3.\n(If you don't do this, you can still use this result in part c.)\nSolution: E[Xj\n4] =\ninf\nx4e-x2/2 dx.\n√1\n2π\n∫\n-inf\n(Extra credit) By parts: let u = x3, v′ = xe-x2/2 ⇒ u′ = 3x2, v = -e-x2/2\ninf\ninf\nE[Xj\n4] = √1\n2π\n[x3e-x2/2∣\n+\n3x2e-x2/2 dx]\ninfty\n√1\n2π\n∫\n-inf\nThe first term is 0 and the second term is the formula for 3E[Xj\n2] = 3 (by part (a)). Thus,\nE[Xj\n4] = 3.\n(c) Deduce from parts (a) and (b) that Var(Xj\n2) = 2.\nSolution: Var(Xj\n2) = E[Xj\n4] - E[Xj\n2]2 = 3 - 1 = 2. QED\n(d) Use the Central Limit Theorem to approximate P (Y100 > 110).\nSolution: E[Y100] = E[100Xj\n2] = 100.\nVar(Y100) = 100Var(Xj) = 200.\nThe CLT says Y100 is approximately normal. Standardizing gives\nP(Y100 > 110) = P(Y100√\n-100 > √10\n200) ≈P(Z> 1/\n√\n2) = 0.24 .\nThis last value was computed using R: 1 - pnorm(1/sqrt(2),0,1).\nProblem 50. More Transforming Normal Distributions\n(a) Suppose Z is a standard normal random variable and let Y = aZ+ b, where a > 0 and\nb are constants.\nShow Y ∼ N(b, a2) (remember our notation for normal distributions uses mean and vari\nance).\nSolution: Let φ(z) and Φ(z) be the PDF and CDF of Z.\nFY (y) = P (Y ≤ y) = P (aZ + b ≤ y) = P (Z ≤ (y - b)/a) = Φ((y - b)/a).\nDifferentiating:\nfY(y) = dy\nd FY(y) = dy\nd Φ((y-b)/a) = aφ((y-b)/a) = √\n2πa\ne-(y-b)2/2a2.\nSince this is the density for N(b, a2) we have shown Y ∼ N(b, a2).\nY -μ\n(b) Suppose Y ∼ N(μ, σ2). Show\nfollows a standard normal distribution.\nσ\nSolution: By part (a), Y ∼ N(μ, σ2) ⇒ Y = σZ+ μ. But, this implies (Y -μ)/σ = Z∼\nN(0, 1).\nQED\nProblem 51. (Sums of normal random variables)\nLet X, Y be independent random variables where X ∼ N(2, 5) and Y ∼ N(5, 9) (we use the\nnotation N(μ, σ2)). Let W = 3X-2Y + 1.\n(a) Compute E[W ] and Var(W ).\n\nPractice Exam 1: All Questions, Spring 2022\nSolution: E[W] = 3E[X] -2E[Y] + 1 = 6 -10 + 1 = -3\nVar(W) = 9Var(X) + 4Var(Y) = 45 + 36 = 81\n(b) It is known that the sum of independent normal distributions is normal. Compute\nP(W ≤ 6).\nSolution: Since the sum of independent normal is normal part (a) shows: W ∼ N(-3, 81).\nLet Z ∼ N(0, 1). We standardize W : P(W≤6) = P(W+ 3 ≤9\n9) = P(Z≤1) ≈ 0.84.\nProblem 52. Let X ∼ U(a, b). Compute E[X] and Var(X).\nSolution: Method 1\nU(a, b) has density f(x) =\n[a, b]. So,\nb - a on\nb\nb\nb\nx2\nb2 - a2\na + b\nE[X] = ∫ xf(x) dx =\nxdx =\n∣ =\n.\nb-a ∫\n2(b-a)\n2(b-a) =\na\na\na\nb\nb\nb\nx3\nb3 - a3\nE[X2] = ∫ x2f(x) dx =\nx2 dx =\n∣ =\nb-a ∫\n3(b-a)\n3(b-a).\na\na\na\nFinding Var(X) now requires a little algebra,\nb3 - a3\nVar(X) = E[X2] - E[X]2 = 3(b - a) - (b + a)2\n4(b3 - a3) - 3(b - a)(b + a)2\nb3 - 3ab2 + 3a2b - a3\n(b - a)3\n(b - a)2\n=\n=\n=\n.\n12(b - a)\n12(b - a)\n12(b - a) =\nMethod 2\nThere is an easier way to find E[X] and Var(X).\nLet U ∼ U(a, b). Then the calculations above show E[U] = 1/2 and (E[U2] = 1/3 ⇒\nVar(U) = 1/3 - 1/4 = 1/12.\nNow, we know X = (b-a)U + a, so E[X] = (b - a)E[U] + a = (b - a)/2 + a = (b + a)/2\nand Var(X) = (b - a)2Var(U) = (b - a)2/12.\nProblem 53. In n + m independent Bernoulli(p) trials, let Sn be the number of successes\nin the first n trials and Tm the number of successes in the last m trials.\n(a) What is the distribution of Sn? Why?\nSolution: Sn ∼ Binomial(n, p), since it is the number of successes in n independent\nBernoulli trials.\n(b) What is the distribution of Tm? Why?\nSolution: Tm ∼ Binomial(m, p), since it is the number of successes in m independent\nBernoulli trials.\n(c) What is the distribution of Sn + Tm? Why?\nSolution: Sn + Tm ∼ Binomial(n + m, p), since it is the number of successes in n + m\nindependent Bernoulli trials.\n\nPractice Exam 1: All Questions, Spring 2022\n(d) Are Sn and Tm independent? Why?\nSolution: Yes, Sn and Tm are independent. We haven't given a formal definition of inde\npendent random variables yet. But, we know it means that knowing Sn gives no information\nabout Tm. This is clear since the first n trials are independent of the last m.\nProblem 54. Compute the median for the exponential distribution with parameter λ.\nSolution: The density for this distribution is f(x) = λ e-λx. We know (or can compute)\nthat the distribution function is F(a) = 1 - e-λa. The median is the value of a such that\nF (a) = 0.5. Thus, 1 - e-λa= 0.5 ⇒ 0.5 = e-λa ⇒ log(0.5) = -λa ⇒ a = log(2)/λ.\nProblem 55. Pareto and the 80-20 rule.\nPareto was an economist who used the Pareto distribution to model the wealth in a society.\nFor a fixed baseline m, the Pareto density with parameter α is\nα mα\nf(x) =\nfor x ≥ m.\nxα+1\nAssume X is a random variable that follows such a distribution.\n(a) Compute P(X > a) (you may assume a ≥ m).\ninf\ninf\nmα\nSolution: P(X> a) = ∫\nxα+1 =\nxα∣\n=\nαmα\n-mα\naα .\na\na\n(b) Pareto's principle is often paraphrased as the 80-20 rule. That is, 80% of the wealth\nis owned by 20% of the people. The rule is only exact for a Pareto distribution with\nα = log(5)/ log(4) = 1.16.\nSuppose α = m = 1. Compute the 0.80 quantile for the Pareto distribution.\nIn general, many phenomena follow the power law described by f(x). You can look up\n'Pareto principle' in Wikipedia to read more about this.\nSolution: We want the value q0.8 where P (X ≤ q0.8) = 0.8.\nThis is equivalent to P (X > q0.8) = 0.2. Using part (a) and the given values of m and α\nwe have\n= 0.2 ⇒ q0.8 = 5.\nq0.8\n7 Joint Probability, Covariance, Correlation\nProblem 56. (Another Arithmetic Puzzle)\nLet X and Y be two independent Bernoulli(0.5) random variables. Define S and T by:\nS = X+ Y\nand T = X-Y.\n(a) Find the joint and marginal pmf's for S and T .\n(b) Are S and T independent.\nSolution: (a) S = X+ Y takes values 0, 1, 2 and T = X-Y takes values -1, 0, 1.\nFirst we make two tables: the joint probability table for X and Y and a table given the values\n(S, T ) corresponding to values of (X, Y ), e.g. (X, Y) = (1, 1) corresponds to (S, T) = (2, 0).\n\nPractice Exam 1: All Questions, Spring 2022\nX\\Y\n1/4\n1/4\n1/4\n1/4\nX\\Y\n0,0\n1,-1\n1,1\n2,0\nY\nValues of (S, T ) corresponding to X and Y\nWe can use the two tables above to write the joint probability table for S and T . The\nmarginal probabilities are given in the table.\nJoint probabilities of X and\nS\\T\n-1\n1/4\n1/4\n1/4\n1/4\n1/4\n1/2\n1/4\n1/4\n1/2\n1/4\nJoint and marginal probabilities of S and T\n(b) No probabilities in the table are the product of the corresponding marginal probabilities.\n(This is easiest to see for the 0 entries.) So, S and T are not independent\nProblem 57. Data is taken on the height and shoe size of a sample of MIT students.\nHeight is coded by 3 values: 1 (short), 2 (average), 3 (tall) and shoe size is coded by 3\nvalues 1 (small), 2 (average), 3 (large). The joint counts are given in the following table.\nShoe \\ Height\n234 225\n180 453 161\n192 157\nLet X be the coded shoe size and Y the height of a random person in the sample.\n(a) Find the joint and marginal pmf of X and Y .\n(b) Are X and Y independent?\nSolution: (a) The joint distribution is found by dividing each entry in the data table by\nthe total number of people in the sample. Adding up all the entries we get 1725. So the\njoint probability table with marginals is\nY \\ X\nThe marginal distribution of X is at the right and of Y is at the bottom.\n(b) X and Y are dependent because, for example,\nP(X = 1 and Y = 1) = 1725 ≈0.136\nis not equal to\nP(X = 1)P(Y = 1) =\n⋅ 1725 ≈ 0.083.\n\nPractice Exam 1: All Questions, Spring 2022\nProblem 58. Let X and Y be two continuous random variables with joint pdf\nf(x, y) = cx2y(1 + y) for 0 ≤ x ≤ 3 and 0 ≤ y ≤ 3,\nand f(x, y) = 0 otherwise.\n(a) Find the value of c.\n(b) Find the probability P(1 ≤X≤2, 0 ≤Y ≤1).\n(c) Determine the joint cdf, F (a, b), of X and Y for a and b between 0 and 3.\n(d) Find marginal cdf FX(a) for a between 0 and 3.\n(e) Find the marginal pdf fX(x) directly from f(x, y) and check that it is the derivative of\nFX(x).\n(f) Are X and Y independent?\nSolution: (a) Total probability must be 1, so\n1 = ∫ ∫ f(x, y) dydx = ∫ ∫ c(x2y+ x2y2) dydx = c⋅ 2 ,\n(Here we skipped showing the arithmetic of the integration) Therefore, c = 243.\n(b)\nP(1 ≤X≤2, 0 ≤Y ≤1) = ∫\n∫\nf(x, y) dydx\n= ∫\n∫\nc(x2y + x2y2) dy dx\n= c ⋅ 18\n= 4374 ≈ 0.016\n(c) For 0 ≤ a ≤ 3 and 0 ≤ b ≤ 3. we have\na\nb\n+ a3b3\nF(a, b) = ∫ ∫ f(x, y)dydx = c(a3b2\n9 )\n(d) Since y = 3 is the maximum value for Y , we have\na3\nFX(a) = F(a, 3) = c(9a\n+ 3a3) = 2ca3 = 27\n(e) For 0 ≤ x ≤ 3, we have, by integrating over the entire range for y,\nfX(x) = ∫ f(x, y) dy = cx2 (3\n+ 3\n) = c27\n2 x2 = 9x2.\nThis is consistent with (c) because dx\nd (x3/27) = x2/9.\n\nPractice Exam 1: All Questions, Spring 2022\n(f) Since f(x, y) separates into a product as a function of x times a function of y we know\nX and Y are independent.\nProblem 59. Let X and Y be two random variables and let r, s, t, and u be real numbers.\n(a) Show that Cov(X+ s, Y + u) = Cov(X, Y ).\n(b) Show that Cov(rX, tY) = rtCov(X, Y ).\n(c) Show that Cov(rX + s, tY + u) = rtCov(X, Y ).\nSolution: (a) First note by linearity of expectation we have E[X + s] = E[X] + s, thus\nX + s-E[X + s] = X -E[X].\nLikewise Y + u-E[Y + u] = Y -E[Y].\nNow using the definition of covariance we get\nCov(X+ s, Y + u) = E[(X+ s-E[X+ s]) ⋅(Y + u-E[Y + u])]\n= E[(X - E[X]) ⋅ (Y - E[Y ])]\n= Cov(X, Y ).\n(b) This is very similar to part (a).\nWe know E[rX] = rE[X], so rX-E[rX] = r(X-E[X]). Likewise tY -E[tY ] = s(Y -E[Y ]).\nOnce again using the definition of covariance we get\nCov(rX, tY) = E[(rX-E[rX])(tY -E[tY])]\n= E[rt(X - E[X])(Y - E[Y ])]\n(Now we use linearity of expectation to pull out the factor of rt)\n= rtE[(X - E[X](Y - E[Y ]))]\n= rtCov(X, Y )\n(c) This is more of the same. We give the argument with far fewer algebraic details\nCov(rX + s, tY + u) = Cov(rX, tY ) (by part (a))\n= rtCov(X, Y ) (by part (b))\nProblem 60. Derive the formula for the covariance: Cov(X, Y) = E[XY ] -E[X]E[Y].\nSolution: Using linearity of expectation, we have\nCov(X, Y) = E[(X-E[X])(Y -E[Y])]\n= E [XY - E[X]Y - E[Y ]X + E[X]E[Y ]]\n= E[XY ] - E[X]E[Y ] - E[Y ]E[X] + E[X]E[Y ]\n= E[XY ] - E[X]E[Y ].\nProblem 61. (Arithmetic Puzzle)\n\nPractice Exam 1: All Questions, Spring 2022\nThe joint and marginal pmf's of X and Y are partly given in the following table.\nX\\Y\n1/6\n...\n1/4\n...\n...\n...\n...\n1/4\n1/3\n1/3\n...\n1/6 1/3\n...\n(a) Complete the table.\n(b) Are X and Y independent?\nSolution: (a) The marginal probabilities have to add up to 1, so the two missing marginal\nprobabilities can be computed: P(X = 3) = 1/3, P(Y = 3) = 1/2. Now each row and\ncolumn has to add up to its respective margin. For example, 1/6 + 0 + P(X = 1, Y = 3) =\n1/3, so P(X = 1, Y = 3) = 1/6. Here is the completed table.\nX\\Y\n1/6\n1/4\n1/12\n1/6\n1/12\n1/4\n1/3\n1/3\n1/3\n1/6\n1/3\n1/2\n(b) No, X and Y are not independent.\nFor example, P(X = 2, Y = 1) = 0 =P(X = 2) ⋅P(Y = 1).\nProblem 62. (Simple Joint Probability)\nLet X and Y each have range {1,2,3,4}. The following formula gives their joint pmf\ni + j\nP(X = i, Y = j) = 80\nCompute each of the following:\n(a) P(X = Y).\n(b) P (XY = 6).\n(c) P(1 ≤X≤2, 2 < Y ≤4).\nSolution: First we'll make the table for the joint pmf. Then we'll be able to answer the\nquestions by summing up entries in the table.\nX\\Y\n2/80 3/80 4/80 5/80\n3/80 4/80 5/80 6/80\n4/80 5/80 6/80 7/80\n5/80 6/80 7/80 8/80\n(a) P(X = Y) = p(1, 1) + p(2, 2) + p(3, 3) + p(4, 4) =\n(b) P(XY = 6) = p(2, 3) + p(3, 2) = 10/80 = 1/8.\n20/80 = 1/4.\n\nPractice Exam 1: All Questions, Spring 2022\n(c) P(1 ≤X≤2, 2 < Y ≤4) = sum of 4 orange probabilities in the upper right corner of\nthe table = 20/80 = 1/4.\nProblem 63. Toss a fair coin 3 times. Let X = the number of heads on the first toss, Y\nthe total number of heads on the last two tosses, and F the number of heads on the first two\ntosses.\n(a) Give the joint probability table for X and Y . Compute Cov(X, Y ).\nSolution: (a) X and Y are independent, so the table is computed from the product of the\nknown marginal probabilities. Since they are independent, Cov(X, Y) = 0.\nY \\X\nPY\n1/8\n1/8\n1/4\n1/4\n1/4\n1/2\n1/8\n1/8\n1/4\nPX\n1/2\n1/2\n(b) Give the joint probability table for X and F . Compute Cov(X, F ).\nSolution: (b) The sample space is Ω = {HHH, HHT, HTH, HTT, THH, THT, TTH,\nTTT}.\nP(X = 0, F = 0) = P({TTH, TTT}) = 1/4.\nP(X = 0, F = 1) = P({THH, THT}) = 1/4.\nP(X = 0, F = 2) = 0.\nP(X = 1, F = 0) = 0.\nP(X = 1, F = 1) = P({HTH, HTT}) = 1/4.\nP(X = 1, F = 2) = P({HHH, HHT}) = 1/4.\nF \\X\nPF\n1/4\n1/4\n1/4\n1/4\n1/2\n1/4\n1/4\nPX\n1/2\n1/2\nCov(X, F) = E[XF ] -E[X]E[F].\n⇒ Cov(X, F) = 3/4 -1/2 =\nE[X] = 1/2, E[F] = 1, E[XF ] = ∑xiyjp(xi, yj) = 3/4.\n1/4.\nProblem 64. Covariance and Independence\nLet X be a random variable that takes values -2, -1, 0, 1, 2; each with probability 1/5.\nLet\nY = X2.\n(a) Fill out the following table giving the joint frequency function for X and Y . Be sure to\ninclude the marginal probabilities.\nX\nY\n-2\n-1\ntotal\ntotal\nSolution:\n\nPractice Exam 1: All Questions, Spring 2022\nX\nY\n-2\n-1\n1/5\n1/5\n1/5\n1/5\n1/5\n1/5\n2/5\n2/5\n1/5\n1/5\n1/5\n1/5\n1/5\nEach column has only one nonzero value. For example, when X = -2 then Y = 4, so in\nthe X = -2 column, only P(X = -2, Y = 4) is not 0.\n(b) Find E[X] and E[Y ].\nSolution: Using the marginal distributions:\nE[X] = 1\n5(-2 -1 + 0 + 1 + 2) = 0.\nE[Y] = 0 ⋅5 + 1 ⋅5 + 4 ⋅5 = 2.\n(c) Show X and Y are not independent.\nSolution: We show the probabilities don't multiply:\nP(X = -2, Y = 0) = 0 =P(X = -2) ⋅P(Y = 0) = 1/25.\nSince these are not equal X and Y are not independent. (It is obvious that X2 is not\nindependent of X.)\n(d) Show Cov(X, Y) = 0.\nThis is an example of uncorrelated but non-independent random variables. The reason this\ncan happen is that correlation only measures the linear dependence between the two variables.\nIn this case, X and Y are not at all linearly related.\nSolution: Using the table from part (a) and the means computed in part (d) we get:\nCov(X, Y) = E[XY ] -E[X]E[Y]\n= 5(-2)(4) + 5\n1(-1)(1) + 1\n5(0)(0) + 5\n1(1)(1) + 1\n5(2)(4)\n= 0.\nProblem 65. Continuous Joint Distributions\nSuppose X and Y are continuous random variables with joint density function f(x, y) = x+y\non the unit square [0, 1] × [0, 1].\n(a) Let F (x, y) be the joint CDF. Compute F (1, 1). Compute F (x, y).\na\nb\nSolution: F(a, b) = P(X≤a, Y≤b) = ∫ ∫(x+ y) dydx.\nb\nInner integral:\nxy+ y\n∣ = xb+ b2\n2 .\na\nx2\na2b + ab2\nOuter integral:\n2 b + b\nx∣ =\n.\nSo\nx2y + xy2\nF (x, y) =\nand\nF(1, 1) = 1.\n\nPractice Exam 1: All Questions, Spring 2022\n(b) Compute the marginal densities for X and Y .\nSolution: fX(x) = ∫ f(x, y) dy = ∫(x+ y) dy = xy+ y\n∣ = x+ 1\n2.\nBy symmetry, fY (y) = y + 1/2.\n(c) Are X and Y independent?\nSolution: To see if they are independent we check if the joint density is the product of the\nmarginal densities.\nf(x, y) = x+ y, fX(x) ⋅fY(y) = (x+ 1/2)(y+ 1/2).\nSince these are not equal, X and Y are not independent.\n(d) Compute E[X], E[Y ], E[X2 + Y 2], Cov(X, Y ).\nSolution: E[X] = ∫ ∫ x(x+ y) dydx = ∫ [x2y+ xy2\n∣] dx = ∫ x2 + x\n2 dx = 12.\n(Or, using (b), E[X] = ∫\nxfX(x) dx = ∫\nx(x+ 1/2) dx = 7/12.)\nBy symmetry E[Y ] = 7/12.\nE[X2 + Y 2] = ∫\n∫\n(x2 + y2)(x+ y) dydx = 6.\nE[XY ] = ∫\n∫\nxy(x+ y) dydx = 3.\n3 - 49\n-1\nCov(X, Y) = E[XY ] -E[X]E[Y] =\n144 =\n144.\nProblem 66. Correlation\nFlip a coin 3 times. Use a joint pmf table to compute the covariance and correlation between\nthe number of heads on the first 2 and the number of heads on the last 2 flips.\nSolution: Let X = the number of heads on the first 2 flips and Y the number in the last\n2. Considering all 8 possibe tosses: HHH, HHT etc we get the following joint pmf for X\nand Y\nY /X\n1/8\n1/8\n1/4\n1/8\n1/4\n1/8\n1/2\n1/8\n1/8\n1/4\n1/4\n1/2\n1/4\nUsing the table we find\nE[XY ] = 4 + 21\n8 + 28\n1 + 41\n8 = 4.\nWe know E[X] = 1 = E[Y ] so\nCov(X, Y) = E[XY ] -E[X]E[Y] = 4\n5 -1 = 1\n4.\n\nPractice Exam 1: All Questions, Spring 2022\nSince X is the sum of 2 independent Bernoulli(0.5) we have σX = √2/4\nCov(X, Y)\n1/4\nCor(X, Y) =\n=\nσXσY\n(2)/4 = 2.\nProblem 67. Correlation\nFlip a coin 5 times. Use properties of covariance to compute the covariance and correlation\nbetween the number of heads on the first 3 and last 3 flips.\nSolution: As usual let Xi = the number of heads on the ith flip, i.e. 0 or 1.\nLet X = X1 + X2 + X3 the sum of the first 3 flips and Y = X3 + X4 + X5 the sum of the\nlast 3. Using the algebraic properties of covariance we have\nCov(X, Y) = Cov(X1 + X2 + X3, X3 + X4 + X5)\n= Cov(X1, X3) + Cov(X1, X4) + Cov(X1, X5)\n+ Cov(X2, X3) + Cov(X2, X4) + Cov(X2, X5)\n+ Cov(X3, X3) + Cov(X3, X4) + Cov(X3, X5)\nBecause the Xi are independent the only non-zero term in the above sum is Cov(X3X3) = Var(X3) = 4\nTherefore, Cov(X, Y) = 1\n4.\nWe get the correlation by dividing by the standard deviations. Since X is the sum of 3\nindependent Bernoulli(0.5) we have σX = √3/4\nCov(X, Y)\n1/4\nCor(X, Y) =\n=\nσXσY\n(3)/4 = 3.\n8 Law of Large Numbers, Central Limit Theorem\nProblem 68. (Table of normal probabilities)\nUse the table of standard normal probabilities to compute the following. (Z is the standard\nnormal.)\n(a) (i) P (Z ≤ 1.5)\n(ii) P (-1.5 < Z < 1.5)\nP (Z > -0.75).\n(b) Suppose X ∼ N(2, (0.5)2). Find (i) P(X ≤ 2)\n(ii) P(1 < X ≤ 1.75).\nSolution: (a) (i) 0.9332 (ii) 0.9332 - 0.0668 = 0.8664\n(iii) By symmetry = P (Z < 0.75) = 0.7734. (Or we could have used 1 - P (Z > -0.75.))\n(b) (i) Since 2 is the mean of the normal distribution, P(X ≤ 2) = 0.5.\n(ii) Standardizing,\n0.5 < Z≤ 1.75 -2\nP(1 < X≤1.75) = P(1 -2\n0.5\n) = P(-2 < Z< -0.5) = 0.3085-0.0228 = 0.2857 .\nProblem 69. Suppose X1, ... , X100 are i.i.d. with mean 1/5 and variance 1/9. Use the\ncentral limit theorem to estimate P (∑ Xi < 30).\n\nPractice Exam 1: All Questions, Spring 2022\nSolution: Standardize:\n< 30 - nμ\nP (∑ Xi< 30) = P(∑X\n√nσ\ni-μ\n√nσ )\ni\n≈P(Z< 30 -20\n10/3 ) (by the central limit theorem)\n= P(Z < 3)\n= 0.9987 (from the table of normal probabilities)\nProblem 70. All or None\nYou have $100 and, never mind why, you must convert it to $1000. Anything less is no\ngood. Your only way to make money is to gamble for it. Your chance of winning one bet is\np.\nHere are two extreme strategies:\nMaximum strategy: bet as much as you can each time. To be smart, if you have less than\n$500 you bet it all. If you have more, you bet enough to get to $1000.\nMinimum strategy: bet $1 each time.\nIf p < 0.5 (the odds are against you) which is the better strategy?\nWhat about p > 0.5?\nSolution: If p < 0.5 your expected winnings on any bet is negative, if p = 0.5 it is 0, and\nif p > 0.5 is is positive. By making a lot of bets the minimum strategy will 'win' you close\nto the expected average. So if p ≤ 0.5 you should use the maximum strategy and if p > 0.5\nyou should use the minumum strategy.\nProblem 71. (Central Limit Theorem)\nLet X1, X2, ... , X81 be i.i.d., each with expected value μ = E[Xi] = 5, and variance σ2 =\nVar(Xi) = 4. Approximate P(X1 + X2 + ⋯X81 > 369), using the central limit theorem.\nSolution: Let T = X1 + X2 + ... + X81. The central limit theorem says that\nT ≈ N(81 ∗5, 81 ∗4) = N(405, 182)\nStandardizing we have\n> 369 - 405\nP(T > 369) = P(T -405\n)\n≈ P (Z > -2)\n≈ 0.975\nThe value of 0.975 comes from the rule-of-thumb that P(|Z| < 2) ≈ 0.95. A more exact\nvalue (using R) is P (Z > -2) ≈ 0.9772.\nProblem 72. (Binomial ≈ normal)\nLet X ∼ binomial(100,1/3).\nAn 'exact' computation in R gives P (X ≤ 30) = 0.2765539. Use the central limit theorem\nto give an approximation of P(X ≤ 30)\n\nPractice Exam 1: All Questions, Spring 2022\nSolution: X ∼ binomial(100, 1/3) means X is the sum of 100 i.i.d. Bernoulli(1/3) random\nvariables Xi.\nWe know E[Xi] = 1/3 and Var(Xi) = (1/3)(2/3) = 2/9. Therefore the central limit theorem\nsays\nX ≈ N(100/3, 200/9)\nStandardization then gives\n≤ 30 - 100/3\nP(X≤30) = P( X-100/3\n√200/9\n) ≈P(Z≤-0.7071068) ≈0.239751\n√200/9\nWe used R to do these calculations The approximation agrees with the 'exact' number to 2\ndecimal places.\nProblem 73. (More Central Limit Theorem)\nThe average IQ in a population is 100 with standard deviation 15 (by definition, IQ is\nnormalized so this is the case). What is the probability that a randomly selected group of\n100 people has an average IQ above 115?\nSolution: Let Xj be the IQ of a randomly selected person. We are given E[Xj] = 100 and\nσXj = 15.\nLet X be the average of the IQ's of 100 randomly selected people. Then we know\nE[X] = 100\nand\nσX = 15/\n√\n100 = 1.5.\nThe problem asks for P (X > 115). Standardizing we get P(X > 115) ≈ P(Z > 10).\nThis is effectively 0.\nProblem 74. Hospitals (binomial, CLT, etc)\n- A certain town is served by two hospitals.\n- Larger hospital: about 45 babies born each day.\n- Smaller hospital about 15 babies born each day.\n- For a period of 1 year, each hospital recorded the days on which more than 60% of the\nbabies born were boys.\n(a) Which hospital do you think recorded more such days?\n(i) The larger hospital.\n(ii) The smaller hospital.\n(iii) About the same (that is, within 5% of each other).\n(b) Let Li (resp., Si) be the Bernoulli random variable which takes the value 1 if more\nthan 60% of the babies born in the larger (resp., smaller) hospital on the ith day were boys.\nDetermine the distribution of Li and of Si.\n\nPractice Exam 1: All Questions, Spring 2022\n(c) Let L (resp., S) be the number of days on which more than 60% of the babies born in\nthe larger (resp., smaller) hospital were boys. What type of distribution do L and S have?\nCompute the expected value and variance in each case.\n(d) Via the CLT, approximate the 0.84 quantile of L (resp., S). Would you like to revise\nyour answer to part (a)?\n(e) What is the correlation of L and S? What is the joint pmf of L and S? Visualize the\nregion corresponding to the event L > S. Express P(L > S) as a double sum.\n(a) Solution: When this question was asked in a study, the number of undergraduates\nwho chose each option was 21, 21, and 55, respectively. This shows a lack of intuition for\nthe relevance of sample size on deviation from the true mean (i.e., variance).\n(b) Solution: The random variable XL, giving the number of boys born in the larger\nhospital on day i, is governed by a Bin(45, 0.5) distribution. So Li has a Ber(pL) distribution\nwith\n( 45\npL = P(X > 27) = ∑\nk ) 0.545 ≈ 0.068\nk=28\nSimilarly, the random variable XS, giving the number of boys born in the smaller hospital\non day i, is governed by a Bin(15, 0.5) distribution. So Si has a Ber(pS) distribution with\n( 15\npS = P(XS > 9) = ∑\nk ) 0.515 ≈ 0.151\nk=10\nWe see that pS is indeed greater than pL, consistent with (ii).\n(c) Solution: Note that L = ∑\ni=1 Si. So L has a Bin(365, pL) distribu\ni=1 Li and S = ∑\ntion and S has a Bin(365, pS) distribution. Thus\nE[L] = 365pL ≈ 25\nE[S] = 365pS ≈ 55\nVar(L) = 365pL(1 - pL) ≈ 23\nVar(S) = 365pS(1 - pS) ≈ 47\n(d) Solution: mean + sd in each case:\nFor L, q0.84 ≈ 25 +\n√\n23.\nFor S, q0.84 ≈ 55 +\n√\n47.\n(e) Since L and S are independent, their joint distribution is determined by multiplying\ntheir individual distributions. Both L and S are binomial with n = 365 and pL and pS\ncomputed above. Thus\npl,sP (L = i and S= j) = p(i, j) = (365\ni )pL\ni(1 -pL)365-i(365\nS(1 -pS)365-j\nj )pj\nThus\nP(L > S) = ∑ ∑ p(i, j) ≈ 0.0000916\ni=0 j=i+1\n(We used R to do the computations.)\n\nPractice Exam 1: All Questions, Spring 2022\n9 R Problems\nR will not be on the exam. However, these problems will help you understand the concepts\nwe've been studying.\nProblem 75. R simulation\nConsider X1, X2, ...all independent and with distribution N(0, 1). Let Xm be the average of\nX1, ...Xn.\n(a) Give E[Xn] and σXn exactly.\nSolution: E[Xj] = 0 ⇒ E[Xn] = 0.\nVar(Xj) = 1 ⇒ Var (X1 + ... + Xn\n) = n ⇒ σXn =\nn\n√1\nn.\n(b) Use a R simulation to estimate E[Xn] and Var(Xn) for n = 1, 9, 100. (You should use\nthe rnorm function to simulate 1000 samples of each Xj.)\nSolution: Here's my R code:\nx = rnorm(100*1000,0,1)\ndata = matrix(x, nrow=100, ncol=1000)\ndata1 = data[1,]\nm1 = mean(data1)\nv1 = var(data1)\ndata9 = colMeans(data[1:9,])\nm9 = mean(data9)\nv9 = var(data9)\ndata100 = colMeans(data)\nm100 = mean(data100)\nv100 = var(data100)\n#display the results\nprint(m1)\nprint(v1)\nprint(m9)\nprint(v9)\nprint(m100)\nprint(v100)\n∑ xk\n∑ xk\nNote if x = [x1, x2, ... , xn] then var(x) actually computes n - 1 instead of\n. There is\nn\na good reason for this which we will learn in the statistics part of the class. For now, it's\nenough to note that if n = 1000 the using n or n - 1 won't make much difference.\nProblem 76. R Exercise\nLet X1, X2, X3, X4, X5 be independent U(0, 1) random variables.\nLet X = X1 + X2 + X3 and Y = X3 + X4 + X5.\nUse the runif() function to simulate 1000 trials of each of these variables. Use these to\nestimate Cov(X, Y ).\nSolution: a = runif(5*1000,0,1)\n\nPractice Exam 1: All Questions, Spring 2022\ndata = matrix(a,5,1000)\nx = colSums(data[1:3,])\ny = colSums(data[3:5,])\nprint(cov(x,y))\nExtra Credit\nCompute this covariance exactly.\nSolution: Method 1 (Algebra)\nFirst, if i = j we know Xi and Xj are independent, so Cov(Xi, Xj) = 0.\nCov(X, Y) = Cov(X1 + X2 + X3, X3 + X4 + X5)\n= Cov(X1, X3) + Cov(X1, X4) + Cov(X1, X5)\n+ Cov(X2, X3) + Cov(X2, X4) + Cov(X2, X5)\n+ Cov(X3, X3) + Cov(X3, X4) + Cov(X3, X5)\n(most of these terms are 0)\n= Cov(X3, X3)\n= Var(X3)\n=\n(known variance of a uniform(0,1) distribution)\nMethod 2 (Multivariable calculus)\nIn 5 dimensional space we have the joint distribution\nf(x1, x2, x3, x4, x5) = 1.\nComputing directly\nE[X] = E[X1 + X2 + X3] = ∫ ∫ ∫ ∫ ∫ (x1 + x2 + x3) dx1 dx2 dx3, dx4 dx5\nfirst integral = 2\n1 + x2 + x3\nsecond integral = 2 + 1\n2 + x3 = 1 + x3\nthird integral = 2\nfourth integral = 2\nfifth integral = 2\nSo, E[X] = 3/2, likewise E[Y ] = 3/2.\nE[XY ] = ∫ ∫ ∫ ∫ ∫ (x1 + x2 + x3)(x3 + x4 + x5) dx1 dx2 dx3 dx4 dx5\n= 7/3.\nCov(X, Y) = E[XY ] -E[X]E[Y] = 12\n1 = 0.08333.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Exam 1 Review: Practice 1: Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_prac_exam01a_sol.pdf",
      "content": "Exam 1 Practice Questions I -solutions, 18.05, Spring 2022\nNotes.\nNot every possible problem can be covered in 13 problems. Look at the other review\nproblems as well as the readings, psets and class problems.\nEven the first 13 problems are much longer than the actual test will be,\nProblem 1. In a ballroom dancing class the students are divided into group A and group\nB. There are six people in group A and seven in group B. If four As and four Bs are\nchosen and paired off, how many pairings are possible?\nSolution: Build the pairings in stages and count the ways to build each stage:\nStage 1: Choose the 4 from group A: (6\n4).\nStage 2: Choose the 4 from group B: (7\n4)\nWe need to be careful because we don't want to build the same 4 couples in multiple ways.\nLine up the 4 A's A1, A2, A3, A4\nStage 3: Choose a partner from the 4 Bs for A1: 4.\nStage 4: Choose a partner from the remaining 3 Bs for A2: 3\nStage 5: Choose a partner from the remaining 2 Bs for A3: 2\nStage 6: Pair the last B with A4: 1\nNumber of possible pairings: (6\n4)(7\n4)4!.\nNote: we could have done stages 3-6 in one go as: Stages 3-6: Arrange the 4 Bs opposite\nthe 4 As: 4! ways.\nProblem 2. Let A and B be two events. Suppose the probability that neither A or B occurs\nis 2/3. What is the probability that one or both occur?\nSolution: We are given P(Ac ∩ Bc) = 2/3 and asked to find P(A ∪ B).\nAc∩Bc= (A∪B)c ⇒P(A∪B) = 1 -P(Ac∩Bc) = 1/3.\nProblem 3. Suppose you are taking a multiple-choice test with c choices for each question.\nIn answering a question on this test, the probability that you know the answer is p. If you\ndon't know the answer, you choose one at random. What is the probability that you knew\nthe answer to a question, given that you answered it correctly?\nSolution: The following tree shows the setting\nGuess\nKnow\nCorrect\nWrong\nCorrect\nWrong\np\n1 -p\n1/c\n1 -1/c\nLet C be the event that you answer the question correctly. Let K be the event that you\nactually know the answer. The left circled node shows P(K∩C) = p. Both circled nodes\n\nExam 1 Practice I, Spring 2022\ntogether show P(C) = p + (1 -p)/c. So,\nP(K∩C)\np\nP(K|C) =\n=\nP(C)\np+ (1 -p)/c\nOr we could use the algebraic form of Bayes' theorem and the law of total probability: Let\nG stand for the event that you're guessing. Then we have,\nP(C|K) = 1, P(K) = p, P(C) = P(C|K)P(K) + P(C|G)P(G) = p+ (1 -p)/c. So,\nP(C|K)P(K)\np\nP(K|C) =\n=\nP(C)\np+ (1 -p)/c\nProblem 4. Two dice are rolled.\nA = 'sum of two dice equals 3'\nB = 'sum of two dice equals 7'\nC = 'at least one of the dice shows a 1'\n(a) What is P (A|C)?\n(b) What is P (B|C)?\n(c) Are A and C independent? What about B and C?\nSolution: Sample space =\nΩ = {(1, 1), (1, 2), (1, 3), ... , (6, 6) } = {(i, j) | i, j = 1, 2, 3, 4, 5, 6 }.\n(Each outcome is equally likely, with probability 1/36.)\nA = {(1, 2), (2, 1)},\nB = {(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)}\nC = {(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)}\nP(A ∩ C)\n2/36\n(a) P(A|C) =\n=\nP(C)\n11/36 = 11..\nP(B ∩ C)\n2/36\n(b) P(B|C) =\n=\nP(C)\n11/36 = 11..\n(c) P(A) = 2/36 = P(A|C), so they are not independent. Similarly, P (B) = 6/36 =\nP (B|C), so they are not independent.\nProblem 5. Suppose that P (A) = 0.4, P (B) = 0.3 and P((A∪B)C) = 0.42. Are A and B\nindependent?\nSolution: We have P(A∪B) = 1 -0.42 = 0.58 and we know because of the inclusion-\nexclusion principle that\nP(A∪B) = P(A) + P(B) -P(A∩B).\nThus,\nP (A ∩ B) = P (A) + P (B) - P (A ∪ B) = 0.4 + 0.3 - 0.58 = 0.12 = (0.4)(0.3) = P (A)P (B).\n\nExam 1 Practice I, Spring 2022\nSo, A and B are independent.\nProblem 6. Suppose that X takes values between 0 and 1 and has probability density\nfunction 2x. Compute Var(X) and Var(X2).\nSolution: We will make use of the formula Var(Y ) = E[Y 2] - E[Y ]2. First we compute\nE[X] = ∫\nx ⋅ 2xdx =\nE[X2] = ∫\nx2 ⋅ 2xdx = 2\nE[X4] = ∫\nx4 ⋅ 2xdx = 3.\nThus,\nVar(X) = E[X2] - (E[X])\n2 = 2 - 4\n9 = 18\nand\nVar(X2) = E[X4] - (E[X2])\n2 = 3 - 1\n4 = 12.\nProblem 7. Suppose 100 people all toss a hat into a box and then proceed to randomly\npick out of a hat. What is the expected number of people to get their own hat back.\nHint: express the number of people who get their own hat as a sum of random variables\nwhose expected value is easy to compute.\nSolution: Let X be the number of people who get their own hat.\nFollowing the hint: let Xj represent whether person j gets their own hat. That is, Xj = 1\nif person j gets their hat and 0 if not.\nWe have, X = ∑Xj, so E[X] = ∑E[Xj].\nj=1\nj=1\nSince person j is equally likely to get any hat, we have P (Xj = 1) = 1/100. Thus, Xj ∼\nBernoulli(1/100) ⇒ E[Xj] = 1/100 ⇒ E[X] = 1.\nProblem 8. Let T be the waiting time for customers in a queue. Suppose that T is\nexponential with pdf f(t) = 2e-2t on [0, inf).\nFind the pdf of the rate at which customers are served R = 1/T .\nSolution: The CDF for T is\nt\nFT(t) = P(T ≤ t) = ∫ 2e-2u du = -e-2u∣0\nt = 1 - e-2t.\nNext, we find the CDF of R. R takes values in (0, inf).\nFor 0 < r,\nFR(r) = P(R≤r) = P(1/T < r) = P(T > 1/r) = 1 -FT(1/r) = e-2/r.\n\n{\n{\nExam 1 Practice I, Spring 2022\nWe differentiate to get fR(r) = dr\nd (e-2/r) = r\n2 e-2/r.\nProblem 9. Suppose that the cdf of X is given by:\n⎧0 for a < 0\n{\nfor 0 ≤ a < 2\nF(a) =\n⎨ 2\nfor 2 ≤ a < 4\n{\n⎩1 for a ≥ 4.\nDetermine the pmf of X.\nSolution: The jumps in the distribution function are at 0, 2, 4. The value of p(a) at a\njump is the height of the jump:\na\np(a)\n1/5\n1/5\n3/5\nProblem 10. Exponential Distribution\nSuppose that buses arrive are scheduled to arrive at a bus stop at noon but are always X\nminutes late, where X is an exponential random variable with probability density function\nfX(x) = λe-λx. Suppose that you arrive at the bus stop precisely at noon.\n(a) Compute the probability that you have to wait for more than five minutes for the bus\nto arrive.\nSolution: We compute\nP(X ≥5) = 1 -P(X < 5) = 1 -∫\nλe-λxdx = 1 - (1 - e-5λ) = e-5λ.\n(b) Suppose that you have already waiting for 10 minutes. Compute the probability that you\nhave to wait an additional five minutes or more.\nSolution: We want P (X ≥ 15|X ≥ 10). First observe that P(X ≥ 15, X ≥ 10) = P(X ≥\n15). From similar computations in (a), we know\nP(X ≥ 15) = e-15λ\nP (X ≥ 10) = e-10λ.\nFrom the definition of conditional probability,\nP(X ≥ 15, X ≥ 10)\nP(X ≥ 15)\nP(X ≥ 15|X ≥ 10) =\n=\nP(X ≥ 10)\nP(X ≥ 10) = e-5λ\nNote: This is an illustration of the memorylessness property of the exponential distribu\ntion.\nProblem 11. Let X and Y be two continuous random variables with joint pdf\nf(x, y) = cx2y(1 + y) for 0 ≤ x ≤ 3 and 0 ≤ y ≤ 3,\nand f(x, y) = 0 otherwise.\n\nExam 1 Practice I, Spring 2022\n(a) Find the value of c.\n(b) Find the probability P(1 ≤X≤2, 0 ≤Y ≤1).\n(c) Determine the joint cdf, F (a, b), of X and Y for a and b between 0 and 3.\n(d) Find marginal cdf FX(a) for a between 0 and 3.\n(e) Find the marginal pdf fX(x) directly from f(x, y) and check that it is the derivative of\nFX(x).\n(f) Are X and Y independent?\nSolution: (a) Total probability must be 1, so\n1 = ∫ ∫ f(x, y) dydx = ∫ ∫ c(x2y+ x2y2) dydx = c⋅ 2 ,\n(Here we skipped showing the arithmetic of the integration) Therefore, c = 243.\n(b)\nP(1 ≤X≤2, 0 ≤Y ≤1) = ∫\n∫\nf(x, y) dydx\n= ∫\n∫\nc(x2y + x2y2) dy dx\n= c ⋅ 18\n= 4374 ≈ 0.016\n(c) For 0 ≤ a ≤ 3 and 0 ≤ b ≤ 3. we have\na\nb\n+ a3b3\nF(a, b) = ∫ ∫ f(x, y)dydx = c(a3b2\n9 )\n(d) Since y = 3 is the maximum value for Y , we have\na3\nFX(a) = F(a, 3) = c(9a\n+ 3a3) = 2ca3 = 27\n(e) For 0 ≤ x ≤ 3, we have, by integrating over the entire range for y,\nfX(x) = ∫ f(x, y) dy = cx2 (3\n+ 3\n) = c27\n2 x2 = 9x2.\nThis is consistent with (c) because dx\nd (x3/27) = x2/9.\n(f) Since f(x, y) separates into a product as a function of x times a function of y we know\nX and Y are independent.\nProblem 12. (Table of normal probabilities)\nUse the table of standard normal probabilities to compute the following. (Z is the standard\nnormal.)\n\nExam 1 Practice I, Spring 2022\n(a) (i) P (Z ≤ 1.5)\n(ii) P (-1.5 < Z < 1.5)\nP (Z > -0.75).\n(b) Suppose X ∼ N(2, (0.5)2). Find (i) P(X ≤ 2)\n(ii) P(1 < X ≤ 1.75).\nSolution: (a) (i) 0.9332 (ii) 0.9332 - 0.0668 = 0.8664\n(iii) By symmetry = P (Z < 0.75) = 0.7734. (Or we could have used 1 - P (Z > -0.75.))\n(b) (i) Since 2 is the mean of the normal distribution, P(X ≤ 2) = 0.5.\n(ii) Standardizing,\n0.5 < Z≤ 1.75 -2\nP(1 < X≤1.75) = P(1 -2\n0.5\n) = P(-2 < Z< -0.5) = 0.3085-0.0228 = 0.2857 .\nProblem 13. (Central Limit Theorem)\nLet X1, X2, ... , X81 be i.i.d., each with expected value μ = E[Xi] = 5, and variance σ2 =\nVar(Xi) = 4. Approximate P(X1 + X2 + ⋯X81 > 369), using the central limit theorem.\nSolution: Let T = X1 + X2 + ... + X81. The central limit theorem says that\nT ≈ N(81 ∗5, 81 ∗4) = N(405, 182)\nStandardizing we have\n> 369 - 405\nP(T > 369) = P(T -405\n)\n≈ P (Z > -2)\n≈ 0.975\nThe value of 0.975 comes from the rule-of-thumb that P(|Z| < 2) ≈ 0.95. A more exact\nvalue (using R) is P (Z > -2) ≈ 0.9772.\nMore problems\nProblem 14. There are 3 arrangements of the word DAD, namely DAD, ADD, and DDA.\nHow many arrangements are there of the word PROBABILITY?\nSolution: Sort the letters: A BB II L O P R T Y. There are 11 letters in all. We build\narrangements by starting with 11 'slots' and placing the letters in these slots, e.g\nA B I B I L O P R T Y\nCreate an arrangement in stages and count the number of possibilities at each stage:\nStage 1: Choose one of the 11 slots to put the A: (11\n1 )\nStage 2: Choose two of the remaining 10 slots to put the B's: (10\n2 )\nStage 3: Choose two of the remaining 8 slots to put the I's: (8\n2)\nStage 4: Choose one of the remaining 6 slots to put the L: (6\n1)\n\nExam 1 Practice I, Spring 2022\nStage 5: Choose one of the remaining 5 slots to put the O: (5\n1)\nStage 6: Choose one of the remaining 4 slots to put the P: (4\n1)\nStage 7: Choose one of the remaining 3 slots to put the R: (3\n1)\nStage 8: Choose one of the remaining 2 slots to put the T: (2\n1)\nStage 9: Use the last slot for the Y: (1\n1)\nNumber of arrangements:\n(11\n1 )(10\n10 ⋅9 8 ⋅7\n2 )(8\n2)(6\n1)(1\n5)(4\n1)(1\n3)(1\n2)(1\n1) = 11 ⋅\n⋅\n⋅ 6 ⋅ 5 ⋅ 4 ⋅ 3 ⋅ 2 ⋅ 1 = 9979200\nNote: choosing 11 out of 1 is so simple we could have immediately written 11 instead of\nbelaboring the issue by writing (11\n1 ). We wrote it this way to show one systematic way to\nthink about problems like this.\nProblem 15. A multiple choice exam has 4 choices for each question. A student has\nstudied enough so that the probability they will know the answer to a question is 0.5, the\nprobability that they will be able to eliminate one choice is 0.25, otherwise all 4 choices seem\nequally plausible. If they know the answer they will get the question right. If not they have\nto guess from the 3 or 4 choices.\nAs the teacher you want the test to measure what the student knows. If the student answers\na question correctly what's the probability they knew the answer?\nSolution: We show the probabilities in a tree:\nKnow\nEliminate 1\nTotal guess\nCorrect\nWrong\nCorrect\nWrong\nCorrect\nWrong\n1/2\n1/4\n1/4\n1/3\n2/3\n1/4\n3/4\nFor a given problem let C be the event the student gets the problem correct and K the\nevent the student knows the answer.\nThe question asks for P (K|C).\nWe'll compute this using Bayes' rule:\nP(C|K) P(K)\n1 ⋅1/2\nP(K|C) =\n=\n= 31 ≈ 0.774 = 77.4%\nP(C)\n1/2 + 1/12 + 1/16\nProblem 16. Compute the expectation and variance of a Bernoulli(p) random variable.\nSolution: Make a table:\n\nExam 1 Practice I, Spring 2022\nX:\nprob:\n(1-p)\np\nX2\n1.\nFrom the table, E[X] = 0 ⋅(1 -p) + 1 ⋅p = p.\nSince X and X2 have the same table E[X2] = E[X] = p.\nTherefore, Var(X) = p-p2 = p(1 -p).\nProblem 17. Transforming Normal Distributions\n= eZ\nSuppose Z ∼ N(0,1) and Y\n.\n(a) Find the cdf FY (a) and pdf fY (y) for Y . (For the CDF, the best you can do is write it\nin terms of Φ the standard normal cdf.)\nSolution: Note, Y follows what is called a log-normal distribution.\nFY(a) = P(Y ≤a) = P(eZ≤a) = P(Z≤ ln(a)) = Φ(ln(a)).\nDifferentiating using the chain rule:\nd\nd\nfy(a) = daFY (a) = daΦ(ln(a)) = aφ(ln(a)) = √\n2π a\ne-(ln(a))2/2.\n(b) We don't have a formula for Φ(z) so we don't have a formula for quantiles. So we have\nto write quantiles in terms of Φ-1.\n(i) Write the 0.33 quantile of Z in terms of Φ-1\n(ii) Write the 0.9 quantile of Y in terms of Φ-1.\n(iii) Find the median of Y .\nSolution: (i) The 0.33 quantile for Z is the value q0.33 such that P (Z ≤ q0.33) = 0.33.\nThat is, we want\nΦ(q0.33) = 0.33 ⇔ q0.33 = Φ-1(0.33) .\n(ii) We want to find q0.9 where\nq0.9 = eΦ-1(0.9) .\nFY (q0.9) = 0.9 ⇔ Φ(ln(q0.9)) = 0.9 ⇔\n= eΦ-1(0.5) = e0 =\n(iii) As in (ii) q0.5\n1 .\nProblem 18. Suppose that X ∼ Bin(n, 0.5). Find the probability mass function of Y = 2X.\nSolution: For y = 0, 2, 4, ... , 2n,\ny\n2) = ( n\nP(Y = y) = P(X =\nn\n.\ny/2) (1\n2)\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Exam 1 Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_exam01_sol.pdf",
      "content": "18.05 Exam 1 Solutions\nProblem 0. (5 pts) Be sure to attach your cheat sheet to your test.\nProblem 1. (10 pts: 4,6) Concept/Quick questions\n(a) (No explanations are necessary.)\nThe plot shows the pdf for three independent random variables X, Y , W . All use the same\nhorizontal and vertical scale.\nfX\nfY\nfW\nWhich random variable has the greatest variance?\nSolution: X.(Variance measures the spread away from the mean.)\n(b) Suppose A and B are two events and P (A) = 0.7, P (B) = 0.3 and P(A∩B) = 0.25.\nCompute each of the following\n(i) Compute P(A ∪ B)\n(ii) Compute P (A|B).\nSolution: (i) Inclusion exclusion: P(A∪B) = 0.7 + 0.3 -0.25 = 0.75.\n(ii) P (A|B) = P (A∩B) = 0.25\nP (B)\n0.3 .\nProblem 2. (15 pts: 10,5)\nYou create passwords as a string of 10 characters such that:\n- 5 of the characters are letters (upper and lower case, i.e. 52 characters) with repetitions\nallowed,\n- 3 are numbers { 0,1,2,3,4,5,6,7,8,9 } with repetitions allowed, and\n- 2 are distinct symbols from the list of 5 symbols: { !, @, #, $, & }.\n(a) How many passwords are there? (No need to simplify your answer.)\nSolution: First, choose the locations of the symbols (10\n2 ).\nThen choose the symbols, since they have to be different and order matters, we get 5 ⋅4.\nThen, choose the locations of the letters: (8\n5).\nThen count the number of ways to choose 5 letters (with replacement) 525.\nThen choose the locations of the numbers: (3\n3) = 1.\nFinally choose the numbers: 103.\nSo, the number of passwords (10\n2 ) ⋅(5\n8) ⋅20 ⋅103 ⋅525.\n\n18.05 Exam 1 Solutions\n(b) With all locations for symbols, letters, or numbers in your 10 character password being\nequally likely, what is the probability that the two symbols are next to each other?\nSolution: Count the ways to get a password where the two symbols are adjacent:\nFirst choose locations for the two symbols: there are 9 adjacent positions.\nThen there are 5 ⋅4 ways to choose the sequence of two symbols.\nThen choose the locations of the letters: (8\n5).\nThen count the number of ways to choose 5 upper or lower case letters (with replacement)\n525.\nThen choose the locations of the numbers: (3\n3).\nThen choose the numbers: 103.\n9 ⋅(5\n8) ⋅5 ⋅4 ⋅103 ⋅525\nSo, P (two adjacent symbols) =\n=\n(10\n2 ) ⋅(8\n5) ⋅5 ⋅4 ⋅103 ⋅525\n(10\n2 ) = 10\nProblem 3. (25 pts: 10,5,5,5)\nYou have 5 four-sided and 3 six-sided dice. You put them in a cup, choose one at random,\nroll the die, and report the result.\nLet D be the number of sides on the chosen die and let R be the result of the roll.\n(a) Make a joint probability table for D and R. Be sure to include the marginal probabilities\nfor D and R.\nSolution: Each element of the table is simply the probability of getting a die with the\nindicated number of sides and then rolling the indicated number. For example,\nP(R = 3 and D= 6) = P(R = 3|D= 6)P(D = 6) =\n⋅\n=\n6 8\n16.\nR\\D\n4-sided\n6-sided\n5/32\n1/16\n7/32\n5/32\n1/16\n7/32\n5/32\n1/16\n7/32\n5/32\n1/16\n7/32\n1/16\n1/16\n1/16\n1/16\n5/8\n3/8\n(b) What is the probability of rolling a 3?\nSolution: This is the sum of the entries in the R = 3 row of the table:\n5/32 + 1/16 = 7/32\n(Do you see why this has to be between 1/6 and 1/4?)\n(c) Compute P(D = 4|R = 3).\nSolution: We compute this as the fraction\nP(D = 4 and R= 3)\n5/32\n=\nP (R = 3)\n7/32 = 5/7.\n\n18.05 Exam 1 Solutions\n(d) Are D and R independent?\nSolution: No, the joint probabilities in the table are not the products of the marginal\nprobabilities. The easiest way to see this is to note that P(R = 6 and D = 4) = 0, which\ndoes not equal P (R = 6)P (D = 4) = 5/128.\nProblem 4. (10 pts)\nA quick screening test for a certain disease has three outcomes: positive, negative and\nuncertain. Suppose it has the following percentages.\nFor someone with the disease: positive 70%, negative 10%, uncertain 20%.\nFor someone without the disease: positive 10%, negative 60%, uncertain 30%.\nSuppose also, that the prevalence of the disease in the population is 2%.\nWhat is the probability that a random person who tests positive has the disease?\nSolution: We organize the problem in a tree. Here: D+ = has disease, D- = does not\nhave disease;\nT + = test is positive,\nother = test is negative or uncertain.\nD+\nD-\nT+\nother\nT+\nother\n0.02\n0.98\n0.7\n0.3\n0.1\n0.9\nP (T +|D+)P (D+)\n0.7 ⋅ 0.02\nP (D+|T +) =\n=\n8 = 0.125 .\nP(T +)\n0.7 ⋅0.02 + 0.1 ⋅0.98 = 112 =\nProblem 5. (25 pts: 5,5,5,5,5)\nTwo students, Xeno and Yolanda are meeting up for lunch. They plan on a time to meet\nat noon. Both have class before so neither will be early. Both have class that starts at 1pm,\nso they will both arrive between 0 and 1 hour late. Let X be the time in hours that Xeno\narrives late and let Y be the time in hours that Yolanda arrives late.\nAssume that the joint pdf of these random variables is f(x, y) = 5/4 -xy.\n(a) Find the two marginal pdfs.\nSolution: To find the marginals we 'integrate out' the other variable.\nfX(x) = ∫\nf(x, y) dy = ∫\n5/4 -xydy = 5/4 - x/2.\nfY(y) = ∫\nf(x, y) dx = ∫\n5/4 -xydx = 5/4 - y/2.\nWe could have used symmetry to deduce fY (y) without any integration.\n(b) Are X and Y independent?\nSolution: Since the joint pdf is not the product of the marginals, i.e. f(x, y) = fX(x)fY(y),\nX and Y are not independent.\n(c) Find E[X], Var(X). (For these, you need to simplify the fractions.)\n\n18.05 Exam 1 Solutions\nSolution: We compute both E[X] and Var(X) = E[X2] - E[X]2 using the marginal pdf\nfX(X) found in part (a).\nE[X] = ∫ xfX(x) dx = ∫ 5x/4 -x2/2 dx = 8 -1\n6 = 24.\nE[X2] = ∫ x2fX(x) dx = ∫ 5x2/4 -x3/2 dx = 5\n8 = 7\n12 -1\n24.\n2 =\nVar(X) = E[X2] - E[X]\n24 - 112\n=\n.\n(d) Compute the covariance Cov(X, Y ) and correlation Cor(X, Y ).\nHint: By symmetry you know the mean and variance of Y are the same as those for X.\nFor this part, there is no need to simplify fractions.\nSolution: By symmetry, we know E[Y ] = E[X] = 11/24 and Var(Y) = Var(X) = 47/242.\nWe use the formula Cov(X, Y) = E[XY ] -E[X]E[Y].\nE[XY ] = ∫ ∫ xyf(x, y) dxdy = ∫ ∫ 5xy/4 -x2y2 dx = 5\n9 = 29\n16 -1\n144.\nCov(X, Y) = E[XY ] -E[X]E[Y] =\n=\n144 -11\n144 -144 ⋅4 = -144 ⋅4\nCov(X, Y )\nCov(X, Y )\n-5/242\n-5\nCor(X, Y) =\n=\n=\n=\n47/242\n47/242\nσXσY\n(e) Set up, but do not compute an expression computing the probability that Xeno and\nYolanda arrive within 6 minutes (0.1 hours) of each other and that Yolanda arrives after\nXeno.\nYour integral will be over a region R in the unit square. You can leave your integral in the\nform ∫∫ h(x, y) dx dy and show R in a figure elsewhere on the page. The function h(x, y)\nR\nmust be specified completely.\nSolution: The integral is ∫∫f(x, y) dxdy = ∫∫5/4 -xydxdy.\nR\nR\nThe region R is the part of the unit square where X < Y and Y -X< 0.1. This is the\nstrip of the triangle shown in the picture\nx\ny\nR(X< Y< X+ 0.1)\n\n18.05 Exam 1 Solutions\nThis was not asked for, but using 18.02 we get\n0.9\nx+0.1\nP(X< Y < X+ 0.1) = ∫\n∫\n5/4 -xydydx+ ∫ ∫ 5/4 -xydydx\nx\n0.9 x\nProblem 6. (10 pts)\nA company manufactures solar panels. When homeowners install the panels, the state pays\n50% of the cost. Because this subsidy is about to expire, the company wants to manufacture\nas many panels as it can in the next 20 days.\nFor a variety of reasons the number of panels it can manufacture in a day is a random variable\nwith each day independent of the others. Suppose the daily output follows a so-called gamma\ndistribution. The pdf of this distribution is not that complicated (f(x) =\nx\n10 e-x/100),\n4! ⋅\nbut we'll let Wikipedia tell us the mean and variance: mean = 500, variance = 5 ⋅ 104.\nEstimate the probability that they will be able to manufacture more than 10,500 panels in\nthe next 20 days.\nSolution: Let S be the total manufactured in 20 days. The problem asks for P (S > 10500).\nSince S is a sum of 20 i.i.d. random variables, the central limit theorem tell us that it is\napproximately normal. We know that one day has mean 500 and variance 5 ⋅ 104. So\nE[S] = 20 ⋅ 500 = 10000 Var(S) = 20 ⋅5 ⋅104 = 105\nσS= 103.\nStandardizing and using the CLT we get\n> 10, 500 - 10, 000\nP(S> 10500) = P(S-10, 000\n)\n≈P(Z > 0.5) = 1 -P(Z ≤0.5) ≈1 -0.6915 = 0.3085\nThe decimal answer came by looking up P (Z < 0.5) ≈ 0.6915 in the standard normal\ntable.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Practice Exam 1b Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_prac_exam01b_sol.pdf",
      "content": "Exam 1 Practice Questions II -solutions, 18.05, Spring 2022\nNotes.\nNot every possible question be covered in 11 problems. Look at the other review problems\nas well as the readings, psets and class problems.\nEven the first 11 problems are much longer than the actual test will be,\nProblem 1. A full house in poker is a hand where three cards share one rank and two cards\nshare another rank. How many ways are there to get a full-house? What is the probability\nof getting a full-house?\nSolution: We build a full-house in stages and count the number of ways to make each\nstage:\nStage 1. Choose the rank of the pair: (13\n1 ).\nStage 2. Choose the pair from that rank, i.e. pick 2 of 4 cards: (4\n2).\nStage 3. Choose the rank of the triple (from the remaining 12 ranks): (12\n1 ).\nStage 4. Choose the triple from that rank: (4\n3).\nNumber of ways to get a full-house: (13\n1 )(4\n2)(12\n1 )(4\n3)\nNumber of ways to pick any 5 cards out of 52: (52\n5 )\n(13\n1 )(4\n2)(12\n1 )(4\n3)\nProbability of a full house:\n≈ 0.00144\n(52\n5 )\nProblem 2. Let C and D be two events with P(C) = 0.25, P (D) = 0.45, and P(C ∩D) =\n0.1. What is P(Cc ∩D)?\nSolution: D is the disjoint union of D ∩ C and D ∩ Cc.\nSo, P(D ∩ C) + P(D ∩ Cc) = P(D)\n⇒ P(D ∩ Cc) = P(D) -P(D ∩ C) = 0.45 -0.1 =\n(We never use P(C) = 0.25.)\n0.35.\n0.1\nD∩Cc\n0.45-0.1\nD\nC\nProblem 3. Corrupted by their power, the judges running the popular game show America's\nNext Top Mathematician have been taking bribes from many of the contestants. Each\nepisode, a given contestant is either allowed to stay on the show or is kicked off.\nIf the contestant has been bribing the judges they will be allowed to stay with probability 1.\nIf the contestant has not been bribing the judges, they will be allowed to stay with probability\n1/3.\n\nExam 1 Practice II, Spring 2022\nSuppose that 1/4 of the contestants have been bribing the judges. The same contestants\nbribe the judges in both rounds, i.e., if a contestant bribes them in the first round, they bribe\nthem in the second round too (and vice versa).\n(a) If you pick a random contestant who was allowed to stay during the first episode, what\nis the probability that they were bribing the judges?\n(b) If you pick a random contestant, what is the probability that they are allowed to stay\nduring both of the first two episodes?\n(c) If you pick random contestant who was allowed to stay during the first episode, what is\nthe probability that they get kicked off during the second episode?\nSolution: The following tree shows the setting. Stay1 means the contestant was allowed\nto stay during the first episode and stay2 means the they were allowed to stay during the\nsecond.\nHonest\nBribe\nStay1\nLeave1\nStay1\nLeave1\nStay2\nLeave2\nStay2\nLeave2\n1/4\n3/4\n1/3\n2/3\n1/3\n2/3\nLet's name the relevant events:\nB = the contestant is bribing the judges\nH = the contestant is honest (not bribing the judges)\nS1 = the contestant was allowed to stay during the first episode\nS2 = the contestant was allowed to stay during the second episode\nL1 = the contestant was asked to leave during the first episode\nL2 = the contestant was asked to leave during the second episode\n(a) We first compute P (S1) using the law of total probability.\nP(S1) = P(S1|B)P(B) + P(S1|H)P(H) = 1 ⋅ 4 + 1 ⋅\n3 4 = 2.\n1/4\nWe therefore have (by Bayes' rule) P (B|S1) = P (S1|B) P (B)\n⋅\nP(S1) = 1 1/2 = 2.\n(b) Using the tree we have the total probability of S2 is\nP(S2) = 4 + 3 ⋅\n⋅3 =\nP (L2 ∩ S1)\n(c) We want to compute P (L2|S1) =\n.\nP (S1)\nFrom the calculation we did in part (a), P (S1) = 1/2. For the numerator, we have (see the\ntree)\nP(L2 ∩ S1) = P(L2 ∩ S1|B)P(B) + P(L2 ∩ S1|H)P(H) = 0 ⋅4 + 2 ⋅\n9 4 = 6\n\nExam 1 Practice II, Spring 2022\n1/6\nTherefore P (L2|S1) = 1/2 = 3.\nProblem 4. Suppose now that events A, B and C are mutually independent with\nP(A) = 0.3,\nP(B) = 0.4,\nP(C) = 0.5.\nCompute the following: (Hint: Use a Venn diagram)\n(i) P(A∩B ∩Cc)\n(ii) P(A∩Bc ∩C)\n(iii) P(Ac ∩B ∩C)\nSolution: By the mutual independence we have\nP(A∩B∩C) = P(A)P(B)P(C) = 0.06\nP(A∩B) = P(A)P(B) = 0.12\nP(A∩C) = P(A)P(C) = 0.15\nP(B∩C) = P(B)P(C) = 0.2\nWe show this in the following Venn diagram\n0.09\n0.14\n0.21\n0.06\n0.14\n0.09\n0.06\nA\nB\nC\nNote that, for instance, P (A ∩ B) is split into two pieces. One of the pieces is P (A ∩ B ∩ C)\nwhich we know and the other we compute as P(A∩B)-P(A∩B∩C) = 0.12-0.06 = 0.06.\nThe other intersections are similar.\nWe can read off the asked for probabilities from the diagram.\n(i) P(A∩B ∩Cc) = 0.06\n(ii) P(A∩Bc ∩C) = 0.09\n(iii) P(Ac ∩B ∩C) = 0.14.\nProblem 5. Suppose X is a random variable with E[X] = 5 and Var(X) = 2. What is\nE[X2]?\nSolution: Use Var(X) = E[X2] - E[X]2 ⇒ 2 = E[X2] - 25 ⇒ E[X2] = 27.\nProblem 6. (a) Suppose that X has probability density function fX(x) = λe-λx for x ≥ 0.\nCompute the cdf, FX(x).\n(b) If Y = X2, compute the pdf and cdf of Y .\n(a) Solution: We have cdf of X,\nx\nλe-λxdx = 1 - e-λx.\nFX(x) = ∫\n\nExam 1 Practice II, Spring 2022\nNow for y ≥ 0, we have\n(b) Solution:\nFY(y) = P(Y ≤y) = P(X2 ≤y) = P(X≤ √y) = 1 - e-λ√y.\nDifferentiating FY (y) with respect to y, we have\nfY (y) = λ\n2 y- 1\n2 e-λ√y.\nProblem 7. For each of the following say whether it can be the graph of a cdf. If it can\nbe, say whether the variable is discrete or continuous.\nx\nF (x)\n0.5\n(i)\nx\nF (x)\n0.5\n(ii)\nx\nF (x)\n0.5\n(iii)\nx\nF (x)\n0.5\n(iv)\nx\nF (x)\n0.5\n(v)\nx\nF (x)\n0.5\n(vi)\nx\nF (x)\n0.5\n(vii)\nx\nF (x)\n0.5\n(viii)\nSolution: (i) yes, discrete,\n(ii) no,\n(iii) no,\n(iv) no,\n(v) yes, continuous\n(vi) no (vii) yes, continuous,\n(viii) yes, continuous.\nProblem 8. Compute the median for the exponential distribution with parameter λ.\nSolution: The density for this distribution is f(x) = λ e-λx. We know (or can compute)\nthat the distribution function is F(a) = 1 - e-λa. The median is the value of a such that\nF (a) = 0.5. Thus, 1 - e-λa= 0.5 ⇒ 0.5 = e-λa ⇒ log(0.5) = -λa ⇒ a = log(2)/λ.\nProblem 9. (Another Arithmetic Puzzle)\nLet X and Y be two independent Bernoulli(0.5) random variables. Define S and T by:\nS = X+ Y\nand T = X-Y.\n(a) Find the joint and marginal pmf's for S and T .\n\nExam 1 Practice II, Spring 2022\n(b) Are S and T independent.\nSolution: (a) S = X+ Y takes values 0, 1, 2 and T = X-Y takes values -1, 0, 1.\nFirst we make two tables: the joint probability table for X and Y and a table given the values\n(S, T ) corresponding to values of (X, Y ), e.g. (X, Y) = (1, 1) corresponds to (S, T) = (2, 0).\nJoint probabilities of\nX\\Y\n1/4\n1/4\n1/4\n1/4\nX and Y\nValues of (S, T )\nX\\Y\n0,0\n1,-1\n1,1\n2,0\ncorresponding to X and Y\nWe can use the two tables above to write the joint probability table for S and T . The\nmarginal probabilities are given in the table.\nS\\T\n-1\n1/4\n1/4\n1/4\n1/4\n1/4\n1/2\n1/4\n1/4\n1/2\n1/4\nJoint and marginal probabilities of S and T\n(b) No probabilities in the table are the product of the corresponding marginal probabilities.\n(This is easiest to see for the 0 entries.) So, S and T are not independent\nProblem 10. Let X and Y be two random variables and let r, s, t, and u be real numbers.\n(a) Show that Cov(X+ s, Y + u) = Cov(X, Y ).\n(b) Show that Cov(rX, tY) = rtCov(X, Y ).\n(c) Show that Cov(rX + s, tY + u) = rtCov(X, Y ).\nSolution: (a) First note by linearity of expectation we have E[X + s] = E[X] + s, thus\nX + s-E[X + s] = X -E[X].\nLikewise Y + u-E[Y + u] = Y -E[Y].\nNow using the definition of covariance we get\nCov(X+ s, Y + u) = E[(X+ s-E[X+ s]) ⋅(Y + u-E[Y + u])]\n= E[(X - E[X]) ⋅ (Y - E[Y ])]\n= Cov(X, Y ).\n(b) This is very similar to part (a).\nWe know E[rX] = rE[X], so rX-E[rX] = r(X-E[X]). Likewise tY -E[tY ] = s(Y -E[Y ]).\nOnce again using the definition of covariance we get\nCov(rX, tY) = E[(rX-E[rX])(tY -E[tY])]\n= E[rt(X - E[X])(Y - E[Y ])]\n(Now we use linearity of expectation to pull out the factor of rt)\n= rtE[(X - E[X](Y - E[Y ]))]\n= rtCov(X, Y )\n\nExam 1 Practice II, Spring 2022\n(c) This is more of the same. We give the argument with far fewer algebraic details\nCov(rX + s, tY + u) = Cov(rX, tY ) (by part (a))\n= rtCov(X, Y ) (by part (b))\nProblem 11. (More Central Limit Theorem)\nThe average IQ in a population is 100 with standard deviation 15 (by definition, IQ is\nnormalized so this is the case). What is the probability that a randomly selected group of\n100 people has an average IQ above 115?\nSolution: Let Xj be the IQ of a randomly selected person. We are given E[Xj] = 100 and\nσXj = 15.\nLet X be the average of the IQ's of 100 randomly selected people. Then we know\nE[X] = 100\nand\nσX = 15/\n√\n100 = 1.5.\nThe problem asks for P (X > 115). Standardizing we get P(X > 115) ≈ P(Z > 10).\nThis is effectively 0.\nMore problems\nProblem 12. 20 politicians are having a tea party, 6 Democrats and 14 Republicans. To\nprepare, they need to choose:\n3 people to set the table, 2 people to boil the water, 6 people to make the scones.\nEach person can only do 1 task. (Note that this doesn't add up to 20. The rest of the people\ndon't help.)\n(a) In how many different ways can they choose which people perform these tasks?\n(b) Suppose that the Democrats all hate tea. If they only give tea to 10 of the 20 people,\nwhat is the probability that they only give tea to Republicans?\n(c) If they only give tea to 10 of the 20 people, what is the probability that they give tea to\n9 Republicans and 1 Democrat?\nSolution: (a) There are (20\n3 ) ways to choose the 3 people to set the table, then (17\n2 ) ways\nto choose the 2 people to boil water, and (15\n6 ) ways to choose the people to make scones.\nSo the total number of ways to choose people for these tasks is\n(20\n20!\n17!\n15!\n20!\n3 )(17\n2 )(15\n=\n⋅\n⋅\n=\n= 775975200.\n6 )\n3! 17! 2! 15! 6! 9!\n3! 2! 6! 9!\n(b) The number of ways to choose 10 of the 20 people is (20\n10) The number of ways to choose\n10 people from the 14 Republicans is (14\nSo the probability that you only choose 10\n10).\nRepublicans is\n(14\n14!\n10)\n10! 4! ≈ 0.00542\n(20\n20!\n10) =\n10! 10!\n\n⋅\n⋅\n⋅\n⋅\n⋅\n⋅\n⋅\n⋅\n⋅\nExam 1 Practice II, Spring 2022\nAlternatively, you could choose the 10 people in sequence and say that there is a 14/20\nprobability that the first person is a Republican, then a 13/19 probability that the second\none is, a 12/18 probability that third one is, etc. This gives a probability of\n17 16\n11.\n(You can check that this is the same as the other answer given above.)\n(c) You can choose 1 Democrat in (6\n1) = 6 ways, and you can choose 9 Republicans in (14\n9 )\nways, so the probability equals\n(14\n14!\n6 ⋅\n9 ) = 6 ⋅ 9! 5! = 6 ⋅14! 10! 10! .\n(20\n20!\n10)\n10! 10!\n9! 5! 20!\nProblem 13. More cards! Suppose you want to divide a 52 card deck into four hands with\n13 cards each. What is the probability that each hand has a king?\nSolution: Let Hi be the event that the ith hand has one king. We have the conditional\nprobabilities\n(4\n1)(48\n(3\n1)(36\n(2\n1)(24\n12)\n12)\n12)\nP (H1) =\n;\nP (H2|H1) =\n;\nP (H3|H1 ∩ H2) =\n(52\n(39\n(26\n13)\n13)\n13)\nP (H4|H1 ∩ H2 ∩ H3) = 1\nP (H1 ∩ H2 ∩ H3 ∩ H4) = P (H4|H1 ∩ H2 ∩ H3) P (H3|H1 ∩ H2) P (H2|H1) P (H1)\n(2\n1)(24\n1)(36\n1)(48\n12)(3\n12)(4\n12)\n=\n.\n(26\n13)(39\n13)(52\n13)\nProblem 14. There is a screening test for prostate cancer that looks at the level of PSA\n(prostate-specific antigen) in the blood. There are a number of reasons besides prostate\ncancer that a man can have elevated PSA levels. In addition, many types of prostate cancer\ndevelop so slowly that that they are never a problem. Unfortunately there is currently no\ntest to distinguish the different types and using the test is controversial because it is hard to\nquantify the accuracy rates and the harm done by false positives.\nFor this problem we'll call a positive test a true positive if it catches a dangerous type of\nprostate cancer. We'll assume the following numbers:\nRate of prostate cancer among men over 50 = 0.0005\nTrue positive rate for the test = 0.9\nFalse positive rate for the test = 0.01\nLet T be the event a man has a positive test and let D be the event a man has a dangerous\ntype of the disease. Find P (D|T ) and P (D|T c).\n\nExam 1 Practice II, Spring 2022\nSolution: You should write this out in a tree! (For example, see the solution to the next\nproblem.)\nWe compute all the pieces needed to apply Bayes' rule. We're given\nP (T |D) = 0.9 ⇒ P (T c|D) = 0.1,\nP(T|Dc) = 0.01 ⇒ P(T c|Dc) = 0.99.\nP(D) = 0.0005 ⇒ P(Dc) = 1 -P(D) = 0.9995.\nWe use the law of total probability to compute P (T ):\nP(T) = P(T|D) P(D) + P(T|Dc) P(Dc) = 0.9 ⋅0.0005 + 0.01 ⋅0.9995 = 0.010445\nNow we can use Bayes' rule to answer the questions:\nP (T |D) P (D)\n0.9 × 0.0005\nP (D|T ) =\n=\n= 0.043\nP (T )\n0.010445\nP(T c|D) P(D)\n0.1 × 0.0005\nP(D|T c) =\n=\n= 5.0 × 10-5\nP (T c)\n0.989555\nProblem 15. Let X be a discrete random variable with pmf p given by:\nx\n-2\n-1\np(x) 1/15 2/15 3/15 4/15 5/15\n(a) Let Y = X2. Find the pmf of Y .\n(b) Find the value the cdf of X at -3/2, 3/4, 7/8, 1, 1.5, 5.\n(c) Find the value the cdf of Y at -3/2, 3/4, 7/8, 1, 1.5, 5.\nSolution: (a) Note: Y = 1 when X = 1 or X = -1, so\nP(Y = 1) = P(X = 1) + P(X = -1).\nValues y of Y\npmf pY (y)\n3/15\n6/15\n6/15\n(b) and (c) To distinguish the distribution functions we'll write FX and FY .\nUsing the tables in part (a) and the definition FX(a) = P (X ≤ a) etc. we get\na\n-3/2\n3/4\n7/8\n1.5\nFX(a)\n1/15\n6/15\n6/15\n10/15\n10/15\nFY (a)\n3/15\n3/15\n9/15\n9/15\nProblem 16. Suppose I play a gambling game with even odds. So, I can wager b dollars\nand I either win or lose b dollars with probability p = 0.5.\nI employ the following strategy to try to guarantee that I win some money.\nI bet $1; if I lose, I double my bet to $2, if I lose I double my bet again. I continue until\nI win. Eventually I'm sure to win a bet and net $1 (run through the first few rounds and\nyou'll see why this is the net).\nIf this really worked casinos would be out of business. Our goal in this problem is to\nunderstand the flaw in the strategy.\n\nExam 1 Practice II, Spring 2022\n(a) Let X be the amount of money bet on the last game (the one I win). X takes values 1,\n2, 4, 8, .... Determine the probability mass function for X. That is, find p(2k), where k is\nin {0, 1, 2, ...}.\nSolution: It is easy to see that (e.g. look at the probability tree) P (2k) = 2k+1 .\n(b) Compute E[X].\ninf\nSolution: E[X] = ∑2k\n= ∑1\n2 = inf. Technically, E[X] is undefined in this case.\n2k+1\nk=0\n(c) Use your answer in part (b) to explain why the stategy is a bad one.\nSolution: Technically, E[X] is undefined in this case. But the value of inf tells us what\nis wrong with the scheme. Since the average last bet is infinite, I need to have an infinite\namount of money in reserve.\nThis problem and solution is often referred to as the St. Petersburg paradox\nProblem 17. Normal Distribution: Throughout these problems, let φ and Φ be the pdf\nand cdf, respectively, of the standard normal distribution Suppose Z is a standard normal\nrandom variable and let X = 3Z+ 1.\n(a) Express P(X ≤ x) in terms of Φ\nSolution: We have\n) = Φ (x-1\nFX(x) = P(X≤x) = P(3Z+ 1 ≤x) = P(Z≤ x-1\n) .\n(b) Differentiate the expression from (a) with respect to x to get the pdf of X, f(x).\nRemember that Φ′(z) = φ(z) and don't forget the chain rule\nSolution: Differentiating with respect to x, we have\nd\n3φ(x - 1\nfX(x) =\nFX(x) =\n) .\ndx\n2 e- x2\nSince φ(x) = (2π)- 1\n2 , we conclude\n- (x-1)2\n2⋅32\nfX(x) =\n,\n√\n2π\ne\nwhich is the probability density function of the N(1, 9) distribution. Note: The arguments\nin (a) and (b) give a proof that 3Z +1 is a normal random variable with mean 1 and variance\n9. See Problem Set 3, Question 5.\n(c) Find P(-1 ≤X ≤1)\nSolution: We have\nP(-1 ≤X≤1) = P(-2\n3 ≤Z≤0) = Φ(0) -Φ (-2\n3) ≈0.2475\n(d) Recall that the probability that Z is within one standard deviation of its mean is approx\nimately 68%. What is the probability that X is within one standard deviation of its mean?\n\nExam 1 Practice II, Spring 2022\nSolution: Since E[X] = 1, Var(X) = 9, we want P(-2 ≤ X ≤ 4). We have\nP(-2 ≤X ≤4) = P(-3 ≤3Z ≤3) = P(-1 ≤Z ≤1) ≈0.68.\nProblem 18. Toss a fair coin 3 times. Let X = the number of heads on the first toss, Y\nthe total number of heads on the last two tosses, and F the number of heads on the first two\ntosses.\n(a) Give the joint probability table for X and Y . Compute Cov(X, Y ).\nSolution: (a) X and Y are independent, so the table is computed from the product of the\nknown marginal probabilities. Since they are independent, Cov(X, Y) = 0.\nY \\X\nPY\n1/8\n1/8\n1/4\n1/4\n1/4\n1/2\n1/8\n1/8\n1/4\nPX\n1/2\n1/2\n(b) Give the joint probability table for X and F . Compute Cov(X, F ).\nSolution: (b) The sample space is Ω = {HHH, HHT, HTH, HTT, THH, THT, TTH,\nTTT}.\nP(X = 0, F = 0) = P({TTH, TTT}) = 1/4.\nP(X = 0, F = 1) = P({THH, THT}) = 1/4.\nP(X = 0, F = 2) = 0.\nP(X = 1, F = 0) = 0.\nP(X = 1, F = 1) = P({HTH, HTT}) = 1/4.\nP(X = 1, F = 2) = P({HHH, HHT}) = 1/4.\nF \\X\nPF\n1/4\n1/4\n1/4\n1/4\n1/2\n1/4\n1/4\nPX\n1/2\n1/2\nCov(X, F) = E[XF ] -E[X]E[F].\n⇒ Cov(X, F) = 3/4 -1/2 =\nE[X] = 1/2, E[F] = 1, E[XF ] = ∑xiyjp(xi, yj) = 3/4.\n1/4.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Exam 2 Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_exam02_sol.pdf",
      "content": "18.05 Exam 2 Solutions\nCheat Sheet\nProblem 0. (5 pts) Be sure to attach your cheat sheet to your test.\nPart I: Concept Questions\nProblem I.1. (10 pts: 2,2,2,2,2) Determine which of the following concepts/statements are\nFrequentist and which are Bayesian. Note: a concept can be either or both. Give a short\nexplanation for each answer.\n(a) P-value\n(b) Prior distribution\n(c) Average of data\n(d) There is a 3% probability that the average weight is between 92.3 mg and 100 mg.\n(e) The odds in favor of H0 against HA are 1 to 3.\n(a) Solution: Frequentist : p-values are used in NHST\n(b) Solution:\nBayesian : Priors are used in Bayesian updating. Frequentists do not\nconsider probabilities of hypotheses.\n(c) Solution: Both : the average of data is a statistic. All statisticians compute statistics!\n(d) Solution: Bayesian : To compute this probability requires knowing the probabilities\nof all the hypotheses.\n(e) Solution: Bayesian : Odds are a way of presenting probabilities. Frequentists do not\nconsider the probabilities of hypotheses.\nProblem I.2. (6 pts: 3,3) Suppose you run a significance test at significance level 0.025,\nand that the test has a power of 95%.\nFor each part of this problem, give your answer and a short explanation.\n(a) Assuming the null hypothesis, what is the probability of a type I error?\n(i) 0.025\n(ii) 0.05\n(iii) 0.95\n(iv) 0.975\n(v) cannot be determined from the information given.\n(b) Assuming the alternative hypothesis, what is the probability of a type II error?\n(i) 0.025\n(ii) 0.05\n(iii) 0.95\n(iv) 0.975\n(v) cannot be determined from the information given.\n(a) Solution: (i) 0.025 : By definition P (type I error) = P(reject | H0) = significance.\n(b) Solution: (ii) 0.05 = 1 -0.95 : By definition P (type II error) = P(non-rejection | HA) =\n1 - P(reject | HA) = 1 - power.\nProblem I.3. (3 pts) The following graphs show the rejection regions and pdfs of the null\nand alternative hypotheses for two different hypothesis tests. Which graph shows the test\nwith the higher power?\n\n18.05 Exam 2 Solutions\nYour answer should be 'left graph' or 'right graph'. Give a short explanation.\nx\nφ(x|H0)\nφ(x|HA)\n.\nreject H0 region\ndo not reject H0 region\nx\nφ(x|H0)\nφ(x|HA)\n.\nreject H0 region\ndo not reject H0 region\nSolution: Left graph : power = P (reject|HA). The left hand graph has much more area\nunder φ(x|HA) and above the rejection region than the right hand graph.\nProblem I.4. (3 pts) You find a coin on the street, with some unknown probability θ of\nlanding heads when tossed. Circle the only reasonable prior for θ. (No explanation needed.)\n(i) Uniform([0, 0.5])\n(ii) Beta(2, 2)\n(iii) N(0.5, 0.25).\nSolution: Beta(2, 2) : It has the correct range and no bias towards heads or tails.\nUniform([0, 0.5]) does not permit θ > 0.5.\nN(0.5, 0.52) has significant amount of probability for θ < 0 and θ > 1.0. This is not allowed.\nProblem I.5. (4 pts: 2,2) For each of the following: Is the prior conjugate to the given\nlikelihood? In each case, a and b are parameters for the priors.\nhypothesis\ndata\nprior\nlikelihood\n(a)\nθ ∈ [0, 1]\nx\nc1θa(1 - θ)b\n(10\nx )θx(1 - θ)10-x\n(b)\nλ ∈ [0, inf)\nx\nc1λa-1e-bλ\nλe-λx\n(a) Solution: Yes : the prior × likelihood has the form cθa+x(1 - θ)b+10-x. This is the\nsame form as the prior, i.e. a constant times θ to a power times (1 - θ) to a power. (In\nfact, a beta distribution.)\n(b) Solution: Yes : the prior × likelihood has the form cλae-(b+x)λ. This is the same\nform as the prior.\nPart II: Problems\nProblem II.1. (12 pts.)\nThe gamma distribution with shape parameter 3 and unknown rate parameter β has range\n(0, inf) and pdf\nβ3x2\nf(x) =\n2 e-βx.\nSuppose the data\n1, 1, 2, 3, 5\nwas drawn independently from such a distribution. Find the maximum likelihood estimate\n(MLE) of β.\n(Note: we shouldn't expect to get integer values for the data. So either our measurements\nwere quite crude or we didn't want you to have to do arithmetic with fractions.)\n\n18.05 Exam 2 Solutions\nSolution: Call the 5 data values: x1, x2, x3, x4, x5. We'll use their numerical values when\nneeded. The likelihood of the given data is\nx2\n1 ⋅ x2\n2 ⋅ x\n3 ⋅ x2\n4 ⋅ x2\n5 e-β(x1+x2+x3+x4+x5)\nf(x1)f(x2)f(x3)f(x4)f(x5) = β15 ⋅\n⋅\nSo the log likelihood is (β) = 15 ln(β) -β(x1 + x2 + x3 + x4 + x5) + c, where c is a constant.\nTaking the derivative and setting it to 0, we get\nl′(β) = 15\nβ - (x1 + x2 + x3 + x4 + x5) = 0.\nSolving for β gives β =\n=\n4 , where in the last step, we used\nx1 + x2 + x3 + x4 + x5\n12 =\nthe given numerical values of the data.\nProblem II.2. (15 pts)\nA random process produces outcomes labeled A, B and C with probabilities θ/2, θ/2, 1 - θ\nrespectively. Here θ is an unknown parameter with value between 0 and 1. You want to\nknow the value of θ.\nBefore running any experiments you have a prior pdf for θ of f(θ) = 3θ2. You then run the\nprocess five times producing data A, B, C, A, B.\nFind the posterior probability density for θ.\nSolution: As usual, we make a Bayesian update table. The data ABCAB has probability\nθ θ\nθ\nθ = (θ\np(ABCBA | θ) =\n⋅\n⋅(1 -θ) ⋅\n⋅\n(1 -θ)\n2)\nhypothesis\nprior\nlikelihood\nP(data | hypoth.)\nBN\nposterior\nθ\n3θ2 dθ\n( θ\n(1 - θ)\n2)\n16θ6(1 - θ) dθ\n56 ⋅ θ6(1 - θ) dθ\nsum\np(data) =\n⋅\n16 56\nHere, the total probability p(data) is computed by integrating the Bayes numerator column\n1 3\np(data) = ∫ 16θ6(1 -θ) dθ =\n⋅\n16 56.\nSo, the posterior pdf for θ is f(θ|data) = 56 ⋅ θ6(1 - θ).\n(We could also have found the normalizing factor by recognizing the posterior as a Beta(7, 2),\n8!\nso the factor is 6!1! = 56.)\nProblem II.3. (12 pts.) It is the year 2122 and a small percentage of children are born\nwith an array of superpowers. The usual super strength and ability to make bad jokes in\nthe direst of dire situations won't manifest themselves till puberty. The one superpower that\n\n18.05 Exam 2 Solutions\nmanifests at age 7 is known as Bayesian intelligence. They can think clearly about statistics\nand can answer virtually any stats question.\nSo, a screening test was developed that asks 7 year-olds to compute the posterior odds that a\n7 year-old who correctly answers the screening question has superpowers. The test is quite\naccurate, but some ordinary children do answer correctly\nSuppose the odds that a random 7 year-old has superpowers is 1/100. Suppose the screening\ntest has a 100% true positive rate and a 10% false positive rate. If a randomly chosen child\ncorrectly answers the question, what are the posterior odds that they have superpowers?\nSolution: The easiest way to do this is to use the formula:\nposterior odds = prior odds × likelihood ratio.\nThe prior odds are given as 1/100.\nP (correct answer | superpower)\nThe likelihood ratio = P (correct answer | no superpower) = 1/10 = 10.\nSo: posterior odds =\n⋅10 =\n10.\nProblem II.4. (15 pts: 3,3,3,3,3)\nYou collect data from an experiment and do a one-sided Z-test with the rejection region in\nthe right tail and significance level 0.1. You find the Z-value is 2.\n(a) Which R code computes the critical value for the rejection region?\n(i) pnorm(0.1, 0, 1)\n(ii) pnorm(0.9, 0, 1)\n(iii) pnorm(0.95, 0, 1)\n(iv) pnorm(2.0, 0, 1)\n(v) 1 - pnorm(2.0, 0, 1)\n(vi) qnorm(0.05, 0, 1)\n(vii) qnorm(0.1, 0, 1)\n(viii) qnorm(0.9, 0, 1)\n(ix) qnorm(0.95, 0, 1)\n(b) Using the tables at the end of the exam, compute this critical value.\n(c) Which R code computes the p-value for this experiment?\n(i) pnorm(0.1, 0, 1)\n(ii) pnorm(0.9, 0, 1)\n(iii) pnorm(0.95, 0, 1)\n(iv) pnorm(2.0, 0, 1)\n(v) 1 - pnorm(2.0, 0, 1)\n(vi) qnorm(0.05, 0, 1)\n(vii) qnorm(0.1, 0, 1)\n(viii) qnorm(0.9, 0, 1)\n(ix) qnorm(0.95, 0, 1)\n(d) Using the tables at the end of the exam, compute the p-value.\n(e) Should you reject the null hypothesis?\n(a) Solution: (viii) qnorm(0.9, 0, 1): the critical value for the right-tail should have 0.1\nprobability to the right, so 0.9 probability to the left.\n(b) Solution: 1.28: the place where the left tail of the normal is 0.9. The table says\nthat's between z = 1.25 and 1.30; interpolation says that it's about 1.28. (R says it's about\n1.281552.)\n(c) Solution: (v)1 - pnorm(2.0, 0, 1): p = P(Z > 2|H0) = 1 -P(Z < 2|H0). This is\nexactly what the code in (v) computes.\n(d) Solution: 0.0228. This is the right tail of the standard normal at 2, or 1 minus the\nleft tail; the table says the left tail is 0.9772.\n\n18.05 Exam 2 Solutions\n(e) Solution: Yes, we should reject: according to (b), the value z = 2 is well inside the\nrejection region z ≥ 1.28. Alternatively, the p-value computed in (d) is much smaller than\nthe significance 0.1. Alternatively, the rule of thumb for 2 standard deviations above the\nmean for a normal distribution says p = P (Z ≥ 2|H0) ≈ 0.025 < 0.1.\nProblem II.5. (15 pts)\nAdult onset diabetes is known to be highly genetically determined. A study was done compar\ning frequencies of a particular allele in a sample of diabetics and a sample of nondiabetics.\nThe data are shown in the following table. (We adjusted the data to make hand calculation\neasier.)\nDiabetic\nNormal\nBb or bb\nBB\nDo a significance test for whether the frequencies of the alleles is different in the two groups\nat a significance level of 0.05.\nSolution: We will use a chi-square test for homogeneity. Remember we need to use all the\ndata!. For hypotheses we have:\nH0: the frequency of alleles is the same for both groups: diabetic and nondiabetic.\nHA: the frequency of alleles is different between the two groups.\nHere is the table of counts. The computation of the expected counts is explained below.\nDiabetic\nNondiabetic\nBb, bb\nBB\nobserved\nexpected\nobserved\nexpected\nThe expected counts are computed as follows. Under H0 the frequency of alleles is the\nsame, call them θ. To find the expected counts we find the MLE of θ using the combined\ndata:\ntotal Bb and bb\nθ =\n= 100 = 0.2.\ntotal subjects\nThen, for example, the expected number of Bb and bb alleles in the diabetic group is\n50 ⋅θ = 10. The other expected counts are computed in the same way.\nThe chi-square test statistic is\n= ∑(observed - expected)2\nX2\n=\n40 ≈ 2.5 + 2.5 + 0.625 + 0.625 ≈ 6.25.\nexpected\n10 + 52\n40 + 52\n10 + 52\nFinally, we need the degrees of freedom: df = 1. This is because we have a two-by-two\ntable and (2-1) ⋅(2-1) = 1. (Or because we can freely fill in the count in one cell and still\nbe consistent with the marginal counts 20, 80, 50, 50, 100, which are all used to compute\nthe expected counts.)\nFrom the χ2 table: p = P(X2 > 6.25|df = 1) < 0.05, in particular interpolated from the\ntable, 0.01 < p < 0.025, but we can also see this directly by noting that the critical value is\n3.84.\n\n18.05 Exam 2 Solutions\nConclusion: Since p < α, we reject H0 in favor of HA, that the frequency of alleles is not\nthe same between the diabetic and nondiabetic groups.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Practice Exam 2a Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_prac_exam02a_sol.pdf",
      "content": "Exam 2 Practice Questions -solutions, 18.05, Spring 2022\n1 Topics\n- Statistics: data, MLE\n- Bayesian inference: prior, likelihood, posterior, predictive probability, probability in\ntervals\n- Frequentist inference: NHST\n2 Using the probability tables\nYou should become familiar with the probability tables at the end of these notes.\nProblem 1. Use the standard normal table to find the following values. In all the problems\nZ is a standard normal random variable.\n(a) (i) P (Z < 1.5)\n(ii) P (Z > 1.5)\n(iii) P (-1.5 < Z < 1.5)\n(iv) P (Z ≤ 1.625)\nSolution: (i) The table gives this value as P (Z < 1.5) = 0.9332.\n(ii) This is the complement of the answer in (i): P (Z > 1.5) = 1 - 0.9332 = 0.0668. Or by\nsymmetry we could use the table for -1.5.\n(iii) We want P(Z < 1.5)-P(Z < -1.5) = P(Z < 1.5)-P(Z > 1.5). This is the difference\nof the answers in (i) and (ii): 0.8664.\n(iv) A rough estimate is the average of P (Z < 1.6) and P (Z < 1.65). That is,\nP (Z < 1.625) ≈ P (Z < 1.6) + P (Z < 1.65)\n0.9452 + 0.9505\n=\n= 0.9479.\n(b) (i) The right-tail with probability α = 0.05.\n(ii) The two-sided rejection region with probability α = 0.2.\n(iii) Find the range for the middle 50% of probability.\nSolution: (i) We are looking for the table entry with probability 0.95. This is between the\ntable entries for z = 1.65 and z = 1.60 and very close to that of z = 1.65. Answer: the\nregion is [1.64, inf). (R gives the 'exact' lower limit as 1.644854.)\n(ii) We want the table entry with probability 0.1. The table probabilities for z = -1.25 and\nz = -1.30 are 0.1056 and 0.0968. Since 0.1 is about 1/2 way from the first to the second\nwe take the left critical value as -1.275. Our region is\n(-inf, -1.275) ∪ (1.275, inf).\n(R gives qnorm(0.1, 0, 1) = -1.2816.)\n(iii) This is the range from q0.25 to q0.75. With the table we estimate q0.25 is about 1/2 of\nthe way from -0.65 to -0.70, i.e. ≈ -0.675. So, the range is [-0.675, 0.675].\n\nExam 2 Practice 2, Spring 2022\nProblem 2. The t-tables are different. They give the right critical values corresponding\nto probabilities. To save space we only give critical values for p ≤ 0.5. You need to use\nthe symmetry of the t-distribution to get them for p < 0.5. That is, tdf, p = -tdf, 1-p, e.g.\nt5, 0.975 = -t5, 0.025.\nUse the t-table to estimate the following values. In all the problems T is a random variable\ndrawn from a t-distribution with the indicated number of degrees of freedom.\n(a) (i) P (T > 1.6), with df = 3\n(ii) P (T < 1.6) with df = 3\n(iii) P (-1.68 < T < 1.68) with df = 49\n(iv) P (-1.6 < T < 1.6) with df = 49\nSolution: (i) The question asks to find which p-value goes with t = 1.6 when df = 3. We\nlook in the df = 3 row of the table and find 1.64 goes with p = 0.100 So, P(T > 1.6 | df =\n3) ≈ 0.1. (The true value is a little bit greater.)\n(ii) P(T < 1.6 | df = 3) = 1 -P(T > 1.6 | df = 3) ≈0.9.\n(iii) Using the df = 49 row of the t-table we find P(T > 1.68 | df = 49) = 0.05.\nNow, by symmetry P (T < -1.68 | df = 49) = 0.05 and P(-1.68 < T < 1.68 | df = 49) = 0.9 .\n(iv) Using the df = 49 row of the t-table we find P(T > 1.68 | df = 49) = 0.05 and\nP(T > 1.30 | df = 49) = 0.1. We can do a rough interpolation: P(T > 1.6 | df = 49) ≈ 0.06.\nNow, by symmetry P(T < -1.6 | df = 49) ≈ 0.06 and P(-1.6 < T < 1.6 | df = 49) ≈ 0.88 .\n(R gives 0.8839727.)\n(b) (i) The critical value for probability α = 0.05 for 8 degrees of freedom.\n(ii) The two-sided rejection region with probability α = 0.2 for 16 degrees of freedom.\n(iii) Find the range for the middle 50% of probability with df = 20.\nSolution: (i) This is a straightforward lookup: The p = 0.05, df = 8 entry is 1.86 .\n(ii) For a two-sided rejection region we need 0.1 probability in each tail. The critical value\nat p = 0.1, df = 16 is 1.34. So (by symmetry) the rejection region is\n(-inf, -1.34) ∪ (1.34, inf).\n(iii) This is the range from q0.25 to q0.75, i.e. from critical values t0.75 to t0.25. The ta\nble only gives critical for 0.2 and 0.3 For df = 20 these are 0.86 and 0.53. We average\nthese to estimate the 0.25 critical value as 0.7. Answer: the middle 50% of probability is\napproximately between t-values -0.7 and 0.7.\n(If we took into account the bell shape of the t-distribution we would estimate the 0.25\ncritical value as slightly closer to 0.53 than 0.86. Indeed R gives the value 0.687.)\nProblem 3. The chi-square tables are different. They give the right critical values corre\nsponding to probabilities.\nUse the chi-square tables table to find the following values. In all the problems X2 is\na random variable drawn from a χ2-distribution with the indicated number of degrees of\nfreedom.\n\nExam 2 Practice 2, Spring 2022\n(a) (i) P(X2 > 1.6), with df = 3\n(ii) P(X2 > 20) with df = 16\nSolution: (i) Looking in the df = 3 row of the chi-square table we see that 1.6 is about\n1/5 of the way between the values for p = 0.7 and p = 0.5. So we approximate P(X2 >\n1.6) ≈ 0.66. (The true value is 0.6594.)\n(ii) Looking in the df = 16 row of the chi-square table we see that 20 is about 1/4 of the\nway between the values for p = 0.2 and p = 0.3. We estimate P(X2 > 20) = 0.25. (The\ntrue value is 0.220)\n(b) (i) The right critical value for probability α = 0.05 for 8 degrees of freedom.\n(ii) The two-sided rejection region with probability α = 0.2 for 16 degrees of freedom.\nSolution: (i) This is in the table in the df = 8 row under p = 0.05. Answer: 15.51\n(ii) We want the critical values for p = 0.9 and p = 0.1 from the df = 16 row of the table.\n[0, 9.31] ∪ [23.54, inf).\n3 Data\nProblem 4. The following data is from a random sample: 5, 1, 3, 3, 8.\nCompute the sample mean, sample standard deviation and sample median.\nSolution: Sample mean 20/5 = 4.\n12 + (-3)2 + (-1)2 + (-1)2 + 42\nSample variance =\n= 7.\n5 -1\n√\n7.\nSample standard deviation =\nSample median = 3.\n4 MLE\nProblem 5. (a) A coin is tossed 100 times and lands heads 62 times. Find the maximum\nlikelihood estimate for the probability θ of heads.\nSolution: The likelihood function is\np(data|θ) = (100\n= cθ62(1 - θ)38.\n62 )θ62(1 - θ)38\nTo find the MLE we find the derivative of the log-likelihood and set it to 0.\nln(p(data|θ)) = ln(c) + 62 ln(θ) + 38 ln(1 - θ).\nd ln(p(data|θ))\n=\n= 0.\ndθ\nθ - 1 -θ\nThe algebra leads to the MLE θ = 62/100 .\n\nExam 2 Practice 2, Spring 2022\n(b) A coin is tossed n times and lands heads k times. Find the maximum likelihood estimate\nfor the probability θ of heads.\nSolution: The computation is identical to part (a). The likelihood function is\np(data|θ) = (n\nk)θk(1 - θ)n-k = cθk(1 - θ)n-k.\nTo find the MLE we set the derivative of the log-likelihood and set it to 0.\nln(p(data|θ)) = ln(c) + k ln(θ) + (n - k) ln(1 - θ).\nd ln(p(data|θ))\nk\nθ - n - k\n=\n= 0.\ndθ\n1 -θ\nThe algebra leads to the MLE θ = k/n .\nProblem 6. Suppose the data set y1, ... , yn is a drawn from a random sample consisting\nof i.i.d. discrete uniform distributions with range 1 to N. Find the maximum likelihood\nestimate of N .\nSolution: If N < max(yi) then the likelihood p(y1, ... , yn|N) = 0. So the likelihood\nfunction is\nif N < max(yi)\np(y1, ... , yn|N) = {0\n( N\n1 )\nn\nif N ≥ max(yi)\nThis is maximized when N is as small as possible. Since N ≥ max(yi) the MLE is\nN = max(yi).\nProblem 7. Suppose data x1, ... , xn is drawn from an exponential distribution exp(λ).\nFind the maximum likelihood for λ.\nSolution: The pdf of exp(λ) is p(x|λ) = λe-λx. So the likelihood and log-likelihood\nfunctions are\np(data|λ) = λne-λ(x1+⋯+xn),\nln(p(data|λ)) = n ln(λ) - λ ∑ xi.\nTaking a derivative with respect to λ and setting it equal to 0:\nd ln(p(data|λ))\nn\n⇒ 1\n∑xi\n= λ-∑xi= 0\n= x.\ndλ\nλ=\nn\nSo the MLE is λ = 1/x .\nProblem 8. Suppose x1, ... , xn is a data set drawn from a geometric(1/a) distribution.\nFind the maximum likelihood estimate of a. Here, geometric(p) means the probability of\nsuccess is p and we run trials until the first success and report the total number of trials,\nincluding the success. For example, the sequence F F F F S is 4 failures followed by a success,\nwhich produces x = 5.\nxi-1 1\nxi-1 1\na. = (a-1\nSolution: P (xi|a) = (1 - 1\n)\na)\na\na.\n\nExam 2 Practice 2, Spring 2022\nSo, the likelihood function is\n∑xi-n\nn\nP (data|a) = (a-1\n(1\na\n)\na)\nThe log likelihood is\nln(P (data|a)) = (∑ xi - n) (ln(a - 1) - ln(a)) - n ln(a).\nTaking the derivative\nd ln(P (data|a))\n= 0 ⇒ ∑xi\n= (∑xi-n) (\na) - n\n= a.\nda\na-1 -1\na\nn\nThe maximum likelihood estimate is a = x .\nProblem 9. You want to estimate the size of an MIT class that is closed to visitors. You\nknow that the students are numbered from 1 to n, where n is the number of students. You\ncall three random students out of the classroom and ask for their numbers, which turn out to\nbe 1, 3, 7. Find the maximum likelihood estimate for n. (Hint: the student #'s are drawn\nfrom a discrete uniform distribution.)\nSolution: If there are n students in the room then for the data 1, 3, 7 (occuring in any\norder) the likelihood is\n⎧0\nfor n < 7\n{\np(data | n) = ⎨\n3!\n{1/(n\n3) = n(n-1)(n-2)\nfor n ≥ 7\n⎩\nMaximizing this does not require calculus. It clearly has a maximum when n is as small as\npossible. Answer: n = 7 .\n5 Bayesian updating: discrete prior, discrete likelihood\nProblem 10. Twins\n(a) Suppose 1/4 of twins are identical and 3/4 of twins are fraternal. If you are pregnant\nwith twins of the same sex, what is the probability that they are identical?\nSolution: This is a Bayes' theorem problem. The likelihoods are\nP(same sex | identical) = 1\nP(different sex | identical) = 0\nP(same sex | fraternal) = 1/2\nP(different sex | fraternal) = 1/2\nThe data is 'the twins are the same sex'. We find the answer with an update table\nhyp.\nprior\nlikelihood\nBayes numer.\nposterior\nidentical\n1/4\n1/4\n2/5\nfraternal\n3/4\n1/2\n3/8\n3/5\nTot.\n5/8\nSo P(identical | same sex) = 2/5 = 0.4 .\n\nExam 2 Practice 2, Spring 2022\n(b) Find the posterior odds the twins are identical. Do this by multiplying the prior odds\nby the Bayes factor (likelihood ratio). Check this by computing the odds directly from your\nanswer to part (a).\nP (identical)\n1/4\nSolution: The prior odds O(identical) =\n3/4 = 1/3.\nP (fraternal) =\nP (same sex | identical)\nThe Bayes factor BF =\n=\nP (same sex|fraternal)\n1/2 = 2.\nSo, posterior odds O(identical | same sex) = O(identical) ⋅BF =\n⋅2 = 3.\nIn part (a) we found the posterior odds O(identical | same sex) = 2/5\n2. This is the same\n3/5 =\nas above.\nProblem 11. Dice.\nYou have a drawer full of 4, 6, 8, 12 and 20-sided dice. You suspect that they are in\nproportion 1:2:10:2:1. Your friend picks one at random and rolls it twice getting 5 both\ntimes.\n(a) What is the probability your friend picked the 8-sided die?\nSolution: The data is 5. Let Hn be the hypothesis the die is n-sided. Here is the update\ntable.\nhyp.\nprior\nlikelihood\nBayes numer.\nposterior\nH4\nH6\nH8\nH12\nH20\n(1/6)2\n(1/8)2\n(1/12)2\n(1/20)2\n2/36\n10/64\n2/144\n1/400\n0.243457\n0.684723\n0.060864\n0.010956\nTot.\n0.22819\nSo P (H8|data) = 0.685.\n(b) (i) What is the probability the next roll will be a 5?\n(ii) What is the probability the next roll will be a 15?\nSolution: We are asked for posterior predictive probabilities. Let x be the value of the\nnext roll. We have to compute the total probability\np(x|data) = ∑p(x|H)p(H|data) = ∑ likelihood × posterior.\nThe sum is over all hypotheses. We can organize the calculation in a table where we multiply\nthe posterior column by the appropriate likelihood column. The total posterior predictive\nprobability is the sum of the product column.\n\nExam 2 Practice 2, Spring 2022\nhyp.\nposterior\nto data\nlikelihood\n(i) x = 5\npost. to (i)\nlikelihood\n(ii) x = 15\npost. to (ii)\nH4\nH6\nH8\nH12\nH20\n0.243457\n0.684723\n0.060864\n0.010956\n1/6\n1/8\n1/12\n1/20\n0.04058\n0.08559\n0.00507\n0.00055\n1/20\n0.00055\nTot.\n0.22819\n0.13179\n0.00055\nSo, (i) p(x = 5|data) = 0.132 and (ii) p(x = 15|data) = 0.00055.\nProblem 12.\nSameer has two coins: one fair coin and one biased coin which lands heads with probability\n3/4. He picks one coin at random (50-50) and flips it repeatedly until he gets a tails.\nAssume that he observes 3 heads before the first tails.\n(a) What are the prior and posterior odds for the fair coin?\nP (fair)\n1/2\nSolution: The prior odds are O(fair) = P (not fair) = 1/2 = 1.\nThe posterior odds are the product of the prior odds and the Bayes factor (likelihood ratio).\nThe data is HHHT . So, the Bayes factor is\nP (data | fair)\n(1/2)3(1/2)\nBF = P (data | unfair) = (3/4)3(1/4) = 27.\nSo the posterior odds O(fair | not fair) = O(fair) ⋅BF = 1 ⋅\n= 27 ≈0.593.\n(b) What are the prior and posterior predictive probabilities of heads on the next flip? Here\nprior predictive means prior to considering the data of the first four flips.\nSolution: The prior predictive probability of heads is\nP (fair)P (heads | fair) + P(3/4 coin)P (heads | 3/4 coin) = 0.5 ⋅0.5 + 0.5 ⋅0.75 = 0.625\nThe posterior predictive probability of heads is\nP (fair | data)P (heads | fair) + P(3/4 coin | data)P (heads | 3/4 coin)\nFrom part (a), we have the posterior odds the coin is fair are 16/27. This tells us the\nposterior probabilities are\nP (fair | data) =\nP(unfair | data) =\n43,\nIn this problem, the coin is unfair is the same as the coin has a 0.75 probability of heads.\nSo, the posterior predictive probability of heads is 16 ⋅0.5 + 27 ⋅0.75 ≈0.657\nMethod 2: We can also find the posterior probabilities needed to make the prediction\nusing a Bayesian update table.\nLet θ be the probability of the selected coin landing on heads. We have two hypothese:\nθ = 1/2 and θ = 3/4\n\nExam 2 Practice 2, Spring 2022\nHyp.\nPrior\nLikelihood\nBayes numer.\nPosterior\nθ = 1/2\n1/2\n(1/2)3(1/2)\n1/25\n16/43\nθ = 3/4\n1/2\n(3/4)3(1/4)\n33/(2 ⋅ 44)\n27/43\nTotal\n-\n43/512\nThese are the same probabilities we got using odds. So, they will give the same posterior\nprediction for heads on the next toss.\n6 Bayesian Updating: continuous prior, discrete likelihood\nProblem 13. Peter and Jerry disagree over whether 18.05 students prefer Bayesian or\nfrequentist statistics. They decide to pick a random sample of 10 students from the class\nand get Shelby to ask each student which they prefer. They agree to start with a prior\nf(θ) ∼ Beta(2, 2), where θ is the percent that prefer Bayesian.\n(a) Let x1 be the number of people in the sample who prefer Bayesian statistics. What is\nthe pmf of x1?\nSolution: x1 ∼ Bin(10, θ).\n(b) Compute the posterior distribution of θ given x1 = 6.\nSolution: We have prior:\nf(θ) = c1θ(1 - θ)\nand likelihood:\np(x1 = 6 | θ) = c2θ6(1 - θ)4,\nwhere\nc2 = (10\n6 ).\nThe Bayes numerator is f(θ)p(x1|θ) = c1c2θ7(1 - θ)5. So the normalized posterior is\nf(θ|x1) = c3θ7(1 - θ)5\nSince the posterior has the form of a Beta(8, 6) distribution it must be a Beta(8, 6) distri\n13!\nbution. We can look up the normalizing coefficient c3 = 7! 5! .\n(c) Use R to compute 50% and 90% probability intervals for θ. Center the intervals so that\nthe leftover probability in both tails is the same.\nSolution: The 50% interval is\n[qbeta(0.25,8,6),\nqbeta(0.75,8,6)] = [0.48330,\n0.66319]\nThe 90% interval is\n[qbeta(0.05,8,6),\nqbeta(0.95,8,6)] = [0.35480,\n0.77604]\n(d) The maximum a posteriori (MAP) estimate of θ (the peak of the posterior) is given by\nθ = 7/12, leading Jerry to concede that a majority of students are Bayesians. In light of\nyour answer to part (c) does Jerry have a strong case?\nSolution: If the majority prefer Bayes then θ > 0.5. Since the 50% interval includes θ < 0.5\nand the 90% interval covers a lot of θ < 0.5 we don't have a strong case that θ > 0.5.\nAs a further test we compute P (θ < 0.5|x1) = pbeta(0.5,8,6) = 0.29053. So there is\nstill a 29% posterior probability that the majority prefers frequentist statistics.\n\nExam 2 Practice 2, Spring 2022\n(e) They decide to get another sample of 10 students and ask Neil to poll them. Write\ndown in detail the expression for the posterior predictive probability that the majority of the\nsecond sample prefer Bayesian statistics. The result will be an integral with several terms.\nDon't bother computing the integral.\nSolution: Let x2 be the result of the second poll. We want p(x2 > 5|x1). We can compute\nthis using the law of total probability:\np(x2 > 5|x1) = ∫\np(x2 > 5|θ)p(θ|x1) dθ.\nThe two factors in the integral are:\np(x2 > 5|θ) = (10\n6 )θ6(1 - θ)4 + (10\n7 )θ7(1 - θ)3 + (10\n8 )θ8(1 - θ)2\n+ (10\n9 )θ9(1 - θ)1 + (10\n10)θ10(1 - θ)0\n13!\np(θ|x1) = 7!5!θ7(1 - θ)5\nThis can be computed exactly or numerically in R using the integrate() function. The\nanswer is P (x2 > 5 |x1 = 6) = 0.5521.\nProblem 14. Coins\nWe have a 'bent' coin with an unknown probability θ of heads. Assume the following:\n- Prior for the value of θ: f(θ) = 2(1 - θ) on [0, 1].\n- Data: toss once and get tails.\n(a) Find the posterior pdf to this data.\nSolution: Here's the update table.\nBayes\nhypoth.\nprior\nlikelihood\nnumerator\nposterior\nθ\n2(1 - θ) dθ\n1 - θ\n2(1 - θ)2 dθ\n3(1 - θ)2 dθ\nTotal\n= ∫\nT\n0 2(1 - θ)2 dθ = 2/3\nPosterior pdf: f(θ|x) = 3(1 - θ)2. (Graph below.)\nNote: We don't really need to compute T . Once we know the posterior density is of the\nform cθ2 we only have to find the value of c which makes it have total probability 1.\n(b) Suppose you toss again and get tails. Update your posterior from part (a) using this\ndata.\nSolution: We use the posterior from part (a) as the prior for this part. Here's the table.\nhypoth.\nprior\nlikelihood\nBayes\nnumerator\nposterior\nθ\n3(1 - θ)2 dθ\n1 - θ\n3(1 - θ)3, dθ\n4(1 - θ)3 dθ\nTotal\n∫\n0 3(1 - θ)3 dθ = 3/4\n\nExam 2 Practice 2, Spring 2022\nPosterior pdf: f(θ|x) = (1 - θ)3.\n(c) On one set of axes graph the prior and the posteriors from parts (a) and (b).\n(c) Solution: Here is the plot of the prior and the two posteriors.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nθ\nPrior, posterior a, posterior b\nPrior\nPosterior a\nPosterior b\nProblem 15. Take your medicine\nA lab has an experimental treatment for a disease. The treatment will cure an unknown\nfraction θ of the patients it's used on. Because it is brand new, they have no idea what θ is,\nso they use a flat prior f(θ) = 1.\nIn a small preliminary study, the treatment cured 16 out of 20 patients.\nUse this data to find the posterior pdf for θ.\nWrite an integral formula for the normalizing factor (total probability of the data), but do\nnot compute it. Call its value T and give the posterior pdf in terms of T .\nSolution: Here's the update table.\nhypoth.\nprior\nlikelihood\nBayes\nnumerator\nposterior\nθ\n1 ⋅ dθ\n(20\n16)θ16(1 - θ)4\n(20\n16)θ16(1 - θ)14 dθ\ncθ16(1 - θ)4 dθ\nTotal\n0 (20\n= ∫\nT\n16)θ16(1 - θ)4 dθ\n(20\n16)\nSo, f(θ|x) = cθ16(1 - θ)4, where c =\n.\nT\nA computation (or Wikipedia) would show c = 16! 4!\n(This is called a Beta distribution.) Here is a plot of the prior and posterior\n\nExam 2 Practice 2, Spring 2022\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nθ\nPrior and posterior\nPrior\nPosterior\n7 Bayesian Updating: normal-normal conjugate pairs\nProblem 16. Suppose that you have a cable whose exact length is θ. You have a ruler\nwith known error normally distributed with mean 0 and variance 10-4. Using this ruler, you\nmeasure your cable, and the resulting measurement x is distributed as N(θ, 10-4).\n(a) Suppose your prior on the length of the cable is θ ∼ N(9, 1). If you then measure\nx = 10, what is your posterior pdf for θ?\nSolution: We have μprior = 9, σ2\n= 1 and σ2 = 10-4. The normal-normal updating\nprior\nformulas are\nn\naμprior + bx\na=\nb=\n,\nσ2\nμpost =\npost =\nσ2\nσ2 ,\na+ b\na+ b .\nprior\nSo we compute a = 1/1, b = 10000, σpost\n= 1/(a + b) = 1/10001 ≈ 9.999 × 10-5 and\naμprior + bx\nμpost =\n= 10001 ≈ 9.9999\na + b\nSo we have posterior distribution f(θ|x = 10) ∼ N(9.9999, 9.999 × 10-5).\n(b) With the same prior as in part (a), compute the total number of measurements needed\nso that the posterior variance of θ is less than 10-6.\nSolution: We have σ2\n= 1 and σ2 = 10-4. The posterior variance of θ given observations\nprior\nx1, ... , xn is given by\n=\n+ n\n1 + n ⋅ 104\nσ2\nσ2\nprior\nWe wish to find n such that the above quantity is less than 10-6. It is not hard to see that\nn = 100 is the smallest value such that this is true.\n\nExam 2 Practice 2, Spring 2022\n8 NHST\nProblem 17. z-test\nSuppose we have 49 data points with sample mean 6.25 and sample variance 12. We want\nto test the following hypotheses\nH0: the data is drawn from a N(4, 102) distribution.\nHA: the data is drawn from N(μ, 102) where μ = 4.\n(a) Test for significance at the α = 0.05 level. Use the tables at the end of this file to\ncompute p-values.\nSolution: Our z-statistic is\n\n6.25 - 4\nx - μ\nz=\n= 1.575\nσ/√n =\n10/7\nUnder the null hypothesis z ∼ N(0, 1) The two-sided p-value is\np= 2 × P(Z > 1.575) = 2 × 0.0576 = 0.1152\nThe probability was computed from the z-table. We interpolated between z = 1.57 and\nz = 1.58 Because p > α we do not reject H0.\n(b) Draw a picture showing the null pdf, the rejection region and the area used to compute\nthe p-value.\nSolution: The null pdf is standard normal as shown. The orange shaded area is over the\nrejection region. The area used to compute significance is shown in orange. The area used\nto compute the p-value is shown with blue stripes. Note, the z-statistic outside the rejection\nregion corresponds to the blue completely covering the orange.\nz\nφ(z|H0) ∼N(0, 1)\n-1.96\n1.96\nreject H0\nreject H0\ndo not reject H0\nz= 1.575\nProblem 18. t-test\nSuppose we have 49 data points with sample mean 6.25 and sample variance 36. We want\nto test the following hypotheses:\n(a) H0: the data is drawn from N(4, σ2), where σ is unknown.\nHA: the data is drawn from N(μ, σ2) where μ = 4.\nTest for significance at the α = 0.05 level. Use the t-table to find the p value.\nSolution: Our t-statistic is\n\n6.25 - 4\nx - μ\n= 2.625\ns/√n =\n6/7\n\nExam 2 Practice 2, Spring 2022\nUnder the null hypothesis t ∼ t48. Using the t-table we find the two-sided p-value is\np = 2 × P(t > 2.625) < 2 × 0.005 = 0.01\nBecause p < α we reject H0.\n(b) Draw a picture showing the null pdf, the rejection region and the area used to compute\nthe p-value for part (a).\nSolution: The null pdf is a t-distribution as shown. The rejection region is shown. The\narea used to compute significance is shown in orange. The area used to compute the p-value\nis shown with blue stripes. Note, the t-statistic is inside the rejection region corresponds.\nThis corresponds to the orange completely covering the blue. The critical values for t48\nwe're looked up in the table.\nz\nφ(t|H0) ∼t48\n-2.011\n2.011\nreject H0\nreject H0\ndo not reject H0\nt= 2.625\nProblem 19. There are lots of good NHST problems in psets 7 and 8, the reading and\nin-class problems, including two-sample t test, chi-square, ANOVA, and F-test for equal\nvariance.\nSolution: See the psets 7 and 8.\nProblem 20. Probability, MLE, goodness of fit\nThere was a multicenter test of the rate of success for a certain medical procedure. At each\nof the 60 centers the researchers tested 12 subjects and reported the number of successes.\n(a) Assume that θ is the probability of success for one patient and let x be the data from\none center. What is the probability mass function of x?\nSolution: This is a binomial distribution. Let θ be the Bernoulli probability of success in\none test.\np(x = k) = (12\nk )θk(1 - θ)12-k, for k = 0, 1, ... , 12.\n(b) Assume that the probability of success θ is the same at each center and the 60 centers\nproduced data:\nx1, x2, ... , x60. Find the MLE for θ. Write your answer in terms of x\nSolution: The likelihood function for the combined data from all 60 centers is\n(12\np(x1, x2, ... , x60 | θ) =\n)θx1(1 -θ)12-x1(12)θx2(1 -θ)12-x2 ⋯( 12 )θx60(1 -θ)12-x60\nx1\nx2\nx60\n= cθ∑xi(1 -θ)∑12-xi\n\nExam 2 Practice 2, Spring 2022\nTo find the maximum we use the log likelihood. At the same time we make the substitution\n60 ⋅ x for ∑ xi.\nln(p(data | θ)) = ln(c) + 60x ln(θ) + 60(12 - x) ln(1 - θ).\nNow we set the derivative to 0:\nd ln(p(data | θ))\n60x - 60(12 - x)\n=\n= 0.\ndθ\nθ\n1 -θ\nSolving for θ we get\nx\nθ = 12.\nParts (c-e) use the following table which gives counts from 60 centers, e.g. x = 2 occurred\nin 17 out of 60 centers.\nx\ncounts\nNote, the possible values of x are 0 to 12. The table shows that x > 5 never occurred.\n(c) Compute x the average number of successes over the 60 centers.\nSolution: The sample mean is\n∑(count × x)\nx =\n∑ counts\n4 ⋅ 0 + 15 ⋅ 1 + 17 ⋅ 2 + 10 ⋅ 3 + 8 ⋅ 4 + 6 ⋅ 5\n=\n= 2.35\n(d) Assuming the probability of success at each center is the same, show that the MLE for\nθ is θ = 0.1958.\nSolution: Just plug x = 2.35 into the formula from part (b): θ = x/12 = 2.35/12 = 0.1958\n\n(e) Do a χ2 goodness of fit to test the assumption that the probability of success is the same\nat each center. Find the p-value and use a significance level of 0.05.\nIn this test the number of degrees of freedom is the number of bins - 2.\nSolution: There were 60 trials in all. Our hypotheses are:\nH0 = the probability of success is the same at all centers. (This determines the probabilities\nof the counts in each cell of our table.)\nHA = the probabilities for the cell counts can be anything as long as they sum to 1, i.e. x\nfollows an arbitrary multinomial distribution.\nUsing the the value for θ in part (d) we have the table below. Here are some details of the\ncomputation\nSince, in principle, x can take any values between 0 and 12, the last cell in the counts is\nreally for x ≥ 5. Thus, the probabilities for x = 0, 1, 2, 3, 4 are computed using R by p(x) =\ndbinom(x, 12, 0.1958). The last probability for x ≥ 5 is computed by sum(dbinom(5:12,\n12, 0.1058)). (It could have been computed using pbinom.)\n\nExam 2 Practice 2, Spring 2022\nThe expected counts are just the probabilities times the number of centers, 60. The com\nponents of X2 are computed using the formula Xi\n2 = (Ei - Oi)2/Ei.\nx\n≥5\nprob\n0.0731\n0.2137\n0.2863\n0.2324\n0.1273\n0.0671\nObserved\nExpected\n4.39\n12.82\n17.18\n13.94\n7.64\n4.03\nXi\n0.0344\n0.3692\n0.0018\n1.1149\n0.0170\n0.9643\nThe χ2 statistic is X2 = ∑Xi\n2 = 2.502. There are 6 cells, so 4 degrees of freedom. The\np-value is\np = 1-pchisq(2.502, 4) = 0.644\nWith this p-value we do not reject H0.\nThe reason the degrees of freedom is two less than the number of cells is that there are two\nconstraints on assigning cell counts assuming HA but consistent with the statistics used to\ncompute the expected counts. They are the total number of observations = 60, and the\ngrand mean x = 2.35.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Practice Exam 2b Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_prac_exam02b_sol.pdf",
      "content": "18.05 Practice Exam 2b Solutions\nProblem 1. Concept questions\n(a) A certain august journal publishes psychological research. They will only publish results\nthat are statistically significant when tested at a significance level of 0.05.\nCould all of their published results be true?\nYes\nNo\nCould all of their published results be false?\nYes\nNo\n(b) True or false: Setting the prior probability of a hypothesis to 0 means that no amount of\ndata will make the posterior probability of that hypothesis the maximum over all hypotheses.\nTrue\nFalse\n(c) A researcher collected data that fit the criteria for a two-sided Z-test. He set the\nsignificance level at 0.05. He ran 80 trials and got a z-value of 1.7. This gave a p-value of\n0.0892, so he could not reject the null hypothesis. Convinced that his alternative hypothesis\nwas correct he ran 80 more trials. The combined data from the 160 trials now had a z-value\nof 2.1. He wrote a paper carefully describing his experiments and submitted it to the journal\nin part (a).\nWill the journal publish his results?\nYes\nNo\n(d) Let θ be the probability of heads for a bent coin. Suppose your prior f(θ) is Beta(6, 8).\nAlso suppose you flip the coin 7 times, getting 2 heads and 5 tails. What is the posterior\npdf f(θ|x)?\nAnswers. (a) Yes and yes. Frequentist statistics don't give the probability an hypothesis\nis true.\n(b) True. Bayesian updating involves multiplying the likelihood and the prior. If the prior\nis 0 then this product will be 0.\n(c) No. The actual experiment that was run would reject the null hypothesis if it were true,\nmore than 5% of the time.\n(d) Beta(8,13).\nProblem 2. The Pareto distribution with parameter α has range [1, inf) and pdf\nα\nf(x) = xα\nSuppose the data\n5, 2, 3\nwas drawn independently from such a distribution. Find the maximum likelihood estimate\n(MLE) of α.\nα\nα\nα\nα3\nSolution: likelihood = φ(data | α) = 5α ⋅ 2α ⋅ 3α = 30α.\n\n18.05 Practice Exam 2b Solutions\nTherefore, log likelihood = ln(φ(data | α)) = 3 ln(α) - α ln(30). We find the maximum\nlikelihood by setting the derivative equal to 0:\ndα\nd ln(φ(data | α)) = α\n3 - ln(30) = 0.\nSolving we get α = ln(30) .\nProblem 3.\nYour friend grabs a die at random from a drawer containing two 4-sided dice, one 8-sided\ndie, and one 12-sided die. They roll the die once and report that the result is 5.\n(a) Make a discrete Bayes table showing the prior, likelihood, and posterior for the type of\ndie rolled given the data.\nSolution: (We include the last column for part (d).)\nhypoth.\nθ\nprior\nlikelihood\nP (x1 = 5 | θ)\nBayes\nnumer.\nposterior\nlikelihood\nP (x2 = 7 | θ)\n4-sided\n8-sided\n12-sided\n1/2\n1/4\n1/4\n1/8\n1/12\n1/32\n1/48\n1/8\n1/12\nT =\n=\n32 + 1\nNote, the posterior probabilites have to have the same ratio as the Bayes numerators, i.e.\n0 ∶\n∶\n∶\n∶5 = 0 ∶3 ∶2.\n48 = 0 5\n(b) What is the prior predictive probability of rolling a 5?\nSolution: P(x1 = 5) = T = 32\n1 + 48\n1 = 96\n5 ≈ 0.0521. (Any of these expressions is a fine\nanswer.)\n(c) What are your posterior odds that the die has 12 sides?\nP(θ = 12 | x1 = 5)\n2/5\nSolution: Odds(θ= 12 | x1 = 5) =\n3/5 = 2/3.\nP(θ=12 | x1 = 5) =\n(d) Given the data of the first roll, what is your probability that the next roll will be a 7?\nSolution: Here we use the posterior probabilities of the for each of the dice to compute\nthe predictive probability\nP(x2 = 7 | x1 = 5) = 0 ⋅0 + 3 ⋅8 + 2 ⋅\n120 ≈0.108\n5 12 =\n(Any of these answers is okay.)\nProblem 4. Everyone knows that giraffes are tall, but how much do they weigh? Let's\nsuppose that the weight of male giraffes is normally distributed with mean 1200 kg and\nstandard deviation 200 kg.\nI volunteered at the zoo and was given the task of weighing their male giraffe Beau. Now\nweighing a giraffe is not easy and the process produces random errors following a N(0, 1002)\ndistribution. To compensate for the inaccuracy of the scale I weighed Beau three times and\ngot the following measurements:\n\n18.05 Practice Exam 2b Solutions\n1250 kg, 1300 kg, 1350 kg\n.\nWhat is the posterior expected value of Beau's weight?\nSolution:\nThis is a normal/normal conjugate prior pair, so we use the normal-normal\nupdate formulas.\nμ = Beau's weight.\nn = 3,\nx = 1300\nPrior ∼ N(1200, 2002), so\n= 1200,\nσ2\n.\nμprior\nprior = 2002\nLikelihood ∼ N(μ | 1002), so\nσ2 = 1002.\nn\na=\n=\nb=\n=\nσ2\n2002 ,\nσ2\n1002 .\npr\na ⋅ μprior + b ⋅ x\n2002 ⋅ 1200 + 1002 ⋅ 1300\n=\n=\nμposterior\na + b\n1/2002 + 3/1002\nProblem 5. Data is drawn from a binomial(5, θ) distribution, where θ is unknown. Here\nis the table of probabilities p(x | θ) for 3 values of θ:\nx\nθ = 0.5\n0.031\n0.156\n0.313\n0.313\n0.156\n0.031\nθ = 0.6\n0.010\n0.077\n0.230\n0.346\n0.259\n0.078\nθ = 0.8\n0.000\n0.006\n0.051\n0.205\n0.410\n0.328\nYou want to run a significance test on the value of θ. You have the following:\nNull hypothesis: θ = 0.5.\nAlternate hypotheses: θ > 0.5.\nSignificance level: α = 0.1.\n(a) Find the rejection region.\n(b) Compute the power of the test for each of the two hypotheses θ = 0.6 and θ = 0.8.\n(c) Suppose you run an experiment and the data gives x = 4. Compute the p-value of this\ndata.\nAnswers. (a) Since the HA is right-sided we use a right-sided rejection region: rejection region x = 5 .\nx\nθ = 0.5\n0.031\n0.156\n0.313\n0.313\n0.156\n0.031\nθ = 0.6\n0.010\n0.077\n0.230\n0.346\n0.259\n0.078\nθ = 0.8\n0.000\n0.006\n0.051\n0.205\n0.410\n0.328\n(b) Power = P (reject | θ).\nθ = 0.6: power = 0.078\nθ = 0.8: power = 0.328\n(c) p= P(x≥4 | θ= 0.5) = 0.156 + 0.031 = 0.187.\nProblem 6. You have data drawn from a normal distribution with a known variance of\n16. You set up the following NHST:\n- H0: data follows a N(2, 42)\n\n18.05 Practice Exam 2b Solutions\n- HA: data follows a N(μ, 42) where μ = 2.\n- Test statistic: standardized sample mean z.\n- Significance level set to α = 0.05.\nYou then collected n = 16 data points with sample mean 1.5.\n(a) Find the rejection region. Draw a graph indicating the null distribution and the rejection\nregion.\n(b) Find the z-value and add it to your picture in part (a).\n(c) Find the p-value for this data and decide whether or not to reject H0 in favor of HA.\nAnswers. (a) The test statistic is z, so we need a Z-graph\nz\nφ(z|H0)\nz.975 = -1.96\nz.025 = 1.96\nreject H0\nreject H0\ndon't reject H0\n.025\n.025\nz(part (b))\nThe rejection region is z < -1.96 or z > 1.96.\nx-2\n1.5 -2\n(b) We standardize x to get z: z=\n=\n= -0.5\nσx\n4/\n√\n(c) p = 2P (Z ≤ -0.5) = 2 ⋅ (0.3085) = 0.6170. Since p > 0.05 we do not reject H0.\nProblem 7. Someone claims to have found a long lost work by Jane Austen. She asks you\nto decide whether or not the book was actually written by Austen.\nYou buy a copy of Sense and Sensibility and count the frequencies of certain common words\non some randomly selected pages. You do the same thing for the 'long lost work'. You get\nthe following table of counts.\nWord\na\nan\nthis\nthat\nSense and Sensibility\nLong lost work\nUsing this data, set up and evaluate a significance test of the claim that the long lost book\nis by Jane Austen. Use a significance level of 0.1.\nSolution: The null hypothesis H0: For the 4 words counted the long lost book has the\nsame relative frequencies as Sense and Sensibility\nTotal word count of both books combined is 500, so the maximum likelihood estimate of\nthe relative frequencies assuming H0 is simply the total count for each word divided by the\ntotal word count.\n\n18.05 Practice Exam 2b Solutions\nWord\na\nan\nthis\nthat\nTotal count\nSense and Sensibility\nLong lost work\ntotals\nrel. frequencies under H0\n240/500\n50/500\n40/500\n170/500\n500/500\nNow the expected counts for each book under H0 are the total count for that book times\nthe relative frequencies in the above table. The following table gives the counts: (observed,\nexpected) for each book.\nWord\na\nan\nthis\nthat\nTotals\nSense and Sensibility\n(150, 144)\n(30, 30)\n(30, 24)\n(90, 102)\n(300, 300)\nLong lost work\n(90, 96)\n(20, 20)\n(10, 16)\n(80, 68)\n(200, 200)\nTotals\n(249, 240)\n(50, 50)\n(40, 40)\n(170, 170)\n(500, 500)\nThe chi-square statistic is\n= ∑ (Oi - Ei)2\nX2\nEi\n=\n24 + 122\n16 + 122\n144 + 30\n+ 62\n102 + 62\n20 + 62\n96 + 02\n≈ 7.9\nThere are 8 cells and all the marginal counts are fixed, so we can freely set the values in 3\ncells in the table, e.g. the 3 blue cells, then the rest of the cells are determined in order to\nmake the marginal totals correct. Thus df = 3.\nLooking in the df = 3 row of the chi-square table we see that X2 = 7.9 gives p between 0.025\nand 0.05. Since this is less than our significance level of 0.1 we reject the null hypothesis that\nthe relative frequencies of the words are the same in both books. Based on the assumption\nthat all her books have similar word frequencies (which is something we could check) we\nconclude that the book is probably not by Jane Austen.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class 01 Slides: Introduction, Counting, and Sets",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_lec01.pdf",
      "content": "Welcome to 18.05\nIntroduction to Probability and Statistics\nSpring 2022\nhttps://xkcd.com/904/\n1/32\nImage courtesy of xkcd. License: CC BY-NC.\n\nAgenda\n- Introductions and class description\n- Administrative stuff\n- Begin probability: sets and counting\n2/32\n\nWebsites\nYou should have received this information in an email.\nIf not let us know.\n- MITx:\n- If you registered for 18.05 you should be able to see the class.\n- Site will have all reading materials, problem sets, etc.\n- Lecture slides and problems will be posted before class. Solutions right\nafter class.\n- Canvas: Has links to everything.\n- Gradescope: For turning in psets and quizzes\n- Piazza: Sign up - there is a link on our Canvas site.\nFor asynchronous help: everyone can ask and answer questions. The\nteaching staff will also monitor the site and respond to questions.\n3/32\n\nActive Learning\nRead the 'Calendar and Information' section on our MITx site.\nAll the educational research shows two things\n- Better and more long lasting educational gains.\n- Many students doubt this at first.\nBefore class\n- Reading and reading questions.\n- Reading questions count toward grade.\n- Lecture will assume you've done the reading.\nIn class:\n- Combination of lecture and problem solving\n- We won't assume you've completely mastered the reading.\n- Will assume a level of familiarity.\n- Use the Piazza discussion board - links on MITx and Canvas.\n- Bring questions to class.\n4/32\n\nClass\nRead the 'Calendar and Information' section on our MITx/18.05r site.\nClass Time\n- TR: Lecture/concept (clicker) questions/board questions\n- Participation on clicker questions counts towards your grade\n- Will use MITx for clicker questions -requires a computer or phone.\n- No computer or phone use -except for clicker questions- in class on\nTR.\n- F: Studio - bring your laptop\nIn-class Groups\n- Groups of 3.\n- You will be able to choose your own group.\n- If you need to find a group or your group needs a third person let us\nknow and we'll help.\nR: for computation, simulation and visualization\n- will teach you everything you need\n- no hardcore programming.\n5/32\n\nProblem Sets\n- Usually due on Mondays\n- Turn in to Gradescope by 10 PM\n- You'll be able to check your numerical answers to\nproblems on our MITx site before the due date.\n- Problem sets will be graded on the logic and\nexplanation of your answer.\n6/32\n\nR, Piazza\nR\n- Free open source package.\n- Very easy to use and install.\n- Instructions and a link for this are on MITx/18.05r.\nPiazza\n- We will use the Piazza discussion forum.\n- Mostly for students to ask questions of each other.\n- Sign up by following the link from our MITx site.\n7/32\n\nCalendar, Information, Policies and Goals\nEverything we just went over and more is in the\nCalendar and Information section of MITx/18.05r\n8/32\n\nFor Next Time\n- Familiarize yourself with the MITx/18.05r site\n- Install R and R Studio\n- Sign up for Piazza and join our class. (Link on our\nCanvas site)\n- Read class 1 notes (a summary of what we'll do today)\n- Go through the class 2 sequence and answer the\nreading questions\n9/32\n\nPlatonic Dice\n4, 6, 8, 12, 20-sided\n10/32\n\nProbability vs. Statistics\nDifferent subjects: both about random processes\nProbability\n- Logically self-contained\n- A few rules for computing probabilities\n- One correct answer\nStatistics\n- Messier and more of an art\n- Seek to make probability based inferences from\nexperimental data\n- No single correct answer\n11/32\n\nCounting: Motivating Examples\nWhat is the probability of getting exactly 1 heads in 3\ntosses of a fair coin?\n12/32\n\nPoker Hands\nDeck of 52 cards\n- 13 ranks: 2, 3, ... , 9, 10, J, Q, K, A\n- 4 suits: ♡, ♠, ♢, ♣,\nPoker hands\n- Consists of 5 cards\n- A one-pair hand consists of two cards having one rank and the\nremaining three cards having three other ranks\n- Example: {2♡, 2♠, 5♡, 8♣, K♢}\n13/32\n\nConcept Question 1\nA one-pair hand consists of two cards having one rank and the\nremaining three cards having three other ranks\nThe probability of a one-pair hand is:\n(1) less than 5%\n(2) between 5% and 10%\n(3) between 10% and 20%\n(4) between 20% and 40%\n(5) greater than 40%\n14/32\n\nSets in Words\nOld New England rule: don't eat clams (or any shellfish) in months\nwithout an 'r' in their name.\n- S = all months\n- L = the month has 31 days\n- R = the month has an 'r' in its name\nS = {Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec}\nL = {Jan, Mar, May, Jul, Aug, Oct, Dec}\nR = {Jan, Feb, Mar, Apr, Sep, Oct, Nov, Dec}\nL ∩ R = {Jan, Mar, Oct, Dec} = months with 31 days and an 'r'\n15/32\n\nVisualize Set Operations with Venn Diagrams\nS\nL\nR\nL∪R\nL∩R\nLc\nL-R\n16/32\n\nProduct of Sets\nS× T = {pairs (s, t) with s in S, t in T }\nMore simply: S × T = {(s, t)}\nSIZE of S× T = (size of S) ⋅ (size of T )\n|S × T | = |S| ⋅|T|.\n17/32\n\nInclusion-Exclusion Principle\nS\nA\nB\nA∩B\n|A∪B| = |A| + |B| -|A∩B|\n18/32\n\nBoard Question 1\nA band consists of singers and guitar players:\n7 people sing, 4 play guitar, 2 do both\nHow many people are in the band?\n19/32\n\nRule of Product\nExample\n3 shirts, 4 pants = 12 outfits\n(set of shirts) × (set of pants) = set of outfits\n|S| ⋅|T| = |S × T |\n(More powerful than it seems.)\n20/32\n\nConcept Questions: DNA\n1. DNA is made of sequences of nucleotides: A, C, G, T.\nHow many DNA sequences of length 3 are there?\n(i) 12\n(ii) 24\n(iii) 64\n(iv) 81\n21/32\n\nConcept Questions: DNA\n1. DNA is made of sequences of nucleotides: A, C, G, T.\nHow many DNA sequences of length 3 are there?\n(i) 12\n(ii) 24\n(iii) 64\n(iv) 81\n2. How many DNA sequences of length 3 are there with\nno repeats?\n(i) 12\n(ii) 24\n(iii) 64\n(iv) 81\n21/32\n\nBoard Question 2\nThere are 5 Competitors in an Olympics 100m final.\nHow many ways can gold, silver, and bronze be awarded?\n22/32\n\nBoard Question 3\nI won't wear green and red together; I think black or\ndenim goes with anything; Here is my wardrobe.\nShirts: 3B, 3R, 2G; sweaters 1B, 2R, 1G; pants 2D,2B.\nHow many different outfits can I wear?\n23/32\n(c) Source unknown. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see https://ocw.mit.edu/help/faq-fair-use.\n\nSolution\nSolution: Suppose we choose shirts first. Depending on whether we\nchoose red compatible or green compatible shirts there are different\nnumbers of sweaters we can choose next. So we split the problem up\nbefore using the rule of product. A multiplication tree is an easy way to\npresent the answer.\nR\nB\nG\nR,B\nR,B,G\nB,G\nB, D\nB, D\nB, D\nShirts\nSweaters\nPants\nMultiplying down the paths of the tree:\nNumber of outfits = (3 × 3 × 4) + (3 × 4 × 4) + (2 × 2 × 4) = 100\n24/32\n\nPermutations\nLining things up. How many ways can you do it?\n'abc' , 'cab' are 2 of the 6 permutations of {a, b, c}\n'ad' , 'da' , 'bc' are three of the twelve\npermutations of two things from {a,b,c,d}\n25/32\n\nPermutations of k from a set of n\nGive all permutations of 3 things out of {a, b, c, d}\n26/32\n\nPermutations of k from a set of n\nGive all permutations of 3 things out of {a, b, c, d}\nabc abd acb acd adb adc\nbac bad bca bcd bda bdc\ncab cad cba cbd cda cdb\ndab dac dba dbc dca dcb\nWould you want to do this for 7 from a set of 10?\n26/32\n\nCombinations\nChoosing subsets - order doesn't matter.\nHow many ways can you do it?\n27/32\n\nCombinations of k from a set of n\nGive all combinations of 3 things out of {a, b, c, d}\nAnswer: {a,b,c}, {a,b,d}, {a,c,d}, {b,c,d}\n28/32\n\nPermutations and Combinations\nnPk = number of permutations (ordered lists)\nof k things from n\n= (n\nnCk\nk) = number of combinations (subsets)\nof k things from n\nn!\n(n\n= nPk\nn!\nnPk =\n=\n(n - k)!\nk) = nCk\nk!\n(n - k)! k!\nProof: Rule of product!\n29/32\n\nPermutations and Combinations\nabc acb bac bca cab cba\n{a, b, c}\nabd adb bad bda dab dba\n{a, b, d}\nacd adc cad cda dac dca\n{a, c, d}\nbcd bdc cbd cdb dbc dcb\n{b, c, d}\nPermutations:\nCombinations:\n4P3\n(4\n3) = 4C3\n4!\n(4\n3) = 4C3 = 4P3 =\n3!\n3! 1!\n30/32\n\nBoard Question 4\n(a) Count the number of ways to get exactly 3 heads in\n10 flips of a coin.\n(b) For a fair coin, what is the probability of exactly 3\nheads in 10 flips?\n31/32\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit:\nhttps://ocw.mit.edu/terms.\n32/32"
    },
    {
      "category": "Resource",
      "title": "Studio 1: Birthday Matches",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_studio1_slides.pdf",
      "content": "Studio 1: Birthday Matches\n- Read Hints-RStudio.pdf (1 page)\nPay attention to 'Cleaning your environment'\n- Go through studio1-instructions.pdf line by line.\n- Work in studio1.r. No need to rename it. The upload\nsite will add your name and a time stamp.\n- You can upload multiple times. We will grade the last\nupload.\n1/2\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit:\nhttps://ocw.mit.edu/terms.\n2/2"
    },
    {
      "category": "Resource",
      "title": "Class 02 Slides: Probability Basics",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_lec02.pdf",
      "content": "Probability: Terminology and Examples\n18.05 Spring 2022\n1/21\n\nAnnouncements/Agenda\nAnnouncemnets\nFor tomorrow:\n- Do the R tutorial on our MITx site (20-30 minutes).\n- Bring your laptop to class\n- The studio packet will be available on MITx after class today. It\nhas detailed instructions.\n- In class we can clear up any confusing instructions.\nAgenda\n- Probability basics\n- sample space\n- events\n- probability function\n- experiments\n2/21\n\nBoard Question 1\nDeck of 52 cards\n- 13 ranks: 2, 3, ... , 9, 10, J, Q, K, A\n- 4 suits: ♡, ♠, ♢, ♣,\nPoker hands\n- Consists of 5 cards\n- A one-pair hand consists of two cards having one rank and the\nremaining three cards having three other ranks\n- Example: {2♡, 2♠, 5♡, 8♣, K♢}\nQuestion\n(a) How many different 5 card hands have exactly one pair?\nHint: practice with how many 2 card hands have exactly one pair.\nHint for hint: use the rule of product.\n(b) What is the probability of getting a one pair poker hand?\n3/21\n\nProbability Cast\n- Experiment: a repeatable procedure\n- Sample space: set of all possible outcomes S (or Ω).\n- Event: a subset of the sample space.\n- Probability function, P (ω): gives the probability for\neach outcome ω ∈ S\n1. Probability is between 0 and 1\n2. Total probability of all possible outcomes is 1.\n4/21\n\nExample (from the reading)\nCoin tossing experiment\nOne trial: toss a fair coin, report heads or tails.\nSample space: S = {H, T}.\nProbability function: P(H) = 0.5, P(T) = 0.5.\nUse tables to summarize:\nOutcomes\nH\nT\nProbability 1/2 1/2\n(Tables can really help in complicated examples)\n5/21\n\nDiscrete sample space\nDiscrete = listable\nExamples of discrete sample spaces\n{a, b, c, d} (finite)\n{0, 1, 2, ...} (infinite)\n{sun,cloud,rain,snow,fog}\n{patient cured, unchanged, died}\n6/21\n\nEvents\nEvents are sets of outcomes:\n- Can describe in words\n- Can describe in notation\n- Can describe with Venn diagrams\nExample. Experiment: toss a coin 3 times.\nEvent:\nYou get 2 or more heads = { HHH, HHT, HTH, THH}\n7/21\n\nClicker Test\nCQ: Can you connect to respond to clicker\nquestions?\n1. No\n2. yes\n8/21\n\nEvents, sets and words\nConcept question 1: What's the event?\n(Connecting words and set notation.)\nExperiment: toss a coin 3 times.\nWhich of following equals the event \"exactly two heads\"?\nA = {THH, HTH, HHT, HHH}\nB = {THH, HTH, HHT}\nC = {HTH, THH}\n(1) A\n(2) B\n(3) C\n(4) B or C\n9/21\n\nEvents, sets and words\nConcept question 2: Describe the event\n(Connecting words and set notation.)\nExperiment: toss a coin 3 times.\nWhich of the following describes the event\n{T HH, HT H, HHT }?\n(1) \"exactly one head\"\n(2) \"exactly one tail\"\n(3) \"at most one tail\"\n(4) none of the above\n10/21\n\nEvents, sets and words\nConcept question 3: Are they disjoint?\n(Connecting words and set notation.)\nExperiment: toss a coin 3 times.\nThe events \"exactly 2 heads\" and \"exactly 2 tails\" are\ndisjoint is.\n(1) True\n(2) False\n11/21\n\nEvents, sets and words\nConcept question 4: Does A imply B?\n(Connecting words and set notation)\nConsider two events: A and B.\nAre the words \"A implies B\" equivalent to A ⊆ B?\n(1) True\n(2) False\n12/21\n\nProbability rules in mathematical notation\nSample space: S = {ω1, ω2, ... , ωn}\nOutcome: ω ∈ S\nProbability between 0 and 1: 0 ≤ P(ω) ≤ 1\nn\nTotal probability is 1: ∑ P (ωj) = 1,\n∑P(ω) = 1\nj=1\nω∈S\nEvent A: P(A) = ∑P(ω)\nω∈A\n13/21\n\nProbability and set operations on events\nEvents A, L, R\n1. Complements: P (Ac) = 1 - P (A).\n2. Disjoint events: If L and R are disjoint then\nP(L∪R) = P(L) + P(R).\n3. Inclusion-exclusion principle: For any L and R:\nP(L ∪ R) = P(L) + P(R) -P(L ∩ R).\nA\nAc\nΩ = A∪Ac, no overlap\nL\nR\nL∪R, no overlap\nL\nR\nL∪R, overlap = L∩R\n14/21\n\nTable question\n- Class has 50 students\n- 20 male (M), 25 brown-eyed (B)\nFor a randomly chosen student, what is the range of\npossible values for p= P(M ∪B)?\n(a) p ≤ 0.4\n(b) 0.4 ≤ p ≤ 0.5\n(c) 0.4 ≤ p ≤ 0.9\n(d) 0.5 ≤ p ≤ 0.9\n(e) 0.5 ≤ p\n15/21\n\nTable Question\nExperiment:\n1. Your table should make 9 rolls of a 20-sided die (one\neach if the table is full).\n2. Check if all rolls at your table are distinct.\nRepeat the experiment five times and record the results.\n16/21\n\nTable Question\nExperiment:\n1. Your table should make 9 rolls of a 20-sided die (one\neach if the table is full).\n2. Check if all rolls at your table are distinct.\nRepeat the experiment five times and record the results.\nFor this experiment, how would you define the sample\nspace, probability function, and event?\nCompute the true probability that all rolls (in one trial)\nare distinct and compare with your experimental result.\n16/21\n\nPreamble: Jon's dice\nJon has three six-sided dice with unusual numbering.\nA game consists of two players each choosing a die. They\nroll once and the highest number wins.\nWhich die would you choose?\n17/21\n\nBoard Question: Jon's dice\n1. Make probabilitiy tables for the the blue and white dice.\n2. Make a probability table for the product sample space of blue and\nwhite.\n3. Use the table to compute the probability that blue beats white.\n4. Pair up with another group. Have one group compare blue vs.\norange and the other compare orange vs. white. Based on the three\ncomparisons, rank the dice from best to worst.\n18/21\n\nConcept Question\nLucky Lucy has a coin that you're quite sure is not fair.\n- They will flip the coin twice\n- It's your job to say whether it more probable that the\ntosses are the same, i.e. HH or TT, or different, i.e.\nHT or TH.\nWhich should you choose?\n1. More probable they are the same\n2. More probable they are different\n3. Doesn't matter\n4. It depends on the actual probabilities of getting heads\nor tails.\n19/21\n\nBoard Question\nLucky Lucy has a coin that you're quite sure is not fair.\n- They will flip the coin twice\n- Let A be the event the tosses are the same, i.e. {HH, TT}\n- Let B be the event the tosses are the different, i.e. {HT, TH}\nLet p be the probability of heads. Compute and compare P (A) and\nP (B).\n(If you don't see the symbolic algebra try p = 0.2, p=0.5)\n20/21\n\nLicense\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit:\nhttps://ocw.mit.edu/terms.\n21/21"
    },
    {
      "category": "Resource",
      "title": "Class 03 Slides: Conditional Probability, Independence, and Bayes' Theorem",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_lec03.pdf",
      "content": "Conditional Probability, Independence, Bayes' Theorem\n18.05 Spring 2022\n1/23\nThis image is in the public domain.\n\nAnnouncements/Agenda\nAnnouncements\n- Slides and problems are posted before class. Solutions right after\nclass.\n- Make use of office hours.\n- Solution code is posted on MITx.\nAgenda\n- Studio 1 comments\n- Conditional probability\n- Multiplication rule; Law of total probability\n- Bayes' Theorem\n- Base rate fallacy\n2/23\n\nStudio 1\n- Really well done! About 1/2 the class did the optional problem.\n- Take-away: Simulation is an (often) easy way to estimate\nprobabilities.\n- In 2(b), the exact probability for n = 23 is slightly more than\n0.5. We accepted 23 or 24 as answers.\n- Comment out extra print or cat statements.\n- Print just what is asked for, plus comments to the grader.\n- Instructions for finding your graded code is on MITx (right side\nof page, under Course Handouts)\n3/23\n\nSample Space Confusions\n1. Sample space = set of all possible outcomes of an experiment.\n2. The size of the set is NOT the sample space.\n3. Outcomes can be sequences of numbers.\nExamples.\n1. Roll 5 dice: S = set of all sequences of 5 numbers between 1 and\n6, e.g. (1, 2, 1, 3, 1, 5) ∈ S.\nThe size |S| = 65 is not a set.\n2. S = set of all sequences of 10 birthdays,\ne.g. (111, 231, 3, 44, 55, 129, 345, 14, 24, 14) ∈ S.\n|S| = 36510\n3. n some number, S = set of all sequences of n birthdays.\n|S| = 365n.\n4/23\n\nConditional Probability\n'the probability of A given B'.\nP(A ∩ B)\nP (A|B) =\n, provided P(B) = 0.\nP (B)\nB\nA\nA∩B\nHTH\nHTT\nTTH\nTTT\nHHH\nHHT\nTHH\nTHT\nA= A∩B\nB\nConditional probability: Abstractly and for coin example\n5/23\n\nTable/Concept Question\n(Discuss with your table and then click in your answer.)\nToss a coin 4 times.\nLet A = 'at least three heads'\nand B = 'first toss is tails'.\n1. What is P (A|B)?\n(a) 1/16\n(b) 1/8\n(c) 1/4\n(d) 1/5\n2. What is P (B|A)?\n(a) 1/16\n(b) 1/8\n(c) 1/4\n(d) 1/5\n6/23\n\nMultiplication Rule, Law of Total Probability\nMultiplication rule:\nP (A ∩ B) = P (A|B) ⋅ P (B).\nLaw of total probability:\nIf B1, B2, B3 partition S then\nP(A) = P(A ∩ B1) + P(A ∩ B2) + P(A ∩ B3)\n= P (A|B1)P (B1) + P (A|B2)P (B2) + P (A|B3)P (B3)\nΩ\nB3\nB2\nB1\nA ∩B1\nA ∩B2 A ∩B3\n7/23\n\nTrees\n- Organize computations\n- Compute total probability\n- Compute Bayes' formula\nExample. Game: 5 orange and 2 blue balls in an urn. A random ball\nis selected and replaced by a ball of the other color; then a second\nball is drawn.\n1. What is the probability the second ball is orange?\n2. What is the probability the first ball was orange given the second\nball was orange?\nB1\nO1\nO2\nB2\nO2\nB2\n5/7\n2/7\n4/7\n3/7\n6/7\n1/7\nFirst draw\nSecond draw\n8/23\n\nConcept (clicker) Question: Trees 1\nA1\nA2\nB1\nB2\nB1\nB2\nC1\nC2\nC1\nC2\nC1\nC2\nC1\nC2\nx\ny\nz\n1. The probability x represents\n(a) P (A1)\n(b) P (A1|B2)\n(c) P (B2|A1)\n(d) P (C1|B2 ∩ A1).\n9/23\n\nConcept (clicker) Question: Trees 2\nA1\nA2\nB1\nB2\nB1\nB2\nC1\nC2\nC1\nC2\nC1\nC2\nC1\nC2\nx\ny\nz\n2. The probability y represents\n(a) P (B2)\n(b) P (A1|B2)\n(c) P (B2|A1)\n(d) P (C1|B2 ∩ A1).\n10/23\n\nConcept Question: Trees 3\nA1\nA2\nB1\nB2\nB1\nB2\nC1\nC2\nC1\nC2\nC1\nC2\nC1\nC2\nx\ny\nz\n3. The probability z represents\n(a) P (C1)\n(b) P (B2|C1)\n(c) P (C1|B2)\n(d) P (C1|B2 ∩ A1).\n11/23\n\nConcept Question: Trees 4\nA1\nA2\nB1\nB2\nB1\nB2\nC1\nC2\nC1\nC2\nC1\nC2\nC1\nC2\nx\ny\nz\n4. The circled node represents the event\n(a) C1\n(b) B2 ∩ C1\n(c) A1 ∩ B2 ∩ C1\n(d) C1|B2 ∩ A1.\n12/23\n\nLet's Make a Deal with Monty Hall\n- One door hides a car, two hide goats.\n- The contestant chooses any door.\n- Monty always opens a different door with a goat. (He\ncan do this because he knows where the car is.)\n- The contestant is then allowed to switch doors if they\nwant.\nWhat is the best strategy for winning a car?\n(a) Switch\n(b) Don't switch\n(c) It doesn't matter\n13/23\nThis image is in the public domain.\n\nBoard question: Monty Hall\nOrganize the Monty Hall problem into a tree and compute\nthe probability of winning if you always switch.\nHint first break the game into a sequence of actions.\n14/23\nThis image is in the public domain.\n\nIndependence\nEvents A and B are independent if the probability that\none occurred is not affected by knowledge that the other\noccurred.\nIndependence ⇔ P (A|B) = P (A) (provided P(B) = 0)\n⇔ P (B|A) = P (B) (provided P(A) = 0)\n(For any A and B)\n⇔ P (A ∩ B) = P (A)P (B)\n15/23\n\nTable Question: Independence\nRoll two dice and consider the following events\n- A = 'first die is 3'\n- B = 'sum is 6'\n- C = 'sum is 7'\nA is independent of\n(a) B and C\n(b) B alone\n(c) C alone\n(d) Neither B or C.\n16/23\n\nBayes' Theorem\nAlso called Bayes' Rule and Bayes' Formula.\nAllows you to find P (A|B) from P (B|A), i.e. to 'invert'\nconditional probabilities.\nP (B|A) ⋅ P (A)\nP (A|B) =\nP (B)\nOften compute the denominator P (B) using the law of\ntotal probability.\n17/23\n\nBoard Question: Evil Squirrels\nOf the one million squirrels on MIT's campus most are good-natured.\nBut one hundred of them are pure evil! An enterprising student in\nCourse 6 develops an \"Evil Squirrel Alarm\" which they offer to sell to\nMIT for a passing grade. MIT decides to test the reliability of the\nalarm by conducting trials.\n18/23\n(c) Bigmacthealmanac. Some rights reserved. License: CC BY-SA. This content is excluded\nfrom our Creative Commons license. For more information, see https://ocw.mit.edu/help/\nfaq-fair-use.\n\nEvil Squirrels Continued\n1000000 squirrels, 100 are evil.\n- When presented with an evil squirrel, the alarm goes\noff 99% of the time.\n- When presented with a good-natured squirrel, the\nalarm goes off 1% of the time.\n(a) If a squirrel sets off the alarm, what is the probability\nthat it is evil?\n(b) Should MIT co-opt the patent rights and employ the\nsystem?\n19/23\n\nOne solution\n(This is a base rate fallacy problem)\nWe are given:\nP (nice) = 0.9999,\nP (evil) = 0.0001 (base rate)\nP (alarm | nice) = 0.01, P (alarm | evil) = 0.99\nP (alarm | evil)P (evil)\nP (evil | alarm) =\nP (alarm)\nP (alarm | evil)P (evil)\n= P (alarm | evil)P (evil) + P(alarm | nice)P (nice)\n(0.99)(0.0001)\n= (0.99)(0.0001) + (0.01)(0.9999)\n≈ 0.01\n20/23\n\nSquirrels continued\nSummary:\nProbability a random test is correct = 0.99\nProbability a positive test is correct ≈ 0.01\nThese probabilities are not the same!\nAlternative method of calculation:\nEvil\nNice\nAlarm\nNo alarm\n1 989901\n100 999900 1000000\n21/23\n\nBoard Question: Dice Game\n1. The Randomizer holds the 6-sided die in one fist and\nthe 8-sided die in the other.\n2. The Roller selects one of the Randomizer's fists and\ncovertly takes the die.\n3. The Roller rolls the die in secret and reports the result\nto the table.\nGiven the reported number, what is the probability that\nthe 6-sided die was chosen? (Find the probability for each\npossible reported number.)\n22/23\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit:\nhttps://ocw.mit.edu/terms.\n23/23"
    },
    {
      "category": "Resource",
      "title": "Class 04 Slides: Discrete Random Variables: Expected Value",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_lec04.pdf",
      "content": "Discrete Random Variables; Expectation\n18.05 Spring 2022\nhttps://en.wikipedia.org/wiki/Bean_machine#/media/File:\nQuincunx_(Galton_Box)_-_Galton_1889_diagram.png\nhttps://www.youtube.com/watch?v=9xUBhhM4vbM\n1/22\nThis image is in the public domain.\n\nAnnouncements/Agenda\n- Office hours today: Gabriel, after class in this room.\n- Evil squirrels from last time\n- Review of reading\n- random variable\n- probability mass function\n- cumulative distribution function\n- expected value\n- Named distributions: Bernoulli, binomial, geometric\n2/22\n\nReading Review\nRandom variable X: assigns a number to each outcome, i.e.\nX ∶S → R\n\"X = a\" denotes the event {ω | X(ω) = a}.\nProbability mass function (pmf) of X is given by\np(a) = P (X = a).\nCumulative distribution function (cdf) of X is given by\nF (a) = P (X ≤ a).\n3/22\n\nExample\nWe play a game where you roll a 4-sided die. The payoff is\n-$2 for a 1, -$1 for a 2, $0 for a 3, $4 for a 4.\nLet X be the payoff function. It is a random variable with the following\ntable.\noutcome ω:\nvalues of X:\n-2\n-1\npmf p(a):\n1/4\n1/4\n1/4\n1/4\ncdf F (a):\n1/4\n2/4\n3/4\n4/4\nThe cdf is the probability 'accumulated' from the left.\nQuestion: What are F (-1), F (0.5), F (-5), F (5), p(-1), p(0.5)?\n4/22\n\nExample\nWe play a game where you roll a 4-sided die. The payoff is\n-$2 for a 1, -$1 for a 2, $0 for a 3, $4 for a 4.\nLet X be the payoff function. It is a random variable with the following\ntable.\noutcome ω:\nvalues of X:\n-2\n-1\npmf p(a):\n1/4\n1/4\n1/4\n1/4\ncdf F (a):\n1/4\n2/4\n3/4\n4/4\nThe cdf is the probability 'accumulated' from the left.\nQuestion: What are F (-1), F (0.5), F (-5), F (5), p(-1), p(0.5)?\nAnswer: F (-1) = 2/4, F (0.5) = 3/4, F(-5) = 0, F(5) = 1,\np(-1) = 1/4, p(0.5) = 0.\n4/22\n\nProperties of PMF and CDF\n1. 0 ≤ p(a) ≤ 1\n2. p(a) sums to 1.\n1. F (a) is nondecreasing\n2. Way to the left, i.e. as a → -inf, F (a) is 0\n3. Way to the right, i.e. as a → inf, F (a) is 1.\n5/22\n\nPlots of PMF and CDF\na\np(a)\n0.5\n0.25\n0.15\na\nF(a)\n0.5\n0.75\n0.91\n6/22\n\nConcept Questions: cdf and pmf\nProblem 1.\nSuppose X a random variable with the\nCDF shown.\nvalues of X:\ncdf F (a): 0.5 0.75 0.9 1\nWhat is P(X ≤ 3)?\n(a) 0.15\n(b) 0.25\n(c) 0.5\n(d) 0.75\n7/22\n\nConcept Questions: cdf and pmf\nProblem 1.\nSuppose X a random variable with the\nCDF shown.\nvalues of X:\ncdf F (a): 0.5 0.75 0.9 1\nWhat is P(X ≤ 3)?\n(a) 0.15\n(b) 0.25\n(c) 0.5\n(d) 0.75\nProblem 2.\nWhat is P(X = 3)?\n(a) 0.15\n(b) 0.25\n(c) 0.5\n(d) 0.75\n7/22\n\nMeaning of Expected Value\nExample. Suppose an experiment produces random values 80, 1, 10\nwith the following table\nvalue:\npmf: 1/5 1/5 3/5\nWhat is the expected average value over many experiments?\nSolution: Say 5 experiments: 80,80,10,10,10. Ave. = 190/5 = 38\nIn 5 experiments, average will vary a lot. What about in 500?\nExpect each number occurs roughly in proportion to its probability.\nSuppose this is exactly what happens and compute the average.\nvalue:\nexpected counts: 100 100 300\n100 ⋅80 + 100 ⋅1 + 300 ⋅10\naverage =\n= ⋅80+ 1⋅1+ 3⋅10 = 22.2\nThis is the 'expected average' = expected value or expectation.\n8/22\n\nExpected Value\nRandom variable X: Takes values x1, x2, ... , xn, has pmf p(xi).\nThe expected value of X is defined by\nn\nE[X] = p(x1)x1 + p(x2)x2 + ... + p(xn)xn = ∑p(xi) xi\ni=1\n- It is a weighted average.\n- In statistics it is called a measure of central tendency.\nProperties of E[X]\n- E[X + Y ] = E[X] + E[Y ]\n(linearity I)\n- E[aX + b] = aE[X] + b\n(linearity II)\n- E[h(X)] = ∑ h(xi) p(xi)\ni\n9/22\n\nExpectation Examples\nExample 1. Find E[X].\n1.\nX:\n2.\npmf:\n1/4\n1/2\n1/8\n1/8\n10/22\n\nExpectation Examples\nExample 1. Find E[X].\n1.\nX:\n2.\npmf:\n1/4\n1/2\n1/8\n1/8\n3.\nE[X] = 3/4 + 4/2 + 5/8 + 6/8 = 33/8\n10/22\n\nExpectation Examples\nExample 1. Find E[X].\n1.\nX:\n2.\npmf:\n1/4\n1/2\n1/8\n1/8\n3.\nE[X] = 3/4 + 4/2 + 5/8 + 6/8 = 33/8\nExample 2. Suppose X ∼ Bernoulli(p). Find E[X].\n1.\nX:\n2.\npmf:\n1 -p\np\n10/22\n\nExpectation Examples\nExample 1. Find E[X].\n1.\nX:\n2.\npmf:\n1/4\n1/2\n1/8\n1/8\n3.\nE[X] = 3/4 + 4/2 + 5/8 + 6/8 = 33/8\nExample 2. Suppose X ∼ Bernoulli(p). Find E[X].\n1.\nX:\n2.\npmf:\n1 -p\np\n3.\nE[X] = (1 -p) ⋅0 + p ⋅1 = p.\n10/22\n\nExpectation Examples\nExample 1. Find E[X].\n1.\nX:\n2.\npmf:\n1/4\n1/2\n1/8\n1/8\n3.\nE[X] = 3/4 + 4/2 + 5/8 + 6/8 = 33/8\nExample 2. Suppose X ∼ Bernoulli(p). Find E[X].\n1.\nX:\n2.\npmf:\n1 -p\np\n3.\nE[X] = (1 -p) ⋅0 + p ⋅1 = p.\nExample 3. Suppose X = X1 + X2 + ... + X12, where E[Xi] = 0.25 for\neach i. Find E[X].\n10/22\n\nExpectation Examples\nExample 1. Find E[X].\n1.\nX:\n2.\npmf:\n1/4\n1/2\n1/8\n1/8\n3.\nE[X] = 3/4 + 4/2 + 5/8 + 6/8 = 33/8\nExample 2. Suppose X ∼ Bernoulli(p). Find E[X].\n1.\nX:\n2.\npmf:\n1 -p\np\n3.\nE[X] = (1 -p) ⋅0 + p ⋅1 = p.\nExample 3. Suppose X = X1 + X2 + ... + X12, where E[Xi] = 0.25 for\neach i. Find E[X].\nE[X] = E[X1] + E[X2] + ... E[X12] = 12 ⋅(0.25) = 3\n(Linearity of expectation.)\n10/22\n\nExpectation Example 4\nExample 4. Consider the random variable X with the following table.\nCompute E[X] and E[X2 + X]\n1.\nX:\n-2\n-1\n2.\npmf:\n1/5\n1/5\n1/5\n1/5\n1/5\n11/22\n\nExpectation Example 4\nExample 4. Consider the random variable X with the following table.\nCompute E[X] and E[X2 + X]\n1.\nX:\n-2\n-1\n2.\npmf:\n1/5\n1/5\n1/5\n1/5\n1/5\n3.\nE[X] = -2/5 - 1/5 + 0/5 + 1/5 + 2/5 = 0\n11/22\n\nExpectation Example 4\nExample 4. Consider the random variable X with the following table.\nCompute E[X] and E[X2 + X]\n1.\nX:\n-2\n-1\n2.\npmf:\n1/5\n1/5\n1/5\n1/5\n1/5\n3.\nE[X] = -2/5 - 1/5 + 0/5 + 1/5 + 2/5 = 0\n4.\nX2 + X:\n5.\nE[X2 + X] = 2/5 + 0/5 + 0/5 + 2/5 + 6/5 = 2\nLine 3 computes E[X] by multiplying the probabilities in line 2 by the\nvalues in line 1 and summing.\nLine 4 gives the values of X2 + X.\nLine 5 computes E[X2 + X] by multiplying the probabilities in line 2 by\nthe values in line 4 and summing. This illustrates the use of the formula\nE[h(X)] = ∑h(xi) p(xi).\ni\n11/22\n\nBoard Question: Computing expectation\n1.\nSuppose X is a random variable with the following\npmf.\nX:\npmf: 1/4 1/2 1/4\nFind E[X] and E[1/X].\n12/22\n\nBoard Question: Interpreting expectation\n2.\n(a) Would you accept a gamble that offers a 10%\nchance to win $95 and a 90% chance of losing $5?\n(b) Would you pay $5 to participate in a lottery that\noffers a 10% percent chance to win $100 and a 90%\nchance to win nothing?\nHint: find the expected value of your winnings in each\ncase.\nThe solutions contain some extended remarks about\nframing bias and tolerance for risk.\n13/22\n\nBoard Question: Musical chairs or linearity of expectation\n3.\nSuppose that there are n people at your table and\neveryone got up, ran around the room, and sat back down\nrandomly (i.e., all seating arrangements are equally likely).\nWhat is the expected value of the number of people\nsitting in their original seat?\n(We will explore this with simulations in Friday Studio.)\n14/22\n\nDeluge of discrete distributions: Bernoulli(p)\nBernoulli(p): binary choice; p = P(1)\nValues:\n1 (for success) 0 (for failure)\nPMF:\np(1) = p, p(0) = 1 -p\nExpectation: X ∼ Bernoulli(p) ⇒ E[X] = p\nIn more neutral language: One toss of a biased coin.\nValues: 1 (for heads) 0 (for tails)\nProbability table\nX:\npmf: 1 -p p\n15/22\n\nDeluge of discrete distributions: Binomial(n, p)\nX ∼Binomial(n,p) = # of successes in n independent Bernoulli(p)\ntrials\nValues:\n0, 1, 2, ..., n\nPMF:\np(k) = (n\nk)pk(1 - p)n-k\nExpectation: X ∼ Binomial(n, p) ⇒ E[X] = np\nIn more neutral language: Number of heads in n tosses of a biased\ncoin.\n16/22\n\nDeluge of discrete distributions: Geometric(p)\nX ∼ Geometric(p) = # of failures before first success in a\nsequence of independent Bernoulli(p) trials\nValues:\n0, 1, 2, 3, ...\nPMF:\np(k) = p(1 - p)k\nExpectation: X ∼ Geometric(p) ⇒ E[X] = (1 - p)/p.\nIn more neutral language: Number of tails before the first heads from\nrepeated tosses of a coin.\nNeutral language helps avoid confusion over whether we want the\nnumber of successes before the first failure or vice versa.\n17/22\n\nBoard Questions\n4.\n(a) Suppose X ∼ Bernoulli(p). Find E[X].\n(This is important! Remember it!)\n(b) Suppose Y = X1 + X2 + ... + X12, where each Xi ∼\nBernoulli(0.25). Find E[Y ].\n5. (Don't let one failure stop you!)\nLet X = # of successes before the second failure of a\nsequence of independent Bernoulli(p) trials. Find the pmf\nof X.\nHint: this requires a small amount of counting.\n18/22\n\nFiction\nGambler's fallacy: [roulette] if black comes up several\ntimes in a row then the next spin is more likely to be red.\nHot hand: NBA players get 'hot'.\n19/22\n\nFact\nP(red) remains the same.\nThe roulette wheel spins are independent. (Monte Carlo,\n1913).\nThe data show that player who has made 5 shots in a row\nis no more likely than usual to make the next shot.\n(Currently, there seems to be some disagreement about\nthis.)\nMore discussion and references given in the solutions to\ntoday's problems.\n20/22\n\nAmnesia\nShow that Geometric(p) is memoryless, i.e.\nP(X = n+ k | X ≥n) = P(X = k)\nExplain why we call this memoryless.\n21/22\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit:\nhttps://ocw.mit.edu/terms.\n22/22"
    },
    {
      "category": "Resource",
      "title": "Class 05a Slides: Variance and Continuous Random Variables",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_lec05a.pdf",
      "content": "Variance; Continuous Random Variables\n18.05 Spring 2022\n1/21\n\nAnnouncements/Agenda\nAnnouncements\n- None\nAgenda\n- Studio 2 comments\n- Variance and standard deviation for discrete variables\n- Calculus warmup\n- Continuous random variables - probability densities\n- Exponential distribution\n2/21\n\nStudio 2 comments\n- Graded studio 2 code is posted in the usual place.\n- Excellent job overall!\n- Please suppress stray printouts. We will start penalizing these.\n- If you used loops, please look at the solutions to see how R lets\nyou do array operations without loops.\n- Expected value of payoff IS NOT payoff of expected value.\nExample. X ∼ Bernoulli(p) and Y = X2 + X.\nE[X] = p,\nE[Y ] = 2p,\nE[X]2 + E[X] = p2 + p = E[Y ].\n3/21\n\nVariance and standard deviation\nX a discrete random variable with mean E[X] = μ.\n- Meaning: spread of probability mass about the mean.\n- Definition as expectation (weighted sum):\nVar(X) = E[(X - μ)2].\n- Computation as sum:\nn\nVar(X) = ∑ p(xi)(xi - μ)2.\ni=1\n- Standard deviation σ = √Var(X).\nUnits for standard deviation = units of X.\n4/21\n\n1. Concept question: Order the variance\nThe graphs below give the pmf for 3 random variables.\nx\n(A)\nx\n(B)\nx\n(C)\nOrder them by size of standard deviation from biggest to smallest.\n(Assume x has the same units in all three.)\n1. ABC\n2. ACB\n3. BAC\n4. BCA\n5. CAB\n6. CBA\n5/21\n\n2. Concept question: Zero variance\nSuppose X is a discrete random variable,\nTrue or False: If Var(X) = 0 then X is constant.\n1. True\n2. False\n6/21\n\nComputation from tables\nExample. Compute the variance and standard deviation\nof X.\nvalues x\npmf p(x) 1/10 2/10 4/10 2/10 1/10\n7/21\n\nComputation from tables\nExample. Compute the variance and standard deviation\nof X.\nvalues x\npmf p(x) 1/10 2/10 4/10 2/10 1/10\nA very useful formula\nThe following formula is often easier to use than the\ndefinition.\nn\nVar(X) = E[X2] -E[X]2 = (∑p(xi)x2\ni) -μ2.\ni=1\nRedo the above computation using this formula.\n(Written solution with posted solutions )\n7/21\n\n3. Concept question: Standard deviation\nMake an intuitive guess: Which pmf has the bigger\nstandard deviation? (Assume w and y have the same\nunits.)\ny\np(y)\n-3\n1/2\npmf for Y\nw\np(W)\n0.1\n0.2\n0.4\npmf for W\n1. Y\n2. W\n8/21\n\n3. Concept question: Standard deviation\nMake an intuitive guess: Which pmf has the bigger\nstandard deviation? (Assume w and y have the same\nunits.)\ny\np(y)\n-3\n1/2\npmf for Y\nw\np(W)\n0.1\n0.2\n0.4\npmf for W\n1. Y\n2. W\nTable question: make probability tables for Y and W\nand compute their standard deviations.\n8/21\n\nAlgebraic properties of variance\nIf a and b are constants then\nVar(aX + b) = a2 Var(X),\nσaX+b = |a| σX.\nIf X and Y are independent random variables then\nVar(X + Y) = Var(X) + Var(Y ).\n9/21\n\nBoard questions\n(a) Let X ∼ Bernoulli(p). Compute Var(X).\n(b) Let Y ∼ Bin(n, p). Show Var(Y) = np(1 -p).\n(c) Suppose X1, X2, ... , Xn are independent and all have\nthe same standard deviation σ = 2. Let X be the average\nof X1, ... , Xn.\nWhat is the standard deviation of X?\n10/21\n\nContinuous random variables\n- Like discrete, except take a continuous range of values\n- Replace probability mass function by probability\ndensity function\n- Replace sums by integrals\n11/21\n\nCalculus warmup for continuous random variables\n1. ∫\nb\nf(x) dx = area under the curve y = f(x).\na\n2. ∫\nb\nf(x) dx = 'sum of f(x) dx'.\na\nConnection between the two views:\nx\ny\na\nb\ny= f(x)\nx\ny\nx0 x1 x2\nxn\nΔx\n⋯\na\nb\ny= f(x)\nArea = f(xi)Δx\nArea is approximately the sum of rectangles:\nb\nn\n∫ f(x) dx ≈ f(x1)Δx+ f(x2)Δx+ ... + f(xn)Δx = ∑f(xi)Δx.\na\n12/21\n\nContinuous random variables: pdf and cdf\n- Continuous range of values:\n[0, 1], [a, b], [0, inf), (-inf, inf).\n- Probability density function (pdf)\nf(x) ≥0;\nP(c≤X≤d) = ∫\nd\nf(x) dx = 'sum' of f(x)dx.\nc\nprob.\nUnits for the pdf are\n(This explains the term density.)\nunit of x\n- Cumulative distribution function (cdf)\nF(x) = P(X ≤ x) = ∫\nx\nf(t) dt.\n-inf\n13/21\n\nVisualization\nx\nf(x)\nc\nd\nP(c≤X≤d)\npdf and probability\nx\nf(x)\nx\nF(x) = P(X≤x)\npdf and cdf\n14/21\n\nProperties of the cdf\n(Same as for discrete distributions)\n- (Definition) F(x) = P(X ≤ x) = ∫\nx\nf(u) du.\n- 0 ≤F(x) ≤1.\n-inf\n- non-decreasing.\n- 0 to the left: lim F(x) = 0.\nx→-inf\n- 1 to the right: lim F(x) = 1.\nx→inf\n- P(c< X≤d) = F(d) -F(c).\n- F ′(x) = f(x).\n15/21\n\nBoard questions\n1.\nSuppose X has range [0, 2] and pdf f(x) = c x2.\n(a) What is the value of c?\n(b) Compute the cdf F (x).\n(c) Compute P(1 ≤X ≤2).\n(d) Plot the pdf and use it to illustrate part (c).\n2.\nSuppose Y has range [0, b] and cdf F (y) = y2/9.\n(a) What is b?\n(b) Find the pdf of Y .\n16/21\n\n4. Discussion questions\nSuppose X is a continuous random variable.\n(a) If the pdf of X is f(x) can there be an x where f(x) = 10?\n17/21\n\n4. Discussion questions\nSuppose X is a continuous random variable.\n(a) If the pdf of X is f(x) can there be an x where f(x) = 10?\n(b) What is P(X = a)?\n17/21\n\n4. Discussion questions\nSuppose X is a continuous random variable.\n(a) If the pdf of X is f(x) can there be an x where f(x) = 10?\n(b) What is P(X = a)?\n(c) Does P(X = a) = 0 mean X never equals a?\n17/21\n\nDiscussion questions\nWhich of the following are graphs of valid cumulative distribution\nfunctions?\nx\n-4 -2\n-0.5\nA.\nx\n-4 -2\n-0.5\nB.\nx\n-4 -2\n-0.5\nC.\nx\n-4 -2\n-0.5\nD.\n18/21\n\nExponential Random Variables\nParameter: λ (called the rate parameter).\nRange:\n[0, inf).\nNotation:\nexponential(λ) or exp(λ).\nDensity:\nf(x) = λe-λx for 0 ≤ x.\nModels:\nWaiting time\nx\nP(3 < X< 7)\n2 4 6 8 10 12 14 16\n0.1\nf(x) = λe-λx\nx\nF(x) = 1 -e-λx\n8 10 12 14 16\nContinuous analogue of geometric distribution!\n19/21\n\nBoard question\nI've noticed that taxis drive past 77 Mass. Ave. on the average of\nonce every 10 minutes.\nSuppose time spent waiting for a taxi is modeled by an exponential\nrandom variable\nX ∼ Exponential(1/10);\nf(x) = 10\n1 e-x/10\n(a) Sketch the pdf of this distribution\n(b) Shade the region which represents the probability of waiting\nbetween 3 and 7 minutes\n(c) Compute the probability of waiting between between 3 and 7\nminutes for a taxi\n(d) Compute and sketch the cdf.\n20/21\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit:\nhttps://ocw.mit.edu/terms.\n21/21"
    },
    {
      "category": "Resource",
      "title": "Class 05b Slides: Gallery of Continuous Variables, Histograms",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_lec05b.pdf",
      "content": "Class 5 (continued): Continuous random variables\n18.05 Spring 2022\nx\ndensity\n0.0\n0.5\n1.0\n1.5\n2.0\n0.2\n0.4\n0.6\n0.8\n1/10 2/10 3/10 4/10\nx\nfrequency\n0.0\n0.5\n1.0\n1.5\n2.0\n1/13\n\nAnnouncements/Agenda\nAnnouncements\n- None\nAgenda\n- Gallery of random variables: exponential, uniform, normal\n- Histograms\n2/13\n\nExponential Random Variables\nParameter: λ (called the rate parameter).\nRange:\n[0, inf).\nNotation:\nexponential(λ) or exp(λ).\nDensity:\nf(x) = λe-λx for 0 ≤ x.\nMean:\nVariance\nλ\nλ2\nModels:\nWaiting time\nx\nP(3 < X< 7)\n2 4 6 8 10 12 14 16\n0.1\nf(x) = λe-λx\nx\nF(x) = 1 -e-λx\n8 10 12 14 16\nContinuous analogue of geometric distribution!\n3/13\n\nBoard question\nI've noticed that taxis drive past 77 Mass. Ave. on the average of\nonce every 10 minutes.\nSuppose time spent waiting for a taxi is modeled by an exponential\nrandom variable\nX ∼ Exponential(1/10);\nf(x) = 10\n1 e-x/10\n(a) Sketch the pdf of this distribution\n(b) Shade the region which represents the probability of waiting\nbetween 3 and 7 minutes\n(c) Compute the probability of waiting between between 3 and 7\nminutes for a taxi\n(d) Compute and sketch the cdf.\n4/13\n\nUniform Random Variables\nParameters:\na, b (limits).\nRange:\n[a, b].\nNotation:\nU(a, b) or uniform(a, b) or unif(a, b)\nDensity:\nf(x) = b - a (constant)\na+ b\n(b-a)2\nMean:\nVariance:\nx\na\nb\nf(x) =\nb-a\nx\na\nb\nF(x) = x-a\nb-afor a≤x≤b\npdf and cdf of uniform(a, b)\n5/13\n\nNormal Random Variables\nParameters: μ, σ (mean and standard deviation).\nRange:\n(-inf, inf).\nNotation:\nN(μ, σ2)\nDensity:\nf(x) =\nσ\n√1\n2π\ne-(x-μ)2/2σ2\nMean: μ\nVariance: σ2\npdf and cdf of N(μ, σ2)\nModels:\nMany things: sums of many independent effects.\nStandard normal: N(0, 1), i.e. mean 0, standard deviation 1.\nx\nμ\nf(x) =\n√\n2πσ\ne-(x-μ)2/(2σ2)\nx\nμ\nF(x)\n6/13\n\nTable question: Gallery of distributions\nOpen the Gallery of probability distributions applet at\nhttps:\n//mathlets.org/mathlets/probability-distributions/\n(a) For the standard normal distribution N(0, 1) how much\nprobability is within 1 of the mean? Within 2? Within 3?\n(b) For N(0, 32) how much probability is within σ of the mean?\nWithin 2σ? Within 3σ.\n(c) Does changing μ change your answer to problem 2?\n(d) Use the applet to find the median of the exp(0.5) distribution.\n(The median is the value of x where half the probability is below x\nand half above.)\n7/13\n\nNormal probabilities\nz\nμ-σ\nμ+ σ\nμ-2σ\nμ+ 2σ\nμ-3σ\nμ+ 3σ\nNormal PDF\nwithin 1 ⋅σ≈68%\nwithin 2 ⋅σ≈95%\nwithin 3 ⋅σ≈99%\n68%\n95%\n99%\nμ\nRules of thumb for standard and general normal\nP(-1 ≤Z ≤1) ≈0.68,\nP(-σ ≤ X -μ ≤ σ) ≈ 0.68\nP(-2 ≤Z ≤2) ≈0.95,\nP(-2σ ≤ X -μ ≤ 2σ) ≈ 0.95\nP(-3 ≤ Z ≤ 3) ≈ 0.997. P(-3σ ≤ X -μ ≤ 3σ) ≈ 0.997\n8/13\n\nManipulating random variables\nExample. Suppose X has range [0, 2] and cumulative distribution\nfunction (cdf) FX(x) = x3/8 over that range. If Y = X2 find the\ncdf and pdf for Y .\n9/13\n\nManipulating random variables\nExample. Suppose X has range [0, 2] and cumulative distribution\nfunction (cdf) FX(x) = x3/8 over that range. If Y = X2 find the\ncdf and pdf for Y .\nSolution: Y has range [0, 4]. To find its cdf you need to remember\nthe definition of the cdf and work carefully through that.\ny3/2\nFY(y) = P(Y ≤ y) = P(X2 ≤ y) = P(X ≤ √y) = FX(√y) = 8 .\nSo the pdf fY (y) = FY\n′ (y) = 16\n3 y1/2.\nNote. We work with the definition of the cdf as a probability. Don't\nguess! Work systematically.\n9/13\n\nBoard question: Manipulating random variables\n(a) Suppose X ∼ uniform(0,2). If Y = 4X, find the range, pdf and\ncdf of Y .\n(b) Suppose X ∼ uniform(0,2). If Y = X3, find the range, pdf and\ncdf of Y .\n(c) Suppose Z ∼ Norm(0, 1) (standard normal). Find the range, pdf\nand cdf of Y = 3Z+ 2.\n10/13\n\nHistograms\nMade by 'binning' data.\nFrequency: height of bar over bin = number of data points in bin.\nDensity: area of bar is the fraction of all data points that lie in the\nbin. So, total area is 1.\nx\ndensity\n0.0\n0.5\n1.0\n1.5\n2.0\n0.2\n0.4\n0.6\n0.8\n1/10 2/10 3/10 4/10\nx\nfrequency\n0.0\n0.5\n1.0\n1.5\n2.0\nCheck that the total area of the histogram on the right is 1.\n11/13\n\nBoard question: Histograms\n(a) Make both a frequency and density histogram from the data\nbelow.\nUse bins of width 0.5 starting at 0. The bins should be right closed.\n1.2\n1.3\n1.6\n1.6\n2.1\n2.2\n2.6\n2.7\n3.1\n3.2\n3.4\n3.8\n3.9\n3.9\n(b) Same question using unequal width bins with edges 0, 1, 3, 4.\n(c) For part (b), why does the density histogram give a more\nreasonable representation of the data?\n12/13\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit:\nhttps://ocw.mit.edu/terms.\n13/13"
    },
    {
      "category": "Resource",
      "title": "Class 06a Slides: Continuous Expectation and Variance, Quantiles, and Law of Large Numbers",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_lec06a.pdf",
      "content": "Continuous Expectation and Variance, Quantiles,\nthe Law of Large Numbers\n18.05 Spring 2022\n-4\n-2\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n1/17\n\nAgenda/Announcements\n- Friday is a normal class in - we'll finish class 6\n- In case of a snow closing, we will cancel class and adjust the pset\nand schedule.\n- R studio graded:\n- Good job!\n- Notice that we are learning how easy simulations are to do\n- -0.25 for stray printouts -this will go up as time goes on\n- Gabriel has OH after class.\nAgenda\n- Expected value\n- Variance and standard deviation\n- Quantiles (median etc.)\n- Histograms\n- Law of Large Numbers (LoLN)\n- Tomorrow: Central Limit Theorem (CLT)\n2/17\n\nExpected value\nExpected value: measure of location, central tendency\nDefinition. X continuous with range [a, b] and pdf f(x):\nE[X] = ∫\nb\nxf(x) dx.\na\nX discrete with values x1, ..., xn and pmf p(xi):\nn\nE[X] = ∑ xip(xi).\ni=1\nContinuous and discrete expectation are essentially the same\nformulas.\n3/17\n\nVariance and standard deviation\nStandard deviation: measure of spread, scale\nDefinition. For any random variable X with mean μ,\nVar(X) = E[(X - μ)2],\nσ = √Var(X)\nX continuous with range [a, b] and pdf f(x):\nVar(X) = ∫\nb\n(x-μ)2f(x) dx.\na\nX discrete with values x1, ..., xn and pmf p(xi):\nn\nVar(X) = ∑(xi - μ)2p(xi).\ni=1\nContinuous and discrete variance are essentially the same formulas.\n4/17\n\nProperties\nProperties: (the same for discrete and continuous)\n1. E[X + Y ] = E[X] + E[Y ].\n2. E[aX + b] = aE[X] + b.\n3. If X and Y are independent then\nVar(X + Y) = Var(X) + Var(Y ).\n4. Var(aX + b) = a2Var(X).\n5. Var(X) = E[X2] - E[X]2.\n5/17\n\nBoard question\nThe random variable X has range [0,1] and pdf f(x) = cx2.\n(a) Find c.\n(b) Find the mean, variance and standard deviation of X.\n(c) Find the median value of X.\n(d) Suppose X1, ... X16 are independent identically-distributed copies\nof X. Let X be their average. What is the standard deviation of X?\n(e) Suppose Y = X4. Compute E[Y ]\n(f) Find the pdf of Y .\n6/17\n\nQuantiles: measure of location\nExample. The 0.6 quantile q0.6 is the x-value, with\nP (X ≤ q0.6) = F (q0.6) = 0.6.\nx\nf(z)\nq0.6 = 0.253\nleft tail area = prob. = 0.6\nx\nF(x)\nq0.6 = 0.253\nF(q0.6) = 0.6\nq0.6: left tail area = 0.6 ⇔ F (q0.6) = 0.6 ⇔ q0.6 = F -1(0.6)\nIn R: qnorm, qbinom, qexp etc. The posted problems for today\ninclude one on using R for quantiles.\n7/17\n\nConcept questions: Greatest median 1\nEach of the curves is the density for a random variable. Where there\nis just one curve they overlap.\nThe median of the black plot is at q. Which density has the greatest\nmedian?\nCurves coincide to here.\nq\n(A)\n1. Black\n2. Orange\n3. Blue\n4. All the same 5. Impossible to tell\n8/17\n\nConcept questions: Greatest median 2\nEach of the curves is the density for a random variable. Where there\nis just one curve they overlap.\nThe median of the black plot is at q. Which density has the greatest\nmedian?\nq\n(B)\n1. Black\n2. Orange\n3. Blue\n4. All the same 5. Impossible to tell\n9/17\n\nHistograms\nMade by 'binning' data.\nFrequency: height of bar over bin = number of data\npoints in bin.\nDensity: area of bar = fraction of all data points that lie\nin the bin. So, total area is 1.\n10/17\n\nExample: equal bin widths\nConsider data 0.5, 0.9, 1.0, 1.3, 1.4, 1.5, 1.8, 1.8, 2.0, 2.0.\nMake frequency and density histograms with bin width 0.5 starting at\n0.0.\nCheck that the total area of the histogram on the left is 1.\nx\ndensity\n0.0\n0.5\n1.0\n1.5\n2.0\n0.2\n0.4\n0.6\n0.8\n1/10 2/10 3/10 4/10\nx\nfrequency\n0.0\n0.5\n1.0\n1.5\n2.0\n- Values on bin boundaries, e.g. 0.5, 1, 1.5, 2 go to left-hand bin.\n- Bins are right-closed, e.g first bin is (0, 0.5].\n- With equal bin widths the histograms look the same. Only\nvertical scale is changed.\n11/17\n\nExample: unequal bin widths\nRepeat the example with unequal bin widths. Put the bin bounds at\n0.0, 0.5, 1.5, 2.0.\nSolution: With unequal bin widths the density and frequency\nhistograms look different\nx\ndensity\n0.0\n0.5\n1.5\n1.0\n2.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1/10\n5/10\n4/10\nx\nfrequency\n0.0\n0.5\n1.5\n1.0\n2.0\nDon't be fooled! These are based on the same data.\n- If using unequal bin widths, always use a density histogram.\n- R will complain if you don't\n- Density histogram looks similar to previous histograms.\n- Frequency histogram is different and misleading\n12/17\n\nBoard question: Histograms\n(a) Make both a frequency and density histogram from the data\nbelow.\nUse bins of width 0.5 starting at 0. The bins should be right closed.\n1.2\n1.3\n1.6\n1.6\n2.1\n2.2\n2.6\n2.7\n3.1\n3.2\n3.4\n3.8\n3.9\n3.9\n(b) Same question using unequal width bins with edges 0, 1, 3, 4.\n(c) For part (b), why does the density histogram give a more\nreasonable representation of the data?\n13/17\n\nLaw of Large Numbers (LoLN)\n- Informally: An average of many measurements is more accurate\nthan a single measurement.\n- Formally: Let X1, X2, ...be i.i.d. random variables all with mean\nμ and standard deviation σ.\nX1 + X2 + ... + Xn\nn\nXn =\n=\n∑Xi.\nn\nn i=1\nThen for any (small number) a, we have\nlim P(|Xn-μ| < a) = 1.\nn→inf\n- By choosing n large enough we can, with probability close to 1,\nmake Xn as close as we want to μ.\nXn is random, so there may be a small probability that it is far\nfrom μ.\n14/17\n\nConcept Question: Desperation\n- You have $100. You need $1000 by tomorrow morning.\n- Your only way to get it is to gamble.\n- If you bet $k, you either win $k with probability p or lose $k with\nprobability 1 - p.\nMaximal strategy: Bet as much as you can, up to what you need,\neach time.\nMinimal strategy: Make a small bet, say $5, each time.\n(a) If p = 0.45, which is the better strategy?\n(a) Maximal\n(b) Minimal\n(c) They are the same\n15/17\n\nConcept Question: Desperation\n- You have $100. You need $1000 by tomorrow morning.\n- Your only way to get it is to gamble.\n- If you bet $k, you either win $k with probability p or lose $k with\nprobability 1 - p.\nMaximal strategy: Bet as much as you can, up to what you need,\neach time.\nMinimal strategy: Make a small bet, say $5, each time.\n(a) If p = 0.45, which is the better strategy?\n(a) Maximal\n(b) Minimal\n(c) They are the same\n(b) If p = 0.8, which is the better strategy?\n(a) Maximal\n(b) Minimal\n(c) They are the same\n15/17\n\nLoLN and histograms\nLoLN implies density histogram converges to pdf:\n-4\n-2\n0.1\n0.2\n0.3\n0.4\n0.5\nHistogram with bin width 0.1 showing 100000 draws from a standard\nnormal distribution. Standard normal pdf is overlaid in blue.\n16/17\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit:\nhttps://ocw.mit.edu/terms.\n17/17"
    },
    {
      "category": "Resource",
      "title": "Class 06b Slides: Central Limit Theorem",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_lec06b.pdf",
      "content": "Central Limit Theorem\n18.05 Spring 2022\n-4\n-2\n0.1\n0.2\n0.3\n0.4\n0.5\n1/19\n\nAgenda\n- Central limit theorem (CLT) (ubiquitous and important)\n- Start class 7: joint distributions\n2/19\n\nStandard deviation of an average\n(Board question from yesterday)\nX1, X2, ..., Xn independent, identically distributed (i.i.d.) random\nvariables.\nAll with mean μ and standard deviation σ.\nX1 + ... + Xn\nAverage = X =\nn\nE[X] = μ and standard deviation of X = σX = √σ\nn.\nReason: This is a calculation using the algebraic properties of mean\nand variance.\nKey conclusion: The average is a better estimate of μ than any\nsingle measurement.\n3/19\n\nStandardization\nRandom variable X with mean μ, standard deviation σ.\nX -μ\nStandardization: Z =\n.\nσ\n- Z has mean 0 and standard deviation 1.\n- Z is dimensionless.\n- Standardizing any normal random variable produces the standard\nnormal.\n- I.e. if X ≈ normal then standardized Z ≈ standard normal.\n4/19\n\nTable question: Standardization\nSuppose X is a random variable with mean μ and standard deviation\nσ. Let Z be the standardization of X.\n(a) Give the formula for Z in terms of X, μ and σ.\n(b) Use the algebraic properties of mean and variance to show Z has\nmean 0 and standard deviation 1.\n5/19\n\nCentral Limit Theorem\nSetting: X1, X2, ...i.i.d. with mean μ and standard dev. σ.\nFor each n:\nSn = X1 + X2 + ... + Xn\nsum\nSn\nXn = n(X1 + X2 + ... + Xn) =\naverage\nn\nKnow:\nE[Sn]\n= nμ,\nVar(Sn)\n= nσ2,\nσSn\n= √n σ\nσ2\nE[Xn] = μ,\nVar(Xn) =\nσXn = √σ\nn.\nn ,\nSn - nμ\nXn - μ\nStandardized sum and average: Zn=\n=\nσ√n\nσ/√n\nCentral Limit Theorem: For large n:\nXn ≈ N (μ, σ2\nSn ≈ N (nμ, nσ2)\nZn ≈ N(0, 1)\nn )\n6/19\n\nCentral Limit Theorem\nX1, X2, ...i.i.d. with mean μ and standard dev. σ.\nCentral Limit Theorem: For large n:\nXn ≈ N (μ, σ2\nSn ≈ N (nμ, nσ2)\nZn ≈ N(0, 1)\nn )\nIn words:\n- Xn is approximately normal: same mean as Xi but a smaller\nvariance.\n- Sn is approximately normal.\n- Standardized Xn and Sn are approximately standard normal.\n7/19\n\nCLT: Pictures 1\nThe standardized average of n i.i.d. Bernoulli(0.5) random variables\nwith n = 1, 2, 12, 64. (See class 6 reading for a description of how\nthese are made.)\n-3\n-2\n-1\n0.0\n0.2\n0.4\nx\nBernoulli: n = 1\n-3\n-2\n-1\n0.0\n0.2\n0.4\nx\nBernoulli: n = 2\n-3\n-2\n-1\n0.0\n0.2\n0.4\nx\nBernoulli: n = 12\n-3\n-2\n-1\n0.0\n0.2\n0.4\nx\nBernoulli: n = 64\n8/19\n\nCLT: Pictures 2\nStandardized average of n i.i.d. uniform random variables with\nn = 1, 2, 4, 12.\n-3\n-2\n-1\n0.0\n0.2\n0.4\nx\nUniform: n = 1\n-3\n-2\n-1\n0.0\n0.2\n0.4\nx\nUniform: n = 2\n-3\n-2\n-1\n0.0\n0.2\n0.4\nx\nUniform: n = 4\n-3\n-2\n-1\n0.0\n0.2\n0.4\nx\nUniform: n = 12\n9/19\n\nCLT: Pictures 3\nThe standardized average of n i.i.d. exponential random variables\nwith n = 1, 2, 8, 64.\n-3\n-2\n-1\n0.0\n0.4\n0.8\nx\nExponential: n = 1\n-3\n-2\n-1\n0.0\n0.2\n0.4\nx\nExponential: n = 2\n-3\n-2\n-1\n0.0\n0.2\n0.4\nx\nExponential: n = 8\n-3\n-2\n-1\n0.0\n0.2\n0.4\nx\nExponential: n = 64\n10/19\n\nCLT: Pictures 4\nThe (non-standardized) average of n Bernoulli(0.5) random variables,\nwith n = 4, 12, 64. (Spikier.)\n-1.0\n0.0\n0.5\n1.0\n1.5\n2.0\n0.0\n0.4\n0.8\nx\nBernoulli: n = 2\n0.0\n0.5\n1.0\n0.0\n1.0\n2.0\nx\nBernoulli: n = 12\n0.0\n0.5\n1.0\nx\nBernoulli: n = 64\n11/19\n\nConcept Question: Normal Distributions\nX has normal distribution, standard deviation σ.\nz\nμ-σ\nμ+ σ\nμ-2σ\nμ+ 2σ\nμ-3σ\nμ+ 3σ\nNormal PDF\nwithin 1 ⋅σ≈68%\nwithin 2 ⋅σ≈95%\nwithin 3 ⋅σ≈99%\n68%\n95%\n99%\nμ\n(a) P(-σ < X -μ < σ) is approximately\n(i) 0.025\n(ii) 0.16\n(iii) 0.68\n(iv) 0.84\n(v) 0.95\n12/19\n\nConcept Question: Normal Distributions\nX has normal distribution, standard deviation σ.\nz\nμ-σ\nμ+ σ\nμ-2σ\nμ+ 2σ\nμ-3σ\nμ+ 3σ\nNormal PDF\nwithin 1 ⋅σ≈68%\nwithin 2 ⋅σ≈95%\nwithin 3 ⋅σ≈99%\n68%\n95%\n99%\nμ\n(a) P(-σ < X -μ < σ) is approximately\n(i) 0.025\n(ii) 0.16\n(iii) 0.68\n(iv) 0.84\n(v) 0.95\n(b) P(X > μ+ 2σ) is approximately\n(i) 0.025\n(ii) 0.16\n(iii) 0.68\n(iv) 0.84\n(v) 0.95\n12/19\n\nBoard Question: CLT\n(a) Carefully write the statement of the central limit theorem.\n(b) To head the newly formed US Dept. of Statistics, suppose that\n50% of the population supports the team of Alessandre, Gabriel,\nSarah and So Hee, 25% support Jen and 25% support Jerry.\nA poll asks 400 random people who they support. What is the\nprobability that at least 55% of those polled prefer the team?\n(c) What is the probability that less than 20% of those polled prefer\nJen?\n13/19\n\nTable Question: Sampling from the standard normal\ndistribution\nHow would you approximate a single random sample from a standard\nnormal distribution using 9 rolls of a ten-sided die?\nNote: μ = 5.5 and σ2 = 8.25 for a single roll of a 10-sided die.\nHint: CLT is about averages.\n14/19\n\nHistogram of 9 roll simulation\nHistogram of standardized 9 roll simulation\nDensity\n-6\n-4\n-2\n0.0\n0.1\n0.2\n0.3\n0.4\nStandard normal is shown in orange.\nX = average of nine rolls: μ = 5.5, σ = √8.25/9.\nX -μ\nStandarized statistic: Z =\n≈ N(0, 1).\nσ\n15/19\n\nContinuity correction\nApproximating a discrete distribution with a continuous is ambiguous.\nPMF for binom(10, 0.5)\nDensity\n0.0\n0.1\n0.2\nHere X ∼ binom(10, 0.5). μX = 5, σX = √10/4.\nThe CLT for Y ∼ N(μX, σX\n2 ), X ≈Y .\nWe know P(X ≤ 4) = P(X ≤ 4.5) = P(X ≤ 4.9999). Should we\napproximate this with FY (4), FY (4.5), FY (4.9999)?\nRule of thumb: Use FY (4) - convenient, often easy to compute.\nContinuity correction: Use FY (4.5) - more accurate.\n16/19\n\nComparing rule of thumb and continuity correction\nCDF\nBinomial CDF and CLT rule of thumb: n=10, p=0.5\n0.0\n0.4\n0.8\nCDF\nBinomial CDF and corrected CLT: n=10, p=0.5\n0.0\n0.4\n0.8\n17/19\n\nBonus problem\nNot for class. Solution will be posted with solutions for today.\nAn accountant rounds to the nearest dollar. We'll assume the error in\nrounding is uniform on [-0.5, 0.5]. Estimate the probability that the\ntotal error in 300 entries is more than $5.\n18/19\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit:\nhttps://ocw.mit.edu/terms.\n19/19"
    },
    {
      "category": "Resource",
      "title": "Class 07 Slides: Joint Distributions, Independence, Covariance and Correlation",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_lec07.pdf",
      "content": "Joint Distributions, Independence\nCovariance and Correlation\n18.05 Spring 2022\nX\\Y\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/30\n\nAnnouncements/Agenda\nAnnouncements\n- Exam 1 on Thursday March 10. Covers classes 1-7.\n- Designed for 1 hour. You will have the full 80 minutes.\n- Review materials will be posted tomorrow.\n- Class on Tuesday 3/8 will be mostly review.\n- For the exam you will be given a table of standard normal\nprobabilities.\n- You can bring in a cheat sheet: 1 side of an 8 × 11 sheet of\npaper. You'll turn it in with the exam for 5 points.\nAgenda\n- Joint distributions: pmf, pdf, cdf\n- Marginal distributions\n- Independence\n- Covariance and correlation\n2/30\n\nJoint Distributions\nX and Y are jointly distributed random variables.\nDiscrete: Probability mass function (pmf): p(xi, yj)\nContinuous: probability density function (pdf): f(x, y)\nBoth: cumulative distribution function (cdf):\nF(x, y) = P(X ≤ x, Y ≤ y)\n3/30\n\nDiscrete joint pmf: example 1\nRoll two dice: X = # on first die, Y = # on second die\nX takes values in 1, 2, ..., 6, Y takes values in 1, 2, ..., 6\nJoint probability table:\nX\\Y\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\npmf: p(i, j) = 1/36 for any i and j between 1 and 6.\n4/30\n\nDiscrete joint pmf: example 2\nRoll two dice: X = # on first die, T = sum of both dice\nX\\T\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\nE.g. p(4, 2) = 0, p(1, 7) = 1/36.\n5/30\n\nContinuous joint distributions\n- X takes values in [a, b],\nY takes values in [c, d]\n- (X, Y ) takes values in [a, b] × [c, d].\n- Joint probability density function (pdf) f(x, y)\nf(x, y) dxdy = probability of being in the small square around (x, y).\n6/30\ndx\ndy\nProb. = f(x, y) dxdy\nx\ny\na\nb\nc\nd\n\nProperties of the joint pmf and pdf\nDiscrete case: probability mass function (pmf)\n1. 0 ≤ p(xi, yj) ≤ 1\n2. Total probability is 1.\nn\nm\n∑ ∑ p(xi, yj) = 1\ni=1 j=1\nContinuous case: probability density function (pdf)\n1. 0 ≤ f(x, y)\n2. Total probability is 1.\nd\nb\n∫ ∫ f(x, y) dxdy = 1\nc\na\nNote: f(x, y) can be greater than 1: it is a density not a probability. 7/30\n\n18.02 in 18.05\n- You should understand double integrals conceptually as double\nsums.\n- You should be able to compute double integrals over rectangles.\n- For a non-rectangular region, when f(x, y) = c is constant, you\nshould know that the double integral is the same as c×(the area\nof the region).\n- You should be able to compute partial derivatives.\n8/30\n\nExample: discrete events\nRoll two dice: X = # on first die, Y = # on second die.\nConsider the event: A = 'Y -X≥2'\nDescribe the event A and find its probability.\n9/30\n\nExample: discrete events\nRoll two dice: X = # on first die, Y = # on second die.\nConsider the event: A = 'Y -X≥2'\nDescribe the event A and find its probability.\nSolution: We can describe A as a set of (X, Y ) pairs:\nA = {(1, 3), (1, 4), (1, 5), (1, 6), (2, 4), (2, 5), (2, 6), (3, 5), (3, 6), (4, 6)}.\nOr we can visualize it by shading the table:\nX\\Y\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\nP (A) = sum of probabilities in shaded cells = 10/36.\n9/30\n\nExample: continuous events\nSuppose (X, Y ) takes values in [0, 1] × [0, 1].\nUniform density f(x, y) = 1.\nVisualize the event 'X > Y ' and find its probability.\n10/30\n\nExample: continuous events\nSuppose (X, Y ) takes values in [0, 1] × [0, 1].\nUniform density f(x, y) = 1.\nVisualize the event 'X > Y ' and find its probability.\nSolution:\nx\ny\n'X> Y'\nThe event takes up half the square. Since the density is uniform this\nis half the probability. That is, P(X > Y) = 0.5.\n10/30\n\nCumulative distribution function\ny\nx\nF(x, y) = P(X ≤ x, Y ≤ y) = ∫ ∫ f(u, v) dudv.\nc\na\n(a and c are the bottom of the ranges of X and Y respectively.)\n∂2F\nf(x, y) = ∂x∂y (x, y).\nProperties\n1. F (x, y) is non-decreasing. That is, as x or y increases F (x, y)\nincreases or remains constant.\n2. F(x, y) = 0 at the lower left of its range.\n3. F(x, y) = 1 at the upper right of its range.\n11/30\n\nMarginal pmf and pdf\nRoll two dice: X = # on first die, T = total on both dice.\nThe pmf of X is found by summing the rows. The pmf of T is found\nby summing the columns. These are called marginal pmfs of the joint\ndistribution.\nX\\T\np(xi)\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\np(tj)\n1/36\n2/36\n3/36\n4/36\n5/36\n6/36\n5/36\n4/36\n3/36\n2/36\n1/36\nFor continuous distributions the marginal pdf fX(x) is found by\nintegrating out the y. Likewise for fY (y).\n12/30\n\nBoard question: Joint distributions\nSuppose X and Y are random variables and\n- (X, Y ) takes values in [0, 1] × [0, 1].\n- the pdf is f(x, y) = x+ y.\n(a) Show f(x, y) is a valid pdf.\n(b) Visualize the event A = 'X > 0.3 and Y > 0.5'. Find its\nprobability.\n(c) Find the cdf F (x, y).\n(d) Use the cdf F (x, y) to find the marginal cdf FX(x) and\nP (X < 0.5).\n(e) Find the marginal pdf fX(x). Use this to find P (X < 0.5).\n(f) See next slide\n13/30\n\nBoard question: continued\n(f) (New scenario) From the following table compute F (3.5, 4).\nX\\Y\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n14/30\n\nIndependence\nEvents A and B are independent if\nP (A ∩ B) = P (A)P (B).\nRandom variables X and Y are independent if\nF (x, y) = FX(x)FY (y).\nDiscrete random variables X and Y are independent if\np(xi, yj) = pX(xi)pY (yj).\nContinuous random variables X and Y are independent if\nf(x, y) = fX(x)fY(y).\nIndependence means probabilities multiply!\n15/30\n\nConcept question: Independence I\nRoll two dice: X = value on first, Y = value on second\nX\\Y\np(xi)\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\np(yj)\n1/6\n1/6\n1/6\n1/6\n1/6\n1/6\nAre X and Y independent?\n1. Yes\n2. No\n16/30\n\nConcept question: Independence II\nRoll two dice: X = value on first, T = sum\nX\\T\np(xi)\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\np(yj)\n1/36\n2/36\n3/36\n4/36\n5/36\n6/36\n5/36\n4/36\n3/36\n2/36\n1/36\nAre X and Y independent?\n1. Yes\n2. No\n17/30\n\nConcept question: Independence III\nWhich of the following joint pdfs are the variables independent?\n(Each of the ranges is a rectangle chosen so that\n∫∫f(x, y) dxdy = 1.)\n(i) f(x, y) = 4x2y3.\n(ii) f(x, y) = 1\n2(x3y+ xy3).\n(iii) f(x, y) = 6e-3x-2y\n(a) i\n(b) ii\n(c) iii\n(d) i, ii\n(e) i, iii\n(f) ii, iii\n(g) i, ii, iii\n(h) None\n18/30\n\nCovariance\nMeasures the degree to which two random variables vary together,\ne.g. height and weight of people.\nX, Y random variables with means μX and μY .\nDefinition: Cov(X, Y) = E[(X-μX)(Y -μY)].\n- Positive covariance: When X is bigger than μX then Y is\n'usually' bigger than μY , and vice versa\n- Negative covariance: When X is bigger than μX then Y is\n'usually' smaller than μY , and vice versa\n- Zero covariance: The sign of X - μX tells us nothing about the\nsign of (Y -μY).\n19/30\n\nProperties of covariance\n1. Cov(aX + b, cY + d) = acCov(X, Y ) for constants a, b, c, d.\n2. Cov(X1 + X2, Y) = Cov(X1, Y ) + Cov(X2, Y ).\n3. Cov(X, X) = Var(X)\n4. Cov(X, Y) = E[XY ] -μXμY = E[XY ] -E[X]E[Y].\n5. If X and Y are independent then Cov(X, Y) = 0.\n6. Warning, the converse is not true: if covariance is 0 the\nvariables might not be independent.\n20/30\n\nTable question\nSuppose we have the following joint probability table.\nY\\X\n-1\np(yj)\n1/2\n1/2\n1/4\n1/4\n1/2\np(xi)\n1/4\n1/2\n1/4\nAt your table work out the covariance Cov(X, Y ).\nAre X and Y independent?\n21/30\n\nTable question\nSuppose we have the following joint probability table.\nY\\X\n-1\np(yj)\n1/2\n1/2\n1/4\n1/4\n1/2\np(xi)\n1/4\n1/2\n1/4\nAt your table work out the covariance Cov(X, Y ).\nAre X and Y independent?\nCovariance = 0. Not independent!\nKey point: covariance measures the linear relationship between X and\nY . It can completely miss a quadratic or higher order relationship.\n21/30\n\nCorrelation\nLike covariance, but removes scale.\nThe correlation coefficient between X and Y is defined by\nCov(X, Y )\nCor(X, Y) = ρ=\n.\nσX σY\nProperties: 1. ρ is the covariance of the standardized versions of X\nand Y .\n2. ρ is dimensionless (it's a ratio).\n3. -1 ≤ ρ ≤ 1.\n4. ρ = 1 if and only if Y = aX+ b with a > 0\n5. ρ = -1 if and only if Y = aX+ b with a < 0.\n22/30\n\nBoard question: Covariance and correlation\nFlip a fair coin 11 times. (The tosses are all independent.)\nLet X = number of heads in the first 6 flips\nLet Y = number of heads on the last 6 flips.\nCompute Cov(X, Y ) and Cor(X, Y ).\n23/30\n\nReal-life correlations\n- Over time, amount of ice cream consumption is correlated with\nnumber of pool drownings.\n- In 1685 (and today) being a student is the most dangerous\nprofession. That is, the average age of those who die is less than\nany other profession.\n- In 90% of bar fights ending in a death the person who started\nthe fight died.\n- Hormone replacement therapy (HRT) is correlated with a lower\nrate of coronary heart disease (CHD).\n24/30\n\nCorrelation is not causation\nEdward Tufte: \"Empirically observed covariation is a necessary but\nnot sufficient condition for causality.\"\n25/30\n\nOverlapping sums of uniform random variables\nWe made two random variables X and Y from overlapping sums of\nuniform random variables\nFor example:\nX = X1 + X2 + X3 + X4 + X5\nY = X3 + X4 + X5 + X6 + X7\nThese are sums of 5 of the Xi with 3 in common.\nIf we sum r of the Xi with s in common we name it (r, s).\nBelow are a series of scatterplots produced using R.\n26/30\n\nScatter plots\n27/30\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.4\n0.8\n(1, 0) cor=0.00, sample_cor=-0.07\ny\nx\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0.0\n0.5\n1.0\n1.5\n2.0\n0.0\n0.5\n1.0\n1.5\n2.0\n(2, 1) cor=0.50, sample_cor=0.48\ny\nx\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n(5, 1) cor=0.20, sample_cor=0.21\ny\nx\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n(10, 8) cor=0.80, sample_cor=0.81\ny\nx\n\nIntuition check\nToss a fair coin 2n + 1 times. Let X be the number of heads on the\nfirst n + 1 tosses and Y the number on the last n + 1 tosses.\nIf n = 1000 then Cov(X, Y ) is:\n(a) 0\n(b) 1/4\n(c) 1/2\n(d) 1\n(e) More than 1\n(f) tiny but not 0\n(This is computed in the answer to the next board question.)\n28/30\n\nBoard question: Even more tosses\nToss a fair coin 2n + 1 times. Let X be the number of heads on the\nfirst n + 1 tosses and Y the number on the last n + 1 tosses.\nCompute Cov(X, Y ) and Cor(X, Y ).\n29/30\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit:\nhttps://ocw.mit.edu/terms.\n30/30"
    },
    {
      "category": "Resource",
      "title": "Class 01: Problems",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_class01_pset.pdf",
      "content": "Class 1 in-class problems, 18.05, Spring 2022\nConcept questions\nConcept question 1. Poker hands\nThe probability of a one-pair hand is:\n(1) less than 5%\n(2) between 5% and 10%\n(3) between 10% and 20%\n(4) between 20% and 40%\n(5) greater than 40%\nConcept question 2. DNA\n1. DNA is made of sequences of nucleotides: A, C, G, T.\nHow many DNA sequences of length 3 are there?\n(i) 12\n(ii) 24\n(iii) 64\n(iv) 81\nConcept question 3. DNA\n2. How many DNA sequences of length 3 are there with no repeats?\n(i) 12\n(ii) 24\n(iii) 64\n(iv) 81\nBoard questions\nBoard question 1. Inclusion/Exclusion\nA band consists of singers and guitar players:\n7 people sing, 4 play guitar, 2 do both\nHow many people are in the band?\nBoard question 2. Rule of product\nThere are 5 Competitors in an Olympics 100m final.\nHow many ways can gold, silver, and bronze be awarded?\nBoard question 3.\nI won't wear green and red together; I think black or denim goes with anything; Here is my\nwardrobe.\nShirts: 3B, 3R, 2G; sweaters 1B, 2R, 1G; pants 2D,2B.\n(c) Source unknown. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see https://ocw.mit.edu/fairuse.\n\n18.05 class 1 problems, Spring 2022\nHow many different outfits can I wear?\nBoard question 4.\n(a) Count the number of ways to get exactly 3 heads in 10 flips of a coin.\n(b) For a fair coin, what is the probability of exactly 3 heads in 10 flips?\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Problem Set 01",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_pset01.pdf",
      "content": "18.05 Problem Set 1, Spring 2022\nProblem 1. (20 pts.) (6 card draw)\nYou can easily look up the probability of 5 card poker hands, e.g. https://en.wikipedia.\norg/wiki/Poker_probability\nFor this problem, let's consider hands with 6 cards. Here are two types:\nTwo-pair: Two cards have one rank, two cards have another rank, and the remaining two\ncards have two different ranks. e.g. {2♡, 2♠, 5♡, 5♣, Q♢, K♢}\nThree-of-a-kind: Three cards have one rank and the remaining three cards have three other\nranks. e.g. {2♡, 2♠, 2♣, 5♣, 9♠, K♡}\nCalculate the probability of each type of hand. Which is more probable?\nProblem 2. (20: 10,10 pts.) (Non-transitive dice)\nIn class we worked with non-transitive dice:\nBlue: 3 3 3 3 3 6; Orange: 1 4 4 4 4 4; White: 2 2 2 5 5 5.\n(a) Find the probability that white beats orange, the probability that orange beats blue\nand the probability that blue beats white.\nCan you line the dice up in order from best to worst? (Hint: this is why these are called\n'non-transitive'.)\n(b) Suppose you roll two white dice against two blue dice. What is the probability that\nthe sum of the white dice is greater than the sum of the blue dice?\nFor hints on this problem and ways to make money with your dice, watch at least the first\nsix minutes of the following video.\nhttps://www.youtube.com/watch?v=zWUrwhaqq_c\nHere a tree is used to organize the calculation rather than a table. We will discuss this\nmethod in Week 2.\nProblem 3. (55: 5,5,10,5,10,10,10 pts.) (Birthdays: counting and simulation)\nIgnoring leap days, the days of the year can be numbered 1 to 365. Assume that birthdays\nare equally likely to fall on any day of the year. Consider a group of n people, of which\nyou are not a member. An element of the sample space S will be a sequence of n birthdays\n(one for each person).\n(a) Define the probability function P for S. (This will depend on n.)\n(b) Consider the following events:\nA: \"someone in the group shares your birthday\"\nB: \"some two people in the group share a birthday\"\nC: \"some three people in the group share a birthday\"\nCarefully describe the subset of S that corresponds to each event.\n(c) Find an exact formula for P (A). What is the smallest n such that P (A) > 0.5?\n(d) Justify why n in part (c) is greater than 365 without doing any computation. (We are\nlooking for a short answer giving a heuristic sense of why this is so.)\n(e) Use R simulation to estimate the smallest n for which P (B) > 0.9. For these simula\ntions, let the number of trials be 10000. (You can reuse your code from Studio 1.)\n\n18.05 Problem Set 1, Spring 2022\nFor this value of n, repeat the simulation a few times to verify that it always gives similar\nresults.\nUsing 10000 trials you saw very little variation in the estimate of P (B). Try this again\nusing 30 trials and verify that the estimated probabilities are much more variable. On your\npset, give the results 7 runs of 30 trials using the value of n you just found.\n(f) Find an exact formula for P (B).\n(g) Use R simulation to estimate the smallest n for which P(C) > 0.5. Again use 10000\ntrials. You will find that two adjacent values of n are equally plausible based on simulations.\nYou may pick either one for your answer.\nNote that it is much harder to find an exact formula for P (C), so simulation is especially\nhandy.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class 02: Problems",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_class02_pset.pdf",
      "content": "Class 2 in-class problems, 18.05, Spring 2022\nConcept questions\nConcept question 1. (What's the event?)\n(Connecting words and set notation.)\nExperiment: toss a coin 3 times.\nWhich of following equals the event \"exactly two heads\"?\nA = {THH, HTH, HHT, HHH}\nB = {THH, HTH, HHT}\nC = {HTH, THH}\n(1) A\n(2) B\n(3) C\n(4) B or C\nConcept question 2. (Describe the event.)\n(Connecting words and set notation.)\nExperiment: toss a coin 3 times.\nWhich of the following describes the event {T HH, HT H, HHT }?\n(1) \"exactly one head\"\n(2) \"exactly one tail\"\n(3) \"at most one tail\"\n(4) none of the above\nConcept question 3. (Are they disjoint?)\n(Connecting words and set notation.)\nExperiment: toss a coin 3 times.\nThe events \"exactly 2 heads\" and \"exactly 2 tails\" are disjoint is.\n(1) True\n(2) False\nConcept question 4. (Does A imply B?)\n(Connecting words and set notation)\nConsider two events: A and B.\nAre the words \"A implies B\" equivalent to A ⊆ B?\n(1) True\n(2) False\nBoard problems\nProblem 1. Poker hands\nDeck of 52 cards\n- 13 ranks: 2, 3, ... , 9, 10, J, Q, K, A\n- 4 suits: ♡, ♠, ♢, ♣,\n\n18.05 class 2 problems, Spring 2022\nA poker hand consists of 5 cards\nA one-pair hand consists of two cards having one rank and the remaining three cards having\nthree other ranks\nExample: {2♡, 2♠, 5♡, 8♣, K♢}\n(a) How many different 5 card hands have exactly one pair?\nHint: practice with how many 2 card hands have exactly one pair.\nHint for hint: use the rule of product.\n(b) What is the probability of getting a one pair poker hand?\nProblem 2. (Inclusion-exclusion)\nSupposes a class has 50 students: 20 male (M), 25 brown-eyed (B)\nFor a randomly chosen student, what is the range of possible values for p= P(M ∪B)?\n(a) p ≤ 0.4\n(b) 0.4 ≤ p ≤ 0.5\n(c) 0.4 ≤ p ≤ 0.9\n(d) 0.5 ≤ p ≤ 0.9\n(e) 0.5 ≤ p\nProblem 3. D20\nConsider the following experiment. Roll a 20-sided die (D20) 9 times. What is the proba\nbility that all 9 rolls are distinct.\n(In class we could actually run this experiment many times with a real die and see how\nwell the experimental data matched the theoretical probability. Later, we will learn how to\nsimulate this experiment in R.)\nFor this experiment, how would you define the sample space, probability function, and\nevent?\nCompute the probability that all rolls (in one trial of 9 rolls) are distinct.\nProblem 4. Jon's Dice\nJon has three six-sided dice with unusual numbering.\nA game consists of two players each choosing a die. They roll once and the highest number\nwins.\nWhich die would you choose?\n\n18.05 class 2 problems, Spring 2022\n1. Make probabilitiy tables for the the blue and white dice.\n2. Make a probability table for the product sample space of blue and white. That is, suppose\nyou roll both dice and record the result as an ordered pair, (blue value, white value). List\nall the possible outcomes and their probabilities.\n3. Use the table to compute the probability that blue beats white.\n4. Pair up with another group. Have one group compare blue vs. orange and the other\ncompare orange vs. white. Based on the three comparisons, rank the dice from best to\nworst.\nProblem 5. Lucky Lucy\nThis is doable, but challenging problem. You should be able to compute P (A) and P (B). It\ntakes a bit of algebraic cleverness to decide if one is always bigger than the other.\nLucky Lucy has a coin that you're quite sure is not fair.\n- They will flip the coin twice\n- Let A be the event the tosses are the same, i.e. {HH, TT}\n- Let B be the event the tosses are the different, i.e. {HT, TH}\nLet p be the probability of heads. Compute and compare P (A) and P (B).\n(If you don't see the symbolic algebra try p = 0.2, p=0.5)\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Problem Set 02",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_pset02.pdf",
      "content": "18.05 Problem Set 2, Spring 2022\nProblem 1. (20: 10,10 pts.) ('Binary' paradox)\nFor this problem assume that puppies are equally probable to be male or female. Likewise\nfor kittens.\nBe sure to carefully justify your answers.\n(a) Our dog Layla had two puppies. The older puppy is female. What is the probability\nthat both puppies are female?\n(b) Our cat Ariel had two kittens. At least one of them is male. What is the probability\nthat both kittens are males?\nProblem 2. (15 pts.) (The blue taxi)\nIn a city with one hundred taxis, 1 is blue and 99 are green. A witness observes a hit-and\nrun by a taxi at night and recalls that the taxi was blue, so the police arrest the blue taxi\ndriver who was on duty that night. The driver proclaims their innocence and hires you to\ndefend them in court. You hire a scientist to test the witness' ability to distinguish blue\nand green taxis under conditions similar to the night of accident. The data suggests that\nthe witness sees blue cars as blue 99% of the time and green cars as blue 2% of the time.\nWrite a speech for the jury to give them reasonable doubt about your client's guilt. Your\nspeech need not be longer than the statement of this question. Keep in mind that most jurors\nhave not taken this course, so an illustrative table may be easier for them to understand\nthan fancy formulas.\nProblem 3. (20 pts.) (Trees of cards) There are 8 cards in a hat:\n{1♡, 1♠, 1♢, 1♣, 2♡, 2♠, 2♢, 2♣}\nYou draw one card at random. If its rank is 1 you draw one more card; if its rank is two\nyou draw two more cards. Let X be the sum of the ranks on the 2 or 3 cards drawn. Find\nE[X]. (Note: all the draws are done without replacement.)\nProblem 4. (25: 5,10,5,5 pts.) (Dice) There are four dice in a drawer: one D4 (4 sides),\none D6 (6-sides), and two D8 (8 sides). As usual, the sides of a die are numbered 1 to n,\nwhere n is the number of sides.\nYour friend secretly grabs one of the four dice at random. Let S be the number of sides on\nthe chosen die.\n(a) What is the pmf of S?\n(b) Now, without showing it to you, your friend rolls the chosen die and tells you the\nresult. Let R be the result of the roll.\nUse Bayes' rule to find P(S = s| R = 3) for s = 4, 6, 8. Which die is most likely if R = 3?\nTerminology: You are computing the pmf of 'S given R = 3'.\n(c) Which die is most likely if R = 6? Hint: You can either repeat the computation in (b),\nor you can reason based on your result in (b).\n(d) Which die is most likely if R = 7? No computations are needed!\n\n18.05 Problem Set 2, Spring 2022\nProblem 5. (10 pts.) (Seating arrangement and relative height) A total of n people\nrandomly take their seats around a circular table with n chairs. No two people have the\nsame height. What is the expected number of people who are shorter than both of their\nimmediate neighbors?\nProblem 6. (20: 3,2,10,5 pts.)\nIn this problem we will use R to simulate flipping a fair coin 50 times. We'll use the\nsimulation to explore 'runs'. That is, sequences of all 1's or 0's.\n(a) Make up a sequence of length 50 consisting of ones and zeros. Try to make the sequence\nlook like it was randomly generated by flipping a coin.\n(b) A run is a sequence of all 1s or all 0s. How long is the longest run in your answer to\npart (a)?\n(c) Now we will use R to simulate 50 tosses of a fair coin and estimate the average length\nof the longest run. The code below simulates one trial. You will need to use a 'for loop' to\nrun 10000 trials. On our MITx site you can find tutorials on both for loops and the rle()\nfunction used in the code. (As usual, choose the Course info tab and click on the R code\nlink under course handouts).\nSample code illustrating the use of rle().\n# R code to simulate 50 flips of a fair coin and find the longest run\n# rle stands for `run length encoding'.\n# rle(trial)$lengths is a vector of the lengths of all the different runs\n# in trial.\nnflips = 50\ntrial = rbinom(nflips, 1, 0.5) # Note: binomial(1, 0.5) = bernoulli(0.5)\nmax_run = max(rle(trial)$lengths)\nSample code illustrating 'for loops'. (These may be useful in the code for this problem.)\n# R code demonstrating `for' loops.\nfor (j in 1:5) {\nprint(j^2)\n}\n(Should produce: 1, 4, 9, 16, 25.)\nsum = 0\nfor (j in 1:5) {\nsum = sum + j\n}\nprint(sum)\n(Should produce: 15.)\nUse R to simulate the average length of the longest run in 50 flips of a fair coin. Do this\nthree times with 10000 trials each time and report the three results.\n(d) A small modification of your code will let you estimate the probability of a run of 8 or\nmore in 50 flips. Do this three times with 10000 trials each time. Report the three results.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class 03: Problems",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_class03_pset.pdf",
      "content": "Class 3 in-class problems, 18.05, Spring 2022\nConcept questions\nConcept question 1.\nToss a coin 4 times. Let A = 'at least three heads' and B = 'first toss is tails'.\n1. What is P (A|B)?\n(a) 1/16\n(b) 1/8\n(c) 1/4\n(d) 1/5\n2. What is P (B|A)?\n(a) 1/16\n(b) 1/8\n(c) 1/4\n(d) 1/5\nConcept question 2. Trees 1.\nA1\nA2\nB1\nB2\nB1\nB2\nC1\nC2\nC1\nC2\nC1\nC2\nC1\nC2\nx\ny\nz\n1. The probability x represents\n(a) P (A1) (b) P (A1|B2) (c) P (B2|A1) (d) P (C1|B2 ∩ A1).\nConcept question 3. Trees 2.\n2. The probability y represents\n(a) P (B2) (b) P (A1|B2) (c) P (B2|A1) (d) P (C1|B2 ∩ A1).\nConcept question 4. Trees 3.\n3. The probability z represents\n(a) P (C1) (b) P (B2|C1) (c) P (C1|B2) (d) P (C1|B2 ∩ A1).\nConcept question 5. Trees 4.\n4. The circled node represents the event\n(a) C1 (b) B2 ∩ C1 (c) A1 ∩ B2 ∩ C1 (d) C1|B2 ∩ A1.\nIn class examples\nClass example 1.\n- Organize computations\n- Compute total probability\n- Compute Bayes' formula\n\n18.05 class 3 problems, Spring 2022\nExample. Game: 5 orange and 2 blue balls in an urn. A random ball is selected and\nreplaced by a ball of the other color; then a second ball is drawn.\n1. What is the probability the second ball is orange?\n2. What is the probability the first ball was orange given the second ball was orange?\nB1\nO1\nO2\nB2\nO2\nB2\n5/7\n2/7\n4/7\n3/7\n6/7\n1/7\nFirst draw\nSecond draw\nBoard questions\nProblem 1. Monty Hall\n- One door hides a car, two hide goats.\n- The contestant chooses any door.\n- Monty always opens a different door with a goat. (He can do this because he knows\nwhere the car is.)\n- The contestant is then allowed to switch doors if they want.\nWhat is the best strategy for winning a car?\n(a) Switch\n(b) Don't switch\n(c) It doesn't matter\nOrganize the Monty Hall problem into a tree and compute the probability of winning if you\nalways switch.\nHint first break the game into a sequence of actions.\nProblem 2. Independence\nRoll two dice and consider the following events\n- A = 'first die is 3'\n- B = 'sum is 6'\n- C = 'sum is 7'\nThis image is in the public domain.\n\n18.05 class 3 problems, Spring 2022\nA is independent of\n(a) B and C\n(b) B alone\n(c) C alone\n(d) Neither B or C.\nProblem 3. Evil Squirrels\nOf the one million squirrels on MIT's campus most are good-natured. But one hundred of\nthem are pure evil! An enterprising student in Course 6 develops an \"Evil Squirrel Alarm\"\nwhich they offer to sell to MIT for a passing grade. MIT decides to test the reliability of\nthe alarm by conducting trials.\n- When presented with an evil squirrel, the alarm goes off 99% of the time.\n- When presented with a good-natured squirrel, the alarm goes off 1% of the time.\n(a) If a squirrel sets off the alarm, what is the probability that it is evil?\n(b) Should MIT co-opt the patent rights and employ the system?\nProblem 4. Dice Game\n1. The Randomizer holds the 6-sided die in one fist and the 8-sided die in the other.\n2. The Roller selects one of the Randomizer's fists and covertly takes the die.\n3. The Roller rolls the die in secret and reports the result to the table.\nGiven the reported number, what is the probability that the 6-sided die was chosen? (Find\nthe probability for each possible reported number.)\n(c) Bigmacthealmanac. Some rights reserved. License: CC BY-SA. This content is excluded\nfrom our Creative Commons license. For more information, see https://ocw.mit.edu/fairuse.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Problem Set 03",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_pset03.pdf",
      "content": "18.05 Problem Set 3, Spring 2022\nProblem 1. (25: 10,5,10 pts.) Independence\nThree events A, B, and C are pairwise independent if each pair is independent. They are\nmutually independent if they are pairwise independent and if, in addition,\nP(A∩B ∩C) = P(A)P(B)P(C).\n(1)\n(a) Suppose we roll two 6-sided die. Consider the events:\nA = 'odd on die 1'\nB = 'odd on die 2'\nC = 'odd sum'\nAre A, B, and C pairwise independent? Are they mutually independent?\n(b) Consider the Venn diagram below. A, B and C are the overlapping circles and the\nprobabilities of each region are as marked. Does equation (1) hold. Are the events A, B, C\nmutually independent?\n0.225\n0.225\n0.175\n0.05\n0.1\n0.1\n0.125\nA\nB\nC\n(c) Consider a litter of n puppies. What value(s) of n makes the events 'the litter has\npuppies of both sexes' and 'there is at most one female' independent.\nProblem 2. (25: 5,5,10,5 pts.) What does the data say?\nSuppose there is an experimental medical treatment for a cancer that, if untreated, is nearly\nalways fatal within 12-15 months. The doctors enroll 5000 patients in a study in which each\npatient is given the treatment and followed for 5 years. Let X be the length of time a random\npatient given the treatment survives. (If a patient is still alive at the end of the study, then\nX = 5 for this patient.)\nAs the statistician it is your job to analyze the data.\nTo load the data into R you should do the following:\n1. Download the data mit18_05_s22_ps3prob2-data.r. You can find this on our course\nwebsite on the page with R code.\n2. Put this file in your 18.05 R directory.\n3. In R studio make sure the working directory is set to your 18.05 R directory.\n4. Run the commands:\n> source('mit18_05_s22_ps3prob2-data.r')\n\n18.05 Problem Set 3, Spring 2022\n> x = get_prob2_data()\nThe variable x should now hold an array of the 5000 data points.\n(a) Use R to compute the mean, variance and standard deviation of the data.\n(b) Use the hist command to get R to plot a frequency histogram of the data. Set the\nhistogram so each bin has width 0.1 years. Print the histogram and turn it in with the pset.\nThe hist() command was introduced in Studio 3. There is also a short tutorial on using\nR to plot histograms on our class R page.\n(c) Using your answers in (a) and (b), write a short paragraph summarizing the data in a\nuseful way.\n(d) Based on the (c), what are your conclusions about the effectiveness of the treatment?\nWhat recommendations would you make for avenues of further research?\nProblem 3. (30: 10,10,10 pts.) Dice\nLet X be the result of rolling a fair 4-sided die. Let Y be the result of rolling a fair 6-sided\ndie. Let Z be the average of X and Y .\n(a) Find the standard deviation of X, of Y , and of Z.\n(b) Carefully graph the pmf and cdf of Z.\n(c) Here is a gambling game: You win 2X dollars if X > Y and lose 1 dollar otherwise. After\nplaying this game 60 times, what is your expected total gain (positive) or loss (negative)?\nProblem 4. (20: 5,5,5,5 pts.) Two scoops\nBoxes of Raisin Bran cereal are 30cm tall. Due to settling, boxes have a higher density of\nraisins at the bottom (h = 0) than at the top (h = 30). Suppose the density (in raisins per\ncm of height) is given by f(h) = 40 -h.\n(a) How many raisins are in a box?\n(b) Let H be the height of a random raisin. Find and graph the pdf g(h) of H.\n(c) Find and graph the cdf G(h) of H.\n(d) What is the probability that a random raisin is in the bottom third of the box?\nProblem 5. (20: 5,5,10 pts.) Gallery of continuous random variables.\nThe pnorm() function on R gives the cdf of the normal distribution, e,g, if X ∼ N(μ, σ2)\nthen pnorm(x, μ, σ) = P (X ≤ x) = FX(x).\n(a) Suppose Z is a standard normal random variable. Use R to compute\n(i) P(Z ≤ 0),\n(ii) P (Z > 1.5)\n(iii) P(|Z| < 1.5).\n(b) Let X ∼ N(μ, σ2) where μ = 2 and σ = 3. Use R to compute\n(i) P(X ≤ μ),\n(ii) P (X - μ > 1.5σ)\n(iii) P (|X - μ| < 1.5σ).\n(c) Let Y ∼ exp(λ). Compute the cdf of Y by integrating the pdf. What is the probability\nY ≤ 1/λ? (You need to do an integration, but you can check your work numerically using\nthe pexp() function in R.)\n\n18.05 Problem Set 3, Spring 2022\nProblem 6. (20: 5,5,5,5 pts.) Birth day\nThe length of human gestation is well-approximated by a normal distribution with mean\nμ = 280 days and standard deviation σ = 8.5 days.\n(a) Graph the corresponding pdf and cdf. You should do this using the dnorm, pnorm and\nplot commands in R. Print the results and turn them in with the pset.\n(b) Suppose your final exam is scheduled for May 18 and your pregnant professor has a\ndue date of May 25. Find the probability she will give birth on or before the day of the\nfinal.\n(c) Find the probability she will give birth in May sometime after the exam. (Assume this\nmeans from the start of May 19 to the end of May 31.)\n(d) The professor decides to move up the exam date so there will be a 95% probability\nthat she will give birth afterward. What date should she pick?\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class 04: Problems",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_class04_pset.pdf",
      "content": "Class 4 in-class problems, 18.05, Spring 2022\nConcept questions\nConcept question 1.\nSuppose X a random variable with the CDF shown.\nvalues of X:\ncdf F (a):\n0.5\n0.75\n0.9\nWhat is P(X ≤ 3)?\n(a) 0.15\n(b) 0.25\n(c) 0.5\n(d) 0.75\nWhat is P(X = 3)? Same distribution as above\n(a) 0.15\n(b) 0.25\n(c) 0.5\n(d) 0.75\nBoard questions\nProblem 1. Computing expectation\nSuppose X is a random variable with the following pmf.\nX:\npmf:\n1/4\n1/2\n1/4\nFind E[X] and E[1/X].\nProblem 2. Interpreting expectation\n(a) Would you accept a gamble that offers a 10% chance to win $95 and a 90% chance of\nlosing $5?\n(b) Would you pay $5 to participate in a lottery that offers a 10% percent chance to win\n$100 and a 90% chance to win nothing?\nHint: find the expected value of your winnings in each case.\nProblem 3. Musical chairs or linearity of expectation\nSuppose that there are n people at your table and everyone got up, ran around the room,\nand sat back down randomly (i.e., all seating arrangements are equally likely).\nWhat is the expected value of the number of people sitting in their original seat?\nProblem 4. Bernoulli\n(a) Suppose X ∼ Bernoulli(p). Find E[X].\n(This is important! Remember it!)\n(b) Suppose Y = X1 + X2 + ... + X12, where each Xi ∼ Bernoulli(0.25). Find E[Y ].\nProblem 5. Don't let one failure stop you\nLet X = # of successes before the second failure of a sequence of independent Bernoulli(p)\ntrials. Find the pmf of X.\nHint: this requires a small amount of counting.\n\n18.05 class 4 problems, Spring 2022\nIn class examples and discussion\nGambler's fallacy. [roulette]\nFallacy: If black comes up several times in a row then the next spin is more likely to be\nred.\nTruth: P(red) remains the same. The roulette wheel spins are independent.\nHot hand\nTheory: NBA players get 'hot'.\nData: The data show that player who has made 5 shots in a row is no more likely than\nusual to make the next shot.\nExtra problems\nExtra 1.\nSuppose X ∼ Binomial(n, p), i.e. the number of successes in n, independent\nBernoulli(p) trials. Explain why X is the sum of n Bernoulli(p) random variables.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Problem Set 04",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_pset04.pdf",
      "content": "18.05 Problem Set 4, Spring 2022\nProblem 1. (25: 5,5,10,5 pts.) Time to failure\nRecall that an exponential random variable X ∼ exp(λ) has pdf given by f(x) = λe-λx on\nx ≥ 0.\n(a) Compute P (X ≥ x).\n(b) Compute the mean and standard deviation of X. You need to set up the necessary\nintegrals, but you can use Wolfram Alpha or another application to do the computation.\n(Of course, it will be good for you if you compute the integrals by hand!)\n(c) Suppose that X1 and X2 are independent exp(λ) random variables. Let T = min(X1, X2).\nFind the cdf and pdf of T . (Hint: first find a formula for P(T ≥ t)?)\nNote: for independent continuous random variables X1, X2, you can assume the following\nformula:\nP (X1 ≥ x1, X2 ≥ x2) = P (X1 ≥ x1)P (X2 ≥ x2).\n(d) Suppose we are testing 3 different brands of light bulbs B1, B2, and B3 whose lifetimes\nare exponential random variables with mean 1/2, 1/3, and 1/5 years, respectively. Assuming\nthat all of the bulbs are independent, what is the expected time before one of the bulb fails.\n(Hint: part (c) was a warmup for this problem.)\nProblem 2. (20: 10,10 pts.) Elections\nTo head the newly formed US Dept. of Statistics, suppose that 50% of the population\nsupports Alessandre, 20% supports Sarah, and the rest are split between Gabriel, Sarah\nand So Hee. A poll asks 400 random people who they support.\n(a) Use the central limit theorem to estimate the probability that at least 52.5% of those\npolled prefer Alessandre?\n(b) Use the central limit theorem to estimate the probability that less than 31% of those\npolled prefer Gabriel, Sarah or So Hee?\nProblem 3. (10 pts.) A penny for your thoughts\nTo save a mint, in 2012 Canada decided to do away with its pennies. The Chubby Chef in\nEquality, Illinois wants to be ready should the United States decide to pass a similar law.\nThe Chubby Chef processes n = 1000 orders of assorted baked goods each day, and will\nround the price of each order to the nearest nickel (e.g., $3.57 rounds to $3.55 while $3.58\nrounds to $3.60). Let p be the probability that the total rounding error over the course of\na day is either greater than 100 or less than -100 cents, i.e. exceeds 100 in absolute value.\nEstimate p using the central limit theorem.\nExtra credit 5 points Simulate this in R with 10000 trials. (Each trial involves 1000\norders.) Print out or hand copy your code and include it. Give the result of running your\ncode 3 times,\nProblem 4. (30: 10,10,10 pts.) Change of scale.\nIn this problem we will look at scaling random variables. This is a simple, but common\n\n18.05 Problem Set 4, Spring 2022\nthing to do. As usual with transformations, if you don't approach it systematically, it is\neasy to make mistakes.\n(a) Suppose the random variable X has an exponential distribution with parameter 1, i.e.\nX ∼ exp(1). Give the range and pdf for the variables X and Y = 3X\nSketch the graph of the density functions for each of these variables.\n(b) For the random variable X from part (a), find the range and pdf of W = aX+b, where\na and b are constants. Assume a > 0.\n(c) Let V = X3. Find the range and pdf of V .\nProblem 5. (30: 10,10,10 pts.) In this problem we will explore how the transformations\nin the previous problem affect the mean and median.\n(a) For the variables X, Y , W in the previous problem, assume each of the variables are\ngiven in units of minutes. Find the expected value, variance and standard deviation of each\nvariable. Be sure to include units in your answer.\nWhat are the units on a and b in the defnition of W ?\n(b) For V from the previous problem, compute E[V ]. As usual, you must set up the\nintegral, but you can use a package like Wolfram Alpha to compute the integral.\n(c) Compute the median value of both X and V .\nProblem 6. (30: 5,5,10,10 pts.) Fat tails\nThis problem will explore the tails of two distributions. The tails are important when we\nwant to think about probabilities of extreme events.\n(a) As an example, in the general population IQ has mean 100 and standard deviation\nof 15. IQ is normally distributed. Use the R function pnorm to give the probability that\na randomly chosen person has IQ greater than 160, i.e. more than 4 standard deviations\nabove the mean.\n(b) Now, in order to be able to use R or Wolfram Alpha without a lot of distracting\nalgebraic manipulation, we'll modify the definition of IQ. Suppose that Modified_IQ has\n√\n3.\nmean 0 and standard deviation\nAssuming Modified_IQ is normally distributed, find the probability that a randomly chosen\nperson has Modified_IQ more than 4 standard deviations above the mean.\n(c) Now assume that Modified_IQ follows a t-distribution with 3 degrees of freedom. Later\nin the class we will work extensively with t-distributions. Here, it will be enough for us to\nknow the following about this distribution.\n- Range: (-inf, inf)\n-2\n- PDF: f(x) = 3π(1 + x\n)\n- Mean: μ = 0\n- Standard deviation: σ =\n√\n\n18.05 Problem Set 4, Spring 2022\n(So this has the same mean and standard deviation as in part (b).)\nFor this problem, you can work with this pdf directly or you can look up how to use the R\nfunctions dt and pt.\nAssuming it follows this t-distribution, find the probability that a randomly chosen person\nhas Modified_IQ more than 4 standard deviations above the mean.\nYou can use R or another calculation package to do the calculation, but you must explicitly\nshow the integral in terms of the probability density.\nCompare this value with the probability in part (b)\n(d) For this problem, compute probabilities using both the normal distribution in part (b)\nand the t-distribution in part (c). Do this for the following probabilities.\n(i) P (Modified_IQ > 20), (ii) P (Modified_IQ > 40), (iii) P (Modified_IQ > 200).\nWhy do we say that the t-distribution has a 'fat tail'?\nHence the moral of this problem: Knowing the mean and standard deviation of a quantity\nis often not enough for predicting the frequency of extreme events (high IQ, 100-year floods,\netc.); you need to know the underlying distribution itself (which often requires finding out\nthe underlying geophysics, geochemistry, and biology). In the solutions we will show you\ngraphs of these distributions zoomed in around 4σ above the mean. If you do that yourself,\nyou will see that they look very different.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class 05a: Problems",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_class05a_pset.pdf",
      "content": "Class 5 in-class problems, 18.05, Spring 2022\nConcept questions\nConcept question 1. Order the variance\nThe graphs below give the pmf for 3 random variables.\nx\n(A)\nx\n(B)\nx\n(C)\nOrder them by size of standard deviation from biggest to smallest. (Assume x has the same\nunits in all three.)\n1. ABC\n2. ACB\n3. BAC\n4. BCA\n5. CAB\n6. CBA\nConcept question 2. Zero variance\nSuppose X is a discrete random variable,\nTrue or False: If Var(X) = 0 then X is constant.\nConcept question 3. Standard deviation\nMake an intuitive guess: Which pmf has the bigger standard deviation? (Assume w and y\nhave the same units.)\ny\np(y)\n-3\n1/2\npmf for Y\nw\np(W)\n0.1\n0.2\n0.4\npmf for W\n1. Y\n2. W\nConcept question 4.\nSuppose X is a continuous random variable.\n(a) If the pdf of X is f(x) can there be an x where f(x) = 10?\n(b) What is P(X = a)?\n(c) Does P(X = a) = 0 mean X never equals a?\n\n18.05 class 5 problems, Spring 2022\nConcept question 5.\nWhich of the following are graphs of valid cumulative distribution functions?\nx\n-4 -2\n-0.5\nA.\nx\n-4 -2\n-0.5\nB.\nx\n-4 -2\n-0.5\nC.\nx\n-4 -2\n-0.5\nD.\nBoard questions\nProblem 1.\n(a) Let X ∼ Bernoulli(p). Compute Var(X).\n(b) Let Y ∼ Bin(n, p). Show Var(Y) = np(1 -p).\n(c) Suppose X1, X2, ... , Xn are independent and all have the same standard deviation σ = 2.\nLet X be the average of X1, ... , Xn.\nWhat is the standard deviation of X?\nProblem 2.\nSuppose X has range [0, 2] and pdf f(x) = c x2.\n(a) What is the value of c?\n(b) Compute the cdf F (x).\n(c) Compute P(1 ≤X ≤2).\n(d) Plot the pdf and use it to illustrate part (c).\nProblem 3.\nSuppose Y has range [0, b] and cdf F (y) = y2/9.\n(a) What is b?\n(b) Find the pdf of Y .\nProblem 4.\nI've noticed that taxis drive past 77 Mass. Ave. on the average of once every 10 minutes.\nSuppose time spent waiting for a taxi is modeled by an exponential random variable\nX ∼ Exponential(1/10);\nf(x) = 10\n1 e-x/10\n(a) Sketch the pdf of this distribution\n(b) Shade the region which represents the probability of waiting between 3 and 7 minutes\n(c) Compute the probability of waiting between between 3 and 7 minutes for a taxi\n(d) Compute and sketch the cdf.\n\n18.05 class 5 problems, Spring 2022\nIn class examples and discussion\nExample. Computation from tables\nCompute the variance and standard deviation of X.\nvalues x\npmf p(x)\n1/10\n2/10\n4/10\n2/10\n1/10\nExample. A very useful formula\nRecompute the previous example using the very useful formula for variance\nn\nVar(X) = E[X2] -E[X]2 = (∑p(xi)x2\ni) -μ2.\ni=1\nExtra problems\nExtra 1.\nLet X take value 1, with equal probability on {1, 2, 3, 4, 5} (X is a uniform\nrandom variable). Compute Var(X).\nLet Y be uniform on {7, 8, 9, 10, 11}. What is the variance of Y ?\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class 05b: Problems",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_class05b_pset.pdf",
      "content": "Class 5b in-class problems, 18.05, Spring 2022\nBoard questions\nProblem 1.\nI've noticed that taxis drive past 77 Mass. Ave. on the average of once every 10 minutes.\nSuppose time spent waiting for a taxi is modeled by an exponential random variable\nX ∼ Exponential(1/10);\nf(x) = 10\n1 e-x/10\n(a) Sketch the pdf of this distribution\n(b) Shade the region which represents the probability of waiting between 3 and 7 minutes\n(c) Compute the probability of waiting between between 3 and 7 minutes for a taxi\n(d) Compute and sketch the cdf.\nProblem 2. Gallery of distributions\nOpen the Gallery of probability distributions applet at\nhttps://mathlets.org/mathlets/probability-distributions/\n(a) For the standard normal distribution N(0, 1) how much probability is within 1 of the\nmean?\nWithin 2? Within 3?\n(b) For N(0, 32) how much probability is within σ of the mean? Within 2σ? Within 3σ.\n(c) Does changing μ change your answer to problem 2?\n(d) Use the applet to find the median of the exp(0.5) distribution.\n(The median is the value of x where half the probability is below x and half above.)\nProblem 3. Manipulating random variables\n(a) Suppose X ∼ uniform(0,2). If Y = 4X, find the range, pdf and cdf of Y .\n(b) Suppose X ∼ uniform(0,2). If Y = X3, find the range, pdf and cdf of Y .\n(c) Suppose Z ∼ Norm(0, 1) (standard normal). Find the range, pdf and cdf of Y =\n3Z + 2.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class 01: Problem Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_class01_pset_sol.pdf",
      "content": "Class 1 in-class problems, 18.05, Spring 2022\nConcept questions\nConcept question 1. Poker hands\nThe probability of a one-pair hand is:\n(1) less than 5%\n(2) between 5% and 10%\n(3) between 10% and 20%\n(4) between 20% and 40%\n(5) greater than 40%\nSolution: We will do this later. Perhaps surprisingly the answer is greater than 40%\nConcept question 2. DNA\n1. DNA is made of sequences of nucleotides: A, C, G, T.\nHow many DNA sequences of length 3 are there?\n(i) 12\n(ii) 24\n(iii) 64\n(iv) 81\nSolution: 1. (iii) 4 × 4 × 4 = 64\nConcept question 3. DNA\n2. How many DNA sequences of length 3 are there with no repeats?\n(i) 12\n(ii) 24\n(iii) 64\n(iv) 81\nSolution: (ii) 4 × 3 × 2 = 24\nBoard questions\nBoard question 1. Inclusion/Exclusion\nA band consists of singers and guitar players:\n7 people sing, 4 play guitar, 2 do both\nHow many people are in the band?\nSolution: In set notation, let B be entire band, S the singers, G the guitar players. We\nknow B = S∪G, so\n|B| = |S ∪G| = |S| + |G| -|S ∩G| = 7 + 4 -2 = 9 .\nBoard question 2. Rule of product\nThere are 5 Competitors in an Olympics 100m final.\nHow many ways can gold, silver, and bronze be awarded?\nSolution: 5 × 4 × 3.\nThere are 5 ways to pick the winner. Once the winner is chosen there are 4 ways to pick\nsecond place and then 3 ways to pick third place.\n\n18.05 class 1 problems, Spring 2022\nBoard question 3.\nI won't wear green and red together; I think black or denim goes with anything; Here is my\nwardrobe.\nShirts: 3B, 3R, 2G; sweaters 1B, 2R, 1G; pants 2D,2B.\nHow many different outfits can I wear?\nSolution: Suppose we choose shirts first. Depending on whether we choose red compatible\nor green compatible shirts there are different numbers of sweaters we can choose next. So\nwe split the problem up before using the rule of product. A multiplication tree is an easy\nway to present the answer.\nR\nB\nG\nR,B\nR,B,G\nB,G\nB, D\nB, D\nB, D\nShirts\nSweaters\nPants\nMultiplying down the paths of the tree:\nNumber of outfits = (3 × 3 × 4) + (3 × 4 × 4) + (2 × 2 × 4) = 100\nBoard question 4.\n(a) Count the number of ways to get exactly 3 heads in 10 flips of a coin.\n(b) For a fair coin, what is the probability of exactly 3 heads in 10 flips?\n(10\n3 ).\nSolution: (a) We have to 'choose' 3 out of 10 flips for heads:\n(b) There are 210 possible outcomes from 10 flips (this is the rule of product). For a fair\ncoin each outcome is equally probable so the probability of exactly 3 heads is\n(10\n3 )\n= 1024 = 0.117\n(c) Source unknown. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see https://ocw.mit.edu/fairuse.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Problem Set 01 Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_pset01_sol.pdf",
      "content": "18.05 Problem Set 1, Spring 2022 Solutions\nProblem 1. (20 pts.) (6 card draw)\nYou can easily look up the probability of 5 card poker hands, e.g. https://en.wikipedia.\norg/wiki/Poker_probability\nFor this problem, let's consider hands with 6 cards. Here are two types:\nTwo-pair: Two cards have one rank, two cards have another rank, and the remaining two\ncards have two different ranks. e.g. {2♡, 2♠, 5♡, 5♣, Q♢, K♢}\nThree-of-a-kind: Three cards have one rank and the remaining three cards have three other\nranks. e.g. {2♡, 2♠, 2♣, 5♣, 9♠, K♡}\nCalculate the probability of each type of hand. Which is more probable?\nSolution: (Reasons below.)\nP (two-pair) = 0.1214,\nP (three-of-a-kind) = 0.03596,\ntwo pairs is more likely\nWe create each hand by a sequence of actions and use the rule of product to count how\nmany ways it can be done. (Critically: the number of choices available at each step is\nindependent of the choices made in the earlier steps.)\ntwo pair\nWe build a hand with two pair with the following steps.\n1. Choose 2 ranks -one for each pair: (13\n2 ) ways.\n2. Choose 2 cards from each chosen rank: 4 cards per suit: (4\n2)(4\n2) ways.\n3. Choose 2 of the remaining ranks for the remaining 2 cards in the hand: (11\n2 ) ways.\n4. Choose 1 card from each of these ranks: (4\n1)(4\n1) ways\nSo, the number of hands that are two pair is\n(13\n2 )(4\n2)(2\n4)(11\n2 )(4\n1)(1\n4) = 2471040.\nWe divide this by the total number of hands to get\nP (two pair) =\n(52\n≈ 0.1214,\n6 )\nNote that this gives the probability only because every hand is equally probable.\nThree-of-a-kind:\nWe build a hand with three of a kind with the following steps.\n1. Choose the rank of the triple: (13\n1 ) ways.\n2. Choose 3 cards from the chosen rank: 4 cards per suit: (4\n3) ways.\n\n18.05 Problem Set 1, Spring 2022 Solutions\n3. Choose 3 of the remaining ranks for the remaining 3 cards in the hand: (12\n3 ) ways.\n4. Choose 1 card from each of these ranks: (4\n1)(4\n1)(1\n4) ways\nSo, the number of hands that are three of a kind is\n(13\n1 )(3\n4)(12\n3 )(1\n4)(4\n1)(1\n4) = 732160.\nWe divide this by the total number of hands to get\nP (two pair) =\n(52\n≈ 0.03596,\n6 )\nNote that this gives the probability only because every hand is equally probable.\nProblem 2. (20: 10,10 pts.) (Non-transitive dice)\nIn class we worked with non-transitive dice:\nBlue: 3 3 3 3 3 6; Orange: 1 4 4 4 4 4; White: 2 2 2 5 5 5.\n(a) Find the probability that white beats orange, the probability that orange beats blue and\nthe probability that blue beats white.\nCan you line the dice up in order from best to worst? (Hint: this is why these are called\n'non-transitive'.)\nSolution: (Reasons below.)\n(a) P(white beats orange) = 7/12\nP(orange beats blue) = 25/36\nP(blue beats white) = 7/12.\nNo you can't line them up since blue beats white beats orange beats blue. You have to\narrange them in a circle. This was the meaning of the graphic in the class 2 slides.\nThe reasons for these answers come from the probability tables.\nBlue die\nWhite die\nOrange die\nOutcomes\nProbability\n5/6\n1/6\n1/2\n1/2\n1/6\n5/6\nThe 2 × 2 tables just below show the dice matched against each other. Each entry is\nthe probability of seeing the pair of numbers corresponding to that entry. We color the\n\n18.05 Problem Set 1, Spring 2022 Solutions\nprobability with the color of the winning die for that pair. (For obvious reasons we use\nblack instead of white when the white die wins.)\nWhite\nOrange\nBlue\n5/12\n5/12\n5/36\n25/36\n1/12\n1/12\n1/36\n5/36\nOrange\n1/12\n1/12\n5/12\n5/12\nTotaling the winning probabilities for each pair gives the probabilities stated at the start of\nthe solution.\n(b) Suppose you roll two white dice against two blue dice. What is the probability that the\nsum of the white dice is greater than the sum of the blue dice?\nFor hints on this problem and ways to make money with your dice, watch at least the first\nsix minutes of the following video.\nhttps://www.youtube.com/watch?v=zWUrwhaqq_c\nHere a tree is used to organize the calculation rather than a table. We will discuss this\nmethod in Week 2.\nSolution: P(sum of 2 white beats sum of 2 blue) = 85/144. Notice, this is a little surprising\nsince a single blue tends to beat a single white.\nThe reasoning is similar to part (a) with slightly larger tables.\n2 Blue die\n2 White die\nOutcomes\nProbability\n25/36\n10/36\n1/36\n1/4\n1/2\n1/4\n2 White\n2 Blue\n25/144\n10/144\n1/144\n50/144\n20/144\n2/144\n25/144\n10/144\n1/144\nSumming the different entries, We see, P (white wins) = 85/144 (so P (blue wins) = 59/144).\nProblem 3. (55: 5,5,10,5,10,10,10 pts.) (Birthdays: counting and simulation)\nIgnoring leap days, the days of the year can be numbered 1 to 365. Assume that birthdays\nare equally likely to fall on any day of the year. Consider a group of n people, of which you\nare not a member. An element of the sample space S will be a sequence of n birthdays (one\nfor each person).\n(a) Define the probability function P for S. (This will depend on n.)\nSolution: The sample space S is the set of all sequences of n birthdays. That is, all\nsequences\nω = (b1, b2, b3, ... , bn),\nwhere each entry is a number between 1 and 365.\nThere are 365n sequences of n birthdays. Since they are all equally likely, P (ω) =\nfor\n365n\nevery sequence ω.\n\n18.05 Problem Set 1, Spring 2022 Solutions\n(b) Consider the following events:\nA: \"someone in the group shares your birthday\"\nB: \"some two people in the group share a birthday\"\nC: \"some three people in the group share a birthday\"\nCarefully describe the subset of S that corresponds to each event.\nSolution: Event A: Suppose my birthday is on day b. Then \"an outcome ω is in A\" is\nequivalent to \"b is in the sequence for ω\", i.e. b = bk for some index k between 1 and n.\nMore symbolically,\nan outcome ω is in A if and only if bk = b for some index k in 1, ... , n.\nEvent B: \"An outcome ω is in B\" is equivalent to \"two of the entries in ω are the same\". That\nis, an outcome ω is in B\nif and only if\nbj = bk for two (different) indices j, k in 1, ... , n.\nEvent C: an outcome ω is in C\nif and only if\nbj = bk = bl for three (distinct) indices\nj, k, l in 1, ... , n.\n(c) Find an exact formula for P (A). What is the smallest n such that P (A) > 0.5?\nSolution: It's easier to calculate P (Ac). Since there are 364 birthdays that are not mine,\nthere are 364n outcomes in Ac.\nP(A) = 1 -P(Ac) = 1 - 364n\n365n .\nWe can find the size of the group needed for P (A) > 0.5 by trial and error, plugging in\ndifferent values of n. Or we can set P (A) = 0.5 and solve for n.\nn\n1 - 364n\n= 0.5 ⇒ (364\nln (364\n= 0.5 ⇒ n ⋅\n365) = ln(0.5) ⇒ n ≈ 252.65\n365n\n365)\nSo there needs to be at least 253 people for it to be more likely than not that one of them\nshares your birthday.\n(d) Justify why n in part (c) is greater than 365 without doing any computation. (We are\nlooking for a short answer giving a heuristic sense of why this is so.)\nSolution: Ignoring the fractions, 365/2 different birthdays would have a 50 percent chance\nof matching your birthday. But, 365/2 people probably don't all have different birthdays,\nso they have a less than 50 percent chance of matching.\n(e) Use R simulation to estimate the smallest n for which P (B) > 0.9. For these simulations,\nlet the number of trials be 10000. (You can reuse your code from Studio 1.)\nFor this value of n, repeat the simulation a few times to verify that it always gives similar\nresults.\nUsing 10000 trials you saw very little variation in the estimate of P (B). Try this again\nusing 30 trials and verify that the estimated probabilities are much more variable. On your\npset, give the results 7 runs of 30 trials using the value of n you just found.\nSolution: Here's the R code I ran to estimate P (B).\n# colMatches is an 18.05 function. It needs to be in the R working directory\n# You can find the file mit18_05_s22_colMatches.r on our class website.\nsource('mit18_05_s22_colMatches.r')\n\n18.05 Problem Set 1, Spring 2022 Solutions\n# Set up the parameters\nndays = 365\nnpeople = 20\nntrials = 10000\nsize_match = 2\nyear = 1:ndays\n# Run ntrials -one per column- using sample() and matrix()\ny = sample(year, npeople*ntrials, replace=TRUE)\ntrials = matrix(y, nrow=npeople, ncol=ntrials)\nw = colMatches(trials, size_match)\nprob_B = mean(w)\nprint(prob_B)\nI ran the code with various values of npeople. Here is a table of the estimated values of\nP (B).\nnpeople\nP (B) (multiple values means multiple runs of the code)\n0.4123\n0.7007\n0.8913, 0.8867, 0.8926, 0.8865\n0.9005, 0.9003, 0.9037, 0.9006, 0.903\nWe see that the estimated probability of B is consistently less than 0.9 for npeople = 40\nand consistently greater than 0.9 for npeople = 41. Therefore: Answer: 41.\nWhen we run the code with ntrials = 30 and npeople = 41 we get the following estimates\nfor P (B).\n0.9333, 0.9333, 0.9, 0.8333, 0.9333, 0.8, 1\nThis is certainly much more variable than the estimates in in the table above.\n(f) Find an exact formula for P (B).\nSolution: It's easier to calculate P (Bc), the probability that all n birthdays are distinct.\nTo choose n different birthdays: there are 365 choices for the first birthday, 364 for the\nsecond birthday, etc. So\n⋅364 ⋯(365 -n+ 1)\n365!\nP(B) = 1 -P(Bc) = 1 - 365\n= 1 -\n365n\n(365 - n)! ⋅ 365n .\n(g) Use R simulation to estimate the smallest n for which P(C) > 0.5. Again use 10000\ntrials. You will find that two adjacent values of n are equally plausible based on simulations.\nYou may pick either one for your answer.\nNote that it is much harder to find an exact formula for P (C), so simulation is especially\nhandy.\nSolution: To estimate P (C) we used the same code as in part (e), except we set size_match\n= 3. Here is the table of (estimated) probabilities we found.\n\n18.05 Problem Set 1, Spring 2022 Solutions\nnpeople\nP (C) (multiple values means multiple runs of the code)\n0.0121\n0.0293\n0.1274\n0.4241\n0.5389\n0.4832\n0.4841\n0.502, 0.4989, 0.4909, 0.5035\n0.5081, 0.5115, 0.5149, 0.5071\nThe estimates for 87 are sometimes above and sometimes below 0.5. So the answer to the\nquestion is either 87 or 88. It seems certain that with 88 people P(C) > 0.5.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class 02: Problem Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_class02_pset_sol.pdf",
      "content": "Class 2 in-class problems, 18.05, Spring 2022\nConcept questions\nConcept question 1. (What's the event?)\n(Connecting words and set notation.)\nExperiment: toss a coin 3 times.\nWhich of following equals the event \"exactly two heads\"?\nA = {THH, HTH, HHT, HHH}\nB = {THH, HTH, HHT}\nC = {HTH, THH}\n(1) A\n(2) B\n(3) C\n(4) B or C\nSolution: : (2) B.\nThe event \"exactly two heads\" determines a unique subset, containing all outcomes that\nhave exactly two heads.\nConcept question 2. (Describe the event.)\n(Connecting words and set notation.)\nExperiment: toss a coin 3 times.\nWhich of the following describes the event {T HH, HT H, HHT }?\n(1) \"exactly one head\"\n(2) \"exactly one tail\"\n(3) \"at most one tail\"\n(4) none of the above\nSolution: (2) \"exactly one tail\"\nNotice that the same event E may be described in words in multiple ways, e.g. \"exactly 2\nheads\" or \"exactly 1 tail\".\nConcept question 3. (Are they disjoint?)\n(Connecting words and set notation.)\nExperiment: toss a coin 3 times.\nThe events \"exactly 2 heads\" and \"exactly 2 tails\" are disjoint is.\n(1) True\n(2) False\nSolution: True: {THH, HTH, HHT} ∩{TTH, THT, HTT} = ∅.\nConcept question 4. (Does A imply B?)\n(Connecting words and set notation)\nConsider two events: A and B.\nAre the words \"A implies B\" equivalent to A ⊆ B?\n(1) True\n(2) False\n\n18.05 class 2 problems, Spring 2022\nSolution: True.\nFor example: If you tossed \"exactly 2 heads\", then you know you tossed \"at least two\nheads\". This says exactly the same thing as\n{THH, HTH, HHT} ⊂ {THH, HTH, HHT, HHH}.\nBoard problems\nProblem 1. Poker hands\nDeck of 52 cards\n- 13 ranks: 2, 3, ... , 9, 10, J, Q, K, A\n- 4 suits: ♡, ♠, ♢, ♣,\nA poker hand consists of 5 cards\nA one-pair hand consists of two cards having one rank and the remaining three cards having\nthree other ranks\nExample: {2♡, 2♠, 5♡, 8♣, K♢}\n(a) How many different 5 card hands have exactly one pair?\nHint: practice with how many 2 card hands have exactly one pair.\nHint for hint: use the rule of product.\n(b) What is the probability of getting a one pair poker hand?\n(a) Solution: We need a systematic algorithm that produces every possible hand exactly\nonce.\nWe can do this two ways: as combinations or as permutations. The keys are:\n1. Be consistent.\n2. Break the problem into a sequence of actions and use the rule of product.\nNote, there are many ways to organize this. We will break it into very small steps in order\nto make the process clear.\nCombinations approach\nConsider a hand as a set of 5 cards, i.e. order doesn't matter.\nCount the number of one-pair hands, by describing a procedure to make each one exactly\nonce.\nAction 1. Choose the rank of the pair: 13 different ranks, choosing 1, so (13\n1 ) ways to do\nthis.\nAction 2. Choose 2 cards from this rank: 4 cards in a rank, choosing 2, so (4\n2) ways to do\nthis.\nAction 3. Choose the 3 cards of different ranks: 12 remaining ranks, so (12\n3 ) ways to do this.\nAction 4. Choose 1 card from each of these ranks: 4 cards in each rank so 43 ways to do\nthis.\n\n18.05 class 2 problems, Spring 2022\nAnswer (Using the rule of product.)\n(13\n(12\n1 ) ⋅(2\n4) ⋅\n3 ) ⋅43 = 1098240\n(b) To compute the probability we have to count all possible 5 card hands. We must stay\nconsistent and count combinations.\nTo make a 5 card hand we choose 5 cards out of 52, so there are\n(52\n5 ) = 2598960\npossible hands. Since each hand is equally probable, the probability of a one-pair hand is\n1098240/2598960 = 0.42257.\nWe redo the problem using a permutation approach.\nThis approach is a little trickier. We include it to show that there is usually more than one\nway to count something.\n(a) Count the number of one-pair hands, where we keep track of the order they are dealt.\nAction 1. (This one is tricky.) Choose the positions in the hand that will hold the pair: 5\ndifferent positions, so (5\n2) ways to do this.\nAction 2. Put a card in the first position of the pair: 52 cards, so 52 ways to do this.\nAction 3. Put a card in the second position of the pair: since this has to match the first\ncard, there are only 3 ways to do this.\nAction 4. Put a card in the first open slot: this can't match the pair so there are 48 ways\nto do this.\nAction 5. Put a card in the next open slot: this can't match the pair or the previous card,\nso there 44 ways to do this.\nAction 6. Put a card in the last open slot: there are 40 ways to do this.\nAnswer: (Using the rule of product.)\n(5\n2) ⋅52 ⋅3 ⋅48 ⋅44 ⋅40 = 131788800\nways to deal a one-pair hand where we keep track of order.\n(b) There are\n52P5 = 52 ⋅51 ⋅50 ⋅49 ⋅48 = 311875200\nfive card hands where order is important. Thus, the probability of a one-pair hand is\n131788800/311875200 = 0.42257.\n(Both approaches give the same answer.)\n\n18.05 class 2 problems, Spring 2022\nProblem 2. (Inclusion-exclusion)\nSupposes a class has 50 students: 20 male (M), 25 brown-eyed (B)\nFor a randomly chosen student, what is the range of possible values for p= P(M ∪B)?\n(a) p ≤ 0.4\n(b) 0.4 ≤ p ≤ 0.5\n(c) 0.4 ≤ p ≤ 0.9\n(d) 0.5 ≤ p ≤ 0.9\n(e) 0.5 ≤ p\nSolution: (d) 0.5 ≤ p ≤ 0.9\nExplanation:\nThe easy way to answer this is that A ∪ B has a minumum of 25 members (when all males\nare brown-eyed) and a maximum of 45 members (when no males have brown-eyes). So, the\nprobability ranges from 0.5 to 0.9\nThinking about it in terms of the inclusion-exclusion principle we have\nP(M ∪B) = P(M) + P(B) -P(M ∩B) = 0.9 -P(M ∩B).\nSo the maximum possible value of P (M ∪B) happens if M and B are disjoint, so P (M ∩B) =\n0. The minimum happens when M ⊂B, so P(M ∩B) = P(M) = 0.4.\nProblem 3. D20\nConsider the following experiment. Roll a 20-sided die (D20) 9 times. What is the probability\nthat all 9 rolls are distinct.\n(In class we could actually run this experiment many times with a real die and see how\nwell the experimental data matched the theoretical probability. Later, we will learn how to\nsimulate this experiment in R.)\nFor this experiment, how would you define the sample space, probability function, and event?\nCompute the probability that all rolls (in one trial of 9 rolls) are distinct.\n20 ⋅ 19 ⋯13 ⋅ 12\nSolution:\n= 0.119.\nReasoning\nFor the sample space S we take all sequences of 9 numbers between 1 and 20. We find the\nsize of S using the rule of product. There are 20 ways to choose the first number in the\nsequence, followed by 20 ways to choose the second, etc. Thus, |S| = 209. Of course, each\nof these outcomes is equally likely.\nIn our case, A is the event 'all 9 numbers in the sequence are distinct'.\nWe can use the rule of product to compute |A| as follows. There are 20 ways to choose the\nfirst number, then 19 ways to choose the second, etc. down to 12 ways to choose the ninth\nnumber. Thus, we have\n|A| = 20 ⋅19 ⋅18 ⋅17 ⋅16 ⋅15 ⋅14 ⋅13 ⋅12\nThat is |A| = 20P9.\n\n18.05 class 2 problems, Spring 2022\nPutting this all together\n20 ⋅ 19 ⋅ 18 ⋅ 17 ⋅ 16 ⋅ 15 ⋅ 14 ⋅ 13 ⋅ 12\nP (A) =\n≈ 0.119\nProblem 4. Jon's Dice\nJon has three six-sided dice with unusual numbering.\nA game consists of two players each choosing a die. They roll once and the highest number\nwins.\nWhich die would you choose?\n1. Make probabilitiy tables for the the blue and white dice.\n2. Make a probability table for the product sample space of blue and white. That is, suppose\nyou roll both dice and record the result as an ordered pair, (blue value, white value). List\nall the possible outcomes and their probabilities.\n3. Use the table to compute the probability that blue beats white.\n4. Pair up with another group. Have one group compare blue vs. orange and the other\ncompare orange vs. white. Based on the three comparisons, rank the dice from best to worst.\nSolution: Here are the probability tables we can use to compare the dice.\nBlue die\nWhite die\nOrange die\nOutcomes\nProbability\n5/6\n1/6\n3/6\n3/6\n1/6\n5/6\n- The 2 × 2 tables show pairs of dice.\n- Each entry is the probability of seeing the pair of numbers corresponding to that\nentry.\n- The color and letter gives the winning die for that pair of numbers. (We use black\ninstead of white when the white die wins.)\nWhite\nOr\nange\nBlue\n15/36 (B)\n3/36 (B)\n15/36 (W)\n3/36 (B)\n5/36 (B)\n1/36 (B)\n25/36 (O)\n5/36 (B)\nOrange\n3/36 (W)\n15/36 (O)\n3/36 (W)\n15/36 (W)\n\n18.05 class 2 problems, Spring 2022\nThe three comparisons are:\nP(blue beats white)\n=\n21/36 = 7/12\nP(white beats orange)\n=\n21/36 = 7/12\nP(orange beats blue)\n=\n25/36\nThus: blue is better than white is better than orange is better than blue.\nThere is no best die: the property of being 'better than' is not transitive.\nProblem 5. Lucky Lucy\nThis is doable, but challenging problem. You should be able to compute P (A) and P (B).\nIt takes a bit of algebraic cleverness to decide if one is always bigger than the other.\nLucky Lucy has a coin that you're quite sure is not fair.\n- They will flip the coin twice\n- Let A be the event the tosses are the same, i.e. {HH, TT}\n- Let B be the event the tosses are the different, i.e. {HT, TH}\nLet p be the probability of heads. Compute and compare P (A) and P (B).\n(If you don't see the symbolic algebra try p = 0.2, p=0.5)\nSolution: We're given P(H) = p. It's probably easiest to introduce a new letter: let\nP(T) = q. Of course, q = 1 -p, or p+ q = 1.\nWe have the following table for two tosses\nOutcomes\nHH\nTT\nHT\nTH\nProbability\np2\nq2\npq\nqp\nSo, P(A) = p2 + q2 and P (B) = 2pq.\nHere's the trick: Since the coin is unfair, p = q. Thus (p-q)2 > 0. Now expand this out:\n(p-q)2 > 0 ⇒ p2 -2pq+ q2 > 0 ⇒ p2 + q2 > 2pq.\nThe last inequality shows, as long as p = q, P (A) > P (B). That is, Lucy is more likely to\nhave both tosses the same.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Problem Set 02 Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_pset02_sol.pdf",
      "content": "18.05 Problem Set 2, Spring 2022 Solutions\nProblem 1. (20: 10,10 pts.) ('Binary' paradox)\nFor this problem assume that puppies are equally probable to be male or female. Likewise\nfor kittens.\nBe sure to carefully justify your answers.\n(a) Our dog Layla had two puppies. The older puppy is female. What is the probability that\nboth puppies are female?\nSolution: Listing the sex of older puppy first, our sample space is {MM, MF , F M, F F } .\nThe event \"the older puppy is female\" is {F M, F F } and the event \"both puppies are female\"\nis {F F } . Thus the probability that both puppies are females given the older is female is\nP(FF)\n1/4\nP(FF|{FM, FF}) = P({FM, FF}) = 1/2 = 2.\n(b) Our cat Ariel had two kittens. At least one of them is male. What is the probability\nthat both kittens are males?\nSolution: The event \"at least one kitten is male\" is {MM, MF , F M} so the probability\nthat both kittens are male is\nP(MM)\n1/4\nP(MM|{MM, MF, FM}) = P({MM, MF, FM}) = 3/4 = 3.\nProblem 2. (15 pts.) (The blue taxi)\nIn a city with one hundred taxis, 1 is blue and 99 are green. A witness observes a hit-and-run\nby a taxi at night and recalls that the taxi was blue, so the police arrest the blue taxi driver\nwho was on duty that night. The driver proclaims their innocence and hires you to defend\nthem in court. You hire a scientist to test the witness' ability to distinguish blue and green\ntaxis under conditions similar to the night of accident. The data suggests that the witness\nsees blue cars as blue 99% of the time and green cars as blue 2% of the time.\nWrite a speech for the jury to give them reasonable doubt about your client's guilt. Your\nspeech need not be longer than the statement of this question. Keep in mind that most jurors\nhave not taken this course, so an illustrative table may be easier for them to understand\nthan fancy formulas.\nSolution: The defense will try to make the case that this is likely to be a case of random\nmis-identification. So we look for the probability a random taxi the witness sees as blue is\nactually blue.\nThis is a question of 'inverting' conditional probability. We know\nP (the witness sees blue|the car is blue)\nbut we'd like to know\nP (the car is blue|the witness sees blue).\nOur first job is to translate this to symbols.\n\n18.05 Problem Set 2, Spring 2022 Solutions\nLet Wb = 'witness sees a blue taxi' and let Wg = 'witness sees a green car'. Further, let Tb\n= 'taxi is blue' and let Tg = 'taxi is green'. With this notation we want to find P (Tb|Wb).\nWe will compute this using Bayes' formula\nP (Wb|Tb) ⋅ P (Tb)\nP (Tb|Wb) =\n.\nP (Wb)\nAll the pieces are represented in the following diagram.\nTG\nTB\nWB\nWG\nWB\nWG\n0.01\n0.99\n0.99\n0.01\n0.02\n0.98\nRandom taxi\nWitness sees\nWe can determine each factor in the right side of Bayes' formula:\nWe are given P (Tb) = 0.01 (so P (Tg) = 0.99).\nWe are given, P (Wb|Tb) = 0.99 and P(Wb|Tg) = 0.02.\nWe compute P (Wb) using the law of total probability:\nP(Wb) = P(Wb|Tb)P(Tb) + P(Wb|Tg)P(Tg) = 0.99 × 0.01 + 0.02 × 0.99 = 0.99 × 0.03.\nPutting all this in Bayes' formula we get\nP (Tb|Wb) = 0.99 × 0.01\n0.99 × 0.03 = 1\nOur speech:\nLadies and gentlemen of the jury. The prosecutor tells you that the witness is nearly flawless\nin their ability to distinguish whether a taxi is green or blue. They claims that this implies\nthat beyond a reasonable doubt the taxi involved in the hit and run was blue. However\nprobability theory shows without any doubt that a random taxi which the witness sees as\nblue is actually blue only 1/3 of the time. This is considerably more than a reasonable\ndoubt. In fact it is more probable than not that the taxi involved in the accident was green.\nIf the probability doesn't fit you must acquit!\nNote: Confusing of P (testimony|guilty) for P (guilty|testimony) is known as the prosecutor's\nfallacy\nProblem 3. (20 pts.) (Trees of cards) There are 8 cards in a hat:\n{1♡, 1♠, 1♢, 1♣, 2♡, 2♠, 2♢, 2♣}\nYou draw one card at random. If its rank is 1 you draw one more card; if its rank is two\nyou draw two more cards. Let X be the sum of the ranks on the 2 or 3 cards drawn. Find\nE[X]. (Note: all the draws are done without replacement.)\nSolution: The following tree shows all the possible ways we can draw cards. The first draw\nis the top row of nodes. The value of the draw is given in the circle. The probability of\nthat draw is given along the edge. Likewise for the second and third draws. At the end of\neach path we give the value of X resulting from those draws.\n\n18.05 Problem Set 2, Spring 2022 Solutions\nX = 2\nX = 3\nX = 4\nX = 5\nX = 5\nX = 6\n4/8\n4/8\n3/7\n4/7\n4/7\n3/7\n3/6\n3/6\n4/6\n2/6\nThis gives us the probability distribution of X:\nvalue x\npmf p(x)\n3/14\n2/7\n1/7\n2/7\n1/14\nSo,\nE[X] = 2 ⋅14 + 3 ⋅7 + 4 ⋅7 + 5 ⋅7 + 6 ⋅\n7 ≈3.7143.\n14 =\nProblem 4. (25: 5,10,5,5 pts.) (Dice) There are four dice in a drawer: one D4 (4 sides),\none D6 (6-sides), and two D8 (8 sides). As usual, the sides of a die are numbered 1 to n,\nwhere n is the number of sides.\nYour friend secretly grabs one of the four dice at random. Let S be the number of sides on\nthe chosen die.\n(a) What is the pmf of S?\nSolution: Here is the pmf:\nvalue s\npmf p(s)\n1/4\n1/4\n1/2\n(b) Now, without showing it to you, your friend rolls the chosen die and tells you the result.\nLet R be the result of the roll.\nUse Bayes' rule to find P(S = s| R = 3) for s = 4, 6, 8. Which die is most likely if R = 3?\nTerminology: You are computing the pmf of 'S given R = 3'.\nSolution: Bayes' rule says\nP(R = 3 | S = s)P(S = s)\nP(S = s|R= 3) =\n.\nP(R = 3)\nWe summarize what we know in a tree. In the tree the notation D4 means the 4-sided die\n(S = 4), likewise R3 means a 3 was rolled (R = 3). Because we only care about the case\nR = 3 the tree does not include other possible rolls.\nD4\nD6\nD8\nR3\nR3\nR3\n1/4\n1/4\n1/2\n1/4\n1/6\n1/8\nChosen die\nRoll result\n\n18.05 Problem Set 2, Spring 2022 Solutions\nWe have the following probabilities (you should identify them in the tree):\nP(R = 3 | S = 4) = 1/4,\nP(R = 3 | S = 6) = 1/6,\nP(R = 3 | S = 8) = 1/8.\nThe law of total probability gives (again, see how the tree tells us this):\nP(R = 3) = P(R = 3 | S= 4)P(S = 4) + P(R = 3 | S= 6)P(S = 6) + P(R = 3 | S= 8)P(S = 8)\n=\n⋅4 + 1 ⋅4 + 1 ⋅\n= 6.\nHence,\nP(R = 3 | S= 4) ⋅P(S = 4)\n(1/4) ⋅(1/4)\nP(S = 4 | R= 3) =\n=\n=\nP(R = 3)\n1/6\nP(R = 3 | S= 6) ⋅P(S = 6)\n(1/6) ⋅(1/4)\nP(S = 6 | R= 3) =\n=\n=\nP(R = 3)\n1/6\nP(R = 3 | S= 8) ⋅P(S = 8)\n(1/8) ⋅(1/2)\nP(S = 8 | R= 3) =\n=\n=\n.\nP(R = 3)\n1/6\nWe see that the 4 and 8 sided dice are equally most likely given R = 3.\n(c) Which die is most likely if R = 6? Hint: You can either repeat the computation in (b),\nor you can reason based on your result in (b).\nSolution: In a similar vein, we have\nP(R = 6|S = 4) = 0,\nP(R = 6|S = 6) = 1/6,\nP(R = 6|S = 8) = 1/8.\nand\nP(R = 6) = 0 ⋅4 + 1 ⋅4 + 1 ⋅ 2 =\n48.\nSo,\n1/24\n1/16\nP(S = 4|R= 6) = 0,\nP(S = 6|R= 6) =\nP(S = 8|R= 6) =\n5/48 = 5,\n5/48 = 5.\nThe eight-sided die is more likely. Note, the denominator is the same in each probability,\ni.e. the total probability P(R = 6), so all we really had to check was the numerator.\n(d) Which die is most likely if R = 7? No computations are needed!\nSolution: The only way to get R = 7 is if we picked an 8-sided die D8.\nProblem 5. (10 pts.) (Seating arrangement and relative height) A total of n people\nrandomly take their seats around a circular table with n chairs. No two people have the same\nheight. What is the expected number of people who are shorter than both of their immediate\nneighbors?\nSolution: Label the seats 1 to n going clockwise around the table. Let Xi be the Bernoulli\nrandom variable with value 1 if the person in seat i is shorter than their neighbors. Then\n\n18.05 Problem Set 2, Spring 2022 Solutions\n∑\nn\nX =\ni=1 Xi represents the total number of people who are shorter than both of their\nneighbors, and, by linearity of expected value,\nn\nn\nE[X] = E[∑Xi] = ∑E[Xi]\ni=1\ni=1\nRecall that this property of expected values holds even when the Xi are dependent, as is\nthe case here!\nAmong 3 random people the probability that the middle one is the shortest is 1/3. Therefore\nXi ∼ Bernoulli(1/3), which implies E[Xi] = 1/3. Therefore the expected number of people\nshorter than both their neighbors is\nn\nE[X] = ∑E[Xi] = n\n3 .\ni=1\nProblem 6. (20: 3,2,10,5 pts.)\nIn this problem we will use R to simulate flipping a fair coin 50 times. We'll use the\nsimulation to explore 'runs'. That is, sequences of all 1's or 0's.\n(a) Make up a sequence of length 50 consisting of ones and zeros. Try to make the sequence\nlook like it was randomly generated by flipping a coin.\nSolution: Any sequence of 50 zeros and ones is valid.\n(b) A run is a sequence of all 1s or all 0s. How long is the longest run in your answer to\npart (a)?\nSolution: Answers will vary because we can't know the sequence you chose. However,\nmost people, when trying to be random, do not put in any long runs. Parts (c) and (d)\nshow these actually happen frequently.\n(c) Now we will use R to simulate 50 tosses of a fair coin and estimate the average length\nof the longest run. The code below simulates one trial. You will need to use a 'for loop' to\nrun 10000 trials. On our MITx site you can find tutorials on both for loops and the rle()\nfunction used in the code. (As usual, choose the Course info tab and click on the R code\nlink under course handouts).\nSample code illustrating the use of rle().\n# R code to simulate 50 flips of a fair coin and find the longest run\n# rle stands for `run length encoding'.\n# rle(trial)$lengths is a vector of the lengths of all the different runs\n# in trial.\nnflips = 50\ntrial = rbinom(nflips, 1, 0.5) # Note: binomial(1, 0.5) = bernoulli(0.5)\nmax_run = max(rle(trial)$lengths)\nSample code illustrating 'for loops'. (These may be useful in the code for this problem.)\n# R code demonstrating `for' loops.\nfor (j in 1:5) {\nprint(j^2)\n\n18.05 Problem Set 2, Spring 2022 Solutions\n}\n(Should produce: 1, 4, 9, 16, 25.)\nsum = 0\nfor (j in 1:5) {\nsum = sum + j\n}\nprint(sum)\n(Should produce: 15.)\nUse R to simulate the average length of the longest run in 50 flips of a fair coin. Do this\nthree times with 10000 trials each time and report the three results.\nSolution: Here is my code with comments\nnflips = 50\nntrials = 10000\ntotal = 0 # We'll keep a running total of all the trials' longest runs\nfor (j in 1:ntrials) {\n# One trial consists of 50 flips\ntrial = rbinom(nflips, 1, 0.5) # binomial(1, 0.5) = bernoulli(0.5)\n# rle() finds the lengths of all the runs in trials. We add the max to total\ntotal = total + max(rle(trial)$lengths)\n}\n# The average maximum run is the total/ntrials\nave_max = total/ntrials\nprint(ave_max)\nMy three runs of this code produced ave_max = 5.9863, 5.9696, 5.9804\n(d) A small modification of your code will let you estimate the probability of a run of 8 or\nmore in 50 flips. Do this three times with 10000 trials each time. Report the three results.\nSolution: Instead of keeping a total we keep a count of the number of trials with a run of\n8 or more\nnflips = 50\nntrials = 10000\n# We'll keep count of all the trials with a run of 8 or more\ncount = 0\nfor (j in 1:ntrials) {\ntrial = rbinom(nflips, 1, 0.5) # binomial(1, 0.5) = bernoulli(0.5)\ncount = count + (max(rle(trial)$lengths) >= 8)\n}\n# The probability of a run of 8 or more is count/ntrials\nprob8 = count/ntrials\nprint(prob8)\nMy run of this code produced prob8 = 0.1596, 0.1637, 0.1632\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class 03: Problem Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_class03_pset_sol.pdf",
      "content": "Class 3 in-class problems, 18.05, Spring 2022\nConcept questions\nConcept question 1.\nToss a coin 4 times. Let A = 'at least three heads' and B = 'first toss is tails'.\n1. What is P (A|B)?\n(a) 1/16\n(b) 1/8\n(c) 1/4\n(d) 1/5\n2. What is P (B|A)?\n(a) 1/16\n(b) 1/8\n(c) 1/4\n(d) 1/5\nSolution: 1. (b) 1/8.\n2. (d) 1/5.\nCounting we find |A| = 5, |B| = 8 and |A ∩ B| = 1. Since all sequences are equally likely\nP(A ∩ B)\n|A ∩ B|\n|B ∩ A|\nP (A|B) =\n=\n= 1/8. P (B|A) =\n= 1/5.\nP (B)\n|B|\n|A|\nConcept question 2. Trees 1.\nA1\nA2\nB1\nB2\nB1\nB2\nC1\nC2\nC1\nC2\nC1\nC2\nC1\nC2\nx\ny\nz\n1. The probability x represents\n(a) P (A1) (b) P (A1|B2) (c) P (B2|A1) (d) P (C1|B2 ∩ A1).\nSolution: (a) P (A1).\nConcept question 3. Trees 2.\n2. The probability y represents\n(a) P (B2) (b) P (A1|B2) (c) P (B2|A1) (d) P (C1|B2 ∩ A1).\nSolution: (c) P (B2|A1).\nConcept question 4. Trees 3.\n3. The probability z represents\n(a) P (C1) (b) P (B2|C1) (c) P (C1|B2) (d) P (C1|B2 ∩ A1).\nSolution: (d) P (C1|B2 ∩ A1).\nConcept question 5. Trees 4.\n4. The circled node represents the event\n(a) C1 (b) B2 ∩ C1 (c) A1 ∩ B2 ∩ C1 (d) C1|B2 ∩ A1.\nSolution: (c) A1 ∩ B2 ∩ C1.\n\n18.05 class 3 problems, Spring 2022\nIn class examples\nClass example 1.\n- Organize computations\n- Compute total probability\n- Compute Bayes' formula\nExample. Game: 5 orange and 2 blue balls in an urn. A random ball is selected and\nreplaced by a ball of the other color; then a second ball is drawn.\n1. What is the probability the second ball is orange?\n2. What is the probability the first ball was orange given the second ball was orange?\nB1\nO1\nO2\nB2\nO2\nB2\n5/7\n2/7\n4/7\n3/7\n6/7\n1/7\nFirst draw\nSecond draw\nSolution: 1. Let O1 be the event the first ball is orange. Likewise for O2, B1, B2. The\nlaw of total probability gives P(O2) =\n⋅7 + 2 ⋅\n=\nP (O1 ∩ O2)\n20/49\n2. Bayes' rule gives P (O1|O2) =\n=\nP (O2)\n32/49 = 32\nBoard questions\nProblem 1. Monty Hall\n- One door hides a car, two hide goats.\n- The contestant chooses any door.\n- Monty always opens a different door with a goat. (He can do this because he knows\nwhere the car is.)\n- The contestant is then allowed to switch doors if they want.\nWhat is the best strategy for winning a car?\n(a) Switch\n(b) Don't switch\n(c) It doesn't matter\n\n18.05 class 3 problems, Spring 2022\nOrganize the Monty Hall problem into a tree and compute the probability of winning if you\nalways switch.\nHint first break the game into a sequence of actions.\nSolution: Let Pswitch be the probability function when the contestant uses the switching\nstrategy. Let C represent a car and G a goat.\nWe will see that Pswitch(C) = 2/3\nOne way to show this is with a tree representing the switching strategy: First the contestant\nchooses a door, (then Monty shows a goat), then the contestant switches doors.\nC1\nG1\nC2\nG2\nC2\nG2\n1/3\n2/3\nChooses\nSwitches\nProbability Switching Wins the Car\nThe (total) probability of C is Pswitch(C) = 3\n1 ⋅0 + 3\n2 ⋅1 = 3\n2 .\nWriting this in symbols using the law of total probability: Here G1 represents a goat is\nbehind the original door chosen and G2 means a goat is behind the door that is switched\nto. Likewise C1 and C2.\nPswitch(C2) = Pswitch(C2|C1)P (C1) + Pswitch(C2|G1)P(G1) = 0 ⋅3 + 1 ⋅3 = 3.\nProblem 2. Independence\nRoll two dice and consider the following events\n- A = 'first die is 3'\n- B = 'sum is 6'\n- C = 'sum is 7'\nA is independent of\n(a) B and C\n(b) B alone\n(c) C alone\n(d) Neither B or C.\nSolution: (c). (Explanation below)\nP (A) = 1/6, P (A|B) = 1/5. Not equal, so not independent.\nP (A) = 1/6, P(A|C) = 1/6. Equal, so independent.\n\n18.05 class 3 problems, Spring 2022\nNotice that knowing B, removes 6 as a possibility for the first die and makes A more\nprobable. So, knowing B occurred changes the probability of A.\nBut, knowing C does not change the probabilities for the possible values of the first roll;\nthey are still 1/6 for each value. In particular, knowing C occured does not change the\nprobability of A.\nWe could also have done this problem by showing\nP (B|A) = P (B) or P (A ∩ B) = P (A)P (B).\nProblem 3. Evil Squirrels\nOf the one million squirrels on MIT's campus most are good-natured. But one hundred of\nthem are pure evil! An enterprising student in Course 6 develops an \"Evil Squirrel Alarm\"\nwhich they offer to sell to MIT for a passing grade. MIT decides to test the reliability of\nthe alarm by conducting trials.\n- When presented with an evil squirrel, the alarm goes off 99% of the time.\n- When presented with a good-natured squirrel, the alarm goes off 1% of the time.\n(a) If a squirrel sets off the alarm, what is the probability that it is evil?\n(b) Should MIT co-opt the patent rights and employ the system?\nOne solution\n(This is a base rate fallacy problem)\nWe are given:\nP (nice) = 0.9999,P (evil) = 0.0001 (base rate)\nP (alarm | nice) = 0.01,P (alarm | evil) = 0.99\n(c) Bigmacthealmanac. Some rights reserved. License: CC BY-SA. This content is excluded\nfrom our Creative Commons license. For more information, see https://ocw.mit.edu/fairuse.\n\n18.05 class 3 problems, Spring 2022\nP (alarm | evil)P (evil)\nP (evil | alarm) =\nP (alarm)\nP (alarm | evil)P (evil)\n= P (alarm | evil)P (evil) + P(alarm | nice)P (nice)\n(0.99)(0.0001)\n= (0.99)(0.0001) + (0.01)(0.9999)\n≈ 0.01\nSummary:\nProbability a random test is correct =\nProbability a positive test is correct ≈\n0.99\n0.01\nThese probabilities are not the same!\nAlternative method of calculation:\nEvil\nNice\nAlarm\nNo alarm\nSolution: (a) This is the same solution as above, but in a more compact notation. Let E\nbe the event that a squirrel is evil. Let A be the event that the alarm goes off. By Bayes'\nTheorem, we have:\nP (A | E)P (E)\nP(E | A) = P(A | E)P(E) + P(A | Ec)P(Ec)\n0.99 1000000\n=\n( 999900\n(0.99) ⋅( 1000000) + (0.01) ⋅ 1000000)\n≈ 0.01.\n(b) No. The alarm would be more trouble than its worth, since for every true positive there\nare about 99 false positives.\nProblem 4. Dice Game\n1. The Randomizer holds the 6-sided die in one fist and the 8-sided die in the other.\n2. The Roller selects one of the Randomizer's fists and covertly takes the die.\n3. The Roller rolls the die in secret and reports the result to the table.\nGiven the reported number, what is the probability that the 6-sided die was chosen? (Find\nthe probability for each possible reported number.)\nSolution: If the number rolled is 1-6 then P (six-sided) = 4/7.\n\n18.05 class 3 problems, Spring 2022\nIf the number rolled is 7 or 8 then P (six-sided) = 0.\nThis is a Bayes' formula problem. For concreteness let's suppose the roll was a 4. What we\nwant to compute is P (6-sided|roll 4). But, what is easy to compute is P (roll 4|6-sided).\nBayes' formula says\nP (roll 4|6-sided)P (6-sided)\nP (6-sided|roll 4) =\nP (4)\n(1/6)(1/2)\n= (1/6)(1/2) + (1/8)(1/2) = 4/7.\nThe denominator is computed using the law of total probability:\nP(4) = P(4|6-sided)P (6-sided) + P (4|8-sided)P (8-sided) =\n⋅2 + 1 ⋅ 2.\nNote that any roll of 1,2,...6 would give the same result. A roll of 7 (or 8) would give clearly\ngive probability 0. This is seen in Bayes' formula because the term P (roll 7|6-sided) = 0.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Problem Set 03 Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_pset03_sol.pdf",
      "content": "18.05 Problem Set 3, Spring 2022 Solutions\nProblem 1. (25: 10,5,10 pts.) Independence\nThree events A, B, and C are pairwise independent if each pair is independent. They are\nmutually independent if they are pairwise independent and if, in addition,\nP(A∩B ∩C) = P(A)P(B)P(C).\n(1)\n(a) Suppose we roll two 6-sided die. Consider the events:\nA = 'odd on die 1'\nB = 'odd on die 2'\nC = 'odd sum'\nAre A, B, and C pairwise independent? Are they mutually independent?\nSolution: We have P(A) = P(B) = P(C) = 1/2. Writing the outcome of die 1 first, we\ncan easily list all outcomes in the following intersections.\nA ∩ B = {(1, 1), (1, 3), (1, 5), (3, 1), (3, 3), (3, 5), (5, 1), (5, 3), (5, 5)}\nA ∩ C = {(1, 2), (1, 4), (1, 6), (3, 2), (3, 4), (3, 6), (5, 2), (5, 4), (5, 6)}\nB ∩ C = {(2, 1), (4, 1), (6, 1), (2, 3), (4, 3), (6, 3), (2, 5), (4, 5), (6, 5)}\nBy counting we see\nP (A ∩ B) = 4\n1 = P (A)P (B).\nLikewise,\nP(A∩C) = 4\n1 = P(A)P(C)\nand\nP(B∩C) = 4\n1 = P(B)P(C).\nSo, we see that A, B, and C are pairwise independent.\nHowever, A∩B∩C = ∅, since if we roll an odd on die 1 and an odd on die 2, then the sum\nof the two will be even. So, in this case,\nP(A∩B ∩C) = 0 = P(A)P(B)P(C),\nand we conclude that A, B and C are not mutually independent.\n(b) Consider the Venn diagram below. A, B and C are the overlapping circles and the\nprobabilities of each region are as marked. Does equation (1) hold. Are the events A, B, C\nmutually independent?\n0.225\n0.225\n0.175\n0.05\n0.1\n0.1\n0.125\nA\nB\nC\n\n18.05 Problem Set 3, Spring 2022 Solutions\nSolution: We start by totaling the probabilities in the regions shown to get the following\nprobabilities.\nP (A) = 0.225 + 0.05 + 0.1 + 0.125 = 0.5,\nP (B) = 0.225 + 0.05 + 0.1 + 0.125 = 0.5\nP(C) = 0.175 + 0.1 + 0.1 + 0.125 = 0.5,\nP(A∩B) = 0.05 + 0.125 = 0.175\nP(A∩C) = 0.1 + 0.125 = 0.225,\nP(B∩C) = 0.1 + 0.125 = 0.225\nP(A∩B ∩C) = 0.125\nWe see that P(A)P(B)P(C) = 0.5 ⋅0.5 ⋅0.5 = 0.125 = P(A∩B∩C). So, yes, equation (1)\nholds.\nBut, P (A)P (B) = 0.5⋅0.5 = 0.25 = P (A∩B). Since, mutual independence requires pairwise\nindependence as well as the multiplication formula for all three events, we see that the three\nevents are not independent. (Likewise P(A)P(C) = P(A∩C) and P(B)P(C) = P(B∩C).)\n(c) Consider a litter of n puppies. What value(s) of n makes the events 'the litter has\npuppies of both sexes' and 'there is at most one female' independent.\nSolution: Let A be the event \"the litter has puppies of both sexes\" and B be the event\n\"there is at most one female.\" In order for A to ever be true, we first assume that n > 1.\nNow, if we let X be the number of female puppies then we have\nP(A) = P(1 ≤X≤n-1),\nP(B) = P(X≤1),\nP(A∩B) = P(X = 1)\nn\nSince X ∼ binomial(n, 1/2) we have: P(X = 0) = 2n and P(X = 1) = (n\n1) 1 = 2n . So,\n2n\nP(A) = 1 -P(X = 0) -P(X = n) = 1 - 2\n2n\nn + 1\nP(B) = P(X = 0) + P(X = 1) =\n2n\nn\nP(A∩B) = P(X = 1) = 2n .\nSince we are told that A and B are independent, we must have P (A)P (B) = P (A ∩ B).\nThis implies\n(n+ 1\nn\n2n ) (1 - 2\nn) = 2n\nSome algebra yields\n(n+ 1) (1 - 2\nn) = n ⇔ n+ 1 - n+ 1 = n ⇔ 2n-1 = n+ 1\n2n-1\nPlugging small values of n into the above equation, we find that the two events are inde\npendent when n = 3.\nIf n = 1 then P(A) = 0, so A and B are (vacuously) independent, i.e. n = 1 is technically\nalso a solution.\nProblem 2. (25: 5,5,10,5 pts.) What does the data say?\nSuppose there is an experimental medical treatment for a cancer that, if untreated, is nearly\nalways fatal within 12-15 months. The doctors enroll 5000 patients in a study in which\n\n18.05 Problem Set 3, Spring 2022 Solutions\neach patient is given the treatment and followed for 5 years. Let X be the length of time\na random patient given the treatment survives. (If a patient is still alive at the end of the\nstudy, then X = 5 for this patient.)\nAs the statistician it is your job to analyze the data.\nTo load the data into R you should do the following:\n1. Download the data mit18_05_s22_ps3prob2-data.r. You can find this on our course\nwebsite on the page with R code.\n2. Put this file in your 18.05 R directory.\n3. In R studio make sure the working directory is set to your 18.05 R directory.\n4. Run the commands:\n> source('mit18_05_s22_ps3prob2-data.r')\n> x = get_prob2_data()\nThe variable x should now hold an array of the 5000 data points.\n(a) Use R to compute the mean, variance and standard deviation of the data.\n(b) Use the hist command to get R to plot a frequency histogram of the data. Set the\nhistogram so each bin has width 0.1 years. Print the histogram and turn it in with the pset.\nThe hist() command was introduced in Studio 3. There is also a short tutorial on using\nR to plot histograms on our class R page.\n(c) Using your answers in (a) and (b), write a short paragraph summarizing the data in a\nuseful way.\n(d) Based on the (c), what are your conclusions about the effectiveness of the treatment?\nWhat recommendations would you make for avenues of further research?\nSolution: Here is the R code for both (a) and (b).\n# 2a: Compute statistics\nsource('mit18_05_s22_ps3prob2-data.r')\nyears = get_prob2_data()\nm2a = mean(years)\nv2a = var(years)\nsd2a = sqrt(var(years))\ncat('2(a) mean =', m2a, ', var = ', v2a, ', std. dev =', sd2a, '\\n' )\n# 2(b): Make histogram. (Export image from R Studio to save)\nhist(years, freq=T, breaks=seq(0, 5, 0.1), main= \"Years of Survival\", col=\"yellow\")\n(a) The results are:\nMean ≈ 2.554528,\nvariance ≈ 4.3018,\nstd. dev ≈ 2.074.\nNote: we used the R function var() to compute the variance. This uses the following\nformula for the variance of n points of data:\n∑\nj=1 (xj - μ)2\n.\nWe will learn the reason for dividing by 4999 = 5000 -1 instead of 5000 when we do the\n\n18.05 Problem Set 3, Spring 2022 Solutions\nstatistics portion of the class. In this case, there is very little difference between dividing\nby 5000 or 4999.\n(b) Here is the histogram produced by the abouve code\nYears of Survival\nyears\nFrequency\n(c) Looking at the distribution we see it is bimodal with a spike at 5 years. About half\nthe patients die in the first year but about half live more than 2.5 years with over 20% still\nalive after 5 years. The spike is because everyone who survives to 5 years is lumped into\nthat category. The average of 2.5 years is not that meaningful because there seem to be\ntwo categories of patients. This is reflected in the large standard deviation.\n(d) The fact that the disease is almost always fatal in 12-15 months gives us an implicit\ncontrol group. So, the treatment appears to be effective for about half the patients. More\nresearch would be needed to understand what characteristics of the disease or patients\npredict the treatment will be effective.\nProblem 3. (30: 10,10,10 pts.) Dice\nLet X be the result of rolling a fair 4-sided die. Let Y be the result of rolling a fair 6-sided\ndie. Let Z be the average of X and Y .\n(a) Find the standard deviation of X, of Y , and of Z.\nSolution: We compute Var(X) = E[X2] - E[X]2 etc. from the tables.\np(x)\n1/4\n1/4\n1/4\n1/4\nX2\nX\np(y)\n1/6\n1/6\n1/6\n1/6\n1/6\n1/6\nY 2\nY\nSo, E[X] = 1\n4(1 + 2 + 3 + 4) = 5\n2 , E[X2] = 1\n4(1 + 4 + 9 + 16) = 15\n2 . Thus,\nVar(X) = E[X2] - E[X]2 = 5/4.\nSimilarly, E[Y ] = 2\n7, E[Y 2] = 91\nSo, Var(Y) = 35\n6 .\n12 .\nSince X and Y are independent,\nVar(Z) = Var (X+ Y\n) = 4 (Var(X) + Var(Y)) =\n24.\nTaking the square root of the variances we get:\nσX =\n√\n5/2 ≈ 1.118\nσY = √35/12 ≈ 1.708\nσZ = √25/24 ≈ 1.021\n\n18.05 Problem Set 3, Spring 2022 Solutions\n(b) Carefully graph the pmf and cdf of Z.\nSolution: To compute the pmf of Z we need all the ways X and Y can be averaged.\nZ= 1\n⟷(X, Y) = {(1, 1)}\nZ= 3/2\n⟷(X, Y) = {(1, 2), (2, 1)}\nZ= 2\n⟷(X, Y) = {(1, 3), (2, 2), (3, 1)}\nZ = 5/2\n⟷ (X, Y ) = {(1, 4), (2, 3), (3, 2), (4, 1)}\nZ= 3\n⟷(X, Y) = {(1, 5), (2, 4), (3, 3), (4, 2)}\nZ = 7/2\n⟷ (X, Y ) = {(1, 6), (2, 5), (3, 4), (4, 3)}\nZ= 4\n⟷(X, Y) = {(2, 6), (3, 5), (4, 4)}\nZ= 9/2\n⟷(X, Y) = {(3, 6), (4, 5)}\nZ= 5\n⟷(X, Y) = {(4, 6)}\nSince each (X, Y ) pair has probability 1/24, the pmf and cdf of Z are\nZ\n3/2\n5/2\n7/2\n9/2\np(z)\n1/24\n2/24\n3/24\n4/24\n4/24\n4/24\n3/24\n2/24\n1/24\nFZ(z)\n1/24\n3/24\n6/24\n10/24\n14/24\n18/24\n21/24\n23/24\n24/24\nWe graph the pmf of Z as point plot and then as a density histogram. The cdf is a staircase\ngraph.\n0.04\n0.08\n0.12\n0.16\nz\nprobability\n0.2\n0.4\n0.6\n0.8\nP(Z <= z)\nz\n(c) Here is a gambling game: You win 2X dollars if X > Y and lose 1 dollar otherwise. After\nplaying this game 60 times, what is your expected total gain (positive) or loss (negative)?\nSolution: We see that the only pairs of (X, Y ) which satisfy X > Y are {(2, 1), (3, 1), (3, 2), (4, 1), (4, 2), (4, 3)} .\nSo P(X> Y) = 24\n6 . Let W be the amount won in one gaime. Again, since each (X, Y ) pair\nhas probability 1/24, W has the probability table\nW\n-1\np(w)\n1/24\n2/24\n3/24\n18/24\nSo E[W] = 4 ⋅ 24 + 6 ⋅ 24 + 8 ⋅ 24 -1 ⋅ 24 = 24 = 12\n= 11\nNow if you played the game 60 times, and received winnings W1, ... , W60, (with E[Wi]\n12 ),\nyour expected total gain is\nE[W1 + ⋯ + W60] = E[W1] + ⋯ + E[W60] = 60 ⋅ 12 = 55.\n\n18.05 Problem Set 3, Spring 2022 Solutions\nProblem 4. (20: 5,5,5,5 pts.) Two scoops\nBoxes of Raisin Bran cereal are 30cm tall. Due to settling, boxes have a higher density of\nraisins at the bottom (h = 0) than at the top (h = 30). Suppose the density (in raisins per\ncm of height) is given by f(h) = 40 -h.\n(a) How many raisins are in a box?\nSolution: The number of raisins is\n∫ f(h)dh = ∫ (40 - h)dh = 750\n(b) Let H be the height of a random raisin. Find and graph the pdf g(h) of H.\nSolution: The probability density is just the actual density divided by the total number\nof raisins. g(h) = 750 (40 - h).\n(c) Find and graph the cdf G(h) of H.\n40h\nSolution: For 0 ≤ h ≤ 30 we have G(h) = ∫\nh\ng(x) dx = 750 - h2\n1500.\nPDF\nh\ng\n0.00\n0.02\n0.04\n0.06\nCDF: G(h) vs h\nh\nG\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nThe R code for these plots is posted in mit18_05_s22_ps3_sol.r\n(d) What is the probability that a random raisin is in the bottom third of the box?\nSolution: Since the height is 30 we need to find P (H ≤ 10).\nP (H ≤ 10) = ∫ g(h)dh =\n(40 - h)dh =\nOr, P(H ≤ 10) = G(10) = 750- 100\n750 ∫\n15.\n1500 = 15.\nProblem 5. (20: 5,5,10 pts.) Gallery of continuous random variables.\nThe pnorm() function on R gives the cdf of the normal distribution, e,g, if X ∼ N(μ, σ2)\nthen pnorm(x, μ, σ) = P (X ≤ x) = FX(x).\n(a) Suppose Z is a standard normal random variable. Use R to compute\n(i) P(Z ≤ 0),\n(ii) P (Z > 1.5)\n(iii) P(|Z| < 1.5).\nSolution: (i) You don't need R for this: P(Z ≤ 0) = 0.5.\n\n18.05 Problem Set 3, Spring 2022 Solutions\n(ii) P(Z > 1.5) = 1 - pnorm(1.5) = 0.0668072.\n(iii) P(|Z| ≤ 1.5) = pnorm(1.5) - pnorm(-1.5) = 0.8663856.\n(b) Let X ∼ N(μ, σ2) where μ = 2 and σ = 3. Use R to compute\n(i) P(X ≤ μ),\n(ii) P (X - μ > 1.5σ)\n(iii) P (|X - μ| < 1.5σ).\nSolution: The trick you were supposed to notice is that these answers are identical to the\nones in part (a). This is because Z = (X - μ)/σ is standard normal. In the R code below\nmu = 2, sigma = 3\n(i) P(X ≤ μ) = pnorm(mu, mu, sigma) = 0.5.\n(ii) P(X-μ> 1.5 ∗σ) = P(X > μ+ 1.5 ∗σ) = 1 - pnorm(mu + 1.5*sigma, mu, sigma)\n= 0.0668072.\n(iii) P(|X-μ| < 1.5∗σ) = P(X < μ+1.5∗σ)-P(X < μ-1.5σ) = pnorm(mu + 1.5*sigma,\nmu, sigma) - pnorm(mu - 1.5*sigma, mu, sigma) = 0.8663856.\n(c) Let Y ∼ exp(λ). Compute the cdf of Y by integrating the pdf. What is the probability\nY ≤ 1/λ? (You need to do an integration, but you can check your work numerically using\nthe pexp() function in R.)\nSolution: The pdf is fY (y) = λe-λy. The range of Y starts at 0. We compute the cdf by\nintegration\nFY(a) = P(Y ≤a) = ∫\na\nλe-λy dy = 1 - e-λa.\nWe use this to compute the probability asked for:\nP (Y ≤ 1/λ) = FY (1/λ) = 1 - e-1 ≈ 0.6321206.\nProblem 6. (20: 5,5,5,5 pts.) Birth day\nThe length of human gestation is well-approximated by a normal distribution with mean\nμ = 280 days and standard deviation σ = 8.5 days.\n(a) Graph the corresponding pdf and cdf. You should do this using the dnorm, pnorm and\nplot commands in R. Print the results and turn them in with the pset.\nSolution: Suppose Y ∼ N(280, 8.5). The pdf, f(y) and cdf F (y) are plotted below.\nPDF: f(x) vs. x\nx\nf\n0.00\n0.02\n0.04\nCDF: F(x) vs x\nx\nF\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(b) Suppose your final exam is scheduled for May 18 and your pregnant professor has a due\ndate of May 25. Find the probability she will give birth on or before the day of the final.\n\n18.05 Problem Set 3, Spring 2022 Solutions\nSolution: There is some ambiguity here depending on the exact time of day of the due\ndate. In order to get whole numbers, we'll assume the due date is at the same hour on the\n25th as the final is on the 18th. So the final is exactly 7 days before the due date. (We'll\naccept any number between 6 and 8.)\nLet X be the number of days before or after May 25 that the baby is born. We want the\nprobability X ≤-7 We know X ∼ N(0, 8.5).\nP(X ≤ -7) = pnorm(-7, 0, 8.5) = 0.205\n(Or we could have computed P(X≤-7) = P(Z≤- 7\n= pnorm(-7/8.5, 0, 1) ≈\n8.5)\n0.205., where Z is standard normal.)\n(c) Find the probability she will give birth in May sometime after the exam. (Assume this\nmeans from the start of May 19 to the end of May 31.)\nSolution: We want the probability that the baby is born between May 19 (X = -6) and\nMay 31 (X = 6). We compute\nP(-6 ≤X≤6) = P(-8.5\n6 ≤Z≤ 8.5\n6 ) ≈0.520\nAgain there is some ambiguity about the range. We'll accept any reasonable choice.\n(d) The professor decides to move up the exam date so there will be a 95% probability that\nshe will give birth afterward. What date should she pick?\nSolution: We want to find x such that P (X ≥ x) = 0.95. That is, we want\nP (Z ≥ 8.5\nx ) = 0.95.\nThis says that x/8.5 must be the 0.05 quantile for Z.\nUsing R: x = 8.5*qnorm(0.05), we find x ≈ -14 (May 11).\nNote: we could have skipped standardizing and computed this with qnorm(0.05, 0, 8.5)\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class 04: Problem Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_class04_pset_sol.pdf",
      "content": "Class 4 in-class problems, 18.05, Spring 2022\nConcept questions\nConcept question 1.\nSuppose X a random variable with the CDF shown.\nvalues of X:\ncdf F (a):\n0.5\n0.75\n0.9\nWhat is P(X ≤ 3)?\n(a) 0.15\n(b) 0.25\n(c) 0.5\n(d) 0.75\nSolution: (d) 0.75. P(X ≤ 3) = F(3) = 0.75.\nWhat is P(X = 3)? Same distribution as above\n(a) 0.15\n(b) 0.25\n(c) 0.5\n(d) 0.75\nSolution: (b) P(X = 3) = F(3) -F(1) = 0.75 -0.5 = 0.25.\nBoard questions\nProblem 1. Computing expectation\nSuppose X is a random variable with the following pmf.\nX:\npmf:\n1/4\n1/2\n1/4\nFind E[X] and E[1/X].\nSolution:\nE[X] = 1 ⋅4 + 2 ⋅2 + 3 ⋅2 = 2\nE[1/X] = 1 ⋅ 4 + 1 ⋅ 2 + 1 ⋅ 4 =\n12.\nProblem 2. Interpreting expectation\n(a) Would you accept a gamble that offers a 10% chance to win $95 and a 90% chance of\nlosing $5?\n(b) Would you pay $5 to participate in a lottery that offers a 10% percent chance to win\n$100 and a 90% chance to win nothing?\nHint: find the expected value of your winnings in each case.\nSolution: (a) Here is the table for this random variable.\nValues:\n-5\nPMF:\n0.1\n0.9\nSo, the expected value is $95 ⋅0.1 -$5 ⋅0.9 = $5\n\n18.05 class 4 problems, Spring 2022\n(b) Has the same table, e.g. if you pay $5 and win $100, your net gain is $95. So, it has\nthe same expected value. If you play enough times, you will average winning $5 for every\ngame played.\nDiscussion\nFraming bias / cost versus loss. The two situations are identical, with an expected value\nof gaining $5. In a study, 132 undergrads were given these questions (in different orders)\nseparated by a short filler problem. 55 gave different preferences to the two events. Of\nthese, 42 rejected (a) but accepted (b). One interpretation is that we are far more willing\nto pay a cost up front than risk a loss. (See Judgment under uncertainty: heuristics and\nbiases by Tversky and Kahneman.)\nLoss aversion and cost versus loss sustain the insurance industry: people pay more in\npremiums than they get back in claims on average (otherwise the industry wouldn't be sus\ntainable), but they buy insurance anyway to protect themselves against substantial losses.\nThink of it as paying $1 each year to protect yourself against a 1 in 1000 chance of losing\n$100 that year. By buying insurance, the expected value of the change in your assets in\none year (ignoring other income and spending) goes from negative 10 cents to negative 1\ndollar. But whereas without insurance you might lose $100, with insurance you always lose\nexactly $1.\nProblem 3. Musical chairs or linearity of expectation\nSuppose that there are n people at your table and everyone got up, ran around the room,\nand sat back down randomly (i.e., all seating arrangements are equally likely).\nWhat is the expected value of the number of people sitting in their original seat?\nSolution: Number the people from 1 to n. Let Xi be the Bernoulli random variable with\nvalue 1 if person i returns to their original seat and value 0 otherwise. Since person i is\nequally likely to sit back down in any of the n seats, the probability that person i returns\nto their original seat is 1/n. Therefore Xi ∼ Bernoulli(1/n) and E[Xi] = 1/n.\nNow, let X be the number of people sitting in their original seat following the rearrangement.\nThen\nX = X1 + X2 + ⋯+ Xn.\nBy linearity of expected values, we have\nn\nn\nE[X] = ∑E[Xi] = ∑1/n = 1.\ni=1\ni=1\nExtra discussion of derangements (Not required - for fun only.)\n- It's neat that the expected value is 1 for any n.\n- If n = 2, then both people either retain their seats or exchange seats. So P(X = 0) = 1/2\nand P(X = 2) = 1/2. In this case, X never equals E[X].\n- The Xi are not independent (e.g. for n = 2, X1 = 1 implies X2 = 1).\n- Expectation behaves linearly even when the variables are dependent.\nNeat fact: A permutation of n people in which nobody returns to their original seat is\ncalled a derangement. The number of derangements turns out to be the nearest integer to\n\n18.05 class 4 problems, Spring 2022\nn!/e. Since there are n! total permutations, we have:\nP (everyone in a different seat) ≈ n!/e = 1/e ≈ 0.3679.\nn!\nIt's surprising that the probability is about 37% regardless of n, and that it converges to\n1/e as n goes to infinity.\nhttps://en.wikipedia.org/wiki/Derangement\nProblem 4. Bernoulli\n(a) Suppose X ∼ Bernoulli(p). Find E[X].\n(This is important! Remember it!)\n(b) Suppose Y = X1 + X2 + ... + X12, where each Xi ∼ Bernoulli(0.25). Find E[Y ].\nSolution: (a) Here's the probability table for a Bernoulli random variable.\nX:\npmf:\n1 -p\np\nSo,\nE[X] = (1 -p) ⋅0 + p ⋅1 = p\n(b) By the linearity of expectation\nE[X] = E[X1] + E[X2] + ... E[X12] = 12 ⋅(0.25) = 3\n(Remember, if X1, ... , X12 are independent, we say X ∼ Binomial(12, 0.25).)\nIn general, if X ∼ Binomial(n, p) then E[X] = np.\nProblem 5. Don't let one failure stop you\nLet X = # of successes before the second failure of a sequence of independent Bernoulli(p)\ntrials. Find the pmf of X.\nHint: this requires a small amount of counting.\nSolution: X takes values 0, 1, 2, .... We will show the pmf is p(n) = (n + 1)pn(1 - p)2.\nFor concreteness, we'll derive this formula for n = 3. Let's list the outcomes with three\nsuccesses before the second failure. If n = 3, then we had 3 successes and 2 failures and the\nfifth trial was a failure. So each such outcome must have the form\n__ __ __ __ F\nwith three S and one F in the first four slots. So we just have to choose which of these four\nslots contains the F . We can list all the possibilities\n{F SSSF , SF SSF , SSF SF , SSSF F }\nIn other words, there are (4\n1) = 4 such outcomes. Each of these outcomes has three S and\ntwo F , so each outcome has probability p3(1 - p)2. The probability that n = 3 is therefore\np(3) = P (X = 3) = 4p3(1 - p)2.\nThe same reasoning works for general n:\np(n) = (n + 1)pn(1 - p)2 = (n + 1)pn(1 - p)2.\n\n18.05 class 4 problems, Spring 2022\nIn class examples and discussion\nGambler's fallacy. [roulette]\nFallacy: If black comes up several times in a row then the next spin is more likely to be\nred.\nTruth: P(red) remains the same. The roulette wheel spins are independent.\nDiscussion: \"On August 18, 1913, at the casino in Monte Carlo, black came up a record\ntwenty-six times in succession [in roulette]. [There] was a near-panicky rush to bet on red,\nbeginning about the time black had come up a phenomenal fifteen times. In application\nof the maturity [of the chances] doctrine, players doubled and tripled their stakes, this\ndoctrine leading them to believe after black came up the twentieth time that there was not\na chance in a million of another repeat. In the end the unusual run enriched the Casino by\nsome millions of francs.\" (I've lost the source for this. Wikipedia has a good article on the\nGambler's Fallacy.)\nHot hand\nTheory: NBA players get 'hot'.\nData: The data show that player who has made 5 shots in a row is no more likely than\nusual to make the next shot.\nDiscussion: See The Hot Hand in Basketball: On the Misperception of Random Sequences\nby Gilovish, Vallone and Tversky. (A link that worked when these slides were written is\nhttps://doi.org/10.1016/0010-0285(85)90010-6)\nThere is still some controversy about this. Some statisticians feel that the authors of the\nabove paper erred in their analysis of the data and the data do support the notion of a hot\nhand in basketball.\nDiscussion: This reverses the Gambler's Fallacy, but is less obviously false, since we cannot\nmodel player psychology as well as we can model the Roulette wheel. The paper by Gilovish\net al uses NBA and college game data to conclude that the hot hand is hot air.\nExtra problems\nExtra 1.\nSuppose X ∼ Binomial(n, p), i.e. the number of successes in n, independent\nBernoulli(p) trials. Explain why X is the sum of n Bernoulli(p) random variables.\nSolution: Let X1, X2, ... , Xn be the n Bernoulli trials. Xi is 1 if it's a success and 0 if it's\na failure. If there are k successes, then k of the Xi are 1 and n - k are 0. If you sum, k\nones and n - k zeros you get k. That is, the sum of the n Bernoulli variables is the number\nof successes. In symbols\nn\n∑ Xi = number of successes = X.\ni=1\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Problem Set 04 Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_pset04_sol.pdf",
      "content": "18.05 Problem Set 4, Spring 2022 Solutions\nProblem 1. (25: 5,5,10,5 pts.) Time to failure\nRecall that an exponential random variable X ∼ exp(λ) has pdf given by f(x) = λe-λx on\nx ≥ 0.\n(a) Compute P (X ≥ x).\nSolution:\nP(X ≥x) = 1 -P(X < x) = 1 -∫\nx\nλe-λxdx = 1 - (1 - e-λx) = e-λx.\n(b) Compute the mean and standard deviation of X. You need to set up the necessary\nintegrals, but you can use Wolfram Alpha or another application to do the computation. (Of\ncourse, it will be good for you if you compute the integrals by hand!)\nSolution: First we compute the mean\ninf\ninf\ninf\nλxe-λx dx = -xe-λx - e-λx\nE[X] = ∫ xf(x) dx = ∫\n∣\n=\nλ\nλ.\nFor the variance, we use the formula Var(X) = E[X2] - E[X] .\ninf\ninf\ninf\nλx2e-λx dx = -x2e-λx - 2xe-λx\n- 2e-λx\nE[X2] = ∫ x2f(x) dx = ∫\n∣\n=\nλ\nλ2\nλ2\nSo, Var(X) = 2\n= λ\n2 . So, standard deviation σ =\nλ2 - λ\nλ.\n(c) Suppose that X1 and X2 are independent exp(λ) random variables. Let T = min(X1, X2).\nFind the cdf and pdf of T . (Hint: first find a formula for P(T ≥ t)?)\nNote: for independent continuous random variables X1, X2, you can assume the following\nformula:\nP (X1 ≥ x1, X2 ≥ x2) = P (X1 ≥ x1)P (X2 ≥ x2).\nSolution: For t ≥ 0, we know that T ≥t if and only if both X1 ≥t and X2 ≥ t. So\nP(T ≥ t) = P(X1 ≥ t, X2 ≥ t). Since X1 and X2 are independent, using part (a) we get,\nP (X1 ≥ t, X2 ≥ t) = P (X1 ≥ t)P (X2 ≥ t) = e-2λt.\nThus, FT (t) = P (T ≤ t) = 1 - e-2λt. Differentiating with respect to t to get the pdf, we\nfind\nT(t) = 2λe-2λt.\nfT (t) = F ′\nThat is, T is an exponential random variable with mean 2λ.\n(d) Suppose we are testing 3 different brands of light bulbs B1, B2, and B3 whose lifetimes\nare exponential random variables with mean 1/2, 1/3, and 1/5 years, respectively. Assuming\n\n18.05 Problem Set 4, Spring 2022 Solutions\nthat all of the bulbs are independent, what is the expected time before one of the bulb fails.\n(Hint: part (c) was a warmup for this problem.)\nSolution: Let X1, X2, and X3 be the lifetimes of bulbs B1, B2 and B3, respectively. Then\nwe know X1 ∼ exp(2), X2 ∼ exp(3), X3 ∼ exp(5). Let T = min(X1, X2, X3). Then T is the\ntime to the first failure of a bulb. Following the same argument as in (c), we have\nP (T ≥ t) = P (X1 ≥ t)P (X2 ≥ t)P (X3 ≥ t) = e-10t.\nThus, the cdf of T is FT (t) = 1 - e-10t and the pdf, fT (t) is given by\nT (t) = 10e-10t.\nfT (t) = F ′\nWe found that T ∼ exp(10). Therefore, E[T] = 10\n1 .\nProblem 2. (20: 10,10 pts.) Elections\nTo head the newly formed US Dept. of Statistics, suppose that 50% of the population\nsupports Alessandre, 20% supports Sarah, and the rest are split between Gabriel, Sarah and\nSo Hee. A poll asks 400 random people who they support.\n(a) Use the central limit theorem to estimate the probability that at least 52.5% of those\npolled prefer Alessandre?\nSolution: Let Xi be the result of polling person i:\nif person i supports Alessandre\nXi = {1\n0 if person i does not support Alessandre\nThen Xi ∼ Bern(0.5) and the number of people who prefer Alessandre is\nS = X1 + ⋯+ X400.\nWe know E[Xi] = 1/2 and Var(Xi) = 1/4. This implies E[S] = 200 and Var(S) = 100.\nThus, the central limit theorem tells us that\nS ≈ N(200, 100).\nThe problem asks for P (S > 210):\n> 210 - 200\nP(S> 210) = P(S-200\n) ≈P(Z> 1) ≈ 0.16 .\n(b) Use the central limit theorem to estimate the probability that less than 31% of those\npolled prefer Gabriel, Sarah or So Hee?\nSolution: Now let Yi = 1 if person i prefers one of Gabriel, Sarah or So Hee and 0\notherwise. We have Y1, ... , Y400 are independent Bern(0.3). So E[Yi] = μ = 0.3 and\nVar(Yi) = (0.3)(0.7) = 0.21. If Y = 400(Y1 + ⋯ + Y400), the Central Limit Theorem tells us\nY - μ\n(Y - 0.3)\n√\nσ/\n√\n=\n√\n0.21\n\n18.05 Problem Set 4, Spring 2022 Solutions\nis approximately standard normal. Thus, using Z for a standard normal random variable,\nP(Y ≤0.31) ≈P(Z< (0.31 -0.3)\n√\n400) = P(Z< 0.4364358) ≈0.67.\n√\n0.21\nProblem 3. (10 pts.) A penny for your thoughts\nTo save a mint, in 2012 Canada decided to do away with its pennies. The Chubby Chef\nin Equality, Illinois wants to be ready should the United States decide to pass a similar\nlaw. The Chubby Chef processes n = 1000 orders of assorted baked goods each day, and will\nround the price of each order to the nearest nickel (e.g., $3.57 rounds to $3.55 while $3.58\nrounds to $3.60). Let p be the probability that the total rounding error over the course of a\nday is either greater than 100 or less than -100 cents, i.e. exceeds 100 in absolute value.\nEstimate p using the central limit theorem.\nSolution: Let S be the total rounding error for a day. The problems asks for\nP(|S| > 100).\nLet Xi be the rounding error (in cents) of the ith order. Then Xi takes values -2, -1, 0, 1, 2,\neach with probability 1\n5. We compute\nE[Xi] = μ = 0,\nVar(Xi) = σ2 = 2.\nThe total rounding error S = X1 + ⋯+ X1000. By the Central Limit Theorem, we know\nthat S ≈ N(0, 2000).\n≥ 100) = P (|S - 0|\n√\n2000 ≥ 100 -0\nP(|S|\n√\n2000 ) ≈ P (|Z| ≥\n0.0253 .\n√100\n2000) =\nExtra credit 5 points Simulate this in R with 10000 trials. (Each trial involves 1000\norders.) Print out or hand copy your code and include it. Give the result of running your\ncode 3 times,\nSolution: Here's my code.\nntrials = 10000\nn_orders = 1000\nthreshold = 100\ncnt_above_threshold = 0\nfor (j in 1:ntrials) {\n# The rounding error is 0, -1, -2, 2, 1 depending on if the price modulo 5\nis 0, 1, 2, 3, 4\n# Generate 1000 random orders, just keep the rounding error\nx = sample(c(0,-1,-2,2,1), n_orders, replace=TRUE)\ntotal_error = sum(x)\nif (abs(total_error) > threshold) {\ncnt_above_threshold = cnt_above_threshold + 1\n}\n}\n\n18.05 Problem Set 4, Spring 2022 Solutions\nprob_above_thresh = cnt_above_threshold/ntrials\nprint(prob_above_thresh)\nIn three runs it gave 0.0245, 0.0263, 0.0216. This agrees nicely with the CLT estimate.\nProblem 4. (30: 10,10,10 pts.) Change of scale.\nIn this problem we will look at scaling random variables. This is a simple, but common\nthing to do. As usual with transformations, if you don't approach it systematically, it is\neasy to make mistakes.\n(a) Suppose the random variable X has an exponential distribution with parameter 1, i.e.\nX ∼ exp(1). Give the range and pdf for the variables X and Y = 3X\nSketch the graph of the density functions for each of these variables.\nSolution: X: The range of X is [0, inf). The pdf is fX(x) = e-x, for x inside the range and\n0 elsewhere.\nY = 3X: The range of Y is [0, inf). To find fY , we work with the cumulative distributions.\nFY (y) = P (Y ≤ y)\ndefinition of CDF\n= P (3X ≤ y)\nbecause Y = 3X\n= P (X ≤ y/3)\nalgebra\n= FX(y/3)\ndefinition of CDF\nNow for the pdf:\nd\n1 d\nfY(y) = dy FY(y) =\ndy FX(y/3) = 3FX\n′ (y/3) = 3fX(y/3).\nThe second to last equality used the chain rule. The last equality is the fact that the pdf is\nthe derivative of the cdf, i.e. fX = FX\n′ .\nSince we know fX(y/3) = e-y/3, we have\nfY (y) = 3\n1 e-y/3 for y between 0 and inf.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nfY(t)\nfX(t)\nt\n\n18.05 Problem Set 4, Spring 2022 Solutions\n(b) For the random variable X from part (a), find the range and pdf of W = aX+ b, where\na and b are constants. Assume a > 0.\nSolution: We find the range and pdf by following the same pattern as in part (a). The\nrange of W is [b, inf).\n) = FX (w - b\nFW(w) = P(W≤w) = P(aX+ b≤w) = P(X≤ w-b\n) .\na\na\nTaking the derivative:\nd\ndw\nd FX (w - b\nX (w - b\na\n1fX (w - b\nfW(w) = dw FW(w) =\n) = aF′\n) =\n) .\na\na\na\nSince we know fX( w-b) = e-(w-b)/a, we have\na\na\n1 e-(w-b)/a for w between b and inf.\nfW (w) =\n(c) Let V = X3. Find the range and pdf of V .\nSolution: We follow the same pattern as in the previous parts. The range of V is [0, inf).\nFV(v) = P(V ≤v) = P(X3 ≤v) = P(X≤v1/3) = FX(v1/3).\nTaking the derivative:\nd\nd\n3v-2/3fX(v1/3) = 3v-2/3e-v1/3\nfV (v) =\n3v-2/3FX\n′ (v1/3) =\n.\ndv FV (v) = dv FX(v1/3) =\nProblem 5. (30: 10,10,10 pts.) In this problem we will explore how the transformations in\nthe previous problem affect the mean and median.\n(a) For the variables X, Y , W in the previous problem, assume each of the variables are\ngiven in units of minutes. Find the expected value, variance and standard deviation of each\nvariable. Be sure to include units in your answer.\nWhat are the units on a and b in the defnition of W ?\nSolution: Since X ∼ Exponential(1) we know\nE[X] = 1 min., Var(X) = 1 min.2, σX= 1 min.\nSince expected value is linear,\nE[Y ] = E[3X] = 3E[X] = 3 min.,\nE[W] = aE[X] + b = (a+ b) min.\nLikewise, the variance is invariant under translation and scales by the square of the multi\nplier:\nVar(Y) = 9 ⋅ Var(X) = 9 min.2,\nσY = 3 min.,\nVar(W) = a2 ⋅ Var(X) = a2 min.2,\nσW = a min..\n\n18.05 Problem Set 4, Spring 2022 Solutions\nBecause both X and W are in units of minutes, a must be dimensionless and b has units of\nminutes.\n(b) For V from the previous problem, compute E[V ]. As usual, you must set up the integral,\nbut you can use a package like Wolfram Alpha to compute the integral.\ninf\ninf\nSolution: E[V] = ∫ vfV(v) dv = 3\n1 ∫ v1/3e-v1/3 dv. This integral can be computed\nusing the change of variable u = v1/3, i.e. u3 = v. The final answer is E[V] = 6. (Wolfram\nAlpha agrees!)\n(c) Compute the median value of both X and V .\nSolution: For X, the median value is the value q0.5 such that FX(q0.5) = 0.5. Now,\nq\nq\nFX(q) = ∫ fX(x) dx = ∫ e-xdx = -e-x|0\nq = 1 - e-q.\nSolving 1 - e-q = 0.5 gives the median of X is q0.5 = ln(2) .\nSince as we saw in the previous problem, FV (v) = FX(v1/3), we have\nFV (v) = 0.5 ⇔ FX(v1/3) = 0.5 ⇔ v1/3 = q0.5.\nThat is, the median of V = X3 is just (the median of X)3, i.e. ln(2)3.\nProblem 6. (30: 5,5,10,10 pts.) Fat tails\nThis problem will explore the tails of two distributions. The tails are important when we\nwant to think about probabilities of extreme events.\n(a) As an example, in the general population IQ has mean 100 and standard deviation of\n15. IQ is normally distributed. Use the R function pnorm to give the probability that a\nrandomly chosen person has IQ greater than 160, i.e. more than 4 standard deviations\nabove the mean.\nSolution: We use the R code p = 1 - pnorm(160, 100, 15). This gives the probability\np = 3.167124 × 10-5.\n(b) Now, in order to be able to use R or Wolfram Alpha without a lot of distracting algebraic\nmanipulation, we'll modify the definition of IQ. Suppose that Modified_IQ has mean 0 and\n√\n3.\nstandard deviation\nAssuming Modified_IQ is normally distributed, find the probability that a randomly chosen\nperson has Modified_IQ more than 4 standard deviations above the mean.\nSolution: One of the important facts about normal distributions, is that, when measured\nin standard deviations above the mean they all give the same probabilities. That is, the\nanswer is exactly the same as in part (a)\n(c) Now assume that Modified_IQ follows a t-distribution with 3 degrees of freedom. Later\nin the class we will work extensively with t-distributions. Here, it will be enough for us to\nknow the following about this distribution.\n- Range: (-inf, inf)\n\n18.05 Problem Set 4, Spring 2022 Solutions\n-2\n- PDF: f(x) = 3π(1 + x\n)\n- Mean: μ = 0\n- Standard deviation: σ =\n√\n(So this has the same mean and standard deviation as in part (b).)\nFor this problem, you can work with this pdf directly or you can look up how to use the R\nfunctions dt and pt.\nAssuming it follows this t-distribution, find the probability that a randomly chosen person\nhas Modified_IQ more than 4 standard deviations above the mean.\nYou can use R or another calculation package to do the calculation, but you must explicitly\nshow the integral in terms of the probability density.\nCompare this value with the probability in part (b)\nSolution: If I is the Modified_IQ of a randomly chosen person, we want to compute\nP(I > 4 ∗\n√\n3). In terms of the pdf this is\ninf\ninf\n-2\n√\n√\n3 )\nP(I> 4\n√\n3) = ∫ f(x) dx = ∫\n3π(1 + x2\ndx\nWe computed this in R with the code p = 1 - pt(4*sqrt(3), 3). This gives the proba\nbility p = 0.003082687.\nThis is a small probability, but it is about 100 times the probability in parts (a) and (b).\n(d) For this problem, compute probabilities using both the normal distribution in part (b)\nand the t-distribution in part (c). Do this for the following probabilities.\n(i) P (Modified_IQ > 20), (ii) P (Modified_IQ > 40), (iii) P (Modified_IQ > 200).\nWhy do we say that the t-distribution has a 'fat tail'?\nHence the moral of this problem: Knowing the mean and standard deviation of a quantity\nis often not enough for predicting the frequency of extreme events (high IQ, 100-year floods,\netc.); you need to know the underlying distribution itself (which often requires finding out\nthe underlying geophysics, geochemistry, and biology). In the solutions we will show you\ngraphs of these distributions zoomed in around 4σ above the mean. If you do that yourself,\nyou will see that they look very different.\nSolution: For x = 20, 40, 200, we use the R code p_t = 1 - pt(x, 3), p_normal = 1\npnorm(x, 0, sqrt(3)). We get\n(i) x = 20: p_t = 0.00014,\np_normal = 0\n(ii) x = 40: p_t = 1.7e-5,\np_normal = 0\n(iii) x = 200: p_t = 1.4e-7,\np_normal = 0\nAll three examples show that for x far from the mean, i.e. in the tail, the t-distribution\nprobability is many orders of magnitude greater than the normal distribution probability\nWe say the t-distribution has a fat tail, because the its tail contains much more probability\nthan the normal distribution. That is, extreme events are much more likely for the t-\ndistribution.\n\n18.05 Problem Set 4, Spring 2022 Solutions\nGraphically, the following figures show the tail of the t-distribution is much greater, i.e.\nfatter, than that of the normal distribution.\n-4\n-2\nt-distribution\nNormal\nx\nt-distribution\nnormal\nx\nLeft: Center of distributions, Right: tails from 3σ to 6σ above mean\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class 05a: Problem Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_class05a_pset_sol.pdf",
      "content": "Class 5 in-class problems, 18.05, Spring 2022\nConcept questions\nConcept question 1. Order the variance\nThe graphs below give the pmf for 3 random variables.\nx\n(A)\nx\n(B)\nx\n(C)\nOrder them by size of standard deviation from biggest to smallest. (Assume x has the same\nunits in all three.)\n1. ABC\n2. ACB\n3. BAC\n4. BCA\n5. CAB\n6. CBA\nSolution: 5. CAB\nAll 3 variables have the same range from 1-5 and all of them are symmetric so their mean\nis right in the middle at 3. (C) has most of its weight at the extremes, so it has the biggest\nspread. (B) has the most weight in the middle so it has the smallest spread.\nFrom biggest to smallest standard deviation we have (C), (A), (B).\nConcept question 2. Zero variance\nSuppose X is a discrete random variable,\nTrue or False: If Var(X) = 0 then X is constant.\nSolution: True. If X can take more than one value with positive probability, than Var(X)\nwill be a sum of positive terms. So, X is constant if and only if Var(X) = 0.\nConcept question 3. Standard deviation\nMake an intuitive guess: Which pmf has the bigger standard deviation? (Assume w and y\nhave the same units.)\ny\np(y)\n-3\n1/2\npmf for Y\nw\np(W)\n0.1\n0.2\n0.4\npmf for W\n1. Y\n2. W\n\n18.05 class 5 problems, Spring 2022\nSolution: The scales along the horizontal axis are so different, that, even though W is\nmore packed towards the center, the bigger scale means its standard deviation is probably\nlarger.\nYou can compute that Var(Y) = 9 and Var(W) = 120.\nConcept question 4.\nSuppose X is a continuous random variable.\n(a) If the pdf of X is f(x) can there be an x where f(x) = 10?\n(b) What is P(X = a)?\n(c) Does P(X = a) = 0 mean X never equals a?\nSolution: (a) Yes. This is a density, it can be greater than 1. Probabilities must be less\nthan 1.\n(b) 0\n(c) No. For a continuous distribution any single value has probability 0. Only a range of\nvalues has non-zero probability.\nConcept question 5.\nWhich of the following are graphs of valid cumulative distribution functions?\nx\n-4 -2\n-0.5\nA.\nx\n-4 -2\n-0.5\nB.\nx\n-4 -2\n-0.5\nC.\nx\n-4 -2\n-0.5\nD.\nSolution: Test 2\nand Test 3.\nGraph A is not a cdf: it takes negative values, but probabilities are positive.\nGraph B is a cdf: it increases from 0 to 1.\nGraph C is a cdf: it increases from 0 to 1.\nGraph D is not a cdf because has a decreasing part. A cdf must be non-decreasing since it\nrepresents accumulated probability.\nBoard questions\nProblem 1.\n(a) Let X ∼ Bernoulli(p). Compute Var(X).\n(b) Let Y ∼ Bin(n, p). Show Var(Y) = np(1 -p).\n(c) Suppose X1, X2, ... , Xn are independent and all have the same standard deviation σ = 2.\nLet X be the average of X1, ... , Xn.\nWhat is the standard deviation of X?\n(a) Solution: For X ∼ Bernoulli(p) we use a table. (We know E[X] = p.)\n\n18.05 class 5 problems, Spring 2022\nX\np(x)\n1 - p\np\n(X - μ)2\np2\n(1 - p)2\nVar(X) = E[(X - μ)2] = (1 - p)p2 + p(1 - p)2 = p(1 - p)\n(b) Y ∼ bin(n, p) means Y is the sum of n independent Bernoulli(p) random variables Y1,\nY2, ..., Yn. For independent variables, the variances add. Since Var(Yj) = p(1 - p) we have\nVar(Y) = Var(Y1) + Var(Y2) + ... + Var(Yn) = np(p - 1).\n(c) Since the variables are independent, we have\nVar(X1 + ... + Xn) = 4n.\nX is the sum scaled by 1/n and the rule for scaling is Var(aX) = a2Var(X), so\nVar(X) = Var(X1 + ⋯+ Xn\n) = n2 Var(X1 + ... + Xn) =\nn\nn.\nThis implies σX = √2\nn .\nNote: this says that the average of n independent measurements varies less than the indi\nvidual measurements.\nProblem 2.\nSuppose X has range [0, 2] and pdf f(x) = c x2.\n(a) What is the value of c?\n(b) Compute the cdf F (x).\n(c) Compute P(1 ≤X ≤2).\n(d) Plot the pdf and use it to illustrate part (c).\n(a) Solution: Total probability must be 1. So\n∫ f(x) dx = ∫ cx2 dx = c3\n8 = 1 ⇒ c = 8 .\n(b) The pdf f(x) is 0 outside of [0, 2] so for 0 ≤ x ≤ 2 we have\nx\nx3\nF (x) = ∫ cu2 du = 3\nc x3 =\n.\nF (x) is 0 fo x < 0 and 1 for x > 2.\n(c) We could compute the probability as ∫\nf(x) dx, but rather than redo the integral let's\nuse the cdf:\nP(1 ≤X≤2) = F(2) -F(1) = 1 - 1\n8 = 8 .\n\n18.05 class 5 problems, Spring 2022\nProblem 3.\nSuppose Y has range [0, b] and cdf F (y) = y2/9.\n(a) What is b?\n(b) Find the pdf of Y .\nSolution: (a) Since the total probability is 1, we have\nF(b) = 1 ⇒ b\n= 1 ⇒ b = 3 .\n2y\n(b) f(y) = F ′(y) = 9 .\nProblem 4.\nI've noticed that taxis drive past 77 Mass. Ave. on the average of once every 10 minutes.\nSuppose time spent waiting for a taxi is modeled by an exponential random variable\nX ∼ Exponential(1/10);\nf(x) = 10\n1 e-x/10\n(a) Sketch the pdf of this distribution\n(b) Shade the region which represents the probability of waiting between 3 and 7 minutes\n(c) Compute the probability of waiting between between 3 and 7 minutes for a taxi\n(d) Compute and sketch the cdf.\nSolution: Sketches for (a), (b), (d)\nx\nP(3 < X< 7)\n2 4 6 8 10 12 14 16\n0.1\nf(x) = λe-λx\nx\nF(x) = 1 -e-λx\n8 10 12 14 16\n(c)\n(3 < X< 7) = ∫ 10\n1 e-x/10 dx = -e-x/10∣3\n7 = e-3/10 - e-7/10 ≈ 0.244\nIn class examples and discussion\nExample. Computation from tables\nCompute the variance and standard deviation of X.\nvalues x\npmf p(x)\n1/10\n2/10\n4/10\n2/10\n1/10\nSolution: From the table we compute the mean:\nμ= E[X] =\n10 + 12\n10 + 4\n10 + 10\n8 + 10\n5 = 3.\n\n18.05 class 5 problems, Spring 2022\nThen we add a line to the table for (X - μ)2.\nvalues X\npmf p(x)\n1/10\n2/10\n4/10\n2/10\n1/10\n(X - μ)2\nUsing the new table we compute variance E[(X - μ)2]:\n1 ⋅4 + 2 ⋅1 + 4 ⋅0 + 2 ⋅1 + 1 ⋅4 = 1.2\nThe standard deviation is then σ =\n√\n1.2.\nExample. A very useful formula\nRecompute the previous example using the very useful formula for variance\nn\nVar(X) = E[X2] -E[X]2 = (∑p(xi)x2\ni) -μ2.\ni=1\nSolution: Here is the table\nvalues X\npmf p(x)\n1/10\n2/10\n4/10\n2/10\n1/10\nX2\nWe know E[X] = 3. We compute\nE[X2] =\n⋅1 + 2 ⋅4 + 4 ⋅9 + 2 ⋅16 + 1 ⋅25 =\n2 =\nSo Var(X) = E[X2] - E[X]\n10 - 9 = 10 = 1.2\nExtra problems\nExtra 1.\nLet X take value 1, with equal probability on {1, 2, 3, 4, 5} (X is a uniform\nrandom variable). Compute Var(X).\nLet Y be uniform on {7, 8, 9, 10, 11}. What is the variance of Y ?\n1 + 2 + 3 + 4 + 5\n1 + 4 + 9 + 16 + 25\nSolution: E[X] =\n= 3. E[X2] =\n= 11. So,\nVar(X) = E[X]2 - E[X]2 = 11 - 9 = 2 .\nSince Y = X+ 6, Var(Y) = Var(X) = 2.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Class 05b: Problem Solutions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_class05b_pset_sol.pdf",
      "content": "Class 5b in-class problems, 18.05, Spring 2022\nBoard questions\nProblem 1.\nI've noticed that taxis drive past 77 Mass. Ave. on the average of once every 10 minutes.\nSuppose time spent waiting for a taxi is modeled by an exponential random variable\nX ∼ Exponential(1/10);\nf(x) = 10\n1 e-x/10\n(a) Sketch the pdf of this distribution\n(b) Shade the region which represents the probability of waiting between 3 and 7 minutes\n(c) Compute the probability of waiting between between 3 and 7 minutes for a taxi\n(d) Compute and sketch the cdf.\nSolution: Sketches for (a), (b), (d)\nx\nP(3 < X< 7)\n2 4 6 8 10 12 14 16\n0.1\nf(x) = λe-λx\nx\nF(x) = 1 -e-λx\n8 10 12 14 16\n(c)\n(3 < X< 7) = ∫ 10\n1 e-x/10 dx = -e-x/10∣3\n7 = e-3/10 - e-7/10 ≈ 0.244\nProblem 2. Gallery of distributions\nOpen the Gallery of probability distributions applet at\nhttps://mathlets.org/mathlets/probability-distributions/\n(a) For the standard normal distribution N(0, 1) how much probability is within 1 of the\nmean?\nWithin 2? Within 3?\n(b) For N(0, 32) how much probability is within σ of the mean? Within 2σ? Within 3σ.\n(c) Does changing μ change your answer to problem 2?\n(d) Use the applet to find the median of the exp(0.5) distribution.\n(The median is the value of x where half the probability is below x and half above.)\nSolution: (a) Using the applet:\nP(-1 ≤ Z ≤ 1) = 0.683, P(-2 ≤ Z ≤ 2) = 0.954, P(-3 ≤ Z ≤ 3) = 0.997.\n(b) We set σ = 3 in the app. Since the mean is 0, the range within σ of the mean is [-3, 3].\nLikewise within 2σ of the mean has range [-6, 6], and 3σ has range [-9, 9].\n\n18.05 class 5b problems, Spring 2022\nLet X ∼ N(0, 32). According to the applet\nP (-σ ≤ X ≤ σ) = 0.683, P (-2σ ≤ X ≤ 2σ) = 0.954, P (-3σ ≤ X ≤ 3σ) = 0.997.\nThese are the same probabilities as in part (a).\n(c) No, changing μ does not change the probability of being in a given range around the\nmean. The range with σ of the mean is [μ - σ, μ + σ] and\nP(μ-σ≤X ≤μ+ σ) = P(-σ ≤X-μ≤σ) = 0.683.\n(d) The median is the value q, where P(X ≤ q) = 0.50. Using the applet for exp(0.5), we\nset the left edge of the probability interval at 0 and adjust the right edge until we get 0.50\nprobability. The applet shows that q is somewhere between 1.35 and 1.40.\nProblem 3. Manipulating random variables\n(a) Suppose X ∼ uniform(0,2). If Y = 4X, find the range, pdf and cdf of Y .\n(b) Suppose X ∼ uniform(0,2). If Y = X3, find the range, pdf and cdf of Y .\n(c) Suppose Z ∼ Norm(0, 1) (standard normal). Find the range, pdf and cdf of Y = 3Z+2.\n(a) Solution: Range of X is [0,2]. Uniform means, for x in this range\nFX(x) = P(X ≤ x) = x/2.\nRange of Y is [0,8]. For y in this range\ny\nFY(y) = P(Y ≤y) = P(4X≤y) = P(X≤y/4) = 8.\nfY (y) = F ′(y) = 8\n(b) Solution: Range of X is [0,2]. Uniform means, for x in this range\nFX(x) = P(X ≤ x) = x/2.\nRange of Y is [0,8]. For y in this range\ny1/3\nFY(y) = P(Y ≤y) = P(X3 ≤y) = P(X≤y1/3) =\n.\n6y-2/3\nfY (y) = F ′(y) =\n(c) Solution: The standard normal has range (-inf, inf), and pdf and cdf\nφ(z) = √1\n2π\ne-z2/2,\nΦ(z).\nThere is no closed form formula for Φ(z) so we leave it as is. We compute its values using\na table (really using a computer).\n\n18.05 class 5b problems, Spring 2022\nY has range (-inf, inf). We manipulate the cdf of Y using its definition as a probability.\n) = Φ (y -2\nFY(y) = P(Y≤y) = P(3Z+ 2 < y) = P(Z< y-2\n) .\nThat's the best we can do for the cdf. For the pdf we take a derivative. (We'll need to use\nthe chain rule.)\n3 φ(y -2\nfY (y) = FY\n′ (y) =\n) .\nWe do have a formula for φ(z). So\nfY (y) =\n√1\n2π\ne-(y-2)2/18.\nNote: this is the pdf for N(5, 32). So\nY ∼ N(5, 32).\nThat is, scaling and shifting a standard normal random variable produces another normal\nrandom variable.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "All Probability Reading",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_probability.pdf",
      "content": "Introduction\nClass 1, 18.05\nJeremy Orloff and Jonathan Bloom\nProbability vs. Statistics\nIn this introduction we will preview what we will be studying in 18.05. Don't worry if many\nof the terms are unfamiliar, they will be explained as the course proceeds.\nProbability and statistics are deeply connected because all statistical statements are at bot-\ntom statements about probability. Despite this the two sometimes feel like very different\nsubjects. Probability is logically self-contained; there are a few rules and answers all follow\nlogically from the rules, though computations can be tricky. In statistics we apply proba-\nbility to draw conclusions from data. This can be messy and usually involves as much art\nas science.\nProbability example\nYou have a fair coin (equal probability of heads or tails). You will toss it 100 times. What\nis the probability of 60 or more heads? There is only one answer (about 0.028444) and we\nwill learn how to compute it.\nStatistics example\nYou have a coin of unknown provenance. To investigate whether it is fair you toss it 100\ntimes and count the number of heads. Let's say you count 60 heads. Your job as a statis-\ntician is to draw a conclusion (inference) from this data. There are many ways to proceed,\nboth in terms of the form the conclusion takes and the probability computations used to\njustify the conclusion. In fact, different statisticians might draw different conclusions.\nNote that in the first example the random process is fully known (probability of heads =\n0.5). The objective is to find the probability of a certain outcome (at least 60 heads) arising\nfrom the random process. In the second example, the outcome is known (60 heads) and the\nobjective is to illuminate the unknown random process (the probability of heads).\nFrequentist vs. Bayesian Interpretations\nThere are two prominent and sometimes conflicting schools of statistics: Bayesian and\nfrequentist.\nTheir approaches are rooted in differing interpretations of the meaning of\nprobability.\nFrequentists say that probability measures the frequency of various outcomes of an ex-\nperiment. For example, saying a fair coin has a 50% probability of heads means that if we\ntoss it many times then we expect about half the tosses to land heads.\nBayesians say that probability is an abstract concept that measures a state of knowledge\nor a degree of belief in a given proposition. In practice Bayesians do not assign a single\nvalue for the probability of a coin coming up heads. Rather they consider a range of values\neach with its own probability of being true.\nIn 18.05 we will study and compare these approaches. The frequentist approach has long\n\n18.05 Class 1, Introduction, Spring 2022\nbeen dominant in fields like biology, medicine, public health and social sciences.\nThe\nBayesian approach has enjoyed a resurgence in the era of powerful computers and big\ndata. It is especially useful when incorporating new data into an existing statistical model,\nfor example, when training a speech or face recognition system. Today, statisticians are\ncreating powerful tools by using both approaches in complementary ways.\nApplications, Toy Models, and Simulation\nProbability and statistics are used widely in the physical sciences, engineering, medicine, the\nsocial sciences, the life sciences, economics and computer science. The list of applications is\nessentially endless: tests of one medical treatment against another (or a placebo), measures\nof genetic linkage, the search for elementary particles, machine learning for vision or speech,\ngambling probabilities and strategies, climate modeling, economic forecasting, epidemiology,\nmarketing, googling...We will draw on examples from many of these fields during this course.\nGiven so many exciting applications, you may wonder why we will spend so much time\nthinking about toy models like coins and dice. By understanding these thoroughly we will\ndevelop a good feel for the simple essence inside many complex real-world problems. In\nfact, the modest coin is a realistic model for any situations with two possible outcomes:\nsuccess or failure of a treatment, an airplane engine, a bet, or even a class.\nSometimes a problem is so complicated that the best way to understand it is through\ncomputer simulation. Here we use software to run virtual experiments many times in order\nto estimate probabilities. In this class we will use R for simulation as well as computation\nand visualization. Don't worry if you're new to R; we will teach you all you need to know.\n\nCounting and Sets\nClass 1, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Know the definitions and notation for sets, intersection, union, complement.\n2. Be able to visualize set operations using Venn diagrams.\n3. Understand how counting is used computing probabilities.\n4. Be able to use the rule of product, inclusion-exclusion principle, permutations and com-\nbinations to count the elements in a set.\nCounting\n2.1\nMotivating questions\nExample 1. A coin is fair if it comes up heads or tails with equal probability. You flip a\nfair coin three times. What is the probability that exactly one of the flips results in a head?\nSolution: With three flips, we can easily list the eight possible outcomes:\n{TTT, TTH, THT, THH, HTT, HTH, HHT, HHH}\nThree of these outcomes have exactly one head:\n{TTH, THT, HTT}\nSince all outcomes are equally probable, we have\nP(1 head in 3 flips) = number of outcomes with 1 head\ntotal number of outcomes\n= 3\n8.\nThink: Would listing the outcomes be practical with 10 flips?\nA deck of 52 cards has 13 ranks (2, 3, ... , 9, 10, J, Q, K, A) and 4 suits (♡, ♠, ♢, ♣,). A\npoker hand consists of 5 cards. A one-pair hand consists of two cards having one rank and\nthree cards having three other ranks, e.g., {2♡, 2♠, 5♡, 8♣, K♢}\nTest your intuition:\nthe probability of a one-pair hand is:\n(a) less than 5%\n(b) between 5% and 10%\n(c) between 10% and 20%\n(d) between 20% and 40%\n(e) greater than 40%\n\n18.05 Class 1, Counting and Sets, Spring 2022\nAt this point we can only guess the probability. One of our goals is to learn how to compute\nit exactly. To start, we note that since every set of five cards is equally probable, we can\ncompute the probability of a one-pair hand as\nP(one-pair) = number of one-pair hands\ntotal number of hands\nSo, to find the exact probability, we need to count the number of elements in each of these\nsets. And we have to be clever about it, because there are too many elements to simply\nlist them all.\nWe will come back to this problem after we have learned some counting\ntechniques.\nSeveral times already we have noted that all the possible outcomes were equally probable\nand used this to find a probability by counting. Let's state this carefully in the following\nprinciple.\nPrinciple: Suppose there are npossible outcomes for an experiment and each is equally\nprobable. If there are kdesirable outcomes then the probability of a desirable outcome is\nk/n. Of course we could replace the word desirable by any other descriptor: undesirable,\nfunny, interesting, remunerative, ...\nConcept question:\nCan you think of a scenario where the possible outcomes are not\nequally probable?\nHere's one scenario: on an exam you can get any score from 0 to 100. That's 101 different\npossible outcomes. Is the probability you get less than 50 equal to 50/101?\n2.2\nSets and notation\nOur goal is to learn techniques for counting the number of elements of a set, so we start\nwith a brief review of sets. (If this is new to you, please come to office hours).\n2.2.1\nDefinitions\nA set Sis a collection of elements. We use the following notation.\nElement: We write x∈Sto mean the element xis in the set S.\nSubset: We say the set Ais a subset of Sif all of its elements are in S. We write this as\nA⊂S.\nComplement:: The complement of Ain Sis the set of elements of Sthat are not in A.\nWe write this as Acor S-A.\nUnion: The union of Aand Bis the set of all elements in Aor B(or both). We write this\nas A∪B.\nIntersection: The intersection of Aand Bis the set of all elements in both Aand B. We\nwrite this as A∩B.\nEmpty set: The empty set is the set with no elements. We denote it ∅.\nDisjoint: Aand Bare disjoint if they have no common elements. That is, if A∩B= ∅.\nDifference: The difference of Aand Bis the set of elements in Athat are not in B. We\nwrite this as A-B.\n\n18.05 Class 1, Counting and Sets, Spring 2022\nLet's illustrate these operations with a simple example.\nExample 2. Start with a set of 10 animals\nS= {Antelope, Bee, Cat, Dog, Elephant, Frog, Gnat, Hyena, Iguana, Jaguar}.\nConsider two subsets:\nM= the animal is a mammal = {Antelope, Cat, Dog, Elephant, Hyena, Jaguar}\nW= the animal lives in the wild = {Antelope, Bee, Elephant, Frog, Gnat, Hyena, Iguana, Jaguar}.\nOur goal here is to look at different set operations.\nIntersection:\nM∩Wcontains all wild mammals: M∩W= {Antelope, Elephant, Hyena, Jaguar}.\nUnion:\nM∪Wcontains all animals that are mammals or wild (or both).\nM∪W= {Antelope, Bee, Cat, Dog, Elephant, Frog, Gnat, Hyena, Iguana, Jaguar}.\nComplement:\nMcmeans everything that is not in M, i.e.\nnot a mammal.\nMc=\n{Bee, Frog, Gnat, Iguana}.\nDifference:\nM-Wmeans everything that's in Mand not in W.\nSo, M-W=\n{Cat, Dog}.\nThere are often many ways to get the same set, e.g.\nMc= S-M,\nM-W= M∩Wc.\nThe relationship between union, intersection, and complement is given by DeMorgan's laws:\n(A∪B)c= Ac∩Bc\n(A∩B)c= Ac∪Bc\nIn words the first law says everything not in (Aor B) is the same set as everything that's\n(not in A) and (not in B). The second law is similar.\n2.2.2\nVenn Diagrams\nVenn diagrams offer an easy way to visualize set operations.\nIn all the figures Sis the region inside the large rectangle, Lis the region inside the left\ncircle and Ris the region inside the right circle. The shaded region shows the set indicated\nunderneath each figure.\nS\nL\nR\nL∪R\nL∩R\nLc\nL-R\n\n18.05 Class 1, Counting and Sets, Spring 2022\nProof of DeMorgan's Laws\n(L∪R)c\nLc\nRc\nLc∩Rc\n(L∩R)c\nLc\nRc\nLc∪Rc\nExample 3. Verify DeMorgan's laws for the subsets A= {1, 2, 3} and B= {3, 4} of the\nset S= {1, 2, 3, 4, 5}.\nSolution: For each law we just work through both sides of the equation and show they are\nthe same.\n1. (A∪B)c= Ac∩Bc:\nRight hand side: A∪B= {1, 2, 3, 4} ⇒(A∪B)c= {5}.\nLeft hand side: Ac= {4, 5}, Bc= {1, 2, 5} ⇒Ac∩Bc= {5}.\nThe two sides are equal.\nQED\n2. (A∩B)c= Ac∪Bc:\nRight hand side: A∩B= {3} ⇒(A∩B)c= {1, 2, 4, 5}.\nLeft hand side: Ac= {4, 5}, Bc= {1, 2, 5} ⇒Ac∪Bc= {1, 2, 4, 5}.\nThe two sides are equal.\nQED\nThink: Draw and label a Venn diagram with Athe set of Brain and Cognitive Science\nmajors and Bthe set of sophomores. Shade the region illustrating the first law. Can you\nexpress the first law in this case as a non-technical English sentence?\n2.2.3\nProducts of sets\nThe product of sets Sand Tis the set of ordered pairs:\nS× T= {(s, t) | s∈S, t∈T}.\nIn words the right-hand side reads \"the set of ordered pairs (s, t) such that sis in Sand t\nis in T.\nThe following diagrams show two examples of the set product.\n\n18.05 Class 1, Counting and Sets, Spring 2022\n×\n(1,1)\n(1,2)\n(1,3)\n(1,4)\n(2,1)\n(2,2)\n(2,3)\n(2,4)\n(3,1)\n(3,2)\n(3,3)\n(3,4)\n{1, 2, 3} × {1, 2, 3, 4}\n[1, 4] × [1, 3] ⊂[0, 5] × [0, 4]\nThe right-hand figure also illustrates that if A⊂Sand B⊂Tthen A× B⊂S× T.\n2.3\nCounting\nIf Sis finite, we use |S| or #Sto denote the number of elements of S.\nTwo useful counting principles are the inclusion-exclusion principle and the rule of product.\n2.3.1\nInclusion-exclusion principle\nThe inclusion-exclusion principle says\n|A∪B| = |A| + |B| -|A∩B|.\nWe can illustrate this with a Venn diagram. Sis all the dots, Ais the dots in the blue\ncircle, and Bis the dots in the red circle.\nS\nA\nB\nA∩B\n|A| is the number of dots in Aand likewise for the other sets. The figure shows that |A|+|B|\ndouble-counts |A∩B|, which is why |A∩B| is subtracted off in the inclusion-exclusion\nformula.\nExample 4. In a band of singers and guitarists, seven people sing, four play the guitar,\nand two do both. How big is the band?\n\n18.05 Class 1, Counting and Sets, Spring 2022\nSolution: Let Sbe the set singers and Gbe the set guitar players. The inclusion-exclusion\nprinciple says\nsize of band = |S∪G| = |S| + |G| -|S∩G| = 7 + 4 -2 = 9.\n2.3.2\nRule of Product\nThe Rule of Product says:\nIf there are nways to perform action 1 and then by mways to perform action\n2, then there are n⋅mways to perform action 1 followed by action 2.\nWe will also call this the multiplication rule.\nExample 5. If you have 3 shirts and 4 pants then you can make 3 ⋅4 = 12 outfits.\nThink: An extremely important point is that the rule of product holds even if the ways to\nperform action 2 depend on action 1, as long as the number of ways to perform action 2 is\nindependent of action 1. To illustrate this:\nExample 6. There are 5 competitors in the 100m final at the Olympics. In how many\nways can the gold, silver, and bronze medals be awarded?\nSolution: There are 5 ways to award the gold. Once that is awarded there are 4 ways to\naward the silver and then 3 ways to award the bronze: answer 5 ⋅4 ⋅3 = 60 ways.\nNote that the choice of gold medalist affects who can win the silver, but the number of\npossible silver medalists is always four.\n2.4\nPermutations and combinations\n2.4.1\nPermutations\nA permutation of a set is a particular ordering of its elements. For example, the set {a, b, c}\nhas six permutations: abc, acb, bac, bca, cab, cba. We found the number of permutations by\nlisting them all. We could also have found the number of permutations by using the rule\nof product. That is, there are 3 ways to pick the first element, then 2 ways for the second,\nand 1 for the third. This gives a total of 3 ⋅2 ⋅1 = 6 permutations.\nIn general, the rule of product tells us that the number of permutations of a set of kelements\nis\nk! = k⋅(k-1) ⋯3 ⋅2 ⋅1.\nWe also talk about the permutations of kthings out of a set of nthings. We show what\nthis means with an example.\nExample 7. List all the permutations of 3 elements out of the set {a, b, c, d}.\nSolution: This is a longer list,\nabc\nacb\nbac\nbca\ncab\ncba\nabd\nadb\nbad\nbda\ndab\ndba\nacd\nadc\ncad\ncda\ndac\ndca\nbcd\nbdc\ncbd\ncdb\ndbc\ndcb\n\n18.05 Class 1, Counting and Sets, Spring 2022\nNote that abcand acbcount as distinct permutations. That is, for permutations the order\nmatters.\nThere are 24 permutations. Note that the rule of product would have told us there are\n4 ⋅3 ⋅2 = 24 permutations without bothering to list them all.\n2.4.2\nCombinations\nIn contrast to permutations, in combinations order does not matter: permutations are lists\nand combinations are sets. We show what we mean with an example\nExample 8. List all the combinations of 3 elements out of the set {a, b, c, d}.\nSolution: Such a combination is a collection of 3 elements without regard to order. So, abc\nand cabboth represent the same combination. We can list all the combinations by listing\nall the subsets of exactly 3 elements.\n{a, b, c}\n{a, b, d}\n{a, c, d}\n{b, c, d}\nThere are only 4 combinations. Contrast this with the 24 permutations in the previous\nexample. The factor of 6 comes because every combination of 3 things can be written in 6\ndifferent orders.\n2.4.3\nFormulas\nWe'll use the following notations.\nnPk= number of permutations (lists) of kdistinct elements from a set of size n\nnCk= (n\nk) = number of combinations (subsets) of kelements from a set of size n\nWe emphasize that by the number of combinations of kelements we mean the number of\nsubsets of size k.\nThese have the following notation and formulas:\nPermutations:\nnPk\n=\nn!\n(n-k)! = n(n-1) ⋯(n-k+ 1)\nCombinations:\nnCk\n=\nn!\nk!(n-k)! = nPk\nk!\nThe notation nCkis read \"nchoose k\". The formula for nPkfollows from the rule of product.\nIt also implies the formula for nCkbecause a subset of size kcan be ordered in k! ways.\nWe can illustrate the relation between permutations and combinations by lining up the\nresults of the previous two examples.\nabc\nacb\nbac\nbca\ncab\ncba\n{a, b, c}\nabd\nadb\nbad\nbda\ndab\ndba\n{a, b, d}\nacd\nadc\ncad\ncda\ndac\ndca\n{a, c, d}\nbcd\nbdc\ncbd\ncdb\ndbc\ndcb\n{b, c, d}\nPermutations: 4P3\nCombinations: 4C3\nNotice that each row in the permutations list consists of all 3! permutations of the corre-\nsponding set in the combinations list.\n\n18.05 Class 1, Counting and Sets, Spring 2022\n2.4.4\nExamples\nExample 9. Count the following:\n(i) The number of ways to choose 2 out of 4 things (order does not matter).\n(ii) The number of ways to list 2 out of 4 things.\n(iii) The number of ways to choose 3 out of 10 things.\nSolution: (i) This is asking for combinations: (4\n2) =\n4!\n2! 2! = 6.\n(ii) This is asking for permuations: 4P2 = 4!\n2! = 12.\n(iii) This is asking for combinations: (10\n3 ) =\n10!\n3! 7! = 10⋅9⋅8\n3⋅2⋅1 = 120.\nExample 10. (i) Count the number of ways to get 3 heads in a sequence of 10 flips of a\ncoin.\n(ii) If the coin is fair, what is the probability of exactly 3 heads in 10 flips?\nSolution: (i) This asks for the number sequences of 10 flips (heads or tails) with exactly\n3 heads. That is, we have to choose exactly 3 out of 10 flips to be heads. This is the same\nquestion as in the previous example.\n(10\n3 ) = 10!\n3! 7! = 10 ⋅9 ⋅8\n3 ⋅2 ⋅1 = 120.\n(ii) Each flip has 2 possible outcomes (heads or tails). So the rule of product says there are\n210 = 1024 sequences of 10 flips. Since the coin is fair each sequence is equally probable.\nSo the probability of 3 heads is\n1024 = 0.117 .\n\nProbability: Terminology and Examples\nClass 2, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Know the definitions of sample space, event and probability function.\n2. Be able to organize a scenario with randomness into an experiment and sample space.\n3. Be able to make basic computations using a probability function.\nTerminology\n2.1\nProbability cast list\n- Experiment: a repeatable procedure with well-defined possible outcomes.\n- Sample space: the set of all possible outcomes. We usually denote the sample space by\nΩ, sometimes by S.\n- Event: a subset of the sample space.\n- Probability function: a function giving the probability for each outcome.\nLater in the course we will learn about\n- Probability density: a continuous distribution of probabilities.\n- Random variable: a random numerical outcome.\n2.2\nSimple examples\nExample 1. Toss a fair coin.\nExperiment: toss the coin, report if it lands heads or tails.\nSample space: Ω = {H, T}.\nProbability function: P(H) = 0.5,\nP(T) = 0.5.\nExample 2. Toss a fair coin 3 times.\nExperiment: toss the coin 3 times, list the results.\nSample space: Ω = {HHH, HHT, HTH, HTT, THH, THT, TTH, TTT}.\nProbability function: Each outcome is equally likely with probability 1/8.\nFor small sample spaces we can put the set of outcomes and probabilities into a probability\ntable.\nOutcomes\nHHH\nHHT\nHTH\nHTT\nTHH\nTHT\nTTH\nTTT\nProbability\n1/8\n1/8\n1/8\n1/8\n1/8\n1/8\n1/8\n1/8\n\n18.05 Class 2, Probability: Terminology and Examples, Spring 2022\nExample 3. Measure the mass of a proton\nExperiment: follow some defined procedure to measure the mass and report the result.\nSample space: Ω = [0, inf), i.e. in principle we can get any positive value.\nProbability function: since there is a continuum of possible outcomes there is no probability\nfunction. Instead we need to use a probability density, which we will learn about later in\nthe course.\nExample 4. Taxis (An infinite discrete sample space)\nExperiment: count the number of taxis that pass 77 Mass. Ave during an 18.05 class.\nSample space: Ω = {0, 1, 2, 3, 4, ...}.\nThis is often modeled with the following probability function known as the Poisson distri-\nbution. (Do not worry about mastering the Poisson distribution just yet):\nP(k) = e-λλk\nk! ,\nwhere λis the average number of taxis. We can put this in a table:\nOutcome\n...\nk\n...\nProbability\ne-λ\ne-λλ\ne-λλ2/2\ne-λλ3/3!\n...\ne-λλk/k!\n...\nQuestion: Accepting that this is a valid probability function, what is\ninf\n∑\nk=0\ne-λλk\nk! ?\nSolution:\nThis is the total probability of all possible outcomes, so the sum equals 1.\n(Note, this also follows from the Taylor series\neλ=\ninf\n∑\nn=0\nλn\nn! .)\nIn a given setup there can be more than one reasonable choice of sample space. Here is a\nsimple example.\nExample 5. Two dice (Choice of sample space)\nSuppose you roll one die. Then the sample space and probability function are\nOutcome\nProbability:\n1/6\n1/6\n1/6\n1/6\n1/6\n1/6\nNow suppose you roll two dice. What should be the sample space? Here are two options.\n1. Record the pair of numbers showing on the dice (first die, second die).\n2.\nRecord the sum of the numbers on the dice.\nIn this case there are 11 outcomes\n{2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}. These outcomes are not all equally likely.\nAs above, we can put this information in tables. For the first case, the sample space is the\nproduct of the sample spaces for each die\n{(1, 1), (2, 1), (3, 1), ... (6, 6)}\nEach of the 36 outcomes is equally likely. (Why 36 outcomes?) For the probability function\nwe will make a two dimensional table with the rows corresponding to the number on the\nfirst die, the columns the number on the second die and the entries the probability.\n\n18.05 Class 2, Probability: Terminology and Examples, Spring 2022\nDie 2\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\nDie 1\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\nTwo dice in a two dimensional table\nIn the second case we can present outcomes and probabilities in our usual table.\noutcome\nprobability\n1/36\n2/36\n3/36\n4/36\n5/36\n6/36\n5/36\n4/36\n3/36\n2/36\n1/36\nThe sum of two dice\nThink: What is the relationship between the two probability tables above?\nWe will see that the best choice of sample space depends on the context. For now, simply\nnote that given the outcome as a pair of numbers it is easy to find the sum.\nNote. Listing the experiment, sample space and probability function is a good way to start\nworking systematically with probability. It can help you avoid some of the common pitfalls\nin the subject.\nEvents.\nAn event is a collection of outcomes, i.e. an event is a subset of the sample space Ω. This\nsounds odd, but it actually corresponds to the common meaning of the word.\nExample 6. Using the setup in Example ?? we would describe the event that you get\nexactly two heads in words by E= 'exactly 2 heads'. Written as a subset this becomes\nE= {HHT, HTH, THH}.\nYou should get comfortable moving between describing events in words and as subsets of\nthe sample space.\nThe probability of an event Eis computed by adding up the probabilities of all of the\noutcomes in E. In this example each outcome has probability 1/8, so we have P(E) = 3/8.\n2.3\nDefinition of a discrete sample space\nDefinition. A discrete sample space is one that is listable, it can be either finite or infinite.\nExamples. {H, T},\n{1, 2, 3},\n{1, 2, 3, 4, ...},\n{2, 3, 5, 7, 11, 13, 17, ...} are all discrete\nsets. The first two are finite and the last two are infinite.\nExample. The interval 0 ≤x≤1 is not discrete, rather it is continuous. We will deal\nwith continuous sample spaces in a few days.\n\n18.05 Class 2, Probability: Terminology and Examples, Spring 2022\n2.4\nThe probability function\nSo far we've been using a casual definition of the probability function. Let's give a more\nprecise one.\nCareful definition of the probability function.\nFor a discrete sample space Sa probability function Passigns to each outcome ωa number\nP(ω) called the probability of ω. Pmust satisfy two rules:\n- Rule 1. 0 ≤P(ω) ≤1\n(probabilities are between 0 and 1).\n- Rule 2. The sum of the probabilities of all possible outcomes is 1 (something must\noccur)\nIn symbols, Rule 2 says: if S= {ω1, ω2, ... , ωn} then P(ω1) + P(ω2) + ... + P(ωn) = 1. Or,\nusing summation notation:\nn\n∑\nj=1\nP(ωj) = 1.\nThe probability of an event Eis the sum of the probabilities of all the outcomes in E. That\nis,\nP(E) = ∑\nω∈E\nP(ω).\nThink: Check Rules 1 and 2 on Examples 1 and 2 above.\nExample 7. Flip until heads (A classic example)\nSuppose we have a coin with probability pof heads and we have the following scenario.\nExperiment: Toss the coin until the first heads. Report the number of tosses.\nSample space: Ω = {1, 2, 3, ...}.\nProbability function: P(n) = (1 -p)n-1p.\nChallenge 1: show the sum of all the probabilities equals 1 (hint: geometric series).\nChallenge 2: justify the formula for P(n) (we will do this soon).\nStopping problems. The previous toy example is an uncluttered version of a general\nclass of problems called stopping rule problems. A stopping rule is a rule that tells you\nwhen to end a certain process. In the toy example above the process was flipping a coin\nand we stopped after the first heads. A more practical example is a rule for ending a series\nof medical treatments. Such a rule could depend on how well the treatments are working,\nhow the patient is tolerating them and the probability that the treatments would continue\nto be effective. One could ask about the probability of stopping within a certain number of\ntreatments or the average number of treatments you should expect before stopping.\nSome rules of probability\nFor events A, Land Rcontained in a sample space Ω.\nRule 1. P(Ac) = 1 -P(A).\nRule 2. If Land Rare disjoint then P(L∪R) = P(L) + P(R).\n\n18.05 Class 2, Probability: Terminology and Examples, Spring 2022\nRule 3. If Land Rare not disjoint, we have the inclusion-exclusion principle:\nP(L∪R) = P(L) + P(R) -P(L∩R)\nWe visualize these rules using Venn diagrams.\nA\nAc\nΩ = A∪Ac, no overlap\nL\nR\nL∪R, no overlap\nL\nR\nL∪R, overlap = L∩R\nWe can also justify them logically.\nRule 1: Aand Acsplit Ω into two non-overlapping regions. Since the total probability\nP(Ω) = 1 this rule says that the probabiity of Aand the probability of 'not A' are comple-\nmentary, i.e. sum to 1.\nRule 2: Land Rsplit L∪Rinto two non-overlapping regions. So the probability of L∪R\nis is split between P(L) and P(R)\nRule 3: In the sum P(L) + P(R) the overlap P(L∩R) gets counted twice. So P(L) +\nP(R) -P(L∩R) counts everything in the union exactly once.\nThink: Rule 2 is a special case of Rule 3.\nFor the following examples suppose we have an experiment that produces a random integer\nbetween 1 and 20. The probabilities are not necessarily uniform, i.e., not necessarily the\nsame for each outcome.\nExample 8. If the probability of an even number is 0.6 what is the probability of an odd\nnumber?\nSolution: Since being odd is complementary to being even, the probability of being odd is\n1-0.6 = 0.4.\nLet's redo this example a bit more formally, so you see how it's done. First, so we can refer\nto it, let's name the random integer X. Let's also name the event 'Xis even' as A. Then the\nevent 'Xis odd' is Ac. We are given that P(A) = 0.6. Therefore P(Ac) = 1 -0.6 = 0.4 .\nExample 9. Consider the 2 events, A: 'Xis a multiple of 2'; B: 'Xis odd and less than\n10'. Suppose P(A) = 0.6 and P(B) = 0.25.\n(i) What is A∩B?\n(ii) What is the probability of A∪B?\nSolution: (i) Since all numbers in Aare even and all numbers in Bare odd, these events\nare disjoint. That is, A∩B= ∅.\n(ii) Since Aand Bare disjoint P(A∪B) = P(A) + P(B) = 0.85.\nExample 10. Let A, Band Cbe the events Xis a multiple of 2, 3 and 6 respectively. If\nP(A) = 0.6, P(B) = 0.3 and P(C) = 0.2 what is P(Aor B)?\nSolution: Note two things. First we used the word 'or' which means union: 'Aor B' =\nA∪B. Second, an integer is divisible by 6 if and only if it is divisible by both 2 and 3.\n\n18.05 Class 2, Probability: Terminology and Examples, Spring 2022\nThis translates into C= A∩B. So the inclusion-exclusion principle says\nP(A∪B) = P(A) + P(B) -P(A∩B) = 0.6 + 0.3 -0.2 = 0.7 .\n\nConditional Probability, Independence and Bayes' Theorem\nClass 3, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Know the definitions of conditional probability and independence of events.\n2. Be able to compute conditional probability directly from the definition.\n3. Be able to use the multiplication rule to compute the total probability of an event.\n4. Be able to check if two events are independent.\n5. Be able to use Bayes' formula to 'invert' conditional probabilities.\n6. Be able to organize the computation of conditional probabilities using trees and tables.\n7. Understand the base rate fallacy thoroughly.\nConditional Probability\nConditional probability answers the question 'how does the probability of an event change\nif we have extra information'. We'll illustrate with an example.\nExample 1. Toss a fair coin 3 times.\n(a) What is the probability of 3 heads?\nSolution: Sample space Ω = {HHH, HHT, HTH, HTT, THH, THT, TTH, TTT}.\nAll outcomes are equally probable, so P(3 heads) = 1/8.\n(b) Suppose we are told that the first toss was heads. Given this information how should\nwe compute the probability of 3 heads?\nSolution: We have a new (reduced) sample space: Ω′ = {HHH, HHT, HTH, HTT}.\nAll outcomes are equally probable, so\nP(3 heads given that the first toss is heads) = 1/4.\nThis is called conditional probability, since it takes into account additional conditions. To\ndevelop the notation, we rephrase (b) in terms of events.\nRephrased (b) Let Abe the event 'all three tosses are heads' = {HHH}.\nLet Bbe the event 'the first toss is heads' = {HHH, HHT, HTH, HTT}.\nThe conditional probability of Aknowing that Boccurred is written\nP(A|B)\nThis is read as\n'the conditional probability of Agiven B'\n\n18.05 Class 3, Conditional Probability, Independence and Bayes' Theorem, Spring 2022\nor\n'the probability of Aconditioned on B'\nor simply\n'the probability of Agiven B'.\nWe can visualize conditional probability as follows. Think of P(A) as the proportion of the\narea of the whole sample space taken up by A. For P(A|B) we restrict our attention to B.\nThat is, P(A|B) is the proportion of area of Btaken up by A, i.e. P(A∩B)/P(B).\nB\nA\nA∩B\nHTH\nHTT\nTTH\nTTT\nHHH\nHHT\nTHH\nTHT\nA= A∩B\nB\nConditional probability: Abstract visualization and coin example\nNote, A⊂Bin the right-hand figure, so there are only two colors shown.\nThe formal definition of conditional probability catches the gist of the above example and\nvisualization.\nFormal definition of conditional probability\nLet Aand Bbe events. We define the conditional probability of Agiven Bas\nP(A|B) = P(A∩B)\nP(B)\n, provided P(B) =0.\n(1)\nLet's redo the coin tossing example using the definition in Equation (1). Recall A= '3 heads'\nand B= 'first toss is heads'. We have P(A) = 1/8 and P(B) = 1/2. Since A∩B= A, we\nalso have P(A∩B) = 1/8. Now according to (1),\nP(A|B) = P(A∩B)\nP(B)\n= 1/8\n1/2 = 1/4,\nwhich agrees with our answer in Example 1 b.\nWe start with a simple example where we can find all the probabilities directly by counting.\nExample 2. Draw two cards from a deck.\nDefine the events: S1 = 'first card is a spade'\nand S2 = 'second card is a spade'. What is the P(S2|S1)?\nSolution: We'll use formula (1) to compute the conditional probability. We have to com-\npute P(S1), P(S2) and P(S1 ∩S2): We know that P(S1) = 1/4 because there are 52 equally\nprobable ways to draw the first card and 13 of them are spades. The same logic says that\nthere are 52 equally probable ways the second card can be drawn, so P(S2) = 1/4.\nAside: The probability P(S2) = 1/4 may seem surprising since the value of first card\ncertainly affects the probabilities for the second card. However, if we look at all possible\ntwo card sequences we will see that every card in the deck has equal probability of being\n\n18.05 Class 3, Conditional Probability, Independence and Bayes' Theorem, Spring 2022\nthe second card. Since 13 of the 52 cards are spades we get P(S2) = 13/52 = 1/4. Another\nway to say this is: if we are not given value of the first card then we have to consider all\npossibilities for the second card.\nContinuing, we compute P(S1 ∩S2) by counting:\nNumber of ways to draw a spade followed by a second spade: 13 ⋅12.\nNumber of ways to draw any card followed by any other card: 52 ⋅51.\nThus,\nP(S1 ∩S2) = 13 ⋅12\n52 ⋅51 = 3/51.\nNow, using (1) we get\nP(S2|S1) = P(S2 ∩S1)\nP(S1)\n= 3/51\n1/4 = 12/51.\nThis case is simple enough that we can check our answer by computing the conditional\nprobability directly: if the first card is a spade then of the 51 cards remaining, 12 are\nspades. So, the probability the second card is also a spade is\nP(S2|S1) = 12/51.\nWarning: In more complicated problems it will be be much harder to compute conditional\nprobability by counting. Usually we have to use Equation (1).\nThink: For S1 and S2 in the previous example, what is P(S2|Sc\n1)?\nMultiplication Rule\nThe following formula is called the multiplication rule.\nP(A∩B) = P(A|B) ⋅P(B).\n(2)\nThis is simply a rewriting of the definition in Equation (1) of conditional probability. We\nwill see that our use of the multiplication rule is very similar to our use of the rule of\nproduct in counting. In fact, the multiplication rule is just a souped up version of the rule\nof product.\nWe start by verifying the multiplication rule for the previous example.\nExample 3. Draw two cards from a deck.\nDefine the events: S1 = 'first card is a spade'\nand S2 = 'second card is a spade'. Verify the multiplication rule.\nSolution: From the previous example, we know P(S2|S1) = 12/51, P(S1 ∩S2) = 3/51, P(S1) =\n1/4. From this it is easy to check that\nP(S2|S1) ⋅P(S1) = 12\n51 ⋅1\n4 = 3\n51 = P(S1 ∩S2).\nLaw of Total Probability\nThe law of total probability will allow us to use the multiplication rule to find probabilities\nin more interesting examples. It involves a lot of notation, but the idea is fairly simple. We\n\n18.05 Class 3, Conditional Probability, Independence and Bayes' Theorem, Spring 2022\nstate the law when the sample space is divided into 3 pieces. It is a simple matter to extend\nthe rule when there are more than 3 pieces.\nLaw of Total Probability\nSuppose the sample space Ω is divided into 3 disjoint events B1, B2, B3 (see the figure\nbelow). Then for any event A:\nP(A) = P(A∩B1) + P(A∩B2) + P(A∩B3)\nP(A) = P(A|B1) P(B1) + P(A|B2) P(B2) + P(A|B3) P(B3)\n(3)\nThe top equation says 'if Ais divided into 3 pieces then P(A) is the sum of the probabilities\nof the pieces'. The bottom equation (3) is called the law of total probability. It is just a\nrewriting of the top equation using the multiplication rule.\nΩ\nB3\nB2\nB1\nA ∩B1\nA ∩B2 A ∩B3\nThe sample space Ω and the event Aare each divided into 3 disjoint pieces.\nThe law holds if we divide Ω into any number of events, so long as they are disjoint and\ncover all of Ω. Such a division is often called a partition of Ω.\nOur first example will be one where we already know the answer and can verify the law.\nExample 4. An urn contains 5 red balls and 2 green balls. Two balls are drawn one after\nthe other. What is the probability that the second ball is red?\nSolution: The sample space is Ω = {rr, rg, gr, gg}.\nLet R1 be the event 'the first ball is red', G1 = 'first ball is green', R2 = 'second ball is\nred', G2 = 'second ball is green'. We are asked to find P(R2).\nLet's compute this same value using the law of total probability (3). First, we'll find the\nconditional probabilities. This is a simple counting exercise.\nP(R2|R1) = 4/6,\nP(R2|G1) = 5/6.\nSince R1 and G1 partition Ω the law of total probability says\nP(R2) = P(R2|R1)P(R1) + P(R2|G1)P(G1)\n(4)\n= 4\n6 ⋅5\n7 + 5\n6 ⋅2\n= 30\n42 = 5\n7.\n\n18.05 Class 3, Conditional Probability, Independence and Bayes' Theorem, Spring 2022\nOf course, this example is simple enough that we could have computed P(R2) directly the\nsame way we found P(S2) directly in the card example. But, we will see that in more\ncomplicated examples the law of total probability is truly necessary.\nProbability urns\nThe example above used probability urns. Their use goes back to the beginning of the\nsubject and we would be remiss not to introduce them. This toy model is very useful. We\nquote from Wikipedia: https://en.wikipedia.org/wiki/Urn_problem\nIn probability and statistics, an urn problem is an idealized mental exercise\nin which some objects of real interest (such as atoms, people, cars, etc.) are\nrepresented as colored balls in an urn or other container. One pretends to draw\n(remove) one or more balls from the urn; the goal is to determine the probability\nof drawing one color or another, or some other properties. A key parameter is\nwhether each ball is returned to the urn after each draw.\nIt doesn't take much to make an example where (3) is really the best way to compute the\nprobability. Here is a game with slightly more complicated rules.\nExample 5. An urn contains 5 red balls and 2 green balls. A ball is drawn. If it's green\na red ball is added to the urn and if it's red a green ball is added to the urn. (The original\nball is not returned to the urn.) Then a second ball is drawn. What is the probability the\nsecond ball is red?\nSolution: The law of total probability says that P(R2) can be computed using the expres-\nsion in Equation (4). Only the values for the probabilities will change. We have\nP(R2|R1) = 4/7,\nP(R2|G1) = 6/7.\nTherefore,\nP(R2) = P(R2|R1)P(R1) + P(R2|G1)P(G1) = 4\n7 ⋅5\n7 + 6\n7 ⋅2\n7 = 32\n49.\nUsing Trees to Organize the Computation\nTrees are a great way to organize computations with conditional probability and the law of\ntotal probability. The figures and examples will make clear what we mean by a tree. As\nwith the rule of product, the key is to organize the underlying process into a sequence of\nactions.\nWe start by redoing Example 5. The sequence of actions are: first draw ball 1 (and add the\nappropriate ball to the urn) and then draw ball 2.\nG1\nR1\nR2\nG2\nR2\nG2\n5/7\n2/7\n4/7\n3/7\n6/7\n1/7\n\n18.05 Class 3, Conditional Probability, Independence and Bayes' Theorem, Spring 2022\nYou interpret this tree as follows. Each dot is called a node. The tree is organized by levels.\nThe top node (root node) is at level 0. The next layer down is level 1 and so on. Each level\nshows the outcomes at one stage of the game. Level 1 shows the possible outcomes of the\nfirst draw. Level 2 shows the possible outcomes of the second draw starting from each node\nin level 1.\nProbabilities are written along the branches. The probability of R1 (red on the first draw)\nis 5/7. It is written along the branch from the root node to the one labeled R1. At the\nnext level we put in conditional probabilities. The probability along the branch from R1 to\nR2 is P(R2|R1) = 4/7. It represents the probability of going to node R2 given that you are\nalready at R1.\nThe muliplication rule says that the probability of getting to any node is just the product of\nthe probabilities along the path to get there. For example, the node labeled R2 at the far left\nreally represents the event R1 ∩R2 because it comes from the R1 node. The multiplication\nrule now says\nP(R1 ∩R2) = P(R1) ⋅P(R2|R1) = 5\n7 ⋅4\n7,\nwhich is exactly multiplying along the path to the node.\nThe law of total probability is just the statement that P(R2) is the sum of the probabilities\nof all paths leading to R2 (the two circled nodes in the figure). In this case,\nP(R2) = 5\n7 ⋅4\n7 + 2\n7 ⋅6\n7 = 32\n49,\nexactly as in the previous example.\n5.1\nShorthand vs. precise trees\nThe tree given above involves some shorthand. For example, the node marked R2 at the\nfar left really represents the event R1 ∩R2, since it ends the path from the root through\nR1 to R2. Here is the same tree with everything labeled precisely. As you can see this tree\nis more cumbersome to make and use. We usually use the shorthand version of trees. You\nshould make sure you know how to interpret them precisely.\nG1\nR1\nR1 ∩R2\nR1 ∩G2\nG1 ∩R2\nG1 ∩G2\nP(R1) = 5/7\nP(G1) = 2/7\nP(R2|R1) = 4/7\nP(G2|R1) = 3/7\nP(R2|G1) = 6/7\nP(G2|G1) = 1/7\nIndependence\nTwo events are independent if knowledge that one occurred does not change the probability\nthat the other occurred. Informally, events are independent if they do not influence one\nanother.\n\n18.05 Class 3, Conditional Probability, Independence and Bayes' Theorem, Spring 2022\nExample 6. Toss a coin twice. We expect the outcomes of the two tosses to be independent\nof one another. In real experiments this always has to be checked. If my coin lands in honey\nand I don't bother to clean it, then the second toss might be affected by the outcome of the\nfirst toss.\nMore seriously, the independence of experiments can by undermined by the failure to clean or\nrecalibrate equipment between experiments or to isolate supposedly independent observers\nfrom each other or a common influence. We've all experienced hearing the same 'fact' from\ndifferent people. Hearing it from different sources tends to lend it credence until we learn\nthat they all heard it from a common source. That is, our sources were not independent.\nTranslating the verbal description of independence into symbols gives\nAis independent of B\nif\nP(A|B) = P(A).\n(5)\nThat is, knowing that Boccurred does not change the probability that Aoccurred. In\nterms of events as subsets, knowing that the realized outcome is in Bdoes not change the\nprobability that it is in A.\nIf Aand Bare independent in the above sense, then the multiplication rule gives\nP(A∩B) = P(A|B) ⋅P(B) = P(A) ⋅P(B).\nThis justifies the following technical definition of independence.\nFormal definition of independence: Two events Aand Bare independent if\nP(A∩B) = P(A) ⋅P(B)\n(6)\nThis is a nice symmetric definition which makes clear that Ais independent of Bif and only\nif Bis independent of A. Unlike the equation with conditional probabilities, this definition\nmakes sense even when P(B) = 0. In terms of conditional probabilities, we have:\n1. If P(B) =0 then Aand Bare independent if and only if P(A|B) = P(A).\n2. If P(A) =0 then Aand Bare independent if and only if P(B|A) = P(B).\nIndependent events commonly arise as different trials in an experiment, as in the following\nexample.\nExample 7. Toss a fair coin twice. Let H1 = 'heads on first toss' and let H2 = 'heads on\nsecond toss'. Are H1 and H2 independent?\nSolution: Since H1 ∩H2 is the event 'both tosses are heads' we have\nP(H1 ∩H2) = 1/4 = P(H1)P(H2).\nTherefore the events are independent.\nWe can ask about the independence of any two events, as in the following two examples.\nExample 8. Toss a fair coin 3 times. Let H1 = 'heads on first toss' and A= 'two heads\ntotal'. Are H1 and Aindependent?\nSolution: We know that P(A) = 3/8. Since this is not 0 we can check if the formula in\nEquation 5 holds. Now, H1 = {HHH, HHT, HTH, HTT} contains exactly two outcomes\n(HHT, HTH) from A, so we have P(A|H1) = 2/4. Since P(A|H1) =P(A) these events\nare not independent.\n\n18.05 Class 3, Conditional Probability, Independence and Bayes' Theorem, Spring 2022\nExample 9. Draw one card from a standard deck of playing cards. Let's examine the\nindependence of 3 events 'the card is an ace', 'the card is a heart' and 'the card is red'.\nDefine the events as A= 'ace', H= 'hearts', R= 'red'.\n(a) We know that P(A) = 4/52 (4 out of 52 cards are aces),\nP(A|H) = 1/13 (1 out of 13\nhearts are aces). Since P(A) = P(A|H) we have that Ais independent of H.\n(b) P(A|R) = 2/26 = 1/13 = P(A). So Ais independent of R. That is, whether the card\nis an ace is independent of whether it is red.\n(c) Finally, what about Hand R? Since P(H) = 1/4 and P(H|R) = 1/2, Hand Rare not\nindependent. We could also see this the other way around: P(R) = 1/2 and P(R|H) = 1,\nso Hand Rare not independent. That is, the suit of a card is not independent of the color\nof the card's suit.\n6.1\nParadoxes of Independence\nAn event Awith probability 0 is independent of itself, since in this case both sides of\nequation (6) are 0. This appears paradoxical because knowledge that Aoccurred certainly\ngives information about whether Aoccurred. We resolve the paradox by noting that since\nP(A) = 0 the statement 'Aoccurred' is vacuous.\nThink: For what other value(s) of P(A) is Aindependent of itself?\nBayes' Theorem\nBayes' theorem is a pillar of both probability and statistics and it is central to the rest of\nthis course. For two events Aand BBayes' theorem (also called Bayes' rule and Bayes'\nformula) says\nP(B|A) = P(A|B) ⋅P(B)\nP(A)\n.\n(7)\nComments: 1. Bayes' rule tells us how to 'invert' conditional probabilities, i.e. to find\nP(B|A) from P(A|B).\n2. In practice, P(A) is often computed using the law of total probability.\nProof of Bayes' rule\nThe key point is that A∩Bis symmetric in Aand B. So the multiplication rule says\nP(B|A) ⋅P(A) = P(A∩B) = P(A|B) ⋅P(B).\nNow divide through by P(A) to get Bayes' rule.\nA common mistake is to confuse the meanings of P(A|B) and P(B|A). They can be very\ndifferent. This is illustrated in the next example.\nExample 10. Toss a coin 5 times. Let H1 = 'first toss is heads' and let HA= 'all 5 tosses\nare heads'. Then P(H1|HA) = 1 but P(HA|H1) = 1/16.\nFor practice, let's use Bayes' theorem to compute P(H1|HA) using P(HA|H1).The terms\n\n18.05 Class 3, Conditional Probability, Independence and Bayes' Theorem, Spring 2022\nare P(HA|H1) = 1/16,\nP(H1) = 1/2,\nP(HA) = 1/32. So,\nP(H1|HA) = P(HA|H1)P(H1)\nP(HA)\n= (1/16) ⋅(1/2)\n1/32\n= 1,\nwhich agrees with our previous calculation.\n7.1\nThe Base Rate Fallacy\nThe base rate fallacy is one of many examples showing that it's easy to confuse the meaning\nof P(B|A) and P(A|B) when a situation is described in words.\nThis is one of the key\nexamples from probability and it will inform much of our practice and interpretation of\nstatistics. You should strive to understand it thoroughly.\nExample 11. The Base Rate Fallacy\nConsider a routine screening test for a disease. Suppose the frequency of the disease in the\npopulation (base rate) is 0.5%. The test is fairly accurate with a 5% false positive rate and\na 10% false negative rate.\nYou take the test and it comes back positive. What is the probability that you have the\ndisease?\nSolution: We will do the computation three times: using trees, tables and symbols. We'll\nuse the following notation for the relevant events:\nD+ = 'you have the disease'\nD-= 'you do not have the disease\nT+ = 'you tested positive'\nT-= 'you tested negative'.\nWe are given P(D+) = 0.005 and therefore P(D-) = 0.995. The false positive and false\nnegative rates are (by definition) conditional probabilities.\nP(false positive) = P(T+|D-) = 0.05\nand\nP(false negative) = P(T-|D+) = 0.1.\nThe complementary probabilities are known as the true negative and true positive rates:\nP(T-|D-) = 1 -P(T+|D-) = 0.95\nand\nP(T+|D+) = 1 -P(T-|D+) = 0.9.\nTrees: All of these probabilities can be displayed quite nicely in a tree.\nD+\nD-\nT+\nT-\nT+\nT-\n0.005\n0.995\n0.9\n0.1\n0.05\n0.95\nThe question asks for the probability that you have the disease given that you tested positive,\ni.e. what is the value of P(D+|T+). We aren't given this value, but we do know P(T+|D+),\nso we can use Bayes' theorem.\nP(D+|T+) = P(T+|D+) ⋅P(D+)\nP(T+)\n.\n\n18.05 Class 3, Conditional Probability, Independence and Bayes' Theorem, Spring 2022 10\nThe two probabilities in the numerator are given. We compute the denominator P(T+)\nusing the law of total probability. Using the tree, we just have to sum the probabilities for\neach of the nodes marked T+\nP(T+) = 0.995 × 0.05 + 0.005 × 0.9 = 0.05425\nThus,\nP(D+|T+) = 0.9 × 0.005\n0.05425\n= 0.082949 ≈8.3%.\nRemarks: This is called the base rate fallacy because the base rate of the disease in the\npopulation is so low that the vast majority of the people taking the test are healthy, and\neven with an accurate test most of the positives will be healthy people. Ask your doctor\nfor his/her guess at the odds.\nTo summarize the base rate fallacy with specific numbers\n95% of all tests are accurate does not imply 95% of positive tests are accurate\nWe will refer back to this example frequently. It and similar examples are at the heart of\nmany statistical misunderstandings.\nOther ways to work Example 11\nTables: Another trick that is useful for computing probabilities is to make a table. Let's\nredo the previous example using a table built with 10000 total people divided according to\nthe probabilites in this example.\nWe construct the table as follows. Pick a number, say 10000 people, and place it as the\ngrand total in the lower right. Using P(D+) = 0.005 we compute that 50 out of the 10000\npeople are sick (D+). Likewise 9950 people are healthy (D-). At this point the table looks\nlike:\nD+\nD-\ntotal\nT+\nT-\ntotal\nUsing P(T+|D+) = 0.9 we can compute that the number of sick people who tested positive\nas 90% of 50 or 45. The other entries are similar. At this point the table looks like the\ntable below on the left. Finally we sum the T+ and T-rows to get the completed table on\nthe right.\nD+\nD-\ntotal\nT+\nT-\ntotal\nD+\nD-\ntotal\nT+\nT-\ntotal\nUsing the complete table we can compute\nP(D+|T+) = |D+ ∩T+|\n|T+|\n= 45\n543 = 8.3%.\nSymbols: For completeness, we show how the solution looks when written out directly in\n\n18.05 Class 3, Conditional Probability, Independence and Bayes' Theorem, Spring 2022 11\nsymbols.\nP(D+|T+) = P(T+|D+) ⋅P(D+)\nP(T+)\n=\nP(T+|D+) ⋅P(D+)\nP(T+|D+) ⋅P(D+) + P(T+|D-) ⋅P(D-)\n=\n0.9 × 0.005\n0.9 × 0.005 + 0.05 × 0.995\n= 8.3%\nVisualization: The figure below illustrates the base rate fallacy. The large blue rectangle\nrepresents all the healthy people. The much smaller orange rectangle represents the sick\npeople. The shaded rectangle represents the people who test positive. The shaded area\ncovers most of the orange area and only a small part of the blue area. Even so, the most of\nthe shaded area is over the blue. That is, most of the positive tests are of healthy people.\nD-\nD+\n7.2\nBayes' rule in 18.05\nAs we said at the start of this section, Bayes' rule is a pillar of probability and statistics.\nWe have seen that Bayes' rule allows us to 'invert' conditional probabilities. When we study\nstatistics we will see that the art of statistical inference involves deciding how to proceed\nwhen one (or more) of the terms on the right side of Bayes' rule is unknown.\n\nDiscrete Random Variables\nClass 4, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Know the definition of a discrete random variable.\n2. Know the Bernoulli, binomial, and geometric distributions and examples of what they\nmodel.\n3. Be able to describe the probability mass function and cumulative distribution function\nusing tables and formulas.\n4. Be able to construct new random variables from old ones.\nRandom Variables\nThis topic is largely about introducing some useful terminology, building on the notions of\nsample space and probability function. The key words are\n1. Random variable\n2. Probability mass function (pmf)\n3. Cumulative distribution function (cdf)\n2.1\nRecap\nA discrete sample space Ω is a finite or listable set of outcomes {ω1, ω2 ...}. The probability\nof an outcome ωis denoted P(ω).\nAn event Eis a subset of Ω. The probability of an event Eis\nP(E) = ∑\nω∈E\nP(ω).\n2.2\nRandom variables as payoff functions\nExample 1. A game with 2 dice.\nRoll a die twice and record the outcomes as (i, j), where iis the result of the first roll and\njthe result of the second. We can take the sample space to be\nΩ = {(1, 1), (1, 2), (1, 3), ... , (6, 6)} = {(i, j) | i, j= 1, ... 6}.\nThe probability function is P(i, j) = 1/36.\n\n18.05 Class 4, Discrete Random Variables, Spring 2022\nIn this game, you win $500 if the sum is 7 and lose $100 otherwise. We give this payoff\nfunction the name Xand describe it formally by\nX(i, j) = {500\nif i+ j= 7\n-100\nif i+ j=7.\nExample 2. We can change the game by using a different payoff function. For example\nY(i, j) = ij-10.\nIn this example if you roll (6, 2) then you win $2. If you roll (2, 3) then you win -$4 (i.e.,\nlose $4).\nQuestion: Which game is the better bet?\nSolution: We will come back to this once we learn about expectation.\nThese payoff functions are examples of random variables.\nA random variable assigns a\nnumber to each outcome in a sample space. More formally:\nDefinition: Let Ω be a sample space. A discrete random variable is a function\nX∶Ω →R\nthat takes a discrete set of values. (Recall that R stands for the real numbers.)\nWhy is Xcalled a random variable? It's 'random' because its value depends on a random\noutcome of an experiment. And we treat Xlike we would a usual variable: we can add it\nto other random variables, square it, and so on.\n2.3\nEvents and random variables\nFor any value awe write X= ato mean the event consisting of all outcomes ωwith\nX(ω) = a.\nExample 3. In Example 1 we rolled two dice and Xwas the random variable\nX(i, j) = {500\nif i+ j= 7\n-100\nif i+ j=7.\nThe event X= 500 is the set {(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)}, i.e. the set of all\noutcomes that sum to 7. So P(X= 500) = 1/6.\nWe allow ato be any value, even values that Xnever takes. In Example 1, we could look\nat the event X= 1000. Since Xnever equals 1000 this is just the empty event (or empty\nset)\n‵X= 1000′ = {} = ∅\nP(X= 1000) = 0.\n\n18.05 Class 4, Discrete Random Variables, Spring 2022\n2.4\nProbability mass function and cumulative distribution function\nIt gets tiring and hard to read and write P(X= a) for the probability that X= a. When\nwe know we're talking about Xwe will simply write p(a). If we want to make Xexplicit\nwe will write pX(a). We spell this out in a definition.\nDefinition: The probability mass function (pmf) of a discrete random variable is the\nfunction p(a) = P(X= a).\nNote:\n1. We always have 0 ≤p(a) ≤1.\n2. We allow ato be any number. If ais a value that Xnever takes, then p(a) = 0.\nExample 4.\nLet Ω be our earlier sample space for rolling 2 dice.\nDefine the random\nvariable Mto be the maximum value of the two dice, i.e.\nM(i, j) = max(i, j).\nFor example, the roll (3,5) has maximum 5, i.e. M(3, 5) = 5.\nWe can describe a random variable by listing its possible values and the probabilities asso-\nciated to these values. For the above example we have:\nvalue a:\npmf p(a):\n1/36\n3/36\n5/36\n7/36\n9/36\n11/36\nFor example, p(2) = 3/36.\nQuestion: What is p(8)?\nSolution: p(8) = 0.\nThink: What is the pmf for Z(i, j) = i+ j? Does it look familiar?\n2.5\nEvents and inequalities\nInequalities with random variables describe events. For example X≤ais the set of all\noutcomes ωsuch that X(w) ≤a.\nExample 5. If our sample space is the set of all pairs of (i, j) coming from rolling two dice\nand Z(i, j) = i+ jis the sum of the dice then\nZ≤4 = {(1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (3, 1)}\n2.6\nThe cumulative distribution function (cdf)\nDefinition: The cumulative distribution function (cdf) of a random variable Xis the\nfunction Fgiven by F(a) = P(X≤a). We will often shorten this to distribution function.\nNote well that the definition of F(a) uses the symbol less than or equal to. This will be\nimportant for getting your calculations exactly right.\nExample. Continuing with the example M, we have\nvalue a:\npmf p(a):\n1/36\n3/36\n5/36\n7/36\n9/36\n11/36\ncdf F(a):\n1/36\n4/36\n9/36\n16/36\n25/36\n36/36\n\n18.05 Class 4, Discrete Random Variables, Spring 2022\nF(a) is called the cumulative distribution function because F(a) gives the total probability\nthat accumulates by adding up the probabilities p(b) as bruns from -infto a. For example,\nin the table above, the entry 16/36 in column 4 for the cdf is the sum of the values of the\npmf from column 1 to column 4. In notation:\nAs events: 'M≤4' = {1, 2, 3, 4};\nF(4) = P(M≤4) = 1/36+3/36+5/36+7/36 = 16/36.\nJust like the probability mass function, F(a) is defined for all values a.\nIn the above\nexample, F(8) = 1, F(-2) = 0, F(2.5) = 4/36, and F(π) = 9/36.\n2.7\nGraphs of p(a) and F(a)\nWe can visualize the pmf and cdf with graphs. For example, let Xbe the number of heads\nin 3 tosses of a fair coin:\nvalue a:\npmf p(a):\n1/8\n3/8\n3/8\n1/8\ncdf F(a):\n1/8\n4/8\n7/8\nThe colored graphs show how the cumulative distribution function is built by accumulating\nprobability as aincreases. The black and white graphs are the more standard presentations.\na\n1/8\n3/8\na\n1/8\n3/8\nProbability mass function for X\na\n1/8\n4/8\n7/8\na\n1/8\n4/8\n7/8\nCumulative distribution function for X\n\n18.05 Class 4, Discrete Random Variables, Spring 2022\na\n1/36\n3/36\n5/36\n7/36\n9/36\n11/36\na\n1/36\n4/36\n9/36\n16/36\n25/36\n36/36\npmf and cdf for the maximum of two dice (Example 4)\na\n9 10 11 12\n1/36\n2/36\n3/36\n4/36\n5/36\n6/36\na\n9 10 11 12\n1/36\n3/36\n6/36\n10/36\n15/36\n21/36\n26/36\n30/36\n33/36\npmf and cdf for the sum of two dice\nHistograms: Later we will see another way to visualize the pmf using histograms. These\nrequire some care to do right, so we will wait until we need them.\n2.8\nProperties of the cdf F\nThe cdf Fof a random variable satisfies several properties:\n1. Fis non-decreasing. That is, its graph never goes down, or symbolically if a≤bthen\nF(a) ≤F(b).\n\n18.05 Class 4, Discrete Random Variables, Spring 2022\n2. 0 ≤F(a) ≤1.\n3. lim\na→infF(a) = 1,\nlim\na→-infF(a) = 0.\nIn words, (1) says the cumulative probability F(a) increases or remains constant as a\nincreases, but never decreases; (2) says the accumulated probability is always between 0\nand 1; (3) says that as agets very large, it becomes more and more certain that X≤aand\nas agets very negative it becomes more and more certain that X> a.\nThink: Why does a cdf satisfy each of these properties?\nSpecific Distributions\n3.1\nBernoulli Distributions\nModel: The Bernoulli distribution models one trial in an experiment that can result in\neither success or failure This is the most important distribution and is also the simplest. A\nrandom variable Xhas a Bernoulli distribution with parameter pif:\n1. Xtakes the values 0 and 1.\n2. P(X= 1) = pand P(X= 0) = 1 -p.\nWe will write X∼Bernoulli(p) or Ber(p), which is read \"Xfollows a Bernoulli distribution\nwith parameter p\" or \"Xis drawn from a Bernoulli distribution with parameter p\".\nA simple model for the Bernoulli distribution is to flip a coin with probability pof heads,\nwith X= 1 on heads and X= 0 on tails. The general terminology is to say Xis 1 on\nsuccess and 0 on failure, with success and failure defined by the context.\nMany decisions can be modeled as a binary choice, such as votes for or against a proposal.\nIf pis the proportion of the voting population that favors the proposal, than the vote of a\nrandom individual is modeled by a Bernoulli(p).\nHere are the table and graphs of the pmf and cdf for the Bernoulli(1/2) distribution and\nbelow that for the general Bernoulli(p) distribution.\nvalue a:\npmf p(a):\n1/2\n1/2\ncdf F(a):\n1/2\na\np(a)\n1/2\na\nF(a)\n1/2\nTable, pmf and cmf for the Bernoulli(1/2) distribution\n\n18.05 Class 4, Discrete Random Variables, Spring 2022\nvalues a:\npmf p(a):\n1-p\np\ncdf F(a):\n1-p\na\np(a)\n1 -p\np\na\nF(a)\n1 -p\nTable, pmf and cmf for the Bernoulli(p) distribution\n3.2\nBinomial Distributions\nThe binomial distribution Binomial(n,p), or Bin(n,p), models the number of successes in n\nindependent Bernoulli(p) trials.\nThere is a hierarchy here. A single Bernoulli trial is, say, one toss of a coin. A single\nbinomial trial consists of nBernoulli trials. For coin flips the sample space for a Bernoulli\ntrial is {H, T}. The sample space for a binomial trial is all sequences of heads and tails of\nlength n. Likewise a Bernoulli random variable takes values 0 and 1 and a binomial random\nvariables takes values 0, 1, 2, ..., n.\nExample 6. Binomial(1,p) is the same as Bernoulli(p).\nExample 7. The number of heads in nflips of a coin with probability pof heads follows\na Binomial(n, p) distribution.\nWe describe X∼Binomial(n, p) by giving its values and probabilities. For notation we will\nuse kto mean an arbitrary number between 0 and n.\nWe remind you that 'nchoose k= (n\nk) = nCkis the number of ways to choose kthings\nout of a collection of nthings and it has the formula\n(n\nk) =\nn!\nk! (n-k)!.\n(1)\n(It is also called a binomial coefficient.) Here is a table for the pmf of a Binomial(n, k) ran-\ndom variable. We will explain how the binomial coefficients enter the pmf for the binomial\ndistribution after a simple example.\nvalues a:\n⋯\nk\n⋯\nn\npmf p(a):\n(1 -p)n\n(n\n1)p1(1 -p)\nn-1\n(n\n2)p2(1 -p)\nn-2\n⋯\n(n\nk)pk(1 -p)\nn-k\n⋯\npn\nExample 8. What is the probability of 3 or more heads in 5 tosses of a fair coin?\nSolution: The binomial coefficients associated with n= 5 are\n(5\n0) = 1,\n(5\n1) =\n5!\n1! 4! = 5 ⋅4 ⋅3 ⋅2 ⋅1\n4 ⋅3 ⋅2 ⋅1\n= 5,\n(5\n2) =\n5!\n2! 3! = 5 ⋅4 ⋅3 ⋅2 ⋅1\n2 ⋅1 ⋅3 ⋅2 ⋅1 = 5 ⋅4\n= 10,\nand similarly\n(5\n3) = 10,\n(5\n4) = 5,\n(5\n5) = 1.\n\n18.05 Class 4, Discrete Random Variables, Spring 2022\nUsing these values we get the following table for X∼Binomial(5,p).\nvalues a:\npmf p(a):\n(1 -p)5\n5p(1 -p)4\n10p2(1 -p)3\n10p3(1 -p)2\n5p4(1 -p)\np5\nWe were told p= 1/2 so\nP(X≥3) = 10 (1\n2)\n(1\n2)\n+ 5 (1\n2)\n(1\n2)\n+ (1\n2)\n= 16\n32 = 1\n2.\nThink: Why is the value of 1/2 not surprising?\n3.3\nExplanation of the binomial probabilities\nFor concreteness, let n= 5 and k= 2 (the argument for arbitrary nand kis identical.) So\nX∼binomial(5, p) and we want to compute p(2). The long way to compute p(2) is to list\nall the ways to get exactly 2 heads in 5 coin flips and add up their probabilities. The list\nhas 10 entries:\nHHTTT, HTHTT, HTTHT, HTTTH, THHTT, THTHT, THTTH, TTHHT, TTHTH,\nTTTHH\nEach entry has the same probability of occurring, namely\np2(1 -p)3.\nThis is because each of the two heads has probability pand each of the 3 tails has proba-\nbility 1 -p. Because the individual tosses are independent we can multiply probabilities.\nTherefore, the total probability of exactly 2 heads is the sum of 10 identical probabilities,\ni.e.\np(2) = 10p2(1 -p)3,\nas shown in the table.\nThis guides us to the shorter way to do the computation. We have to count the number of\nsequences with exactly 2 heads. To do this we need to choose 2 of the tosses to be heads\nand the remaining 3 to be tails. The number of such sequences is the number of ways to\nchoose 2 out of 5 things, that is (5\n2). Since each such sequence has the same probability,\np2(1 -p)3, we get the probability of exactly 2 heads p(2) = (5\n2)p2(1 -p)3.\nHere are some binomial probability mass functions:\na\n0.1\n0.2\nBinomial(10, 0.5)\na\n0.1\n0.2\n0.3\n0.4\nBinomial(10, 0.1)\na\n0.1\n0.2\nBinomial(20, 0.1)\n\n18.05 Class 4, Discrete Random Variables, Spring 2022\n3.4\nGeometric Distributions\nA geometric distribution models the number of tails before the first head in a sequence of\ncoin flips (Bernoulli trials).\nExample 9. (a) Flip a coin repeatedly. Let Xbe the number of tails before the first heads.\nSo, Xcan equal 0, i.e. the first flip is heads, 1, 2, .... In principle it takes any nonnegative\ninteger value.\n(b) Give a flip of tails the value 0, and heads the value 1. In this case, Xis the number of\n0's before the first 1.\n(c) Give a flip of tails the value 1, and heads the value 0. In this case, Xis the number of\n1's before the first 0.\n(d) Call a flip of tails a success and heads a failure. So, Xis the number of successes before\nthe first failure.\n(e) Call a flip of tails a failure and heads a success. So, Xis the number of failures before\nthe first success.\nYou can see this models many different scenarios of this type. The most neutral language\nis the number of tails before the first head.\nFormal definition. The random variable Xfollows a geometric distribution with param-\neter pif\n- Xtakes the values 0, 1, 2, 3, ...\n- its pmf is given by p(k) = P(X= k) = (1 -p)kp.\nWe denote this by X∼geometric(p) or geo(p). In table form we have:\nvalue\na:\n...\nk\n...\npmf\np(a):\np\n(1 -p)p\n(1 -p)2p\n(1 -p)3p\n...\n(1 -p)kp\n...\nTable: X∼geometric(p): X= the number of 0s before the first 1.\nWe will show how this table was computed in an example below.\nThe geometric distribution is an example of a discrete distribution that takes an infinite\nnumber of possible values. Things can get confusing when we work with successes and\nfailure since we might want to model the number of successes before the first failure or we\nmight want the number of failures before the first success. To keep straight things straight\nyou can translate to the neutral language of the number of tails before the first heads.\n\n18.05 Class 4, Discrete Random Variables, Spring 2022\na\n0.0\n0.1\n0.2\n0.3\n0.4\na\n0.0\n0.2\n0.4\n0.6\n0.8\npmf and cdf for the geometric(1/3) distribution\nExample 10.\nComputing geometric probabilities.\nSuppose that the inhabitants of an\nisland plan their families by having babies until the first girl is born. Assume the probability\nof having a girl with each pregnancy is 0.5 independent of other pregnancies, that all babies\nsurvive and there are no multiple births. What is the probability that a family has kboys?\nSolution: In neutral language we can think of boys as tails and girls as heads. Then the\nnumber of boys in a family is the number of tails before the first heads.\nLet's practice using standard notation to present this. So, let Xbe the number of boys in\na (randomly-chosen) family. So, Xis a geometric random variable. We are asked to find\np(k) = P(X= k). A family has kboys if the sequence of children in the family from oldest\nto youngest is\nBBB... BG\nwith the first kchildren being boys. The probability of this sequence is just the product\nof the probability for each child, i.e. (1/2)k⋅(1/2) = (1/2)k+1. (Note: The assumptions of\nequal probability and independence are simplifications of reality.)\nThink: What is the ratio of boys to girls on the island?\nMore geometric confusion. Another common definition for the geometric distribution is the\nnumber of tosses until the first heads. In this case Xcan take the values 1, i.e. the first\nflip is heads, 2, 3, .... This is just our geometric random variable plus 1. The methods of\ncomputing with it are just like the ones we used above.\n3.5\nUniform Distribution\nThe uniform distribution models any situation where all the outcomes are equally likely.\nX∼uniform(N).\nXtakes values 1, 2, 3, ... , N, each with probability 1/N. We have already seen this distribu-\ntion many times when modeling to fair coins (N= 2), dice (N= 6), birthdays (N= 365),\nand poker hands (N= (52\n5 )).\n\n18.05 Class 4, Discrete Random Variables, Spring 2022\n3.6\nDiscrete Distributions Applet\nThe applet at https://mathlets.org/mathlets/probability-distributions/ gives a\ndynamic view of some discrete distributions. The graphs will change smoothly as you move\nthe various sliders. Try playing with the different distributions and parameters.\nThis applet is carefully color-coded. Two things with the same color represent the same or\nclosely related notions. By understanding the color-coding and other details of the applet,\nyou will acquire a stronger intuition for the distributions shown.\n3.7\nOther Distributions\nThere are a million other named distributions arising is various contexts. We don't expect\nyou to memorize them (we certainly have not!), but you should be comfortable using a\nresource like Wikipedia to look up a pmf. For example, take a look at the info box at the\ntop right of https://en.wikipedia.org/wiki/Hypergeometric_distribution. The info\nbox lists many (surely unfamiliar) properties in addition to the pmf.\nArithmetic with Random Variables\nWe can do arithmetic with random variables. For example, we can add subtract, multiply\nor square them.\nThere is a simple, but extremely important idea for counting. It says that if we have a\nsequence of numbers that are either 0 or 1 then the sum of the sequence is the number of\n1s.\nExample 11. Consider the sequence with five 1s\n1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0.\nIt is easy to see that the sum of this sequence is 5 the number of 1s.\nWe illustrate this idea by counting the number of heads in ntosses of a coin.\nExample 12. Toss a fair coin ntimes. Let Xjbe 1 if the jth toss is heads and 0 if it's\ntails. So, Xjis a Bernoulli(1/2) random variable. Let Xbe the total number of heads in\nthe ntosses. Assuming the tosses are independent we know X∼binomial(n, 1/2). We can\nalso write\nX= X1 + X2 + X3 + ... + Xn.\nAgain, this is because the terms in the sum on the right are all either 0 or 1. So, the sum\nis exactly the number of Xjthat are 1, i.e. the number of heads.\nThe important thing to see in the example above is that we've written the more complicated\nbinomial random variable Xas the sum of extremely simple random variables Xj. This will\nallow us to manipulate Xalgebraically.\nThink: Suppose Xand Yare independent and X∼binomial(n, 1/2) and Y∼binomial(m, 1/2).\nWhat kind of distribution does X+ Yfollow? (Answer: binomial(n+ m, 1/2). Why?)\n\n18.05 Class 4, Discrete Random Variables, Spring 2022\nExample 13.\nSuppose Xand Yare independent random variables with the following\ntables.\nValues of X\nx:\npmf\npX(x):\n1/10\n2/10\n3/10\n4/10\nValues of Y\ny:\npmf\npY(y):\n1/15\n2/15\n3/15\n4/15\n5/15\nCheck that the total probability for each random variable is 1. Make a table for the random\nvariable X+ Y.\nSolution: The first thing to do is make a two-dimensional table for the product sample\nspace consisting of pairs (x, y), where xis a possible value of Xand yone of Y. To help\ndo the computation, the probabilities for the Xvalues are put in the far right column and\nthose for Yare in the bottom row. Because Xand Yare independent the probability for\n(x, y) pair is just the product of the individual probabilities.\nYvalues\nXvalues\n1/150\n2/150\n3/150\n4/150\n5/150\n2/150\n4/150\n6/150\n8/150\n10/150\n3/150\n6/150\n9/150\n12/150\n15/150\n4/150\n8/150\n12/150\n16/150\n20/150\n1/15\n2/15\n3/15\n4/15\n5/15\n1/10\n2/10\n3/10\n4/10\nThe diagonal stripes show sets of squares where X+ Yis the same. All we have to do to\ncompute the probability table for X+ Yis sum the probabilities for each stripe.\nX+ Yvalues:\npmf:\n1/150\n4/150\n10/150\n20/150\n30/150\n34/150\n31/150\n20/150\nWhen the tables are too big to write down we'll need to use purely algebraic techniques to\ncompute the probabilities of a sum. We will learn how to do this in due course.\n\nDiscrete Random Variables: Expected Value\nClass 4, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Know how to compute expected value (mean) of a discrete random variable.\n2. Know the expected value of Bernoulli, binomial and geometric random variables.\nExpected Value\nIn the R reading questions for this lecture, you simulated the average value of rolling a die\nmany times. You should have gotten a value close to the exact answer of 3.5. To motivate\nthe formal definition of the average, or expected value, we first consider some examples.\nExample 1. Suppose we have a six-sided die marked with five 5 3's and one 6. (This was\nthe red one from our non-transitive dice.) What would you expect the average of 6000 rolls\nto be?\nSolution: If we knew the value of each roll, we could compute the average by summing\nthe 6000 values and dividing by 6000. Without knowing the values, we can compute the\nexpected average as follows.\nSince there are five 3's and one six we expect roughly 5/6 of the rolls will give 3 and 1/6 will\ngive 6. Assuming this to be exactly true, we have the following table of values and counts:\nvalue:\nexpected counts:\nThe average of these 6000 values is then\n5000 ⋅3 + 1000 ⋅6\n= 5\n6 ⋅3 + 1\n6 ⋅6 = 3.5\nWe consider this the expected average in the sense that we 'expect' each of the possible\nvalues to occur with the given frequencies.\nExample 2. We roll two standard 6-sided dice. You win $1000 if the sum is 2 and lose\n$100 otherwise. How much do you expect to win on average per trial?\nSolution: The probability of a 2 is 1/36. If you play Ntimes, you can 'expect' 1\n36 ⋅N\nof the trials to give a 2 and 35\n36 ⋅Nof the trials to give something else. Thus your total\nexpected winnings are\n1000 ⋅N\n36 -100 ⋅35N\n36 .\nTo get the expected average per trial we divide the total by N:\nexpected average = 1000 ⋅1\n36 -100 ⋅35\n36 = -69.44.\n\n18.05 Class 4, Discrete Random Variables: Expected Value, Spring 2022\nThink: Would you be willing to play this game one time? Multiple times?\nNotice that in both examples the sum for the expected average consists of terms which are\na value of the random variable times its probabilitiy. This leads to the following definition.\nDefinition: Suppose Xis a discrete random variable that takes values x1, x2, ..., xnwith\nprobabilities p(x1), p(x2), ..., p(xn). The expected value of Xis denoted E[X] and defined\nby\nE[X] =\nn\n∑\nj=1\np(xj) xj= p(x1)x1 + p(x2)x2 + ... + p(xn)xn.\nNotes:\n1. The expected value is also called the mean or average of Xand often denoted by μ\n(\"mu\").\n2. As seen in the above examples, the expected value need not be a possible value of the\nrandom variable. Rather it is a weighted average of the possible values.\n3. Expected value is a summary statistic, providing a measure of the location or central\ntendency of a random variable.\n4. If all the values are equally probable then the expected value is just the usual average of\nthe values.\nExample 3. Find E[X] for the random variable X with table:\nvalues of X:\npmf:\n1/6\n1/6\n2/3\nSolution: E[X] = 1\n6 ⋅1 + 1\n6 ⋅3 + 2\n3 ⋅5 = 24\n6 = 4\nExample 4. Let Xbe a Bernoulli(p) random variable. Find E[X].\nSolution: Xtakes values 1 and 0 with probabilities pand 1 -p, so\nE[X] = p⋅1 + (1 -p) ⋅0 = p.\nImportant: This is an important example. Be sure to remember that the expected value of\na Bernoulli(p) random variable is p.\nThink: What is the expected value of the sum of two dice?\n2.1\nMean and center of mass\nYou may have wondered why we use the name 'probability mass function'.\nHere's one\nreason: if we place an object of mass p(xj) at position xjfor each j, then E[X] is the\nposition of the center of mass. Let's recall the latter notion via an example.\nExample 5. Suppose we have two masses along the x-axis, mass m1 = 500 at position\nx1 = 3 and mass m2 = 100 at position x2 = 6. Where is the center of mass?\n\n18.05 Class 4, Discrete Random Variables: Expected Value, Spring 2022\nSolution: Intuitively we know that the center of mass is closer to the larger mass.\nx\nm1\nm2\nFrom physics we know the center of mass is\nx= m1x1 + m2x2\nm1 + m2\n= 500 ⋅3 + 100 ⋅6\n= 3.5.\nWe call this formula a 'weighted' average of the x1 and x2. Here x1 is weighted more heavily\nbecause it has more mass.\nNow look at the definition of expected value E[X]. It is a weighted average of the values of\nXwith the weights being probabilities p(xi) rather than masses! We might say that \"The\nexpected value is the point at which the distribution would balance\". Note the similarity\nbetween the physics example and Example 1.\n2.2\nAlgebraic properties of E[X]\nWhen we add, scale or shift random variables the expected values do the same.\nThe\nshorthand mathematical way of saying this is that E[X] is linear.\n1. If Xand Yare random variables on a sample space Ω then\nE[X+ Y] = E[X] + E[Y]\n2. If aand bare constants then\nE[aX+ b] = aE[X] + b.\nWe will think of aX+ bas scaling Xby aand shifting it by b.\nBefore proving these properties, let's see them in action with a few examples.\nExample 6. Roll two dice and let Xbe the sum. Find E[X].\nSolution: Let X1 be the value on the first die and let X2 be the value on the second\ndie.\nSince X= X1 + X2 we have E[X] = E[X1] + E[X2].\nEarlier we computed that\nE[X1] = E[X2] = 3.5, therefore E[X] = 7.\nExample 7. Let X∼binomial(n, p). Find E[X].\nSolution: Recall that Xmodels the number of successes in nBernoulli(p) random variables,\nwhich we'll call X1, ... Xn. The key fact, which we highlighted in the previous reading for\nthis class, is that\nX= X1 + X2 + ... + Xn=\nn\n∑\nj=1\nXj.\nNow we can use the Algebraic Property (1) to make the calculation simple.\nX=\nn\n∑\nj=1\nXj⇒E[X] = ∑\nj\nE[Xj] = ∑\nj\np= np.\n\n18.05 Class 4, Discrete Random Variables: Expected Value, Spring 2022\nWe could have computed E[X] directly as\nE[X] =\nn\n∑\nk=0\nkp(k) =\nn\n∑\nk=0\nk(n\nk)pk(1 -p)n-k.\nIt is possible to show that the sum of this series is indeed np. We think you'll agree that\nthe method using Property (1) is much easier.\nExample 8. (For infinite random variables the mean does not always exist.)\nSuppose X\nhas an infinite number of values according to the following table.\nvalues x:\n...\n2k\n...\npmf p(x):\n1/2\n1/22\n1/23\n...\n1/2k\n...\nTry to compute the mean.\nSolution: The mean is\nE[X] =\ninf\n∑\nk=1\n2k1\n2k=\ninf\n∑\nk=1\n1 = inf.\nThe mean does not exist! This can happen with infinite series.\nExample 9. Mean of a geometric distribution\nLet X∼geo(p). Recall this means Xtakes values k= 0, 1, 2, ...with probabilities p(k) =\n(1 -p)kp. (Xmodels the number of tails before the first heads in a sequence of Bernoulli\ntrials.) The mean is given by\nE[X] = 1 -p\np\n.\nTo see this requires a clever trick. Mathematicians love this sort of thing and we hope you\nare able to follow the logic and enjoy it. In this class we will not ask you to come up with\nsomething like this on an exam.\nHere's the trick.: to compute E[X] we have to sum the infinite series\nE[X] =\ninf\n∑\nk=0\nk(1 -p)kp.\nNow, we know the sum of the geometric series:\ninf\n∑\nk=0\nxk=\n1 -x.\nDifferentiate both sides:\ninf\n∑\nk=0\nkxk-1 =\n(1 -x)2 .\nMultiply by x:\ninf\n∑\nk=0\nkxk=\nx\n(1 -x)2 .\nReplace xby 1 -p:\ninf\n∑\nk=0\nk(1 -p)k= 1 -p\np2 .\nMultiply by p:\ninf\n∑\nk=0\nk(1 -p)kp= 1 -p\np\n.\nThis last expression is the mean.\nE[X] = 1 -p\np\n.\n\n18.05 Class 4, Discrete Random Variables: Expected Value, Spring 2022\nExample 10. Flip a fair coin until you get heads for the first time. What is the expected\nnumber of times you flipped tails?\nSolution: The number of tails before the first head is modeled by X∼geo(1/2). From the\nprevious example E[X] = 1/2\n1/2 = 1. This is a surprisingly small number.\nExample 11. Michael Jordan, perhaps the greatest basketball player ever, made 80% of\nhis free throws. In a game what is the expected number he would make before his first miss.\nSolution: Here is an example where we want the number of successes before the first\nfailure. Using the neutral language of heads and tails: success is tails (probability 1 -p)\nand failure is heads (probability = p). Therefore p= 0.2 and the number of tails (made\nfree throws) before the first heads (missed free throw) is modeled by a X∼geo(0.2). We\nsaw in Example 9 that this is\nE[X] = 1 -p\np\n= 0.8\n0.2 = 4.\n2.3\nExpected values of functions of a random variable\n(The change of variables formula.)\nIf Xis a discrete random variable taking values x1, x2, ...and his a function then h(X) is\na new random variable. Its expected value is\nE[h(X)] = ∑\nj\nh(xj)p(xj).\nWe illustrate this with several examples.\nExample 12. Let Xbe the value of a roll of one die and let Y= X2. Find E[Y].\nSolution: Since there are a small number of values we can make a table.\nX\nY\nprob\n1/6\n1/6\n1/6\n1/6\n1/6\n1/6\nNotice the probability for each Yvalue is the same as that of the corresponding Xvalue.\nSo,\nE[Y] = E[X2] = 12 ⋅1\n6 + 22 ⋅1\n6 + ... + 62 ⋅1\n6 = 15.167.\nExample 13. Roll two dice and let Xbe the sum. Suppose the payoff function is given by\nY= X2 -6X+ 1. Is this a good bet?\nSolution: We have E[Y] =\n∑\nj=2\n(j2 -6j+ 1)p(j), where p(j) = P(X= j).\nWe show the table, but really we'll use R to do the calculation.\nX\nY\n-7\n-8\n-7\n-4\nprob\n1/36\n2/36\n3/36\n4/36\n5/36\n6/36\n5/36\n4/36\n3/36\n2/36\n1/36\nHere's the R code I used to compute E[Y] = 13.833.\n\n18.05 Class 4, Discrete Random Variables: Expected Value, Spring 2022\nx = 2:12\ny = x^2 - 6*x + 1\np = c(1 2 3 4 5 6 5 4 3 2 1)/36\nave = sum(p*y)\nIt gave E[Y] = 13.833.\nTo answer the question above: since the expected payoff is positive it looks like a bet worth\ntaking.\nQuiz: If Y= h(X) does E[Y] = h(E[X])? Solution: NO!!! This is not true in general!\nThink: Is it true in the previous example?\nQuiz: If Y= 3X+ 77 does E[Y] = 3E[X] + 77?\nSolution: Yes. By property (2), scaling and shifting does behave like this.\n2.4\nProofs of the algebraic properties of E[X]\nWe finish by proving the algebraic properties of E[X].\n1. For random variables Xand Yon a sample space Ω: E[X+ Y] = E[X] + E[Y]>\n2. For constants a, band random variable X: E[aX+ b] = aE[X] + b.\nThe proof of Property (1) is simple, but there is some subtlety in even understanding what\nit means to add two random variables. Recall that the value of random variable is a number\ndetermined by the outcome of an experiment. To add Xand Ymeans to add the values of\nXand Yfor the same outcome. In table form this looks like:\noutcome ω:\nω1\nω2\nω3\n...\nωn\nvalue of X:\nx1\nx2\nx3\n...\nxn\nvalue of Y:\ny1\ny2\ny3\n...\nyn\nvalue of X+ Y:\nx1 + y1\nx2 + y2\nx3 + y3\n...\nxn+ yn\nprob. P(ω):\nP(ω1)\nP(ω2)\nP(ω3)\n...\nP(ωn)\nThe proof of (1) follows immediately:\nE[X+ Y] = ∑(xi+ yi)P(ωi) = ∑xiP(ωi) + ∑yiP(ωi) = E[X] + E[Y].\nThe proof of Property (2) only takes one line.\nE[aX+ b] = ∑p(xi)(axi+ b) = a∑p(xi)xi+ b∑p(xi) = aE[X] + b.\nThe bterm in the last expression follows because ∑p(xi) = 1.\n\nVariance of Discrete Random Variables\nClass 5, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Be able to compute the variance and standard deviation of a random variable.\n2. Understand that standard deviation is a measure of scale or spread.\n3. Be able to compute variance using the properties of scaling and linearity.\nSpread\nThe expected value (mean) of a random variable is a measure of location or central tendency.\nIf you had to summarize a random variable with a single number, the mean would be a good\nchoice. Still, the mean leaves out a good deal of information. For example, the random\nvariables Xand Ybelow both have mean 0, but their probability mass is spread out about\nthe mean quite differently.\nvalues X\n-2\n-1\npmf p(x)\n1/10\n2/10\n4/10\n2/10\n1/10\nvalues Y\n-3\npmf p(y)\n1/2\n1/2\nIt's probably a little easier to see the different spreads in plots of the probability mass\nfunctions. We use bars instead of dots to give a better sense of the mass.\nx\np(x)\n-1\n-2\n1/10\n2/10\n4/10\npmf for X\ny\np(y)\n-3\n1/2\npmf for Y\npmf's for two different distributions both with mean 0\nIn the next section, we will learn how to quantify this spread.\nVariance and standard deviation\nTaking the mean as the center of a random variable's probability distribution, the variance\nis a measure of how much the probability mass is spread out around this center. We'll start\nwith the formal definition of variance and then unpack its meaning.\nDefinition: If Xis a random variable with mean E[X] = μ, then the variance of Xis\ndefined by\nVar(X) = E[(X-μ)2].\n\n18.05 Class 5, Variance of Discrete Random Variables, Spring 2022\nThe standard deviation σof Xis defined by\nσ= √Var(X).\nIf the relevant random variable is clear from context, then the variance and standard devi-\nation are often denoted by σ2 and σ('sigma'), just as the mean is μ('mu').\nWhat does this mean? First, let's rewrite the definition explicitly as a sum. If Xtakes\nvalues x1, x2, ... , xnwith probability mass function p(xi) then\nVar(X) = E[(X-μ)2] =\nn\n∑\ni=1\np(xi)(xi-μ)2.\nIn words, the formula for Var(X) says to take a weighted average of the squared distance\nto the mean. By squaring, we make sure we are averaging only non-negative values, so that\nthe spread to the right of the mean won't cancel that to the left. By using expectation,\nwe are weighting high probability values more than low probability values. (See Example 2\nbelow.)\nNote on units:\n1. σhas the same units as X.\n2. Var(X) has the same units as the square of X. So if Xis in meters, then Var(X) is in\nmeters squared.\nBecause σand Xhave the same units, the standard deviation is a natural measure of spread.\nLet's work some examples to make the notion of variance clear.\nExample 1. Compute the mean, variance and standard deviation of the random variable\nXwith the following table of values and probabilities.\nvalue x\npmf p(x)\n1/4\n1/4\n1/2\nSolution: First we compute E[X] = 7/2. Then we extend the table to include (X-7/2)2.\nvalue x\np(x)\n1/4\n1/4\n1/2\n(x-7/2)2\n25/4\n1/4\n9/4\nNow the computation of the variance is similar to that of expectation:\nVar(X) = 25\n4 ⋅1\n4 + 1\n4 ⋅1\n4 + 9\n4 ⋅1\n2 = 11\n4 .\nTaking the square root we have the standard deviation σ= √11/4.\nExample 2. For each random variable X, Y, Z, and Wplot the pmf and compute the\nmean and variance.\n(i)\nvalue x\npmf p(x)\n1/5\n1/5\n1/5\n1/5\n1/5\n(ii)\nvalue y\npmf p(y)\n1/10\n2/10\n4/10\n2/10\n1/10\n\n18.05 Class 5, Variance of Discrete Random Variables, Spring 2022\n(iii)\nvalue z\npmf p(z)\n5/10\n5/10\n(iv)\nvalue w\npmf p(w)\nSolution: Each random variable has the same mean 3, but the probability is spread out\ndifferently. In the plots below, we order the pmf's from largest to smallest variance: Z, X,\nY, W.\nz\np(z)\n.5\npmf for Z\nx\np(x)\n1/5\npmf for X\ny\np(y)\n.1\n.3\n.5\npmf for Y\nW\np(w)\npmf for W\nNext we'll verify our visual intuition by computing the variance of each of the variables.\nAll of them have mean μ= 3. Since the variance is defined as an expected value, we can\ncompute it using the tables.\n(i)\nvalue x\npmf p(x)\n1/5\n1/5\n1/5\n1/5\n1/5\n(X-μ)2\nVar(X) = E[(X-μ)2] = 4\n5 + 1\n5 + 0\n5 + 1\n5 + 4\n5 = 2 .\n(ii)\nvalue y\np(y)\n1/10\n2/10\n4/10\n2/10\n1/10\n(Y-μ)2\nVar(Y) = E[(Y-μ)2] =\n10 + 2\n10 + 0\n10 + 2\n10 + 4\n10 = 1.2 .\n(iii)\nvalue z\npmf p(z)\n5/10\n5/10\n(Z-μ)2\nVar(Z) = E[(Z-μ)2] = 20\n10 + 20\n10 = 4 .\n\n18.05 Class 5, Variance of Discrete Random Variables, Spring 2022\n(iv)\nvalue w\npmf p(w)\n(W-μ)2\nVar(W) = 0 . Note that Wdoesn't vary, so it has variance 0!\n3.1\nThe variance of a Bernoulli(p) random variable.\nBernoulli random variables are fundamental, so we should know their variance.\nIf X∼Bernoulli(p) then\nVar(X) = p(1 -p).\nProof: We know that E[X] = p. We compute Var(X) using a table.\nvalues X\npmf p(x)\n1 -p\np\n(X-μ)2\n(0 -p)2\n(1 -p)2\nVar(X) = (1 -p)p2 + p(1 -p)2 = (1 -p)p(1 -p+ p) = (1 -p)p.\nAs with all things Bernoulli, you should remember this formula.\nThink: For what value of pdoes Bernoulli(p) have the highest variance? Try to answer\nthis by plotting the PMF for various p.\n3.2\nA word about independence\nSo far we have been using the notion of independent random variable without ever carefully\ndefining it. For example, a binomial distribution is the sum of independent Bernoulli trials.\nThis may (should?) have bothered you. Of course, we have an intuitive sense of what inde-\npendence means for experimental trials. We also have the probabilistic sense that random\nvariables Xand Yare independent if knowing the value of Xgives you no information\nabout the value of Y.\nIn a few classes we will work with continuous random variables and joint probability func-\ntions. After that we will be ready for a full definition of independence. For now we can use\nthe following definition, which is exactly what you expect and is valid for discrete random\nvariables.\nDefinition: The discrete random variables Xand Yare independent if\nP(X= a, Y= b) = P(X= a)P(Y= b)\nfor any values a, b. That is, the probabilities multiply.\n3.3\nProperties of variance\nThe three most useful properties for computing variance are:\n1. If Xand Yare independent then Var(X+ Y) = Var(X) + Var(Y).\n\n18.05 Class 5, Variance of Discrete Random Variables, Spring 2022\n2. For constants aand b, Var(aX+ b) = a2Var(X).\n3. Var(X) = E[X2] -E[X]2.\nFor Property 1, note carefully the requirement that Xand Yare independent. We will\nreturn to the proof of Property 1 in a later class.\nProperty 3 gives a formula for Var(X) that is often easier to use in hand calculations. The\ncomputer is happy to use the definition! We'll prove Properties 2 and 3 after some examples.\nExample 3. Suppose Xand Yare independent and Var(X) = 3 and Var(Y) = 5. Find:\n(i) Var(X+ Y),\n(ii) Var(3X+ 4),\n(iii) Var(X+ X),\n(iv) Var(X+ 3Y).\nSolution: To compute these variances we make use of Properties 1 and 2.\n(i) Since Xand Yare independent, Var(X+ Y) = Var(X) + Var(Y) = 8.\n(ii) Using Property 2, Var(3X+ 4) = 9 ⋅Var(X) = 27.\n(iii) Don't be fooled! Property 1 fails since Xis certainly not independent of itself. We can\nuse Property 2: Var(X+ X) = Var(2X) = 4 ⋅Var(X) = 12. (Note: if we mistakenly used\nProperty 1, we would the wrong answer of 6.)\n(iv) We use both Properties 1 and 2.\nVar(X+ 3Y) = Var(X) + Var(3Y) = 3 + 9 ⋅5 = 48.\nExample 4. Use Property 3 to compute the variance of X∼Bernoulli(p).\nSolution: From the table\nX\np(x)\n1 -p\np\nX2\nwe have E[X2] = p. So Property 3 gives\nVar(X) = E[X2] -E[X]2 = p-p2 = p(1 -p).\nThis agrees with our earlier calculation.\nExample 5. Redo Example 1 using Property 3.\nSolution: From the table\nX\np(x)\n1/4\n1/4\n1/2\nX2\nwe have E[X] = 7/2 and\nE[X2] = 12 ⋅1\n4 + 32 ⋅1\n4 + 52 ⋅1\n2 = 60\n4 = 15.\nSo Var(X) = 15 -(7/2)2 = 11/4 -as before in Example 1.\n3.4\nVariance of binomial(n,p)\nSuppose X∼binomial(n, p). Since Xis the sum of independent Bernoulli(p) variables and\neach Bernoulli variable has variance p(1 -p) we have\nX∼binomial(n, p) ⇒Var(X) = np(1 -p).\n\n18.05 Class 5, Variance of Discrete Random Variables, Spring 2022\n3.5\nProof of properties 2 and 3\nProof of Property 2: This follows from the properties of E[X] and some algebra.\nLet μ= E[X]. Then E[aX+ b] = aμ+ band\nVar(aX+b) = E[(aX+b-(aμ+b))2] = E[(aX-aμ)2] = E[a2(X-μ)2] = a2E[(X-μ)2] = a2Var(X).\nProof of Property 3: We use the properties of E[X] and a bit of algebra. Remember\nthat μis a constant and that E[X] = μ.\nE[(X-μ)2] = E[X2 -2μX+ μ2]\n= E[X2] -2μE[X] + μ2\n= E[X2] -2μ2 + μ2\n= E[X2] -μ2\n= E[X2] -E[X]2.\nQED\nTables of Distributions and Properties\nDistribution\nrange X\npmf p(x)\nmean E[X]\nvariance Var(X)\nBernoulli(p)\n0, 1\np(0) = 1 -p,\np(1) = p\np\np(1 -p)\nBinomial(n, p)\n0, 1,..., n\np(k) = (n\nk)pk(1 -p)n-k\nnp\nnp(1 -p)\nUniform(n)\n1, 2, ..., n\np(k) = 1\nn\nn+ 1\nn2 -1\nGeometric(p)\n0, 1, 2,...\np(k) = p(1 -p)k\n1 -p\np\n1 -p\np2\nLet Xbe a discrete random variable with range x1, x2, ...and pmf p(xj).\nExpected Value:\nVariance:\nSynonyms:\nmean, average\nNotation:\nE[X], μ\nVar(X), σ2\nDefinition:\nE[X] = ∑\nj\np(xj)xj\nE[(X-μ)2] = ∑\nj\np(xj)(xj-μ)2\nScale and shift:\nE[aX+ b] = aE[X] + b\nVar(aX+ b) = a2Var(X)\nLinearity:\n(for any X, Y)\n(for X, Yindependent)\nE[X+ Y] = E[X] + E[Y]\nVar(X+ Y) = Var(X) + Var(Y)\nFunctions of X:\nE[h(X)] = ∑p(xj) h(xj)\nAlternative formula:\nVar(X) = E[X2] -E[X]2 = E[X2] -μ2\n\nContinuous Random Variables\nClass 5, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Know the definition of a continuous random variable.\n2. Know the definition of the probability density function (pdf) and cumulative distribution\nfunction (cdf).\n3. Be able to explain why we use probability density for continuous random variables.\nIntroduction\nWe now turn to continuous random variables. All random variables assign a number to\neach outcome in a sample space. Whereas discrete random variables take on a discrete set\nof possible values, continuous random variables have a continuous set of values.\nComputationally, to go from discrete to continuous we simply replace sums by integrals. It\nwill help you to keep in mind that (informally) an integral is just a continuous sum.\nExample 1. Since time is continuous, the amount of time Jon is early (or late) for class is\na continuous random variable. Let's go over this example in some detail.\nSuppose you measure how early Jon arrives to class each day (in units of minutes). That\nis, the outcome of one trial in our experiment is a time in minutes. We'll assume there are\nrandom fluctuations in the exact time he shows up. Since in principle Jon could arrive, say,\n3.43 minutes early, or 2.7 minutes late (corresponding to the outcome -2.7), or at any other\ntime, the sample space consists of all real numbers. So the random variable which gives the\noutcome itself has a continuous range of possible values.\nIt is too cumbersome to keep writing 'the random variable', so in future examples we might\nwrite: Let T= \"time in minutes that Jon is early for class on any given day.\"\nCalculus Warmup\nWhile we will assume you can compute the most familiar forms of derivatives and integrals\nby hand, we do not expect you to be calculus whizzes. For tricky expressions, we'll let the\ncomputer do most of the calculating. Conceptually, you should be comfortable with two\nviews of a definite integral.\n1. ∫\nb\na\nf(x) dx= area under the curve y= f(x).\n2. ∫\nb\na\nf(x) dx= 'sum of f(x) dx'.\n\n18.05 Class 5, Continuous Random Variables, Spring 2022\nThe connection between the two is:\narea ≈sum of rectangle areas\n=\nf(x1)Δx+ f(x2)Δx+ ... + f(xn)Δx=\nn\n∑\nf(xi)Δx.\nAs the width Δxof the intervals gets smaller the approximation becomes better.\nx\ny\na\nb\ny= f(x)\nx\ny\nx0 x1 x2\nxn\nΔx\n⋯\na\nb\ny= f(x)\nArea = f(xi)Δx\nArea is approximately the sum of rectangles\nNote: In calculus you learned to compute integrals by finding antiderivatives.\nThis is\nimportant for calculations, but don't confuse this method for the reason we use integrals.\nOur interest in integrals comes primarily from its interpretation as a 'sum' and to a much\nlesser extent its interpretation as area.\nContinuous Random Variables and Probability Density Func-\ntions\nA continuous random variable takes a range of values, which may be finite or infinite in\nextent. Here are a few examples of ranges: [0, 1], [0, inf), (-inf, inf), [a, b].\nDefinition: A random variable Xis continuous if there is a function f(x) such that for\nany c≤dwe have\nP(c≤X≤d) = ∫\nd\nc\nf(x) dx.\n(1)\nThe function f(x) is called the probability density function (pdf).\nThe pdf always satisfies the following properties:\n1. f(x) ≥0 (fis nonnegative).\n2. ∫\ninf\n-inf\nf(x) dx= 1\n(This is equivalent to: P(-inf< X< inf) = 1).\nThe probability density function f(x) of a continuous random variable is the analogue of\nthe probability mass function p(x) of a discrete random variable. Here are two important\ndifferences:\n1. Unlike p(x), the pdf f(x) is not a probability. You have to integrate it to get proba-\nbility. (See section 4.2 below.)\n2. Since f(x) is not a probability, there is no restriction that f(x) be less than or equal\nto 1.\n\n18.05 Class 5, Continuous Random Variables, Spring 2022\nNote: In Property 2, we integrated over (-inf, inf) since we did not know the range of values\ntaken by X. Formally, this makes sense because we just define f(x) to be 0 outside of the\nrange of X. In practice, we would integrate between bounds given by the range of X.\n4.1\nGraphical View of Probability\nIf you graph the probability density function of a continuous random variable Xthen\nP(c≤X≤d) = area under the graph between cand d.\nx\nf(x)\nc\nd\nP(c≤X≤d)\nThink: What is the total area under the pdf f(x)?\n4.2\nThe terms 'probability mass' and 'probability density'\nWhy do we use the terms mass and density to describe the pmf and pdf? What is the\ndifference between the two? The simple answer is that these terms are completely analogous\nto the mass and density you saw in physics and calculus. We'll review this first for the\nprobability mass function and then discuss the probability density function.\nMass as a sum:\nIf masses m1, m2, m3, and m4 are set in a row at positions x1, x2, x3, and x4, then the\ntotal mass is m1 + m2 + m3 + m4.\nx\nm1\nx1\nm2\nx2\nm3\nx3\nm4\nx4\nWe can define a 'mass function' p(x) with p(xj) = mjfor j= 1, 2, 3, 4, and p(x) = 0\notherwise. In this notation the total mass is p(x1) + p(x2) + p(x3) + p(x4).\nThe probability mass function behaves in exactly the same way, except it has the dimension\nof probability instead of mass.\nMass as an integral of density:\nSuppose you have a rod of length Lmeters with varying density f(x) kg/m. (Note the units\nare mass/length.)\nx\nx1\nx2\nx3\nxi\nΔx\nxn= L\nmass of ith piece ≈f(xi)Δx\n\n18.05 Class 5, Continuous Random Variables, Spring 2022\nIf the density varies continuously, we must find the total mass of the rod by integration:\ntotal mass = ∫\nL\nf(x) dx.\nThis formula comes from dividing the rod into small pieces and 'summing' up the mass of\neach piece. That is:\ntotal mass ≈\nn\n∑\ni=1\nf(xi) Δx\nIn the limit as Δxgoes to zero the sum becomes the integral.\nThe probability density function behaves exactly the same way, except it has units of\nprobability/(unit x) instead of kg/m.\nIndeed, equation (1) is exactly analogous to the\nabove integral for total mass.\nWhile we're on a physics kick, note that for both discrete and continuous random variables,\nthe expected value is simply the center of mass or balance point.\nExample 2. Suppose Xhas pdf f(x) = 3 on [0, 1/3] (this means f(x) = 0 outside of\n[0, 1/3]). Graph the pdf and compute P(0.1 ≤X≤0.2) and P(0.1 ≤X≤1).\nSolution: P(0.1 ≤X≤0.2) is shown below at left. We can compute the integral:\nP(0.1 ≤X≤0.2) = ∫\n0.2\n0.1\nf(x) dx= ∫\n0.2\n0.1\n3 dx= 0.3.\nOr we can find the area geometrically:\narea of rectangle = 3 ⋅0.1 = 0.3.\nP(0.1 ≤X≤1) is shown below at right. Since there is only area under f(x) up to 1/3, we\nhave P(0.1 ≤X≤1) = 3 ⋅(1/3 -0.1) = 0.7.\nx\nf(x)\n1/3\n.1\n.2\nP(0.1 ≤X≤0.2)\nx\nf(x)\n1/3\n.1\nP(0.1 ≤X≤1)\nThink: In the previous example f(x) takes values greater than 1.\nWhy does this not\nviolate the rule that probabilities are always between 0 and 1?\nNote on notation. We can define a random variable by giving its range and probability\ndensity function. For example we might say, let Xbe a random variable with range [0,1]\n\n18.05 Class 5, Continuous Random Variables, Spring 2022\nand pdf f(x) = x/2. Implicitly, this means that Xhas no probability density outside of the\ngiven range. If we wanted to be absolutely rigorous, we would say explicitly that f(x) = 0\noutside of [0,1], but in practice this won't be necessary.\nExample 3. Let Xbe a random variable with range [0,1] and pdf f(x) = Cx2. What is\nthe value of C?\nSolution: Since the total probability must be 1, we have\n∫\nf(x) dx= 1\n⇔\n∫\nCx2 dx= 1.\nBy evaluating the integral, the equation at right becomes\nC/3 = 1 ⇒\nC= 3 .\nNote: We say the constant Cabove is needed to normalize the density so that the total\nprobability is 1.\nExample 4. Let Xbe the random variable in the Example 3. Find P(X≤1/2).\nSolution: P(X≤1/2) = ∫\n1/2\n3x2 dx= x3∣\n1/2\n= 1\n8.\nThink: For this X(or any continuous random variable):\n- What is P(a≤X≤a)?\n- What is P(X= 0)?\n- Does P(X= a) = 0 mean that Xcan never equal a?\nIn words the above questions get at the fact that the probability that a random person's\nheight is exactly 5'9\" (to infinite precision, i.e. no rounding!) is 0. Yet it is still possible\nthat someone's height is exactly 5'9\". So the answers to the thinking questions are 0, 0, and\nNo.\n4.3\nCumulative Distribution Function\nThe cumulative distribution function (cdf) of a continuous random variable Xis defined in\nexactly the same way as the cdf of a discrete random variable.\nF(b) = P(X≤b).\nNote well that the definition is about probability. When using the cdf you should first think\nof it as a probability. Then when you go to calculate it you can use\nF(b) = P(X≤b) = ∫\nb\n-inf\nf(x) dx,\nwhere f(x) is the pdf of X.\nNotes:\n1. For discrete random variables, we defined the cumulative distribution function but did\n\n18.05 Class 5, Continuous Random Variables, Spring 2022\nnot have much occasion to use it. The cdf plays a far more prominent role for continuous\nrandom variables.\n2. As before, we started the integral at -infbecause we did not know the precise range of\nX. Formally, this still makes sense since f(x) = 0 outside the range of X. In practice, we'll\nknow the range and start the integral at the start of the range.\n3. In practice we often say 'Xhas distribution F(x)' rather than 'Xhas cumulative distri-\nbution function F(x).'\nExample 5. Find the cumulative distribution function for the density in Example 2.\nSolution: For ain [0,1/3] we have F(a) = ∫\na\nf(x) dx= ∫\na\n3 dx= 3a.\nSince f(x) is 0 outside of [0,1/3] we know F(a) = P(X≤a) = 0 for a< 0 and F(a) = 1\nfor a> 1/3. Putting this all together we have\nF(a) =\n⎧\n{\n⎨\n{\n⎩\nif a< 0\n3a\nif 0 ≤a≤1/3\nif 1/3 < a.\nHere are the graphs of f(x) and F(x).\nx\nf(x)\n1/3\nx\nF(x)\n1/3\nNote the different scales on the vertical axes. Remember that the vertical axis for the pdf\nrepresents probability density and that of the cdf represents probability.\nExample 6. Find the cdf for the pdf in Example 3, f(x) = 3x2 on [0, 1]. Suppose Xis a\nrandom variable with this distribution. Find P(X< 1/2).\nSolution: f(x) = 3x2 on [0,1] ⇒F(a) = ∫\na\n3x2 dx= a3 on [0,1]. Therefore,\nF(a) =\n⎧\n{\n⎨\n{\n⎩\nif a< 0\na3\nif 0 ≤a≤1\nif 1 < a\nThus, P(X< 1/2) = F(1/2) = 1/8. Here are the graphs of f(x) and F(x):\nx\nf(x)\nx\nF(x)\n\n18.05 Class 5, Continuous Random Variables, Spring 2022\n4.4\nProperties of cumulative distribution functions\nHere is a summarry of the most important properties of cumulative distribution functions\n(cdf)\n1. (Definition) F(x) = P(X≤x)\n2. 0 ≤F(x) ≤1\n3. F(x) is non-decreasing, i.e. if a≤bthen F(a) ≤F(b).\n4. lim\nx→infF(x) = 1\nand\nlim\nx→-infF(x) = 0\n5. P(a≤X≤b) = F(b) -F(a)\n6. F′(x) = f(x).\nProperties 2, 3, 4 are identical to those for discrete distributions. The graphs in the previous\nexamples illustrate them.\nProperty 5 can be seen algebraically:\n∫\nb\n-inf\nf(x) dx= ∫\na\n-inf\nf(x) dx+ ∫\nb\na\nf(x) dx\n⇔∫\nb\na\nf(x) dx= ∫\nb\n-inf\nf(x) dx-∫\na\n-inf\nf(x) dx\n⇔P(a≤X≤b) = F(b) -F(a).\nProperty 5 can also be seen geometrically. The orange region below represents F(b) and\nthe striped region represents F(a). Their difference is P(a≤X≤b).\nx\na\nb\nP(a≤X≤b)\nProperty 6 is the fundamental theorem of calculus.\n4.5\nProbability density as a dartboard\nWe find it helpful to think of sampling values from a continuous random variable as throw-\ning darts at a funny dartboard. Consider the region underneath the graph of a pdf as a\ndartboard. Divide the board into small equal size squares and suppose that when you throw\na dart you are equally likely to land in any of the squares. The probability the dart lands\nin a given region is the fraction of the total area under the curve taken up by the region.\nSince the total area equals 1, this fraction is just the area of the region. If Xrepresents\nthe x-coordinate of the dart, then the probability that the dart lands with x-coordinate\nbetween aand bis just\nP(a≤X≤b)\n=\narea under f(x) between aand b\n=\n∫\nb\na\nf(x) dx.\n\nGallery of Continuous Random Variables\nClass 5, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Be able to give examples of what uniform, exponential and normal distributions are used\nto model.\n2. Be able to give the range and pdf's of uniform, exponential and normal distributions.\nIntroduction\nHere we introduce a few fundamental continuous distributions. These will play important\nroles in the statistics part of the class. For each distribution, we give the range, the pdf,\nthe cdf, and a short description of situations that it models. These distributions all depend\non parameters, which we specify.\nAs you look through each distribution do not try to memorize all the details; you can always\nlook those up. Rather, focus on the shape of each distribution and what it models.\nAlthough it comes towards the end, we call your attention to the normal distribution. It is\neasily the most important distribution defined here.\n2.1\nParametrized distributions\nWhen we studied discrete random variables we learned, for example, about the Bernoulli(p)\ndistribution. The probability pused to define the distribution is called a parameter and\nBernoulli(p) is called a parametrized distribution. For example, tosses of fair coin follow a\nBernoulli distribution where the parameter p= 0.5. When we study statistics one of the\nkey questions will be to estimate the parameters of a distribution. For example, if I have\na coin that may or may not be fair then I know it follows a Bernoulli(p) distribution, but\nI don't know the value of the parameter p. I might run experiments and use the data to\nestimate the value of p.\nAs another example, the binomial distribution Binomial(n, p) depends on two parameters\nnand p.\nIn the following sections we will look at specific parametrized continuous distributions.\nThe applet https://mathlets.org/mathlets/probability-distributions/ allows you\nto visualize the pdf and cdf of these distributions and to dynamically change the parameters.\nUniform distribution\n1. Parameters:\na, b.\n2. Range: [a, b].\n\n18.05 Class 5, Gallery of Continuous Random Variables, Spring 2022\n3. Notation:\nuniform(a, b) or U(a, b).\n4. Probability density function (pdf):\nf(x) =\nb-afor a≤x≤b.\n5. Cumulative distribution function (cdf):\nF(x) = (x-a)/(b-a) for a≤x≤b.\n6. Models:\nSitutations where all outcomes in the range have equal probability (more\nprecisely all outcomes have the same probability density).\nGraphs:\nx\na\nb\nb-a\nf(x)\nx\na\nb\nF(x)\npdf and cdf for uniform(a,b) distribution.\nExample 1. 1. Suppose we have a tape measure with markings at each millimeter. If we\nmeasure (to the nearest marking) the length of items that are roughly a meter long, the\nrounding error will be uniformly distributed between -0.5 and 0.5 millimeters.\n2. Many board games use spinning arrows (spinners) to introduce randomness. When spun,\nthe arrow stops at an angle that is uniformly distributed between 0 and 2πradians.\n3. In most pseudo-random number generators, the basic generator simulates a uniform\ndistribution and all other distributions are constructed by transforming the basic generator.\nExponential distribution\n1. Parameter:\nλ.\n2. Range:\n[0, inf).\n3. Notation:\nexponential(λ) or exp(λ).\n4. Probability density function (pdf):\nf(x) = λe-λxfor 0 ≤x.\n5. Cumulative distribution function (cdf): (This is an easy integral.)\nF(x) = 1 -e-λxfor x≥0\n6. Right tail distribution: P(X> x) = 1 -F(x) = e-λx. (Note: this is defined as\nP(X> x), i.e. that Xis to the right of xon the number line.)\n7. Models:\nThe waiting time for a continuous process to change state.\nExample 2. If I step out to 77 Mass Ave after class and wait for the next taxi, my waiting\ntime in minutes is exponentially distributed. We will see that in this case λis given by\n1/(average number of taxis that pass per minute).\n\n18.05 Class 5, Gallery of Continuous Random Variables, Spring 2022\nExample 3. The exponential distribution models the waiting time until an unstable isotope\nundergoes nuclear decay. In this case, the value of λis related to the half-life of the isotope.\nMemorylessness: There are other distributions that also model waiting times, but the\nexponential distribution has the additional property that it is memoryless. Here's what this\nmeans in the context of Example 2: suppose that the probability that a taxi arrives within\nthe first five minutes is p. If I wait five minutes and, in this case, no taxi arrives, then the\nprobability that a taxi arrives within the next five minutes is still p. That is, my previous\nwait of 5 minutes has no impact on the length of my future wait!\nBy contrast, suppose I were to instead go to Kendall Square subway station and wait for\nthe next inbound train. Since the trains are coordinated to follow a schedule (e.g., roughly\n12 minutes between trains), if I wait five minutes without seeing a train then there is a far\ngreater probability that a train will arrive in the next five minutes. In particular, waiting\ntime for the subway is not memoryless, and a better model would be the uniform distribution\non the range [0,12].\nThe memorylessness of the exponential distribution is analogous to the memorylessness\nof the (discrete) geometric distribution, where having flipped 5 tails in a row gives no\ninformation about the next 5 flips. Indeed, the exponential distribution is precisely the\ncontinuous counterpart of the geometric distribution, which models the waiting time for a\ndiscrete process to change state. More formally, memoryless means that the probability of\nwaiting tmore minutes is independent of the amount of time already waited. In symbols,\nP(X> s+ t| X> s) = P(X> t).\nProof of memorylessness: We know that\n(X> s+ t) ∩(X> s) = (X> s+ t),\nsince the event 'waited at least sminutes' contains the event 'waited at least s+ tminutes'.\nTherefore the formula for conditional probability gives\nP(X> s+ t| X> s) = P(X> s+ t)\nP(X> s)\n= e-λ(s+t)\ne-λs\n= e-λt= P(X> t).\nThe probability P(X> s+ t) = e-λ(s+t) is the formula for the right tail probability given\nabove.\nGraphs:\n\n18.05 Class 5, Gallery of Continuous Random Variables, Spring 2022\nNormal distribution\nIn 1809, Carl Friedrich Gauss published a monograph introducing several notions that have\nbecome fundamental to statistics: the normal distribution, maximum likelihood estimation,\nand the method of least squares (we will cover all three in this course). For this reason,\nthe normal distribution is also called the Gaussian distribution, and it is by far the most\nimportant continuous distribution.\n1. Parameters:\nμ, σ.\n2. Range:\n(-inf, inf).\n3. Notation:\nNormal(μ, σ2) or N(μ, σ2).\n4. Probability density function (pdf):\nf(x) =\nσ\n√\n2π\ne-(x-μ)2/2σ2.\n5. Cumulative distribution function (cdf): F(x) has no formula, so use tables or software\nsuch as pnorm in R to compute F(x).\n6. Models:\nMeasurement error, intelligence/ability, height, averages of lots of data.\nThe standard normal distribution N(0, 1) has mean 0 and variance 1. We reserve Zfor a\nstandard normal random variable, φ(z) =\n√\n2π\ne-z2/2 for the standard normal density, and\nΦ(z) for the standard normal distribution.\nNote: we will define mean and variance for continuous random variables next time. They\nhave the same interpretations as in the discrete case.\nAs you might guess, the normal\ndistribution N(μ, σ2) has mean μ, variance σ2, and standard deviation σ.\nHere are some graphs of normal distributions. Note that they are shaped like a bell curve.\nNote also that as σincreases they become more spread out.\n\n18.05 Class 5, Gallery of Continuous Random Variables, Spring 2022\nThe bell curve: First we show the standard normal probability density and cumulative\ndistribution functions. Below that is a selection of normal densities. Notice that the graph\nis centered on the mean and the bigger the variance the more spread out the curve.\n-4\n-2\n0.1\n0.2\n0.3\n0.4\n0.5\nz\nφ(z)\nStandard normal pdf\n-4\n-2\n0.2\n0.4\n0.6\n0.8\n1.0\nz\nΦ(z)\nStandard normal cdf\n-4\n-2\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nN(0, 1)\nN(4.5, 0.5)\nN(4.5, 2.25)\nN(6.5, 1.0)\nN(8.0, 0.5)\nNotation note.\nIn the figure above we use our notation N(μ, σ2).\nSo, for example,\nN(8, 0.5) has variance 0.5 and standard deviation σ=\n√\n0.5 ≈0.7071.\n5.1\nNormal probabilities\nTo make approximations it is useful to remember the following rule of thumb for three\napproximate probabilities from the standard normal distribution:\nP(-1 ≤Z≤1) ≈0.68,\nP(-2 ≤Z≤2) ≈0.95,\nP(-3 ≤Z≤3) ≈0.99.\nThe figure below shows these probabilities as areas under the graph of the standard normal\npdf φ(z).\n\n18.05 Class 5, Gallery of Continuous Random Variables, Spring 2022\nz\n-σ\nσ\n-2σ\n2σ\n-3σ\n3σ\nNormal PDF\nwithin 1 ⋅σ≈68%\nwithin 2 ⋅σ≈95%\nwithin 3 ⋅σ≈99%\n68%\n95%\n99%\nSymmetry calculations\nWe can use the symmetry of the standard normal distribution about z= 0 to make some\ncalculations.\nExample 4. The rule of thumb says P(-1 ≤Z≤1) ≈0.68. Use this to estimate Φ(1).\nSolution: Φ(1) = P(Z≤1). In the figure, the two tails (in blue) have combined area\n1 -0.68 = 0.32. By symmetry the left tail has area 0.16 (half of 0.32), so P(Z≤1) ≈\n0.68 + 0.16 = 0.84.\nz\n-1\nP(-1 ≤Z≤1)\nP(Z≥1)\nP(Z≤-1)\n.34\n.34\n.16\n.16\n5.2\nUsing R to compute the standard normal cdf\n# Use the R function pnorm(x, μ, σ) to compute F(x) for N(μ, σ2)\npnorm(1,0,1)\n[1] 0.8413447\npnorm(0,0,1)\n[1] 0.5\npnorm(1,0,2)\n[1] 0.6914625\npnorm(1,0,1) - pnorm(-1,0,1)\n[1] 0.6826895\npnorm(5,0,5) - pnorm(-5,0,5)\n[1] 0.6826895\n# Of course z can be a vector of values\npnorm(c(-3,-2,-1,0,1,2,3),0,1)\n[1] 0.001349898 0.022750132 0.158655254 0.500000000 0.841344746 0.977249868 0.998650102\n\n18.05 Class 5, Gallery of Continuous Random Variables, Spring 2022\nNote:\nThe R function pnorm(x, μ, σ) uses σwhereas our notation for the normal distri-\nbution N(μ, σ2) uses σ2.\nHere's a table of values with fewer decimal points of accuracy\nz:\n-2\n-1\n0.3\n0.5\nΦ(z):\n0.0228\n0.1587\n0.5000\n0.6179\n0.6915\n0.8413\n0.9772\n0.9987\nExample 5. Use R to compute P(-1.5 ≤Z≤2).\nSolution: This is Φ(2) -Φ(-1.5) = pnorm(2,0,1) - pnorm(-1.5,0,1) = 0.91044\nPareto and other distributions\nIn 18.05, we only have time to work with a few of the many wonderful distributions that are\nused in probability and statistics. We hope that after this course you will feel comfortable\nlearning about new distributions and their properties when you need them. Wikipedia is\noften a great starting point.\nThe Pareto distribution is one common, beautiful distribution that we will not have time\nto cover in depth.\n1. Parameters:\nm> 0 and α> 0.\n2. Range:\n[m, inf).\n3. Notation:\nPareto(m, α).\n4. Density:\nf(x) = αmα\nxα+1 .\n5. Distribution: (easy integral)\nF(x) = 1 -mα\nxα, for x≥m\n6. Tail distribution: P(X> x) = mα/xα, for x≥m.\n7. Models:\nThe Pareto distribution models a power law, where the probability that\nan event occurs varies as a power of some attribute of the event. Many phenomena\nfollow a power law, such as the size of meteors, income levels across a population, and\npopulation levels across cities. See Wikipedia for loads of examples:\nhttps://en.wikipedia.org/wiki/Pareto_distribution#Applications\n\nManipulating Continuous Random Variables\nClass 5, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Be able to find the pdf and cdf of a random variable defined in terms of a random variable\nwith known pdf and cdf.\nTransformations of Random Variables\nWe frequently transform a known random variable into a new one by applying a formula.\nFor example we might look at Y= aX+ bor Y= X2. In this section we will see how to\nfind the probability density and cumulative distribution of Yfrom those of X.\nFor discrete random variables it was often possible do this by looking at probability tables.\nFor continuous random variables we will need to use systematic algebraic techniques. We\nwill see that transforming the pdf is just the change of variables ('u-substitution') from\ncalculus. To transform the cdf directly we will rely on its definition as a probability.\nLet's remind ourselves of the basics:\n- The cdf of Xis FX(x) = P(X≤x).\n- The pdf of Xis related to FXby fX(x) = F′\nX(x).\n2.1\nTransforming the cdf\nExample 1. Suppose Xhas range [0, 2] and cdf FX(x) = x2/4. What is the range, pdf\nand cdf of Y= X2?\nSolution: The range is easy: [0, 4].\nTo find the cdf we work systematically from the definition. For this example we will break\nit down into tiny steps, so you can see the thought process in detail.\nStep 1. Use definition:\nFY(y) = P(Y≤y).\nStep 2. Replace Yby its formula in X:\nP(Y≤y) = P(X2 ≤y).\nStep 3. Algebraically manipulate this to isolate the X:\nP(X2 ≤y) = P(X≤√y)\nStep 4. Notice that this is exactly the definition of FX:\nP(X≤√y) = FX(√y)\n\n18.05 Class 5, Manipulating Continuous Random Variables, Spring 2022\nStep 5. Use the known formula for FX:\nFX(√y) = (√y)2/4 = y/4.\nFollowing the chain from step 1 to step 5 we have the cdf:\nFY(y) = P(Y≤y) = P(X2 ≤y) = P(X≤√y) = FX(√y) = y/4.\nFinally, to find the pdf we can just differentiate the cdf:\nfY(y) = d\ndyFY(y) = 1\n4.\n2.2\nTransforming the pdf directly\nAn alternative way to find the pdf directly is by change of variables. We present this for\ncompleteness and for anyone who prefers it as a method. Our observation is that most\npeople find the cdf easier to transform.\nIn calculus you learned the 'u'-substitution. We'll do a calculus example to remind you how\nthis goes and then apply it to the pdf.\nExample 2. Calculus example. Convert the integral ∫(x2 + 1)7 dxinto an integral in\nu= x2 + 1.\nSolution: We have to convert each part of the integral from xto u:\n(x2 + 1)7 = u7\ndu= 2xdx,\ntherefore\ndx= du\n2x=\ndu\n√\nu-1\nNow we replacing each piece in the integral we get\n∫(x2 + 1)7 dx= ∫u7\ndu\n√\nu-1\n.\nExample 3. Find the pdf of Yin Example 1 directly using the method of 'u'-substitution.\n(In this case, 'u' will actually be 'y'.)\nSolution: The trick is to remember that probability is given by an integral ∫fX(x)dx.\nWe are given the change of variable y= x2, so we change the integral from one in xto one\nin y.\ny= x2 ⇒dy= 2xdx,\ntherefore\ndx= dy\n2√y.\nWe are given FX(x) = x2/4, so we can compute fX(x) = F′\nX(x) = x/2. Changing this to y\nwe have\nfX(x) = √y/2.\nPutting the two pieces together we have the transformation\nfX(x) dx=\n√y\ndy\n2√y= 1\n4 dy\n\n18.05 Class 5, Manipulating Continuous Random Variables, Spring 2022\nSince this is a probability, the factor in front of dyis the probability density. That is,\nfY(y) = 1/4, exactly as in Example 1.\nHere are a few more examples. We do them a little more quickly than the above examples.\nExample 4. Let X∼exp(λ), so fX(x) = λe-λxon [0, inf]. What is the probability density\nof Y= X2?\nSolution: We will do this using the change of variables for the pdf.\ny= x2 ⇒dy= 2xdx,\ntherefore\ndx=\ndy\n2√y\nfX(x) = λe-λx= λe-λ√y.\nCombining these we get,\nfX(x) dx= λe-λ√ydy\n2√y= fY(y) dy.\nSo we conclude that\nfY(y) =\nλ\n2√ye-λ√y.\nExample 5. Redo the previous example using the cdf.\nSolution: The cdf for the exponential random variable Xis FX(x) = 1 -e-λx. Therefore,\nfor Y= X2 we have\nFY(y) = P(Y≤y) = P(X2 ≤y) = P(X≤√y) = FX(√y) = 1 -e-λ√y.\nWe have found FY(y). If we wanted fY(y) we could take the derivative. We would get the\nsame answer as in the previous example.\nExample 6. Assume X∼N(5, 32) then Z= X-5\nis standard normal, i.e., Z∼N(0, 1).\nSolution: Again using the change of variables and the formula for fX(x) we have\nz= x-5\n⇒dz= dx\n3 ,\ntherefore\ndx= 3 dz\nFor this example we will transform fX(x) dxin one line instead of two.\nfX(x) dx=\n√\n2π\ne-(x-5)2/(2⋅32) dx=\n√\n2π\ne-z2/2 3 dz=\n√\n2π\ne-z2/2 dz= fZ(z) dz\nTherefore fZ(z) =\n√\n2π\ne-z2/2. Since this is exactly the density for N(0, 1) we have shown\nthat Zis standard normal.\nThis example shows an important general property of normal random variables which we\nstate as a theorem.\nTheorem. Standardization of normal random variables.\nAssume X∼N(μ, σ2). Show that Z= X-μ\nσ\nis standard normal, i.e., Z∼N(0, 1).\n\n18.05 Class 5, Manipulating Continuous Random Variables, Spring 2022\nProof. This is exactly the same computation as the previous example with μreplacing 5\nand σreplacing 3. We show the computation without comment.\nz= x-μ\nσ\n⇒dz= dx\nσ\n⇒dx= σdz\nfX(x) dx=\nσ\n√\n2π\ne-(x-μ)2/(2⋅σ2) dx=\nσ\n√\n2π\ne-z2/2 σdz=\n√\n2π\ne-z2/2 dz= fZ(z) dz\nTherefore fZ(z) =\n√\n2π\ne-z2/2. This shows Zis standard normal.\nWe call the change from Xto Zin this theorem standardization because it converts Xfrom\nan arbitrary normal random variable to a standard normal variable.\n\nExpectation, Variance and Standard Deviation for\nContinuous Random Variables\nClass 6, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Be able to compute and interpret expectation, variance, and standard deviation for\ncontinuous random variables.\n2. Be able to compute and interpret quantiles for discrete and continuous random variables.\nIntroduction\nSo far we have looked at expected value, standard deviation, and variance for discrete\nrandom variables. These summary statistics have the same meaning for continuous random\nvariables:\n- The expected value μ= E[X] is a measure of location or central tendency.\n- The standard deviation σis a measure of the spread or scale.\n- The variance σ2 = Var(X) is the square of the standard deviation.\nTo move from discrete to continuous, we will simply replace the sums in the formulas by\nintegrals. We will do this carefully and go through many examples in the following sections.\nIn the last section, we will introduce another type of summary statistic, quantiles. You may\nalready be familiar with the 0.5 quantile of a distribution, otherwise known as the median\nor 50th percentile.\nExpected value of a continuous random variable\nDefinition:\nLet Xbe a continuous random variable with range [a, b] and probability\ndensity function f(x). The expected value of Xis defined by\nE[X] = ∫\nb\na\nxf(x) dx.\nLet's see how this compares with the formula for a discrete random variable:\nE[X] =\nn\n∑\ni=1\nxip(xi).\nThe discrete formula says to take a weighted sum of the values xiof X, where the weights\nare the probabilities p(xi).\nRecall that f(x) is a probability density.\nIts units are\n\n18.05 Class 6, Expectation and Variance for Continuous Random Variables, Spring 2022 2\nprob/(unit of X). So f(x) dxrepresents the probability that Xis in an infinitesimal range\nof width dxaround x. Thus we can interpret the formula for E[X] as a weighted integral\nof the values xof X, where the weights are the probabilities f(x) dx.\nAs before, the expected value is also called the mean or average.\n3.1\nExamples\nLet's go through several example computations. Where the solution requires an integration\ntechnique, we push the computation of the integral to the appendix.\nExample 1. Let X∼uniform(0, 1). Find E[X].\nSolution: Xhas range [0, 1] and density f(x) = 1. Therefore,\nE[X] = ∫\nxdx= x2\n2 ∣\n= 1\n2 .\nNot surprisingly the mean is at the midpoint of the range.\nExample 2. Let Xhave range [0, 2] and density 3\n8x2. Find E[X].\nSolution:\nE[X] = ∫\nxf(x) dx= ∫\n8x3 dx= 3x4\n32 ∣\n= 3\n2 .\nDoes it make sense that this Xhas mean is in the right half of its range?\nSolution: Yes. Since the probability density increases as xincreases over the range, the\naverage value of xshould be in the right half of the range.\nx\nf(x)\nμ= 1.5\nμis \"pulled\" to the right of the midpoint 1 because there is more mass to the right.\nExample 3. Let X∼exp(λ). Find E[X].\nSolution: The range of Xis [0, inf) and its pdf is f(x) = λe-λx. So (details in appendix)\nE[X] = ∫\ninf\nxλe-λxdx= -xe-λx-e-λx\nλ∣\ninf\n=\nλ.\nx\nf(x) = λe-λx\nμ= 1/λ\nMean of an exponential random variable\n\n18.05 Class 6, Expectation and Variance for Continuous Random Variables, Spring 2022 3\nExample 4. Let Z∼N(0, 1). Find E[Z].\nSolution: The range of Zis (-inf, inf) and its pdf is φ(z) =\n√\n2π\ne-z2/2. So (details in\nappendix)\nE[Z] = ∫\ninf\n-inf\n√\n2π\nze-z2/2 dz= -\n√\n2π\ne-z2/2∣\ninf\n-inf\n= 0 .\nz\nφ(z)\nμ= 0\nThe standard normal distribution is symmetric and has mean 0.\n3.2\nProperties of E[X]\nThe properties of E[X] for continuous random variables are the same as for discrete ones:\n1. If Xand Yare random variables on a sample space Ω then\nE[X+ Y] = E[X] + E[Y].\n(linearity I)\n2. If aand bare constants then\nE[aX+ b] = aE[X] + b.\n(linearity II)\nExample 5. In this example we verify that for X∼N(μ, σ) we have E[X] = μ.\nSolution: Example (4) showed that for standard normal Z, E[Z] = 0. We could mimic\nthe calculation there to show that E[X] = μ. Instead we will use the linearity properties of\nE[X]. In the class 5 notes on manipulating random variables we showed that if X∼N(μ, σ2)\nis a normal random variable we can standardize it:\nZ= X-μ\nσ\n∼N(0, 1).\nInverting this formula we have X= σZ+ μ. The linearity of expected value now gives\nE[X] = E[σZ+ μ] = σE[Z] + μ= μ\n3.3\nExpectation of Functions of X\nThis works exactly the same as the discrete case. if h(x) is a function then Y= h(X) is a\nrandom variable and\nE[Y] = E[h(X)] = ∫\ninf\n-inf\nh(x)fX(x) dx.\nExample 6. Let X∼exp(λ). Find E[X2].\nSolution: Using integration by parts we have\nE[X2] = ∫\ninf\nx2λe-λxdx= [-x2e-λx-2x\nλe-λx-2\nλ2 e-λx]\ninf\n=\nλ2 .\n\n18.05 Class 6, Expectation and Variance for Continuous Random Variables, Spring 2022 4\nVariance\nNow that we've defined expectation for continuous random variables, the definition of vari-\nance is identical to that of discrete random variables.\nDefinition:\nLet Xbe a continuous random variable with mean μ. The variance of Xis\nVar(X) = E[(X-μ)2].\n4.1\nProperties of Variance\nThese are exactly the same as in the discrete case.\n1. If Xand Yare independent then Var(X+ Y) = Var(X) + Var(Y).\n2. For constants aand b, Var(aX+ b) = a2Var(X).\n3. Theorem: Var(X) = E[X2] -E[X]2 = E[X2] -μ2.\nFor Property 1, note carefully the requirement that Xand Yare independent.\nProperty 3 gives a formula for Var(X) that is often easier to use in hand calculations. The\nproofs of properties 2 and 3 are essentially identical to those in the discrete case. We will\nnot give them here.\nExample 7. Let X∼uniform(0, 1). Find Var(X) and σX.\nSolution: In Example 1 we found μ= 1/2. Next we compute\nVar(X) = E[(X-μ)2] = ∫\n(x-1/2)2 dx=\n12 .\nExample 8. Let X∼exp(λ). Find Var(X) and σX.\nSolution: In Examples 3 and 6 we computed\nE[X] = ∫\ninf\nxλe-λxdx= 1\nλ\nand\nE[X2] = ∫\ninf\nx2λe-λxdx= 2\nλ2 .\nSo by Property 3,\nVar(X) = E[X2] -E[X]2 = 2\nλ2 -1\nλ2 = 1\nλ2\nand\nσX= 1\nλ.\nWe could have skipped Property 3 and computed this directly from Var(X) = ∫\ninf\n0 (x-1/λ)2λe-λxdx.\nExample 9. Let Z∼N(0, 1). Show Var(Z) = 1.\nNote: The notation for normal variables is X∼N(μ, σ2). This is certainly suggestive, but\nas mathematicians we need to prove that E[X] = μand Var(X) = σ2. Above we showed\nE[X] = μ. This example shows that Var(Z) = 1, just as the notation suggests. In the next\nexample we'll show Var(X) = σ2.\nSolution: Since E[Z] = 0, we have\nVar(Z) = E[Z2] =\n√\n2π\n∫\ninf\n-inf\nz2e-z2/2 dz.\n\n18.05 Class 6, Expectation and Variance for Continuous Random Variables, Spring 2022 5\n(using integration by parts with u= z, v′ = ze-z2/2 ⇒u′ = 1, v= -e-z2/2)\n=\n√\n2π\n(-ze-z2/2∣\ninf\n-inf) +\n√\n2π\n∫\ninf\n-inf\ne-z2/2 dz.\nThe first term equals 0 because the exponential goes to zero much faster than zgrows at\nboth ±inf. The second term equals 1 because it is exactly the total probability integral of\nthe pdf φ(z) for N(0, 1). So Var(X) = 1.\nExample 10. Let X∼N(μ, σ2). Show Var(X) = σ2.\nSolution: This is an exercise in change of variables. Letting z= (x-μ)/σ, we have\nVar(X) = E[(X-μ)2] =\n√\n2πσ\n∫\ninf\n-inf\n(x-μ)2e-(x-μ)2/2σ2 dx\n=\nσ2\n√\n2π\n∫\ninf\n-inf\nz2e-z2/2 dz= σ2.\nThe integral in the last line is the same one we computed for Var(Z).\nQuantiles\nDefinition: The median of Xis the value xfor which P(X≤x) = 0.5, i.e. the value\nof xsuch that P(X≤x) = P(X≥x).\nIn other words, Xhas equal probability of\nbeing above or below the median, and each probability is therefore 1/2. In terms of the\ncdf F(x) = P(X≤x), we can equivalently define the median as the value xsatisfying\nF(x) = 0.5.\nThink: What is the median of Z?\nSolution: By symmetry, the median is 0.\nExample 11. Find the median of X∼exp(λ).\nSolution: The cdf of Xis F(x) = 1 -e-λx. So the median is the value of xfor which\nF(x) = 1 -e-λx= 0.5.. Solving for xwe find: x= (ln 2)/λ.\nThink: In this case the median does not equal the mean of μ= 1/λ. Based on the graph\nof the pdf of Xcan you argue why the median is to the left of the mean.\nDefinition: The pth quantile of Xis the value qpsuch that P(X≤qp) = p.\nNotes. 1. In this notation the median is q0.5.\n2. We will usually write this in terms of the cdf: F(qp) = p.\nWith respect to the pdf f(x), the quantile qpis the value such that there is an area of pto\nthe left of qpand an area of 1 -pto the right of qp. In the examples below, note how we\ncan represent the quantile graphically using either area of the pdf or height of the cdf.\nExample 12. Find the 0.6 quantile for X∼U(0, 1).\n\n18.05 Class 6, Expectation and Variance for Continuous Random Variables, Spring 2022 6\nSolution: The cdf for Xis F(x) = xon the range [0,1]. So q0.6 = 0.6.\nx\nf(x)\nq0.6 = 0.6\nleft tail area = prob = 0.6\nx\nF(x)\nq0.6 = 0.6\nF(q0.6) = 0.6\nq0.6: left tail area = 0.6 ⇔F(q0.6) = 0.6\nExample 13. Find the 0.6 quantile of the standard normal distribution.\nSolution: We don't have a formula for the cdf, so we use the R 'quantile function' qnorm.\nq0.6 = qnorm(0.6, 0, 1) = 0.25335\nz\nφ(z)\nq0.6 = 0.253\nleft tail area = prob. = 0.6\nz\nΦ(z)\nq0.6 = 0.253\nF(q0.6) = 0.6\nq0.6: left tail area = 0.6 ⇔F(q0.6) = 0.6\nQuantiles give a useful measure of location for a random variable. We will use them more\nin coming lectures.\n5.1\nPercentiles, deciles, quartiles\nFor convenience, quantiles are often described in terms of percentiles, deciles or quartiles.\nThe 60th percentile is the same as the 0.6 quantile. For example you are in the 60th percentile\nfor height if you are taller than 60 percent of the population, i.e. the probability that you\nare taller than a randomly chosen person is 60 percent.\nLikewise, deciles represent steps of 1/10. The third decile is the 0.3 quantile. Quartiles are\nin steps of 1/4. The third quartile is the 0.75 quantile and the 75th percentile.\nAppendix: Integral Computation Details\nFrom Example 3 Let X∼exp(λ). Find E[X].\nThe range of Xis [0, inf) and its pdf is f(x) = λe-λx. Therefore\nE[X] = ∫\ninf\nxf(x) dx= ∫\ninf\nλxe-λxdx\n\n18.05 Class 6, Expectation and Variance for Continuous Random Variables, Spring 2022 7\n(using integration by parts with u= x, v′ = λe-λx⇒u′ = 1, v= -e-λx)\n= -xe-λx∣\ninf\n0 + ∫\ninf\ne-λxdx\n= 0 -e-λx\nλ∣\ninf\n= 1\nλ.\nWe used the fact that xe-λxand e-λxgo to 0 as x→inf.\nFrom Example 4 Let Z∼N(0, 1). Find E[Z].\nThe range of Zis (-inf, inf) and its pdf is φ(z) =\n√\n2π\ne-z2/2. By symmetry the mean must\nbe 0. The only mathematically tricky part is to show that the integral converges, i.e. that\nthe mean exists at all (some random variable do not have means, but we will not encounter\nthis very often.) For completeness we include the argument, though this is not something\nwe will ask you to do. We first compute the integral from 0 to inf:\n∫\ninf\nzφ(z) dz=\n√\n2π\n∫\ninf\nze-z2/2 dz.\nThe u-substitution u= z2/2 gives du= zdz. So the integral becomes\n√\n2π\n∫\ninf\nze-z2/2 dz. =\n√\n2π\n∫\ninf\ne-udu= -e-u|\ninf\n= 1\nSimilarly, ∫\n-inf\nzφ(z) dz= -1. Adding the two pieces together gives E[Z] = 0.\nFrom Example 6 Let X∼exp(λ). Find E[X2].\nE[X2] = ∫\ninf\nx2f(x) dx= ∫\ninf\nλx2e-λxdx\n(using integration by parts with u= x2, v′ = λe-λx⇒u′ = 2x, v= -e-λx)\n= -x2e-λx∣\ninf\n0 + ∫\ninf\n2xe-λxdx\n(the first term is 0, for the second term use integration by parts: u= 2x, v′ = e-λx\n⇒\nu′ = 2, v= -e-λx\nλ)\n= -2xe-λx\nλ∣\ninf\n+ ∫\ninf\ne-λx\nλ\ndx\n= 0 -2e-λx\nλ2 ∣\ninf\n= 2\nλ2 .\n\nCentral Limit Theorem and the Law of Large Numbers\nClass 6, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Understand the statement of the law of large numbers.\n2. Understand the statement of the central limit theorem.\n3. Be able to use the central limit theorem to approximate probabilities of averages and\nsums of independent identically-distributed random variables.\nIntroduction\nWe all understand intuitively that the average of many measurements of the same unknown\nquantity tends to give a better estimate than a single measurement. Intuitively, this is\nbecause the random error of each measurement cancels out in the average. In these notes\nwe will make this intuition precise in two ways: the law of large numbers (LoLN) and the\ncentral limit theorem (CLT).\nBriefly, both the law of large numbers and central limit theorem are about many independent\nsamples from same distribution. The LoLN tells us two things:\n1. The average of many independent samples is (with high probability) close to the mean\nof the underlying distribution.\n2. The density histogram of many independent samples is (with high probability) close\nto the graph of the density of the underlying distribution.\nTo be absolutely correct mathematically we need to make these statements more precise,\nbut as stated they are a good way to think about the law of large numbers.\nThe central limit theorem says that the sum or average of many independent copies of a\nrandom variable is approximately a normal random variable. The CLT goes on to give\nprecise values for the mean and standard deviation of the normal variable.\nThese are both remarkable facts.\nPerhaps just as remarkable is the fact that often in\npractice ndoes not have to be all that large.\n2.1\nThere is more to experimentation than mathematics\nThe mathematics of the LoLN says that the average of a lot of independent samples from a\nrandom variable will almost certainly approach the mean of the variable. The mathematics\ncannot tell us if the tool or experiment is producing data worth averaging. For example,\nif the measuring device is defective or poorly calibrated then the average of many mea-\nsurements will be a highly accurate estimate of the wrong thing! This is an example of\n\n18.05 Class 6, Central Limit Theorem and the Law of Large Numbers, Spring 2022\nsystematic error or sampling bias, as opposed to the random error controlled by the law of\nlarge numbers.\nThe law of large numbers\nSuppose X1, X2, ..., Xnare independent random variables with the same underlying dis-\ntribution. In this case, we say that the Xiare independent and identically-distributed, or\ni.i.d. In particular, the Xiall have the same mean μand standard deviation σ.\nLet Xnbe the average of X1, ... , Xn:\nXn= X1 + X2 + ⋯+ Xn\nn\n= 1\nn\nn\n∑\ni=1\nXi.\nNote that Xnis itself a random variable.\nThe law of large numbers and central limit\ntheorem tell us about the value and distribution of Xn, respectively.\nLoLN: As ngrows, the probability that Xnis close to μgoes to 1.\nCLT: As ngrows, the distribution of Xnconverges to the normal distribution N(μ, σ2/n).\nBefore giving a more formal statement of the LoLN, let's unpack its meaning through a\nconcrete example (we'll return to the CLT later on).\nExample 1. Averages of Bernoulli random variables\nSuppose each Xiis an independent flip of a fair coin, so Xi∼Bernoulli(0.5) and μ= 0.5.\nThen Xnis the proportion of heads in nflips, and we expect that this proportion is close to\n0.5 for large n. Randomness being what it is, this is not guaranteed; for example we could\nget 1000 heads in 1000 flips, though the probability of this occurring is very small.\nSo our intuition translates to: with high probability the sample average Xnis close to the\nmean 0.5 for large n. We'll demonstrate by doing some calculations in R. You can find the\ncode used for 'class 6 prep' in the usual place on our website.\nTo start we'll look at the probability of being within 0.1 of the mean. We can express this\nprobability as\nP(|Xn-0.5| < 0.1)\nor equivalently\nP(0.4 ≤Xn≤0.6)\nThe law of large numbers says that this probability goes to 1 as the number of flips ngets\nlarge. Our R code produces the following values for P(0.4 ≤Xn≤0.6).\nn= 10:\npbinom(6, 10, 0.5) - pbinom(3, 10, 0.5)\n= 0.65625\nn= 50:\npbinom(30, 50, 0.5) - pbinom(19, 50, 0.5)\n= 0.8810795\nn= 100:\npbinom(60, 100, 0.5) - pbinom(39, 100, 0.5)\n= 0.9647998\nn= 500:\npbinom(300, 500, 0.5) - pbinom(199, 500, 0.5)\n= 0.9999941\nn= 1000:\npbinom(600, 1000, 0.5) - pbinom(399, 1000, 0.5)\n= 1\nAs predicted by the LoLN the probability goes to 1 as ngrows.\nWe redo the computations to see the probability of being within 0.01 of the mean. Our R\ncode produces the following values for P(0.49 ≤Xn≤0.51).\n\n18.05 Class 6, Central Limit Theorem and the Law of Large Numbers, Spring 2022\nn= 10:\npbinom(5, 10, 0.5) - pbinom(4, 10, 0.5)\n= 0.2460937\nn= 100:\npbinom(51, 100, 0.5) - pbinom(48, 100, 0.5)\n= 0.2356466\nn= 1000:\npbinom(510, 1000, 0.5) - pbinom(489, 1000, 0.5)\n= 0.49334\nn= 10000:\npbinom(5100, 10000, 0.5) - pbinom(4899, 10000, 0.5)\n= 0.9555742\nAgain we see the probability of being close to the mean going to 1 as ngrows. Since 0.01\nis smaller than 0.1 it takes larger values of nto raise the probability to near 1.\nThis convergence of the probability to 1 is the LoLN in action! Whenever you're confused,\nit will help you to keep this example in mind. So we see that the LoLN says that with high\nprobability the average of a large number of independent trials from the same distribution\nwill be very close to the underlying mean of the distribution. Now we're ready for the\nformal statement.\n3.1\nFormal statement of the law of large numbers\nTheorem (Law of Large Numbers): Suppose X1, X2, ..., Xn, ... are i.i.d. random variables\nwith mean μ. For each n, let Xnbe the average of the first nvariables. Then for any a> 0,\nwe have\nlim\nn→infP(|Xn-μ| < a) = 1.\nThis says precisely that as nincreases the probability of being within aof the mean goes\nto 1. Think of aas a small tolerance of error from the true mean μ.\nLooking back at Example 1, we see that for tosses of a fair coin: If we choose the number\nof tosses n= 500, then with probability p= 0.99999, the experimental frequency of heads\nXnwill be within a= 0.1 of 0.5. In words, this tells us that, on average, only 1 in 100,000\nexperiments will produce an experimental frequency outside this range. If we decrease the\ntolerance aand/or increase the probability p, then nwill need to be larger.\nHistograms\nWe can summarize multiple samples x1, ... , xnof a random variable in a histogram. Here\nwe want to carefully construct histograms so that they resemble the area under the pdf.\nWe will then see how the LoLN applies to histograms.\nThe step-by-step instructions for constructing a density or frequency histogram are as fol-\nlows.\n1. Pick an interval of the real line and divide it into mintervals, with endpoints b0, b1, ...,\nbm. Usually these are equally sized, so let's assume this to start.\nx\nb0\nb1\nb2\nb3\nb4\nb5\nb6\nSix equally-sized bins\nEach of the intervals is called a bin. For example, in the figure above the first bin is\n[b0, b1] and the last bin is [b5, b6]. Each bin has a bin width, e.g. b1 -b0 is the first bin\nwidth. Usually the bins all have the same width, called the bin width of the histogram.\n\n18.05 Class 6, Central Limit Theorem and the Law of Large Numbers, Spring 2022\n2. Place each xiinto the bin that contains its value. If xilies on the boundary of two bins,\nwe'll put it in the left bin (this is the R default, though it can be changed).\n3. To draw a frequency histogram: put a vertical bar above each bin. The height of the\nbar should equal the number of xiin the bin.\n4. To draw a density histogram: put a vertical bar above each bin. The area of the bar\nshould equal the fraction of all data points that lie in the bin.\nNotes:\n1. When all the bins have the same width, the frequency histogram bars have area propor-\ntional to the count. So the density histogram results from simply by dividing the height of\neach bar by the total area of the frequency histogram. Ignoring the vertical scale, the two\nhistograms look identical.\n2. Caution: if the bin widths differ, the frequency and density histograms may look very\ndifferent. There is an example of this below. Don't let anyone fool you by manipulating\nbin widths to produce a histogram that suits their mischievous purposes!\nIn 18.05, we'll stick with equally-sized bins. In general, we prefer the density histogram\nsince its vertical scale is the same as that of the pdf.\nExamples. Here are some examples of histograms, all with the data [0.5,1,1,1.5,1.5,1.5,2,2,2,2].\nThe R code that drew them is in the file 'class6-prep-b.r'. You can find it in the usual place\non our website.\n1. Here the density and frequency plots look the same but have different vertical scales.\nHistogram of x\nx\nDensity\n0.5\n1.0\n1.5\n2.0\n0.0\n0.2\n0.4\n0.6\n0.8\nHistogram of x\nx\nFrequency\n0.5\n1.0\n1.5\n2.0\nBins centered at 0.5, 1, 1.5, 2, i.e. width 0.5, bounds at 0.25, 0.75, 1.25, 1.75, 2.25.\n2. Note the values are all on the bin boundaries and are put into the left-hand bin. That\nis, the bins are right-closed, e.g the first bin is for values in the right-closed interval (0, 0.5].\n\n18.05 Class 6, Central Limit Theorem and the Law of Large Numbers, Spring 2022\nHistogram of x\nx\nDensity\n0.0\n0.5\n1.0\n1.5\n2.0\n0.0\n0.2\n0.4\n0.6\n0.8\nHistogram of x\nx\nFrequency\n0.0\n0.5\n1.0\n1.5\n2.0\nBin bounds at 0, 0.5, 1, 1.5, 2.\n3. Here we show density histograms based on different bin widths. Note that the scale\nkeeps the total area equal to 1. The gaps are bins with zero counts.\nHistogram of x\nx\nDensity\n0.0\n0.5\n1.0\n1.5\n2.0\n0.0\n0.2\n0.4\n0.6\nHistogram of x\nx\nDensity\n0.5\n1.0\n1.5\n2.0\n0.0\n0.5\n1.0\n1.5\nLeft: wide bins;\nRight: narrow bins.\n4. Here we use unqual bin widths, so the density and frequency histograms look different\nHistogram of x\nx\nDensity\n0.0\n0.5\n1.0\n1.5\n2.0\n0.0\n0.2\n0.4\n0.6\n0.8\nHistogram of x\nx\nFrequency\n0.0\n0.5\n1.0\n1.5\n2.0\nDon't be fooled! These are based on the same data.\nThe density histogram is the better choice with unequal bin widths. In fact, R will complain\n\n18.05 Class 6, Central Limit Theorem and the Law of Large Numbers, Spring 2022\nif you try to make a frequency histogram with unequal bin widths. Compare the frequency\nhistogram with unequal bin widths with all the other histograms we drew for this data. It\nclearly looks different. What happened is that by combining the data in bins (0.5, 1] and\n(1, 1.5] into one bin (0.5, 1.5) we effectively made the height of both smaller bins greater.\nThe reason the density histogram is nice is discussed in the next section.\n4.1\nThe law of large numbers and histograms\nThe law of large number has an important consequence for density histograms.\nLoLN for histograms: With high probability the density histogram of a large number\nof samples from a distribution is a good approximation of the graph of the underlying pdf\nf(x) over the range of the histogram.\nLet's illustrate this by generating a density histogram with bin width 0.1 from 100000 draws\nfrom a standard normal distribution. As you can see, the density histogram very closely\ntracks the graph of the standard normal pdf φ(z).\n-4\n-2\n0.1\n0.2\n0.3\n0.4\n0.5\nDensity histogram of 10000 draws from a standard normal distribution, with φ(z) in blue.\nThe Central Limit Theorem\nWe now prepare for the statement of the CLT.\n5.1\nStandardization\nGiven a random variable Xwith mean μand standard deviation σ, we define its standard-\nization of Xas the new random variable\nZ= X-μ\nσ\n.\n\n18.05 Class 6, Central Limit Theorem and the Law of Large Numbers, Spring 2022\nNote that Zhas mean 0 and standard deviation 1.\nNote also that if Xhas a normal\ndistribution, then the standardization of Xis the standard normal distribution Zwith\nmean 0 and variance 1. This explains the term 'standardization' and the notation of Z\nabove.\n5.2\nStatement of the Central Limit Theorem\nSuppose X1, X2, ..., Xn, ...are i.i.d. random variables each having mean μand standard\ndeviation σ. For each n, let Sndenote the sum and let Xnbe the average of X1, ... , Xn.\nSn= X1 + X2 + ... + Xn=\nn\n∑\ni=1\nXi\nXn= X1 + X2 + ... + Xn\nn\n= Sn\nn.\nThe properties of mean and variance show\nE[Sn]\n= nμ,\nVar(Sn)\n= nσ2,\nσSn\n= √nσ\nE[Xn]\n= μ,\nVar(Xn)\n= σ2\nn,\nσXn\n=\nσ\n√n.\nSince they are multiples of each other, Snand Xnhave the same standardization\nZn= Sn-nμ\nσ√n\n= Xn-μ\nσ/√n\nCentral Limit Theorem: For large n,\nXn≈N(μ, σ2/n),\nSn≈N(nμ, nσ2),\nZn≈N(0, 1).\nNotes: 1. In words: Xnis approximately a normal distribution with the same mean as X\nbut a smaller variance.\n2. Snis approximately normal.\n3. Standardized Xnand Snare approximately standard normal.\nThe central limit theorem allows us to approximate a sum or average of i.i.d random vari-\nables by a normal random variable. This is extremely useful because it is usually easy to\ndo computations with the normal distribution.\nA precise statement of the CLT is that the cdf's of Znconverge to Φ(z):\nlim\nn→infFZn(z) = Φ(z).\nThe proof of the Central Limit Theorem is more technical than we want to get in 18.05. It\nis accessible to anyone with a decent calculus background.\n\n18.05 Class 6, Central Limit Theorem and the Law of Large Numbers, Spring 2022\n5.3\nStandard Normal Probabilities\nTo apply the CLT, we will want to have some normal probabilities at our fingertips. The\nfollowing probabilities appeared in Class 5. Let Z∼N(0, 1), a standard normal random\nvariable. Then with rounding we have:\n1. P(|Z| < 1) ≈0.68\n2. P(|Z| < 2) ≈0.95; more precisely P(|Z| < 1.96) ≈0.95.\n3. P(|Z| < 3) ≈0.997\nThese numbers are easily computed in R using pnorm. However, they are well worth re-\nmembering as rules of thumb. You should think of them as:\n1. The probability that a normal random variable is within 1 standard deviation of its\nmean is 0.68.\n2. The probability that a normal random variable is within 2 standard deviations of its\nmean is 0.95.\n3. The probability that a normal random variable is within 3 standard deviations of its\nmean is 0.997.\nThis is shown graphically in the following figure.\nz\nμ-σ\nμ+ σ\nμ-2σ\nμ+ 2σ\nμ-3σ\nμ+ 3σ\nNormal PDF\nwithin 1 ⋅σ≈68%\nwithin 2 ⋅σ≈95%\nwithin 3 ⋅σ≈99%\n68%\n95%\n99%\nμ\nClaim: From these numbers we can derive:\n1. P(Z< 1) ≈0.84\n2. P(Z< 2) ≈0.977\n3. P(Z< 3) ≈0.999\nProof: We know P(|Z| < 1) = 0.68. The remaining probability of 0.32 is in the two regions\nZ> 1 and Z< -1. These regions are referred to as the right-hand tail and the left-hand\ntail respectively. By symmetry each tail has area 0.16. Thus,\nP(Z< 1) = P(|Z| < 1) + P(left-hand tail) = 0.84\nThe other two cases are handled similarly.\n5.4\nApplications of the CLT\nExample 2. Flip a fair coin 100 times. Estimate the probability of more than 55 heads.\n\n18.05 Class 6, Central Limit Theorem and the Law of Large Numbers, Spring 2022\nSolution: Let Xjbe the result of the jth flip, so Xj= 1 for heads and Xj= 0 for tails.\nThe total number of heads is\nS= X1 + X2 + ... + X100.\nWe know E[Xj] = 0.5 and Var(Xj) = 1/4. Since n= 100, we have\nE[S] = 50,\nVar(S) = 25\nand\nσS= 5.\nThe central limit theorem says that the standardization of Sis approximately N(0, 1). The\nquestion asks for P(S> 55). Standardizing and using the CLT we get\nP(S> 55) = P(S-50\n> 55 -50\n) ≈P(Z> 1) = 0.16.\nHere Zis a standard normal random variable and P(Z> 1) = 1 -P(Z< 1) ≈0.16.\nExample 3. Estimate the probability of more than 220 heads in 400 flips.\nSolution: This is nearly identical to the previous example. Now μS= 200 and σS= 10\nand we want P(S> 220). Standardizing and using the CLT we get:\nP(S> 220) = P(S-μS\nσS\n> 220 -200\n) ≈P(Z> 2) = 0.025.\nAgain, Z∼N(0, 1) and the rules of thumb show P(Z> 2) = 0.025.\nNote: Even though 55/100 = 220/400, the probability of more than 55 heads in 100 flips\nis larger than the probability of more than 220 heads in 400 flips. This is due to the LoLN\nand the larger value of nin the latter case.\nExample 4. Estimate the probability of between 40 and 60 heads in 100 flips.\nSolution: As in the first example, E[S] = 50,\nVar(S) = 25 and σS= 5. So\nP(40 ≤S≤60) = P(40 -50\n≤S-50\n≤60 -50\n) ≈P(-2 ≤Z≤2)\nWe can compute the right-hand side using our rule of thumb. For a more accurate answer\nwe use R:\npnorm(2) - pnorm(-2) = 0.954 ...\nRecall that in Section 3 we used the binomial distribution to compute an answer of 0.965....\nSo our approximate answer using CLT is off by about 1%.\nThink: Would you expect the CLT method to give a better or worse approximation of\nP(200 < S< 300) with n= 500?\nWe encourage you to check your answer using R.\nExample 5.\nPolling.\nWhen taking a political poll the results are often reported as a\nnumber with a margin of error. For example 52% ± 3% favor candidate A. The rule of\nthumb is that if you poll npeople then the margin of error is ±1/√n. We will now see\nexactly what this means and that it is an application of the central limit theorem.\n\n18.05 Class 6, Central Limit Theorem and the Law of Large Numbers, Spring 2022\nSuppose there are 2 candidates A and B. Suppose further that the fraction of the population\nwho prefer A is p0. That is, if you ask a random person who they prefer then the probability\nthey'll answer A is po\nTo run the poll a pollster selects npeople at random and asks 'Do you support candidate\nA or candidate B?' Thus we can view the poll as a sequence of nindependent Bernoulli(p0)\ntrials, X1, X2, ..., Xn, where Xiis 1 if the ith person prefers A and 0 if they prefer B. The\nfraction of people polled that prefer A is just the average X.\nWe know that each Xi∼Bernoulli(p0) so,\nE[Xi] = p0\nand σXi= √p0(1 -p0).\nTherefore, the central limit theorem tells us that\nX≈N(p0, σ2/n),\nwhere σ= √p0(1 -p0).\nIn a normal distribution 95% of the probability is within 2 standard deviations of the mean.\nThis means that in 95% of polls of npeople the sample mean Xwill be within 2σ/√nof\nthe true mean p0. The final step is to note that for any value of p0 we have σ≤1/2. (It is\nan easy calculus exercise to see that 1/4 is the maximum value of σ2 = p0(1 -p0).) This\nmeans that we can conservatively say that in 95% of polls of npeople the sample mean\nXis within 1/√nof the true mean. The frequentist statistician then takes the interval\nX± 1/√nand calls it the 95% confidence interval for p0.\nA word of caution: it is tempting and common, but wrong, to think that there is a 95%\nprobability the true fraction p0 is in a particular confidence interval. This is subtle, but\nthe error is the same one as thinking you have a disease if a test with a 95% true positive\nrate comes back positive. We will go into this in much more detail when we learn about\nconfidence intervals.\n5.5\nWhy use the CLT\nSince the probabilities in the above examples can be computed exactly using the binomial\ndistribution, you may be wondering what is the point of finding an approximate answer\nusing the CLT. In fact, we were only able to compute these probabilities exactly because\nthe Xiwere Bernoulli and so the sum Swas binomial. In general, the distribution of the\nXimay not be familiar, or may not even be known, so you will not be able to compute the\nprobabilities for Sexactly. It can also happen that the exact computation is possible in\ntheory but too computationally intensive in practice, even for a computer. The power of\nthe CLT is that it applies whenever Xihas a mean and a variance. Though the CLT applies\nto many distributions, we will see in the next section that some distributions require larger\nnfor the approximation to be a good one.\n5.6\nHow big does nhave to be to apply the CLT?\nShort answer: often, not that big.\nThe following sequences of pictures show the convergence of averages to a normal distribu-\ntion.\n\n18.05 Class 6, Central Limit Theorem and the Law of Large Numbers, Spring 2022\nFirst we show the standardized average of ni.i.d. uniform random variables with n=\n1, 2, 4, 8, 12. The pdf of the average is in blue and the standard normal pdf is in red. By\nthe time n= 12 the fit between the standardized average and the true normal looks very\ngood.\n-3\n-2\n-1\n0.0\n0.2\n0.4\nx\nUniform: n = 1\n-3\n-2\n-1\n0.0\n0.2\n0.4\nx\nUniform: n = 2\n-3\n-2\n-1\n0.0\n0.2\n0.4\nx\nUniform: n = 4\n-3\n-2\n-1\n0.0\n0.2\n0.4\nx\nUniform: n = 8\n-3\n-2\n-1\n0.0\n0.2\n0.4\nx\nUniform: n = 12\nNext we show the standardized average of ni.i.d. exponential random variables with\nn= 1, 2, 4, 8, 16, 64. Notice that this asymmetric density takes more terms to converge to\nthe normal density.\n-3\n-2\n-1\n0.0\n0.4\n0.8\nx\nExponential: n = 1\n-3\n-2\n-1\n0.0\n0.2\n0.4\nx\nExponential: n = 2\n-3\n-2\n-1\n0.0\n0.2\n0.4\nx\nExponential: n = 4\n-3\n-2\n-1\n0.0\n0.2\n0.4\nx\nExponential: n = 8\n-3\n-2\n-1\n0.0\n0.2\n0.4\nx\nExponential: n = 16\n-3\n-2\n-1\n0.0\n0.2\n0.4\nx\nExponential: n = 64\nNext we show the (non-standardized) average of nexponential random variables with n=\n1, 2, 4, 16, 64. Notice how this standard deviation shrinks as ngrows, resulting in a spikier\n(more peaked) density.\n\n18.05 Class 6, Central Limit Theorem and the Law of Large Numbers, Spring 2022\n-2\n-1\n0.0\n0.4\n0.8\nx\nExponential: n = 1\n-2\n-1\n0.0\n0.4\nx\nExponential: n = 2\n-2\n-1\n0.0\n0.4\n0.8\nx\nExponential: n = 4\n-2\n-1\n0.0\n0.5\n1.0\n1.5\nx\nExponential: n = 16\n-2\n-1\n0.0\n1.0\n2.0\n3.0\nx\nExponential: n = 64\nThe central limit theorem works for discrete variables also. Here is the standardized average\nof ni.i.d. Bernoulli(0.5) random variables with n= 1, 2, 12, 64. Notice that as ngrows, the\naverage can take more values, which allows the discrete distribution to 'fill in' the normal\ndensity.\n-3\n-2\n-1\n0.0\n0.2\n0.4\nx\nBernoulli: n = 1\n-3\n-2\n-1\n0.0\n0.2\n0.4\nx\nBernoulli: n = 2\n-3\n-2\n-1\n0.0\n0.2\n0.4\nx\nBernoulli: n = 12\n-3\n-2\n-1\n0.0\n0.2\n0.4\nx\nBernoulli: n = 64\nNote. In order to put the binomial (sum of Bernoulli) and normal distribution on the same\naxes, we had to convert the binomial probability mass function to a density. We did this\nby making it a bar graph with bars centered on each value and with bar width equal to the\ndistance between values. Then the height of each bar is chosen so that the area equals the\nprobability of the corresponding value.\nFinally we show the (non-standardized) average of nBernoulli(0.5) random variables, with\nn= 4, 12, 64. Notice how the standard deviation gets smaller resulting in a spikier (more\npeaked) density. (In these figures, rather than plotting colored bars, we made the bars\nwhite and only plotted a blue line at the center of each bar.\n\n18.05 Class 6, Central Limit Theorem and the Law of Large Numbers, Spring 2022\n-1.0\n0.0\n0.5\n1.0\n1.5\n2.0\n0.0\n0.4\n0.8\nx\nBernoulli: n = 2\n0.0\n0.5\n1.0\n0.0\n1.0\n2.0\nx\nBernoulli: n = 12\n0.0\n0.5\n1.0\nx\nBernoulli: n = 64\n\nAppendix\nClass 6, 18.05\nJeremy Orloff and Jonathan Bloom\nIntroduction\nIn this appendix we give more formal mathematical material that is not strictly a part of\n18.05. This will not be on homework or tests. We give this material to emphasize that in\ndoing mathematics we should be careful to specify our hypotheses completely and give clear\ndeductive arguments to prove our claims. We hope you find it interesting and illuminating.\nWith high probability the density histogram resembles the\ngraph of the probability density function:\nWe stated that one consequence of the law of large numbers is that as the number of samples\nincreases the density histogram of the samples has an increasing probability of matching the\ngraph of the underlying pdf or pmf. This is a good rule of thumb, but it is rather imprecise.\nIt is possible to make more precise statements. It will take some care to make a sensible\nand precise statement, which will not be quite so sweeping.\nSuppose we have an experiment that produces data according to the random variable X\nand suppose we generate nindependent samples from X. Call them\nx1, x2, ... , xn.\nBy a bin we mean a range of values, i.e.\n(b1, b2].\nThe data point xkis in this bin if\nb1 < xk≤b2. (For the left-most bin, we would use an interval closed on both sides.) To\nmake a density histogram of the data we divide the range of Xinto mbins and calculate\nthe fraction of the data in each bin.\nNow, let pkbe the probability a random data point is in the kth bin. This is this probability\nfor an indicator (Bernoulli) random variable Bk,jwhich is 1 if the jth data point is in the\nbin and and 0 otherwise.\nStatement 1. Let\npkbe the fraction of the data in bin k. As the number nof data points\ngets large the probability that\npkis close to pkapproaches 1. Said differently, given any\nsmall number, call it athe probability P(| pk-pk| < a) depends on n, and as ngoes to\ninfinity this probability goes to 1.\nProof. Let\nBkbe the average of Bk,j. Since E[Bk,j] = pk, the law of large number says\nexactly that\nP(| Bk-pk| < a)\napproaches 1 as ngoes to infinity.\nBut, since the Bk,jare indicator variables, their average is exactly\npk, the fraction of the\ndata in bin k. Replacing\nBkby\npkin the above equation gives\nP(| pk-pk| < a)\napproaches 1 as ngoes to infinity.\nThis is exactly what Statement 1 claimed.\n\n18.05 Class 6, Appendix, Spring 2022\nStatement 2. The same statement holds for a finite number of bins simultaneously. That\nis, for bins 1 to mwe have\nP( (| B1-p1| < a), (| B2-p2| < a), ... , (| Bm-pm| < a) )\napproaches 1 as ngoes to infinity.\nProof. First we note the following probability rule, which is a consequence of the inclusion\nexclusion principle: If two events Aand Bhave P(A) = 1 -α1 and P(B) = 1 -α2 then\nP(A∩B) ≥1 -(α1 + α2).\nNow, Statement 1 says that for any αwe can find nlarge enough that P(| Bk-pk| < a) >\n1 -α/mfor each bin separately. By the probability rule, the probability of the intersection\nof all these events is at least 1 -α. Since we can let αbe as small as we want by letting n\ngo to infinity, in the limit we get probability 1 as claimed.\nStatement 3. If f(x) is a continuous probability density with range [a, b] then by taking\nenough data and having a small enough bin width we can insure that with high probability\nthe density histogram is as close as we want to the graph of f(x).\nProof. We will only sketch the argument. Assume the bin around xhas width is Δx. If\nΔxis small enough then the probability a data point is in the bin is approximately f(x)Δx.\nStatement 2 guarantees that if nis large enough then with high probability the fraction\nof data in the bin is also approximately f(x)Δx. Since this is the area of the bin we see\nthat its height will be approximately f(x). That is, with high probability the height of the\nhistogram over any point xis close to f(x). This is what Statement 3 claimed.\nNote. If the range is infinite or the density goes to infinity at some point we need to be\nmore careful. There are statements we could make for these cases.\nThe Chebyshev inequality\nOne proof of the LoLN follows from the following key inequality.\nThe Chebyshev inequality. Suppose Yis a random variable with mean μand variance σ2.\nThen for any positive value a, we have\nP(|Y-μ| ≥a) ≤Var(Y)\na2\n.\nIn words, the Chebyshev inequality says that the probability that Ydiffers from the mean\nby more than ais bounded by Var(Y)/a2. Morally, the smaller the variance of Y, the\nsmaller the probability that Yis far from its mean.\nProof of the LoLN: Since Var( Xn) = Var(X)/n, the variance of the average\nXngoes to\nzero as ngoes to infinity. So the Chebyshev inequality for Y=\nXnand fixed aimplies\nthat as ngrows, the probability that\nXnis farther than afrom μgoes to 0. Hence the\nprobability that\nXnis within aof μgoes to 1, which is the LoLN.\nProof of the Chebyshev inequality: The proof is essentially the same for discrete and\ncontinuous Y. We'll assume Yis continuous and also that μ= 0, since replacing Yby Y-μ\n\n18.05 Class 6, Appendix, Spring 2022\ndoes not change the variance. So\nP(|Y| ≥a) = ∫\n-a\n-inf\nf(y) dy+ ∫\ninf\na\nf(y) dy≤∫\n-a\n-inf\ny2\na2 f(y) dy+ ∫\ninf\na\ny2\na2 f(y) dy\n≤∫\ninf\n-inf\ny2\na2 f(y) dy= Var(Y)\na2\n.\nThe first inequality uses that y2/a2 ≥1 on the intervals of integration. The second in-\nequality follows because including the range [-a, a] only makes the integral larger, since the\nintegrand is positive.\nThe need for variance\nWe didn't lie to you, but we did gloss over one technical fact. Throughout we assumed\nthat the underlying distributions had a variance.\nFor example, the proof of the law of\nlarge numbers made use of the variance by way of the Chebyshev inequality. But there are\ndistributions which do not have a mean and variance because the sums or integrals for these\ndo not converge to a finite number. For such distributions the law of large numbers may\nnot be true.\nIn 18.05 we won't have to worry about this, but if you go deeper into statistics this may\nbecome important. For those who are interested: a standard example you can look up or\nplay with in R is the Cauchy distribution.\n\nJoint Distributions, Independence\nClass 7, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Understand what is meant by a joint pmf, pdf and cdf of two random variables.\n2. Be able to compute probabilities and marginals from a joint pmf or pdf.\n3. Be able to test whether two random variables are independent.\nIntroduction\nIn science and in real life, we are often interested in two (or more) random variables at the\nsame time. For example, we might measure the height and weight of giraffes, or the IQ\nand birthweight of children, or the frequency of exercise and the rate of heart disease in\nadults, or the level of air pollution and rate of respiratory illness in cities, or the number of\nFacebook friends and the age of Facebook members.\nThink: What relationship would you expect in each of the five examples above? Why?\nIn such situations the random variables have a joint distribution that allows us to compute\nprobabilities of events involving both variables and understand the relationship between the\nvariables. This is simplest when the variables are independent. When they are not, we use\ncovariance and correlation as measures of the nature of the dependence between them.\nJoint Distribution\n3.1\nDiscrete case\nSuppose Xand Yare two discrete random variables and that Xtakes values {x1, x2, ... , xn}\nand Ytakes values {y1, y2, ... , ym}. The ordered pair (X, Y) take values in the product\n{(x1, y1), (x1, y2), ... (xn, ym)}. The joint probability mass function (joint pmf) of Xand Y\nis the function p(xi, yj) giving the probability of the joint outcome X= xi, Y= yj.\nWe organize this in a joint probability table as shown:\n\n18.05 Class 7, Joint Distributions, Independence, Spring 2022\nX\\Y\ny1\ny2\n...\nyj\n...\nym\nx1\np(x1, y1)\np(x1, y2)\n⋯\np(x1, yj)\n⋯\np(x1, ym)\nx2\np(x2, y1)\np(x2, y2)\n⋯\np(x2, yj)\n⋯\np(x2, ym)\n⋯\n⋯\n⋯\n⋯\n⋯\n⋯\n⋯\n⋯\n⋯\n⋯\n⋯\n⋯\n⋯\n⋯\nxi\np(xi, y1)\np(xi, y2)\n⋯\np(xi, yj)\n⋯\np(xi, ym)\n⋯\n⋯\n⋯\n⋯\n⋯\n⋯\nxn\np(xn, y1)\np(xn, y2)\n⋯\np(xn, yj)\n⋯\np(xn, ym)\nExample 1. Roll two dice. Let Xbe the value on the first die and let Ybe the value on\nthe second die. Then both Xand Ytake values 1 to 6 and the joint pmf is p(i, j) = 1/36\nfor all iand jbetween 1 and 6. Here is the joint probability table:\nX\\Y\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\nExample 2. Roll two dice. Let Xbe the value on the first die and let Tbe the total on\nboth dice. Here is the joint probability table:\nX\\T\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\nA joint probability mass function must satisfy two properties:\n1. 0 ≤p(xi, yj) ≤1\n2. The total probability is 1. We can express this as a double sum:\nn\n∑\ni=1\nm\n∑\nj=1\np(xi, yj) = 1\n\n18.05 Class 7, Joint Distributions, Independence, Spring 2022\n3.2\nContinuous case\nThe continuous case is essentially the same as the discrete case: we just replace discrete sets\nof values by continuous intervals, the joint probability mass function by a joint probability\ndensity function, and the sums by integrals.\nIf Xtakes values in [a, b] and Ytakes values in [c, d] then the pair (X, Y) takes values in\nthe product [a, b] × [c, d]. The joint probability density function (joint pdf) of Xand Y\nis a function f(x, y) giving the probability density at (x, y). That is, the probability that\n(X, Y) is in a small rectangle of width dxand height dyaround (x, y) is f(x, y) dxdy.\ndx\ndy\nProb. = f(x, y) dxdy\nx\ny\na\nb\nc\nd\nA joint probability density function must satisfy two properties:\n1. 0 ≤f(x, y)\n2. The total probability is 1. We now express this as a double integral:\n∫\nd\nc\n∫\nb\na\nf(x, y) dxdy= 1\nNote: as with the pdf of a single random variable, the joint pdf f(x, y) can take values\ngreater than 1; it is a probability density, not a probability.\nIn 18.05 we won't expect you to be experts at double integration. Here's what we will\nexpect.\n- You should understand double integrals conceptually as double sums.\n- You should be able to compute double integrals over rectangles.\n- For a non-rectangular region, when f(x, y) = cis constant, you should know that the\ndouble integral is the same as the c× (the area of the region).\n3.3\nEvents\nRandom variables are useful for describing events. Recall that an event is a set of outcomes\nand that random variables assign numbers to outcomes. For example, the event 'X> 1'\nis the set of all outcomes for which Xis greater than 1. These concepts readily extend to\npairs of random variables and joint outcomes.\n\n18.05 Class 7, Joint Distributions, Independence, Spring 2022\nExample 3. In Example 1, describe the event B= 'Y-X≥2' and find its probability.\nSolution: We can describe Bas a set of (X, Y) pairs:\nB= {(1, 3), (1, 4), (1, 5), (1, 6), (2, 4), (2, 5), (2, 6), (3, 5), (3, 6), (4, 6)}.\nWe can also describe it visually\nX\\Y\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\nThe event Bconsists of the outcomes in the shaded squares.\nThe probability of Bis the sum of the probabilities in the orange shaded squares, so P(B) =\n10/36.\nExample 4. Suppose Xand Yboth take values in [0,1] with uniform density f(x, y) = 1.\nVisualize the event 'X> Y' and find its probability.\nSolution: Jointly Xand Ytake values in the unit square. The event 'X> Y' corresponds\nto the shaded lower-right triangle below. Since the density is constant, the probability is\njust the fraction of the total area taken up by the event. In this case, it is clearly 0.5.\nx\ny\n'X> Y'\nThe event 'X> Y' in the unit square.\nExample 5. Suppose Xand Yboth take values in [0,1] with density f(x, y) = 4xy. Show\nf(x, y) is a valid joint pdf, visualize the event A= 'X< 0.5 and Y> 0.5' and find its\nprobability.\nSolution: Jointly Xand Ytake values in the unit square.\n\n18.05 Class 7, Joint Distributions, Independence, Spring 2022\nx\ny\nA\nThe event Ain the unit square.\nTo show f(x, y) is a valid joint pdf we must check that it is positive (which it clearly is)\nand that the total probability is 1.\nTotal probability = ∫\n∫\n4xydxdy= ∫\n[2x2y]\n0 dy= ∫\n2ydy= 1.\nQED\nThe event Ais just the upper-left-hand quadrant. Because the density is not constant we\nmust compute an integral to find the probability.\nP(A) = ∫\n0.5\n∫\n0.5\n4xydydx= ∫\n0.5\n[2xy2]\n0.5 dx= ∫\n0.5\n3x\n2 dx=\n16 .\n3.4\nJoint cumulative distribution function\nSuppose Xand Yare jointly-distributed random variables. We will use the notation 'X≤\nx, Y≤y' to mean the event 'X≤xand Y≤y'. The joint cumulative distribution function\n(joint cdf) is defined as\nF(x, y) = P(X≤x, Y≤y)\nContinuous case: If Xand Yare continuous random variables with joint density f(x, y)\nover the range [a, b] × [c, d] then the joint cdf is given by the double integral\nF(x, y) = ∫\ny\nc\n∫\nx\na\nf(u, v) dudv.\nTo recover the joint pdf, we differentiate the joint cdf. Because there are two variables we\nneed to use partial derivatives:\nf(x, y) = ∂2F\n∂x∂y(x, y).\nDiscrete case: If Xand Yare discrete random variables with joint pmf p(xi, yj) then the\njoint cdf is give by the double sum\nF(x, y) = ∑\nxi≤x\n∑\nyj≤y\np(xi, yj).\n\n18.05 Class 7, Joint Distributions, Independence, Spring 2022\n3.5\nProperties of the joint cdf\nThe joint cdf F(x, y) of Xand Ymust satisfy several properties:\n1. F(x, y) is non-decreasing: i.e.\nif xor yincrease then F(x, y) must stay constant or\nincrease.\n2. F(x, y) = 0 at the lower-left of the joint range.\nIf the lower left is (-inf, -inf) then this means\nlim\n(x,y)→(-inf,-inf) F(x, y) = 0.\n3. F(x, y) = 1 at the upper-right of the joint range.\nIf the upper-right is (inf, inf) then this means\nlim\n(x,y)→(inf,inf) F(x, y) = 1.\nExample 6. Find the joint cdf for the random variables in Example 5.\nSolution: The event 'X≤xand Y≤y' is a rectangle in the unit square.\nx\ny\n(x, y)\n'X≤x& Y≤y'\nTo find the cdf F(x, y) we compute a double integral:\nF(x, y) = ∫\ny\n∫\nx\n4uvdudv= x2y2 .\nExample 7. In Example 1, compute F(3.5, 4).\nSolution: We redraw the joint probability table. Notice how similar the picture is to the\none in the previous example.\nF(3.5, 4) is the probability of the event 'X≤3.5 and Y≤4'. We can visualize this event\nas the shaded rectangles in the table:\nX\\Y\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n\n18.05 Class 7, Joint Distributions, Independence, Spring 2022\nThe event 'X≤3.5 and Y≤4'.\nAdding up the probability in the shaded squares we get F(3.5, 4) = 12/36 = 1/3.\nNote. One unfortunate difference between the continuous and discrete visualizations is that\nfor continuous variables the value increases as we go up in the vertical direction while the\nopposite is true for the discrete case. We have experimented with changing the discrete\ntables to match the continuous graphs, but it causes too much confusion. We will just have\nto live with the difference!\n3.6\nMarginal distributions\nWhen Xand Yare jointly-distributed random variables, we may want to consider only one\nof them, say X. In that case we need to find the pmf (or pdf or cdf) of Xwithout Y. This\nis called a marginal pmf of the joint pmf (or pdf or cdf). The next example illustrates the\nway to compute this and the reason for the term 'marginal'.\n3.7\nMarginal pmf\nExample 8. In Example 2 we rolled two dice and let Xbe the value on the first die and\nTbe the total on both dice. Compute the marginal pmf for Xand for T.\nSolution: In the table each row represents a single value of X. So the event 'X= 3' is the\nthird row of the table. To find P(X= 3) we simply have to sum up the probabilities in this\nrow. We put the sum in the right-hand margin of the table. Likewise P(T= 5) is just the\nsum of the column with T= 5. We put the sum in the bottom margin of the table.\nX\\T\np(xi)\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\np(tj)\n1/36\n2/36\n3/36\n4/36\n5/36\n6/36\n5/36\n4/36\n3/36\n2/36\n1/36\nComputing the marginal probabilities P(X= 3) = 1/6 and P(T= 5) = 4/36.\nNote: Of course in this case we already knew the pmf of Xand of T. It is good to see that\nour computation here is in agreement!\nAs motivated by this example, marginal pmfs are obtained from the joint pmf by summing:\npX(xi) = ∑\nj\np(xi, yj),\npY(yj) = ∑\ni\np(xi, yj)\nThe term marginal refers to the fact that the values are written in the margins of the table.\n\n18.05 Class 7, Joint Distributions, Independence, Spring 2022\n3.8\nMarginal pdf\nFor a continous joint density f(x, y) with range [a, b] × [c, d], the marginal pdfs are:\nfX(x) = ∫\nd\nc\nf(x, y) dy,\nfY(y) = ∫\nb\na\nf(x, y) dx.\nCompare these with the marginal pmfs above; as usual the sums are replaced by integrals.\nWe say that to obtain the marginal for X, we integrate out Yfrom the joint pdf and vice\nversa.\nExample 9. Suppose (X, Y) takes values on the square [0, 1]×[1, 2] with joint pdf f(x, y) =\n3x3y. Find the marginal pdfs fX(x) and fY(y).\nSolution: To find fX(x) we integrate out yand to find fY(y) we integrate out x.\nfX(x) = ∫\n3x3ydy= [4\n3x3y2]\n= 4x3\nfY(y) = ∫\n3x3ydx= [2\n3x4y1]\n= 2\n3y.\nExample 10. Suppose (X, Y) takes values on the unit square [0, 1] × [0, 1] with joint pdf\nf(x, y) = 3\n2(x2 + y2). Find the marginal pdf fX(x) and use it to find P(X< 0.5).\nSolution:\nfX(x) = ∫\n2(x2 + y2) dy= [3\n2x2y+ y3\n2 ]\n= 3\n2x2 + 1\n2 .\nP(X< 0.5) = ∫\n0.5\nfX(x) dx= ∫\n0.5\n2x2 + 1\n2 dx= [1\n2x3 + 1\n2x]\n0.5\n=\n16 .\n3.9\nMarginal cdf\nFinding the marginal cdf from the joint cdf is easy. If Xand Yjointly take values on\n[a, b] × [c, d] then\nFX(x) = F(x, d),\nFY(y) = F(b, y).\nIf dis infthen this becomes a limit FX(x) = lim\ny→infF(x, y). Likewise for FY(y).\nExample 11. The joint cdf in the last example was F(x, y) = 1\n2(x3y+xy3) on [0, 1]×[0, 1].\nFind the marginal cdfs and use FX(x) to compute P(X< 0.5).\nSolution: We have FX(x) = F(x, 1) = 1\n2(x3 + x) and FY(y) = F(1, y) = 1\n2(y+ y3). So\nP(X< 0.5) = FX(0.5) = 1\n2(0.53 + 0.5) =\n16: exactly the same as before.\n3.10\n3D visualization\nWe visualized P(a< X< b) as the area under the pdf f(x) over the interval [a, b]. Since\nthe range of values of (X, Y) is already a two dimensional region in the plane, the graph of\n\n18.05 Class 7, Joint Distributions, Independence, Spring 2022\nf(x, y) is a surface over that region. We can then visualize probability as volume under the\nsurface.\nThink: Summoning your inner artist, sketch the graph of the joint pdf f(x, y) = 4xyand\nvisualize the probability P(A) as a volume for Example 5.\nIndependence\nWe are now ready to give a careful mathematical definition of independence. Of course, it\nwill simply capture the notion of independence we have been using up to now. But, it is nice\nto finally have a solid definition that can support complicated probabilistic and statistical\ninvestigations.\nRecall that events Aand Bare independent if\nP(A∩B) = P(A)P(B).\nRandom variables Xand Ydefine events like 'X≤2' and 'Y> 5'. So, Xand Yare\nindependent if any event defined by Xis independent of any event defined by Y.\nThe\nformal definition that guarantees this is the following.\nDefinition: Jointly-distributed random variables Xand Yare independent if their joint\ncdf is the product of the marginal cdfs:\nF(X, Y) = FX(x)FY(y).\nFor discrete variables this is equivalent to the joint pmf being the product of the marginal\npmfs.:\np(xi, yj) = pX(xi)pY(yj).\nFor continous variables this is equivalent to the joint pdf being the product of the marginal\npdfs.:\nf(x, y) = fX(x)fY(y).\nOnce you have the joint distribution, checking for independence is usually straightforward\nalthough it can be tedious.\nExample 12. For discrete variables independence means the probability in a cell must be\nthe product of the marginal probabilities of its row and column. In the first table below\nthis is true: every marginal probability is 1/6 and every cell contains 1/36, i.e. the product\nof the marginals. Therefore Xand Yare independent.\nIn the second table below most of the cell probabilities are not the product of the marginal\nprobabilities. For example, none of marginal probabilities are 0, so none of the cells with 0\nprobability can be the product of the marginals.\n\n18.05 Class 7, Joint Distributions, Independence, Spring 2022\nX\\Y\np(xi)\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\np(yj)\n1/6\n1/6\n1/6\n1/6\n1/6\n1/6\nX\\T\np(xi)\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/6\np(yj)\n1/36\n2/36\n3/36\n4/36\n5/36\n6/36\n5/36\n4/36\n3/36\n2/36\n1/36\nExample 13. For continuous variables independence means you can factor the joint pdf\nor cdf as the product of a function of xand a function of y.\n(i) Suppose Xhas range [0, 1/2], Yhas range [0, 1] and f(x, y) = 96x2y3 then Xand Yare\nindependent. The marginal densities are fX(x) = 24x2 and fY(y) = 4y3.\n(ii) If f(x, y) = 1.5(x2+y2) over the unit square then Xand Yare not independent because\nthere is no way to factor f(x, y) into a product fX(x)fY(y).\n(iii) If F(x, y) = 1\n2(x3y+ xy3) over the unit square then Xand Yare not independent\nbecause the cdf does not factor into a product FX(x)FY(y).\n\nCovariance and Correlation\nClass 7, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Understand the meaning of covariance and correlation.\n2. Be able to compute the covariance and correlation of two random variables.\nCovariance\nCovariance is a measure of how much two random variables vary together. For example,\nheight and weight of giraffes have positive covariance because when one is big the other\ntends also to be big.\nDefinition:\nSuppose Xand Yare random variables with means μXand μY.\nThe\ncovariance of Xand Yis defined as\nCov(X, Y) = E[(X-μX)(Y-μY)].\n2.1\nProperties of covariance\n1. Cov(aX+ b, cY+ d) = acCov(X, Y)\nfor constants a, b, c, d.\n2. Cov(X1 + X2, Y) = Cov(X1, Y) + Cov(X2, Y).\n3. Cov(X, X) = Var(X)\n4. Cov(X, Y) = E[XY] -μXμY.\n5. Var(X+ Y) = Var(X) + Var(Y) + 2Cov(X, Y)\nfor any Xand Y.\n6. If Xand Yare independent then Cov(X, Y) = 0.\nWarning: The converse is false: zero covariance does not always imply independence.\nNotes. 1. Property 4 is like the similar property for variance. Indeed, if X= Yit is exactly\nthat property: Var(X) = E[X2] -μ2\nX.\nBy Property 5, the formula in Property 6 reduces to our earlier formula Var(X+ Y) =\nVar(X) + Var(Y) when Xand Yare independent.\nWe give the proofs below.\nHowever, understanding and using these properties is more\nimportant than memorizing their proofs.\n\n18.05 Class 7, Covariance and Correlation, Spring 2022\n2.2\nSums and integrals for computing covariance\nSince covariance is defined as an expected value we compute it in the usual way as a sum\nor integral.\nDiscrete case: If Xand Yhave joint pmf p(xi, yj) then\nCov(X, Y) =\nn\n∑\ni=1\nm\n∑\nj=1\np(xi, yj)(xi-μX)(yj-μY) = (\nn\n∑\ni=1\nm\n∑\nj=1\np(xi, yj)xiyj) -μXμY.\nContinuous case: If Xand Yhave joint pdf f(x, y) over range [a, b] × [c, d] then\nCov(X, Y) = ∫\nd\nc\n∫\nb\na\n(x-μx)(y-μy)f(x, y) dxdy= (∫\nd\nc\n∫\nb\na\nxyf(x, y) dxdy) -μxμy.\n2.3\nExamples\nExample 1. Flip a fair coin 3 times. Let Xbe the number of heads in the first 2 flips\nand let Ybe the number of heads on the last 2 flips (so there is overlap on the middle flip).\nCompute Cov(X, Y).\nSolution: We'll do this twice, first using the joint probability table and the definition of\ncovariance, and then using the properties of covariance.\nWith 3 tosses there are only 8 outcomes {HHH, HHT,...}, so we can create the joint prob-\nability table directly.\nX\\Y\np(xi)\n1/8\n1/8\n1/4\n1/8\n2/8\n1/8\n1/2\n1/8\n1/8\n1/4\np(yj) 1/4\n1/2\n1/4\nFrom the marginals we compute E[X] = 1 = E[Y]. Now we use use the definition:\nCov(X, Y) = E[(X-μx)(Y-μY)] = ∑\ni,j\np(xi, yj)(xi-1)(yj-1)\nWe write out the sum leaving out all the terms that are 0, i.e. all the terms where xi= 1\nor yi= 1 or the probability is 0.\nCov(X, Y) = 1\n8(0 -1)(0 -1) + 1\n8(2 -1)(2 -1) = 1\n4.\nWe could also have used property 4 to do the computation: From the full table we compute\nE[XY] = 1 ⋅2\n8 + 21\n8 + 21\n8 + 41\n8 = 5\n4.\n\n18.05 Class 7, Covariance and Correlation, Spring 2022\nSo Cov(XY) = E[XY] -μXμY= 5\n4 -1 = 1\n4.\nNext we redo the computation of Cov(X, Y) using the properties of covariance. As usual,\nlet Xibe the result of the ith flip, so Xi∼Bernoulli(0.5). We have\nX= X1 + X2\nand\nY= X2 + X3.\nWe know E[Xi] = 1/2 and Var(Xi) = 1/4. Therefore using Property 2 of covariance, we\nhave\nCov(X, Y) = Cov(X1+X2, X2+X3) = Cov(X1, X2)+Cov(X1, X3)+Cov(X2, X2)+Cov(X2, X3).\nSince the different tosses are independent we know\nCov(X1, X2) = Cov(X1, X3) = Cov(X2, X3) = 0.\nLooking at the expression for Cov(X, Y) there is only one non-zero term\nCov(X, Y) = Cov(X2, X2) = Var(X2) = 1\n4 .\nExample 2. (Zero covariance does not imply independence.) Let Xbe a random vari-\nable that takes values -2, -1, 0, 1, 2; each with probability 1/5. Let Y= X2. Show that\nCov(X, Y) = 0 but Xand Yare not independent.\nSolution: We make a joint probability table:\nY\\X\n-2\n-1\np(yj)\n1/5\n1/5\n1/5\n1/5\n2/5\n1/5\n1/5\n2/5\np(xi)\n1/5\n1/5\n1/5\n1/5\n1/5\nUsing the marginals we compute means E[X] = 0 and E[Y] = 2.\nNext we show that Xand Yare not independent. To do this all we have to do is find one\nplace where the product rule fails, i.e. where p(xi, yj) =p(xi)p(xj):\nP(X= -2, Y= 0) = 0\nbut\nP(X= -2) ⋅P(Y= 0) = 1/25.\nSince these are not equal Xand Yare not independent. Finally we compute covariance\nusing Property 4:\nCov(X, Y) = 1\n5(-8 -1 + 1 + 8) -μXμy= 0.\nDiscussion: This example shows that Cov(X, Y) = 0 does not imply that Xand Yare\nindependent. In fact, Xand X2 are as dependent as random variables can be: if you know\nthe value of Xthen you know the value of X2 with 100% certainty.\n\n18.05 Class 7, Covariance and Correlation, Spring 2022\nThe key point is that Cov(X, Y) measures the linear relationship between Xand Y. In\nthe above example Xand X2 have a quadratic relationship that is completely missed by\nCov(X, Y).\nContinuous covariance works the same way, except our computations are done with integrals\ninstead of sums. Here is an example.\nExample 3. Continuous covariance. Suppose Xand Yare jointly distributed random\nvariables, with range on the unit square [0, 1] × [0, 1] and joint pdf f(x, y) = 2x3 + 2y3.\n(i) Verify the f(x, y) is a valid probability density.\n(ii) Compute μXand μY.\n(iii) Compute the covariance of Cov(X, Y)\nSolution: Part of the point of this example is to show how to set up and compute the inte-\ngrals using a joint density function. Since the pdf here is a polynomial, these computations\nare relatively easy.\n(i) A valid pdf has two properties: it is nonnegative and the total integral over the entire\njoint range is 1.\nNonnegativity is clear: f(x, y) ≥0. The integral is not hard to compute\n∫\n∫\nf(x, y) dxdy= ∫\n∫\n2x3 + 2y3 dxdy\nInner integral: ∫\n2x3 + 2y3 dx= x4\n2 + 2xy3∣\n= 1\n2 + 2y3.\nOuter integral: ∫\n2 + 2y3 dy= y\n2 + y4\n2 ∣\n= 1.\nSo, the integral over the entire joint range is 1. Thus, f(x, y) = x+ yis a valid probability\ndensity.\n(ii) We need to compute integrals to find the means. We will write down the integrals, but\nnot show the details of their computation. (Also, by symmetry, we know the two means are\nthe same.)\nμX= ∫\n∫\nxf(x, y) dxdy= ∫\n∫\n2x4 + 2xy3) dxdy= 13\nμY= ∫\n∫\nyf(x, y) dxdy= ∫\n∫\n2yx3 + 2y4 dxdy= 13\n(iii) We know Cov(X, Y) = E[(X-μx)(Y-μY)]. This is an integral. Again, we will write\ndown the integral, but not show details of its computation,\nCov(X, Y) = E[(X-μx)(Y-μY)] = ∫\n∫\n(x-13/20)(y-13/20)f(x, y) dxdy\n= ∫\n∫\n(x-7/12)(y-7/12)(2x3 + 2y3) dxdy= -9\n\n18.05 Class 7, Covariance and Correlation, Spring 2022\n(In fact, we wrote down the integral in the most straightforward way, but secretly we did\nthe computation by computing E[XY] -E[X]E[Y].)\nHere's a plot of the pseudo-random samples generated from this distribution. Because the\nR code could do it easily, we also include a plot with a more extreme density function.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nx\ny\nSamples from f(x, y) = 2x3 + 2y3.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nx\ny\nSamples from f(x, y) = 10x19 + 10y19.\nCorrelation\nThe units of covariance Cov(X, Y) are 'units of Xtimes units of Y'. This makes it hard to\ncompare covariances: if we change scales then the covariance changes as well. Correlation\nis a way to remove the scale from the covariance.\nDefinition: The correlation coefficient between Xand Yis defined by\nCor(X, Y) = ρ= Cov(X, Y)\nσXσY\n.\n3.1\nProperties of correlation\n1. ρis the covariance of the standardizations of Xand Y.\n2. ρis dimensionless (it's a ratio!).\n3. -1 ≤ρ≤1. Furthermore,\nρ= +1 if and only if Y= aX+ bwith a> 0,\nρ= -1 if and only if Y= aX+ bwith a< 0.\nProperty 3 shows that ρmeasures the linear relationship between variables. If the corre-\nlation is positive then when Xis large, Ywill tend to large as well. If the correlation is\nnegative then when Xis large, Ywill tend to be small.\nExample 2 above shows that correlation can completely miss higher order relationships.\nExample 4. We continue Example 1. To compute the correlation we divide the covariance\nby the standard deviations.\nIn Example 1 we found Cov(X, Y) = 1/4 and Var(X) =\n\n18.05 Class 7, Covariance and Correlation, Spring 2022\n2Var(Xj) = 1/2. So, σX= 1/\n√\n2. Likewise σY= 1/\n√\n2. Thus\nCor(X, Y) = Cov(X, Y)\nσXσY\n= 1/4\n1/2 = 1\n2.\nWe see a positive correlation, which means that larger Xtend to go with larger Yand\nsmaller Xwith smaller Y. In Example 1 this happens because toss 2 is included in both X\nand Y, so it contributes to the size of both.\nExample 5. Look back at Example 3. See if you can compute the following.\nVar(X) = 31/400, so σX= √31/400 ≈0.28\nVar(Y) = Var(X), so σY≈0.28\nCor(X, Y) = Cov(X, Y)\nσXσY\n≈-0.29.\n3.2\nBivariate normal distributions\nThe bivariate normal distribution has density\nf(x, y) = e\n-1\n2(1-ρ2) [ (x-μX)2\nσ2\nX\n+ (y-μY)2\nσ2\nY\n-\n2ρ(x-μx)(y-μy)\nσxσy\n]\n2πσXσY√1 -ρ2\nFor this distribution, the marginal distributions for Xand Yare normal and the correlation\nbetween Xand Yis ρ.\nIn the figures below we used R to simulate the distribution for various values of ρ. Individ-\nually Xand Yare standard normal, i.e. μX= μY= 0 and σX= σY= 1. The figures show\nscatter plots of the results.\nThese plots and the next set show an important feature of correlation. We divide the data\ninto quadrants by drawing a horizontal and a verticle line at the means of the ydata and\nxdata respectively. A positive correlation corresponds to the data tending to lie in the 1st\nand 3rd quadrants. A negative correlation corresponds to data tending to lie in the 2nd\nand 4th quadrants. You can see the data gathering about a line as ρbecomes closer to ±1.\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n-3\n-2\n-1\n-3\n-1\nrho=0.00\ny\nx\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n-4\n-3\n-2\n-1\n-3\n-1\nrho=0.30\ny\nx\n\n18.05 Class 7, Covariance and Correlation, Spring 2022\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n-3\n-2\n-1\n-3\n-1\nrho=0.70\ny\nx\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nGG\nGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG G\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n-4\n-2\n-4\n-2\nrho=1.00\ny\nx\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n-3\n-2\n-1\n-2\nrho=-0.50\ny\nx\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\n-3\n-2\n-1\n-4\n-2\nrho=-0.90\ny\nx\n3.3\nOverlapping uniform distributions\nWe ran simulations in R of the following scenario. X1, X2, ..., X20 are i.i.d and follow a\nU(0, 1) distribution. Xand Yare both sums of the same number of Xi. We call the number\nof Xicommon to both Xand Ythe overlap. The notation in the figures below indicates\nthe number of Xibeing summed and the number which overlap. For example, 5,3 indicates\nthat Xand Ywere each the sum of 5 of the Xiand that 3 of the Xiwere common to both\nsums. (The data was generated using rand(1,1000);)\nUsing the linearity of covariance it is easy to compute the theoretical correlation.\nFor\neach plot we give both the theoretical correlation and the correlation of the data from the\nsimulated sample.\n\n18.05 Class 7, Covariance and Correlation, Spring 2022\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.4\n0.8\n(1, 0) cor=0.00, sample_cor=-0.07\ny\nx\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0.0\n0.5\n1.0\n1.5\n2.0\n0.0\n0.5\n1.0\n1.5\n2.0\n(2, 1) cor=0.50, sample_cor=0.48\ny\nx\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n(5, 1) cor=0.20, sample_cor=0.21\ny\nx\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n(5, 3) cor=0.60, sample_cor=0.63\ny\nx\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n(10, 5) cor=0.50, sample_cor=0.53\ny\nx\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n(10, 8) cor=0.80, sample_cor=0.81\ny\nx\n\n18.05 Class 7, Covariance and Correlation, Spring 2022\nProof of the properties of covariance and correlation\n4.1\nProofs of the properties of covariance\n1 and 2 follow from similar properties for expected value.\n3. This is the definition of variance:\nCov(X, X) = E[(X-μX)(X-μX)] = E[(X-μX)2] = Var(X).\n4. Recall that E[X-μx] = 0. So\nCov(X, Y) = E[(X-μX)(Y-μY)]\n= E[XY-μXY-μYX+ μXμY]\n= E[XY] -μXE[Y] -μYE[X] + μXμY\n= E[XY] -μXμY-μXμY+ μXμY\n= E[XY] -μXμY.\n5. Using properties 3 and 2 we get\nVar(X+Y) = Cov(X+Y, X+Y) = Cov(X, X)+2Cov(X, Y)+Cov(Y, Y) = Var(X)+Var(Y)+2Cov(X, Y).\n6. If Xand Yare independent then f(x, y) = fX(x)fY(y). Therefore\nCov(X, Y) = ∫∫(x-μX)(y-μY)fX(x)fY(y) dxdy\n= ∫(x-μX)fX(x) dx∫(y-μY)fY(y) dy\n= E[X-μX]E[Y-μY]\n= 0.\n4.2\nProof of Property 3 of correlation\n(This is for the mathematically interested.)\n0 ≤Var ( X\nσX\n-Y\nσY\n) = Var ( X\nσX\n) + Var ( Y\nσY\n) -2Cov ( X\nσX\n, Y\nσY\n) = 2 -2ρ\nThis implies ρ≤1\nLikewise 0 ≤Var ( X\nσX\n+ Y\nσY\n), so -1 ≤ρ.\nIf ρ= 1 then 0 = Var ( X\nσX\n-Y\nσY\n) ⇒\nX\nσX\n-Y\nσY\n= c.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "All Statistics Reading",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_statistics.pdf",
      "content": "Introduction to Statistics\nClass 10, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Know the three overlapping \"phases\" of statistical practice.\n2. Know what is meant by the term statistic.\nIntroduction to statistics\nStatistics deals with data. Generally speaking, the goal of statistics is to make inferences\nbased on data. We can divide this the process into three phases: collecting data, describing\ndata and analyzing data. This fits into the paradigm of the scientific method. We make\nhypotheses about what's true, collect data in experiments, describe the results, and then\ninfer from the results the strength of the evidence concerning our hypotheses.\n2.1\nExperimental design\nThe design of an experiment is crucial to making sure the collected data is useful. The\nadage 'garbage in, garbage out' applies here. A poorly designed experiment will produce\npoor quality data, from which it may be impossible to draw useful, valid inferences. To\nquote R.A. Fisher one of the founders of modern statistics,\nTo consult a statistician after an experiment is finished is often merely to ask\nhim to conduct a post-mortem examination.\nHe can perhaps say what the\nexperiment died of.\n2.2\nDescriptive statistics\nRaw data often takes the form of a massive list, array, or database of labels and numbers.\nTo make sense of the data, we can calculate summary statistics like the mean, median, and\ninterquartile range. We can also visualize the data using graphical devices like histograms,\nscatterplots, and the empirical cdf. These methods are useful for both communicating and\nexploring the data to gain insight into its structure, such as whether it might follow a\nfamiliar probability distribution.\n2.3\nInferential statistics\nUltimately we want to draw inferences about the world.\nOften this takes the form of\nspecifying a statistical model for the random process by which the data arises. For example,\nsuppose the data takes the form of a series of measurements whose error we believe follows\na normal distribution. (Note this is always an approximation since we know the error must\n\n18.05 Class 10, Introduction to Statistics, Spring 2022\nhave some bound while a normal distribution has range (-inf, inf).) We might then use the\ndata to provide evidence for or against this hypothesis. Our focus in 18.05 will be on how\nto use data to draw inferences about model parameters. For example, assuming gestational\nlength follows a N(μ, σ) distribution, we'll use the data of the gestational lengths of, say,\n500 pregnancies to draw inferences about the values of the parameters μand σ. Similarly,\nwe may model the result of a two-candidate election by a Bernoulli(p) distribution, and use\npoll data to draw inferences about the value of p.\nWe can rarely make definitive statements about such parameters because the data itself\ncomes from a random process (such as choosing who to poll). Rather, our statistical evidence\nwill always involve probability statements. Unfortunately, the media and public at large\nare wont to misunderstand the probabilistic meaning of statistical statements.\nIn fact,\nresearchers themselves often commit the same errors. In this course, we will emphasize the\nmeaning of statistical statements alongside the methods which produce them.\nExample 1. To study the effectiveness of new treatment for cancer, patients are recruited\nand then divided into an experimental group and a control group. The experimental group\nis given the new treatment and the control group receives the current standard of care.\nData collected from the patients might include demographic information, medical history,\ninitial state of cancer, progression of the cancer over time, treatment cost, and the effect of\nthe treatment on tumor size, remission rates, longevity, and quality of life. The data will\nbe used to make inferences about the effectiveness of the new treatment compared to the\ncurrent standard of care.\nNotice that this study will go through all three phases described above. The experimental\ndesign must specify the size of the study, who will be eligible to join, how the experimental\nand control groups will be chosen, how the treatments will be administered, whether or\nnot the subjects or doctors know who is getting which treatment, and precisely what data\nwill be collected, among other things. Once the data is collected it must be described and\nanalyzed to determine whether it supports the hypothesis that the new treatment is more\n(or less) effective than the current one(s), and by how much. These statistical conclusions\nwill be framed as precise statements involving probabilities.\nAs noted above, misinterpreting the exact meaning of statistical statements is a common\nsource of error which has led to tragedy on more than one occasion.\nExample 2. In 1999 in Great Britain, Sally Clark was convicted of murdering her two\nchildren after each child died weeks after birth (the first in 1996, the second in 1998). Her\nconviction was largely based on a faulty use of statistics to rule out sudden infant death\nsyndrome. Though her conviction was overturned in 2003, she developed serious psychiatric\nproblems during and after her imprisonment and died of alcohol poisoning in 2007. See\nhttps://en.wikipedia.org/wiki/Sally_Clark\nThis TED talk discusses the Sally Clark case and other instances of poor statistical intuition:\nhttps://www.youtube.com/watch?v=kLmzxmRcUTo\n2.4\nWhat is a statistic?\nWe give a simple definition whose meaning is best elucidated by examples.\nDefinition. A statistic is anything that can be computed from the collected data.\n\n18.05 Class 10, Introduction to Statistics, Spring 2022\nExample 3. Consider the data of 1000 rolls of a die. All of the following are statistics:\nthe average of the 1000 rolls; the number of times a 6 was rolled; the sum of the squares of\nthe rolls minus the number of even rolls. It's hard to imagine how we would use the last\nexample, but it is a statistic. On the other hand, the probability of rolling a 6 is not a\nstatistic, whether or not the die is truly fair. Rather this probability is a property of the die\n(and the way we roll it) which we can estimate using the data. Such an estimate is given\nby the statistic 'proportion of the rolls that were 6'.\nExample 4. Suppose we treat a group of cancer patients with a new procedure and collect\ndata on how long they survive post-treatment. From the data we can compute the average\nsurvival time of patients in the group. We might employ this statistic as an estimate of the\naverage survival time for future cancer patients following the new procedure. The \"expected\nsurvival time\" for the new procedure (if that even has a meaning) is not a statistic.\nExample 5. Suppose we ask 1000 residents whether or not they support the proposal to\nlegalize marijuana in Massachusetts. The proportion of the 1000 who support the proposal\nis a statistic. The proportion of all Massachusetts residents who support the proposal is\nnot a statistic since we have not queried every single one (note the word \"collected\" in the\ndefinition). Rather, we hope to draw a statistical conclusion about the state-wide proportion\nbased on the data of our random sample.\nThe following are two general types of statistics we will use in 18.05.\n1. Point statistics: a single value computed from data, such as the sample average xnor\nthe sample standard deviation sn.\n2. Interval statistics: an interval [a, b] computed from the data. This is really just a pair of\npoint statistics, and will often be presented in the form x± s.\nReview of Bayes' theorem\nWe cannot stress strongly enough how important Bayes' theorem is to our view of inferential\nstatistics. Recall that Bayes' theorem allows us to 'invert' conditional probabilities. That\nis, if Hand Dare events, then Bayes' theorem says\nP(H|D) = P(D|H)P(H)\nP(D)\n.\nIn scientific experiments we start with a hypothesis and collect data to test the hypothesis.\nWe will often let Hrepresent the event 'our hypothesis is true' and let Dbe the collected\ndata. In these words Bayes' theorem says\nP(hypothesis is true | data) = P(data |hypothesis is true) ⋅P(hypothesis is true)\nP(data)\nThe left-hand term is the probability our hypothesis is true given the data we collected.\nThis is precisely what we'd like to know. When all the probabilities on the right are known\nexactly, we can compute the probability on the left exactly. This will be our focus next\nweek. Unfortunately, in practice we rarely know the exact values of all the terms on the\n\n18.05 Class 10, Introduction to Statistics, Spring 2022\nright. Statisticians have developed a number of ways to cope with this lack of knowledge\nand still make useful inferences. We will be exploring these methods for the rest of the\ncourse.\nExample 6. Screening for a disease redux\nSuppose a screening test for a disease has a 1% false positive rate and a 1% false negative\nrate. Suppose also that the rate of the disease in the population is 0.002. Finally suppose\na randomly selected person tests positive. In the language of hypothesis and data we have:\nHypothesis: H= 'the person has the disease'\nData: D= 'the test was positive.'\nWhat we want to know: P(H|D) = P(the person has the disease | a positive test)\nIn this example all the probabilities on the right are known so we can use Bayes' theorem\nto compute what we want to know.\nP(hypothesis | data) = P(the person has the disease | a positive test)\n= P(H|D)\n= P(D|H)P(H)\nP(D)\n=\n0.99 ⋅0.002\n0.99 ⋅0.002 + 0.01 ⋅0.998\n= 0.166\nBefore the test we would have said the probability the person had the disease was 0.002.\nAfter the test we see the probability is 0.166.\nThat is, the positive test provides some\nevidence that the person has the disease.\n\nMaximum Likelihood Estimates\nClass 10, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Be able to define the likelihood function for a parametric model given data.\n2. Be able to compute the maximum likelihood estimate of unknown parameter(s).\nIntroduction\nSuppose we know we have data consisting of values x1, ... , xndrawn from an exponential\ndistribution. The question remains: which exponential distribution?!\nWe have casually referred to the exponential distribution or the binomial distribution or the\nnormal distribution. In fact the exponential distribution exp(λ) is not a single distribution\nbut rather a one-parameter family of distributions. Each value of λdefines a different distri-\nbution in the family, with pdf fλ(x) = λe-λxon [0, inf). Similarly, a binomial distribution\nbin(n, p) is determined by the two parameters nand p, and a normal distribution N(μ, σ2)\nis determined by the two parameters μand σ2 (or equivalently, μand σ). Parameterized\nfamilies of distributions are often called parametric distributions or parametric models.\nWe are often faced with the situation of having random data which we know (or believe)\nis drawn from a parametric model, whose parameters we do not know. For example, in\nan election between two candidates, polling data constitutes draws from a Bernoulli(p)\ndistribution with unknown parameter p.\nIn this case we would like to use the data to\nestimate the value of the parameter p, as the latter predicts the result of the election.\nSimilarly, assuming gestational length follows a normal distribution, we would like to use\nthe data of the gestational lengths from a random sample of pregnancies to draw inferences\nabout the values of the parameters μand σ2.\nOur focus so far has been on computing the probability of data arising from a parametric\nmodel with known parameters. Statistical inference flips this on its head: we will estimate\nthe probability of parameters given a parametric model and observed data drawn from it.\nIn the coming weeks we will see how parameter values are naturally viewed as hypotheses,\nso we are in fact estimating the probability of various hypotheses given the data.\nMaximum Likelihood Estimates\nThere are many methods for estimating unknown parameters from data.\nWe will first\nconsider the maximum likelihood estimate (MLE), which answers the question:\nFor which parameter value does the observed data have the biggest probability?\nThe MLE is an example of a point estimate because it gives a single value for the unknown\nparameter (later our estimates will involve intervals and probabilities). Two advantages of\n\n18.05 Class 10, Maximum Likelihood Estimates , Spring 2022\nthe MLE are that it is often easy to compute and that it agrees with our intuition in simple\nexamples. We will explain the MLE through a series of examples.\nExample 1. A coin is flipped 100 times. Given that there were 55 heads, find the maximum\nlikelihood estimate for the probability pof heads on a single toss.\nBefore actually solving the problem, let's establish some notation and terms.\nWe can think of counting the number of heads in 100 tosses as an experiment. For a given\nvalue of p, the probability of getting 55 heads in this experiment is the binomial probability\nP(55 heads) = (100\n55 )p55(1 -p)45.\nThe probability of getting 55 heads depends on the value of p, so let's include pin by using\nthe notation of conditional probability:\nP(55 heads | p) = (100\n55 )p55(1 -p)45.\nYou should read P(55 heads | p) as:\n'the probability of 55 heads given p,'\nor more precisely as\n'the probability of 55 heads given that the probability of heads on a single toss is p.'\nHere are some standard terms we will use as we do statistics.\n- Experiment: Flip the coin 100 times and count the number of heads.\n- Data: The data is the result of the experiment. In this case it is '55 heads'.\n- Parameter(s) of interest: We are interested in the value of the unknown parameter p.\n- Likelihood, or likelihood function: this is P(data | p). Note it is a function of both the\ndata and the parameter p. In this case the likelihood is\nP(55 heads | p) = (100\n55 )p55(1 -p)45.\nNotes: 1. The likelihood P(data | p) changes as the parameter of interest pchanges.\n2. Look carefully at the definition. One typical source of confusion is to mistake the likeli-\nhood P(data | p) for P(p| data). We know from our earlier work with Bayes' theorem that\nP(data | p) and P(p| data) are usually very different.\nDefinition: Given data the maximum likelihood estimate (MLE) for the parameter pis\nthe value of pthat maximizes the likelihood P(data | p). That is, the MLE is the value of\npfor which the data is most likely.\nSolution: For the problem at hand, we saw above that the likelihood\nP(55 heads | p) = (100\n55 )p55(1 -p)45.\n\n18.05 Class 10, Maximum Likelihood Estimates , Spring 2022\nWe'll use the notation\npfor the MLE. We use calculus to find it by taking the derivative of\nthe likelihood function and setting it to 0.\nd\ndpP(data |p) = (100\n55 )(55p54(1 -p)45 -45p55(1 -p)44) = 0.\nSolving this for pwe get\n55p54(1 -p)45 = 45p55(1 -p)44\n55(1 -p) = 45p\n55 = 100p\nthe MLE is\np= 0.55\nNote: 1. The MLE for pturned out to be exactly the fraction of heads we saw in our data.\n2. The MLE is computed from the data. That is, it is a statistic.\n3. Officially we need to check that this critical point is actually the maximum. We could\nuse the second derivative test. Another way is to notice that we are interested only in\n0 ≤p≤1; that the probability is bigger than zero for 0 < p< 1; and that the probability\nis equal to zero for p= 0 and for p= 1. From these facts it follows that the critical point\nmust be the unique maximum.\n3.1\nLog likelihood\nIf is often easier to work with the natural log of the likelihood function. For short this is\nsimply called the log likelihood. Since ln(x) is an increasing function, the maxima of the\nlikelihood and log likelihood coincide.\nExample 2. Redo the previous example using log likelihood.\nSolution: We had the likelihood P(55 heads | p) = (100\n55 )p55(1 -p)45. Therefore the log\nlikelihood is\nln(P(55 heads | p) = ln ((100\n55 )) + 55 ln(p) + 45 ln(1 -p).\nMaximizing likelihood is the same as maximizing log likelihood. We check that calculus\ngives us the same answer as before:\nd\ndp(log likelihood) = d\ndp[ln ((100\n55 )) + 55 ln(p) + 45 ln(1 -p)]\n= 55\np-\n1 -p= 0\n⇒55(1 -p) = 45p\n⇒p= 0.55\n\n18.05 Class 10, Maximum Likelihood Estimates , Spring 2022\n3.2\nMaximum likelihood for continuous distributions\nFor continuous distributions, we use the probability density function to define the likelihood.\nWe show this in a few examples. In the next section we explain how this is analogous to\nwhat we did in the discrete case.\nExample 3. Light bulbs\nSuppose that the lifetime of Badger brand light bulbs is modeled by an exponential distri-\nbution with (unknown) parameter λ. We test 5 bulbs and find they have lifetimes of 2, 3,\n1, 3, and 4 years, respectively. What is the MLE for λ?\nSolution: We need to be careful with our notation. With five different values it is best to\nuse subscripts. Let Xibe the lifetime of the ith bulb and let xibe the value Xitakes. Then\neach Xihas pdf fXi(xi) = λe-λxi. We assume the lifetimes of the bulbs are independent,\nso the joint pdf is the product of the individual densities:\nf(x1, x2, x3, x4, x5 | λ) = (λe-λx1)(λe-λx2)(λe-λx3)(λe-λx4)(λe-λx5) = λ5e-λ(x1+x2+x3+x4+x5).\nNote that we write this as a conditional density, since it depends on λ. Viewing the data\nas fixed and λas variable, this density is the likelihood function. Our data had values\nx1 = 2, x2 = 3, x3 = 1, x4 = 3, x5 = 4.\nSo the likelihood and log likelihood functions with this data are\nf(2, 3, 1, 3, 4 | λ) = λ5e-13λ,\nln(f(2, 3, 1, 3, 4 | λ) = 5 ln(λ) -13λ\nFinally we use calculus to find the MLE:\nd\ndλ(log likelihood) = 5\nλ-13 = 0 ⇒\nλ= 5\n13 .\nNote: 1.\nIn this example we used an uppercase letter for a random variable and the\ncorresponding lowercase letter for the value it takes. This will be our usual practice.\n2. The MLE for λturned out to be the reciprocal of the sample mean\nx, so X∼exp( λ)\nsatisfies E[X] =\nx.\nThe following example illustrates how we can use the method of maximum likelihood to\nestimate multiple parameters at once.\nExample 4. Normal distributions\nSuppose the data x1, x2, ... , xnis drawn from a N(μ, σ2) distribution, where μand σare\nunknown. Find the maximum likelihood estimate for the pair (μ, σ2).\nSolution: Let's be precise and phrase this in terms of random variables and densities. Let\nuppercase X1, ... , Xnbe i.i.d. N(μ, σ2) random variables, and let lowercase xibe the value\nXitakes. The density for each Xiis\nfXi(xi) =\n√\n2πσ\ne-(xi-μ)2\n2σ2 .\nSince the Xiare independent their joint pdf is the product of the individual pdf's:\nf(x1, ... , xn| μ, σ) = (\n√\n2πσ\n)\nn\ne-∑\nn\ni=1\n(xi-μ)2\n2σ2 .\n\n18.05 Class 10, Maximum Likelihood Estimates , Spring 2022\nFor the fixed data x1, ... , xn, the likelihood and log likelihood are\nf(x1, ... , xn|μ, σ) = (\n√\n2πσ\n)\nn\ne-∑n\ni=1\n(xi-μ)2\n2σ2 ,\nln(f(x1, ... , xn|μ, σ)) = -nln(\n√\n2π)-nln(σ)-\nn\n∑\ni=1\n(xi-μ)2\n2σ2\n.\nSince ln(f(x1, ... , xn|μ, σ)) is a function of the two variables μ, σwe use partial derivatives\nto find the MLE. The easy value to find is\nμ:\n∂f(x1, ... , xn|μ, σ)\n∂μ\n=\nn\n∑\ni=1\n(xi-μ)\nσ2\n= 0 ⇒\nn\n∑\ni=1\nxi= nμ⇒\nμ= ∑\nn\ni=1 xi\nn\n= x.\nTo find\nσwe differentiate and solve for σ:\n∂f(x1, ... , xn|μ, σ)\n∂σ\n= -n\nσ+\nn\n∑\ni=1\n(xi-μ)2\nσ3\n= 0 ⇒\nσ2 = ∑\nn\ni=1(xi-μ)2\nn\n.\nWe already know\nμ= x, so we use that as the value for μin the formula for\nσ. We get the\nmaximum likelihood estimates\nμ\n= x\n= the mean of the data\nσ2\n=\nn\n∑\ni=1\nn(xi-\nμ)2 =\nn\n∑\ni=1\nn(xi-x)2\n= the unadjusted variance of the data.\n(Later we will learn that the sample variance is ∑\nn\ni=1(xi-\nμ)2\nn-1\n.)\nExample 5. Uniform distributions\nSuppose our data x1, ... xnare independently drawn from a uniform distribution U(a, b).\nFind the MLE for aand b.\nSolution: This example is different from the previous ones in that we won't use calculus\nto find the MLE. The density for U(a, b) is\nb-aon [a, b]. Therefore our likelihood function\nis\nf(x1, ... , xn| a, b) = {(\nb-a)\nn\nif all xiare in the interval [a, b]\notherwise.\nThis is maximized by making b-aas small as possible. The only restriction is that the\ninterval [a, b] must include all the data. Thus the MLE for the pair (a, b) is\na= min(x1, ... , xn)\nb= max(x1, ... , xn).\nExample 6. Capture/recapture method\nThe capture/recapture method is a way to estimate the size of a population in the wild.\nThe method assumes that each animal in the population is equally likely to be captured by\na trap.\nSuppose 10 animals are captured, tagged and released. A few months later, 20 animals are\ncaptured, examined, and released. 4 of these 20 are found to be tagged. Estimate the size\nof the wild population using the MLE for the probability that a wild animal is tagged.\n\n18.05 Class 10, Maximum Likelihood Estimates , Spring 2022\nSolution: Our unknown parameter nis the number of animals in the wild. Our data is\nthat 4 out of 20 recaptured animals were tagged (and that there are 10 tagged animals).\nThe likelihood function is\nP(data | nanimals) = (n-10\n16 )(10\n4 )\n( n\n20)\n(The numerator is the number of ways to choose 16 animals from among the n-10 untagged\nones times the number of was to choose 4 out of the 10 tagged animals. The denominator\nis the number of ways to choose 20 animals from the entire population of n.) We can use\nR to compute that the likelihood function is maximized when n= 50. This should make\nsome sense. It says our best estimate is that the fraction of all animals that are tagged is\n10/50 which equals the fraction of recaptured animals which are tagged.\nExample 7. Hardy-Weinberg. Suppose that a particular gene occurs as one of two\nalleles (Aand a), where allele Ahas frequency θin the population. That is, a random copy\nof the gene is Awith probability θand awith probability 1 -θ. Since a diploid genotype\nconsists of two genes, the probability of each genotype is given by:\ngenotype\nAA\nAa\naa\nprobability\nθ2\n2θ(1 -θ)\n(1 -θ)2\nSuppose we test a random sample of people and find that k1 are AA, k2 are Aa, and k3 are\naa. Find the MLE of θ.\nSolution: The likelihood function is given by\nP(k1, k2, k3 | θ) = (k1 + k2 + k3\nk1\n)(k2 + k3\nk2\n)(k3\nk3\n)θ2k1(2θ(1 -θ))k2(1 -θ)2k3.\nSo the log likelihood is given by\nconstant + 2k1 ln(θ) + k2 ln(θ) + k2 ln(1 -θ) + 2k3 ln(1 -θ)\nWe set the derivative equal to zero:\n2k1 + k2\nθ\n-k2 + 2k3\n1 -θ\n= 0\nSolving for θ, we find the MLE is\nθ=\n2k1 + k2\n2k1 + 2k2 + 2k3\n,\nwhich is simply the fraction of Aalleles among all the genes in the sampled population.\nWhy we use the density to find the MLE for continuous\ndistributions\nThe idea for the maximum likelihood estimate is to find the value of the parameter(s) for\nwhich the data has the highest probability. In this section we 'll see that we're doing this\n\n18.05 Class 10, Maximum Likelihood Estimates , Spring 2022\nis really what we are doing with the densities. We will do this by considering a smaller\nversion of the light bulb example.\nExample 8. Suppose we have two light bulbs whose lifetimes follow an exponential(λ)\ndistribution.\nSuppose also that we independently measure their lifetimes and get data\nx1 = 2 years and x2 = 3 years. Find the value of λthat maximizes the probability of this\ndata.\nSolution: The main paradox to deal with is that for a continuous distribution the proba-\nbility of a single value, say x1 = 2, is zero. We resolve this paradox by remembering that a\nsingle measurement really means a range of values, e.g. in this example we might check the\nlight bulb once a day. So the data x1 = 2 years really means x1 is somewhere in a range of\n1 day around 2 years.\nIf the range is small we call it dx1. The probability that X1 is in the range is approximated\nby fX1(x1|λ) dx1. This is illustrated in the figure below. The data value x2 is treated in\nexactly the same way.\nx\ndensity fX1(x1|λ)\nλ\nx1\ndx1\nprobability ≈fX1(x1|λ) dx1\nx\ndensity fX2(x2|λ)\nλ\nx2\ndx2\nprobability ≈fX2(x2|λ) dx2\nThe usual relationship between density and probability for small ranges.\nSince the data is collected independently the joint probability is the product of the individual\nprobabilities. Stated carefully\nP(X1 in range, X2 in range|λ) ≈fX1(x1|λ) dx1 ⋅fX2(x2|λ) dx2\nFinally, using the values x1 = 2 and x2 = 3 and the formula for an exponential pdf we have\nP(X1 in range, X2 in range|λ) ≈λe-2λdx1 ⋅λe-3λdx2 = λ2e-5λdx1 dx2.\nNow that we have a genuine probability we can look for the value of λthat maximizes it.\nLooking at the formula above we see that the factor dx1 dx2 will play no role in finding the\nmaximum. So for the MLE we drop it and simply call the density the likelihood:\nlikelihood = f(x1, x2|λ) = λ2e-5λ.\nThe value of λthat maximizes this is found just like in the example above. It is\nλ= 2/5.\nAppendix: Properties of the MLE\nFor the interested reader, we note several nice features of the MLE. These are quite technical\nand will not be on any exams.\n\n18.05 Class 10, Maximum Likelihood Estimates , Spring 2022\nThe MLE behaves well under transformations. That is, if\npis the MLE for pand gis a\none-to-one function, then g( p) is the MLE for g(p). For example, if\nσis the MLE for the\nstandard deviation σthen ( σ)2 is the MLE for the variance σ2.\nFurthermore, under some technical smoothness assumptions, the MLE is asymptotically\nunbiased and has asymptotically minimal variance. To explain these notions, note that\nthe MLE is itself a random variable since the data is random and the MLE is computed\nfrom the data. Let x1, x2, ... be an infinite sequence of samples from a distribution with\nparameter p. Let\npnbe the MLE for pbased on the data x1, ... , xn.\nAsymptotically unbiased means that as the amount of data grows, the mean of the MLE\nconverges to p. In symbols: E[ pn] →pas n→inf. Of course, we would like the MLE to be\nclose to pwith high probability, not just on average, so the smaller the variance of the MLE\nthe better. Asymptotically minimal variance means that as the amount of data grows, the\nMLE has the minimal variance among all unbiased estimators of p. In symbols: for any\nunbiased estimator\npnand ε> 0 we have that Var( pn) + ε> Var( pn) as n→inf.\n\nBayesian Updating with Discrete Priors\nClass 11, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Be able to apply Bayes' theorem to compute probabilities.\n2. Be able to define and to identify the roles of prior probability, likelihood (Bayes term),\nposterior probability, data and hypothesis in the application of Bayes' Theorem.\n3. Be able to use a Bayesian update table to compute posterior probabilities.\nReview of Bayes' theorem\nRecall that Bayes' theorem allows us to 'invert' conditional probabilities. If Hand Dare\nevents, then:\nP(H| D) = P(D| H)P(H)\nP(D)\nOur view is that Bayes' theorem forms the foundation for inferential statistics. We will\nbegin to justify this view today.\n2.1\nThe base rate fallacy\nWhen we first learned Bayes' theorem we worked an example about screening tests showing\nthat P(D|H) can be very different from P(H|D).\nIn the appendix we work a similar\nexample. If you are not comfortable with Bayes' theorem you should read the example in\nthe appendix now.\nTerminology and Bayes' theorem in tabular form\nWe now use a coin tossing problem to introduce terminology and a tabular format for Bayes'\ntheorem. This will provide a simple, uncluttered example that shows our main points.\nExample 1. There are three types of coins which have different probabilities of landing\nheads when tossed.\n- Type Acoins are fair,\nwith probability 0.5 of heads\n- Type Bcoins are bent and have probability 0.6 of heads\n- Type Ccoins are bent and have probability 0.9 of heads\nSuppose I have a drawer containing 5 coins: 2 of type A, 2 of type B, and 1 of type C. I\nreach into the drawer and pick a coin at random. Without showing you the coin I flip it\nonce and get heads. What is the probability it is type A? Type B? Type C?\n\n18.05 Class 11, Bayesian Updating with Discrete Priors, Spring 2022\nSolution: Let A, B, and Cbe the event that the chosen coin was type A, type B, and\ntype C. Let Dbe the event that the toss is heads. The problem asks us to find\nP(A|D),\nP(B|D),\nP(C|D).\nBefore applying Bayes' theorem, let's introduce some terminology.\n- Experiment: pick a coin from the drawer at random, flip it, and record the result.\n- Data: the result of our experiment. In this case the event D= 'heads'. We think of\nDas data that provides evidence for or against each hypothesis.\n- Hypotheses: we are testing three hypotheses: the coin is type A, Bor C.\n- Prior probability: the probability of each hypothesis prior to tossing the coin (collect-\ning data). Since the drawer has 2 coins of type A, 2 of type Band 1 of type Cwe\nhave\nP(A) = 0.4,\nP(B) = 0.4,\nP(C) = 0.2.\n- Likelihood: (This is the same likelihood we used for the MLE.) The likelihood function\nis P(D|H), i.e., the probability of the data assuming that the hypothesis is true. Most\noften we will consider the data as fixed and let the hypothesis vary. For example,\nP(D|A) = probability of heads if the coin is type A. In our case the likelihoods are\nP(D|A) = 0.5,\nP(D|B) = 0.6,\nP(D|C) = 0.9.\nThe name likelihood is so well established in the literature that we have to teach\nit to you. However in colloquial language likelihood and probability are synonyms.\nThis leads to the likelihood function often being confused with the probability of a\nhypothesis. Because of this we'd prefer to use the name Bayes' term. However since\nwe are stuck with 'likelihood' we will try to use it very carefully and in a way that\nminimizes any confusion.\n- Posterior probability: the probability (posterior to) of each hypothesis given the data\nfrom tossing the coin.\nP(A|D),\nP(B|D),\nP(C|D).\nThese posterior probabilities are what the problem asks us to find.\nWe now use Bayes' theorem to compute each of the posterior probabilities. We are going\nto write this out in complete detail so we can pick out each of the parts (Remember that\nthe data Dis that the toss was heads.)\nFirst we organize the probabilities into a tree:\nA\nB\nC\nH\nT\nH\nT\nH\nT\n0.4\n0.4\n0.2\n0.5\n0.5\n0.6\n0.4\n0.9\n0.1\nProbability tree for choosing and tossing a coin.\n\n18.05 Class 11, Bayesian Updating with Discrete Priors, Spring 2022\nBayes' theorem says, e.g. P(A|D) = P(D|A)P(A)\nP(D)\n. The denominator P(D) is computed\nusing the law of total probability:\nP(D) = P(D|A)P(A)+P(D|B)P(B)+P(D|C)P(C) = 0.5⋅0.4+0.6⋅0.4+0.9⋅0.2 = 0.62.\nNow each of the three posterior probabilities can be computed:\nP(A|D) = P(D|A)P(A)\nP(D)\n= 0.5 ⋅0.4\n0.62\n= 0.2\n0.62\nP(B|D) = P(D|B)P(B)\nP(D)\n= 0.6 ⋅0.4\n0.62\n= 0.24\n0.62\nP(C|D) = P(D|C)P(C)\nP(D)\n= 0.9 ⋅0.2\n0.62\n= 0.18\n0.62\nNotice that the total probability P(D) is the same in each of the denominators and that it\nis the sum of the three numerators. We can organize all of this very neatly in a Bayesian\nupdate table:\nhypothesis\nprior\nlikelihood\nBayes numerator\nposterior\n(numerator/P(D))\nH\nP(H)\nP(D|H)\nP(D|H)P(H)\nP(H|D)\nA\n0.4\n0.5\n0.2\n0.3226\nB\n0.4\n0.6\n0.24\n0.3871\nC\n0.2\n0.9\n0.18\n0.2903\ntotal\nNO SUM\nP(D) = 0.62\nThe Bayes numerator is the product of the prior and the likelihood. We see in each of the\nBayes' formula computations above that the posterior probability is obtained by dividing\nthe Bayes numerator by P(D) = 0.62. We also see that the law of law of total probability\nsays that P(D) is the sum of the entries in the Bayes numerator column.\nBayesian updating: The process of going from the prior probability P(H) to the pos-\nterior P(H|D) is called Bayesian updating. Bayesian updating uses the data to alter our\nunderstanding of the probability of each of the possible hypotheses.\n3.1\nImportant things to notice\n1. There are two types of probabilities: Type one is the standard probability of data, e.g.\nthe probability of heads is p= 0.9. Type two is the probability of the hypotheses, e.g.\nthe probability the chosen coin is type A, Bor C. This second type has prior (before\nthe data) and posterior (after the data) values.\n2. The posterior (after the data) probabilities for each hypothesis are in the last column.\nWe see that coin Bis now the most probable, though its probability has decreased from\na prior probability of 0.4 to a posterior probability of 0.39. Meanwhile, the probability\nof type Chas increased from 0.2 to 0.29.\n3. The Bayes numerator column determines the posterior probability column. To compute\nthe latter, we simply divided each numerator by P(D), i.e. rescaled the Bayes numerators\nso that they sum to 1.\n\n18.05 Class 11, Bayesian Updating with Discrete Priors, Spring 2022\n4. If all we care about is finding the most likely hypothesis, the Bayes numerator works as\nwell as the normalized posterior.\n5. The likelihood column does not sum to 1. The likelihood function is not a probability\nfunction.\n6. The posterior probability represents the outcome of a 'tug-of-war' between the likelihood\nand the prior. When calculating the posterior, a large prior may be deflated by a small\nlikelihood, and a small prior may be inflated by a large likelihood.\n7. The maximum likelihood estimate (MLE) for Example 1 is hypothesis C, with a likeli-\nhood P(D|C) = 0.9. The MLE is useful, but you can see in this example that it is not\nthe entire story, since type Bhas the greatest posterior probability.\nTerminology in hand, we can express Bayes' theorem in various ways:\nP(H|D) = P(D|H)P(H)\nP(D)\nP(hypothesis|data) = P(data|hypothesis)P(hypothesis)\nP(data)\nWith the data fixed, the denominator P(D) just serves to normalize the total posterior\nprobability to 1. So we can also express Bayes' theorem as a statement about the propor-\ntionality of two functions of H(i.e, of the last two columns of the table).\nP(hypothesis|data) ∝P(data|hypothesis)P(hypothesis)\nThis leads to the most elegant form of Bayes' theorem in the context of Bayesian updating:\nposterior ∝likelihood × prior\n3.2\nPrior and posterior probability mass functions\nEarlier in the course we saw that it is convenient to use random variables and probability\nmass functions. To do this we had to assign values to events (head is 1 and tails is 0). We\nwill do the same thing in the context of Bayesian updating.\nOur standard notations will be:\n- θis the value of the hypothesis.\n- p(θ) is the prior probability mass function of the hypothesis.\n- p(θ|D) is the posterior probability mass function of the hypothesis given the data.\n- p(D|θ) is the likelihood function. (This is not a pmf!)\nIn Example 1 we can represent the three hypotheses A, B, and Cby θ= 0.5, 0.6, 0.9. For\nthe data we'll let x= 1 mean heads and x= 0 mean tails. Then the prior and posterior\nprobabilities in the table define the prior and posterior probability mass functions.\n\n18.05 Class 11, Bayesian Updating with Discrete Priors, Spring 2022\nHypothesis\nθ\nprior pmf p(θ)\nposterior pmf p(θ|x= 1)\nA\n0.5\nP(A) = p(0.5) = 0.4\nP(A|D) = p(0.5|x= 1) = 0.3226\nB\n0.6\nP(B) = p(0.6) = 0.4\nP(B|D) = p(0.6|x= 1) = 0.3871\nC\n0.9\nP(C) = p(0.9) = 0.2\nP(C|D) = p(0.9|x= 1) = 0.2903\nHere are plots of the prior and posterior pmf's from the example.\nθ\np(θ)\n.5 .6\n.9\n.2\n.4\nθ\np(θ|x= 1)\n.5 .6\n.9\n.2\n.4\nPrior pmf p(θ) and posterior pmf p(θ|x= 1) for Example 1\nIf the data was different then the likelihood column in the Bayesian update table would be\ndifferent. We can plan for different data by building the entire likelihood table ahead of\ntime. In the coin example there are two possibilities for the data: the toss is heads or the\ntoss is tails. So the full likelihood table has two likelihood columns:\nhypothesis\nlikelihood p(x|θ)\nθ\np(x= 0|θ)\np(x= 1|θ)\n0.5\n0.5\n0.5\n0.6\n0.4\n0.6\n0.9\n0.1\n0.9\nImportant convention. Notice that in the above table we used the value of θas the\nhypothesis. Of course, hypothesizing 'θ= 0.5' is exactly the same as hypothesizing 'the coin\nis type A'. It is also useful in settings where we haven't named all the possible hypotheses.\nExample 2. Using the notation p(θ), etc., redo Example 1 assuming the flip was tails.\nSolution: Since the data has changed, the likelihood column in the Bayesian update table\nis now for x= 0. That is, we must take the p(x= 0|θ) column from the likelihood table.\nBayes\nhypothesis\nprior\nlikelihood\nnumerator\nposterior\nθ\np(θ)\np(x= 0 | θ)\np(x= 0 | θ)p(θ)\np(θ| x= 0)\n0.5\n0.4\n0.5\n0.2\n0.5263\n0.6\n0.4\n0.4\n0.16\n0.4211\n0.9\n0.2\n0.1\n0.02\n0.0526\ntotal\nNO SUM\n0.38\nNow the probability that θ= 0.5, i.e. the coin is type A, has increased from 0.4 to 0.5263,\nwhile the probability that θ= 0.9, i.e the coin is type C, has decreased from 0.2 to only\n0.0526. Here are the corresponding plots:\n\n18.05 Class 11, Bayesian Updating with Discrete Priors, Spring 2022\nθ\np(θ)\n.5 .6\n.9\n.2\n.4\nθ\np(θ|x= 0)\n.5 .6\n.9\n.2\n.4\nPrior pmf p(θ) and posterior pmf p(θ|x= 0)\n3.3\nFood for thought.\nSuppose that in Example 1 you didn't know how many coins of each type were in the\ndrawer. You picked one at random and got heads. How would you go about deciding which\nhypothesis (coin type) if any was most supported by the data?\nUpdating again and again\nIn life we are continually updating our beliefs with each new experience of the world. In\nBayesian inference, after updating the prior to the posterior, we can take more data and\nupdate again! For the second update, the posterior from the first data becomes the prior\nfor the second data.\nExample 3. Suppose you have picked a coin as in Example 1. You flip it once and get\nheads. Then you flip the same coin and get heads again. What is the probability that the\ncoin was type A? Type B? Type C?\nSolution: As we update several times the table gets big, so we use a smaller font to fit it\nin:\nBayes\nBayes\nhypothesis\nprior\nlikelihood 1\nnumerator 1\nlikelihood 2\nnumerator 2\nposterior 2\nθ\np(θ)\np(x1 = 1|θ)\np(x1 = 1|θ)p(θ)\np(x2 = 1|θ)\np(x2 = 1|θ)p(x1 = 1|θ)p(θ)\np(θ|x1 = 1, x2 = 1)\n0.5\n0.4\n0.5\n0.2\n0.5\n0.1\n0.2463\n0.6\n0.4\n0.6\n0.24\n0.6\n0.144\n0.3547\n0.9\n0.2\n0.9\n0.18\n0.9\n0.162\n0.3990\ntotal\nNO SUM\nNO SUM\n0.406\nNote that the second Bayes numerator is computed by multiplying the first Bayes numerator\nand the second likelihood; since we are only interested in the final posterior, there is no\nneed to normalize until the last step. As shown in the last column and plot, after two heads\nthe type C hypothesis has finally taken the lead!\n\n18.05 Class 11, Bayesian Updating with Discrete Priors, Spring 2022\nθ\np(θ)\n.5 .6\n.9\n.2\n.4\nθ\np(θ|x= 1)\n.5 .6\n.9\n.2\n.4\nθ\np(θ|x1 = 1, x2 = 1)\n.5 .6\n.9\n.2\n.4\nprior p(θ),\nfirst posterior p(θ|x1 = 1),\nsecond posterior p(θ|x1 = 1, x2 = 1)\nAppendix: the base rate fallacy\nExample 4. A screening test for a disease is both sensitive and specific. By that we mean\nit is usually positive when testing a person with the disease and usually negative when\ntesting someone without the disease. Let's assume the true positive rate is 99% and the\nfalse positive rate is 2%. Suppose the prevalence of the disease in the general population is\n0.5%. If a random person tests positive, what is the probability that they have the disease?\nSolution: As a review we first do the computation using trees. Next we will redo the\ncomputation using tables.\nLet's use notation established above for hypotheses and data: let H+ be the hypothesis\n(event) that the person has the disease and let H-be the hypothesis they do not. Likewise,\nlet T+ and T-represent the data of a positive and negative screening test respectively. We\nare asked to compute P(H+|T+).\nWe are given\nP(T+|H+) = 0.99,\nP(T+|H-) = 0.02,\nP(H+) = 0.005.\nFrom these we can compute the false negative and true negative rates:\nP(T-|H+) = 0.01,\nP(T-|H-) = 0.98\nAll of these probabilities can be displayed quite nicely in a tree.\nH-\nH+\nT+\nT-\nT+\nT-\n0.005\n0.995\n0.99\n0.01\n0.02\n0.98\nBayes' theorem yields\nP(H+|T+) = P(T+|H+)P(H+)\nP(T+)\n=\n0.99 ⋅0.005\n0.99 ⋅0.005 + 0.02 ⋅0.995 = 0.19920 ≈20%\nNow we redo this calculation using a Bayesian update table:\n\n18.05 Class 11, Bayesian Updating with Discrete Priors, Spring 2022\nBayes\nhypothesis\nprior\nlikelihood\nnumerator\nposterior\nH\nP(H)\nP(T+|H)\nP(T+|H)P(H)\nP(H|T+)\nH+\n0.005\n0.99\n0.00495\n0.19920\nH-\n0.995\n0.02\n0.01990\n0.80080\ntotal\nNO SUM\nP(T+) = 0.02485\nThe table shows that the posterior probability P(H+|T+) that a person with a positive\ntest has the disease is about 20%. This is far less than the sensitivity of the test (99%) but\nmuch higher than the prevalence of the disease in the general population (0.5%).\n\nBayesian Updating: Probabilistic Prediction\nClass 12, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Be able to use the law of total probability to compute prior and posterior predictive\nprobabilities.\nIntroduction\nIn the previous class we looked at updating the probability of hypotheses based on data.\nWe can also use the data to update the probability of each possible outcome of a future\nexperiment. In this class we will look at how this is done.\n2.1\nProbabilistic prediciton; words of estimative probability (WEP)\nThere are many ways to word predictions:\n- Prediction: \"It will rain tomorrow.\"\n- Prediction using words of estimative probability (WEP): \"It is likely to rain tomor-\nrow.\"\n- Probabilistic prediction: \"Tomorrow it will rain with probability 60% (and not rain\nwith probability 40%).\"\nEach type of wording is appropriate at different times.\nIn this class we are going to focus on probabilistic prediction and precise quantitative state-\nments. You can see https://en.wikipedia.org/wiki/Words_of_Estimative_Probability\nfor an interesting discussion about the appropriate use of words of estimative probability.\nThe article also contains a list of weasel words such as 'might', 'cannot rule out', 'it's\nconceivable' that should be avoided as almost certain to cause confusion.\nThere are many places where we want to make a probabilistic prediction. Examples are\n- Medical treatment outcomes\n- Weather forecasting\n- Climate change\n- Sports betting\n- Elections\n- ...\n\n18.05 Class 12, Bayesian Updating: Probabilistic Prediction, Spring 2022\nThese are all situations where there is uncertainty about the outcome and we would like as\nprecise a description of what could happen as possible.\nPredictive Probabilities\nProbabilistic prediction simply means assigning a probability to each possible outcomes of\nan experiment.\nRecall the coin example from the previous class notes: there are three types of coins which\nare indistinguishable apart from their probability of landing heads when tossed.\n- Type Acoins are fair, with probability 0.5 of heads\n- Type Bcoins have probability 0.6 of heads\n- Type Ccoins have probability 0.9 of heads\nYou have a drawer containing 4 coins: 2 of type A, 1 of type B, and 1 of type C. You reach\ninto the drawer and pick a coin at random. We let Astand for the event 'the chosen coin\nis of type A'. Likewise for Band C.\n3.1\nPrior predictive probabilities\nBefore taking data we can compute the probability that our chosen coin will land heads (or\ntails) if flipped. Let DHbe the event it lands heads and let DTthe event it lands tails. We\ncan use the law of total probability to determine the probabilities of these events. Either\nby drawing a tree or directly proceeding to the algebra, we get:\nA\nB\nC\nDH\nDT\nDH\nDT\nDH\nDT\n0.5\n0.25\n0.25\n0.5\n0.5\n0.6\n0.4\n0.9\n0.1\nCoin type\nFlip result\nP(DH) = P(DH|A)P(A) + P(DH|B)P(B) + P(DH|C)P(C)\n= 0.5 ⋅0.5 + 0.6 ⋅0.25 + 0.9 ⋅0.25 = 0.625\nP(DT) = P(DT|A)P(A) + P(DT|B)P(B) + P(DT|C)P(C)\n= 0.5 ⋅0.5 + 0.4 ⋅0.25 + 0.1 ⋅0.25 = 0.375\nDefinition: These probabilities give a (probabilistic) prediction of what will happen if the\ncoin is tossed. Because they are computed before we collect any data they are called prior\npredictive probabilities.\n3.2\nPosterior predictive probabilities\nSuppose we flip the coin once and it lands heads. We now have data D, which we can use\nto update the prior probabilities of our hypotheses to posterior probabilities. Last class we\nlearned to use a Bayes table to facilitate this computation:\n\n18.05 Class 12, Bayesian Updating: Probabilistic Prediction, Spring 2022\nBayes\nhypothesis\nprior\nlikelihood\nnumerator\nposterior\nH\nP(H)\nP(D|H)\nP(D|H)P(H)\nP(H|D)\nA\n0.5\n0.5\n0.25\n0.4\nB\n0.25\n0.6\n0.15\n0.24\nC\n0.25\n0.9\n0.225\n0.36\ntotal\nNO SUM\n0.625\nHaving flipped the coin once and gotten heads, we can compute the probability that our\nchosen coin will land heads (or tails) if flipped a second time. We proceed just as before, but\nusing the posterior probabilities P(A|D), P(B|D), P(C|D) in place of the prior probabilities\nP(A), P(B), P(C).\nA\nB\nC\nDH\nDT\nDH\nDT\nDH\nDT\n0.4\n0.24\n0.36\n0.5\n0.5\n0.6\n0.4\n0.9\n0.1\nCoin type\nFlip result\nP(DH|D) = P(DH|A)P(A|D) + P(DH|B)P(B|D) + P(DH|C)P(C|D)\n= 0.5 ⋅0.4 + 0.6 ⋅0.24 + 0.9 ⋅0.36 = 0.668\nP(DT|D) = P(DT|A)P(A|D) + P(DT|B)P(B|D) + P(DT|C)P(C|D)\n= 0.5 ⋅0.4 + 0.4 ⋅0.24 + 0.1 ⋅0.36 = 0.332\nDefinition: These probabilities give a (probabilistic) prediction of what will happen if the\ncoin is tossed again. Because they are computed after collecting data and updating the\nprior to the posterior, they are called posterior predictive probabilities.\nNote that heads on the first toss increases the probability of heads on the second toss.\n3.3\nReview\nHere's a succinct description of the preceding sections that may be helpful:\nEach hypothesis gives a different probability of heads, so the total probability of heads is\na weighted average. For the prior predictive probability of heads, the weights are given by\nthe prior probabilities of the hypotheses. For the posterior predictive probability of heads,\nthe weights are given by the posterior probabilities of the hypotheses.\nRemember: Prior and posterior probabilities are for hypotheses. Prior predictive and\nposterior predictive probabilities are for outcomes. To keep this straight, remember that\nthe predictive probabilities are used to predict future outcomes, i.e. data.\n\nBayesian Updating: Odds\nClass 12, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Be able to convert between odds and probability.\n2. Be able to update prior odds to posterior odds using Bayes factors.\n3. Understand how Bayes factors measure the extent to which data provides evidence for\nor against a hypothesis.\nOdds\nWhen comparing two events, it common to phrase probability statements in terms of odds.\nDefinition The odds of event Eversus event E′ are the ratio of their probabilities P(E)/P(E′).\nIf unspecified, the second event is assumed to be the complement Ec. So the odds of Eare:\nO(E) = P(E)\nP(Ec).\nFor example, O(rain) = 2 means that the probability of rain is twice the probability of no\nrain (2/3 versus 1/3). We might say 'the odds of rain are 2 to 1.'\nExample. For a fair coin, O(heads) = 1/2\n1/2 = 1. We might say the odds of heads are 1 to\n1 or fifty-fifty.\nExample. For a standard die, the odds of rolling a 4 are 1/6\n5/6 = 1\n5. We might say the odds\nare '1 to 5 for' or '5 to 1 against' rolling a 4.\nExample. The probability of a pair in a five card poker hand is 0.42257. So the odds of a\npair are 0.42257/(1-0.42257) = 0.73181.\nWe can go back and forth between probability and odds as follows.\nConversion formulas: if P(E) = pthen O(E) =\np\n1 -p. If O(E) = qthen P(E) =\nq\n1 + q.\nNotes:\n1. The second formula simply solves q= p/(1 -p) for p.\n2. Probabilities are between 0 and 1, while odds are between 0 to inf.\n3. The property P(Ec) = 1 -P(E) becomes O(Ec) = 1/O(E).\nExample. Let Fbe the event that a five card poker hand is a full house. Then P(F) =\n0.00145214 so O(F) = 0.0014521/(1 -0.0014521) = 0.0014542.\nThe odds not having a full house are O(Fc) = (1 -0.0014521)/0.0014521 = 687 = 1/O(F).\n\n18.05 Class 12, Bayesian Updating: Odds, Spring 2022\n4. If P(E) or O(E) is small then O(E) ≈P(E). This follows from the conversion formulas.\nExample. In the poker example where F= 'full house' we saw that P(F) and O(F) differ\nonly in the fourth significant digit.\nUpdating odds\n3.1\nIntroduction\nIn Bayesian updating, we used the likelihood of data to update prior probabilities of hy-\npotheses to posterior probabilities. In the language of odds, we will update prior odds to\nposterior odds. One of our key points will be that the data can provide evidence supporting\nor negating a hypothesis depending on whether its posterior odds are greater or less than\nits prior odds.\nWe'll begin by returning to our familiar example of a screening test for a disease.\nExample 1. Briefly, a screening test for a disease is both sensitive and specific. Assume\nthe true positive rate is 99% and the false positive rate is 2%. Suppose the prevalence of the\ndisease in the general population is 0.5%. For a randomly chosen person, what are the prior\nodds that they have the disease? Suppose they test positive, now what are the posterior\nodds that they have the disease? By what factor have the odds changed as a result of the\ntest?\nSolution: We'll use our, by now, standard notation:\nH+ = have disease, H-= do not have disease, T+ = test positive, T-= test negative.\nTo start with the prior odds that they have the disease are\nO(H+) = P(H+)\nP(H-) = 0.005\n0.995 ≈0.005\nFor the posterior odds, we'll do the computation with trees and then repeat it with tables.\nHere is the tree describing the scenario.\nH-\nH+\nT+\nT-\nT+\nT-\n0.005\n0.995\n0.99\n0.01\n0.02\n0.98\nBayes' theorem yields\nO(H+|T+) = P(H+|T+)\nP(H-|T+) = P(T+|H+)P(H+)/P(T+)\nP(T+|H-)P(H-)/P(T+) = P(T+|H+)P(H+)\nP(T+|H-)P(H-)\nThis is great! For the odds, the total probability P(T+) cancels and does not need to be\ncomputed. Putting in numbers we see the posterior odds are\nO(H+|T+) = 0.99 ⋅0.005\n0.02 ⋅0.995 ≈50 ⋅0.005 = 1/4.\n\n18.05 Class 12, Bayesian Updating: Odds, Spring 2022\nWe've structured the presentation so you can easily see that the posterior odds are only one\nin four. However, the posterior odds are about a factor of 50 greater than the prior odds.\nRedoing this calculation using a Bayesian update table:\nBayes\nhypothesis\nprior\nlikelihood\nnumerator\nposterior\nH\nP(H)\nP(T+|H)\nP(T+|H)P(H)\nP(H|T+)\nH+\n0.005\n0.99\n0.00495\n0.19920\nH-\n0.995\n0.02\n0.01990\n0.80080\ntotal\nNO SUM\nP(T+) = 0.02485\nThe prior odds are computed using the prior column of the table.\nAs above, they are\nP(H+)\nP(H-) = 0.005\n0.995.\nThe posterior odds are computed using either the posterior or Bayes numerator columns of\nthe table. We can use either column, because, they only differ by the normalazing factor of\nP(T+) in the denominator of the posteriors. We get the same answer as above:\nO(H+|T+) = 0.00495\n0.01990 ≈5\n20.\nYou should see that these odds come by multiplying the prior odds by the ratio of the\nlikelihoods.\n3.2\nExample: Marfan syndrome\nMarfan syndrome is a genetic disease of connective tissue that occurs in 1 of every 15000\npeople. The main ocular features of Marfan syndrome include bilateral ectopia lentis (lens\ndislocation), myopia and retinal detachment. About 70% of people with Marfan syndrome\nhave a least one of these ocular features; only 7% of people without Marfan syndrome do.\n(We don't guarantee the accuracy of these numbers, but they will work perfectly well for\nour example.)\nIf a person has at least one of these ocular features, what are the odds that they have\nMarfan syndrome?\nSolution: This is a standard Bayesian updating problem. Our hypotheses are:\nM= 'the person has Marfan syndrome'\nMc= 'the person does not have Marfan\nsyndrome'\nThe data is:\nF= 'the person has at least one ocular feature'.\nWe are given the prior probability of Mand the likelihoods of Fgiven Mor Mc:\nP(M) = 1/15000,\nP(F|M) = 0.7,\nP(F|Mc) = 0.07.\nAs before, we can compute the posterior probabilities using a table:\n\n18.05 Class 12, Bayesian Updating: Odds, Spring 2022\nBayes\nhypothesis\nprior\nlikelihood\nnumerator\nposterior\nH\nP(H)\nP(F|H)\nP(F|H)P(H)\nP(H|F)\nM\n0.000067\n0.7\n0.0000467\n0.00066\nMc\n0.999933\n0.07\n0.069995\n0.99933\ntotal\nno sum\nP(F) = 0.07004\nFirst we find the prior odds:\nO(M) = P(M)\nP(Mc) =\n1/15000\n14999/15000 =\n14999 ≈0.000067.\nThe posterior odds are given by the ratio of the posterior probabilities or the Bayes numer-\nators, since the normalizing factor will be the same in both numerator and denominator.\nO(M|F) = P(M|F)\nP(Mc|F) = P(F|M)P(M)\nP(F|Mc)P(Mc) = 0.000667.\nThe posterior odds are a factor of 10 larger than the prior odds. In that sense, having an\nocular feature is strong evidence in favor of the hypothesis M. However, because the prior\nodds are so small, it is still highly unlikely the person has Marfan syndrome.\nBayes factors and strength of evidence\nThe factor of 10 in the previous example is called a Bayes factor or a likelihood ratio. The\nexact definition is the following.\nDefinition: For a hypothesis Hand data D, the Bayes factor is the ratio of the likelihoods:\nBayes factor = P(D|H)\nP(D|Hc).\nThis is also called the likelihood ratio.\nLet's see exactly where the Bayes factor arises in updating odds. We have\nO(H|D) = P(H|D)\nP(Hc|D)\n= P(D|H) P(H)\nP(D|Hc)P(Hc)\n= P(D|H)\nP(D|Hc) ⋅P(H)\nP(Hc)\n= P(D|H)\nP(D|Hc) ⋅O(H)\nposterior odds = Bayes factor × prior odds\nFrom this formula, we see that the Bayes' factor (BF) tells us whether the data provides\nevidence for or against the hypothesis.\n\n18.05 Class 12, Bayesian Updating: Odds, Spring 2022\n- If BF> 1 then the posterior odds are greater than the prior odds.\nSo the data\nprovides evidence for the hypothesis.\n- If BF< 1 then the posterior odds are less than the prior odds. So the data provides\nevidence against the hypothesis.\n- If BF= 1 then the prior and posterior odds are equal.\nSo the data provides no\nevidence either way.\nThe following example is taken from the textbook Information Theory, Inference, and\nLearning Algorithms by David J. C. Mackay, who has this to say regarding trial evidence.\nIn my view, a jury's task should generally be to multiply together carefully\nevaluated likelihood ratios from each independent piece of admissible evidence\nwith an equally carefully reasoned prior probability. This view is shared by many\nstatisticians but learned British appeal judges recently disagreed and actually\noverturned the verdict of a trial because the jurors had been taught to use Bayes'\ntheorem to handle complicated DNA evidence.\nExample 2. Two people have left traces of their own blood at the scene of a crime. A\nsuspect , Oliver, is tested and found to have type 'O' blood. The blood groups of the two\ntraces are found to be of type 'O' (a common type in the local population, having frequency\n60%) and type 'AB' (a rare type, with frequency 1%). Does this data (type 'O' and 'AB'\nblood were found at the scene) give evidence in favor of the proposition that Oliver was one\nof the two people present at the scene of the crime?\"\nSolution: There are two hypotheses:\nS= 'Oliver and another unknown person were at the scene of the crime\nSc= 'two unknown people were at the scene of the crime'\nThe data is:\nD= 'type 'O' and 'AB' blood were found'\nThe Bayes factor for Oliver's presence is BFOliver = P(D|S)\nP(D|Sc). We compute the numerator\nand denominator of this separately.\nThe data says that both type O and type AB blood were found. If Oliver was at the scene\nthen 'type O' blood would be there. So P(D|S) is the probability that the other person\nhad type AB blood. We are told this is 0.01, so P(D|S) = 0.01.\nIf Oliver was not at the scene then there were two random people one with type O and\none with type AB blood. The probability of this is 2 ⋅0.6 ⋅0.01.* Thus the Bayes factor for\nOliver's presence is\nBFOliver = P(D|S)\nP(D|Sc) =\n0.01\n2 ⋅0.6 ⋅0.01 = 0.83.\nSince BFOliver < 1, the data provides (weak) evidence against Oliver being at the scene.\n*The factor of 2 is, perhaps surprising. The following careful counting will explain it. Suppose there\nare Npeople in the population, NOhave type O blood and NABhave type AB. So NO/N= 0.6\n\n18.05 Class 12, Bayesian Updating: Odds, Spring 2022\nand NAB/N= 0.01. We want the probability that a random choice of 2 people will pick one of type\nO and one of type AB. This is clearly\n(NO\n1 )(NAB\n1 )\n(N\n2)\n=\nNONAB\nN(N-1)/2 = 2 ⋅NO\nN⋅NAB\nN-1 ≈2 ⋅0.6 ⋅0.01.\nIn the last approximation, we assumed that Nis large enough the NAB/(N-1) ≈NAB/N.\nExample 3. Another suspect Alberto is found to have type 'AB' blood. Do the same data\ngive evidence in favor of the proposition that Alberto was one of the two people present at\nthe crime?\nSolution: Reusing the above notation with Alberto in place of Oliver we have:\nBFAlberto = P(D|S)\nP(D|Sc) =\n0.6\n2 ⋅0.6 ⋅0.01 = 50.\nSince BFAlberto ≫1, the data provides strong evidence in favor of Alberto being at the\nscene.\nNotes:\n1. In both examples, we have only computed the Bayes factor, not the posterior odds. To\ncompute the latter, we would need to know the prior odds that Oliver (or Alberto) was at\nthe scene based on other evidence.\n2. Note that if 50% of the population had type O blood instead of 60%, then the Oliver's\nBayes factor would be 1 (neither for nor against). More generally, the break-even point\nfor blood type evidence is when the proportion of the suspect's blood type in the general\npopulation equals the proportion of the suspect's blood type among those who left blood\nat the scene.\n4.1\nUpdating again and again\nSuppose we collect data in two stages, first D1, then D2. We have seen in our dice and coin\nexamples that the final posterior can be computed all at once or in two stages where we\nfirst update the prior using the likelihoods for D1 and then update the resulting posterior\nusing the likelihoods for D2. The latter approach works whenever likelihoods multiply:\nP(D1, D2|H) = P(D1|H)P(D2|H).\nSince likelihoods are conditioned on hypotheses, we say that D1 and D2 are conditionally\nindependent if the above equation holds for every hypothesis H.\nExample. There are five dice in a drawer, with 4, 6, 8, 12, and 20 sides (these are the\nhypotheses). I pick a die at random and roll it twice. The first roll gives 7. The second roll\ngives 11. Are these results conditionally independent? Are they independent?\nSolution: These results are conditionally independent. For example, for the hypothesis of\nthe 8-sided die we have:\nP(7 on roll 1 | 8-sided die) = 1/8\nP(11 on roll 2 | 8-sided die) = 0\nP(7 on roll 1, 11 on roll 2 | 8-sided die) = 0\n\n18.05 Class 12, Bayesian Updating: Odds, Spring 2022\nFor the hypothesis of the 20-sided die we have:\nP(7 on roll 1 | 20-sided die) = 1/20\nP(11 on roll 2 | 20-sided die) = 1/20\nP(7 on roll 1, 11 on roll 2 | 20-sided die) = (1/20)2\nHowever, the results of the rolls are not independent. That is:\nP(7 on roll 1, 11 on roll 2) =P(7 on roll 1)P(11 on roll 2).\nIntuitively, this is because a 7 on the roll 1 allows us to rule out the 4- and 6-sided dice,\nmaking an 11 on roll 2 more likely. Let's check this intuition by computing both sides\nprecisely. On the righthand side we have:\nP(7 on roll 1) = 1\n5 ⋅1\n8 + 1\n5 ⋅1\n12 + 1\n5 ⋅1\n20 = 31\nP(11 on roll 2) = 1\n5 ⋅1\n12 + 1\n5 ⋅1\n20 = 2\nOn the lefthand side we have:\nP(7 on roll 1, 11 on roll 2) = P(11 on roll 2 | 7 on roll 1)P(7 on roll 1)\n= (30\n93 ⋅1\n12 + 6\n31 ⋅1\n20) ⋅31\n= 17\n465 ⋅31\n600 =\nHere 30\n93 and\n31 are the posterior probabilities of the 12- and 20-sided dice given a 7 on roll\n1. We conclude that, without conditioning on hypotheses, the rolls are not independent.\nReturning the to general setup, if D1 and D2 are conditionally independent for Hand Hc\nthen it makes sense to consider each Bayes factor independently:\nBFi= P(Di|H)\nP(Di|Hc).\nThe prior odds of Hare O(H). The posterior odds after D1 are\nO(H|D1) = BF1 ⋅O(H).\nAnd the posterior odds after D1 and D2 are\nO(H|D1, D2) = BF2 ⋅O(H|D1)\n= BF2 ⋅BF1 ⋅O(H)\nWe have the beautifully simple notion that updating with new data just amounts to mul-\ntiplying the current posterior odds by the Bayes factor of the new data.\nExample 4. Other symptoms of Marfan Syndrome\nRecall from the earlier example that the Bayes factor for a least one ocular feature (F) is\nBFF= P(F|M)\nP(F|Mc) = 0.7\n0.07 = 10.\n\n18.05 Class 12, Bayesian Updating: Odds, Spring 2022\nThe wrist sign (W) is the ability to wrap one hand around your other wrist to cover your\npinky nail with your thumb. Assume 10% of the population have the wrist sign, while 90%\nof people with Marfan's have it. Therefore the Bayes factor for the wrist sign is\nBFW= P(W|M)\nP(W|Mc) = 0.9\n0.1 = 9.\nWe will assume that Fand Ware conditionally independent symptoms. That is, among\npeople with Marfan syndrome, ocular features and the wrist sign are independent, and\namong people without Marfan syndrome, ocular features and the wrist sign are independent.\nGiven this assumption, the posterior odds of Marfan syndrome for someone with both an\nocular feature and the wrist sign are\nO(M|F, W) = BFW⋅BFF⋅O(M) = 9 ⋅10 ⋅\n14999 ≈\n1000.\nWe can convert the posterior odds back to probability, but since the odds are so small the\nresult is nearly the same:\nP(M|F, W) ≈\n1000 + 6 ≈0.596%.\nSo ocular features and the wrist sign are both strong evidence in favor of the hypothesis\nM, and taken together they are very strong evidence. Again, because the prior odds are so\nsmall, it is still unlikely that the person has Marfan syndrome, but at this point it might be\nworth undergoing further testing given potentially fatal consequences of the disease (such\nas aortic aneurysm or dissection).\nNote also that if a person has exactly one of the two symptoms, then the product of the\nBayes factors is near 1 (either 9/10 or 10/9). So the two pieces of data essentially cancel\neach other out with regard to the evidence they provide for Marfan's syndrome.\nLog odds\nIn practice, people often find it convenient to work with the natural log of the odds in place\nof odds. Naturally enough these are called the log odds. The Bayesian update formula\nO(H|D1, D2) = BF2 ⋅BF1 ⋅O(H)\nbecomes\nln(O(H|D1, D2)) = ln(BF2) + ln(BF1) + ln(O(H)).\nWe can interpret the above formula for the posterior log odds as the sum of the prior log\nodds and all the evidence ln(BFi) provided by the data. Note that by taking logs, evidence\nin favor (BFi> 1) is positive and evidence against (BFi< 1) is negative.\nTo avoid lengthier computations, we will work with odds rather than log odds in this course.\nLog odds are nice because sums are often more intuitive then products. Log odds also play a\ncentral role in logistic regression, an important statistical model related to linear regression.\n\nBayesian Updating with Continuous Priors\nClass 13, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Understand a parameterized family of distributions as representing a continuous range\nof hypotheses for the observed data.\n2. Be able to state Bayes' theorem and the law of total probability for continous densities.\n3. Be able to apply Bayes' theorem to update a prior probability density function to a\nposterior pdf given data and a likelihood function.\n4. Be able to interpret and compute posterior predictive probabilities.\nIntroduction\nUp to now we have only done Bayesian updating when we had a finite number of hypothesis,\ne.g. our dice example had five hypotheses (4, 6, 8, 12 or 20 sides). Now we will study\nBayesian updating when there is a continuous range of hypotheses. The Bayesian update\nprocess will be essentially the same as in the discrete case. As usual when moving from\ndiscrete to continuous we will need to replace the probability mass function by a probability\ndensity function, and sums by integrals.\nThe first few sections of this note are devoted to working with pdfs. In particular we will\ncover the law of total probability and Bayes' theorem. We encourage you to focus on how\nthese are essentially identical to the discrete versions. After that, we will apply Bayes'\ntheorem and the law of total probability to Bayesian updating.\nExamples with continuous ranges of hypotheses\nHere are three standard examples with continuous ranges of hypotheses.\nExample 1. Suppose you have a system that can succeed or fail with probability p. Then\nwe can hypothesize that pis anywhere in the range [0, 1]. That is, we have a continuous\nrange of hypotheses. We will often model this example with a 'bent' coin with unknown\nprobability pof heads.\nExample 2. The lifetime of a certain isotope is modeled by an exponential distribution\nexp(λ). In principle, the mean lifetime 1/λcan be any real number in (0, inf).\nExample 3. We are not restricted to a single parameter. In principle, the parameters μ\nand σof a normal distribution can be any real numbers in (-inf, inf) and (0, inf), respectively.\nIf we model gestational length for single births by a normal distribution, then from millions\nof data points we know that μis about 40 weeks and σis about one week.\n\n18.05 Class 13, Bayesian Updating with Continuous Priors, Spring 2022\nIn all of these examples we modeled the random process giving rise to the data by a distri-\nbution with parameters -called a parametrized distribution. Every possible choice of the\nparameter(s) is a hypothesis, e.g. we can hypothesize that the probability of succcess in\nExample 1 is p= 0.7313. We have a continuous set of hypotheses because we could take\nany value between 0 and 1.\nNotational conventions\n4.1\nParametrized models\nAs in the examples above our hypotheses often take the form a certain parameter has value\nθ.\nWe will often use the letter θto stand for an arbitrary hypothesis.\nThis will leave\nsymbols like p, f, and xto take there usual meanings as pmf, pdf, and data. Also, rather\nthan saying 'the hypothesis that the parameter of interest has value θ' we will simply say\nthe hypothesis θ.\n4.2\nBig and little letters\nWe have two parallel notations for outcomes and probability:\n1. (Big letters) Event A, probability function P(A).\n2. (Little letters) Value x, pmf p(x) or pdf f(x).\nThese notations are related by P(X= x) = p(x), where xis a value the discrete random\nvariable Xand 'X= x' is the corresponding event.\nWe carry these notations over to the probabilities used in Bayesian updating.\n1. (Big letters) From hypotheses Hand data Dwe compute several associated probabilities\nP(H), P(D), P(H|D), P(D|H).\nIn the coin example we might have H0.6 = 'the chosen coin has probability 0.6 of heads',\nD= '3 flips landed HHT', so P(D|H0.6) = (0.6)2(0.4)\n2. (Small letters) Hypothesis values θand data values xboth have probabilities or proba-\nbility densities:\np(θ)\np(x)\np(θ|x)\np(x|θ)\nf(θ)\nf(x)\nf(θ|x)\nf(x|θ)\nIn the coin example we might have θ= 0.6 and xis the sequence 1, 1, 0. So, p(x|θ) =\n(0.6)2(0.4). We might also write p(x= 1, 1, 0|θ= 0.6) to emphasize the values of xand θ.\nAlthough we will still use both types of notation, from now on we will mostly use the small\nletter notation involving pmfs and pdfs. Hypotheses will usually be parameters represented\nby Greek letters (θ, λ, μ, σ, ... ) while data values will usually be represented by English\nletters (x, xi, y, ... ).\n\n18.05 Class 13, Bayesian Updating with Continuous Priors, Spring 2022\nQuick review of pdf and probability\nSuppose Xis a random variable with pdf f(x).\nRecall f(x) is a density; its units are\nprobability/(units of x).\nx\nf(x)\nc\nd\nP(c≤X≤d)\nx\nf(x)\nx\ndx\nprobability f(x)dx\nThe probability that the value of Xis in [c, d] is given by\n∫\nd\nc\nf(x) dx.\nThe probability that Xis in an infinitesimal range dxaround xis f(x) dx. In fact, the\nintegral formula is just the 'sum' of these infinitesimal probabilities. We can visualize these\nprobabilities by viewing the integral as area under the graph of f(x).\nIn order to manipulate probabilities instead of densities in what follows, we will make\nfrequent use of the notion that f(x) dxis the probability that Xis in an infinitesimal range\naround xof width dx. Please make sure that you fully understand this notion.\nContinuous priors, discrete likelihoods\nIn the Bayesian framework we have probabilities of hypotheses -called prior and posterior\nprobabilities- and probabilities of data given a hypothesis -called likelihoods. In earlier\nclasses both the hypotheses and the data had discrete ranges of values. We saw in the\nintroduction that we might have a continuous range of hypotheses. The same is true for\nthe data, but for today we will assume that our data can only take a discrete set of values.\nIn this case, the likelihood of data xgiven hypothesis θis written using a pmf: p(x|θ).\nWe will use the following coin example to explain these notions. We will carry this example\nthrough in each of the succeeding sections.\nExample 4. Suppose we have a bent coin with unknown probability θof heads. In this\ncase, we'll say the coin is of 'type θ' and we'll label the hypothesis that a random coin is\nof type θby Hθ. The value of θis random and could be anywhere between 0 and 1. For\nthis and the examples that follow we'll suppose that the value of θfollows a distribution\nwith continuous prior probability density f(θ) = 2θ. We have a discrete likelihood because\ntossing a coin has only two outcomes, x= 1 for heads and x= 0 for tails.\np(x= 1|Hθ) = θ,\np(x= 0|Hθ) = 1 -θ.\nAs we stated earlier, we will often write θfor the hypothesis Hθ. So the above probabilities\nbecome\np(x= 1|θ) = θ,\np(x= 0|θ) = 1 -θ.\n\n18.05 Class 13, Bayesian Updating with Continuous Priors, Spring 2022\nThink: This can be tricky to wrap your mind around. We have a continuous range of\ntypes of coins -we identify the type by the value of the parameter θ. We are able to choose\na coin at random and the type chosen has a probability density f(θ).\nIt may help to see that the discrete examples we did in previous classes are similar. In one\nexample, we had three types of coin with probability of heads 0.5, 0.6, or 0.9. So, we called\nour hypotheses H0.5, H0.6, H0.9 and these had prior probabilities P(H0.5) etc. In other\nwords, we had a type of coin with an unknown probability of heads, we had hypotheses\nabout that probability and each of these hypotheses had a prior probability.\nThe law of total probability\nThe law of total probability for continuous probability distributions is essentially the same\nas for discrete distributions. We replace the prior pmf by a prior pdf and the sum by an\nintegral. We start by reviewing the law for the discrete case.\nRecall that for a discrete set of hypotheses H1, H2, ... Hnthe law of total probability says\nP(D) =\nn\n∑\ni=1\nP(D|Hi)P(Hi).\n(1)\nThis is the total prior probability of Dbecause we used the prior probabilities P(Hi)\nIn the little letter notation with θ1, θ2, ... , θnfor hypotheses and xfor data the law of total\nprobability is written\np(x) =\nn\n∑\ni=1\np(x|θi)p(θi).\n(2)\nWe also called this the prior predictive probability of the outcome xto distinguish it from\nthe prior probability of the hypothesis θ.\nLikewise, there is a law of total probability for continuous pdfs. We state it as a theorem\nusing little letter notation.\nTheorem. Law of total probability. Suppose we have a continuous parameter θin the\nrange [a, b], and discrete random data x. Assume θis itself random with density f(θ) and\nthat xand θhave likelihood p(x|θ). In this case, the total probability of xis given by the\nformula.\np(x) = ∫\nb\na\np(x|θ)f(θ) dθ\n(3)\nProof. Our proof will be by analogy to the discrete version: The probability term p(x|θ)f(θ) dθ\nis perfectly analogous to the term p(x|θi)p(θi) in Equation 2 (or the term P(D|Hi)P(Hi)\nin Equation 1). Continuing the analogy: the sum in Equation 2 becomes the integral in\nEquation 3\nAs in the discrete case, when we think of θas a hypothesis explaining the probability of the\ndata we call p(x) the prior predictive probability for x.\nExample 5. (Law of total probability.) Continuing with Example 4. We have a bent coin\nwith probability θof heads. The value of θis random with prior pdf f(θ) = 2θon [0, 1].\n\n18.05 Class 13, Bayesian Updating with Continuous Priors, Spring 2022\nSuppose I am about to flip the coin. What is the total probability of heads, i.e what is the\nprior predictive probability of heads?\nSolution: In Example 4 we noted that the likelihoods are p(x= 1|θ) = θand p(x= 0|θ) =\n1 -θ. So the total probability of x= 1 is\np(x= 1) = ∫\np(x= 1|θ) f(θ) dθ= ∫\nθ⋅2θdθ= ∫\n2θ2dθ= 2\n3.\nSince the prior is weighted towards higher probabilities of heads, so is the total probability\nof heads.\nBayes' theorem for continuous probability densities\nThe statement of Bayes' theorem for continuous pdfs is essentially identical to the statement\nfor pmfs. We state it including dθso we have genuine probabilities:\nTheorem. Bayes' Theorem. Use the same assumptions as in the law of total probability,\ni.e. θis a continuous parameter with pdf f(θ) and range [a, b]; xis random discrete data;\ntogether they have likelihood p(x|θ). With these assumptions:\nf(θ|x) dθ= p(x|θ)f(θ) dθ\np(x)\n=\np(x|θ)f(θ) dθ\n∫\nb\nap(x|θ)f(θ) dθ\n.\n(4)\nProof. Since this is a statement about probabilities it is just the usual statement of Bayes'\ntheorem. We hope this is clear.\nIt is important enough to spell out somewhat formally: Let Θ be the random variable that\nproduces the value θ. Consider the events\nH= 'Θ is in an interval of width dθaround the value θ'\nand\nD= 'the value of the data is x'.\nThen P(H) = f(θ) dθ,\nP(D) = p(x), and P(D|H) = p(x|θ). Now our usual form of\nBayes' theorem becomes\nf(θ|x) dθ= P(H|D) = P(D|H)P(H)\nP(D)\n= p(x|θ)f(θ) dθ\np(x)\nLooking at the first and last terms in this equation we see the new form of Bayes' theorem.\nFinally, we firmly believe that it is more conducive to careful thinking about probability\nto keep the factor of dθin the statement of Bayes' theorem. But because it appears in the\nnumerator on both sides of Equation 4 many people drop the dθand write Bayes' theorem\nin terms of densities as\nf(θ|x) = p(x|θ)f(θ)\np(x)\n=\np(x|θ)f(θ)\n∫\nb\nap(x|θ)f(θ) dθ\n.\n\n18.05 Class 13, Bayesian Updating with Continuous Priors, Spring 2022\nBayesian updating with continuous priors\nNow that we have Bayes' theorem and the law of total probability we can finally get to\nBayesian updating. Before continuing with Example 4, we point out two features of the\nBayesian updating table that appears in the next example:\n1. The table for continuous priors is very simple: since we cannot have a row for each of\nan infinite number of hypotheses we'll have just one row which uses a variable to stand for\nall hypotheses Hθ.\n2. By including dθ, all the entries in the table are probabilities and all our usual probability\nrules apply.\nExample 6. (Bayesian updating.) Continuing Examples 4 and 5. We have a bent coin\nwith unknown probability θof heads. The value of θis random with prior pdf f(θ) = 2θ.\nSuppose we flip the coin three times and get the sequence HTT. Compute the posterior\npdf for θ.\nSolution: We make the usual update table, with an added column giving the range of\nvalues that θcan take. We make the first row an abstract version of Bayesian updating and\nthe second row is Bayesian updating for this particular example. In later examples we will\nskip that abstract version.\nhypothesis\nrange\nprior\nlikelihood\nBayes\nposterior\nnumerator\nHθ\nθrange\nf(θ) dθ\np(x= 1, 1, 0|θ)\np(x= 1, 1, 0|θ)f(θ) dθ\nf(θ|x= 1, 1, 0) dθ\nHθ\n[0, 1]\n2θdθ\nθ2(1 -θ)\n2θ3(1 -θ) dθ\n20θ3(1 -θ) dθ\ntotal\n[0, 1]\n∫\nb\naf(θ) dθ= 1\nno sum\np(x= 1, 1, 0)\n= ∫\n0 2θ3(1 -θ) dθ= 1/10\nTherefore the posterior pdf (after seeing HHT) is f(θ|x) = 20θ3(1 -θ) .\nWe have a number of comments:\n1. Since we used the prior probability f(θ) dθ, the hypothesis should have been:\n'the unknown paremeter is in an interval of width dθaround θ'.\nEven for us that is too much to write, so you will have to think it everytime we write that\nthe hypothesis is θor Hθ.\n2. The posterior pdf for θis found by removing the dθfrom the posterior probability in\nthe table.\nf(θ|x) = 20θ3(1 -θ).\n3. (i) As always p(x) is the total probability. Since we have a continuous distribution\ninstead of a sum we compute an integral.\n(ii) Notice that by including dθin the table, it is clear what integral we need to compute\nto find the total probability p(x).\n4. The table organizes the continuous version of Bayes' theorem. Namely, the posterior pdf\n\n18.05 Class 13, Bayesian Updating with Continuous Priors, Spring 2022\nis related to the prior pdf and likelihood function via:\nf(θ|x) dθ=\np(x|θ) f(θ)dθ\n∫\nb\nap(x|θ)f(θ) dθ\n= p(x|θ) f(θ)\np(x)\ndθ\nRemoving the dθin the numerator of both sides we have the statement in terms of densities.\n5. Regarding both sides as functions of θ, we can again express Bayes' theorem in the form:\nf(θ|x) ∝p(x|θ) ⋅f(θ)\nposterior ∝likelihood × prior.\n9.1\nFlat priors\nOne important prior is called a flat or uniform prior.\nA flat prior assumes that every\nhypothesis is equally probable. For example, if θhas range [0, 1] then f(θ) = 1 is a flat\nprior.\nExample 7. (Flat priors.) We have a bent coin with unknown probability θof heads.\nSuppose we toss it once and get heads. Assume a flat prior and find the posterior probability\nfor θ.\nSolution: This is similar Example 6 with a different prior and data.\nhypothesis\nrange\nprior\nlikelihood\nBayes numerator\nposterior\nθ\nθ\nf(θ) dθ\np(x= 1|θ)\nf(θ|x= 1) dθ\nθ\n[0, 1]\n1 ⋅dθ\nθ\nθdθ\n2θdθ\ntotal\n[0, 1]\n∫\nb\naf(θ) dθ= 1\nno sum\np(x= 1) = ∫\nθdθ= 1/2\n9.2\nUsing the posterior pdf\nExample 8. In the previous example the prior probability was flat. First show that this\nmeans that a priori the coin is equally like to be biased towards heads or tails. Then, after\nobserving one heads, what is the (posterior) probability that the coin is biased towards\nheads?\nSolution: Since the parameter θis the probability the coin lands heads, the first part of the\nproblem asks us to show P(θ> 0.5) = 0.5 and the second part asks for P(θ> 0.5 | x= 1).\nThese are easily computed from the prior and posterior pdfs respectively.\nThe prior probability that the coin is biased towards heads is\nP(θ> 0.5) = ∫\n0.5\nf(θ) dθ= ∫\n0.5\n1 ⋅dθ= θ|\n0.5 = 1\n2.\nThe probability of 1/2 means the coin is equally likely to be biased toward heads or tails.\nThe posterior probabilitiy that it's biased towards heads is\nP(θ> 0.5|x= 1) = ∫\n0.5\nf(θ|x= 1) dθ= ∫\n0.5\n2θdθ= θ2∣\n0.5 = 3\n4.\n\n18.05 Class 13, Bayesian Updating with Continuous Priors, Spring 2022\nWe see that observing one heads has increased the probability that the coin is biased towards\nheads from 1/2 to 3/4.\nPredictive probabilities\nJust as in the discrete case we are also interested in using the posterior probabilities of the\nhypotheses to make predictions for what will happen next.\nExample 9. (Prior and posterior prediction.) Continuing Examples 4, 5, 6: we have a\ncoin with unknown probability θof heads and the value of θhas prior pdf f(θ) = 2θ. Find\nthe prior predictive probability of heads. Then suppose the first flip was heads and find the\nposterior predictive probabilities of both heads and tails on the second flip.\nSolution: For notation let x1 be the result of the first flip and let x2 be the result of the\nsecond flip. The prior predictive probability is exactly the total probability computed in\nExamples 5 and 6.\np(x1 = 1) = ∫\np(x1 = 1|θ)f(θ) dθ= ∫\n2θ2 dθ= 2\n3.\nThe posterior predictive probabilities are the total probabilities computed using the poste-\nrior pdf. From Example 6 we know the posterior pdf is f(θ|x1 = 1) = 3θ2. So the posterior\npredictive probabilities are\np(x2 = 1|x1 = 1) = ∫\np(x2 = 1|θ, x1 = 1)f(θ|x1 = 1) dθ= ∫\nθ⋅3θ2 dθ= 3/4\np(x2 = 0|x1 = 1) = ∫\np(x2 = 0|θ, x1 = 1)f(θ|x1 = 1) dθ= ∫\n(1 -θ) ⋅3θ2 dθ= 1/4\n(More simply, we could have computed p(x2 = 0|x1 = 1) = 1 -p(x2 = 1|x1 = 1) = 1/4.)\n(Optional) From discrete to continuous Bayesian updat-\ning\nThis section is optional. In it we will try to develop intuition for the transition from discrete\nto continuous Bayesian updating. We'll walk a familiar road from calculus. Namely we will:\n(i) divide the continuous range of hypotheses into a finite number of short intervals.\n(ii) create the discrete updating table for the finite number of hypotheses.\n(iii) consider how the table changes as the number of hypotheses goes to infinity.\nIn this way, we'll see the prior and posterior pmfs converge to the prior and posterior pdfs.\nExample 10. To keep things concrete, we will work with the same prior and data as in\nExample 7. We have a 'bent' coin with a flat prior f(θ) = 1. Our data is we tossed the coin\nonce and got heads.\nOur goal is to go from discrete to continuous by increasing the number of hypotheses.\n\n18.05 Class 13, Bayesian Updating with Continuous Priors, Spring 2022\n4 hypotheses. Suppose we have four types of coins that have probability of heads 1/8,\n3/8, 5/8 and 7/8 respectively. If one coin is chosen at random, our hypotheses for its type\nare\nH1 ∶θ= 1/8,\nH2 ∶θ= 3/8,\nH3 ∶θ= 5/8,\nH4 ∶θ= 7/8.\nTo get this, we divided [0, 1] into 4 equal intervals: [0, 1/4], [1/4, 1/2], [1/2, 3/4], [3/4, 1].\nEach interval has width Δθ= 1/4. We put our the value of θfor our coin types at the\ncenters of the four intervals.\n(Just as with forming Riemann sums in calculus, it's not important where in each interval\nwe choose θ. The center is one easy choice.)\nLet's name each of these values θj= j/8, where j= 1, 3, 5, 7.\nThe flat prior gives each hypothesis a probability of 1/4 = 1 ⋅Δθ. We have the table:\nhypothesis\nprior\nlikelihood\nBayes num.\nposterior\nTotal\n-\nn\n∑\ni=1\nθiΔθ\nθ= θ1 = 1\n4 ⋅1\n0.0625\nθ= θ2 = 3\n4 ⋅3\n0.1875\nθ= θ3 = 5\n4 ⋅5\n0.3125\nθ= θ4 = 7\n4 ⋅7\n0.4375\nHere are the density histograms of the prior and posterior pmf. The prior and posterior\npdfs from Example 7 are superimposed on the histograms in red.\nx\ndensity\n1/8\n3/8\n5/8\n7/8\n.5\n1.5\nx\ndensity\n1/8\n3/8\n5/8\n7/8\n.5\n1.5\n8 hypotheses. Next we slice [0,1] into 8 intervals each of width Δθ= 1/8 and use the\ncenter of each slice for our 8 hypotheses θi.\nθ1:\n'θ= 1/16',\nθ2:\n'θ= 3/16',\nθ3:\n'θ= 5/16',\nθ4:\n'θ= 7/16'\nθ5:\n'θ= 9/16',\nθ6:\n'θ= 11/16',\nθ7:\n'θ= 13/16',\nθ8:\n'θ= 15/16'\nThe flat prior gives each hypothesis the probablility 1/8 = 1 ⋅Δθ. Here are the table and\ndensity histograms.\n\n18.05 Class 13, Bayesian Updating with Continuous Priors, Spring 2022\nhypothesis\nprior\nlikelihood\nBayes num.\nposterior\nTotal\n-\nn\n∑\ni=1\nθiΔθ\nθ= θ1 =\n8 ⋅1\n0.0156\nθ= θ2 =\n8 ⋅3\n0.0469\nθ= θ3 =\n8 ⋅5\n0.0781\nθ= θ4 =\n8 ⋅7\n0.1094\nθ= θ5 =\n8 ⋅9\n0.1406\nθ= θ6 = 11\n8 ⋅11\n0.1719\nθ= θ7 = 13\n8 ⋅13\n0.2031\nθ= θ8 = 15\n8 ⋅15\n0.2344\nx\ndensity\n1/16\n3/16\n5/16\n7/16\n9/16\n11/16\n13/16\n15/16\n.5\n1.5\nx\ndensity\n1/16\n3/16\n5/16\n7/16\n9/16\n11/16\n13/16\n15/16\n.5\n1.5\n20 hypotheses. Finally we slice [0,1] into 20 pieces. This is essentially identical to the\nprevious two cases. Let's skip right to the density histograms.\nx\ndensity\n3/7\n.5\n1.5\nx\ndensity\n3/7\n.5\n1.5\nLooking at the sequence of plots we see how the prior and posterior density histograms\nconverge to the prior and posterior probability density functions.\n\nNotational conventions\nClass 13, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Be able to work with the various notations and terms we use to describe probabilities\nand likelihood.\nIntroduction\nWe've introduced a number of different notations for probability, hypotheses and data. We\ncollect them here, to have them in one place.\nNotation and terminology for data and hypotheses\nThe problem of labeling data and hypotheses is a tricky one. When we started the course\nwe talked about outcomes, e.g. heads or tails. Then when we introduced random variables\nwe gave outcomes numerical values, e.g. 1 for heads and 0 for tails. This allowed us to do\nthings like compute means and variances. We need to do something similar now. Recall\nour notational conventions:\n- Events are labeled with capital letters, e.g. A, B, C.\n- A random variable is capital Xand takes values small x.\n- The connection between values and events: 'X= x' is the event that Xtakes the\nvalue x.\n- The probability of an event is capital P(A).\n- A discrete random variable has a probability mass function small p(x) The connection\nbetween Pand pis that P(X= x) = p(x).\n- A continuous random variable has a probability density function f(x) The connection\nbetween Pand fis that P(a≤X≤b) = ∫\nb\naf(x) dx.\n- For a continuous random variable Xthe probability that Xis in an infinitesimal\ninterval of width dxaround xis f(x) dx.\nIn the context of Bayesian updating we have similar conventions.\n- We use capital letters, especially H, to indicate a hypothesis, e.g. H= 'the coin is\nfair'.\n\n18.05 Class 13, Notational conventions, Spring 2022\n- We use lower case letters, especially θ, to indicate the hypothesized value of a model\nparameter, e.g. the probability the coin lands heads is θ= 0.5.\n- We use upper case letters, especially D, when talking about data as events.\nFor\nexample, D= 'the sequence of tosses was HTH.\n- We use lower case letters, especially x, when talking about data as values. For exam-\nple, the sequence of data was x1, x2, x3 = 1, 0, 1.\n- When the set of hypotheses is discrete we can use the probability of individual hy-\npotheses, e.g. p(θ). When the set is continuous we need to use the probability for an\ninfinitesimal range of hypotheses, e.g. f(θ) dθ.\nThe following table summarizes this for discrete θand continuous θ.\nIn both cases we\nare assuming a discrete set of possible outcomes (data) x. Tomorrow we will deal with a\ncontinuous set of outcomes.\nBayes\nhypothesis\nprior\nlikelihood\nnumerator\nposterior\nH\nP(H)\nP(D|H)\nP(D|H)P(H)\nP(H|D)\nDiscrete θ:\nθ\np(θ)\np(x|θ)\np(x|θ)p(θ)\np(θ|x)\nContinuous θ:\nθ\nf(θ) dθ\np(x|θ)\np(x|θ)f(θ) dθ\nf(θ|x) dθ\nRemember the continuous hypothesis θis really a shorthand for 'the parameter θis in an\ninterval of width dθaround θ'.\n\nContinuous Data with Continuous Priors\nClass 14, 18.05\nJeremy Orloff and Jonathan Bloom\nThis reading is not assigned. It goes into a little more detail on Bayesian updating where\nboth hypotheses and data are continuous.\nLearning Goals\n1. Be able to construct a Bayesian update table for continuous hypotheses and continuous\ndata.\n2. Be able to recognize the pdf of a normal distribution and determine its mean and variance.\nIntroduction\nWe are now ready to do Bayesian updating when both the hypotheses and the data take\ncontinuous values. The pattern is the same as what we've done before, so let's first review\nthe previous two cases.\nPrevious cases\n1. Discrete hypotheses, discrete data\nNotation\n- Hypotheses H\n- Data x\n- Prior P(H)\n- Likelihood p(x| H)\n- Posterior P(H| x).\nExample 1. Suppose we have data xand three possible explanations (hypotheses) for the\ndata that we'll call A, B, C. Suppose also that the data can take two possible values, -1\nand 1.\nIn order to use the data to help estimate the probabilities of the different hypotheses we\nneed a prior pmf and a likelihood table. Assume the prior and likelihoods are given in\nthe following table. (For this example we are only concerned with the formal process of\nBayesian updating. So we just made up the prior and likelihoods.)\n\n18.05 Class 14, Continuous Data with Continuous Priors, Spring 2022\nhypothesis\nprior\nH\nP(H)\nA\n0.1\nB\n0.3\nC\n0.6\nPrior probabilities\nhypothesis\nlikelihood p(x| H)\nH\nx= -1\nx= 1\nA\n0.2\n0.8\nB\n0.5\n0.5\nC\n0.7\n0.3\nLikelihoods\nNaturally, each entry in the likelihood table is a likelihood p(x| H). For instance the 0.2\nrow Aand column x= -1 is the likelihood p(x= -1 | A).\nQuestion: Suppose we run one trial and obtain the data x1 = 1. Use this to find the\nposterior probabilities for the hypotheses.\nSolution: The data picks out one column from the likelihood table which we then use in\nour Bayesian update table.\nBayes\nhypothesis\nprior\nlikelihood\nnumerator\nposterior\nH\nP(H)\np(x= 1 | H)\np(x| H)P(H)\nP(H| x) = p(x| H)P(H)\np(x)\nA\n0.1\n0.8\n0.08\n0.195\nB\n0.3\n0.5\n0.15\n0.366\nC\n0.6\n0.3\n0.18\n0.439\ntotal\nno sum\np(x) = 0.41\nTo summarize: the prior probabilities of hypotheses and the likelihoods of data given hy-\npothesis were given; the Bayes numerator is the product of the prior and likelihood; the\ntotal probability p(x) is the sum of the probabilities in the Bayes numerator column; and\nwe divide by p(x) to normalize the Bayes numerator.\nNote: As usual, the term 'no sum' in the likelihood column is not literally true. What it\nmeans is that the sum is not meaningful to us. In particular, we don't expect the likelihood\ncolumn to sum to 1.\n2. Continuous hypotheses, discrete data\nNow suppose that we have data xthat can take a discrete set of values and a continuous\nparameter θthat determines the distribution the data is drawn from.\nNotation\n- Hypotheses θ\n- Data x\n- Prior f(θ) dθ\n- Likelihood p(x| θ)\n- Posterior f(θ| x) dθ.\n\n18.05 Class 14, Continuous Data with Continuous Priors, Spring 2022\nNote: Here we multiplied by dθto express the prior and posterior as probabilities. As\ndensities, we have the prior pdf f(θ) and the posterior pdf f(θ| x).\nExample 2. Assume that x∼Binomial(5, θ). So θis in the range [0, 1] and the data x\ncan take six possible values, 0, 1, ..., 5.\nSince there is a continuous range of values we use a pdf to describe the prior on θ. Let's\nsuppose the prior is f(θ) = 2θ. We can still make a likelihood table, though it only has one\nrow representing an arbitrary hypothesis θ.\nhypothesis\nlikelihood p(x| θ)\nx= 0\nx= 1\nx= 2\nx= 3\nx= 4\nx= 5\nθ\n(5\n0)(1 -θ)5\n(5\n1)θ(1 -θ)4\n(5\n2)θ2(1 -θ)3\n(5\n3)θ3(1 -θ)2\n(5\n4)θ4(1 -θ)\n(5\n5)θ5\nLikelihoods\nQuestion: Suppose we run one trial and obtain the data x= 2. Use this to find the\nposterior pdf for the parameter (hypotheses) θ.\nSolution: As before, the data picks out one column from the likelihood table which we\ncan use in our Bayesian update table. Since we want to work with probabilities we write\nf(θ)dθand f(θ| x) dθfor the pdfs.\nhypothesis\nprior\nlikelihood\nBayes\nposterior\n(for x= 2)\nnumerator\nθ\nf(θ) dθ\np(x| θ)\np(x| θ)f(θ) dθ\nf(θ| x) dθ= p(x| θ)f(θ) dθ\np(x)\nθ\n2θdθ\n(5\n2)θ2(1 -θ)3\n2(5\n2)θ3(1 -θ)3 dθ\nf(θ| x) dθ=\n7!\n3! 3!θ3(1 -θ)3 dθ\ntotal\nno sum\np(x)\n= ∫\n0 2(5\n2)θ3(1 -θ)3 dθ\n= 2(5\n2) 3! 3!\n7!\nTo summarize: the prior probabilities of hypotheses and the likelihoods of data given hy-\npothesis were given; the Bayes numerator is the product of the prior and likelihood; the\ntotal probability p(x) is the integral of the probabilities in the Bayes numerator column;\nand we divide by p(x) to normalize the Bayes numerator.\nContinuous hypotheses and continuous data\nWhen both data and hypotheses are continuous, the only change to the previous example is\nthat the likelihood function uses a pdf φ(x| θ) instead of a pmf p(x| θ). The general shape\nof the Bayesian update table is the same.\nNotation\n\n18.05 Class 14, Continuous Data with Continuous Priors, Spring 2022\n- Hypotheses θ. For continuous hypotheses, this really means that we hypothesize that\nthe parameter is in a small interval of size dθaround θ.\n- Data x. For continuous data, this really means that the data is in a small interval of\nsize dxaround x.\n- Prior f(θ)dθ. This is our initial belief about the probability that the parameter is in\na small interval of size dθaround θ.\n- Likelihood φ(x| θ) dx. This is the (calculated) probability that the data is in a small\ninterval of size dxaround x, ASSUMING the hypothesis θ.\n- Posterior f(θ| x) dθ. This is the (calculated) probability that the parameter is in a\nsmall interval of size dθaround θ, GIVEN the data x.\nSimplifying the notation.\nIn the previous cases we included dθso that we were working\nwith probabilities instead of densities.\nWhen both data and hypotheses are continuous\nwe will need both dθand dx. This makes things conceptually simpler, but notationally\ncumbersome. To simplify the notation we will sometimes allow ourselves to drop dxin our\ntables. This is fine because the data xis fixed in each calculation. We keep the dθbecause\nthe hypothesis θis allowed to vary.\nFor comparison, we first show the general table in simplified notation followed immediately\nafterward by the table showing both infinitesimals.\nBayes\nhypoth.\nprior\nlikelihood\nnumerator\nposterior\nθ\nf(θ) dθ\nφ(x| θ)\nφ(x| θ)f(θ) dθ\nf(θ| x) dθ= φ(x| θ)f(θ) dθ\nφ(x)\ntotal\nno sum\nφ(x) = ∫φ(x| θ)f(θ) dθ\n(integrate over θ)\n= prior prob. density for data x\nBayesian update table without dx\nBayes\nhypoth.\nprior\nlikelihood\nnumerator\nposterior\nθ\nf(θ) dθ\nφ(x| θ) dx\nφ(x| θ)f(θ) dθdx\nf(θ| x) dθ\n= φ(x| θ)f(θ) dθdx\nφ(x) dx\n= φ(x| θ)f(θ) dθ\nφ(x)\ntotal\nno sum\nφ(x) dx= (∫φ(x| θ)f(θ) dθ) dx\nBayesian update table with dθand dx\nTo summarize: the prior probabilities of hypotheses and the likelihoods of data given hy-\npothesis were given; the Bayes numerator is the product of the prior and likelihood; the\n\n18.05 Class 14, Continuous Data with Continuous Priors, Spring 2022\ntotal probability φ(x) dxis the integral of the probabilities in the Bayes numerator column;\nwe divide by φ(x) dxto normalize the Bayes numerator.\nA digression on notational messes\nWe have chosen to use the notation φ(x), φ(x| θ) for the pdfs of data and f(θ), f(θ| x) for\nthe pdfs of hypotheses. This is nice because φis a Greek f, but the different symbols help\nus distinguish the two types of pdfs. Many, perhaps most, writers use the same letter ffor\nboth. This forces the reader to look at the arguments to the function to understand what\nis meant. That is, f(x|θ) is the probability of data given an hypothesis, i.e. the likelihood\nand f(θ|x) is the probability of an hypothesis given the data, i.e. the posterior pdf.\nAs mathematicians this makes us pull our hair out. But, to be fair, there is a philosoph-\nical underpinning to this notation. We can think of fas a universal probability density\nwhich gives the probability of absolutely any combination of things. Thus f(x, y) is the\njoint probability density for the quantities denoted by xand y. If we just write f(x) the\nimplication is that this means the marginal density for x, i.e. the density for xwhen yis\nallowed to take any value. Similarly we can write f(x, y|z) for the conditional density of x\nand ygiven z.\nNormal hypothesis, normal data\nA standard example of continuous hypotheses and continuous data assumes that both the\ndata and prior follow normal distributions. The following example assumes that the variance\nof the data is known.\nExample 3. Suppose we have data x= 5 which was drawn from a normal distribution\nwith unknown mean θand standard deviation 1.\nx∼N(θ, 1)\nSuppose further, that our prior distribution for the unknown parameter θis θ∼N(2, 1).\nLet xrepresent an arbitrary data value.\n(a) Make a Bayesian table with prior, likelihood, and Bayes numerator.\n(b) Show that the posterior distribution for θis normal as well.\n(c) Find the mean and variance of the posterior distribution.\nSolution: As we did with the tables above, a good compromise on the notation is to include\ndθbut not dx. The reason for this is that the total probability is computed by integrating\nover θand the dθreminds of us that.\nOur prior pdf is\nf(θ) =\n√\n2π\ne-(θ-2)2/2.\nThe likelihood function is\nφ(x= 5 | θ) =\n√\n2π\ne-(5-θ)2/2.\n\n18.05 Class 14, Continuous Data with Continuous Priors, Spring 2022\nWe know we are going to multiply the prior and the likelihood, so we carry out that algebra\nfirst. In the very last step we give the complicated constant factor the name c1.\nprior ⋅likelihood =\n√\n2π\ne-(θ-2)2/2 ⋅\n√\n2π\ne-(5-θ)2/2\n= 1\n2πe-(2θ2-14θ+29)/2\n= 1\n2πe-(θ2-7θ+29/2)\n(complete the square)\n= 1\n2πe-((θ-7/2)2+9/4)\n= e-9/4\n2πe-(θ-7/2)2)\n= c1e-(θ-7/2)2\nIn the last step we named the complicated constant factor c1.\nBayes\nposterior\nhypothesis\nprior\nlikelihood\nnumerator\nf(θ| x= 5) dθ\nθ\nf(θ) dθ\nφ(x= 5 | θ)\nφ(x= 5 | θ)f(θ) dθ\nφ(x= 5 | θ)f(θ) dθ\nφ(x= 5)\nθ\n√\n2πe-(θ-2)2/2 dθ\n√\n2πe-(5-θ)2/2\nc1e-(θ-7/2)2\nc2e-(θ-7/2)2\ntotal\nno sum\nφ(x= 5) = ∫φ(x= 5 | θ)f(θ) dθ\nWe can see by the form of the posterior pdf that it is a normal distribution. Because the\nexponential for a normal distribution is e-(θ-μ)2/2σ2 we have mean μ= 7/2 and 2σ2 = 1, so\nvariance σ2 = 1/2.\nWe don't need to bother computing the total probability; it is just used for normalization\nand we already know the normalization constant\nσ\n√\n2π\nfor a normal distribution.\nTo\nsummarize,\nThe posterior pdf follows a N(7/2, 1/2) distribution.\nHere is the graph of the prior and the posterior pdfs for this example. Note how the data\n'pulls' the prior (the wider bell on the left) towards the data. The posterior is the narrower\nbell on the right. After collecting data, we have a new opinion about the mean, and we are\nmore sure of this new opinion.\n\n18.05 Class 14, Continuous Data with Continuous Priors, Spring 2022\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nprior = orange;\nposterior = blue;\ndata = red line\nNow we'll repeat the previous example for general x. When reading this if you mentally\nsubstitute 5 for xyou will understand the algebra.\nExample 4. Suppose our data xis drawn from a normal distribution with unknown mean\nθand standard deviation 1.\nx∼N(θ, 1)\nSuppose further, that our prior distribution for the unknown parameter θis θ∼N(2, 1).\nSolution: As before, we show the algebra used to simplify the Bayes numerator: The prior\npdf and likelihood function are\nf(θ) =\n√\n2π\ne-(θ-2)2/2\nf(x| θ) =\n√\n2π\ne-(x-θ)2/2.\nThe Bayes numerator is the product of the prior and the likelihood:\nprior ⋅likelihood =\n√\n2π\ne-(θ-2)2/2 ⋅\n√\n2π\ne-(x-θ)2/2\n= 1\n2πe-(2θ2-(4+2x)θ+4+x2)/2\n= 1\n2πe-(θ2-(2+x)θ+(4+x2)/2)\n(complete the square)\n= 1\n2πe-((θ-(1+x/2))2-(1+x/2)2+(4+x2)/2)\n= c1e-(θ-(1+x/2))2\nJust as in the previous example, in the last step we replaced all the constants, including\nthe exponentials that just involve x, by by the simple constant c1.\nNow the Bayesian update table becomes\n\n18.05 Class 14, Continuous Data with Continuous Priors, Spring 2022\nBayes\nposterior\nhypothesis\nprior\nlikelihood\nnumerator\nf(θ| x) dθ\nθ\nf(θ) dθ\nφ(x| θ)\nφ(x| θ)f(θ) dθ\nφ(x| θ)f(θ) dθ\nφ(x)\nθ\n√\n2πe-(θ-2)2/2 dθ\n√\n2πe-(x-θ)2/2\nc1e-(θ-(1+x/2))2\nc2e-(θ-(1+x/2))2\nθ\nθ∼N(2, 1)\nx∼N(θ, 1)\nθ∼N(1 + x/2, 1/2)\ntotal\nno sum\nφ(x) = ∫φ(x| θ)f(θ) dθ\nAs in the previous example we can see by the form of the posterior that it must be a normal\ndistribution with mean 1 + x/2 and variance 1/2. That is,\nThe posterior pdf follows a N(1 + x/2, 1/2) distribution.\nYou should compare this with the case x= 5 in the previous example.\nPredictive probabilities\nSince the data xis continuous it has prior and posterior predictive pdfs. The prior predictive\npdf is the total probability density computed at the bottom of the Bayes numerator column:\nφ(x) = ∫f(x|θ)f(θ) dθ,\nwhere the integral is computed over the entire range of θ.\nThe posterior predictive pdf has the same form as the prior predictive pdf, except it uses\nthe posterior probabilities for θ:\nφ(x2|x1) = ∫φ(x2|θ, x1)f(θ|x1) dθ,\nWe usually assume that x1 and x2 are conditionally independent. That is,\nφ(x2|θ, x1) = φ(x2|θ).\nIn this case the formula for the posterior predictive pdf is a little simpler:\nφ(x2|x1) = ∫φ(x2|θ)f(θ|x1) dθ.\n\nConjugate priors: Beta and normal\nClass 15, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Be familiar with the 2-parameter family of beta distributions and its normalization.\n2. Understand the benefits of conjugate priors.\n3. Be able to update a beta prior given a Bernoulli, binomial, or geometric likelihood.\n4. Understand and be able to use the formula for updating a normal prior given a normal\nlikelihood with known variance.\nIntroduction\nOur main goal here is to introduce the idea of conjugate priors and look at some specific\nconjugate pairs. These simplify the job of Bayesian updating to simple arithmetic. We'll\nstart by introducing the beta distribution and using it as a conjugate prior with a binomial\nlikelihood. After that we'll look at other conjugate pairs.\nBeta distribution\nThe beta distribution Beta(a, b) is a two-parameter distribution with range [0, 1] and pdf\nf(θ) =\n(a+ b-1)!\n(a-1)!(b-1)!θa-1(1 -θ)b-1\nWe have made an applet so you can explore the shape of the beta distribution as you vary\nthe parameters:\nhttps://mathlets.org/mathlets/beta-distribution/.\nAs you can see in the applet, the beta distribution may be defined for any real numbers\na> 0 and b> 0. In 18.05 we will stick to integers aand b, but you can get the full story\nhere: https://en.wikipedia.org/wiki/Beta_distribution\nIn the context of Bayesian updating, aand bare often called hyperparameters to distinguish\nthem from the unknown parameter θrepresenting our hypotheses. In a sense, aand bare\n'one level up' from θsince they parameterize its pdf.\n3.1\nA simple but important observation!\nIf a pdf f(θ) with range [0, 1] has the form cθa-1(1 -θ)b-1 then f(θ) is a Beta(a, b) distri-\nbution and the normalizing constant must be\nc=\n(a+ b-1)!\n(a-1)! (b-1)!.\n\n18.05 Class 15, Conjugate priors: Beta and normal, Spring 2022\nThis follows because the constant cmust normalize the pdf to have total probability 1.\nThere is only one such constant and it is given in the formula for the beta distribution.\nA similar observation holds for normal distributions, exponential distributions, and so on.\n3.2\nBeta priors and posteriors for binomial random variables\nExample 1. Suppose we have a bent coin with unknown probability θof heads. We toss\nit 12 times and get 8 heads and 4 tails. Starting with a flat prior, show that the posterior\npdf is a Beta(9, 5) distribution.\nSolution: This is nearly identical to examples from the previous class. We'll call the data\nfrom all 12 tosses x1.\nIn the following table we call the leading constant factor in the\nposterior column c2. Our simple observation will tell us that it has to be the constant\nfactor from the beta pdf.\nThe data is 8 heads and 4 tails. Since this comes from a binomial(12, θ) distribution, the\nlikelihood p(x1|θ) = (12\n8 )θ8(1 -θ)4. Thus the Bayesian update table is\nBayes\nhypothesis\nprior\nlikelihood\nnumerator\nposterior\nθ\n1 ⋅dθ\n(12\n8 ) θ8(1 -θ)4\n(12\n8 ) θ8(1 -θ)4 dθ\nc2 θ8(1 -θ)4 dθ\ntotal\nT= (12\n8 ) ∫\nθ8(1 -θ)4 dθ\nFor the posterior pdf, our simple observation holds with a= 9 and b= 5. Therefore the\nposterior pdf follows a Beta(9, 5) distribution and we have\nf(θ|x1) = c2θ8(1 -θ)4, where c2 = 13!\n8! 4!.\nNote: We explicitly included the binomial coefficient (12\n8 ) in the likelihood. We could just\nas easily have given it a name, say c1 and not bothered making its value explicit.\nExample 2. Now suppose we toss the same coin again, getting nheads and mtails. Using\nthe posterior pdf of the previous example as our new prior pdf, show that the new posterior\npdf is that of a Beta(9 + n, 5 + m) distribution.\nSolution: It's all in the table. We'll call the data of these n+ madditional tosses x2. This\ntime we won't make the binomial coefficient explicit. Instead we'll just call it c3. Whenever\nwe need a new label we will simply use cwith a new subscript.\nBayes\nhyp.\nprior\nlikelihood\nnumerator\nposterior\nθ\nc2θ8(1 -θ)4 dθ\nc3 θn(1 -θ)m\nc2c3 θn+8(1 -θ)m+4 dθ\nc4 θn+8(1 -θ)m+4 dθ\ntotal\nT= ∫\nc2c3 θn+8(1 -θ)m+4 dθ\nAgain our simple observation holds and therefore the posterior pdf\nf(θ|x1, x2) = c4θn+8(1 -θ)m+4\n\n18.05 Class 15, Conjugate priors: Beta and normal, Spring 2022\nfollows a Beta(n+ 9, m+ 5) distribution.\nNote: Flat beta. The Beta(1, 1) distribution is the same as the uniform distribution on\n[0, 1], which we have also called the flat prior on θ. This follows by plugging a= 1 and\nb= 1 into the definition of the beta distribution, giving f(θ) = 1.\nSummary: If the probability of heads is θ, the number of heads in n+ mtosses follows a\nbinomial(n+ m, θ) distribution. We have seen that if the prior on θis a beta distribution\nthen so is the posterior; only the parameters a, bof the beta distribution change!\nWe\nsummarize precisely how they change in a table. We assume the data is nheads and m\ntails in n+ mtosses.\nhypothesis\ndata\nprior\nlikelihood\nposterior\nθ\nx= n, m\nBeta(a, b)\nbinomial(n+ m, θ)\nBeta(a+ n, b+ m)\nθ\nx= n, m\nc1θa-1(1 -θ)b-1 dθ\nc2θn(1 -θ)m\nc3θa+n-1(1 -θ)b+m-1 dθ\nConjugate priors\nThe beta distribution is called a conjugate prior for the binomial distribution. This means\nthat if the likelihood function is binomial, then a beta prior gives a beta posterior -this is\nwhat we saw in the previous examples. In fact, the beta distribution is a conjugate prior\nfor the Bernoulli and geometric distributions as well.\nWe will soon see another important example: the normal distribution is its own conjugate\nprior. In particular, if the likelihood function is normal with known variance, then a normal\nprior gives a normal posterior.\nConjugate priors are useful because they reduce Bayesian updating to modifying the param-\neters of the prior distribution (so-called hyperparameters) rather than computing integrals.\nWe saw this for the beta distribution in the last table. For many more examples see:\nhttps://en.wikipedia.org/wiki/Conjugate_prior_distribution\nWe now give a definition of conjugate prior. It is best understood through the examples in\nthe subsequent sections.\nDefinition. Suppose we have data with likelihood function φ(x|θ) depending on a hy-\npothesized parameter θ.\nAlso suppose the prior distribution for θis one of a family of\nparametrized distributions. If the posterior distribution for θis in this family then we say\nthe the family of priors are conjugate priors for the likelihood.\nThis definition will be illustrated with specific examples in the sections below.\nBeta priors\nIn this section, we will show that the beta distribution is a conjugate prior for binomial,\nBernoulli, and geometric likelihoods.\n\n18.05 Class 15, Conjugate priors: Beta and normal, Spring 2022\n5.1\nBinomial likelihood\nWe saw above that the beta distribution is a conjugate prior for the binomial distribution.\nThis means that if the likelihood function is binomial and the prior distribution is beta then\nthe posterior is also beta.\nMore specifically, suppose that the likelihood follows a binomial(N, θ) distribution where N\nis known and θis the (unknown) parameter of interest. We also have that the data xfrom\none trial is an integer between 0 and N. Then for a beta prior we have the following table:\nhypoth.\ndata\nprior\nlikelihood\nposterior\nθ\nx\nBeta(a, b)\nf(θ) = c1θa-1(1 -θ)b-1\nbinomial(N, θ)\np(x|θ) = c2θx(1 -θ)N-x\nBeta(a+ x, b+ N-x)\nf(θ|x) = c3θa+x-1(1 -θ)b+N-x-1\nThe table is simplified by writing the normalizing coefficients as c1, c2 and c3 respectively.\nIf needed, we can recover the values of the c1 and c2 by recalling (or looking up) the\nnormalizations of the beta and binomial distributions.\nc1 =\n(a+ b-1)!\n(a-1)! (b-1)!\nc2 = (N\nx) =\nN!\nx! (N-x)!\nc3 =\n(a+ b+ N-1)!\n(a+ x-1)! (b+ N-x-1)!\n5.2\nBernoulli likelihood\nThe beta distribution is a conjugate prior for the Bernoulli distribution. This is actually\na special case of the binomial distribution, since Bernoulli(θ) is the same as binomial(1,\nθ). We do it separately because it is slightly simpler and of special importance. In the\ntable below, we show the updates corresponding to success (x= 1) and failure (x= 0) on\nseparate rows.\nhypothesis\ndata\nprior\nlikelihood\nposterior\nθ\nx\nBeta(a, b)\nBernoulli(θ)\nBeta(a+ 1, b) or Beta(a, b+ 1)\nθ\nx= 1\nf(θ) = c1θa-1(1 -θ)b-1\np(x|θ) = θ\nBeta(a+ 1, b): f(θ|x) = c3θa(1 -θ)b-1\nθ\nx= 0\nf(θ) = c1θa-1(1 -θ)b-1\np(x|θ) = 1 -θ\nBeta(a, b+ 1): f(θ|x) = c4θa-1(1 -θ)b\nThe constants c1, c3 and c4 have the same formulas as in the previous (binomial likelihood\ncase) with N= 1.\n5.3\nGeometric likelihood\nRecall that the geometric(θ) distribution describes the probability of xsuccesses before\nthe first failure, where the probability of success on any single independent trial is θ. The\ncorresponding pmf is given by p(x) = θx(1 -θ).\nNow suppose that we have a data point x, and our hypothesis θis that xis drawn from a\ngeometric(θ) distribution. From the table we see that the beta distribution is a conjugate\nprior for a geometric likelihood as well:\nhypothesis\ndata\nprior\nlikelihood\nposterior\nθ\nx\nBeta(a, b)\n= f(θ) = c1θa-1(1 -θ)b-1\ngeometric(θ)\n= p(x|θ) = θx(1 -θ)\nBeta(a+ x, b+ 1)\nf(θ|x) = c3θa+x-1(1 -θ)b\n\n18.05 Class 15, Conjugate priors: Beta and normal, Spring 2022\nAt first it may seem strange that the beta distribution is a conjugate prior for both the\nbinomial and geometric distributions. The key reason is that the geometric likelihood is\nproportional to a binomial likelihood as a function of θ. Let's illustrate this in a concrete\nexample.\nExample 3. While traveling through the Mushroom Kingdom, Mario and Luigi find some\nrather unusual coins. They agree on a prior of f(θ) ∼Beta(5,5) for the probability of heads,\nthough they disagree on what experiment to run to investigate θfurther.\n(a) Mario decides to flip a coin 5 times. He gets four heads in five flips.\n(b) Luigi decides to flip a coin until the first tails. He gets four heads before the first tail.\nShow that Mario and Luigi will arrive at the same posterior on θ, and calculate this posterior.\nSolution: We will show that both Mario and Luigi find the posterior pdf for θis a Beta(9, 6)\ndistribution.\nMario's table\nhypothesis\ndata\nprior\nlikelihood\nposterior\nθ\nx= 4\nBeta(5, 5)\n= c1θ4(1 -θ)4\nbinomial(5, θ)\n= (5\n4)θ4(1 -θ)\n???\n= c3θ8(1 -θ)5\nLuigi's table\nhypothesis\ndata\nprior\nlikelihood\nposterior\nθ\nx= 4\nBeta(5, 5)\n= c1θ4(1 -θ)4\ngeometric(θ)\n= θ4(1 -θ)\n???\n= c3θ8(1 -θ)5\nSince both Mario and Luigi's posteriors have the form of a Beta(9, 6) distribution that's\nwhat they both must be. The normalizing factor must be the same in both cases because\nit's determined by requiring the total probability to be 1.\nBayesian updating with continuous hypotheses and contin-\nuous data\nThe idea here is essentially identical to the Bayesian updating we've already done. The\nonly change is, with a continuous likelihood, we have to compute the total probability of\nthe data (i.e. sum of the Bayes numerator column, i.e. normalizing factor) as an integral\ninstead of a sum. We will cover this briefly. For those who are interested, a bit more detail\nis given in an optional note.\nNotation\n- Hypotheses θ. For continuous hypotheses, this really means that we hypothesize that\nthe parameter is in a small interval of size dθaround θ.\n- Data x. For continuous data, this really means that the data is in a small interval of\nsize dxaround x.\n- Prior f(θ)dθ. This is our initial belief about the probability that the parameter is in\na small interval of size dθaround θ.\n\n18.05 Class 15, Conjugate priors: Beta and normal, Spring 2022\n- Likelihood φ(x| θ). So the probability that the data is in a small interval of size dx\naround x, ASSUMING the hypothesis θis φ(x| θ) dx\n- Posterior f(θ| x) dθ. This is the (calculated) probability that the parameter is in a\nsmall interval of size dθaround θ, GIVEN the data x.\nBayes\nhypoth.\nprior\nlikelihood\nnumerator\nposterior\nθ\nf(θ) dθ\nφ(x| θ)\nφ(x| θ)f(θ) dθ\nf(θ| x) dθ= φ(x| θ)f(θ) dθ\nφ(x)\ntotal\nno sum\nφ(x) = ∫φ(x| θ)f(θ) dθ\n(integrate over θ)\n= prior prob. density for data x\nContinuous-continuous Bayesian update table\nTo summarize: the prior probabilities of hypotheses and the likelihoods of data given hy-\npothesis were given; the Bayes numerator is the product of the prior and likelihood; the\ntotal likelihood φ(x) is the integral of the probabilities in the Bayes numerator column; we\ndivide by φ(x) to normalize the Bayes numerator.\nNormal begets normal\nWe now turn to an important example of coninuous-continuous updating: the normal dis-\ntribution is its own conjugate prior. In particular, if the likelihood function is normal with\nknown variance, then a normal prior gives a normal posterior. Now both the hypotheses\nand the data are continuous.\nSuppose we have a measurement x∼N(θ, σ2) where the variance σ2 is known. That is, the\nmean θis our unknown parameter of interest and we are given that the likelihood comes\nfrom a normal distribution with variance σ2. If we choose a normal prior pdf\nf(θ) ∼N(μprior, σ2\nprior)\nthen the posterior pdf is also normal: f(θ|x) ∼N(μpost, σ2\npost) where\nμpost\nσ2\npost\n= μprior\nσ2\nprior\n+ x\nσ2 ,\nσ2\npost\n=\nσ2\nprior\n+ 1\nσ2\n(1)\nThe following form of these formulas is easier to read and shows that μpost is a weighted\naverage between μprior and the data x.\na=\nσ2\nprior\nb= 1\nσ2 ,\nμpost = aμprior + bx\na+ b\n,\nσ2\npost =\na+ b.\n(2)\nWith these formulas in mind, we can express the update via the table:\n\n18.05 Class 15, Conjugate priors: Beta and normal, Spring 2022\nhypothesis\ndata\nprior\nlikelihood\nposterior\nθ\nx\nf(θ) ∼N(μprior, σ2\nprior)\n= c1 exp (\n-(θ-μprior)2\n2σ2\nprior\n)\nφ(x|θ) ∼N(θ, σ2)\n= c2 exp ( -(x-θ)2\n2σ2\n)\nf(θ|x) ∼N(μpost, σ2\npost)\n= c3 exp (\n-(θ-μpost)2\n2σ2\npost\n)\nWe leave the proof of the general formulas to the problem set. It is an involved algebraic\nmanipulation which is essentially the same as the following numerical example.\nExample 4. Suppose we have prior θ∼N(4, 8), and likelihood function likelihood x∼\nN(θ, 5). Suppose also that we have one measurement x1 = 3. Show the posterior distribution\nis normal.\nSolution: We will show this by grinding through the algebra which involves completing\nthe square.\nprior: f(θ) = c1 e-(θ-4)2/16;\nlikelihood: φ(x1|θ) = c2 e-(x1-θ)2/10 = c2 e-(3-θ)2/10\nWe multiply the prior and likelihood to get the posterior:\nf(θ|x1) = c3 e-(θ-4)2/16 e-(3-θ)2/10\n= c3 exp (-(θ-4)2\n-(3 -θ)2\n)\nWe complete the square in the exponent\n-(θ-4)2\n-(3 -θ)2\n= -5(θ-4)2 + 8(3 -θ)2\n= -13θ2 -88θ+ 152\n= -θ2 -88\n13θ+ 152\n80/13\n= -(θ-44/13)2 + 152/13 -(44/13)2\n80/13\n.\nTherefore the posterior is\nf(θ|x1) = c3 e-(θ-44/13)2+152/13-(44/13)2\n80/13\n= c4 e-(θ-44/13)2\n80/13\n.\nThis has the form of the pdf for N(44/13, 40/13).\nQED\nFor practice we check this against the formulas (2).\nμprior = 4,\nσ2\nprior = 8,\nσ2 = 5 ⇒a= 1\n8,\nb= 1\n5.\nTherefore\nμpost = aμprior + bx\na+ b\n= 44\n13 = 3.38\nσ2\npost =\na+ b= 40\n13 = 3.08.\n\n18.05 Class 15, Conjugate priors: Beta and normal, Spring 2022\n7.1\nA word on weighted averages\nThe updating formula 2 gives μpost as a weighted average of the μprior and the data. The\nweight on μprior is a/(a+ b), and the weight on the data is b/(a+ b). These weights are\nalways positive numbers summing to 1. If bis very large (that is, if the data has a tiny\nvariance) then most of the weight is on the data. If ais very large (that is, σ2\nprior is small,\ni.e. if you are very confident in your prior) then most of the weight is on the prior.\nIn the above example the variance on the prior was bigger than the variance on the data,\nso awas smaller than b; so the weight was mostly on the data. The posterior 3.38 for the\nmean was closer to the data 3 than to the prior 4 for the mean.\n7.2\nExamples of normal-normal updating\nExample 5. Suppose that we know the data x∼N(θ, 4/9) and we have prior N(0, 1). We\nget one data value x= 6.5. Describe the changes to the pdf for θin updating from the\nprior to the posterior.\nSolution: μprior = 0, σ2\nprior = 1, σ2 = 4/9. So, using the updating formulas 2 we have\na= 1,\nb=\n4/9 = 9\n4,\nμpost = aμprior + bx\na+ b\n= 4.5,\nσ2\npost =\na+ b= 4\n13.\nHere is a graph of the prior and posterior pdfs with the data point marked by a red line.\n-2\n0.0\n0.2\n0.4\n0.6\nPrior in blue, posterior in orange, data = red line\nWe see that the posterior mean is closer to the data point than the prior mean We also see\nthat the posterior distribution is taller and narrower than the prior, i.e. it has a smaller\nvariance. The smaller variance says that we are now more certain about where the value of\nθlies.\nExample 6. Use the formulas 2 to show that for normal-normal Bayesian updating we\nhave:\n1. The posterior mean is always between the data point and the prior mean.\n2.\nThe posterior variance is smaller than both the prior variance and σ.\nThat is, our\n\n18.05 Class 15, Conjugate priors: Beta and normal, Spring 2022\nposterior uncetainty is smaller than both our prior uncertainty and the uncertainty in the\ndata.\nSolution: Using the update formulas 2, we have The posterior mean is the weighted average\nof the prior mean and the data, so it must lie between the prior mean and the data.\nAlso, the posterior variance is\nσ2\npost =\na+ b< 1\na= σ2\nprior\nThat is the posterior has smaller variance than the prior, i.e. data makes us more certain\nabout where in its range θlies.\nLikewise σ2\npost =\na+ b< 1\nb= σ2. So, the posterior variance is smaller than σ2.\n7.3\nMore than one data point\nExample 7. Suppose we have data x1, x2, x3. Use the formulas (1) to update sequentially.\nSolution: Let's label the prior mean and variance as μ0 and σ2\n0. The updated means and\nvariances will be μiand σ2\ni. In sequence we have\nσ2\n= 1\nσ2\n+ 1\nσ2 ;\nμ1\nσ2\n= μ0\nσ2\n+ x1\nσ2\nσ2\n= 1\nσ2\n+ 1\nσ2 = 1\nσ2\n+ 2\nσ2 ;\nμ2\nσ2\n= μ1\nσ2\n+ x2\nσ2 = μ0\nσ2\n+ x1 + x2\nσ2\nσ2\n= 1\nσ2\n+ 1\nσ2 = 1\nσ2\n+ 3\nσ2 ;\nμ3\nσ2\n= μ2\nσ2\n+ x3\nσ2 = μ0\nσ2\n+ x1 + x2 + x3\nσ2\nThe example generalizes to ndata values x1, ... , xn:\nNormal-normal update formulas for ndata points\nμpost\nσ2\npost\n= μprior\nσ2\nprior\n+ nx\nσ2 ,\nσ2\npost\n=\nσ2\nprior\n+ n\nσ2 ,\nx= x1 + ... + xn\nn\n.\n(3)\nAgain we give the easier to read form, showing μpost is a weighted average of μprior and the\nsample average\nx:\na=\nσ2\nprior\nb= n\nσ2 ,\nμpost = aμprior + bx\na+ b\n,\nσ2\npost =\na+ b.\n(4)\nInterpretation: μpost is a weighted average of μprior and\nx. If the number of data points is\nlarge then the weight bis large and\nxwill have a strong influence on the posterior. If σ2\nprior\nis small then the weight ais large and μprior will have a strong influence on the posterior.\nTo summarize:\n1. Lots of data has a big influence on the posterior.\n2. High certainty (low variance) in the prior has a big influence on the posterior.\nThe actual posterior is a balance of these two influences.\n\nChoosing priors\nClass 16, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Learn that the choice of prior affects the posterior.\n2. See that too rigid a prior can make it difficult to learn from the data.\n3. See that more data lessens the dependence of the posterior on the prior.\n4. Be able to make a reasonable choice of prior, based on prior understanding of the system\nunder consideration.\nIntroduction\nUp to now we have always been handed a prior pdf. In this case, statistical inference from\ndata is essentially an application of Bayes' theorem. When the prior is known there is no\ncontroversy on how to proceed. The art of statistics starts when the prior is not known\nwith certainty. There are two main schools on how to proceed in this case: Bayesian and\nfrequentist. For now we are following the Bayesian approach. Starting next week we will\nlearn the frequentist approach.\nRecall that given data Dand a hypothesis Hwe used Bayes' theorem to write\nP(H|D) = P(D|H) ⋅P(H)\nP(D)\nposterior ∝likelihood ⋅prior.\nBayesian: Bayesians make inferences using the posterior P(H|D), and therefore always\nneed a prior P(H). If a prior is not known with certainty the Bayesian must try to make\na reasonable choice. There are many ways to do this and reasonable people might make\ndifferent choices. In general it is good practice to justify your choices and to explore a range\nof priors to see if they all point to the same conclusion.\nFrequentist: Very briefly, frequentists do not try to create a prior. Instead, they make\ninferences using the likelihood P(D|H).\nWe will compare the two approaches in detail once we have more experience with each. For\nnow we simply list two benefits of the Bayesian approach.\n1. The posterior probability P(H|D) for the hypothesis given the evidence is usually exactly\nwhat we'd like to know. The Bayesian can say something like 'the parameter of interest has\nprobability 0.95 of being between 0.49 and 0.51.'\n2. The assumptions that go into choosing the prior can be clearly spelled out.\nMore good data: It is always the case that more and better data allows for stronger\nconclusions and lessens the influence of the prior. The emphasis should be as much on\nbetter data (quality) as on more data (quantity).\n\n18.05 Class 16, Choosing priors, Spring 2022\nExample: Dice\nSuppose we have a drawer full of dice, each of which has either 4, 6, 8, 12, or 20 sides. This\ntime, we do not know how many of each type are in the drawer. A die is picked at random\nfrom the drawer and rolled 5 times. The results in order are 4, 2, 4, 7, and 5.\n3.1\nUniform prior\nSuppose we have no idea what the distribution of dice in the drawer might be. In this case\nit's reasonable to use a flat prior. Here is the update table for the posterior probabilities\nthat result from updating after each roll. In order to fit all the columns, we leave out the\nBayes numerators.\nhyp.\nprior\nlik1\npost1\nlik2\npost2\nlik3\npost3\nlik4\npost4\nlik5\npost5\nH4\n1/5\n1/4\n0.370\n1/4\n0.542\n1/4\n0.682\n0.000\n0.000\nH6\n1/5\n1/6\n0.247\n1/6\n0.241\n1/6\n0.202\n0.000\n1/6\n0.000\nH8\n1/5\n1/8\n0.185\n1/8\n0.135\n1/8\n0.085\n1/8\n0.818\n1/8\n0.876\nH12\n1/5\n1/12\n0.123\n1/12\n0.060\n1/12\n0.025\n1/12\n0.161\n1/12\n0.115\nH20\n1/5\n1/20\n0.074\n1/20\n0.022\n1/20\n0.005\n1/20\n0.021\n1/20\n0.009\nThis should look familiar. Given the data the final posterior is heavily weighted towards\nhypthesis H8 that the 8-sided die was picked.\n3.2\nOther priors\nTo see how much the above posterior depended on our choice of prior, let's try some other\npriors. Suppose we have reason to believe that there are ten times as many 20-sided dice\nin the drawer as there are each of the other types. The table becomes:\nhyp.\nprior\nlik1\npost1\nlik2\npost2\nlik3\npost3\nlik4\npost4\nlik5\npost5\nH4\n0.071\n1/4\n0.222\n1/4\n0.453\n1/4\n0.650\n0.000\n0.000\nH6\n0.071\n1/6\n0.148\n1/6\n0.202\n1/6\n0.193\n0.000\n1/6\n0.000\nH8\n0.071\n1/8\n0.111\n1/8\n0.113\n1/8\n0.081\n1/8\n0.688\n1/8\n0.810\nH12\n0.071\n1/12\n0.074\n1/12\n0.050\n1/12\n0.024\n1/12\n0.136\n1/12\n0.107\nH20\n0.714\n1/20\n0.444\n1/20\n0.181\n1/20\n0.052\n1/20\n0.176\n1/20\n0.083\nEven here the final posterior is heavily weighted to the hypothesis H8.\nWhat if the 20-sided die is 100 times more likely than each of the others?\nhyp.\nprior\nlik1\npost1\nlik2\npost2\nlik3\npost3\nlik4\npost4\nlik5\npost5\nH4\n0.0096\n1/4\n0.044\n1/4\n0.172\n1/4\n0.443\n0.000\n0.000\nH6\n0.0096\n1/6\n0.030\n1/6\n0.077\n1/6\n0.131\n0.000\n1/6\n0.000\nH8\n0.0096\n1/8\n0.022\n1/8\n0.043\n1/8\n0.055\n1/8\n0.266\n1/8\n0.464\nH12\n0.0096\n1/12\n0.015\n1/12\n0.019\n1/12\n0.016\n1/12\n0.053\n1/12\n0.061\nH20\n0.9615\n1/20\n0.889\n1/20\n0.689\n1/20\n0.354\n1/20\n0.681\n1/20\n0.475\nWith such a strong prior belief in the 20-sided die, the final posterior gives a lot of weight\nto the theory that the data arose from a 20-sided die, even though it extremely unlikely the\n\n18.05 Class 16, Choosing priors, Spring 2022\n20-sided die would produce a maximum of 7 in 5 roles. The posterior now gives roughly\neven odds that an 8-sided die versus a 20-sided die was picked.\n3.3\nRigid priors\nMild cognitive dissonance. Too rigid a prior belief can overwhelm any amount of data.\nSuppose I've got it in my head that the die has to be 20-sided.\nSo I set my prior to\nP(H20) = 1 with the other 4 hypotheses having probability 0. Look what happens in the\nupdate table.\nhyp.\nprior\nlik1\npost1\nlik2\npost2\nlik3\npost3\nlik4\npost4\nlik5\npost5\nH4\n1/4\n1/4\n1/4\nH6\n1/6\n1/6\n1/6\n1/6\nH8\n1/8\n1/8\n1/8\n1/8\n1/8\nH12\n1/12\n1/12\n1/12\n1/12\n1/12\nH20\n1/20\n1/20\n1/20\n1/20\n1/20\nNo matter what the data, a hypothesis with prior probability 0 will have posterior probabil-\nity 0. In this case I'll never get away from the hypothesis H20, although I might experience\nsome mild cognitive dissonance.\nSevere cognitive dissonance. Rigid priors can also lead to absurdities. Suppose I now\nhave it in my head that the die must be 4-sided. So I set P(H4) = 1 and the other prior\nprobabilities to 0. With the given data on the fourth roll I reach an impasse. A roll of 7\ncan't possibly come from a 4-sided die. Yet this is the only hypothesis I'll allow. My Bayes\nnumerator is a column of all zeros which cannot be normalized.\nhyp.\nprior\nlik1\npost1\nlik2\npost2\nlik3\npost3\nlik4\nBayes numer4\npost4\nH4\n1/4\n1/4\n1/4\n???\nH6\n1/6\n1/6\n1/6\n???\nH8\n1/8\n1/8\n1/8\n1/8\n???\nH12\n1/12\n1/12\n1/12\n1/12\n???\nH20\n1/20\n1/20\n1/20\n1/20\n???\nI must adjust my belief about what is possible or, more likely, I'll suspect you of accidently\nor deliberately messing up the data.\nExample: Malaria\nHere is a real example adapted from Statistics, A Bayesian Perspective by Donald Berry:\nBy the 1950s scientists had begun to formulate the hypothesis that carriers of the sickle-cell\ngene were more resistant to malaria than noncarriers. There was a fair amount of circum-\nstantial evidence for this hypothesis. It also helped explain the persistance of an otherwise\ndeleterious gene in the population. In one experiment scientists injected 30 African volun-\nteers with malaria. Fifteen of the volunteers carried one copy of the sickle-cell gene and the\nother 15 were noncarriers. Fourteen out of 15 noncarriers developed malaria while only 2\n\n18.05 Class 16, Choosing priors, Spring 2022\nout of 15 carriers did. Does this small sample support the hypothesis that the sickle-cell\ngene protects against malaria?\nLet Srepresent a carrier of the sickle-cell gene and Nrepresent a non-carrier. Let D+\nindicate developing malaria and D-indicate not developing malaria. The data can be put\nin a table.\nD+\nD-\nS\nN\nBefore analysing the data we should say a few words about the experiment and experimental\ndesign. First, it is clearly unethical: to gain some information they infected 16 people with\nmalaria. We also need to worry about bias. How did they choose the test subjects? Is\nit possible the noncarriers were weaker and thus more susceptible to malaria than the\ncarriers? Berry points out that it is reasonable to assume that an injection is similar to\na mosquito bite, but it is not guaranteed. This last point means that if the experiment\nshows a relation between sickle-cell and protection against injected malaria, we need to\nconsider the hypothesis that the protection from mosquito transmitted malaria is weaker or\nnon-existent. Finally, we will frame our hypothesis as 'sickle-cell protects against malaria',\nbut really all we can hope to say from a study like this is that 'sickle-cell is correlated with\nprotection against malaria'.\nModel. For our model let θSbe the probability that an injected carrier Sdevelops malaria\nand likewise let θNbe the probability that an injected noncarrier Ndevelops malaria. We\nassume independence between all the experimental subjects. With this model, the likelihood\nis a function of both θSand θN:\nP(data|θS, θN) = cθ2\nS(1 -θS)13θ14\nN(1 -θN).\nAs usual we leave the constant factor cas a letter.\n(It is a product of two binomial\ncoefficients: c= (15\n2 )(15\n14).)\nHypotheses. Each hypothesis consists of a pair (θN, θS). To keep things simple we will\nonly consider a finite number of values for these probabilities. We could easily consider\nmany more values or even a continuous range of hypotheses. Assume θSand θNare each\none of 0, 0.2, 0.4, 0.6, 0.8, 1. This leads to two-dimensional tables.\nFirst is a table of hypotheses. The color coding indicates the following:\n1.\nLight blue squares along the diagonal are where θS= θN, i.e.\nsickle-cell makes no\ndifference one way or the other.\n2. Orange and darker blue squares above the diagonal are where θN> θS, i.e. sickle-cell\nprovides some protection against malaria.\n3. In the orange squares θN-θS≥0.6, i.e. sickle-cell provides a lot of protection.\n4. White squares below diagonal are where θS> θN, i.e. sickle-cell actually increases the\nprobability of developing malaria.\n\n18.05 Class 16, Choosing priors, Spring 2022\nθN\\θS\n0.2\n0.4\n0.6\n0.8\n(0,1)\n(.2,1)\n(.4,1)\n(.6,1)\n(.8,1)\n(1,1)\n0.8\n(0,.8) (.2,.8) (.4,.8) (.6,.8) (.8,.8) (1,.8)\n0.6\n(0,.6) (.2,.6) (.4,.6) (.6,.6) (.8,.6) (1,.6)\n0.4\n(0,.4) (.2,.4) (.4,.4) (.6,.4) (.8,.4) (1,.4)\n0.2\n(0,.2) (.2,.2) (.4,.2) (.6,.2) (.8,.2) (1,.2)\n(0,0)\n(.2,0)\n(.4,0)\n(.6,0)\n(.8,0)\n(1,0)\nHypotheses on level of protection due to S:\norange = strong;\ndarker blue = some;\nlight blue = none;\nwhite = negative.\nNext is the table of likelihoods. (Actually we've taken advantage of our indifference to scale\nand scaled all the likelihoods by 100000/cto make the table more presentable.) Notice that,\nto the precision of the table, many of the likelihoods are 0. The color coding is the same as\nin the hypothesis table. We've highlighted the biggest likelihoods with a thick black border.\nθN\\θS\n0.2\n0.4\n0.6\n0.8\n0.00000 0.00000 0.00000 0.00000 0.00000 0.00000\n0.8\n0.00000 1.93428 0.18381 0.00213 0.00000 0.00000\n0.6\n0.00000 0.06893 0.00655 0.00008 0.00000 0.00000\n0.4\n0.00000 0.00035 0.00003 0.00000 0.00000 0.00000\n0.2\n0.00000 0.00000 0.00000 0.00000 0.00000 0.00000\n0.00000 0.00000 0.00000 0.00000 0.00000 0.00000\nLikelihoods p(data|θS, θN) scaled by 100000/c\n4.1\nFlat prior\nSuppose we have no opinion whatsoever on whether and to what degree sickle-cell protects\nagainst malaria. In this case it is reasonable to use a flat prior. Since there are 36 hypotheses\neach one gets a prior probability of 1/36. This is given in the table below. Remember each\nsquare in the table represents one hypothesis. Because it is a probability table we include\nthe marginal pmfs.\nθN\\θS\n0.2\n0.4\n0.6\n0.8\np(θN)\n1/36 1/36 1/36 1/36 1/36 1/36\n1/6\n0.8\n1/36 1/36 1/36 1/36 1/36 1/36\n1/6\n0.6\n1/36 1/36 1/36 1/36 1/36 1/36\n1/6\n0.4\n1/36 1/36 1/36 1/36 1/36 1/36\n1/6\n0.2\n1/36 1/36 1/36 1/36 1/36 1/36\n1/6\n1/36 1/36 1/36 1/36 1/36 1/36\n1/6\np(θS)\n1/6\n1/6\n1/6\n1/6\n1/6\n1/6\nFlat prior p(θS, θN): every hypothesis (square) has equal probability\nTo compute the posterior we simply multiply the likelihood table by the prior table and\n\n18.05 Class 16, Choosing priors, Spring 2022\nnormalize. Normalization means making sure the entire table sums to 1.\nθN\\θS\n0.2\n0.4\n0.6\n0.8\np(θN|data)\n0.00000 0.00000 0.00000 0.00000 0.00000 0.00000\n0.00000\n0.8\n0.00000 0.88075 0.08370 0.00097 0.00000 0.00000\n0.96542\n0.6\n0.00000 0.03139 0.00298 0.00003 0.00000 0.00000\n0.03440\n0.4\n0.00000 0.00016 0.00002 0.00000 0.00000 0.00000\n0.00018\n0.2\n0.00000 0.00000 0.00000 0.00000 0.00000 0.00000\n0.00000\n0.00000 0.00000 0.00000 0.00000 0.00000 0.00000\n0.00000\np(θS|data) 0.00000 0.91230 0.08670 0.00100 0.00000 0.00000\n1.00000\nPosterior to flat prior: p(θS, θN|data)\nTo decide whether Sconfers protection against malaria, we compute the posterior proba-\nbilities of 'some protection' and of 'strong protection'. These are computed by summing the\ncorresponding squares in the posterior table.\nSome protection: P(θN> θS) = sum of orange and darker blue = 0.99995\nStrong protection: P(θN-θS> 0.6) = sum of orange = 0.88075\nWorking from the flat prior, it is effectively certain that sickle-cell provides some protection\nand very probable that it provides strong protection.\n4.2\nInformed prior\nThe experiment was not run without prior information. There was a lot of circumstantial\nevidence that the sickle-cell gene offered some protection against malaria. For example it\nwas reported that a greater percentage of carriers survived to adulthood.\nHere's one way to build an informed prior. We'll reserve a reasonable amount of probability\nfor the hypotheses that Sgives no protection. Let's say 24% split evenly among the 6 (light\nblue) cells where θN= θS. We know we shouldn't set any prior probabilities to 0, so let's\nspread 6% of the probability evenly among the 15 white cells below the diagonal. That\nleaves 70% of the probability for the 15 orange and darker blue squares above the diagonal.\nθN\\θS\n0.2\n0.4\n0.6\n0.8\np(θN)\n0.04667 0.04667 0.04667 0.04667 0.04667 0.04000 0.27333\n0.8\n0.04667 0.04667 0.04667 0.04667 0.04000 0.00400 0.23067\n0.6\n0.04667 0.04667 0.04667 0.04000 0.00400 0.00400 0.18800\n0.4\n0.04667 0.04667 0.04000 0.00400 0.00400 0.00400 0.14533\n0.2\n0.04667 0.04000 0.00400 0.00400 0.00400 0.00400 0.10267\n0.04000 0.00400 0.00400 0.00400 0.00400 0.00400 0.06000\np(θS)\n0.27333 0.23067 0.18800 0.14533 0.10267 0.06000\n1.0\nInformed prior p(θS, θN): makes use of prior information that sickle-cell is protective.\nWe then compute the posterior pmf.\n\n18.05 Class 16, Choosing priors, Spring 2022\nθN\\θS\n0.2\n0.4\n0.6\n0.8\np(θN|data)\n0.00000 0.00000 0.00000 0.00000 0.00000 0.00000\n0.00000\n0.8\n0.00000 0.88076 0.08370 0.00097 0.00000 0.00000\n0.96543\n0.6\n0.00000 0.03139 0.00298 0.00003 0.00000 0.00000\n0.03440\n0.4\n0.00000 0.00016 0.00001 0.00000 0.00000 0.00000\n0.00017\n0.2\n0.00000 0.00000 0.00000 0.00000 0.00000 0.00000\n0.00000\n0.00000 0.00000 0.00000 0.00000 0.00000 0.00000\n0.00000\np(θS|data) 0.00000 0.91231 0.08669 0.00100 0.00000 0.00000\n1.00000\nPosterior to informed prior: p(θS, θN|data)\nWe again compute the posterior probabilities of 'some protection' and 'strong protection'.\nSome protection: P(θN> θS) = sum of orange and darker blue = 0.99996\nStrong protection: P(θN-θS> 0.6) = sum of orange = 0.88076\nNote that the informed posterior is nearly identical to the flat posterior.\n4.3\nPDALX\nThe following plot is based on the flat prior.\nFor each x, it gives the probability that\nθN-θS≥x. To make it smooth we used many more hypotheses.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nx\nProb. diff. at least x\nProbability the difference θN-θSis at least x(PDALX).\nNotice that it is almost certain that the difference is at least 0.4.\n\nProbability intervals\nClass 16, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Be able to find probability intervals given a pmf or pdf.\n2. Understand how probability intervals summarize belief in Bayesian updating.\n3. Be able to use subjective probability intervals to construct reasonable priors.\n4. Be able to construct subjective probability intervals by systematically estimating quan-\ntiles.\nProbability intervals\nSuppose we have a pmf p(θ) or pdf f(θ) describing our belief about the value of an unknown\nparameter of interest θ.\nDefinition: A p-probability interval for θis an interval [a, b] with P(a≤θ≤b) = p.\nNotes.\n1. In the discrete case with pmf p(θ), this means ∑a≤θi≤bp(θi) = p.\n2. In the continuous case with pdf f(θ), this means ∫\nb\naf(θ) dθ= p.\n3.\nWe may say 90%-probability interval to mean 0.9-probability interval.\nProbability\nintervals are also called credible intervals to contrast them with confidence intervals, which\nwe will introduce in the frequentist unit.\nExample 1. Between the 0.05 and 0.55 quantiles is a 0.5 probability interval. There are\nmany 50% probability intervals, e.g. the interval from the 0.25 to the 0.75 quantiles.\nIn particular, notice that the p-probability interval for θis not unique.\nQ-notation. We can phrase probability intervals in terms of quantiles. Recall that the\ns-quantile for θis the value qswith P(θ≤qs) = s. So for s≤t, the amount of probability\nbetween the s-quantile and the t-quantile is just t-s.\nIn these terms, a p-probability\ninterval is any interval [qs, qt] with t-s= p.\nExample 2. We have 0.5 probability intervals [q0.25, q0.75] and [q0.05, q0.55].\nSymmetric probability intervals.\nThe interval [q0.25, q0.75] is symmetric because the amount of probability remaining on either\nside of the interval is the same, namely 0.25. If the pdf is not too skewed, the symmetric\ninterval is usually a good default choice.\nMore notes.\n\n18.05 Class 16, Probability intervals, Spring 2022\n1. Different p-probability intervals for θmay have different widths. We can make the width\nsmaller by centering the interval under the highest part of the pdf. Such an interval is\nusually a good choice since it contains the most likely values. See the examples below for\nnormal and beta distributions.\n2. Since the width can vary for fixed p, a larger pdoes not always mean a larger width.\nHere's what is true: if a p1-probability interval is fully contained in a p2-probability interval,\nthen p1 is smaller than p2.\nProbability intervals for a normal distribution. The figure shows a number of prob-\nability intervals for the standard normal.\n1. All of the blue bars span a 0.68-probability interval. Notice that the smallest blue bar\nruns between -1 and 1. This runs from the 16th percentile to the 84th percentile so it is a\nsymmetric interval.\n2. All the green bars span a 0.9-probability interval. They are longer than the blue bars\nbecause they include more probability. Note again that the shortest green bar is symmetric.\n-3\n-2\n-1\n-0.1\n0.0\n0.1\n0.2\n0.3\n0.4\nN(0, 1)\norange = 0.5, blue = 0.68, green = 0.9\nProbabilitiy intervals for a beta distribution. The following figure shows probability\nintervals for a beta distribution. Notice how the two blue bars have very different lengths\nyet cover the same probability p= 0.68.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nbeta(10, 4)\norange = 0.5, blue = 0.68, green = 0.9\n\n18.05 Class 16, Probability intervals, Spring 2022\nUses of probability intervals\n3.1\nSummarizing and communicating your beliefs\nProbability intervals are an intuitive and effective way to summarize and communicate your\nbeliefs. It's hard to describe an entire function f(θ) to a friend in words. If the function isn't\nfrom a parameterized family then it's especially hard. Even with a beta distribution, it's\neasier to interpret \"I think θis between 0.45 and 0.65 with 50% probability\" than \"I think θ\nfollows a beta(8,6) distribution\". An exception to this rule of communication might be the\nnormal distribution, but only if the recipient is also comfortable with standard deviation.\nOf course, what we gain in clarity we lose in precision, since the function contains more\ninformation than the probability interval.\nProbability intervals also play well with Bayesian updating. If we update from the prior\nf(θ) to the posterior f(θ|x), then the p-probability interval for the posterior will tend to be\nshorter than than the p-probability interval for the prior. In this sense, the data has made\nus more certain. See for example the election example below.\nConstructing a prior using subjective probability intervals\nProbability intervals are also useful when we do not have a pmf or pdf at hand. In this\ncase, subjective probability intervals give us a method for constructing a reasonable prior\nfor θ\"from scratch\". The thought process is to ask yourself a series of questions, e.g., 'what\nis my expected value for θ?'; 'my 0.5-probability interval?'; 'my 0.9-probability interval?'\nThen build a prior that is consistent with these intervals.\n4.1\nEstimating the intervals directly\nExample 3. Building priors\nIn 2013 there was a special election for a congressional seat in a district in South Carolina.\nThe election pitted Republican Mark Sanford against Democrat Elizabeth Colbert Busch.\nLet θbe the fraction of the population who favored Sanford. Our goal in this example is\nto build a subjective prior for θ. We'll use the following prior evidence.\n- Sanford is a former S. Carolina Congressman and Governor\n- In 2009, while Governor, he had to resign after he was discovered to be having an\naffair in Argentina while he claimed to be hiking the Appalachian trail.\n- In 2013 Sanford won the Republican primary over 15 primary opponents.\n- In the district in the 2012 presidential election the Republican Romney beat the\nDemocrat Obama 58% to 40%.\n- The Colbert bump: Elizabeth Colbert Busch is the sister of well-known comedian\nStephen Colbert.\n\n18.05 Class 16, Probability intervals, Spring 2022\nOur strategy will be to use our intuition to construct some probability intervals and then\nfind a beta distribution that approximately matches these intervals. This is subjective so\nsomeone else might give a different answer.\nStep 1. Use the evidence to construct 0.5 and 0.9 probability intervals for θ.\nWe'll start by thinking about the 90% interval. The single strongest prior evidence is the\n58% to 40% of Romney over Obama. Given the negatives for Sanford we don't expect he'll\nwin much more than 58% of the vote. So we'll put the top of the 0.9 interval at 0.65. With\nall of Sanford's negatives he could lose big. So we'll put the bottom at 0.3.\n0.9 interval:\n[0.3, 0.65]\nFor the 0.5 interval we'll pull these endpoints in. It really seems unlikely Sanford will get\nmore votes than Romney, so we can leave 0.25 probability that he'll get above 57%. The\nlower limit seems harder to predict. So we'll leave 0.25 probability that he'll get under 42%.\n0.5 interval:\n[0.42, 0.57]\nStep 2. Use our 0.5 and 0.9 probability intervals to pick a beta distribution that approx-\nimats these intervals. We used the R function pbeta and a little trial and error to choose\nbeta(11,12). Here is our R code.\na = 11\nb = 12\npbeta(0.65, a, b) - pbeta(0.3, a, b)\npbeta(0.57, a, b) - pbeta(0.42, a, b)\nThis computed P([0.3, 0.65]) = 0.91 and P([0.42, 0.57]) = 0.52. So our intervals are actually\n0.91 and 0.52-probability intervals. This is pretty close to what we wanted!\nThe plot below shows the density of beta(11,12). The horizontal orange line shows our\ninterval [0.42, 0.57] and the blue line shows our interval [0.3, 0.65].\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPDF for beta(11,12)\nθ\n[0.42, 0.57] = 0.91 interval\n[0.3, 0.65] = 0.52 interval\nbeta(11,12) fitting 0.5 and 0.9 probability intervals\n4.2\nConstructing a prior by estimating quantiles\nThe method in Example 3 gives a good feel for building priors from probability intervals.\nHere we illustrate a slightly different way of building a prior by estimating quantiles. The\n\n18.05 Class 16, Probability intervals, Spring 2022\nbasic strategy is to first estimate the median, then divide and conquer to estimate the first\nand third quantiles. Finally you choose a prior distribution that fits these estimates.\nExample 4. Redo the Sanford vs. Colbert-Busch election example using quantiles.\nSolution: We start by estimating the median. Just as before the single strongest evidence\nis the 58% to 40% victory of Romney over Obama. However, given Sanford's negatives and\nBusch's Colbert bump we'll estimate the median at 0.47.\nIn a district that went 58 to 40 for the Republican Romney it's hard to imagine Sanford's\nvote going a lot below 40%. So we'll estimate Sanford 25th percentile as 0.40. Likewise,\ngiven his negatives it's hard to imagine him going above 58%, so we'll estimate his 75th\npercentile as 0.55.\nWe used R to search through values of aand bfor the beta distribution that matches these\nquartiles the best. Since the beta distribution does not require aand bto be integers we\nlooked for the best fit to 1 decimal place. We found beta(9.9, 11.0). Above is a plot of\nbeta(9.9,11.0) with its actual quartiles shown. These match the desired quartiles pretty\nwell.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPDF for beta(9.9,11.0)\nθ\nq0.25 = 0.399\nq0.5 = 0.472\nq0.75 = 0.547\nbeta(9.9, 11.0) matching desired quartiles\nHistoric note. In the election Sanford won 54% of the vote and Busch won 45.2%. (Source:\nhttps://elections.huffingtonpost.com/2013/mark-sanford-vs-elizabeth-colbert-busch-sc1\n\nThe Frequentist School of Statistics\nClass 17, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Be able to explain the difference between the frequentist and Bayesian approaches to\nstatistics.\n2. Know our working definition of a statistic and be able to distinguish a statistic from a\nnon-statistic.\nIntroduction\nAfter much foreshadowing, the time has finally come to switch from Bayesian statistics to\nfrequentist statistics. For much of the twentieth century, frequentist statistics has been the\ndominant school. If you've ever encountered confidence intervals, p-values, t-tests, or χ2-\ntests, you've seen frequentist statistics. With the rise of high-speed computing and big data,\nBayesian methods are becoming more common. After we've studied frequentist methods\nwe will compare the strengths and weaknesses of the two approaches.\n2.1\nThe fork in the road\nBoth schools of statistics start with probability. In particular both know and love Bayes'\ntheorem:\nP(H|D) = P(D|H) P(H)\nP(D)\n.\nWhen the prior is known exactly, all statisticians will use Bayes' formula. For Bayesian\ninference we take Hto be a hypothesis and Dsome data. Over the last few weeks we\nhave seen that, given a prior and a likelihood model, Bayes' theorem is a complete recipe\nfor updating our beliefs in the face of new data. This works perfectly when the prior was\nknown perfectly. We saw this in our dice examples. We also saw examples of a disease with\na known frequency in the general population and a screening test of known accuracy.\nIn practice we saw that there is usually no universally accepted prior - different people\nwill have different a priori beliefs - but we would still like to make useful inferences from\ndata. Bayesians and frequentists take fundamentally different approaches to this challenge,\nas summarized in the figure below.\n\n18.05 Class 17, The Frequentist School of Statistics, Spring 2022\nProbability\n(mathematics)\nStatistics\n(art)\nP(H|D) = P(D|H)P(H)\nP(D)\nEveryone uses Bayes'\nformula when the prior\nP(H) is known.\nPPosterior(H|D) = P(D|H)Pprior(H)\nP(D)\nLikelihood L(H; D) = P(D|H)\nBayesian path\nFrequentist path\nBayesians require a prior, so\nthey develop one from the best\ninformation they have.\nWithout a known prior, frequen-\ntists draw inferences from just\nthe likelihood function.\nThe reasons for this split are both practical (ease of implementation and computation) and\nphilosophical (subjectivity versus objectivity and the nature of probability).\n2.2\nWhat is probability?\nThe main philosophical difference concerns the meaning of probability. The term frequentist\nrefers to the idea that probabilities represent long term frequencies of repeatable random\nexperiments. For example, 'a coin has probability 1/2 of heads' means that the relative\nfrequency of heads (number of heads out of number of flips) goes to 1/2 as the number of\nflips goes to infinity. This means the frequentist finds it nonsensical to specify a probability\ndistribution for a parameter with a fixed value. While Bayesians are happy to use probability\nto describe their incomplete knowledge of a fixed parameter, frequentists reject the use of\nprobability to quantify degree of belief in hypotheses.\nExample 1. Suppose I have a bent coin with unknown probability θof heads. The value\nof θmay be unknown, but it is a fixed value. Thus, to the frequentist there can be no prior\npdf f(θ). By comparison the Bayesian may agree that θhas a fixed value, but interprets\nf(θ) as representing uncertainty about that value. Both the Bayesian and the frequentist\nare perfectly happy with p(heads | θ) = θ, since the longterm frequency of heads given θis\nθ.\nIn short, Bayesians put probability distributions on everything (hypotheses and data), while\nfrequentists put probability distributions on (random, repeatable, experimental) data given\na hypothesis. For the frequentist when dealing with data from an unknown distribution\nonly the likelihood has meaning. The prior and posterior do not.\nWorking definition of a statistic\nOur view of statistics is that it is the art of drawing conclusions (making inferences) from\ndata. With that in mind we can make a simple working definition of a statistic. There is a\nmore formal definition, but we don't need to introduce it at this point.\n\n18.05 Class 17, The Frequentist School of Statistics, Spring 2022\nStatistic. A statistic is anything that can be computed from data and known values. Some-\ntimes to be more precise we'll say a statistic is a rule for computing something from data\nand the value of the statistic is what is computed. This can include computing likelihoods\nwhere we hypothesize values of the model parameters. But it does not include anything\nthat requires we know the true value of a model parameter with unknown value.\nExamples. 1. The mean of data is a statistic. It is a rule that says given data x1, ... , xn\ncompute x1+...+xn\nn\n.\n2. The maximum of data is a statistic. It is a rule that says to pick the maximum value of\nthe data x1, ... , xn.\n3. Suppose x∼N(μ, 32) where μis unknown. Then the likelihood\nφ(x|μ= 7)\n=\n√\n2π\ne-(x-7)2\nis a statistic. However, the distance of xfrom the true mean μis not a statistic since we\ncannot compute it without knowing μ\n4. If our data x1, ... , xnis drawn from N(μ, 32), where μis unknown, then z= x-5\n3/√nis a\nstatistic, since it is computed from the data and known values. More generally, if μ0 is a\nknown value then z= x-μ0\n3/√nis a statistic. However, since μis not known, z= x-μ\n3/√nis\nnot a statistic.\nNote. We will usually stick with our Bayesian practice of using the symbol φfor continuous\nlikelihoods.\nThis will help remind us that Frequentists don't have prior and posterior\nprobabilities for hypotheses.\nPoint statistic. A point statistic is a single value computed from data. For example, the\nmean and the maximum are both point statistics. The maximum likelihood estimate is also\na point statistic since it is computed directly from the data based on a likelihood model.\nInterval statistic. An interval statistic is an interval computed from data. For example,\nthe range from the minimum to maximum of x1, ... , xnis an interval statistic, e.g. the data\n0.5, 1.0, 0.2, 3.0, 5.0 has range [0.2, 5.0].\nSet statistic. A set statistic is a set computed from data.\nExample. Suppose we have five dice: 4, 6, 8, 12 and 20-sided. We pick one at random and\nroll it once. The value of the roll is the data. The set of dice for which this roll is possible\nis a set statistic. For example, if the roll is a 10 then the value of this set statistic is {12,\n20}. If the roll is a 7 then this set statistic has value {8, 12, 20}.\nIt's important to remember that a statistic is itself a random variable since it is computed\nfrom random data. For example, if data is drawn from N(μ, σ2) then the mean of ndata\npoints follows the distribution N(μ, σ2/n)).\nSampling distribution. The probability distribution of a statistic is called its sampling\ndistribution.\nPoint estimate. We can use statistics to make a point estimate of a parameter θ. For\nexample, if our data is drawn from a normal distribution with unknown mean θ. Then the\ndata mean\nxis a point estimate of θ.\n\nNull Hypothesis Significance Testing I\nClass 17, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Know the definitions of the significance testing terms: null hypothesis, alternative hy-\npothesis, NHST, simple hypothesis, composite hypothesis, significance level, power.\n2. Be able to design and run a significance test for Bernoulli or binomial data.\n3. Be able to compute a p-value for a normal hypothesis and use it in a significance test.\nIntroduction\nFrequentist statistics is often applied in the framework of null hypothesis significance testing\n(NHST). We will look at the Neyman-Pearson paradigm which focuses on one hypothesis\ncalled the null hypothesis. There are other paradigms for hypothesis testing, but Neyman-\nPearson is the most common. Stated simply, this method asks if the data is well outside\nthe region where we would expect to see it under the null hypothesis. If so, then we reject\nthe null hypothesis in favor of a second hypothesis called the alternative hypothesis. The\nreasoning is that such extreme data is very unlikely in a world where the null hypothesis is\ntrue.\nWe have said before that statistics is an art. We will see that this statement is certainly\nvalid when we discuss choosing null and alternative hypotheses.\nThe computations done here all involve the likelihood function. There are two main differ-\nences between what we'll do here and what we did in Bayesian updating.\n1. The evidence of the data will be considered purely through the likelihood function: it\nwill not be weighted by our prior beliefs.\n2. We will need a notion of extreme data, e.g. 95 out of 100 heads in a coin toss or a mayfly\nthat lives for a month.\n2.1\nA suggestion of how to learn this material\nThere are seemingly a lot of terms with similar definitions. If you pay careful attention\nto the figures and how they are shaded and labeled we think you will find that is not so\ncomplicated.\n2.2\nMotivating examples\nExample 1. Suppose you want to decide whether a coin is fair. If you toss it 100 times\nand get 85 heads, would you think the coin is likely to be unfair? What about 60 heads? Or\n52 heads? Most people would guess that 85 heads is strong evidence that the coin is unfair,\n\n18.05 Class 17, Null Hypothesis Significance Testing I, Spring 2022\nwhereas 52 heads is no evidence at all. Sixty heads is less clear. Null hypothesis significance\ntesting (NHST) is a frequentist approach to thinking quantitatively about these questions.\nExample 2.\nSuppose you want to compare a new medical treatment to a placebo or\nto the current standard of care. What sort of evidence would convince you that the new\ntreatment is better than the placebo or the current standard? Again, NHST is a quantitative\nframework for answering these questions.\nSignificance testing\nWe'll start by listing the ingredients for NHST. Formally they are pretty simple. There is\nan art to choosing good ingredients. We will explore the art in examples. If you have never\nseen NHST before just scan this list now and come back to it after reading through the\nexamples and explanations given below.\n3.1\nIngredients\n- H0: the null hypothesis. This is the default assumption for the model generating the\ndata.\n- HA: the alternative hypothesis. If we reject the null hypothesis we accept this alter-\nnative as the best explanation for the data.\n- X: the test statistic. We compute this from the data. It is a random variable, because\nit is computed from random data.\n- Null distribution: the probability distribution of Xassuming H0.\n- Rejection region: if Xis in the rejection region we reject H0 in favor of HA.\n- Non-rejection region: the complement to the rejection region. If Xis in this region\nwe do not reject H0. Note that we say 'do not reject' rather than 'accept' because\nusually the best we can say is that the data does not support rejecting H0.\nThe null hypothesis H0 and the alternative hypothesis HAplay different roles. Typically\nwe choose H0 to be either a simple hypothesis or the default which we'll only reject if we\nhave enough evidence against it. The examples below will clarify this.\nNHST Terminology\nIn this section we will use one extended example to introduce and explore the terminology\nused in null hypothesis significance testing (NHST).\nExample 3. To test whether a coin is fair we flip it 10 times. If we get an unexpectedly\nlarge or small number of heads we'll suspect the coin is unfair. To make this precise in the\nlanguage of NHST we set up the ingredients as follows. Let θbe the probability that the\ncoin lands heads when flipped.\n\n18.05 Class 17, Null Hypothesis Significance Testing I, Spring 2022\n1. Null hypothesis: H0 = 'the coin is fair', i.e. θ= 0.5.\n2. Alternative hypothesis: HA= 'the coin is not fair', i.e. θ=0.5\n3. Test statistic: X= number of heads in 10 flips\n4. Null distribution: This is the probability function based on the null hypothesis\np(x| θ= 0.5) ∼binomial(10, 0.5).\nHere is the probability table for the null distribution.\nx\np(x| H0)\n.001\n.010\n.044\n.117\n.205\n.246\n.205\n.117\n.044\n.010\n.001\n5. Rejection region: under the null hypothesis we expect to get about 5 heads in 10 tosses.\nWhereas, under the alternate hypothesis we expect the number of heads to be biased either\nabove or below 5. So, we'll reject H0 in favor of HAif the number of heads is much fewer\nor greater than 5. Let's set the rejection region as {0, 1, 2, 8, 9, 10}. That is, if the number\nof heads in 10 tosses is in this region we will reject the hypothesis that the coin is fair in\nfavor of the hypothesis that it is not.\nWe can summarize all this in the graph and probability table below. The rejection region\nconsists of those values of xin orange, i.e. 0, 1, 2, 8, 9, 10. The probabilities corresponding\nto it are shaded in orange. We also show the null distribution as a stem plot with the\nrejection values of xin orange.\nx\np(x|H0) 0.001 0.010 0.044 0.117 0.205 0.246 0.205 0.117 0.044 0.010 0.001\nRejection region and null probabilities as a table for Example 3.\nx\np(x| H0)\n.05\n.15\n.25\nRejection region and null probabilities as a stem plot for Example 3.\nNotes for Example 3:\n1. The null hypothesis is the cautious default: we won't claim the coin is unfair unless we\nhave compelling evidence.\n2. The rejection region consists of data that is extreme under the null hypothesis and more\nlikely under the alternative hypothesis. That is, it consists of the outcomes that are in the\ntail of the null distribution away from the high probability center. As we'll discuss soon,\nhow far away depends on the significance level αof the test.\n3. If we get 3 heads in 10 tosses, then the test statistic is in the non-rejection region. The\nusual scientific language would be to say that the data 'does not support rejecting the null\nhypothesis'. Even if we got 5 heads, we would not claim that the data proves the null\nhypothesis is true.\nQuestion: If we have a fair coin what is the probability that we will decide incorrectly it\nis unfair?\n\n18.05 Class 17, Null Hypothesis Significance Testing I, Spring 2022\nSolution: The null hypothesis is that the coin is fair. The question asks for the probability\nthe data from a fair coin will be in the rejection region. That is, the probability that we\nwill get 0, 1, 2, 8, 9 or 10 heads in 10 tosses. This is the sum of the probabilities in the\ntable in shaded orange boxes. That is,\nP(rejecting H0 | H0 is true) = 0.11\nBelow we will continue with Example 3, define more terms used in NHST and see how to\nquantify properties of the significance test.\n4.1\nSimple and composite hypotheses\nDefinition: simple hypothesis: A simple hypothesis is one for which we can specify its\ndistribution completely. A typical simple hypothesis is that a parameter of interest takes a\nspecific value.\nDefinition: composite hypotheses: If its distribution cannot be fully specified, we say\nthat the hypothesis is composite. A typical composite hypothesis is that a parameter of\ninterest lies in a range of values.\nIn Example 3 the null hypothesis is that θ= 0.5, so the null distribution is binomial(10, 0.5).\nSince the null distribution is fully specified, H0 is simple. The alternative hypothesis is that\nθ=0.5. This is really many hypotheses in one: θcould be 0.51, 0.7, 0.99, etc. Since the\nalternative distribution binomial(10, θ) is not fully specified, HAis composite.\nExample 4. Suppose we have data x1, ... , xn. Suppose also that our hypotheses are\nH0: the data is drawn from N(0, 1)\nHA: the data is drawn from N(1, 1).\nThese are both simple hypotheses - each hypothesis completely specifies a distribution.\nExample 5. (Composite hypotheses.) Now suppose that our hypotheses are\nH0: the data is drawn from a Poisson distribution of unknown parameter.\nHA: the data is not drawn from a Poisson distribution.\nThese are both composite hypotheses, as they don't fully specify the distribution.\nExample 6. In an ESP experiment a subject is asked to identify the suits of 100 cards\ndrawn (with replacement) from a deck of cards. Let Tbe the number of successes. The\n(simple) null hypothesis that the subject does not have ESP is given by\nH0: T∼binomial(100, 0.25)\nThe (composite) alternative hypothesis that the subject has ESP is given by\nHA: T∼binomial(100, p) with p> 0.25\nAnother (composite) alternative hypothesis that something besides pure chance is going on,\ni.e. the subject has ESP or anti-ESP. This is given by\nHA: T∼binomial(100, p), with p=0.25\nValues of p< 0.25 represent hypotheses that the subject has a kind of anti-esp.\n\n18.05 Class 17, Null Hypothesis Significance Testing I, Spring 2022\n4.2\nTypes of error\nThere are two types of errors we can make. We can incorrectly reject the null hypothesis\nwhen it is true or we can incorrectly fail to reject it when it is false. These are unimagina-\ntively labeled type I and type II errors. We summarize this in the following table.\nTrue state of nature\nH0\nHA\nOur\nReject H0\nType I error\ncorrect decision\ndecision\n'Don't reject' H0\ncorrect decision\nType II error\nType I: false rejection of H0\nType II: false non-rejection ('acceptance') of H0\n4.3\nSignificance level and power\nSignificance level and power are used to quantify the quality of the significance test. Ideally\na significance test would not make errors. That is, it would not reject H0 when H0 was true\nand would reject H0 in favor of HAwhen HAwas true. Altogether there are 4 important\nprobabilities corresponding to the 2 × 2 table just above.\nP(reject H0|H0)\nP(reject H0|HA)\nP(do not reject H0|H0)\nP(do not reject H0|HA)\nThe two probabilities we focus on are:\nSignificance level\n= P(reject H0|H0)\n= probability we incorrectly reject H0\n= P(type I error).\nPower\n= probability we correctly reject H0\n= P(reject H0|HA)\n= 1 -P(type II error).\nIdeally, a hypothesis test should have a small significance level (near 0) and a large power\n(near 1). Here are two analogies to help you remember the meanings of significance and\npower.\nSome analogies\n1. Think of H0 as the hypothesis 'nothing noteworthy is going on', i.e. 'the coin is fair',\n'the treatment is no better than placebo' etc. And think of HAas the opposite: 'something\ninteresting is happening'. Then power is the probability of detecting something interesting\nwhen it's present and significance level is the probability of mistakenly claiming something\ninteresting has occured.\n2. In the U.S. criminal defendents are presumed innocent until proven guilty beyond a\nreasonable doubt. We can phrase this in NHST terms as\nH0: the defendent is innocent (the default)\nHA: the defendent is guilty.\nSignificance level is the probability of finding an innocent person guilty.\nPower is the\nprobability of correctly finding a guilty party guilty. 'Beyond a reasonable doubt' means\nwe should demand the significance level be very small.\n\n18.05 Class 17, Null Hypothesis Significance Testing I, Spring 2022\nComposite hypotheses\nHAis composite in Example 3, so the power is different for different values of θ. We expand\nthe previous probability table to include some alternate values of θ. We do the same with\nthe stem plots. As always in the NHST game, we look at likelihoods: the probability of the\ndata given a hypothesis.\nThe rejection range consists of the extreme values of x. The non-rejection range (3-7) is a\nset of center values of x.\nx\nH0 ∶p(x|θ= 0.5) 0.001 0.010 0.044 0.117 0.205 0.246 0.205 0.117 0.044 0.010\n.001\nHA∶p(x|θ= 0.6) 0.000 0.002 0.011 0.042 0.111 0.201 0.251 0.215 0.121 0.040 0.006\nHA∶p(x|θ= 0.7) 0.000 0.000 0.001 0.009 0.037 0.103 0.200 0.267 0.233 0.121 0.028\np(x| H0)\n.05\n.15\n.25\n3 4 5 6 7\n0 1 2\n8 9 10\np(x| θ= .6)\n.05\n.15\n.25\n3 4 5 6 7\n0 1 2\n8 9 10\nx\np(x| θ= .7)\n.05\n.15\n.25\n3 4 5 6 7\n0 1 2\n8 9 10\nRejection region and null and alternative probabilities for example 3\nWe use the probability table to compute the significance level and power of this test.\nSignificance level\n= probability we reject H0 when it is true\n= probability the test statistic is in the rejection region when H0 is true\n= probability the test stat. is in the rejection region of the H0 row of the table\n= sum of shaded orange boxes in the θ= 0.5 row\n= 0.11\nPower when θ= 0.6\n= probability we reject H0 when θ= 0.6\n= probability the test statistic is in the rejection region when θ= 0.6\n= probability the test stat. is in the rejection region of the θ= 0.6 row of the table\n= sum of blue boxes in the θ= 0.6 row\n= 0.180\nPower when θ= 0.7\n= probability we reject H0 when θ= 0.7\n= probability the test statistic is in the rejection region when θ= 0.7\n= probability the test stat. is in the rejection region of the θ= 0.7 row of the table\n= sum of blue boxes in the θ= 0.7 row\n= 0.384\nWe see that the power is greater for θ= 0.7 than for θ= 0.6. This isn't surprising since we\nexpect it to be easier to recognize that a 0.7 coin is unfair than is is to recognize 0.6 coin\nis unfair. Typically, we get higher power when the alternate hypothesis is farther from the\nnull hypothesis. In Example 3, it would be quite hard to distinguish a fair coin from one\nwith θ= 0.51.\n\n18.05 Class 17, Null Hypothesis Significance Testing I, Spring 2022\n4.4\nConceptual sketches\nWe illustrate the notions of null hypothesis, rejection region and power with some sketches\nof the pdfs for the null and alternative hypotheses.\n4.4.1\nNull distribution: rejection and non-rejection regions\nThe first diagram below illustrates a null distribution with rejection and non-rejection re-\ngions. Also shown are two possible test statistics: x1 and x2.\nx\nφ(x|H0)\n-3\nx1\nx2\nreject H0\nreject H0\ndon't reject H0\nThe test statistic x1 is in the non-rejection region. So, if our data produced the test statistic\nx1 then we would not reject the null hypothesis H0. On the other hand the test statistic x2\nis in the rejection region, so if our data produced x2 then we would reject the null hypothesis\nin favor of the alternative hypothesis.\nThere are several things to note in this picture.\n1. The rejection region consists of values far from the center of the null distribution.\n2. The rejection region is two-sided. We will also see examples of one-sided rejection regions\nas well.\n3. The probability of rejection (significance) is the shaded area under the curve, i.e. the\nprobability of data being in the rejection region assuming H0 is true.\n4. The alternative hypothesis is not mentioned. We reject or don't reject H0 based only\non the likelihood φ(x|H0), i.e. the probability of the test statistic conditioned on H0. As\nwe will see, the alternative hypothesis HAshould be considered when choosing a rejection\nregion, but formally it does not play a role in rejecting or not rejecting H0.\n5. Sometimes we rather lazily call the non-rejection region the acceptance region. This is\ntechnically incorrect because we never truly accept the null hypothesis. We either reject or\nsay the data does not support rejecting H0. This is often summarized by the statement:\nyou can never prove the null hypothesis.\n4.4.2\nHigh and low power tests\nThe next two figures show high and low power tests. In both tests the null distributions\nare standard normal. Likewise, the alternative distributions are both normal with variance\n1, but different means: -4.0 in the top figure and -0.4 in the bottom one.\n\n18.05 Class 17, Null Hypothesis Significance Testing I, Spring 2022\nSince the alternative distribution is to the left of the null distribution we use a one-sided\nrejection region. It is the same for both tests. The shaded area under φ(x|H0) represents\nthe significance level - it is the same for both tests. Remember the significance level can be\ndescribed two ways:\n- The probability of falsely rejecting the null hypothesis when it is true.\n- The probabilitiy the test statistic falls in the rejection region even though H0 is true.\nLikewise, the shaded area under φ(x|HA) represents the power, i.e. the probability that\nthe test statistic is in the rejection (of H0) region when HAis true. Both tests have the\nsame significance level, but different powers. When φ(x|HA) has considerable overlap with\nφ(x|H0) the power will tend to be low (bottom figure) and when they are well separated it\ntends to be high (top figure). It is well worth your while to thoroughly understand these\ngraphical representations of significance testing.\nx\nφ(x|H0)\nφ(x|HA)\n-4\n.\nreject H0 region\nnon-reject H0 region\nHigh power test\nx1\nx2\nx3\nx\nφ(x|H0)\nφ(x|HA)\n-0.4\n.\nreject H0 region\nnon-reject H0 region\nLow power test\nx1\nx2\nx3\nIn the top figure we see that the means of the null and alternative distributions are 4\nstandard deviations apart. Since the areas under the densities have very little overlap the\ntest has high power. That is, if HAwas true, then seeing the data x3 would be rare and\nsurprising and similarly for any point in the non-rejection region. That is, if HAis the\ntrue distribution we are extremely likely to correctly reject the null hypothesis, i.e. we are\nunlikely to make a type II error.\nIn the bottom figure we see that the means of the null and alternative distributions are just\n0.4 standard deviations apart. Since the areas under the densities have a lot of overlap the\ntest has low power. That is, if the data xis drawn from HAit is highly likely to be in the\nnon-rejection region. For example x3 would be not be a very surprising outcome for the HA\n\n18.05 Class 17, Null Hypothesis Significance Testing I, Spring 2022\ndistribution. That is, if HAis the true distribution, we are highly likely to make a type II\nerror.\nTypically we can increase the power of a test by increasing the amount of data and thereby\ndecreasing the variance of the null and alternative distributions. In experimental design it\nis important to determine ahead of time the number of trials or subjects needed to achieve\na desired power.\nExample 7. Suppose a drug for a disease is being compared to a placebo. We choose our\nnull and alternative hypotheses as\nH0 = the drug does not work better than the placebo\nHA= the drug works better than the placebo\nThe power of the hypothesis test is the probability that the test will conclude that the drug\nis better, if it is indeed truly better. The significance level is the probability that the test\nwill conclude that the drug works better, when in fact it does not. We will look at this in\nmore detail below.\nDesigning a hypothesis test\nFormally all a hypothesis test requires is H0, HA, a test statistic and a rejection region. In\npractice the design is often done using the following steps.\n1. Pick the null hypothesis H0.\nThe choice of H0 and HAis not mathematics. It's art and custom. We often choose H0 to\nbe simple. Or we often choose H0 to be the simplest or most cautious explanation, i.e. no\neffect of drug, no ESP, no bias in the coin.\n2. Decide if HAis one-sided or two-sided.\nIn Example 3 we wanted to know if the coin was unfair. An unfair coin could be biased for\nor against heads, so HA∶θ=0.5 is a two-sided hypothesis. If we only care whether or not\nthe coin is biased for heads we could use the one-sided hypothesis HA∶θ> 0.5.\n3. Pick a test statistic.\nFor example, the sample mean, sample total, or sample variance. Often the choice is obvious.\nSome standard statistics that we will encounter are z, t, and χ2. We will learn to use these\nstatistics as we work examples over the next few classes. One thing we will say repeatedly\nis that the distributions that go with these statistics are always conditioned on the null\nhypothesis. That is, we will compute likelihoods such as φ(z| H0).\n4. Pick a significance level and determine the rejection region.\nWe will usually use αto denote the significance level. The Neyman-Pearson paradigm is to\npick αin advance. Typical values are 0.1, 0.05, 0.01. Recall that the significance level is\nthe probability of a type I error, i.e. of incorrectly rejecting the null hypothesis when it is\ntrue. The value we choose will depend on the consequences of a type I error.\nOnce the significance level is chosen we can determine the rejection region in the tail(s) of\nthe null distribution. In Example 3, HAis two sided so the rejection region is split between\nthe two tails of the null distribution. This distribution is given in the following table:\n\n18.05 Class 17, Null Hypothesis Significance Testing I, Spring 2022\nx\np(x|H0) 0.001 0.010 0.044 0.117 0.205 0.246 0.205 0.117 0.044 0.010 0.001\nIf we set α= 0.05 then the rejection region must contain at most 0.05 probability. For a\ntwo-sided rejection region this is split between the two tails, so we get\n{0, 1, 9, 10}.\nIf we set α= 0.01 the rejection region is\n{0, 10}.\nWe show these in tables with the rejection region (values of x) inside the orange rectangle\nand the corresponding null likelihoods in the shaded orange boxes.\nx\np(x|H0) 0.001 0.010 0.044 0.117 0.205 0.246 0.205 0.117 0.044 0.010 0.001\nx\np(x|H0) 0.001 0.010 0.044 0.117 0.205 0.246 0.205 0.117 0.044 0.010 0.001\nTables with shaded rejection regions for α= 0.05 (top) and α= 0.01 (bottom)\nSuppose we change HAto 'the coin is biased in favor of heads'. We now have a one-sided\nhypothesis θ> 0.5. Our rejection region will now be in the right-hand tail since we don't\nwant to reject H0 in favor of HAif we get a small number of heads. Now if α= 0.05 the\nrejection region is the one-sided range\n{9, 10}.\nIf we set α= 0.01 then the rejection region is\n{10}.\nAs above we show the one sided rejection regions in tables.\nx\np(x|H0) 0.001 0.010 0.044 0.117 0.205 0.246 0.205 0.117 0.044 0.010 0.001\nx\np(x|H0) 0.001 0.010 0.044 0.117 0.205 0.246 0.205 0.117 0.044 0.010 0.001\nTables with shaded one-sided rejection regions for α= 0.05 (top) and α= 0.01 (bottom)\n5. Determine the power(s).\nAs we saw in Example 3, once the rejection region is set we can determine the power of the\ntest at various values of the alternate hypothesis.\nExample 8. (Consequences of significance) If α= 0.1 then we'd expect a 10% type\nI error rate. That is, we expect to reject the null hypothesis in 10% of those experiments\n\n18.05 Class 17, Null Hypothesis Significance Testing I, Spring 2022\nwhere the null hypothesis is true. Whether 0.1 is a reasonable signficance level depends on\nthe decisions that will be made using it.\nFor example, if you were running an experiment to determine if your chocolate is more than\n72% cocoa then a 10% error type I error rate is probably okay. That is, falsely believing\nsome 72% chocalate is greater that 72%, is probably acceptable. On the other hand, if your\nforensic lab is identifying fingerprints for a murder trial then a 10% type I error rate, i.e.\nmistakenly claiming that fingerprints found at the crime scene belonged to someone who\nwas truly innocent, is definitely not acceptable.\nSignificance for a composite null hypothesis. If H0 is composite then P(type I error) depends\non which member of H0 is true. In this case the significance level is defined as the maximum\nof these probabilities.\nCritical values\nCritical values are like quantiles except they refer to the probability to the right of the value\ninstead of the left.\nExample 9. Use R to find the 0.05 critical value for the standard normal distribution.\nSolution: We label this critical value z0.05. The critical value z0.05 is just the 0.95 quantile,\ni.e. it has 5% probability to its right and therefore 95% probability to its left. We computed\nit with the R function qnorm: qnorm(0.95, 0, 1), which returns 1.64.\nIn a typical significance test the rejection region consists of one or both tails of the null\ndistribution. The value of the test statistic that marks the start of the rejection region is a\ncritical value. We show this and the notation used in some examples.\nExample 10. Critical values and rejection regions. Suppose our test statistic xhas null\ndistribution N(100, 152), i.e. φ(x|H0) ∼N(100, 152). Suppose also that our rejection region\nis right-sided and we have a significance level of 0.05. Find the critical value and sketch the\nnull distribution and rejection region.\nSolution: The notation used for the critical value with right tail containing probability\n0.05 is x0.05. The critical value x0.05 is just the 0.95 quantile, i.e. it has 5% probability\nto its right and therefore 95% probability to its left. We computed it with the R function\nqnorm: qnorm(0.95, 100, 15), which returned 124.7. This is shown in the figure below.\nx\nφ(x|H0) ∼N(100, 152)\nx0.05\nreject H0\nnon-reject H0\nx0.05 = 124.7\nα= shaded area\n= 0.05\nExample 11. Critical values and rejection regions. Repeat the previous example for a\nleft-sided rejection region with significance level 0.05.\nSolution: In this case the critical value has 0.05 probability to its left and therefore 0.95\n\n18.05 Class 17, Null Hypothesis Significance Testing I, Spring 2022\nprobability to its right. So we label it x0.95. Since it is the 0.05 quantile compute it with\nthe R function: qnorm(0.05, 100, 15), which returned 75.3.\nx\nφ(x|H0) ∼N(100, 152)\nx0.95\nreject H0\nnon-reject H0\nx0.95 = 75.3\nα= shaded area\n= 0.05\nExample 12. Critical values. Repeat the previous example for a two-sided rejection region.\nPut half the significance in each tail.\nSolution: To have a total significance of 0.05 we put 0.025 in each tail. That is, the left\ntail starts at x0.975 = q0.025 and the right tail starts at x0.025 = q0.975. We compute these\nvalues with qnorm(0.025, 100, 15) and qnorm(0.975, 100, 15). The values are shown\nin the figure below.\nx\nφ(x|H0) ∼N(100, 152)\nx0.975\nx0.025\nreject H0\nreject H0\nnon-reject H0\nx0.025 = 129.4\nx0.975 = 70.6\nα= shaded area\n= 0.05\np-values\nIn practice people often specify the significance level and do the significance test using what\nare called p-values. We will first define p-value and then state the p-test. After that, we\nwill illustrate it with figures and examples.\nDefinition. The p-value is the probability, assuming the null hypothesis, of seeing data at\nleast as extreme as the experimental data. What 'at least as extreme' means depends on\nthe experimental design.\nP-test. If the p-value is less than the significance level αthen we reject H0. Otherwise we\ndo not reject H0.\nWe first illustrate p-values graphically and then we will work a simple one-sided example.\nWe will look at two-sided examples in later classes.\nSuppose we have a right-sided alternate hypothesis, so our rejection region is in the right\ntail of the range of possible outcomes. This is illustrated in the following figure.\n\n18.05 Class 17, Null Hypothesis Significance Testing I, Spring 2022\nx\nφ(x|H0) = null pdf\nxq\nreject H0\nnon-reject H0\nxq= critical point\nα= shaded area\nRight-sided rejection region\nSuppose we get data x1 which is in the rejection region. This is shown in the figure below.\nx\nφ(x|H0) = null pdf\nxq\nx1\nreject H0\nnon-reject H0\nxq= critical point\nα= shaded area\np= striped area\np-value = probability of data 'more extreme' than x1: p< α\nSince the rejection region is right-sided, the phrase 'at least as extreme' means all values to\nthe right of x1. So, the p-value is the area of the striped region.\nHere is the key to connecting the p-test, rejection and signifcance:\nSince x1 is in the rejection region, the striped area pis less than the shaded area α. That\nis, p< α. In other words, we reject the null hypothesis when p< α.\nFor completeness we show a figure where x1 is not in the rejection region, so p> α. That\nis, we do not reject H0 when p> α.\nx\nφ(x|H0) = null pdf\nxq\nx1\nreject H0\nnon-reject H0\nxq= critical point\nα= shaded area\np= striped area\np-value = probability of data 'more extreme' than x1: p> α\n7.1\nExample: z-tests\nWhen our test statistic is standard normal we will call it z, and the corresponding test for\nsignificance will be called a z-test.\nExample 13. The z-test for normal hypotheses\nIQ is normally distributed in the population according to a N(100, 152) distribution. We\nsuspect that most MIT students have above average IQ so we frame the following hypothe-\n\n18.05 Class 17, Null Hypothesis Significance Testing I, Spring 2022\nses.\nH0\n= MIT student IQs are distributed identically to the general population\n= MIT IQ's follow a N(100, 152) distribution.\nHA\n= MIT student IQs tend to be higher than those of the general population\n= the average MIT student IQ is greater than 100.\nNotice that HAis one-sided.\nSuppose we test 9 students and find they have an average IQ of x= 112. Can we reject H0\nat a significance level α= 0.05?\nSolution: To compute pwe first standardize the data: Under the null hypothesis\nx∼\nN(100, 152/9) and therefore\nz=\nx-100\n15/\n√\n9 = 36\n15 = 2.4 ∼N(0, 1).\nThat is, the null distribution for zis standard normal. We call za z-statistic, we will use\nit as our test statistic.\nFor a right-sided alternative hypothesis the phrase 'data at least as extreme' is a one-sided\ntail to the right of z. The p-value is then\np= P(Z≥2.4) = 1- pnorm(2.4,0,1) = 0.0081975.\nSince p≤αwe reject the null hypothesis. The reason this works is explained below. We\nphrase our conclusion as\nWe reject the null hypothesis in favor of the alternative hypothesis that MIT\nstudents have higher IQs on average. We have done this at significance level\n0.05 with a p-value of 0.008.\nNotes: 1. The average x= 112 is random: if we ran the experiment again we could get a\ndifferent value for x.\n2. We could use the statistic xdirectly. Standardizing is fairly standard because, with\npractice, we will have a good feel for the meaning of different z-values.\nThe justification for rejecting H0 when p≤αis given in the following figure.\nx\nφ(z|H0) ∼N(0, 1)\n-1\nz0.05\n2.4\nreject H0\nnon-reject H0\nz0.05 = 1.645\nα= shaded area = 0.05\np= striped area = 0.008\nIn this example α= 0.05, z0.05 = 1.64 and the rejection region is the range to the right\nof z0.05. Also, z= 2.4 and the p-value is the probability to the right of z. The picture\nillustrates that\n- z= 2.64 is in the rejection region\n\n18.05 Class 17, Null Hypothesis Significance Testing I, Spring 2022\n- is the same as zis to the right of z0.05\n- is the same as the probability to the right of zis less than 0.05\n- which means p< 0.05.\nMore examples\nHypothesis testing is widely used in inferential statistics. We don't expect that the following\nexamples will make perfect sense at this time. Read them quickly just to get a sense of how\nhypothesis testing is used. We will explore the details of these examples in class.\nExample 14. The chi-square statistic and goodness of fit. (Rice, example B, p.313)\nTo test the level of bacterial contamination, milk was spread over a grid with 400 squares.\nThe amount of bacteria in each square was measured. We summarize in the table below.\nThe bottom row of the table is the number of different squares that had a given amount of\nbacteria.\nAmount of bacteria\nNumber of squares\nWe compute that the average amount of bacteria per square is 2.44. Since the Poisson(λ)\ndistribution is used to model counts of relatively rare events and the parameter λis the\nexpected value of the distribution.\nwe decide to see if these counts could come from a\nPoisson distribution. To do this we first graphically compare the observed frequencies with\nthose expected from Poisson(2.44).\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nNumber of bacteria in square\nNumber of squares\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nPoisson(2.44)\nObserved\nThe picture is suggestive, so we do a hypothesis test with\nH0 ∶the samples come from a Poisson(2.44) distribution.\nHA∶the samples come from a different distribution.\n\n18.05 Class 17, Null Hypothesis Significance Testing I, Spring 2022\nWe use a chi-square statistic, so called because it (approximately) follows a chi-square\ndistribution. To compute X2 we first combine the last few cells in the table so that the\nminimum expected count is around 5 (a general rule-of-thumb in this game.)\nThe expected number of squares with a certain amount of bacteria comes from considering\n400 trials from a Poisson(2.44) distribution, e.g., with\n= 2.44 the expected number of\nsquares with 3 bacteria is 400 × e-\n3! = 84.4.\nThe chi-square statistic is ∑(Oi-Ei)2\nEi\n, where Oiis the observed number and Eiis the\nexpected number.\nNumber per square\n> 6\nObserved\nExpected\n34.9\n85.1\n103.8\n84.4\n51.5\n25.1\n10.2\n5.0\nComponent of X2\n12.8\n4.2\n5.5\n6.0\n1.7\n0.14\n0.15\n44.5\nSumming up we get X2 = 74.9.\nSince the mean (2.44) and the total number of trials (400) are fixed, the 8 cells only have\n6 degrees of freedom. So, assuming H0, our chi-square statistic follows (approximately) a\nχ2\n6 distribution. Using this distribution, P(X2 > 74.59) = 0 (to at least 6 decimal places).\nThus we decisively reject the null hpothesis in favor of the alternate hypothesis that the\ndistribution is not Poisson(2.44).\nTo analyze further, look at the individual components of X2. There are large contributions\nin the tail of the distribution, so that is where the fit goes awry.\nExample 15. Student's ttest.\nSuppose we want to compare a medical treatment for increasing life expectancy with a\nplacebo. We give npeople the treatment and mpeople the placebo. Let X1, ... , Xnbe the\nnumber of years people live after receiving the treatment. Likewise, let Y1, ... , Ymbe the\nnumber of years people live after receiving the placebo. Let\nXand\nYbe the sample means.\nWe want to know if the difference between\nXand\nYis statistically significant. We frame\nthis as a hypothesis test. Let μXand μYbe the (unknown) means.\nH0 ∶μX= μY,\nHA∶μX=μY.\nWith certain assumptions and a proper formula for the pooled standard error spthe test\nstatistic t=\nX-\nY\nsp\nfollow a tdistribution with n+ m-2 degrees of freedom.\nSo our\nrejection region is determined by a threshold t0 with P(t> t0) = α.\n\nNull Hypothesis Significance Testing II\nClass 18, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Be able to list the steps common to all null hypothesis significance tests.\n2. Be able to define and compute the probability of Type I and Type II errors.\n3. Be able to look up and apply one- and two-sample t-tests.\nIntroduction\nWe continue our study of significance tests. In these notes we will introduce two new tests:\none-sample t-tests and two-sample t-tests. You should pay careful attention to the fact that\nevery test makes some assumptions about the data - often that is drawn from a normal\ndistribution. You should also notice that all the tests follow the same pattern. It is just the\ncomputation of the test statistic and the type of the null distribution that changes.\nReview\n3.1\nSetting up and running a significance test\nThere is a fairly standard set of steps one takes to set up and run a null hypothesis signifi-\ncance test.\n1. Design an experiment to collect data and choose a test statistic xto be computed\nfrom the data. The key requirement here is to know the null distribution φ(x|H0).\nTo compute power, one must also know the alternative distribution φ(x|HA).\n2. Decide if the test is one or two-sided based on HAand the form of the null distribution.\n3. Choose a significance level αfor rejecting the null hypothesis.\n4. Decide how much data you need to collect to achieve the desired power for the test.\n5. Run the experiment to collect data x1, x2, ... , xn.\n6. Compute the test statistic x.\n7. Compute the p-value corresponding to xusing the null distribution.\n8. If p< α, reject the null hypothesis in favor of the alternative hypothesis.\n\n18.05 Class 18, Null Hypothesis Significance Testing II, Spring 2022\nNotes.\n1. Rather than choosing a significance level, you could instead choose a rejection region\nand reject H0 if xfalls in this region.\nThe corresponding significance level is then the\nprobability, assuming H0, that xfalls in the rejection region.\n2. The null hypothesis is often the 'cautious hypothesis'. The lower we set the significance\nlevel, the more \"evidence\" we will require before rejecting our cautious hypothesis in favor\nof a more sensational alternative. It is standard practice to publish the pvalue itself so that\nothers may draw their own conclusions.\n3. A key point of confusion: A significance level of 0.05 does not mean the test only\nmakes mistakes 5% of the time.\nIt means that if the null hypothesis is true, then the\nprobability the test will mistakenly reject it is 5%. The power of the test measures the\naccuracy of the test when the alternative hypothesis is true. Namely, the power of the\ntest is the probability of rejecting the null hypothesis if the alternative hypothesis is true.\nTherefore the probability of falsely failing to reject the null hypothesis is 1 minus the power.\n4. Another key point of confusion: We use p-values, but conceptually the p-value is\njust a computational trick. After choosing a test statistic, the conceptual order is: first\npick a significance level, then use this to define the rejection region. We reject the null\nhypothesis if the test statistic is in the rejection region. All the p-value does is tell us in\none computation whether or not the test stastic is in the rejection region.\nErrors. We can summarize these two types of errors and their probabilities as follows:\nType I error\n=\nrejecting H0 when H0 is true.\nType II error\n=\nfailing to reject H0 when HAis true.\nP(type I error)\n=\nprobability of falsely rejecting H0\n=\nP(test statistic is in the rejection region | H0)\n=\nsignificance level of the test\nP(type II error)\n=\nprobability of falsely not rejecting H0\n=\nP(test statistic is in the acceptance region | HA)\n=\n1 - power.\nHelpful analogies.\nIn terms of medical testing for a disease: a Type I error is a false positive and a Type II\nerror is a false negative.\nIn a jury trial, a Type I error is convicting an innocent defendant and a Type II error is\nacquitting a guilty defendant.\n3.2\nPower\nWe discussed power in the Class 17 notes. Power is the probabilitiy of correctly rejecting\nthe null hypothesis. It depends on the alternative hypothesis HAbeing considered.\nThe ideal test has power equal to 1.0 and significance equal to 0.0. Of course, in general, this\nis impossible. And we want to find some compromise where power is high and signficance\nis low.\nIn symbols: power = P(data is in the rejection region | HA).\n\n18.05 Class 18, Null Hypothesis Significance Testing II, Spring 2022\nCompare this with: signficance = P(data is in the rejection region | H0).\nUnderstanding a significance test\nQuestions to ask:\n1. How did they collect data? What is the experimental setup?\n2. What are the null and alternative hypotheses?\n3. What type of significance test was used?\nDoes the data match the criteria needed to use this type of test?\nHow robust is the test to deviations from these criteria?\n4. For example, some tests comparing two groups of data assume that the groups are\ndrawn from distributions that have the same variance. This needs to be verified before\napplying the test. Often the check is done using another significance test designed to\ncompare the variances of two groups of data.\n5. How is the p-value computed?\nA significance test comes with a test statistic and a null distribution. In most tests\nthe p-value is\np= P(data at least as extreme as what we got | H0)\nWhat does 'data at least as extreme as the data we saw' mean? For example, is the\ntest one or two-sided?\n6. What is the significance level αfor this test? If p< αthen the experimenter will\nreject H0 in favor of HA.\n7. What is the power of the test?\nttests\nMany significance tests assume that the data are drawn from a normal distribution, so\nbefore using such a test you should examine the data to see if the normality assumption is\nreasonable. We will describe how to do this in more detail later, but plotting a histogram\nis a good start. Like the z-test, the one-sample and two-sample t-tests that we consider\nbelow start from this normality assumption.\nWe don't expect you to memorize all the computational details of these tests and those to\nfollow. In real life, you have access to textbooks, google, and wikipedia; on the exam, you'll\nhave your notecard. Instead, you should be able to identify when a t-test is appropriate\nand apply this test after looking up the details and using a table or software like R.\n\n18.05 Class 18, Null Hypothesis Significance Testing II, Spring 2022\n5.1\nz-test\nLet's first review the z-test.\n- Data: we assume x1, x2, ... , xn∼N(μ, σ2), where μis unknown and σis known.\n- Null hypothesis: μ= μ0 for some specific value μ0\n- Test statistic:\nz= x-μ0\nσ/√n\n=\nstandardized mean\n- Null distribution: φ(z| H0) is the pdf of Z∼N(0, 1)\n- One-sided p-value (right side): p= P(Z≥z| H0)\nOne-sided p-value (left side): p= P(Z≤z| H0)\nTwo-sided p-value: p= {2P(Z≥z)\nif z> 0\n2P(Z≤z)\nif z< 0..\nBecause of the symmetry of the distribution around 0, we can also write this as\np= P(|Z| ≥|z|).\nSee Example 1b for the rationale for this.\nExample 1. Suppose that we have data that follows a normal distribution of unknown\nmean μand known variance 4. Let the null hypothesis H0 be that μ= 2. Let the alternative\nhypothesis HAbe that μ> 2. Suppose we collect the following data:\n3, 2, 5, 7, 1\nAt a significance level of α= 0.05, should we reject the null hypothesis?\nSolution: There are 5 data points with average x= 3.6. Because we have normal data\nwith a known variance we should use a ztest. Our zstatistic is\nz= x-μ0\nσ/√n= 3.6 -2\n2/\n√\n= 1.79\nOur test is one-sided because the alternative hypothesis is one-sided. So (using R) our\np-value is\np= P(Z> z) = P(Z> 1.79) = 0.037\nSince p< α= 0.05, we reject the null hypothesis in favor of the alternative hypothesis that\nμ> 2.\nWe can visualize the test as follows:\n\n18.05 Class 18, Null Hypothesis Significance Testing II, Spring 2022\nx\nφ(z|H0) ∼N(0, 1)\n-1\n1.64 1.79\nreject H0\nnon-reject H0\nRejection region = shaded orange\n(starts at q0.95 = z0.05 = 1.64)\nα= shaded orange area = 0.05\nz= statistic = black dot = 1.79\np= blue stripe area = 0.037\nExample 1b.\nRepeat Example 1 as a two-sided test, i.e. with HAbeing μ=2.\nSolution: Let's do the test and then we'll explain the rationale behind the computation of\nthe p-value.\nSince z> 0, p= 2P(Z> z) = 0.074. Since, p> α= 0.05, the data does not support\nrejecting the null hypothesis in favor of HA.\nReason for the factor of 2 in the computation of p\nThe reason is essentially arithmetic. Remember, the purpose of the p-value is that p≤α\nindicates that the test statistic is in the rejection region.\nThe picture below illustrates the following. For a two-sided test, each side of the rejection\nregion has probability α/2. So, if the test statistic is on the right, then it is in the rejection\nregion if P(Z> z) ≤α/2, i.e. if p= 2P(Z> z) ≤α\nx\nφ(z|H0) ∼N(0, 1)\n-1\n1.96\n-1.96\n1.79\n-1.79\nreject H0\nreject H0\nnon-reject H0\nRight rejection region = shaded orange\n(starts at q0.975 = z0.025 = 1.96)\nα/2 = right shaded orange area = 0.025\nz= statistic = black dot = 1.79\np/2 = right blue stripe area = 0.037\nLeft rejection region = shaded orange\n(starts at q0.025 = z0.925 = -1.96)\nα/2 = left shaded orange area = 0.025\n-z= black dot = 1.79\np/2 = left blue stripe area = 0.037\n5.2\nThe Student tdistribution\n'Student' is the pseudonym used by the William Gosset who first described this test and\ndistribution. See\nhttps://en.wikipedia.org/wiki/Student's_t-test\nThe t-distribution is symmetric and bell-shaped like the normal distribution.\nIt has a\nparameter dfwhich stands for degrees of freedom.\nFor dfsmall the t-distribution has\nmore probability in its tails than the standard normal distribution. As dfincreases t(df)\nbecomes more and more like the standard normal distribution.\n\n18.05 Class 18, Null Hypothesis Significance Testing II, Spring 2022\nHere is a simple applet that shows t(df) and compares it to the standard normal distribution:\nhttps://mathlets.org/mathlets/t-distribution/\n-4\n-2\n0.0\n0.2\n0.4\nt densities for m degrees of freedom\nN(0,1)\nm=8\nm=4\nm=2\nAs degrees of freedom increases the t-distribution becomes normal\n5.3\nR\nAs usual in R, the functions pt, dt, qt, rt correspond to cdf, pdf, quantiles, and random\nsampling for a tdistribution. Remember that you can type ?dt in RStudio to view the help\nfile specifying the parameters of dt. For example, pt(1.65,3) computes the probability\nthat xis less than or equal 1.65 given that xis sampled from the tdistribution with 3\ndegrees of freedom, i.e. P(x≤1.65) given that x∼t(3)).\n5.4\nOne sample t-test\nFor the z-test, we assumed that the variance of the underlying distribution of the data was\nknown. However, it is often the case that we don't know σand therefore we must estimate\nit from the data. In these cases, we use a one sample t-test instead of a z-test and the\nstudentized mean in place of the standardized mean\n- Data: we assume x1, x2, ... , xn∼N(μ, σ2), where both μand σare unknown.\n- Null hypothesis: μ= μ0 for some specific value μ0\n- Test statistic:\nt= x-μ0\ns/√n\nwhere\ns2 =\nn-1\nn\n∑\ni=1\n(xi-x)2.\nHere tis called the Studentized mean and s2 is called the sample variance. The latter\nis an estimate of the true variance σ2.\n- Null distribution: φ(t| H0) is the pdf of T∼t(n-1), the tdistribution with n-1\ndegrees of freedom.*\n\n18.05 Class 18, Null Hypothesis Significance Testing II, Spring 2022\n- One-sided p-value (right side): p= P(T≥t| H0)\nOne-sided p-value (left side): p= P(T≤t| H0)\nTwo-sided p-value: p= {2P(T≥t)\nif t> 0\n2P(T≤t)\nif t< 0.\nBecause of the symmetry of the distribution around 0, we can also write this as\np= P(|T| ≥|t|).\n*Important note. This is a good example of how we will work with significance tests.\nOnce we know the distribution of the test statistic, all the tests have the same basic form.\nIn this case, we make use of a theorem that says, for normal data the Studentized mean\nfollows a t-distribution. We will not prove this in 18.05, but you can look up the proof if\nyou want: https://en.wikipedia.org/wiki/Student's_t-distribution#Derivation\nExample 2. Now suppose that in Example 1 the variance is unknown. That is, we have\ndata that follows a normal distribution of unknown mean μand and unknown variance σ.\nSuppose we collect the same data as before:\n1, 2, 3, 6, -1\nAs above, let the null hypothesis H0 be that μ= 0 and the alternative hypothesis HAbe\nthat μ> 0. At a significance level of α= 0.05, should we reject the null hypothesis?\nSolution: There are 5 data points with average x= 2.2. Because we have normal data\nwith unknown mean and unknown variance we should use a one-sample ttest. Computing\nthe sample variance we get\ns2 = 1\n4 ((1 -2.2)2 + (2 -2.2)2 + (3 -2.2)2 + (6 -2.2)2 + (-1 -2.2)2) = 6.7\nOur t-statistic is the Studentized mean:\nt= x-μ0\ns/√n=\n2.2 -0\n√\n6.7/\n√\n= 1.901\nOur test is one-sided because the alternative hypothesis is one-sided.\nSo (using R) the\np-value is\np= P(T> t) = P(T> 1.901) = 1-pt(1.901,4) = 0.065\nSince p> 0.05, we do not reject the null hypothesis.\nWe can visualize the test as follows:\nx\nφ(y|H0) ∼t(4)\n-1\n2.13\n1.9\nreject H0\nnon-reject H0\nRejection region = shaded orange\n(starts at q0.95 = t0.05 = 2.13)\nα= shaded orange area = 0.05\nt= statistic = black dot = 1.90\np= blue stripe area = 0.065\n\n18.05 Class 18, Null Hypothesis Significance Testing II, Spring 2022\n5.5\nTwo-sample t-test with equal variances\nWe next consider the case of comparing the means of two samples. For example, we might\nbe interested in comparing the mean efficacies of two medical treatments.\n- Data: We assume we have two sets of data drawn from normal distributions\nx1, x2, ... , xn∼N(μ1, σ2)\ny1, y2, ... , ym∼N(μ2, σ2)\nwhere the means μ1 and μ2 and the variance σ2 are all unknown. Notice the assump-\ntion that the two distributions have the same variance. Also notice that there are n\nsamples in the first group and msamples in the second.\n- Null hypothesis: μ1 = μ2 (the values of μ1 and μ2 are not specified)\n- Test statistic:\nt= x-y\nsp\n,\nwhere s2\npis the pooled variance\ns2\np= (n-1)s2\nx+ (m-1)s2\ny\nn+ m-2\n( 1\nn+ 1\nm)\nHere s2\nxand s2\nyare the sample variances of the xiand yjrespectively. The expression\nfor tis somewhat complicated, but the basic idea remains the same and it still results\nin a known null distribution.\n- Null distribution: φ(t| H0) is the pdf of T∼t(n+ m-2).\n- One-sided p-value (right side): p= P(T> t| H0)\nOne-sided p-value (left side): p= P(T< t| H0)\nTwo-sided p-value: p= P(|T| > |t|).\nNote 1: Some authors use a different notation. They define the pooled variance as\ns2\np-other-authors = (n-1)s2\nx+ (m-1)s2\ny\nn+ m-2\nand what we called the pooled variance they point out is the estimated variance of x-y.\nThat is,\ns2\np= sp-other-authors × (1/n+ 1/m) ≈s2\nx-y\nNote 2: There is a version of the two-sample t-test that allows the two groups to have\ndifferent variances. In this case the test statistic is a little more complicated but R will\nhandle it with equal ease.\nNote 3: We reiterate our 'important note' from above: It can be proved that under the\nassumptions on the data (independent samples, normal data, equal variances), the null\ndistribution is a t-distribution.\nWe won't prove this in 18.05.\nBut knowing it, we can\n\n18.05 Class 18, Null Hypothesis Significance Testing II, Spring 2022\nwork with and understand the gist of the two-sample t-test in exactly the same way we can\nunderstand other significance tests.\nExample 3.\nThe following data comes from a real study in which 1408 women were\nadmitted to a maternity hospital for (i) medical reasons or through (ii) unbooked emergency\nadmission. The duration of pregnancy is measured in complete weeks from the beginning\nof the last menstrual period. We can summarize the data as follows:\nMedical: 775 observations with\nxM= 39.08 and s2\nM= 7.77.\nEmergency: 633 observations with\nxE= 39.60 and s2\nE= 4.95\nSet up and run a two-sample t-test to investigate whether the mean duration differs for the\ntwo groups.\nWhat assumptions did you make?\nSolution: The pooled variance for this data is\ns2\np= 774(7.77) + 632(4.95)\n( 1\n775 +\n633) = 0.0187\nThe tstatistic for the null distribution is\n\nxM-\n\nyE\nsp\n= -3.8064\nWe have 1406 degrees of freedom. Using R to compute the two-sided p-value we get\np= P(|T| > |t|) = 2*pt(-3.8064, 1406) = 0.00015\npis very small, much smaller than α= 0.05 or α= 0.01. Therefore we reject the null\nhypothesis in favor of the alternative that there is a difference in the mean durations.\nRather than compute the two-sided p-value exactly using a t-distribution we could have\nnoted that with 1406 degrees of freedom the tdistribution is essentially standard normal\nand 3.8064 is almost 4 standard deviations. So\nP(|t| ≥3.8064) ≈P(|z| ≥3.8064) < 0.001\nWe assumed the data was normal and that the two groups had equal variances. Given the\nlarge difference between the sample variances this assumption may not be warranted.\nIn fact, there are other significance tests that test whether the data is approximately normal\nand whether the two groups have the same variance. In practice one might apply these first\nto determine whether a ttest is appropriate in the first place. We don't have time to go\ninto normality tests here, but we will see the Fdistribution used for equality of variances\nnext week.\nhttps://en.wikipedia.org/wiki/Normality_test\nhttps://en.wikipedia.org/wiki/F-test_of_equality_of_variances\n\nNull Hypothesis Significance Testing III\nClass 19, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Given hypotheses and data, be able to identify an appropriate significance test from\na list of common ones.\n2. Given hypotheses, data, and a suggested significance test, know how to look up details\nand apply the significance test.\nIntroduction\nIn these notes we will collect together some of the most common significance tests, though\nby necessity we will leave out many other useful ones. Still, all significance tests follow the\nsame basic pattern in their design and implementation, so by learning the ones we include\nyou should be able to apply other ones as needed.\nDesigning a null hypothesis significance test (NHST):\n- Specify null and alternative hypotheses.\n- Choose a test statistic whose null distribution and alternative distribution(s) are\nknown.\n- Specify a rejection region. Very often this is done implicitly by specifying a significance\nlevel αand a method for computing p-values based on the tails of the null distribution.\n- Compute power using the alternative distribution(s).\nRunning a NHST:\n- Collect data and compute the test statistic.\n- Check if the test statistic is in the rejection region. Most often this is done implicitly\nby checking if p< α. If so, we 'reject the null hypothesis in favor of the alternative\nhypothesis'.\nOtherwise we conclude 'the data does not support rejecting the null\nhypothesis'.\nNote the careful phrasing: when we fail to reject H0, we do not conclude that H0 is true.\nThe failure to reject may have other causes. For example, we might not have enough data\nto clearly distinguish H0 and HA, whereas more data might indicate that we should reject\nH0.\n\n18.05 Class 19, Null Hypothesis Significance Testing III, Spring 2022\nPopulation parameters and sample statistics\nExample 1. If we randomly select 10 men from a population and measure their heights we\nsay that we have sampled the heights from the population. In this case the sample mean,\nsay x, is the mean of the sampled heights. It is a statistic and we know its value explicitly.\nOn the other hand, the true average height of the population, say μ, is unknown and we\ncan only estimate its value. We call μa population parameter.\nThe main purpose of significance testing is to use sample statistics to draw conlusions about\npopulation parameters. For example, we might test if the average height of men in a given\npopulation is greater than 70 inches.\nA gallery of common significance tests related to the nor-\nmal distribution\nWe will show a number of tests that all assume normal data. For completeness we will\ninclude the zand ttests we've already explored.\nYou shouldn't try to memorize these tests. It is a hopeless task to memorize the tests given\nhere and even more hopeless to memorize all the tests we've left out. Rather, your goal\nshould be to be able to find the correct test when you need it. Pay attention to the types\nof hypotheses the tests are designed to distinguish and the assumptions about the data\nneeded for the test to be valid. We will work through the details of these tests in class and\non homework.\nThe null distributions for all of these tests are all related to the normal distribution by\nexplicit formulas. We will not go into the details of these distributions or the arguments\nshowing how they arise as the null distributions in our significance tests. However, the\narguments are accessible to anyone who knows calculus and is interested in understanding\nthem. Given the name of any distribution, you can easily look up the details of its con-\nstruction and properties online. You can also use R to explore the distribution numerically\nand graphically.\nWhen analyzing data with any of these tests one thing of key importance is to verify that\nthe assumptions are true or at least approximately true. For example, you shouldn't use a\ntest that assumes the data is normal unless you've checked that the data is approximately\nnormal.\nThe script class19.r contains examples of using R to run some of these tests. It is posted in\nour usual place for R code.\n4.1\nz-test\n- Use: Test if the population mean equals a hypothesized mean.\n- Data: x1, x2, ... , xn.\n- Assumptions: The data are independent normal samples:\nxi∼N(μ, σ2) where μis unknown, but σis known.\n- H0: For a specified μ0, μ= μ0.\n\n18.05 Class 19, Null Hypothesis Significance Testing III, Spring 2022\n- HA:\nTwo-sided:\nμ=μ0\none-sided-greater:\nμ> μ0\none-sided-less:\nμ< μ0\n- Test statistic: z= x-μ0\nσ/√n\n- Null distribution: φ(z| H0) is the pdf of\nZ∼N(0, 1).\n- p-value:\nTwo-sided:\np= P(|Z| > z)\n=\n2*(1-pnorm(abs(z), 0, 1))\none-sided-greater:\np= P(Z> z)\n=\n1 - pnorm(z, 0, 1)\none-sided-less:\np= P(Z< z)\n=\npnorm(z, 0, 1)\n- R code: There does not seem to be a single R function in the base R packages that\nruns a z-test. There are other packages you can install that have a z.test function.\nOf course, it is easy enough to get R to compute the zscore and p-value. There is an\nexample of this in class19.r.\nExample 2. We quickly reprise our example from the class 17 notes.\nIQ is normally distributed in the population according to a N(100, 152) distribution. We\nsuspect that most MIT students have above average IQ so we frame the following hypothe-\nses.\nH0\n= MIT student IQs are distributed identically to the general population\n= MIT IQ's follow a N(100, 152) distribution.\nHA\n= MIT student IQs tend to be higher than those of the general population\n= the average MIT student IQ is greater than 100.\nNotice that HAis one-sided.\nSuppose we test 9 students and find they have an average IQ of\nx= 112. Can we reject H0\nat a significance level α= 0.05?\nSolution: Our test statistic is\nz=\nx-100\n15/\n√\n9 = 36\n15 = 2.4.\nThe right-sided p-value is therefore\np= P(Z≥2.4) = 1- pnorm(2.4,0,1) = 0.0081975.\nSince p≤αwe reject the null hypothesis in favor of the alternative hypothesis that MIT\nstudents have higher IQs on average.\n4.2\nOne-sample t-test of the mean\n- Use: Test if the population mean equals a hypothesized mean.\n- Data: x1, x2, ... , xn.\n- Assumptions: The data are independent normal samples:\nxi∼N(μ, σ2) where both μand σare unknown.\n\n18.05 Class 19, Null Hypothesis Significance Testing III, Spring 2022\n- H0: For a specified μ0, μ= μ0\n- HA:\nTwo-sided:\nμ=μ0\none-sided-greater:\nμ> μ0\none-sided-less:\nμ< μ0\n- Test statistic: t= x-μ0\ns/√n,\nwhere s2 is the sample variance:\ns2 =\nn-1\nn\n∑\ni=1\n(xi-x)2\n- Null distribution: φ(t| H0) is the pdf of\nT∼t(n-1).\n(Student t-distribution with n-1 degrees of freedom)\n- p-value:\nTwo-sided:\np= P(|T| > t)\n=\n2*(1-pt(abs(t), n-1))\none-sided-greater:\np= P(T> t)\n=\n1 - pt(t, n-1)\none-sided-less:\np= P(T< t)\n=\npt(t, n-1)\n- R code example: For data x= 1, 3, 5, 7, 2 we can run a one-sample t-test with H0:\nμ0 = 2.5 using the R command:\nt.test(x, mu = 2.5, alternative=two.sided)\nThis will return a several pieces of information including the mean of the data, t-value\nand the two-sided p-value. See the help for this function for other argument settings.\nExample 3. Look in the class 18 notes or slides for an example of this test. The class 19\nexample R code also gives an example.\n4.3\nTwo-sample t-test for comparing means\n4.3.1\nThe case of equal variances\nWe start by describing the test assuming equal variances.\n- Use: Test if the population means from two populations differ by a hypothesized\namount.\n- Data: x1, x2, ... , xnand y1, y2, ... , ym.\n- Assumptions: Both groups of data are independent normal samples:\nxi∼N(μx, σ2)\nyj∼N(μy, σ2)\nwhere both μxand μyare unknown and possibly different. The variance σ2 is un-\nknown, but the same for both groups.\n- H0: For a specified Δμthe difference of means μx-μy= Δμ\n- HA:\nTwo-sided:\nμx-μy=Δμ\none-sided-greater:\nμx-μy> Δμ\none-sided-less:\nμx-μy< Δμ\n\n18.05 Class 19, Null Hypothesis Significance Testing III, Spring 2022\n- Test statistic: t= x-y-Δμ\nsP\n,\nwhere\ns2\nxand s2\nyare the sample variances of the xand ydata respectively, and s2\nP\nis (sometimes called) the pooled sample variance:\ns2\np= (n-1)s2\nx+ (m-1)s2\ny\nn+ m-2\n( 1\nn+ 1\nm)\nand\ndf= n+ m-2\n- Null distribution: φ(t| H0) is the pdf of\nT∼t(df),\nthe t-distribution with df=\nn+ m-2 degrees of freedom.\n- p-value:\nTwo-sided:\np= P(|T| > t)\n=\n2*(1-pt(abs(t), df))\none-sided-greater:\np= P(T> t)\n=\n1 - pt(t, df)\none-sided-less:\np= P(T< t)\n=\npt(t, df)\n- R code: The R function t.test will run a two-sample t-test. See the example code\nin class19.r. In t.test the argument mu is used for what we have called Δμ.\nNotes: 1. Most often the test is done with Δμ= 0. That is, the null hypothesis is the the\nmeans are equal, i.e. μx-μy= 0.\n2. If the xand ydata have the same length n= m, then the formula for s2\npbecomes\nsimpler:\ns2\np= s2\nx+ s2\ny\nn\nExample 4. Look in the class 18 notes or slides for an example of the two-sample t-test.\n4.3.2\nThe case of unequal variances\nThere is a form of the t-test for when the variances are not assumed equal. It is sometimes\ncalled Welch's t-test.\nThis looks exactly the same as the case of equal except for a small change in the assumptions\nand the formula for the pooled variance:\n- Use: Test if the population means from two populations differ by a hypothesized\namount.\n- Data: x1, x2, ... , xnand y1, y2, ... , ym.\n- Assumptions: Both groups of data are independent normal samples:\nxi∼N(μx, σ2\nx)\nyj∼N(μy, σ2\ny)\nwhere both μxand μyare unknown and possibly different. The variances σ2\nxand σ2\ny\nare unknown and not assumed to be equal.\n- H0, HA: Exactly the same as the case of equal variances.\n\n18.05 Class 19, Null Hypothesis Significance Testing III, Spring 2022\n- Test statistic: t= x-y-Δμ\nsP\n,\nwhere\ns2\nxand s2\nyare the sample variances of the xand ydata respectively, and s2\nP\nis (sometimes called) the pooled sample variance:\ns2\np= s2\nx\nn+ s2\ny\nm\nand\ndf=\n(s2\nx/n+ s2\ny/m)2\n(s2x/n)2/(n-1) + (s2y/m)2/(m-1)\n- Null distribution: φ(t| H0) is the pdf of\nT∼t(df),\nthe tdistribution with df\ndegrees of freedom.\n- p-value: Exactly the same as the case of equal variances.\n- R code: The function t.test also handles this case if you set the argument var.equal=FALSE.\nNotes. 1. In truth, the null distribution given above only approximates the exact null\ndistribution.\n2. Notice that the degrees of freedom are unlikely to be a whole number.\n3. Some people recommend always using Welch's t-test, even if the variances are believed\nto be equal. This avoids making the assumption that the variances are equal and has very\nlittle downside if they are equal.\n4.3.3\nThe paired two-sample t-test\nWhen the data naturally comes in pairs (xi, yi), we can use the paired two-sample t-test.\n(After checking the assumptions are valid!)\nExample 5. To measure the effectiveness of a cholesterol lowering medication we might\ntest each subject before and after treatment with the drug. So for each subject we have a\npair of measurements:\nxi= cholesterol level before treatment\nyi= cholesterol level after treatment.\nExample 6. To measure the effectiveness of a cancer treatment we might pair each subject\nwho received the treatment with one who did not. In this case we would want to pair\nsubjects who are similar in terms of stage of the disease, age, sex, etc.\n- Use: Test if the average difference between paired values in a population equals a\nhypothesized value.\n- Data: x1, x2, ... , xnand y1, y2, ... , ynmust have the same length.\n- Assumptions: The differences wi= xi-yibetween the paired samples are independent\ndraws from a normal distribution N(μ, σ2), where μand σare unknown.\n- H0: For a specified μ0, μ= μ0.\n- HA:\nTwo-sided:\nμ=μ0\none-sided-greater:\nμ> μ0\none-sided-less:\nμ< μ0\n\n18.05 Class 19, Null Hypothesis Significance Testing III, Spring 2022\n- Test statistic: t= w-μ0\ns/√n,\nwhere s2 is the sample variance:\ns2 =\nn-1\nn\n∑\ni=1\n(wi-w)2\n- Null distribution: φ(t| H0) is the pdf of\nT∼t(n-1).\n(Student t-distribution with n-1 degrees of freedom)\n- p-value:\nTwo-sided:\np= P(|T| > t)\n=\n2*(1-pt(abs(t), n-1))\none-sided-greater:\np= P(T> t)\n=\n1 - pt(t, n-1)\none-sided-less:\np= P(T< t)\n=\npt(t, n-1)\n- R code: The R function t.test will do a paired two-sample test if you set the argu-\nment paired=TRUE. You can also run a one-sample t-test on x-y. There are examples\nof both of these in class19.r\nNotes. 1. This is just a one-sample t-test using wi.\n2. Another way to write the assumption is that we assume a relation between xiand yiof\nthe form yi= xi+μ+e. Here μis some (unknown) constant, and eis random error (noise)\nof mean 0 and (unknown) variance σ2.\nExample 7. The following example is taken from Rice 1\nTo study the effect of cigarette smoking on platelet aggregation Levine (1973) drew blood\nsamples from 11 subjects before and after they smoked a cigarette and measured the extent\nto which platelets aggregated. Here is the data:\nBefore\nAfter\nDifference\n-1\nThe null hypothesis is that smoking had no effect on platelet aggregation, i.e. that the\ndifference between before and after should have mean μ0 = 0. We ran a paired two-sample\nt-test to test this hypothesis. Here is the R code: (It's also in class19.r.)\nbefore.cig = c(25,25,27,44,30,67,53,53,52,60,28)\nafter.cig = c(27,29,37,56,46,82,57,80,61,59,43)\nmu0 = 0\nresult = t.test(after.cig, before.cig, alternative=\"two.sided\", mu=mu0, paired=TRUE)\nprint(result)\nHere is the output:\nPaired t-test\ndata: after.cig and before.cig\nt = 4.2716, df = 10, p-value = 0.001633\nalternative hypothesis: true difference in means is not equal to 0\nmean of the differences: 10.27273\nWe got the same results with the one-sample t-test:\nt.test(after.cig - before.cig, mu=0)\n1John Rice, Mathematical Statistics and Data Analysis, 2nd edition, p. 412. This example references P.H\nLevine (1973) An acute effect of cigarette smoking on platelet function. Circulation, 48, 619-623.\n\n18.05 Class 19, Null Hypothesis Significance Testing III, Spring 2022\n4.4\nOne-way ANOVA (F-test for equal means)\n- Use: Test if the population means from ngroups are all the same.\n- Data: (ngroups, msamples from each group)\nx1,1,\nx1,2,\n... ,\nx1,m\nx2,1,\nx2,2,\n... ,\nx2,m\n...\nxn,1,\nxn,2,\n... ,\nxn,m\n- Assumptions: Data for each group is an independent normal sample drawn from\ndistributions with (possibly) different means but the same variance:\nx1,j\n∼N(μ1, σ2)\nx2,j\n∼N(μ2, σ2)\n...\nxn,j\n∼N(μn, σ2)\nThe group means μiare unknown and possibly different. The variance σis unknown,\nbut the same for all groups.\n- H0: All the means are identical μ1 = μ2 = ... = μn.\n- HA: Not all the means are the same.\n- Test statistic: f= MSB\nMSW,\nwhere\nxi\n= mean of group i\n= xi,1 + xi,2 + ... + xi,m\nm\n.\nx\n= grand mean of all the data.\ns2\ni\n= sample variance of group i\n=\nm-1\nm\n∑\nj=1\n(xi,j-\nxi)2.\nMSB\n= between group variance\n= m× sample variance of group means\n=\nm\nn-1\nn\n∑\ni=1\n( xi-x)2.\nMSW\n= average within group variance\n= sample mean of s2\n1, ... , s2\nn\n= s2\n1 + s2\n2 + ... + s2\nn\nn\n- Idea: If the μiare all equal, test statistic f, which is a ratio, should be near 1. If they\nare not equal then MSBshould be larger while MSWshould remain about the same,\nso fshould be larger. We won't give a proof of this.\n- Null distribution: φ(f| H0) is the pdf of\nF∼F(n-1, n(m-1)).\nThis is the F-distribution with (n-1) and n(m-1) degrees of freedom. Several\nF-distributions are plotted below.\n- p-value: p= P(F> f) = 1- pf(f, n-1, n*(m-1)))\n\n18.05 Class 19, Null Hypothesis Significance Testing III, Spring 2022\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nx\nF(3,4)\nF(10,15)\nF(30,15)\nNotes: 1. ANOVA tests whether all the means are the same. It does not test whether\nsome subset of the means are the same.\n2. There is a test where the variances are not assumed equal.\n3. There is a test where the groups don't all have the same number of samples.\n4. R has a function aov() to run ANOVA tests.\n5. See: https://en.wikipedia.org/wiki/F-test\nExample 8. The table shows patients' perceived level of pain (on a scale of 1 to 6) after\n3 different medical procedures.\nT1\nT2\nT3\n(1) Set up and run an F-test comparing the means of these 3 treatments.\n(2) Based on the test, what might you conclude about the treatments?\nSolution: Using the code below, the Fstatistic is 0.325 and the p-value is 0.729 At any\nreasonable significance level we will fail to reject the null hypothesis that the average pain\nlevel is the same for all three treatments..\nNote, it is not reasonable to conclude the the null hypothesis is true. With just 5 data\npoints per procedure we might simply lack the power to distinguish different means.\nR code to perform the test\n# DATA ----\nT1 = c(2,4,1,5,3)\nT2 = c(3,4,6,1,4)\nT3 = c(2,1,3,3,5)\nprocedure = c(rep('T1',length(T1)),rep('T2',length(T2)),rep('T3',length(T3)))\npain = c(T1,T2,T3)\ndata.pain = data.frame(procedure,pain)\naov.data = aov(pain∼procedure,data=data.pain) # do the analysis of variance\nprint(summary(aov.data)) # show the summary table\n# class19.r also shows code to compute the ANOVA by hand.\n\n18.05 Class 19, Null Hypothesis Significance Testing III, Spring 2022\nThe summary shows a p-value (shown as Pr(>F)) of 0.729. Therefore we do not reject the\nnull hypothesis that all three group population means are the same.\n4.5\nChi-square test for goodness of fit\nThis is a test of how well a hypothesized probability distribution fits a set of data. The test\nstatistic is called a chi-square statistic and the null distribution associated to the chi-square\nstatistic is the chi-square distribution. It is denoted by χ2(df) where the parameter dfis\ncalled the degrees of freedom.\nSuppose we have an unknown probability mass function given by the following table.\nOutcomes\nω1\nω2\n...\nωn\nProbabilities\np1\np2\n...\npn\nIn the chi-square test for goodness of fit we hypothesize a set of values for the probabilities.\nTypically we will hypothesize that the probabilities follow a known distribution with certain\nparameters, e.g. binomial, Poisson, multinomial. The test then tries to determine if this\nset of probabilities could reasonably have generated the data we collected.\n- Use: Test whether discrete data fits a specific finite probability mass function.\n- Data: An observed count Oifor each possible outcome ωi.\n- Assumptions: None\n- H0: The data was drawn from a specific discrete distribution.\n- HA: The data was drawn from a different distribution.\n- Test statistic: The data consists of observed counts Oifor each ωi. From the null hy-\npothesis probability table we get a set of expected counts Ei. There are two statistics\nthat we can use:\nLikelihood ratio statistic G= 2 ∗∑Oiln (Oi\nEi\n)\nPearson's chi-square statistic X2 = ∑(Oi-Ei)2\nEi\n.\nIt is a theorem that under the null hypothesis X2 ≈Gand both are approximately\nchi-square. Before computers, X2 was used because it was easier to compute. Now,\nit is better to use Galthough you will still see X2 used quite often.\n- Degrees of freedom df: For chi-square tests the number of degrees of freedom can be\na bit tricky. In this case df= n-1. It is computed as the number of cell counts\nthat can be freely set under HAconsistent with the statistics needed to compute the\nexpected cell counts assuming H0.\n- Null distribution: Assuming H0, both statistics (approximately) follow a chi-square\ndistribution with dfdegrees of freedom. That is both φ(G| H0) and φ(X2 | H0) have\n(approximately) the same pdf as Y∼χ2(df).\n- p-value: Extreme data means large values of X2, i.e. large differences between the\nobserved and expected counts. So,\np\n=\nP(Y> G)\n=\n1 - pchisq(G, df)\np\n=\nP(Y> X2)\n=\n1 - pchisq(X2, df)\n\n18.05 Class 19, Null Hypothesis Significance Testing III, Spring 2022\n- R code: The R function chisq.test can be used to do the computations for a chi-\nsquare test use X2. For Gyou either have to do it by hand or find a package that has\na function. (It will probably be called likelihood.test or G.test.\nNotes. 1. When the likelihood ratio statistic Gis used the test is also called a G-test or\na likelihood ratio test.\nExample 9. First chi-square example. Suppose we have an experiment that produces\nnumerical data. For this experiment the possible outcomes are 0, 1, 2, 3, 4, 5 or more. We\nrun 51 trials and count the frequency of each outcome, getting the following data:\nOutcomes\n≥5\nObserved counts\nSuppose our null hypothesis H0 is that the data is drawn from 51 trials of a binomial(8,\n0.5) distribution and our alternative hypothesis HAis that the data is drawn from some\nother distribution. Do all of the following:\n1. Make a table of the observed and expected counts.\n2. Compute both the likelihood ratio statistic Gand Pearson's chi-square statistic X2.\n3. Compute the degrees of freedom of the null distribution.\n4. Compute the p-values corresponding to Gand X2.\nSolution: All of the R code used for this example is in class19.r.\n1. Assuming H0 the data truly comes from a binomial(8, 0.5) distribution. We have 51\ntotal observations, so the expected count for each outcome is just 51 times its probability.\nWe computed the binomial(8, 0.5) probabilities and expected counts in R:\nOutcomes\n≥5\nObserved counts\nH0 probabilities\n0.0039\n0.0313\n0.1094\n0.2188\n0.2734\n0.3633\nExpected counts\n0.20\n1.59\n5.58\n11.16\n13.95\n18.53\n2. Using the formulas above we compute that X2 = 116.41 and G= 66.08\n3.\nThe only statistic used in computing the expected counts was the total number of\nobservations 51. So, the degrees of freedom is 5, i.e we can set 5 of the cell counts freely\nand the last is determined by requiring that the total number is 51.\n4. The p-values are pG=1 - pchisq(G, 5) and pX2 = 1 - pchisq(X2, 5). Both p-\nvalues are effectively 0. For almost any significance level we would reject H0 in favor of\nHA.\n4.5.1\nDegrees of freedom in chi-square tests\nWe alreay gave a quick definition of degrees of freedom for a chi-square test. Here we will\ntry to go a little slower in showing how to compute degrees of freedom.\nTo start, recall that in a chi-square test, our table has nobserved counts. Then, we use\nobserved counts and the null hypothesis to compute nexpected counts. This is typically\ndone by computing some statistics and using them estimate the parameters needed to\ncompute the expected counts. For example, in the previous example the statistic computed\nwas the total number of counts.\n\n18.05 Class 19, Null Hypothesis Significance Testing III, Spring 2022\nNow, imagine that we are allowed to fabricate the nobserved counts, but we demand that\nour made up observations produce the same statistics as the true observed counts. That is,\nour imaginary observed counts need to produce the same expected counts as the true data.\nThe degrees of freedom is the number of fake observed counts we can freely choose. The\nrest will be determined by our constraint that they produce the same statistics.\nExample 10. (Degrees of freedom.) Suppose we have the following observed counts\nOutcomes\nObserved counts\nSuppose our null hypothesis H0 is that the data producing these counts was drawn from\n50 trials of a binomial(5, θ) distribution. Our alternative hypothesis HAis that the data\nis drawn from some other distribution.\nRun a chi-square test of these hypotheses with\nsignificance 0.05.\nSolution: To compute expected counts we need a value of θ. Since it is not known, we\nhave to use the data to estimate it.\nThe total number of observations is 50. So, the mean of the data is\nm= 6 ⋅0 + 9 ⋅1 + 13 ⋅2 + 12 ⋅3 + 7 ⋅4 + 3 ⋅5\n= 2.28.\nThe expected value of a binom(5,θ) distribution, is 5θ. The maximum likelihood estimate\nfor θis\nθ= m/5 = 0.456.\nNow, just like in the previous example, we can compute expected counts for each possible\noutcome. The expected count of outcome kis 50 ⋅p(k). In R this is 50*dbinom(k, 50,\nθ). We have the following table\nOutcomes\nObserved counts\nExpected counts\n2.38\n9.98\n16.74\n14.03\n5.88\n0.99\nTo determine the degrees of freedom:\n(i) We have 6 observed counts.\n(ii) To compute the expected counts we need the total number of counts = 50 and our\nestimate of\nθ= m/5 That is, we have two constraints: the total number of counts is 50,\nand mean m= 2.28.\nSo, to get the same expected counts, we could choose 4 of the observed counts freely and\nthen set last two counts so the constraints are met. Thus, there are 4 degrees of freedom.\nMore briefly: 6 observed counts - 2 constraints = 4 degrees of freedom.\nUsing R we can compute G= 8.01, with p-value 0.09. Thus, at significance level 0.05 we\nwould not reject the null hypothesis.\nNote, the X2 statistic is 11.05 with p-value 0.026. Clearly this example is a borderline case,\nsince we reject H0 when using X2 and we don't when using G.\nIt bears repeating, for reasons like this, we never say H0 is false. The most we can say is\nthat the data does not support rejecting H0.\n\n18.05 Class 19, Null Hypothesis Significance Testing III, Spring 2022\n4.5.2\nMore examples\nExample 11. Mendel's genetic experiments (Adapted from Rice Mathematical Statis-\ntics and Data Analysis, 2nd ed., example C, p.314)\nIn one of his experiments on peas Mendel looked at 2 genetic trait pairs: smooth/wrinkled\nand yellow/green. Symbolically we label a smooth gene Sand a wrinkled gene s. Likewise\nwe use Yand yfor yellow and green respectively.\nMendel started by selecting a parent generation of homozygous plants. They were either\nsmooth/yellow (genes SSYY) and wrinkled/green (genes ssyy). He crossed the smooth/yellow\nwith the wrinkled/green peas creating the, so called, F1 generation consisting of plants with\ngenes SsYy. Since smooth (S) and yellow Yare both dominant traits, all these plants were\nsmooth/yellow.\nHe then crossed 556 pairs of the F1 generation to create the F2 generation. We would expect\n1/4 of the F2 generation to have two smooth genes (SS), 1/4 to have two wrinkled genes (ss),\nand the remaining 1/2 to be heterozygous (Ss). We also expect these fractions for yellow\n(Y) and green (y) genes. If the color and smoothness genes are inherited independently\nand smooth and yellow are both dominant we'd expect the following table of frequencies\nfor phenotypes.\nYellow\nGreen\nSmooth\n9/16\n3/16\n3/4\nWrinkled\n3/16\n1/16\n1/4\n3/4\n1/4\nProbability table for the null hypothesis\nSo from the 556 crosses the expected number of smooth yellow peas is 556×9/16 = 312.75.\nLikewise for the other possibilities. Here is a table giving the observed and expected counts\nfrom Mendel's experiments.\nObserved count\nExpected count\nSmooth yellow\n312.75\nSmooth green\n104.25\nWrinkled yellow\n104.25\nWrinkled green\n34.75\nThe null hypothesis is that the observed counts are random samples distributed according\nto the frequency table given above. We use the counts to compute our statistics\nThe likelihood ratio statistic is\nG= 2 ∗∑Oiln (Oi\nEi\n)\n= 2 ∗(315 ln ( 315\n412.75) + 108 ln ( 108\n104.25) + 102 ln ( 102\n104.25) + 31 ln ( 31\n34.75))\n= 0.618\nPearson's chi-square statistic is\nX2 = ∑(Oi-Ei)2\nEi\n=\n2.752\n312.75 + 3.752\n104.25 + 2.252\n104.25 + 3.752\n34.75 = 0.604\n\n18.05 Class 19, Null Hypothesis Significance Testing III, Spring 2022\nYou can see that the two statistics are very close. This is usually the case. In general the\nlikelihood ratio statistic is more robust and should be preferred.\nThe degrees of freedom is 3, because there are 4 observed quantities and one relation between\nthem, i.e. they sum to 556. So, under the null hypothesis Gfollows a χ2(3) distribution.\nUsing R to compute the p-value we get\np= 1- pchisq(0.618, 3) = 0.8923\nAssuming the null hypothesis we would see data at least this extreme almost 90% of the\ntime. We would not reject the null hypothesis for any reasonable significance level.\nThe p-value using Pearson's statistic is 0.8955 -nearly identical.\nThe script class19.r shows these calculations and also how to use chisq.test to run a\nchi-square test directly.\n4.6\nChi-square test for homogeneity\nThis is a test to see if several independent sets of random data are all drawn from the same\ndistribution. (The meaning of homogeneity in this case is that all the distributions are the\nsame.)\n- Use: Test whether mdifferent independent sets of discrete data are drawn from the\nsame distribution.\n- Outcomes: ω1, ω2, ... , ωnare the possible outcomes. These are the same for each set\nof data.\n- Data: We assume mindependent sets of data giving counts for each of the possible\noutcomes. That is, for data set iwe have an observed count Oi,jfor each possible\noutcome ωj.\n- Assumptions: None\n- H0: Each data set is drawn from the same distribution. (We don't specify what this\ndistribution is.)\n- HA: The data sets are not all drawn from the same distribution.\n- Test statistic: See the example below. There are mncells containing counts for each\noutcome for each data set. Using the null distribution we can estimate expected counts\nfor each of the data sets. The statistics X2 and Gare computed exactly as above.\n- Degrees of freedom df: (m-1)(n-1). (See the example below.)\n- The null distribution χ2(df). The p-values are computed just as in the chi-square test\nfor goodness of fit.\n- R code: The R function chisq.test can be used to do the computations for a chi-\nsquare test use X2. For Gyou either have to do it by hand or find a package that has\na function. (It will probably be called likelihood.test or G.test.\nExample 12. Someone claims to have found a long lost work by William Shakespeare.\nThey ask you to test whether or not the play was actually written by Shakespeare .\n\n18.05 Class 19, Null Hypothesis Significance Testing III, Spring 2022\nYou go to https://www.opensourceshakespeare.org and pick a random 12 pages from\nKing Lear and count the use of common words. You do the same thing for the 'long lost\nwork'. You get the following table of counts.\nWord\na\nan\nthis\nthat\nKing Lear\nLong lost work\nUsing this data, set up and evaluate a significance test of the claim that the long lost book\nis by William Shakespeare. Use a significance level of 0.1.\nSolution: The null hypothesis H0: For the 4 words counted the long lost book has the\nsame relative frequencies as the counts taken from King Lear.\nThe total word count of both books combined is 500, so the the maximum likelihood estimate\nof the relative frequencies assuming H0 is simply the total count for each word divided by\nthe total word count.\nWord\na\nan\nthis\nthat\nTotal count\nKing Lear\nLong lost work\ntotals\nrel. frequencies under H0\n240/500\n50/500\n40/500\n170/500\n500/500\nNow the expected counts for each book under H0 are the total count for that book times\nthe relative frequencies in the above table. The following table gives the counts: (observed,\nexpected) for each book.\nWord\na\nan\nthis\nthat\nTotals\nKing Lear\n(150, 144)\n(30, 30)\n(30, 24)\n(90, 102)\n(300, 300)\nLong lost work\n(90, 96)\n(20, 20)\n(10, 16)\n(80, 68)\n(200, 200)\nTotals\n(249, 240)\n(50, 50)\n(40, 40)\n(170, 170)\n(500, 500)\nThe chi-square statistic is\nX2 = ∑(Oi-Ei)2\nEi\n= 62\n144 + 02\n30 + 62\n24 + 122\n102 + 62\n96 + 02\n20 + 62\n16 + 122\n≈7.9\nThere are 8 cells and all the marginal counts are fixed because they were needed to determine\nthe expected counts. To be consistent with these statistics we could freely set the values\nin 3 cells in the table, e.g.\nthe 3 blue cells, then the rest of the cells are determined\nin order to make the marginal totals correct.\nThus df= 3.\n(Or we could recall that\ndf= (m-1)(n-1) = (3)(1) = 3, where mis the number of columns and nis the number\nof rows.)\nUsing R we find p = 1-pchisq(7.9,3) = 0.048. Since this is less than our significance\nlevel of 0.1 we reject the null hypothesis that the relative frequencies of the words are the\nsame in both books.\nIf we make the further assumption that all of Shakespeare's plays have similar word fre-\nquencies (which is something we could check) we conclude that the book is probably not\n\n18.05 Class 19, Null Hypothesis Significance Testing III, Spring 2022\nby Shakespeare.\n4.7\nOther tests\nThere are far too many other tests to even make a dent. We will see some of them in\nclass and on psets. Again, we urge you to master the paradigm of NHST and recognize the\nimportance of choosing a test statistic with a known null distribution.\n\nComparison of frequentist and Bayesian inference.\nClass 20, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Be able to explain the difference between the p-value and a posterior probability to a\ndoctor.\nIntroduction\nWe have now learned about two schools of statistical inference: Bayesian and frequentist.\nBoth approaches allow one to evaluate evidence about competing hypotheses. In these notes\nwe will review and compare the two approaches, starting from Bayes' formula.\nBayes' formula as touchstone\nIn our first unit (probability) we learned Bayes' formula, a perfectly abstract statement\nabout conditional probabilities of events:\nP(A| B) = P(B| A)P(A)\nP(B)\n.\nWe began our second unit (Bayesian inference) by reinterpreting the events in Bayes' for-\nmula:\nP(H| D) = P(D| H)P(H)\nP(D)\n.\nNow His a hypothesis and Dis data which may give evidence for or against H. Each\nterm in Bayes' formula has a name and a role.\n- The prior P(H) is the probability that His true before the data is considered.\n- The posterior P(H| D) is the probability that His true after the data is considered.\n- The likelihood P(D| H) is the evidence about Hprovided by the data D.\n- P(D) is the total probability of the data taking into account all possible hypotheses.\nIf the prior and likelihood are known for all hypotheses, then Bayes' formula computes the\nposterior exactly. Such was the case when we rolled a die randomly selected from a cup\nwhose contents you knew. We call this the deductive logic of probability theory, and it gives\na direct way to compare hypotheses, draw conclusions, and make decisions.\nIn most experiments, the prior probabilities on hypotheses are not known. In this case, our\nrecourse is the art of statistical inference: we either make up a prior (Bayesian) or do our\nbest using only the likelihood (frequentist).\n\n18.05 Class 20, Comparison of frequentist and Bayesian inference., Spring 2022\nThe Bayesian school models uncertainty by a probability distribution over hypotheses.\nOne's ability to make inferences depends on one's degree of confidence in the chosen prior,\nand the robustness of the findings to alternate prior distributions may be relevant and\nimportant.\nThe frequentist school only uses conditional distributions of data given specific hypotheses.\nThe presumption is that some hypothesis (parameter specifying the conditional distribution\nof the data) is true and that the observed data is sampled from that distribution.\nIn\nparticular, the frequentist approach does not depend on a subjective prior that may vary\nfrom one investigator to another.\nThese two schools may be further contrasted as follows:\nBayesian inference\n- uses probabilities for both hypotheses and data.\n- depends on the prior and likelihood of observed data.\n- requires one to know or construct a 'subjective prior'.\n- dominated statistical practice before the 20th century.\n- may be computationally intensive due to integration over many parameters.\nFrequentist inference (NHST)\n- never uses or gives the probability of a hypothesis (no prior or posterior).\n- depends on the likelihood P(D| H)) for both observed and unobserved data.\n- does not require a prior.\n- dominated statistical practice during the 20th century.\n- tends to be less computationally intensive.\nFrequentist measures like p-values and confidence intervals continue to dominate research,\nespecially in the life sciences.\nHowever, in the current era of powerful computers and\nbig data, Bayesian methods have undergone an enormous renaissance in fields like ma-\nchine learning and genetics. There are now a number of large, ongoing clinical trials using\nBayesian protocols, something that would have been hard to imagine a generation ago.\nWhile professional divisions remain, the consensus forming among top statisticians is that\nthe most effective approaches to complex problems often draw on the best insights from\nboth schools working in concert.\nCritiques and defenses\n4.1\nCritique of Bayesian inference\n1. The main critique of Bayesian inference is that a subjective prior is, well, subjective.\nThere is no single method for choosing a prior, so different people will produce different\npriors and may therefore arrive at different posteriors and conclusions.\n\n18.05 Class 20, Comparison of frequentist and Bayesian inference., Spring 2022\n2. Furthermore, there are philosophical objections to assigning probabilities to hypotheses,\nas hypotheses do not constitute outcomes of repeatable experiments in which one can mea-\nsure long-term frequency. Rather, a hypothesis is either true or false, regardless of whether\none knows which is the case. A coin is either fair or unfair; treatment 1 is either better or\nworse than treatment 2; the sun will or will not come up tomorrow.\n4.2\nDefense of Bayesian inference\n1. The probability of hypotheses is exactly what we need to make decisions. When the\ndoctor tells me a screening test came back positive I want to know what is the probability\nthis means I'm sick. That is, I want to know the probability of the hypothesis \"I'm sick\".\n2. Using Bayes' theorem is logically rigorous. Once we have a prior all our calculations\nhave the certainty of deductive logic.\n3. By trying different priors we can see how sensitive our results are to the choice of prior.\n4. It is easy to communicate a result framed in terms of probabilities of hypotheses.\n5. Even though the prior may be subjective, one can specify the assumptions used to arrive\nat it, which allows other people to challenge it or try other priors.\n6. The evidence derived from the data is independent of notions about 'data more extreme'\nthat depend on the exact experimental setup (see the \"Stopping rules\" section below).\n7. Data can be used as it comes in. There is no requirement that every contingency be\nplanned for ahead of time.\n4.3\nCritique of frequentist inference\n1. It is ad-hoc and does not carry the force of deductive logic. Notions like 'data more\nextreme' are not well defined. The p-value depends on the exact experimental setup (see\nthe \"Stopping rules\" section below).\n2. Experiments must be fully specified ahead of time. This can lead to paradoxical seeming\nresults. See the 'voltmeter story' in:\nhttps://en.wikipedia.org/wiki/Likelihood_principle\n3. The p-value and significance level are notoriously prone to misinterpretation. Careful\nstatisticians know that a significance level of 0.05 means the probability of a type I error\nis 5%. That is, if the null hypothesis is true then 5% of the time it will be rejected due to\nrandomness. Many (most) other people erroneously think a p-value of 0.05 means that the\nprobability of the null hypothesis is 5%.\nStrictly speaking you could argue that this is not a critique of frequentist inference but,\nrather, a critique of popular ignorance. Still, the subtlety of the ideas certainly contributes\nto the problem. (see \"Mind your p's\" below).\n4.4\nDefense of frequentist inference\n1. It is objective: all statisticians will agree on the p-value. Any individual can then decide\nif the p-value warrants rejecting the null hypothesis.\n\n18.05 Class 20, Comparison of frequentist and Bayesian inference., Spring 2022\n2. Hypothesis testing using frequentist significance testing is applied in the statistical anal-\nysis of scientific investigations, evaluating the strength of evidence against a null hypothesis\nwith data. The interpretation of the results is left to the user of the tests. Different users\nmay apply different significance levels for determining statistical significance.\nFrequen-\ntist statistics does not pretend to provide a way to choose the significance level; rather it\nexplicitly describes the trade-off between type I and type II errors.\n3. Frequentist experimental design demands a careful description of the experiment and\nmethods of analysis before starting. This helps control for experimenter bias.\n4. The frequentist approach has been used for over 100 years and we have seen tremendous\nscientific progress. Although the frequentist themself would not put a probability on the\nbelief that frequentist methods are valuable, shouldn't this history give the Bayesian a\nstrong prior belief in the utility of frequentist methods?\nMind your p's.\nWe run a two-sample t-test for equal means, with α= 0.05, and obtain a p-value of 0.04.\nWhat are the odds that the two samples are drawn from distributions with the same mean?\n(a) 19/1\n(b) 1/19\n(c) 1/20\n(d) 1/24\n(e) unknown\nSolution: (e) unknown. Frequentist methods only give probabilities of statistics condi-\ntioned on hypotheses. They do not give probabilities of hypotheses.\nStopping rules\nWhen running a series of trials we need a rule on when to stop. Two common rules are:\n1. Run exactly ntrials and stop.\n2. Run trials until you see a certain result and then stop.\nIn this example we'll consider two coin tossing experiments.\nExperiment 1: Toss the coin exactly 6 times and report the number of heads.\nExperiment 2: Toss the coin until the first tails and report the number of heads.\nJon is worried that his coin is biased towards heads, so before using it in class he tests it\nfor fairness. He runs an experiment and reports to Jerry that his sequence of tosses was\nHHHHHT. But Jerry is only half-listening, and he forgets which experiment Jon ran to\nproduce the data.\nFrequentist approach.\nSince he's forgotten which experiment Jon ran, Jerry the frequentist decides to compute\nthe p-values for both experiments given Jon's data.\nLet θbe the probability of heads. We have the null and one-sided alternative hypotheses\nH0 ∶θ= 0.5,\nHA∶θ> 0.5.\nExperiment 1: The null distribution is binomial(6, 0.5) so, the one sided p-value is the\nprobability of 5 or 6 heads in 6 tosses. Using R we get\np= 1 - pbinom(4, 6, 0.5) = 0.1094.\n\n18.05 Class 20, Comparison of frequentist and Bayesian inference., Spring 2022\nExperiment 2: The null distribution is geometric(0.5) so, the one sided p-value is the prob-\nability of 5 or more heads before the first tails. Using R we get\np= 1 - pgeom(4, 0.5) = 0.0313.\nUsing the typical significance level of 0.05, the same data leads to opposite conclusions! We\nwould reject H0 in experiment 2, but not in experiment 1.\nThe frequentist is fine with this. The set of possible outcomes is different for the different\nexperiments so the notion of extreme data, and therefore p-value, is different. For example,\nin experiment 1 we would consider THHHHHto be as extreme as HHHHHT. In ex-\nperiment 2 we would never see THHHHHsince the experiment would end after the first\ntails.\nBayesian approach.\nJerry the Bayesian knows it doesn't matter which of the two experiments Jon ran, since\nthe binomial and geometric likelihood functions (columns) for the data HHHHHTare\nproportional. In either case, he must make up a prior, and he chooses Beta(3,3). This is a\nrelatively flat prior concentrated over the interval 0.25 ≤θ≤0.75.\nSee https://mathlets.org/mathlets/beta-distribution/\nSince the beta and binomial (or geometric) distributions form a conjugate pair the Bayesian\nupdate is simple. Data of 5 heads and 1 tails gives a posterior distribution Beta(8,4). Here\nis a graph of the prior and the posterior. The blue lines at the bottom are 50% and 90%\nprobability intervals for the posterior.\n0.0\n1.0\n2.0\n3.0\nθ\n.25\n.50\n.75\n1.0\nPrior Beta(3,3)\nPosterior Beta(8,4)\nPrior and posterior distributions with 0.5 and 0.9 probability intervals\nHere are the relevant computations in R:\nPosterior 50% probability interval: qbeta(c(0.25, 0.75), 8, 4) = [0.58 0.76]\nPosterior 90% probability interval: qbeta(c(0.05, 0.95), 8, 4) = [0.44 0.86]\nP(θ> 0.50 | data) = 1- pbeta(0.5, posterior.a, posterior.b) = 0.89\nStarting from the prior Beta(3,3), the posterior probability that the coin is biased toward\nheads is 0.89.\n\n18.05 Class 20, Comparison of frequentist and Bayesian inference., Spring 2022\nMaking decisions\nQuite often the goal of statistical inference is to help with making a decision, e.g. whether\nor not to undergo surgery, how much to invest in a stock, whether or not to go to graduate\nschool, etc.\nIn statistical decision theory, consequences of taking actions are measured by a utility\nfunction. The utility function assigns a weight to each possible outcome; in the language of\nprobability, it is simply a random variable.\nFor example, in my investments I could assign a utility of dto the outcome of a gain of\nddollars per share of a stock (if d< 0 my utility is negative). On the other hand, if my\ntolerance for risk is low, I will assign a more negative utility to losses than to gains (say,\n-d2 if d< 0 and dif d≥0).\nA decision rule combines the expected utility with evidence for each hypothesis given by\nthe data (e.g., p-values or posterior distributions) into a formal statistical framework for\nmaking decisions.\nIn this setting, the frequentist will consider the expected utility given a hypothesis\nE[U| H]\nwhere Uis the random variable representing utility.\nThere are frequentist methods for\ncombining the expected utility with p-values of hypotheses to guide decisions.\nThe Bayesian can combine E[U| H] with the posterior (or prior if it's before data is col-\nlected) to create a Bayesian decision rule.\nIn either framework, two people considering the same investment may have different utility\nfunctions and make different decisions. For example, a riskier stock (with higher potential\nupside and downside) will be more appealing with respect to the first utility function above\nthan with respect to the second (loss-averse) one.\nA significant theoretical result is that for any decision rule there is a Bayesian decision rule\nwhich is, in a precise sense, at least as good a rule.\nThe likelihood principle\nWe briefly mention the likelihood principle. It can be stated succinctly as\n'All of the evidence from data is contained in its likelihood function'\n- Controversial\n- Consistent with Bayesian updating.\nIt only uses the column in the likelihood table that is for the data we actually saw.\n- Inconsistent with NHST.\nComputing significance and p-values uses the entire likelihood table. That is, it relies\non the probabilities of both observed and unobserved data (the full experimental\ndesign).\n\n18.05 Class 20, Comparison of frequentist and Bayesian inference., Spring 2022\n- See\nhttps://en.wikipedia.org/wiki/Likelihood_principle\n\nConfidence intervals based on normal data\nClass 22, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Be able to determine whether an expression defines a valid interval statistic.\n2. Be able to compute zand tconfidence intervals for the mean given normal data.\n3. Be able to compute the χ2 confidence interval for the variance given normal data.\n4. Be able to define the confidence level of a confidence interval.\n5. Be able to explain the relationship between the zconfidence interval (and confidence\nlevel) and the znon-rejection region (and significance level) in NHST.\nIntroduction\nWe continue to survey the tools of frequentist statistics. Suppose we have a model (proba-\nbility distribution) for observed data with an unknown parameter. We have seen how NHST\nuses data to test the hypothesis that the unknown parameter has a particular value.\nWe have also seen how point estimates like the MLE use data to provide an estimate of the\nunknown parameter. On its own, a point estimate like\nx= 2.2 carries no information about\nits accuracy; it's just a single number, regardless of whether its based on ten data points or\none million data points.\nFor this reason, statisticians augment point estimates with confidence intervals. For exam-\nple, to estimate an unknown mean μwe might be able to say that our best estimate of\nthe mean is x= 2.2 with a 95% confidence interval [1.2, 3.2]. Another way to describe the\ninterval is: x± 1.\nWe will leave to later the explanation of exactly what the 95% confidence level means.\nFor now, we'll note that taken together the width of the interval and the confidence level\nprovide a measure on the strength of the evidence supporting the hypothesis that the μis\nclose to our estimate x. You should think of the confidence level of an interval as analogous\nto the significance level of a NHST. As explained below, it is no accident that we often see\nsignificance level α= 0.05 and confidence level 0.95 = 1 -α.\nWe will first explore confidence intervals in situations where you will easily be able to com-\npute by hand: zand tconfidence intervals for the mean and χ2 confidence intervals for the\nvariance. We will use R to handle all the computations in more complicated cases. Indeed,\nthe challenge with confidence intervals is not their computation, but rather interpreting\nthem correctly and knowing how to use them in practice.\n\n18.05 Class 22, Confidence intervals based on normal data, Spring 2022\nInterval statistics\nRecall that our definition of a statistic is anything that can be computed from data. In\nparticular, the formula for a statistic cannot include unknown quantities.\nExample 1. Suppose x1, ... , xnis drawn from N(μ, σ2) where μand σare unknown.\n(i) xand x-5 are statistics.\n(ii) x-μis not a statistic since μis unknown.\n(iii) If μ0 a known value, then x-μ0 is a statistic. This case arises when we consider the\nnull hypothesis μ= μ0. For example, if the null hypothesis is μ= 5, then the statistic\nx-μ0 is just x-5 from (i).\nWe can play the same game with intervals to define interval statistics\nExample 2. Suppose x1, ... , xnis drawn from N(μ, σ2) where μis unknown.\n(i) The interval [x-2.2, x+ 2.2] = x± 2.2 is an interval statistic.\n(ii) If σis known, then [x-2σ\n√n, x+ 2σ\n√n] is an interval statistic.\n(iii) On the other hand, if σis unknown then [x-2σ\n√n, x+ 2σ\n√n] is not an interval statistic.\n(iv) If s2 is the sample variance, then [x-2s\n√n, x+ 2s\n√n] is an interval statistic because s2\nis computed from the data.\nWe will return to (ii) and (iv), as these are respectively the zand tconfidence intervals for\nestimating μ.\nTechnically an interval statistic is nothing more than a pair of point statistics giving the\nlower and upper bounds of the interval. Our reason for emphasizing that the interval is a\nstatistic is to highlight the following:\n1. The interval is random - new random data will produce a new interval.\n2. As frequentists, we are perfectly happy using it because it doesn't depend on the value\nof an unknown parameter or hypothesis.\n3. As usual with frequentist statistics we have to assume a certain hypothesis, e.g. value\nof μ, before we can compute probabilities about the interval.\nExample 3. Suppose we draw nsamples x1, ... , xnfrom a N(μ, 1) distribution, where\nμis unknown. Suppose we wish to know the probability that 0 is in the interval\n[x-2, x+ 2]. Without knowing the value of μthis is impossible. However, we can\ncompute this probability for any given (hypothesized) value of μ.\n4. A warning which will be repeated: Be careful in your thinking about these probabili-\nties. Confidence intervals are a frequentist notion. Since frequentists do not compute\nprobabilities of hypotheses, the confidence level is never a probability that the\nunknown parameter is in the specific confidence interval computed from\nthe given data.\n\n18.05 Class 22, Confidence intervals based on normal data, Spring 2022\nzconfidence intervals for the mean\nThroughout this section we will assume that we have normally distributed data:\nx1, x2, ... , xn∼N(μ, σ2).\nAs we often do, we will introduce the main ideas through examples, building on what\nwe know about rejection and non-rejection regions in NHST until we have constructed a\nconfidence interval.\n4.1\nDefinition of zconfidence intervals for the mean\nWe start with zconfidence intervals for the mean. First we'll give the formula. Then we'll\nwalk through the derivation in one entirely numerical example. This will give us the basic\nidea. Then we'll repeat this example, replacing the explicit numbers by symbols. Finally\nwe'll work through a computational example.\nDefinition: Suppose the data x1, ... , xn∼N(μ, σ2), with unknown mean μand known\nvariance σ2. The (1 -α) confidence interval for μis\n[x-\nzα/2 ⋅σ\n√n\n,\nx+\nzα/2 ⋅σ\n√n\n] ,\n(1)\nwhere zα/2 is the right critical value P(Z> zα/2) = α/2.\nFor example, if α= 0.05 then zα/2 = 1.96 so the 0.95 (or 95%) confidence interval is\n[x-1.96σ\n√n, x+ 1.96σ\n√n] .\nWe've created an applet that generates normal data and displays the corresponding zcon-\nfidence interval for the mean. It also shows the t-confidence interval, as discussed in the\nnext section. Play around to get a sense for random intervals!\nhttps://mathlets.org/mathlets/confidence-intervals/\nExample 4.\nSuppose we collect 100 data points from a N(μ, 32) distribution and the\nsample mean is x= 12. Give the 95 % confidence interval for μ.\nSolution: Using formula 1, this is trivial to compute: the 95% confidence interval for μis\n[x-1.96σ\n√n, x+ 1.96σ\n√n] = [12 -1.96 ⋅3\n, 12 + 1.96 ⋅3\n]\n4.2\nExplaining the definition part 1: non-rejection regions\nOur next goal is to explain the definition 1 starting from our knowledge of rejection/non-\nrejection regions. The phrase 'non-rejection region' is not pretty, but we will discipline\nourselves to use it instead of the inacurate phrase 'acceptance region'.\n\n18.05 Class 22, Confidence intervals based on normal data, Spring 2022\nExample 5. Suppose that n= 12 data points are drawn from N(μ, 52) where μis unknown.\nAs usual, call the average of the data x. Set up a two-sided z-test of H0 ∶μ= 2.71 at\nsignificance level α= 0.05. Describe the rejection and non-rejection regions.\nSolution: Under the null hypothesis (μ= 2.71) we have\nz= x-2.71\n5/\n√\n∼N(0, 1)\nWe know that, for α= 0.05, the non-rejection region for zis\n[-1.96, 1.96].\nThat is, we do not reject if, assuming H0, zis within two standard deviations of the\nstandardized mean. By definition, this means\nP(-1.96 ≤z≤1.96 | μ= 2.71) = 0.95.\nAnd, the rejection region is\n(-inf, -1.96) ∪(1.96, inf).\nFor confidence intervals, we will want to unwind the definition of zand write the regions in\nterms of x. This allows us to directly use the natural statistic x.\nExample 6. Redo the previous example using xas the test statistic.\nSolution: Under the null hypothesis (μ= 2.71) we have xi∼N(2.71, 52) and thus\nx∼N(2.71, 52/12)\nwhere 52/12 is the variance x.\nWe know that for normal data, significance α= 0.05\ncorresponds to a rejection region starting 1.96 standard deviations from the hypothesized\nmean. That is,\nNon-rejection region: We do not reject H0 if xis in the interval\n[2.71 -1.96 ⋅5\n√\n,\n2.71 + 1.96 ⋅5\n√\n] = [-0.12, 5.54].\nThat is, we do not reject if, assuming H0, xis within two standard deviations of the\nhypothesized mean. By definition, this means\nP(-0.12 ≤x≤5.54 | μ= 2.71) = 0.95.\nRejection region:\n(-inf, 2.71 -1.96 ⋅5\n√\n] ∪[2.71 + 1.96 ⋅5\n√\n, inf) = (-inf, -0.12] ∪[5.54, inf).\nThe following figure shows the rejection and non-rejection regions for x. The regions repre-\nsent ranges of xso they are represented by the colored bars on the xaxis. The area of the\nshaded region in the tails is the significance level.\n\n18.05 Class 22, Confidence intervals based on normal data, Spring 2022\nx\nN(2.71, 52/12)\n-0.12\n5.54\n2.71\nThe rejection (orange) and non-rejection (blue) regions for x.\nNow, what about different data or null hypotheses. This is straight-forward, let's redo the\nprevious example using symbols for all quantities.\nExample 7. Suppose that ndata points are drawn from N(μ, σ2) where μis unknown and\nσis known. Set up a two-sided significance test of H0 ∶μ= μ0 using the statistic xat\nsignificance level α. Describe the rejection and non-rejection regions.\nSolution: Under the null hypothesis μ= μ0 we have xi∼N(μ0, σ2) and thus\nx∼N(μ0, σ2/n),\nwhere σ2/nis the variance (σx)2 of xand μ0, σand nare all known values.\nLet zα/2 be the critical value: P(Z> zα/2) = α/2. Then the non-rejection and rejection\nregions are separated by the values of xthat are zα/2 ⋅σxfrom the hypothesized mean.\nSince σx=\nσ\n√nwe have\nNon-rejection region: we do not reject H0 if xis in the interval\n[μ0 -\nzα/2 ⋅σ\n√n\n,\nμ0 +\nzα/2 ⋅σ\n√n\n]\n(2)\nRejection region:\n(-inf, μ0 -\nzα/2 ⋅σ\n√n\n] ∪[μ0 +\nzα/2 ⋅σ\n√n\n, inf) .\nWe get the same figure as above, with the explicit numbers replaced by symbolic values.\nx\nN(μ0, σ2/n)\nμ0\nμ0 -\nzα/2⋅σ\n√n\nμ0 +\nzα/2⋅σ\n√n\nThe rejection (orange) and non-rejection (blue) regions for x.\n4.3\nManipulating intervals: algebraic pivoting\nWe need to get comfortable manipulating intervals. In general, we will make use of the\ntype of 'obvious' statements that can be hard to get across. First is the notion of pivoting.\nStripping away the statistical terms, pivoting is the following algebraic maneuver.\n\n18.05 Class 22, Confidence intervals based on normal data, Spring 2022\nExample 8. Algebraic pivoting. Suppose we have two variables aand b. Suppose also\nthat ais in the interval [b-4, b+ 6]. Show that bis in the interval [a-6, a+ 4].\nSolution: We are given, b-4 ≤a≤b+ 6. Therefore,\n-4 ≤a-b≤6 ⇒4 ≥b-a≥-6 ⇒a+ 4 ≥b≥a-6.\nQED\nThis is called pivoting because the roles of aand bare reversed along with the direction of\nthe inequalities.\nIn the example above, the ranges on either side of bare different. Quite often they will be\nthe same. Here are some simple numerical examples of pivoting for symmetric intervals.\nExample 9. (i) 1.5 is in the interval [0-2.3, 0+2.3], so 0 is in the interval [1.5-2.3, 1.5+2.3]\n(ii) Likewise 1.5 is not in the interval [0-1, 0+1], so 0 is not in the interval [1.5-1, 1.5+1].\n4.4\nPivoting non-rejection intervals to confidence intervals\nFor normal data, the non-rejection region for xis an interval centered on μ0. By pivoting,\nwe get the confidence interval for μcentered on x.\nExample 10. Suppose we have ndata points with a sample mean xand hypothesized\nmean μ0 = 2.71. Suppose also that the null distribution is xi∼N(μ0, 32). Then with a\nsignificance level of 0.05 we have:\n(1a) The non-rejection region is centered on μ0 = 2.71. That is, we don't reject H0 if xis\nin the interval\n[μ0 -1.96σ\n√n, μ0 + 1.96σ\n√n]\n(1b) Assuming the null hypothesis we have\nP(xis in the non-rejection region | H0) = 1 -α= 0.95.\nThat is,\nP(μ0 -1.96σ\n√n\n≤x≤μ0 + 1.96σ\n√n| H0) = 0.95\n(2a) Pivoting (1a) gives: we don't reject H0 if μ0 is in the interval\n[x-1.96σ\n√n, x+ 1.96σ\n√n]\n(2b) Pivoting (1b) gives: assuming the null hypothesis we have\nP(x-1.96σ\n√n\n≤μ0 ≤x+ 1.96σ\n√n| H0) = 0.95\nThe interval in (2a) is called the 0.95 confidence interval for μ. It is centered on x, it has\nthe same width as the non-rejection region.\n\n18.05 Class 22, Confidence intervals based on normal data, Spring 2022\nAgain, notice the symmetry: the statement 'xis in the non-rejection interval around μ0' is\nequivalent to 'μ0 is in the confidence interval [x-1.96σ, x+ 1.96σ] around x'.\nHere is a visualization of pivoting from intervals around μ0 to intervals around x. In the\nfigures, μ0 = 1 and x= 1.5. The first pair of intervals have width 2 and the second pair\nhave width 4.6.\n-2\n-1\nμ0\nx\nμ0 ± 1\ninterval centered on μ0 does not contain x\nx± 1\ninterval centered on xdoes not contain μ0\nμ0 ± 2.3\ninterval centered on μ0 contains x\nx± 2.3\ninterval centered on xcontains μ0\nThe first pair of intervals shows the interval μ0 ± 1 pivoted to the interval x± 1. Since x\nis not in the first interval, μ0 is not in the pivoted interval. In the second pair of intervals,\nsince xis in the interval μ0 ± 2.3, we see μ0 is in the pivoted interval x± 2.3.\n4.5\nSummary of normal confidence intervals: definition and properties\nSuppose x1, x2, ... , xnare independent data from a N(μ, σ2) distribution. We assume μis\nunknown, but σis known.\n- Definition. The 1 -αconfidence interval for μis\n[x-\nzα/2σ\n√n, x+\nzα/2σ\n√n] ,\nwhere zα/2 is standard normal α/2 critical value.\n- The confidence interval only depends on xand known values, so it is a statistic.\n- The confidence interval is random: different data generate different intervals.\n- If the null hypothesis is μ= μ0, then the confidence interval is found by pivoting the\nnon-rejection region. If μ0 is in the 1 -αconfidence interval, then we do not reject\nH0 at significance level α. Likewise, we do reject H0 at significance level αif μ0 is not\nin the 1 -αconfidence interval.\n- Assuming H0, then in 95% of random trials the 95% confidence interval will contain\nμ0.\nThe following figure illustrates how we don't reject H0 if the confidence interval around x\ncontains μ0 and we reject H0 if the confidence interval doesn't contain μ0. There is a lot in\nthe figure so we will list carefully what you are seeing:\n1. We started with the figure from Example 6 which shows the null distribution for μ0 = 2.71\nand the rejection and non-rejection regions.\n2. We added two possible values of the statistic x, i.e. x1 and x2, and their confidence\nintervals. Note that the width of each interval is exactly the same as the width of the\nnon-rejection region since both use ±1.96 ⋅5\n√\n.\n\n18.05 Class 22, Confidence intervals based on normal data, Spring 2022\nThe first value, x1, is in the non-rejection region and its interval includes the null hypothesis\nμ0 = 2.71. This illustrates that not rejecting H0 corresponds to the confidence interval\ncontaining μ0.\nThe second value, x2, is in the rejection region and its interval does not contain μ0. This\nillustrates that rejecting H0 corresponds to the confidence interval not containing μ0.\nx\nN(2.71, 52/12)\n-.12\n5.54\n2.71\nx1\nx2\nThe non-rejection region (blue) and two confidence intervals (light blue).\nWe can still wring one more essential observation out of this example. Our choice of null\nhypothesis μ= 2.71 was completely arbitrary. If we replace μ= 2.71 by any other hy-\npothesis μ= μ0 then the confidence interval is the same, i.e. it does not depend on any\nhypothesis.\n4.6\nExplaining the definition part 3: translating a general non-rejection\nregion to a confidence interval\nNote that the specific values of σand nin the preceding example were of no particular\nconsequence, so they can be replaced by their symbols. In this way we can take Example 7\nquickly through the same steps as Example6.\nIn words, Equation 2 and the corresponding figure say that we don't reject if\nxis in the interval μ0 ±\nzα/2σ\n√n.\nThis is exactly equivalent to saying that we don't reject if\nμ0 is in the interval x±\nzα/2σ\n√n.\n(3)\nWe can rewrite equation 3 as: at significance level αwe don't reject if\nthe interval\n[x-\nzα/2 ⋅σ\n√n\n,\nx+\nzα/2 ⋅σ\n√n\n]\ncontains μ0.\n(4)\nWe call the interval 4 a (1 -α)\nconfidence interval because, assuming μ= μ0, on average\nit will contain\nμ0\nin the fraction (1 -α) of random trials.\nThe following figure illustrates the point that μ0 is in the (1-α) confidence interval around\nxis equivalent to xis in the non-rejection region (at significance level α) for H0 ∶μ0 = μ.\nThe figure shows x1 is in the non-rejection region for μ0, so the confidence interval around\nx1 contains μ0.\nSimilarly, x2 is not in the non-rejection region for μ0, so the confidence interval around x2\ndoes not contain μ0.\n\n18.05 Class 22, Confidence intervals based on normal data, Spring 2022\nNote, that the confidence intervals and the non-rejection region all have the same width!\nx\nN(μ0, σ2/n)\nμ0 -zα/2 ⋅\nσ\n√n\nμ0 + zα/2 ⋅\nσ\n√n\nμ0\nx1\nx2\n4.7\nComputational example\nExample 11. Suppose the data 2.5, 5.5, 8.5, 11.5 was drawn from a N(μ, 102) distribution\nwith unknown mean μ.\n(a) Compute the point estimate xfor μand the corresponding 50%, 80% and 95% confidence\nintervals.\n(b) Consider the null hypothesis μ= 1. Would you reject H0 at α= 0.05? α= 0.20?\nα= 0.50? Do these two ways: first by checking if the hypothesized value of μis in the\nrelevant confidence interval and second by constructing a rejection region.\nSolution: (a) We compute that x= 7.0. The critical points are\nz0.025 = qnorm(0.975) = 1.96,\nz0.1 = qnorm(0.9) = 1.28,\nz0.25 = qnorm(0.75) =\n0.67.\nSince n= 4 we have x∼N(μ, 102/4), i.e. σx= 5. So we have:\n95% conf. interval\n=\n[x-z0.025σx,\nx+ z0.025σx]\n=\n[7 -1.96 ⋅5,\n7 + 1.96 ⋅5]\n=\n[-2.8,\n16.8]\n80% conf. interval\n=\n[x-z0.1σx,\nx+ z0.1σx]\n=\n[7 -1.28 ⋅5,\n7 + 1.28 ⋅5]\n=\n[ 0.6,\n13.4]\n50% conf. interval\n=\n[x-z0.75σx,\nx+ z0.75σx]\n=\n[7 -0.67 ⋅5,\n7 + 0.67 ⋅5]\n=\n[ 3.65,\n10.35]\nEach of these intervals is a range estimate of μ. Notice that the higher the confidence level,\nthe wider the interval needs to be.\n(b) Since μ= 1 is in the 95% and 80% confidence intervals, we would not reject the null\nhypothesis at the α= 0.05 or α= 0.20 levels. Since μ= 1 is not in the 50% confidence\ninterval, we would reject H0 at the α= 0.5 level.\nWe construct the rejection regions using the same critical values as in part (a). The differ-\nence is that rejection regions are intervals centered on the hypothesized value for μ: μ0 = 1\nand confidence intervals are centered on x. Here are the rejection regions.\nα= 0.05\n⇒\n(-inf, μ0 -z0.025σx]\n∪\n[μ0 + z0.025σx, inf)\n=\n(-inf, -8.8]\n∪\n[10.8, inf)\nα= 0.20\n⇒\n(-inf, μ0 -z0.1σx]\n∪\n[μ0 + z0.1σx, inf)\n=\n(-inf, -5.4]\n∪\n[7.4, inf)\nα= 0.25\n⇒\n(-inf, μ0 -z0.25σx]\n∪\n[μ0 + z0.25σx, inf)\n=\n(-inf, -2.35]\n∪\n[4.35, inf)\nTo to do the NHST we must check whether or not x= 7 is in the rejection region.\n\n18.05 Class 22, Confidence intervals based on normal data, Spring 2022\nα= 0.05:\n7 < 10.8 is not in the rejection region.\nWe do not reject the hypothesis that μ= 1 at a significance level of 0.05.\nα= 0.2:\n7 < 7.4 is not in the rejection region.\nWe do not reject the hypothesis that μ= 1 at a significance level of 0.2.\nα= 0.5:\n7 > 4.35 is in the rejection region.\nWe reject the hypothesis that μ= 1 at a significance level 0.5.\nWe get the same answers using either method.\nt-confidence intervals for the mean\nThis will be nearly identical to normal confidence intervals. In this setting σis not known,\nso we have to make the following replacements.\n1. Use sx=\ns\n√ninstead of σx=\nσ\n√n. Here sis the sample variance we used before in\nt-tests\n2. Use t-critical values instead of z-critical values.\n5.1\nDefinition of t-confidence intervals for the mean\nDefinition: Suppose that x1, ... , xn∼N(μ, σ2), where the values of the mean μand the\nstandard deviation σare both unknown. . The (1 -α) confidence interval for μis\n[x-\ntα/2 ⋅s\n√n\n,\nx+\ntα/2 ⋅s\n√n\n] ,\n(5)\nhere tα/2 is the right critical value P(T> tα/2) = α/2 for T∼t(n-1) and s2 is the sample\nvariance of the data.\n5.2\nConstruction of tconfidence intervals\nFor tconfidence intervals we repeat the construction of normal confidence intervals with σ\nreplaced by its estimate s.\nSuppose that ndata points are drawn from N(μ, σ2) where μand σare unknown. We'll\nderive the tconfidence interval following the same pattern as for the zconfidence interval.\nUnder the null hypothesis μ= μ0, we have xi∼N(μ0, σ2). So the studentized mean follows\na Student tdistribution with n-1 degrees of freedom:\nt= x-μ0\ns/√n∼t(n-1).\nLet tα/2 be the critical value: P(T> tα/2) = α/2, where T∼t(n-1). We know from\nrunning one-sample t-tests that the non-rejection region is given by\n|t| ≤tα/2\n\n18.05 Class 22, Confidence intervals based on normal data, Spring 2022\nUsing the definition of the t-statistic to write the rejection region in terms of xwe get: at\nsignificance level αwe don't reject if\n|x-μ0|\ns/√n\n≤tα/2\n⇔\n|x-μ0| ≤tα/2 ⋅\ns\n√n.\nGeometrically, the right hand side says that we don't reject if\nμ0 is within tα/2 ⋅\ns\n√nof x.\nThis is exactly equivalent to saying that we don't reject if\nthe interval\n[x-\ntα/2 ⋅s\n√n\n,\nx+\ntα/2 ⋅s\n√n\n]\ncontains μ0.\nThis interval is the confidence interval defined in 5.\nExample 12. Suppose the data 2.5, 5.5, 8.5, 11.5 was drawn from a N(μ, σ2) distribution\nwith μand σboth unknown.\nGive interval estimates for μby finding the 95%, 80% and 50% confidence intervals.\nSolution: By direct computation we have x= 7 and s2 = 15. The critical points are\nt0.025 = qt(0.975) = 3.18, t0.1 = qt(0.9) = 1.64, and t0.25 = qt(0.75) = 0.76.\n95% conf. interval\n=\n[x-t0.025 ⋅\ns\n√n,\nx+ t0.025 ⋅\ns\n√n]\n=\n[0.84,\n13.16]\n80% conf. interval\n=\n[x-t0.1 ⋅\ns\n√n,\nx+ t0.1 ⋅\ns\n√n]\n=\n[3.82,\n10.18]\n50% conf. interval\n=\n[x-t0.25 ⋅\ns\n√n,\nx+ t0.25 ⋅\ns\n√n]\n=\n[5.53,\n8.47]\nAll of these confidence intervals give interval estimates for the value of μ. Again, notice\nthat the higher the confidence level, the wider the corresponding interval.\nChi-square confidence intervals for the variance\nWe now turn to an interval estimate for the unknown variance.\nDefinition: Suppose the data x1, ... , xnis drawn from N(μ, σ2) with mean μand standard\ndeviation σboth unknown. The (1 -α) confidence interval for the variance σ2 is\n[(n-1)s2\ncα/2\n,\n(n-1)s2\nc1-α/2\n] .\n(6)\nHere cα/2 is the right critical value P(X2 > cα/2) = α/2 for X2 ∼χ2(n-1) and s2 is the\nsample variance of the data.\nThe derivation of this interval is nearly identical to that of the previous derivations, now\nstarting from the chi-square test for variance. The basic fact we need is that, for data drawn\nfrom N(μ, σ2), the statistic\n(n-1)s2\nσ2\n\n18.05 Class 22, Confidence intervals based on normal data, Spring 2022\nfollows a chi-square distribution with n-1 degrees of freedom. So given the null hypothesis\nH0 ∶σ= σ0, the test statistic is (n-1)s2/σ2\n0 and the non-rejection region at significance\nlevel αis\nc1-α/2 < (n-1)s2\nσ2\n< cα/2.\nPivoting algebra converts this to\n(n-1)s2\nc1-α/2\n> σ2\n0 > (n-1)s2\ncα/2\n.\nThis says we don't reject if\nthe interval\n[(n-1)s2\ncα/2\n, (n-1)s2\nc1-α/2\n]\ncontains σ2\nThis is our (1 -α) confidence interval.\nA difference from the zand tconfidence intervals is that this chi-square confidence intervals\nare not exactly symmetric around the estimator s2.\nThe reason is that the chi-square\ndistribution (with n-1 degrees of freedom) is not symmetric around its mean n-1.\nWe will continue our exploration of confidence intervals next class. In the meantime, truly\nthe best way is to internalize the meaning of the confidence level is to experiment with the\nconfidence interval applet:\nhttps://mathlets.org/mathlets/confidence-intervals/\n\nConfidence Intervals: Three Views\nClass 23, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Be able to find z, tand χ2 confidence intervals using the corresponding standardized\nstatistics.\n2. Be able to use a hypothesis test to find a confidence interval for an unknown parameter.\n3. Refuse to answer questions that ask, in essence, 'given a confidence interval what is\nthe probability or odds that it contains the true value of the unknown parameter?'\nIntroduction\nOur approach to confidence intervals in the previous reading was a combination of stan-\ndardized statistics and hypothesis testing. Today we will consider each of these perspectives\nseparately, as well as introduce a third formal viewpoint. Each provides its own insight.\n1. Standardized statistic. Most confidence intervals are based on standardized statistics\nwith known distributions like z, tor χ2. This provides a straightforward way to construct\nand interpret confidence intervals as a point estimate plus or minus some error.\n2. Hypothesis testing. Confidence intervals may also be constructed from hypothesis\ntests. In cases where we don't have a standardized statistic this method will still work. It\nagrees with the standardized statistic approach in cases where they both apply.\nThis view connects the notions of significance level αfor hypothesis testing and confidence\nlevel 1 -αfor confidence intervals; we will see that in both cases αis the probability of\nmaking a 'type 1' error. This gives some insight into the use of the word confidence. This\nview also helps to emphasize the frequentist nature of confidence intervals.\n3. Formal. The formal definition of confidence intervals is perfectly precise and general.\nIn a mathematical sense it gives insight into the inner workings of confidence intervals.\nHowever, because it is so general it sometimes leads to confidence intervals without useful\nproperties.\nWe will not dwell on this approach.\nWe offer it mainly for those who are\ninterested.\nConfidence intervals via standardized statistics\nThe strategy here is essentially the same as in the previous reading. Assuming normal data\nwe have what we called standardized statistics like the standardized mean, Studentized\nmean, and standardized variance.\nThese statistics have well known distributions which\ndepend on hypothesized values of μand σ. We then use algebra to produce confidence\nintervals for μor σ.\n\n18.05 Class 23, Confidence Intervals: Three Views, Spring 2022\nDon't let the algebraic details distract you from the essentially simple idea underlying\nconfidence intervals: we start with a standardized statistic (e.g., z, tor χ2) and use some\nalgebra to get an interval that depends only on the data and known parameters.\n3.1\nz-confidence intervals for the mean: normal data with known stan-\ndard deviation\nz-confidence intervals for the mean of normal data are based on the standardized mean, i.e.\nthe z-statistic. We start with nindependent normal samples\nx1, x2, ... , xn∼N(μ, σ2).\nWe assume that μis the unknown parameter of interest and σis known. Notationally, let's\nwrite the (unknown) true value of μas μ0\nWe know that the standardized mean is standard normal:\nz= x-μ0\nσ/√n∼N(0, 1).\nFor the standard normal critical value zα/2 we have: P(-zα/2 < Z< zα/2) = 1 -α. Thus,\nP(-zα/2 < x-μ\nσ/√n< zα/2 | μ= μ0) = 1 -α\nA little bit of algebra puts this in the form of an interval around μ:\nP(x-zα/2 ⋅σ\n√n< μ< x+ zα/2 ⋅σ\n√n| μ= μ0) = 1 -α\nWe can emphasize that the interval depends only on the statistic xand the known value σ\nby writing this as\nP([x-zα/2 ⋅σ\n√n, x+ zα/2 ⋅σ\n√n]\ncontains μ| μ= μ0) = 1 -α.\nThis is the (1 -α) z-confidence interval for μ. We often write it using the shorthand\nx± zα/2 ⋅σ\n√n\nThink of it as x± error.\nMake sure you notice that the probabilities are conditioned on μ= μ0. As with all frequen-\ntist statistics, we have to fix hypothesized values of the parameters in order to compute\nprobabilities.\n3.2\nt-confidence intervals for the mean: normal data with unknown mean\nand standard deviation\nt-confidence intervals for the mean of normal data are based on the Studentized mean, i.e.\nthe t-statistic.\n\n18.05 Class 23, Confidence Intervals: Three Views, Spring 2022\nAgain we have x1, x2, ... , xn∼N(μ, σ2), but now we assume both μand σare unknown. As\nwe did above, let's write the (unknown) true value of μas μ0. We know that the Studentized\nmean follows a Student tdistribution with n-1 degrees of freedom. That is,\nt= x-μ0\ns/√n∼t(n-1),\nwhere s2 is the sample variance.\nNow all we have to do is replace the standardized mean by the Studentized mean and the\nsame logic we used for zgives us the t-confidence interval: start with\nP(-tα/2 < x-μ\ns/√n< tα/2 | μ= μ0) = 1 -α.\nA little bit of algebra isolates μin the middle of an interval:\nP(x-tα/2 ⋅\ns\n√n< μ< x+ tα/2 ⋅\ns\n√n| μ= μ0) = 1 -α\nWe can emphasize that the interval depends only on the statistics xand sby writing this\nas\nP([x-tα/2 ⋅\ns\n√n, x+ tα/2 ⋅\ns\n√n]\ncontains μ| μ= μ0) = 1 -α.\nThis is the (1 -α) t-confidence interval for μ. We often write it using the shorthand\nx± tα/2 ⋅\ns\n√n\nThink of it as x± error.\n3.3\nChi-square confidence intervals for variance: normal data with un-\nknown mean and standard deviation\nYou guessed it: χ2-confidence intervals for the variance of normal data are based on the\nstandardized variance, i.e. the χ2-statistic.\nWe follow the same logic as above to get a χ2-confidence interval for σ2. Because this is\nthe third time through it we'll move a little more quickly.\nWe assume we have nindependent normal samples:\nx1, x2, ... , xn∼N(μ, σ2). We assume\nthat μand σare both unknown and write the (unknown) true value of σas σ0.\nThe\nstandardized variance is\nX2 = (n-1)s2\nσ2\n∼χ2(n-1).\nWe know that the X2 statistic follows a χ2 distribution with n-1 degrees of freedom.\nFor Zand twe used, without comment, the symmetry of the distributions to replace z1-α/2\nby -zα/2 and t1-α/2 by -tα/2. Because the χ2 distribution is not symmetric we need to be\nexplicit about the critical values on both the left and the right. That is,\nP(c1-α/2 < X2 < cα/2) = 1 -α,\n\n18.05 Class 23, Confidence Intervals: Three Views, Spring 2022\nwhere cα/2 and c1-α/2 are right tail critical values. Thus,\nP(c1-α/2 < (n-1)s2\nσ2\n< cα/2 | σ= σ0) = 1 -α\nA little bit of algebra puts this in the form of an interval around σ2:\nP((n-1)s2\ncα/2\n< σ2 < (n-1)s2\nc1-α/2\n| σ= σ0) = 1 -α\nWe can emphasize that the interval depends only on the statistic s2 by writing this as\nP([(n-1)s2\ncα/2\n, (n-1)s2\nc1-α/2\n]\ncontains σ2 | σ= σ0) = 1 -α.\nThis is the (1 -α) χ2-confidence interval for σ2.\nConfidence intervals via hypothesis testing\nSuppose we have data drawn from a distribution with a parameter θwhose value is unknown.\nA significance test for the value θhas the following short description.\n1. Set the null hypothesis H0 ∶θ= θ0 for some special value θ0, e.g. we often have H0 ∶\nθ= 0.\n2. Use the data to compute the value of a test statistic, call it x.\n3. If xis far enough into the tail of the null distribution (the distribution assuming the null\nhypothesis) then we reject H0.\nIn the case where there is no special value to test we may still want to estimate θ. This is\nthe reverse of significance testing; rather than seeing if we should reject a specific value of\nθbecause it doesn't fit the data we want to find the range of values of θthat do, in some\nsense, fit the data. This gives us the following definitions.\nDefinition. Given a value xof the test statistic, the (1-α) confidence interval contains all\nvalues θ0 which are not rejected (at significance level α) when they are the null hypothesis.\nDefinition. A type 1 CI error occurs when the confidence interval does not contain the\ntrue value of θ.\nFor a (1 -α) confidence interval the type 1 CI error rate is α.\nExample 1. Here is an example relating confidence intervals and hypothesis tests. Suppose\ndata xis drawn from a binomial(12, θ) distribution with θunknown. Let α= 0.1 and create\nthe (1 -α) = 90% confidence interval for each possible value of x.\nSolution: Our strategy is to look at one possible value of θat a time and choose rejection\nregions for a significance test with α= 0.1. Once this is done, we will know, for each value\nof x, which values of θare not rejected, i.e. the confidence interval associated with x.\nTo start we set up a likelihood table for binomial(12, θ) in Table 1. Each row shows the\nprobabilities p(x|θ) for one value of θ. To keep the size manageable we only show θin\nincrements of 0.1.\n\n18.05 Class 23, Confidence Intervals: Three Views, Spring 2022\nθ\\x\n1.0\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.00\n0.9\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.02\n0.09\n0.23\n0.38\n0.28\n0.8\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.02\n0.05\n0.13\n0.24\n0.28\n0.21\n0.07\n0.7\n0.00\n0.00\n0.00\n0.00\n0.01\n0.03\n0.08\n0.16\n0.23\n0.24\n0.17\n0.07\n0.01\n0.6\n0.00\n0.00\n0.00\n0.01\n0.04\n0.10\n0.18\n0.23\n0.21\n0.14\n0.06\n0.02\n0.00\n0.5\n0.00\n0.00\n0.02\n0.05\n0.12\n0.19\n0.23\n0.19\n0.12\n0.05\n0.02\n0.00\n0.00\n0.4\n0.00\n0.02\n0.06\n0.14\n0.21\n0.23\n0.18\n0.10\n0.04\n0.01\n0.00\n0.00\n0.00\n0.3\n0.01\n0.07\n0.17\n0.24\n0.23\n0.16\n0.08\n0.03\n0.01\n0.00\n0.00\n0.00\n0.00\n0.2\n0.07\n0.21\n0.28\n0.24\n0.13\n0.05\n0.02\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.1\n0.28\n0.38\n0.23\n0.09\n0.02\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.0\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nTable 1. Likelihood table for Binomial(12, θ)\nTables 2-4 below show the rejection region (in orange) and non-rejection region (in blue)\nfor the various values of θ. To emphasize the row-by-row nature of the process the Table 2\njust shows these regions for θ= 1.0, then Table 3 adds in regions for θ= 0.9 and Table 4\nshows them for all the values of θ.\nImmediately following the tables we give a detailed explanation of how the rejection/non-\nrejection regions were chosen.\nθ\\x\nsignificance\n1.0\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.00\n0.000\n0.9\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.02\n0.09\n0.23\n0.38\n0.28\n0.8\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.02\n0.05\n0.13\n0.24\n0.28\n0.21\n0.07\n0.7\n0.00\n0.00\n0.00\n0.00\n0.01\n0.03\n0.08\n0.16\n0.23\n0.24\n0.17\n0.07\n0.01\n0.6\n0.00\n0.00\n0.00\n0.01\n0.04\n0.10\n0.18\n0.23\n0.21\n0.14\n0.06\n0.02\n0.00\n0.5\n0.00\n0.00\n0.02\n0.05\n0.12\n0.19\n0.23\n0.19\n0.12\n0.05\n0.02\n0.00\n0.00\n0.4\n0.00\n0.02\n0.06\n0.14\n0.21\n0.23\n0.18\n0.10\n0.04\n0.01\n0.00\n0.00\n0.00\n0.3\n0.01\n0.07\n0.17\n0.24\n0.23\n0.16\n0.08\n0.03\n0.01\n0.00\n0.00\n0.00\n0.00\n0.2\n0.07\n0.21\n0.28\n0.24\n0.13\n0.05\n0.02\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.1\n0.28\n0.38\n0.23\n0.09\n0.02\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.0\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nTable 2. Likelihood table for binomial(12, θ) with rejection (orange)/non-rejection (blue)\nregions for θ= 1.0\nθ\\x\nsignificance\n1.0\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.00\n0.000\n0.9\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.02\n0.09\n0.23\n0.38\n0.28\n0.026\n0.8\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.02\n0.05\n0.13\n0.24\n0.28\n0.21\n0.07\n0.7\n0.00\n0.00\n0.00\n0.00\n0.01\n0.03\n0.08\n0.16\n0.23\n0.24\n0.17\n0.07\n0.01\n0.6\n0.00\n0.00\n0.00\n0.01\n0.04\n0.10\n0.18\n0.23\n0.21\n0.14\n0.06\n0.02\n0.00\n0.5\n0.00\n0.00\n0.02\n0.05\n0.12\n0.19\n0.23\n0.19\n0.12\n0.05\n0.02\n0.00\n0.00\n0.4\n0.00\n0.02\n0.06\n0.14\n0.21\n0.23\n0.18\n0.10\n0.04\n0.01\n0.00\n0.00\n0.00\n0.3\n0.01\n0.07\n0.17\n0.24\n0.23\n0.16\n0.08\n0.03\n0.01\n0.00\n0.00\n0.00\n0.00\n0.2\n0.07\n0.21\n0.28\n0.24\n0.13\n0.05\n0.02\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.1\n0.28\n0.38\n0.23\n0.09\n0.02\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.0\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nTable 3. Likelihood table: rejection (orange)/non-rejection (blue) regions for θ= 1.0 and\n0.9\n\n18.05 Class 23, Confidence Intervals: Three Views, Spring 2022\nθ\\x\nsignificance\n1.0\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.00\n0.000\n0.9\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.02\n0.09\n0.23\n0.38\n0.28\n0.026\n0.8\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.02\n0.05\n0.13\n0.24\n0.28\n0.21\n0.07\n0.073\n0.7\n0.00\n0.00\n0.00\n0.00\n0.01\n0.03\n0.08\n0.16\n0.23\n0.24\n0.17\n0.07\n0.01\n0.052\n0.6\n0.00\n0.00\n0.00\n0.01\n0.04\n0.10\n0.18\n0.23\n0.21\n0.14\n0.06\n0.02\n0.00\n0.077\n0.5\n0.00\n0.00\n0.02\n0.05\n0.12\n0.19\n0.23\n0.19\n0.12\n0.05\n0.02\n0.00\n0.00\n0.092\n0.4\n0.00\n0.02\n0.06\n0.14\n0.21\n0.23\n0.18\n0.10\n0.04\n0.01\n0.00\n0.00\n0.00\n0.077\n0.3\n0.01\n0.07\n0.17\n0.24\n0.23\n0.16\n0.08\n0.03\n0.01\n0.00\n0.00\n0.00\n0.00\n0.052\n0.2\n0.07\n0.21\n0.28\n0.24\n0.13\n0.05\n0.02\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.073\n0.1\n0.28\n0.38\n0.23\n0.09\n0.02\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.026\n0.0\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.000\nTable 4. Likelihood table: rejection (orange)/non-rejection (blue) regions for θ= 0.0 to\n1.0\nChoosing the rejection and non-rejection regions in the tables\nThe first problem we confront is how exactly to choose the rejection region. We used two\nrules:\n1. The total probabilitiy of the rejection region, i.e. the significance, should be less than or\nequal to 0.1. (Since we have a discrete distribution it is impossible to make the significance\nexactly 0.1.)\n2. We build the rejection region by choosing values of xone at a time, always picking the\nunused value with the smallest probability. We stop when the next value would make the\nsignificance more that 0.1.\nThere are other ways to choose the rejection region which would result in slight differences.\nOur method is one reasonable way.\nTable 2 shows the rejection (orange) and non-rejection (blue) regions for θ= 1.0. This is\na special case because most of the probabilities in this row are 0.0. We'll move right on to\nthe next table and step through the process for that.\nIn Table 3, let's walk through the steps used to find these regions for θ= 0.9.\n- The smallest probability is when x= 0, so x= 0 is in the rejection region.\n- The next smallest is when x= 1, so x= 1 is in the rejection region.\n- We continue with x= 2, ... , 8. At this point the total probability in the rejection\nregion is 0.026.\n- The next smallest probability is when x= 9. Adding this probability (0.09) to 0.026\nwould put the total probability over 0.1. So we leave x= 9 out of the rejection region\nand stop the process.\nNote three things for the θ= 0.9 row:\n1. None of the probabilities in this row are truly zero, though some are small enough that\nthey equal 0 to 2 decimal places.\n2. We show the significance for this value of θin the right hand margin. More precisely, we\nshow the significance level of the NHST with null hypothesis θ= 0.9 and the given rejection\nregion.\n\n18.05 Class 23, Confidence Intervals: Three Views, Spring 2022\n3. The rejection region consists of values of x. When we say the rejection region is shown\nin orange we really mean the rejection region contains the values of xcorresponding to the\nprobabilities highlighted in orange.\nThink: Look back at the θ= 1.0 row and make sure you understand why the rejection\nregion is x= 0, ... , 11 and the significance is 0.000.\nExample 2. Using Table 4 determine the 0.90 confidence interval when x= 8.\nSolution: The 90% confidence interval consists of all those θthat would not be rejected\nby an α= 0.1 hypothesis test when x= 8. Looking at the table, the blue (non-rejected)\nentries in the column x= 8 correspond to 0.5 ≤θ≤0.8: the confidence interval is [0.5, 0.8].\nRemark: The point of this example is to show how confidence intervals and hypothesis\ntests are related. Since Table 4 has only finitely many values of θ, our answer is close but\nnot exact. Using a computer we could look at many more values of θ. For this problem we\nused R to find that, correct to 2 decimal places, the confidence interval is [0.42, 0.85].\nExample 3.\nExplain why the expected type one CI error rate will be at most 0.092,\nprovided that the true value of θis in the table.\nSolution: The short answer is that this is the maximum significance for any θin Table 4.\nExpanding on that slightly: we make a type one CI error if the confidence interval does not\ncontain the true value of θ, call it θtrue. This happens exactly when the data xis in the\nrejection region for θtrue. The probability of this happening is the significance for θtrue and\nthis is at most 0.092.\nRemark: The point of this example is to show how confidence level, type one CI error rate\nand significance for each hypothesis are related. As in the previous example, we can use R\nto compute the significance for many more values of θ. When we do this we find that the\nmaximum significance for any θis 0.1 ocurring when θ≈0.0452.\nSummary notes:\n1. We start with a test statistic x. The confidence interval is random because it depends\non x.\n2. For each hypothesized value of θwe make a significance test with significance level αby\nchoosing rejection regions.\n3. For a specific value of xthe associated confidence interval for θconsists of all θthat\naren't rejected for that value, i.e. all θthat have xin their non-rejection regions.\n4. Because the distribution is discrete we can't always achieve the exact significance level,\nso our confidence interval is really an 'at least 90% confidence interval'.\nExample 4. Open the applet https://mathlets.org/mathlets/confidence-intervals/.\nWe want you to play with the applet to understand the random nature of confidence inter-\nvals and the meaning of confidence as (1 - type I CI error rate).\n(a) Read the help. It is short and will help orient you in the applet. Play with different\nsettings of the parameters to see how they affect the size of the confidence intervals.\n(b) Set the number of trials to N= 1. Click the 'Run N trials' button repeatedly and see\nthat each time data is generated the confidence intervals jump around.\n(c) Now set the confidence level to c= 0.5. As you click the 'Run N trials' button you\n\n18.05 Class 23, Confidence Intervals: Three Views, Spring 2022\nshould see that about 50% of the confidence intervals include the true value of μ. The 'Z\ncorrect' and 't correct' values should change accordingly.\n(d) Now set the number of trials to N= 100. With c= 0.8. The 'Run N trials' button will\nnow run 100 trials at a time. Only the last confidence interval will be shown in the graph,\nbut the trials all run and the 'percent correct' statistics will be updated based on all 100\ntrials.\nClick the run trials button repeatedly. Watch the correct rates start to converge to the\nconfidence level. To converge even faster, set N= 1000.\nFormal view of confidence intervals\nRecall: An interval statistic is an interval Ixcomputed from data x. An interval is deter-\nmined by its lower and upper bounds, and these are random because xis random.\nWe suppose that xis drawn from a distribution with pdf f(x|θ) where the parameter θis\nunknown.\nDefinition: A (1 -α) confidence interval for θis an interval statistic Ixsuch that\nP(Ixcontains θ0 | θ= θ0) = 1 -α\nfor all possible values of θ0.\nWe wish this was simpler, but a definition is a definition and this definition is one way to\nweigh the evidence provided by the data x. Let's unpack it a bit.\nThe confidence level of an interval statistic is a probability concerning a random interval\nand a hypothesized value θ0 for the unknown parameter. Precisely, it is the probability\nthat the random interval Ix(computed from random data x) contains the value θ0, given\nthat the model parameter truly is θ0. Since the true value of θis unknown, the frequentist\nstatistician defines 95% confidence intervals so that the 0.95 probability is valid no matter\nwhich hypothesized value of the parameter is actually true.\nComparison with Bayesian probability intervals\nConfidence intervals are a frequentist notion, and as we've repeated many times, frequentists\ndon't assign probabilities to hypotheses, e.g., to the value of an unknown parameter. Rather\nthey compute likelihoods; that is, probabilities about data or associated statistics given a\nhypothesis (note the condition θ= θ0 in the formal view of confidence intervals). Note that\nthe construction of confidence intervals proceeds entirely from the full likelihood table.\nIn contrast Bayesian posterior probability intervals are truly the probability that the value\nof the unknown parameter lies in the reported range. We add the usual caveat that this\ndepends on the specific choice of a (possibly subjective) Bayesian prior.\nThis distinction between the two is subtle because Bayesian posterior probability intervals\nand frequentist confidence intervals share the following properties:\n1. They start from a model f(x|θ) for observed data xwith unknown parameter θ.\n\n18.05 Class 23, Confidence Intervals: Three Views, Spring 2022\n2. Given data x, they give an interval I(x) specifying a range of values for θ.\n3. They come with a number (say 0.95) that is the probability of something.\nIn practice, many people misinterpret confidence intervals as Bayesian probability inter-\nvals, forgetting that frequentists never place probabilities on hypotheses (this is analogous\nto mistaking the p-value in NHST for the probability that H0 is false). The next section\nexplores this mistake in some detail. The harm of this misinterpretation is somewhat miti-\ngated by that fact that, given enough data and a reasonable prior, Bayesian and frequentist\nintervals often work out to be quite similar.\nFor an amusing example illustrating how they can be quite different, see the first answer\nin the link just below (involving chocolate chip cookies!). This example uses the formal\ndefinitions and is really about confidence sets instead of confidence intervals.\nhttps://stats.stackexchange.com/questions/2272/whats-the-difference-between-a-confidence-interval-and-a-credible-interval\nMisinterpreting confidence intervals\nIt is very tempting to think that given a 95% confidence interval for, say, the mean, the\nprobability that the true mean is in the confidence interval is 95%.\nWe know this can't be true because the value of the mean can only be hypothesized and\nFrequentists don't assign probabilities to hypotheses. To be more concrete, if the mean is\nθand the confidence interval is [45, 55] then the statement 45 ≤θ≤55 is a hypothesis, so\nasking for the probability that 45 ≤θ≤55, is asking for the probability of a hypothesis.\nThe mistake is subtle and hard to wrap your mind around. It boils down to a question of\nwhat is being randomly sampled. Here is an attempt to explain the issue.\nFirst, consider a test for a disease.\nAssume a person is given the test.\nLet T+ be a\npositive test and D+ be that they have the disease. Assume the test is 95% accurate, i.e.\nP(T+|D+) = 0.95. We know (base rate fallacy) that this does not imply that P(D+|T+) =\n0.95.\nLet's look at this from a different angle: Implicit in P(T+|D+) = 0.95 is the following\nexperiment: Draw a random person from the set of all people with the disease and give\nthem the test. Then 95% will test positive. That is, the population sampled is all people\nwith the disease and the event considered is that the chosen person in that population tests\npositive.\nFor P(D+|T+), the experiment is to draw a random person from the set of all people who\ntested positive. The probability is the fraction who have the disease. That is, the population\nsampled is all people who test positive and the event considered is that the chosen person\nin that population has the disease.\nWe can't expect P(T+|D+) and P(D+|T+) to be the same, since we're sampling from\ndifferent populations and looking at different events. The probability P(D+|T+) can be\ncomputed using Bayes' theorem from the (prior) probability P(D+) and the likelihoods\nP(T+|D+), P(T+|D-).\nConfidence intervals are a little more abstract, but the analysis is similar. Just as in testing\nfor a disease, the populations sampled will be different. One source of difficulty is that the\n\n18.05 Class 23, Confidence Intervals: Three Views, Spring 2022\nevents are essentially the same in both cases.\nLet's assume we have a distribution with unknown mean θ0. We generate some data and\ncompute a 95% confidence interval for the mean. The value of 95% comes from the following\nimplied experiment: imagine having run many trials and created a confidence interval for\neach one. Then, 95% of confidence intervals contain the true mean. In notation,\nP(random interval contains θ0) = 0.95.\nThat is, the random sample is drawn from the set of all confidence intervals generated by\nour trials. The event in question is that the chosen interval contains the true mean.\nWhat if we run one experiment and generate the 95% confidence interval, call it I. To a\nFrequentist, the true mean is not random and we have a fixed interval. So, to the Frequentist,\nit makes no sense to ask about the probability θ0 is in I.\nTo a Bayesian, it is fine to consider θ0 as randomly drawn from a probability distribution\n-they often interpret it as a description of the uncertainty of our knowledge. So, they can\nask for the probability\nP(random θ0 is in a given I).\nSo, here, the random sample is drawn from the set of possible means and the event consid-\nered is that the chosen mean is contained in the given interval.\nAs in the disease testing example, what population is being randomly sampled is different\nin the two cases, i.e in the first we have a random interval, in the second we have a random\nvalue for the true mean. As noted above, in both cases the event is that the true mean is\nin the interval.\nWe finish by noting that P(random θ0 is in I) can be computed using Bayes' theorem and\ndepends on the prior distribution for the true mean and the likelihoods that each mean will\ngenerate the given confidence interval. The formula is a little unwieldy. Here it is.\n- Call the interval Iand the true mean θ0.\n- Call the data I. This is a shorthand for the data that the interval Iis based on.\n- Let p(θ) be the prior probability that θ0 = θ.\n- The likelihood f(I|θ) is the probability (or density) that the experiment would produce\nthe interval Igiven θ0 = θ.\n- Let p(θ|I) be the posterior probability that θ0 = θ. This is the updated probability\nfound using the Bayes' theorem.\nBayes' theorem gives us\np(θ|I) = f(I|θ)p(θ)\nf(I)\n, where f(I) = ∑\nθ\nf(I|θ)p(θ).\nSo we have, P(θ0 ∈I|I) = ∑\nθin I\np(θ|I). (As usual, if θhas a continuous range of values,\nthen the sums will be replaced by integrals.)\n\nConfidence Intervals for the Mean of Non-normal Data\nClass 23, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Be able to derive the formula for conservative normal confidence intervals for the\nproportion θin Bernoulli data.\n2. Be able to find rule-of-thumb 95% confidence intervals for the proportion θof a\nBernoulli distribution.\n3. Be able to find large sample confidence intervals for the mean of a general distribution.\nIntroduction\nSo far, we have focused on constructing confidence intervals for data drawn from a normal\ndistribution. We'll now switch gears and learn about confidence intervals for the mean when\nthe data is not necessarily normal.\nWe will first look carefully at estimating the probability θof success when the data is drawn\nfrom a Bernoulli(θ) distribution - recall that θis also the mean of the Bernoulli distribution.\nThen we will consider the case of a large sample from an unknown distribution. In this case\nwe can appeal to the central limit theorem to justify the use z-confidence intervals.\nBernoulli data and polling\nOne common use of confidence intervals is for estimating the proportion θin a Bernoulli(θ)\ndistribution. For example, suppose we want to use a political poll to estimate the proportion\nof the population that supports candidate A, or equivalent the probability θthat a random\nperson supports candidate A. In this case we have a simple rule-of-thumb that allows us to\nquickly compute a confidence interval.\n3.1\nConservative normal confidence intervals\nSuppose we have i.i.d. data x1, x2, ... , xnall drawn from a Bernoulli(θ) distribution. then\na conservative normal (1 -α) confidence interval for θis given by\nx± zα/2 ⋅\n2√n.\n(1)\nThe proof given below uses the central limit theorem and the observation that σ= √θ(1 -θ) ≤\n1/2.\nYou will also see in the derivation below that this formula is conservative, providing an 'at\nleast (1 -α)' confidence interval.\n\n18.05 Class 23, Confidence Intervals for the Mean of Non-normal Data , Spring 2022\nExample 1. A pollster asks 196 people if they prefer candidate A to candidate B and finds\nthat 120 prefer Aand 76 prefer B. Find the 95% conservative normal confidence interval\nfor θ, the proportion of the population that prefers A.\nSolution: We have x= 120/196 = 0.612, α= 0.05 and z0.025 = 1.96. The formula says a\n95% confidence interval is\nI≈0.612 ± 1.96\n2 ⋅14 = 0.612 ± 0.007.\n3.2\nProof of Formula 1\nThe proof of Formula 1 will rely on the following fact.\nFact. The standard deviation of a Bernoulli(θ) distribution is at most 0.5.\nProof of fact: Let's denote this standard deviation by σθto emphasize its dependence on\nθ. The variance is then σ2\nθ= θ(1 -θ). It's easy to see using calculus or by graphing this\nparabola that the maximum occurs when θ= 1/2. Therefore the maximum variance is 1/4,\nwhich implies that the standard deviation σpis less the √1/4 = 1/2.\nProof of formula (1). The proof relies on the central limit theorem which says that (for\nlarge n) the distribution of xis approximately normal with mean θand standard deviation\nσθ/√n. For normal data we have the (1 -α) z-confidence interval\nx± zα/2 ⋅σθ\n√n\nThe trick now is to replace σθby 1\n2: since σθ≤1\n2 the resulting interval around x\nx± zα/2 ⋅\n2√n\nis always at least as wide as the interval using ± σθ/√n. A wider interval is more likely to\ncontain the true value of θso we have a 'conservative' (1 -α) confidence interval for θ.\nAgain, we call this conservative because\n2√noverestimates the standard deviation of\nx,\nresulting in a wider interval than is necessary to achieve a (1 -α) confidence level.\n3.3\nHow political polls are reported\nPolitical polls are often reported as a value with a margin-of-error. For example you might\nhear\n52% favor candidate A with a margin-of-error of ±5%.\nThe actual precise meaning of this is\nif θis the proportion of the population that supports A then the point\nestimate for θis 52% and the 95% confidence interval is 52% ± 5%.\nNotice that reporters of polls in the news do not mention the 95% confidence. You just\nhave to know that that's what pollsters do.\n\n18.05 Class 23, Confidence Intervals for the Mean of Non-normal Data , Spring 2022\nThe 95% rule-of-thumb confidence interval.\nRecall that the (1 -α) conservative normal confidence interval is\nx± zα/2 ⋅\n2√n.\nIf we use the standard approximation z0.025 = 2 (instead of 1.96) we get the rule-of thumb\n95% confidence interval for θ:\nx±\n√n.\nExample 2. Polling. Suppose there will soon be a local election between candidate Aand\ncandidate B. Suppose that the fraction of the voting population that supports Ais θ.\nTwo polling organizations ask voters who they prefer.\n1. The firm of Fast and First polls 40 random voters and finds 22 support A.\n2. The firm of Quick but Cautious polls 400 random voters and finds 190 support A.\nFind the point estimates and 95% rule-of-thumb confidence intervals for each poll. Explain\nhow the statistics reflect the intuition that the poll of 400 voters is more accurate.\nSolution: For poll 1 we have\nPoint estimate:\nx= 22/40 = 0.55\nConfidence interval:\nx±\n√n= 0.55 ±\n√\n= 0.55 ± 0.16 = 55% ± 16%.\nFor poll 2 we have\nPoint estimate:\nx= 190/400 = 0.475\nConfidence interval:\nx±\n√n= 0.475 ±\n√\n= 0.475 ± 0.05 = 47.5% ± 5%.\nThe greater accuracy of the poll of 400 voters is reflected in the smaller margin of error, i.e.\n5% for the poll of 400 voters vs. 16% for the poll of 40 voters.\nOther binomial proportion confidence intervals\nThere are many methods of producing confidence intervals for the proportion pof a binomial(n,\np) distribution. For a number of other common approaches, see:\nhttps://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval\nLarge sample confidence intervals\nOne typical goal in statistics is to estimate the mean of a distribution. When the data follows\na normal distribution we could use confidence intervals based on standardized statistics to\nestimate the mean.\nBut suppose the data x1, x2, ... , xnis drawn from a distribution with pmf or pdf f(x) that\nmay not be normal or even parametric. If the distribution has finite mean and variance\nand if nis sufficiently large, then the following version of the central limit theorem shows\nwe can still use a standardized statistic.\nCentral Limit Theorem: For large n, the sampling distribution of the studentized mean\nis approximately standard normal:\nx-μ\ns/√n\n≈\nN(0, 1).\n\n18.05 Class 23, Confidence Intervals for the Mean of Non-normal Data , Spring 2022\nSo for large nthe (1 -α) confidence interval for μis approximately\n[ x-\ns\n√n⋅zα/2,\nx+\ns\n√n⋅zα/2]\nwhere zα/2 is the α/2 critical value for N(0, 1). This is called the large sample confidence\ninterval.\nExample 3. How large must nbe?\nRecall that a type 1 CI error occurs when the confidence interval does not contain the true\nvalue of the parameter, in this case the mean. Let's call the value (1 -α) the nominal\nconfidence level. We say nominal because unless nis large we shouldn't expect the true\ntype 1 CI error rate to be α.\nWe can run numerical simulations to approximate of the true confidence level. We expect\nthat as ngets larger the true confidence level of the large sample confidence interval will\nconverge to the nominal value.\nWe ran such simulations for xdrawn from the exponential distribution exp(1) (which is far\nfrom normal). For several values of nand nominal confidence level cwe ran 100,000 trials.\nEach trial consisted of the following steps:\n1. draw nsamples from exp(1).\n2. compute the sample mean\nxand sample standard deviation s.\n3. construct the large sample cconfidence interval: x± zα/2 ⋅\ns\n√n.\n4. check for a type 1 CI error, i.e. see if the true mean μ= 1 is not in the interval.\nWith 100,000 trials, the empirical confidence level should closely approximate the true level.\nFor comparison we ran the same tests on data drawn from a standard normal distribution.\nHere are the results.\nnominal conf.\nn\n1 -α\nsimulated conf.\n0.95\n0.905\n0.90\n0.856\n0.80\n0.762\n0.95\n0.930\n0.90\n0.879\n0.80\n0.784\n0.95\n0.938\n0.90\n0.889\n0.80\n0.792\n0.95\n0.947\n0.90\n0.897\n0.80\n0.798\nnominal conf.\nn\n1 -α\nsimulated conf.\n0.95\n0.936\n0.90\n0.885\n0.80\n0.785\n0.95\n0.944\n0.90\n0.894\n0.80\n0.796\n0.95\n0.947\n0.900\n0.896\n0.800\n0.797\n0.950\n0.949\n0.900\n0.898\n0.800\n0.798\nSimulations for exp(1)\nSimulations for N(0, 1).\nFor the exp(1) distribution we see that for n= 20 the simulated confidence of the large\nsample confidence interval is less than the nominal confidence 1 -α. But for n= 100 the\nsimulated confidence and nominal confidence are quite close. So for exp(1), nsomewhere\nbetween 50 and 100 is large enough for most purposes.\n\n18.05 Class 23, Confidence Intervals for the Mean of Non-normal Data , Spring 2022\nThink: For n= 20 why is the simulated confidence for the N(0, 1) distribution is smaller\nthan the nominal confidence?\nThis is because we used zα/2 instead of tα/2. For large nthese are quite close, but for n= 20\nthere is a noticable difference, e.g. z0.025 = 1.96 and t0.025 = 2.09.\n\nBootstrap confidence intervals\nClass 24, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Be able to construct and sample from the empirical distribution of data.\n2. Be able to explain the bootstrap principle.\n3. Be able to design and run an empirical percentile or basic bootstrap to compute\nconfidence intervals.\n4. Be able to design and run a parametric bootstrap to compute confidence intervals.\nIntroduction\nThe empirical bootstrap is a statistical technique popularized by Bradley Efron in 1979.\nThough remarkably simple to implement, the bootstrap would not be feasible without\nmodern computing power.\nThe key idea is to perform computations on the data itself\nto estimate the variation of statistics that are themselves computed from the same data.\nThat is, the data is 'pulling itself up by its own bootstrap.' (A google search of 'by ones\nown bootstraps' will give you the etymology of this metaphor.) Such techniques existed\nbefore 1979, but Efron widened their applicability and demonstrated how to implement the\nbootstrap effectively using computers. He also coined the term 'bootstrap' 1.\nThe empircal bootstrap is also known as the nonparametric bootstrap.\nOur main application of the bootstrap will be to estimate the variation of point estimates;\nthat is, to estimate confidence intervals. An example will make our goal clear.\nExample 1. Suppose we have data\nx1, x2, ... , xn\nIf we knew the data was drawn from N(μ, σ2) with the unknown mean μand known variance\nσ2 then we have seen that\n[x-1.96 σ\n√n, x+ 1.96 σ\n√n]\nis a 95% confidence interval for μ.\nNow suppose the data is drawn from some completely unknown distribution. To have a\nname we'll call this distribution Fand its (unknown) mean μ. We can still use the sample\nmean xas a point estimate of μ. But how can we find a confidence interval for μaround\nx? Our answer will be to use the bootstrap!\n1Paraphrased from Dekking et al. A Modern Introduction to Probabilty and Statistics, Springer, 2005,\npage 275.\n\n18.05 Class 24, Bootstrap confidence intervals, Spring 2022\nIn fact, we'll see that the bootstrap handles other statistics as easily as it handles the mean.\nFor example: the median, other percentiles or the trimmed mean.\nThese are statistics\nwhere, even for normal distributions, it can be difficult to compute a confidence interval\nfrom theory alone.\nSampling\nIn statistics to sample from a set is to choose elements from that set. In a random sample\nthe elements are chosen randomly. There are two common methods for random sampling.\nSampling without replacement\nSuppose we draw 10 cards at random from a deck of 52 cards without putting any of the\ncards back into the deck between draws. This is called sampling without replacement or\nsimple random sampling. With this method of sampling our 10 card sample will have no\nduplicate cards.\nSampling with replacement\nNow suppose we draw 10 cards at random from the deck, but after each draw we put the\ncard back in the deck and shuffle the cards. This is called sampling with replacement. With\nthis method, the 10 card sample might have duplicates. It's even possible that we would\ndraw the 6 of hearts all 10 times.\nThink: What's the probability of drawing the 6 of hearts 10 times in a row?\nExample 2. We can view rolling an 8-sided die repeatedly as sampling with replacement\nfrom the set {1,2,3,4,5,6,7,8}. Since each number is equally likely, we say we are sampling\nuniformly from the data.\nNote. In practice if we take a small sample from a very large set then it doesn't matter\nwhether we sample with or without replacement. For example, if we randomly sample 400\nout of 300 million people in the U.S., then it is so unlikely that the same person will be\npicked twice that there is no real difference between sampling with or without replacement.\nThe empirical distribution of data\nThe empirical distribution of data is simply the distribution that you see in the data. Let's\nillustrate this with an example.\nExample 3. Suppose we roll an 8-sided die 10 times and get the following data, written\nin increasing order:\n1, 1, 2, 3, 3, 3, 3, 4, 7, 7.\nImagine writing these values on 10 slips of paper, putting them in a hat and drawing one\nat random. Then, for example, the probability of drawing a 3 is 4/10 and the probability\nof drawing a 4 is 1/10. The full empirical distribution can be put in a probability table\nvalue x\np(x)\n2/10\n1/10\n4/10\n1/10\n2/10\nNotation. If we label the true distribution the data is drawn from as F, then we'll label\n\n18.05 Class 24, Bootstrap confidence intervals, Spring 2022\nthe empirical distribution of the data as F∗. If we have enough data then the law of large\nnumbers tells us that F∗should be a good approximation of F.\nExample 4. In the dice example just above, the true and empirical distributions are:\nvalue x\ntrue p(x)\n1/8\n1/8\n1/8\n1/8\n1/8\n1/8\n1/8\n1/8\nempirical p(x)\n2/10\n1/10\n4/10\n1/10\n2/10\nThe true distribution Fand the empirical distribution F∗of the 8-sided die.\nBecause F∗is derived strictly from data we call it the empirical distribution of the data.\nWe will also call it the resampling distribution. Notice that we always know F∗explicitly.\nIn particular the expected value of F∗is just the sample mean x.\nResampling\nThe empirical (or nonparametric) bootstrap proceeds by resampling from the data. We\ncontinue the dice example above.\nExample 5. Suppose we have 10 data points, given in increasing order:\n1, 1, 2, 3, 3, 3, 3, 4, 7, 7\nWe view this as a sample taken from some underlying distribution. To resample is to sample\nwith replacement from the empirical distribution, e.g. put these 10 numbers in a hat and\ndraw one at random. Then put the number back in the hat and draw again. You draw as\nmany numbers as the desired size of the resample.\nTo get us a little closer to implementing this on a computer we rephrase this in the following\nway. Label the 10 data points x1, x2, ... , x10. To resample is to draw a number jfrom the\nuniform distribution on {1, 2, ... , 10} and take xjas our resampled value. In this case we\ncould do so by rolling a 10-sided die. For example, if we roll a 6 then our resampled value\nis x6 = 3, the 6th element in our list.\nIf we want a resampled data set of size 5, then we roll the 10-sided die 5 times and choose\nthe corresponding elements from the list of data. If the 5 rolls are\n5, 3, 6, 6, 1\nthen the resample is\n3, 2, 3, 3, 1.\nNotes: 1. Because we are sampling with replacement, the same data point can appear\nmultiple times when we resample.\n2. Also because we are sampling with replacement, we can have a resample data set of any\nsize we want, e.g. we could resample 1000 times.\nOf course, in practice one uses a software package like R to do the resampling.\n\n18.05 Class 24, Bootstrap confidence intervals, Spring 2022\n5.1\nStar notation\nIf we have sample data of size n\nx1, x2, ... , xn\nthen we denote a resample of size mby adding a star to the symbols\nx∗\n1, x∗\n2, ... , x∗\nm\nSimilarly, just as xis the mean of the original data, we write x∗for the mean of the resampled\ndata.\nThe empirical bootstrap\nSuppose we have ndata points\nx1, x2, ... , xn\ndrawn from a distribution F. An empirical bootstrap sample (or nonparametric bootstrap\nsample) is a resample of the same size n:\nx∗\n1, x∗\n2, ... , x∗\nn.\nYou should think of the latter as a sample of size ndrawn from the empirical distribution\nF∗. For any statistic vcomputed from the original sample data, we can define a bootstrap\nstatistic v∗. They both use the same formula but v∗is computed using the resampled data.\nWith this notation we can state the bootstrap principle.\n6.1\nThe bootstrap principle\nThe bootstrap setup is as follows:\n1. x1, x2, ... , xnis a data sample drawn from a distribution F.\n2. uis a statistic computed from the sample.\n3. F∗is the empirical distribution of the data (the resampling distribution).\n4. x∗\n1, x∗\n2, ... , x∗\nnis a resample of the data of the same size as the original sample\n5. u∗is the statistic computed from the resample.\nThen the bootstrap principle says that\n1. F∗is approximately equal to F.\n2. The statistic uis approximated by u∗.\n3. The variation of uis approximated by the variation of u∗.\nOur real interest is in point 3: we can approximate the variation of uby that of u∗. It turns\nout that, in practice, the bootstrap gives a reasonable approximation of the variation. We\nwill exploit this to estimate the size of confidence intervals.\n6.2\nWhy the resample is the same size as the original sample\nThis is straightforward: the variation of the statistic uwill depend on the size of the sample.\nIf we want to approximate this variation we need to use resamples of the same size.\n\n18.05 Class 24, Bootstrap confidence intervals, Spring 2022\nThe empirical bootstrap confidence interval\nHere we will show two methods of computing an empirical confidence interval: the percentile\nbootstrap confidence interval in part (c) below and the basic bootstrap confidence interval\nin part (d).\nA search of the internet credits these names to Efron. Below, we will briefly discuss the\nmerits of each. The basic confidence interval is also called the reverse percentile confidence\ninterval.\nWe will illustrate both methods with a 'toy' example using the same data.\nExample 6. Toy example. We start with a made-up set of data that is small enough to\nshow each step explicitly. The sample data is\n30, 37, 36, 43, 42, 43, 43, 46, 41, 42\n(a) Use the data to give a point estimate of the mean of the underlying distribution.\n(b) Resample this to get 20 bootstrap samples.\n(c) Use the bootstrap samples to find the 80% bootstrap percentile confidence interval for\nthe mean.\n(d) Use the bootstrap samples to find the 80% bootstrap basic confidence interval for the\nmean.\nNote: R code for this example is shown in the section 'R annotated scripts' below. The\ncode is also implemented in the R script, class24-empiricalbootstrap.r which is posted\nwith our other R code.\nSolution: (a) The sample mean is x= 40.3, this is our point estimate.\n(b) Our original sample contains 10 points. We used R to generate 20 bootstrap samples,\neach of size 10. Each of the 20 columns in the following array is one bootstrap sample. The\nvalues under the line are the means of the columns\n39.9 42.4 39.4 41.9 42.3 39.2 40.1 42.5 41.2 41.0 40.7 40.8 40.9 40.2 40.7 39.6 41.0 42.6 41.2 41.4\n(c) For the percentile method, we first compute x∗from each of our bootstrap samples.\nHere they are, sorted in increasing order.\n39.2 39.4 39.6 39.9 40.1 40.2 40.7 40.7 40.8 40.9 41.0 41.0 41.2 41.2 41.4 41.9\n42.3 42.4 42.5 42.6\nThe percentile method says to use the distribution of x∗as an approximation to the dis-\ntribution of x. The 80% confidence interval stretches from the 10th to the 90th percentile.\n\n18.05 Class 24, Bootstrap confidence intervals, Spring 2022\nSince we have 20 bootstrap means, these are given by the 2nd and 18th elements in our list.\nTherefore the boostrap 80% percentile confidence interval is [39.4, 42.4].\nTo make the example readable, we only computed 20 bootstrap means. The beautiful key\nto the bootstrap is, that since x∗is computed by resampling the original data, we can have\na computer simulate x∗as many times as we'd like. Hence, by the law of large numbers, we\ncan estimate the distribution of x∗with high precision.\nNote: In our R code we use the quantile function to find the percentile values. This is easier\nthan figuring out the index. It also is a little more sophisticated in finding the quantiles for\na discrete set of values.\n(d) The basic method is quite similar to the percentile method. The difference is that uses\nan alegebraic 'pivot' analogous to the pivot used in finding zor tconfidence intervals. As\nin Example 1, to make the confidence interval we want to know how much the distribution\nof xvaries around μ. That is, we'd like to know the distribution of\nδ= x-μ.\nIf we knew this distribution, then we could use the same algebra we saw when pivoting from\nnon-rejection regions to confidence intervals. That is, we could find δ0.1 and δ0.9, the 0.1\nand 0.9 critical values of δ. Then we would have\nP(δ0.9 ≤x-μ≤δ0.1 | μ) = 0.8 ⇔P(x-δ0.9 ≥μ≥x-δ0.1 | μ) = 0.8\nwhich gives an 80% confidence interval of\n[x-δ0.1, x-δ0.9] .\n(As always with confidence intervals, we hasten to point out that the probabilities computed\nabove are probabilities concerning the statistic xgiven that the true mean is μ.)\nThe bootstrap principle offers a practical approach to estimating the distribution of δ=\nx-μ. It says that we can approximate it by the distribution of\nδ∗= x∗-x\nwhere x∗is the mean of an empirical bootstrap sample.\nHere is the beautiful key: since δ∗is computed by resampling the original data, we can have\na computer simulate δ∗as many times as we'd like. Hence, by the law of large numbers, we\ncan estimate the distribution of δ∗with high precision.\nNext we compute δ∗= x∗-xfor each bootstrap sample (i.e. each column) and sort them\nfrom smallest to biggest:\n-1.1 -0.9 -0.7 -0.4 -0.2 -0.1 0.4 0.4 0.5 0.6 0.7 0.7 0.9 0.9 1.1 1.6 2.0 2.1 2.2 2.3\nWe will approximate the critical values δ0.1 and δ0.9 by δ∗\n0.1 and δ∗\n0.9. Since δ∗\n0.1 is at the\n90th percentile we choose the 18th element in the list, i.e. 2.1. Likewise, since δ∗\n0.9 is at the\n10th percentile we choose the 2nd element in the list, i.e. -0.9.\nTherefore our bootstrap 80% basic confidence interval for μis\n[x-δ∗\n0.1, x-δ∗\n0.9] = [40.3 -2.1, 40.3 + 0.9] = [38.2, 41.2]\nIn this example we only generated 20 bootstrap samples so they would fit on the page.\nUsing R, we would generate 10000 or more bootstrap samples in order to obtain a very\naccurate estimate of δ∗\n0.1 and δ∗\n0.9.\n\n18.05 Class 24, Bootstrap confidence intervals, Spring 2022\nJustification for the bootstrap principle\nThe bootstrap is remarkable because resampling gives us a decent estimate on how the\npoint estimate might vary. We can only give you a 'hand-waving' explanation of this, but\nit's worth a try. The bootstrap is based roughly on the law of large numbers, which says,\nin short, that with enough data the empirical distribution will be a good approximation of\nthe true distribution. Visually it says that the histogram of the data should approximate\nthe density of the true distribution.\nFirst let's note what resampling cannot do for us: it cannot improve our point estimate.\nFor example, if we estimate the mean μby xthen in the bootstrap we would compute x∗\nfor many resamples of the data. If we took the average of all the x∗we would expect it to\nbe very close to x. This wouldn't tell us anything new about the true value of μ.\nEven with a fair amount of data the match between the true and empirical distributions is\nnot perfect, so there will be error in our estimates for the mean (or any other value). But\nthe amount of variation in the estimates is much less sensitive to differences between the\ntrue density and the data histogram: as long as they are reasonably close, the empirical and\ntrue distributions will exhibit the similar amounts of variation. So, in general the bootstrap\nprinciple is more robust when approximating the distribution of relative variation than\nwhen approximating absolute distributions.\nWhat we have in mind is the scenario of our examples. The distribution (over different sets\nxof experimental data) of xis 'centered' at μand the distribution (over different bootstrap\nsamples x∗of x) of x∗is centered at x. If there is a significant separation between xand μ\nthen these two distributions will also differ significantly. On the other hand the distribution\nof δ= x-μdescribes the variation of xabout its center. Likewise the distribution of\nδ∗= x∗-xdescribes the variation of x∗about its center. So even if the centers are quite\ndifferent the two variations about the centers can be approximately equal.\nThe figure below illustrates how the empirical distribution approximates the true distribu-\ntion. To make the figure we generate 100 random values from a chi-square distribution with\n3 degrees of freedom. The figure shows the pdf of the true distribution as a blue line and a\nhistogram of the empirical distribution in orange.\n0.00\n0.10\n0.20\nThe true and empirical distributions are approximately equal.\n\n18.05 Class 24, Bootstrap confidence intervals, Spring 2022\nOther statistics\nSo far in this class we've avoided confidence intervals for the median and other statistics\nbecause their sample distributions are hard to describe theoretically. The bootstrap has no\nsuch problem. In fact, to handle the median all we have to do is change 'mean' to 'median'\nin the R code from Example 6.\nExample 7. Old Faithful: confidence intervals for the median\nOld Faithful is a geyser in Yellowstone National Park in Wyoming:\nhttps://en.wikipedia.org/wiki/Old_Faithful\nThere is a publicly available data set which gives the durations of 272 consecutive eruptions.\nHere is a histogram of the data.\nQuestion: Estimate the median length of an eruption and give a 90% percentile bootstrap\nconfidence interval for the median.\nSolution: The full answer to this question is in the R file oldfaithful.r and the Old\nFaithful data set. Both are posted on the class R code page. (Look under 'Other R code'\nfor the old faithful script and data.)\nNote: the code in oldfaithful.r assumes that the data oldfaithful-data.txt is in the\ncurrent working directory.\nLet's walk through a summary of the steps needed to answer the question.\n1. Data: x1, ... , x272\n2. Data median: xmedian = 240\n3. Find the median x∗\nmedian of a bootstrap sample x∗\n1, ... , x∗\n272. Repeat 1000 times.\nPut these 1000 values in order and pick out the 0.05 and 0.95 quantiles , i.e. the 50th and\n950th biggest values. (In R we do this using the quantile() function) Call these m∗\n0.05 and\nm∗\n0.95.\n5. The 90% percentile confidence interval for the medium is [m∗\n0.05, m∗\n0.95].\nThe bootstrap 90% CI we found for the Old Faithful data was [230, 246]. Since we used 1000\nbootstrap samples a new simulation starting from the same sample data should produce a\nsimilar interval. If in Step 3 we increase the number of bootstrap samples to 10000, then\n\n18.05 Class 24, Bootstrap confidence intervals, Spring 2022\nthe intervals produced by simulation would tend to be even more similar to each other.\nOne common strategy is to increase the number of bootstrap samples until the resulting\nsimulations produce intervals that vary less than some acceptable level.\nExample 8. Using the Old Faithful data, estimate P(|x-μ| > 5 | μ).\nSolution: We proceed exactly as in the previous example in finding a basic bootstrap\nconfidence interval, except we use the mean instead of the median.\n1. Data: x1, ... , x272\n2. Data mean: x= 209.27\n3. Find the mean x∗of 1000 empirical bootstrap samples: x∗\n1, ... , x∗\n272.\n4. Compute the bootstrap differences\nδ∗= x∗-x\n5. The bootstrap principle says that we can use the distribution of δ∗as an approximation\nfor the distribution δ= x-μ. Thus,\nP(|x-μ| > 5 | μ) = P(|δ| > 5 | μ) ≈P(|δ∗| > 5)\nOne bootstrap simulation for the Old Faithful data gave 0.230 for this probability.\nParametric bootstrap\nBefore getting started, we note that we only show the simplest algorithm for parametric\nbootstrap confidence intervals. In practice, more sophisticated algorithms are used. Since\nthe bootsrap principle is the same in all cases, we feel it is worth encountering the simple\nalgorithm in 18.05.\nThe examples in the previous sections all used the empirical bootstrap, which makes no as-\nsumptions at all about the underlying distribution and draws bootstrap samples by resam-\npling the data. In this section we will look at the parametric bootstrap. The main difference\nbetween the parametric and empirical bootstrap is the source of the bootstrap sample. For\nthe parametric bootstrap, we generate the bootstrap sample from a parametrized distribu-\ntion.\nAnother difference, is that, for the parametric bootstrap, we will use basic bootstrap con-\nfidence intervals. It's hard to find a definitive answer on whether the percentile or basic\ninterval is better in this case. As we noted above, the real answer seems to be that, in\npractice, parametric bootstrap confidence intervals are computed with more sophisticated\nalgorithms.\nHere are the elements of using the parametric bootstrap to estimate a confidence interval\nfor a parameter.\n0. Data: x1, ... , xndrawn from a distribution F(θ) with unknown parameter θ.\n1. A statistic\nθthat estimates θ.\n2. Our bootstrap samples are drawn from F( θ).\n\n18.05 Class 24, Bootstrap confidence intervals, Spring 2022\n3. For each bootstrap sample\nx∗\n1, ... , x∗\nn\nwe compute\nθ∗and the bootstrap difference δ∗=\nθ∗-θ.\n4. The bootstrap principle says that the distribution of δ∗approximates the distribution of\nδ=\nθ-θ.\n5. Use the bootstrap differences to make a bootstrap confidence interval for θ.\nExample 9. Suppose the data x1, ... , x300 is drawn from an exp(λ) distribution. Assume\nalso that the data mean x= 2. Estimate λand give a 95% parametric bootstrap confidence\ninterval for λ.\nSolution: This is implemented in the R script class24-parametricbootstrap.r which is\nposted with our other R code.\nIt's will be easiest to explain the solution using commented code.\n# Parametric bootstrap\n# Given 300 data points with mean 2.\n# Assume the data is exp(lambda)\n# PROBLEM: Compute a 95% parametric bootstrap confidence interval for lambda\n# We are given the number of data points and mean\nn = 300\nxbar = 2\n# The MLE for lambda is 1/xbar\nlambda_hat = 1.0/xbar\n# Generate the bootstrap samples\n# Each column is one bootstrap sample (of 300 resampled values)\nn_boot = 1000\n# Here's the key difference with the empirical bootstrap:\n# We draw the bootstrap sample from Exponential(lambda_hat).\nx = rexp(n*n_boot, lambda_hat)\nbootstrap_sample = matrix(x, nrow=n, ncol=n_boot)\n# Compute the bootstrap lambda_star\nlambda_star = 1.0/colMeans(bootstrap_sample)\n# Compute the differences\ndelta_star = lambda_star - lambda_hat\n# Find the 0.05 and 0.95 quantile for delta_star\nd = quantile(delta_star, c(0.05,0.95))\n# Calculate the 95% confidence interval for lambda.\nci = lambda_hat - c(d[2], d[1])\n# This line of code is just one way to format the output text.\n# sprintf is an old C function for doing this. R has many other\n# ways to do the same thing.\ns = sprintf(\"Confidence interval for lambda: [%.3f, %.3f]\", ci[1], ci[2])\n\n18.05 Class 24, Bootstrap confidence intervals, Spring 2022\ncat(s)\nBuilding a better bootstrap\nThe first thing to say, is that we have just scratched the surface of bootstrap techniques.\nThere are more sophisticated methods that correct for bias in the original sample or for\nskewness in the underlying distribution.\nThere are also methods for when the original\nsample size is small.\nFor the nonparametric bootstrap, there are different opinions about which of the basic and\npercentile methods gives the most accurate results. What is clear is that, if the empirical\ndistribution is symmetric, then the basic and percentile confidence intervals are equivalent.\nIf the distribution is skewed then the two intervals are skewed in opposite directions.\nHesterberg in (https://www.tandfonline.com/doi/full/10.1080/00031305.2015.1089789)\nis fairly convincing that the percentile method performs better on skewed distributions.\nOn the other hand, Rice says of the percentile method, \"Although this direct equation of\nquantiles of the bootstrap sampling distribution with confidence limits may seem initially\nappealing, it's rationale is somewhat obscure.\" 2\nWe'll give Hesterberg the final word. He contributed several excellent responses to a dis-\ncussion group. Unfortunately the link is no longer active, but here is one of his posts.\nSkewness is a fact of life. So what do you do when there is skewness? A\nbootstrap percentile interval is usually a good choice - much better than a\nsymmetric interval like t, that pretends there is no skewness. It is fine for a\nbeginning stats student, and for most applications in practice. Someone more\nadvanced (or using easy-to-use software) can use a BCa or bootstrap-t interval,\nfor better accuracy.\nBootstrapping the median is a different issue - for small samples the median\nis quite sensitive to whether the population being sampled from is continuous\nor discrete. So if the population is continuous, but you use the nonparametric\nbootstrap that draws from a discrete distribution (the data), the bootstrap dis-\ntribution won't look like the true sampling distribution. Even so, the bootstrap\npercentile interval is not bad in this case, close to the exact confidence interval\nfor the median (typically the same or one order statistic different).\nR annotated transcripts\n12.1\nUsing R to generate a empirical bootstrap confidence intervals\nThis code only generates 20 bootstrap samples. In real practice we would generate many\nmore bootstrap samples. It is making a bootstrap confidence interval for the mean. This\ncode is implemented in the R script class24-empiricalbootstrap.r which is posted with\nour other R code.\n2John Rice, Mathematical Statistics and Data Analysis, 2nd edition, p. 272.\n\n18.05 Class 24, Bootstrap confidence intervals, Spring 2022\n# Data for the example 6\nx = c(30,37,36,43,42,43,43,46,41,42)\nn = length(x)\n# sample mean\nxbar = mean(x)\nn_boot = 20\n# Generate 20 bootstrap samples, i.e. an n x 20 array of\n# random resamples from x\ntmp_data = sample(x, n*n_boot, replace=TRUE)\nbootstrap_sample = matrix(tmp_data, nrow=n, ncol=n_boot)\n# Compute the means x∗\nxbar_star = colMeans(bootstrap_sample)\n# Calculate the bounds for the 80% percentile confidence interval.\npercentile_ci = quantile(xbar_star, c(0.1, 0.9))\ncat('80% percentile confidence interval: ',percentile_ci, '\\n')\n# Compute δ∗for each bootstrap sample\ndelta_star = xbar_star - xbar\n# Find the 0.1 and 0.9 quantiles for delta_star\nd = quantile(delta_star, c(0.1, 0.9))\n# Calculate the bounds for the 80% basic confidence interval.\n# Note how pivoting reverses the order of d[1] and d[2]\nbasic_ci = xbar - c(d[2], d[1])\ncat('80% basic confidence interval: ',basic_ci, '\\n')\n# ALTERNATIVE: the quantile() function is sophisticated about\n# choosing a quantile between two data points. A less sophisticated\n# approach is to pick the quantiles by sorting xbar_start and delta_star and\n# choosing the index that corresponds to the desired quantiles.\n# This is what we did in the text above. We show the code using this for the\npercentile method below.\n# Sort the results\nsorted_xbar_star = sort(xbar_star)\n# Find the 0.1 and 0.9 quantiles values of xbar_star\nq1_alt = sorted_xbar_star[2]\nq9_alt = sorted_xbar_star[18]\n# Find and print the 80% percentile confidence interval for the mean\nci_alt = c(q1_alt, q9_alt)\ncat('Alternative confidence interval: ',ci_alt, '\\n')\n\nLinear regression\nClass 26, 18.05\nJeremy Orloff and Jonathan Bloom\nLearning Goals\n1. Be able to use the method of least squares to fit a line to bivariate data.\n2. Be able to give a formula for the total squared error when fitting any type of curve to\ndata.\n3. Be able to say the words homoscedasticity and heteroscedasticity.\nIntroduction\nSuppose we have collected bivariate data (xi, yi), i= 1, ... , n. The goal of linear regression\nis to model the relationship between xand yby finding a function y= f(x) that is a close\nfit to the data. The modeling assumptions we will use are that xiis not random and that\nyiis a function of xiplus some random noise.\nWith these assumptions xis called the\nindependent or predictor variable and yis called the dependent or response variable.\nHere is a series of examples showing the results of linear regression. We will discuss the\ndetails of how to do linear regression in the next section.\nExample 1. The cost of a first class stamp in cents over time is given in the following list.\n0.05 (1963)\n0.06 (1968)\n0.08 (1971)\n0.10 (1974)\n0.13 (1975)\n0.15 (1978)\n0.20 (1981)\n0.22 (1985)\n0.25 (1988)\n0.29 (1991)\n0.32 (1995)\n0.33 (1999)\n0.34 (2001)\n0.37 (2002)\n0.39 (2006)\n0.41 (2007)\n0.42 (2008)\n0.44 (2009)\n0.45 (2012)\n0.46 (2013)\n0.49 (2015)\n0.49 (2017)\n0.50 (2018)\n0.55 (2019)\nUsing the R function lm we found the 'least squares fit' for a line to this data is\ny= -0.21390 + 0.88203x,\nwhere xis the number of years since 1960 and yis in cents.\nUsing this result we 'predict' that in 2021 (x= 61) the cost of a stamp will be 53.6 cents\n(since -0.21390 + 0.88203 ⋅61 = 53.6).\n\n18.05 Class 26, Linear regression, Spring 2022\nx\ny\nStamp cost (cents) vs. time (years since 1960). Orange dot is predicted cost in 2021.\nNote that none of the data points actually lie on the line. Rather this line has the 'best fit'\nwith respect to all the data, with a small error for each data point.\n(Note, the actual cost of a stamp dropped in January 2021 was 55 cents.\nSee https:\n//en.wikipedia.org/wiki/History_of_United_States_postage_rates)\nExample 2. Suppose we have npairs of fathers and adult sons. Let xiand yibe the\nheights of the ith father and son, respectively. The least squares line for this data could be\nused to predict the adult height of a young boy from that of his father.\nExample 3. We are not limited to best fit lines. For all positive d, the method of least\nsquares may be used to find a polynomial of degree dwith the 'best fit' to the data. Here's\na figure showing the least squares fit of a parabola (d= 2).\nFitting a parabola, ax2 + bx+ c, to data\nExample 4. In fact, we can use linear regression to fit many other types of curves to\nbivariate data.\n\n18.05 Class 26, Linear regression, Spring 2022\nFitting a line using least squares\nSuppose we have data (xi, yi) as above. Our first goal is to find the line\ny= ax+ b\nthat 'best fits' the data. Our model says that each yiis predicted by xiup to some error εi:\nyi= axi+ b+ εi.\nSo\nεi= yi-axi-b.\nThe method of least squares finds the values\naand bof aand bthat minimize the sum of\nthe squared errors:\nS(a, b) = ∑ε2\ni= ∑\ni\n(yi-axi-b)2.\nUsing calculus or linear algebra (details in the appendix), we find\na= sxy\nsxx\nb=\ny-ax\n(1)\nwhere\nx= 1\nn∑xi,\ny= 1\nn∑yi,\nsxx=\n(n-1) ∑(xi-x)2,\nsxy=\n(n-1) ∑(xi-x)(yi-y).\nHere\nxis the sample mean of x,\nyis the sample mean of y, sxxis the sample variance of x,\nand sxyis the sample covariance of xand y.\nExample 5. Use least squares to fit a line to the following data: (0,1), (2,1), (3,4).\nSolution: In our case, (x1, y1) = (0, 1), (x2, y2) = (2, 1) and (x3, y3) = (3, 4). So\nx= 5\n3,\ny= 2,\nsxx= 7\n3,\nsxy= 2\nUsing the above formulas we get\na= 6\n7,\nb= 4\n7.\nSo the least squares line has equation y= 6\n7x+ 4\n7. This is shown as the orange line in the\nfollowing figure. We will discuss the blue parabola soon.\nx\ny\nLeast squares fit of a line (orange) and a parabola (blue)\n\n18.05 Class 26, Linear regression, Spring 2022\nSimple linear regression: It's a little confusing, but the word linear in 'linear regression'\ndoes not refer to fitting a line. We will explain its meaning below. However, the most\ncommon curve to fit is a line. When we fit a line to bivariate data it is called simple linear\nregression.\n3.1\nResiduals\nFor a line the model is\nyi=\nax+ b+ εi.\nWe think of\naxi+ bas predicting or explaining yi. The left-over term εiis called the residual,\nwhich we think of as random noise or measurement error. A useful visual check of the linear\nregression model is to plot the residuals. The data points should hover near the regression\nline. The residuals should look about the same across the range of x.\nGG\nGG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nGG\nGG\nG\nG\nGG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nGGG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nGGG\nG\nG\nGG\nG\nG\nGG\nGGG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nx\ny\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nGG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nGG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n-3\n-2\n-1\nx\ne\nData with regression line (left) and residuals (right). Note the homoscedasticity.\n3.2\nHomoscedasticity\nAn important assumption of the linear regression model is that the residuals εihave the\nsame variance for all i. This is called homoscedasticity. You can see this is the case for\nboth figures above. The data hovers in the band of fixed width around the regression line\nand at every xthe residuals have about the same vertical spread.\nBelow is a figure showing heteroscedastic data. The vertical spread of the data increases as\nxincreases. Before using least squares on this data we would need to transform the data\nto be homoscedastic.\n\n18.05 Class 26, Linear regression, Spring 2022\nGG\nG\nGGG\nGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nGG\nGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nGG\nG\nGGG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nGG\nGG\nGG\nGG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nGG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGG\nx\ny\nHeteroscedastic Data\nLinear regression for fitting polynomials\nWhen we fit a line to data it is called simple linear regression. We can also use linear\nregression to fit polynomials to data. The use of the word linear in both cases may seem\nconfusing. This is because the word 'linear' in linear regression does not refer to fitting a\nline. Rather it refers to the linear algebraic equations for the unknown parameters.\nExample 6. Take the same data as in Example 5 and use least squares to find the best\nfitting parabola to the data.\nSolution: A parabola has the formula y= ax2 + bx+ c. The squared error is\nS(a, b, c) = ∑(yi-(ax2\ni+ bxi+ c))2.\nAfter substituting the given values for each xiand yi, we can use calculus to find the triple\n(a, b, c) that minimizes S.\nWith this data, we find that the least squares parabola has\nequation\ny= x2 -2x+ 1.\nNote that for 3 points the quadratic fit is perfect.\nx\ny\nLeast squares fit of a line (orange) and a parabola (blue)\nExample 7. The pairs (xi, yi) may give the age and vocabulary size of a nchildren. Since\nwe expect that young children acquire new words at an accelerating pace, we might guess\nthat a higher order polynomial would best fit the data.\n\n18.05 Class 26, Linear regression, Spring 2022\nExample 8. (Transforming the data) Sometimes it is necessary to transform the data\nbefore using linear regression. For example, let's suppose the relationship is exponential,\ni.e. y= ceax. Then\nln(y) = ax+ ln(c).\nSo we can use simple linear regression on the data (xi, ln(yi)) to obtain a model\nln(y) =\nax+ b\nand then exponentiate to obtain the exponential model\ny= ebeax.\n4.1\nOverfitting\nYou can always achieve a better fit by using a higher order polynomial. For instance, given 6\ndata points (with distinct xi) one can always find a fifth order polynomial that goes through\nall of them. This can result in what's called overfitting. That is, fitting the noise as well\nas the true relationship between xand y. An overfit model will fit the original data better\nbut perform less well on predicting yfor new values of x. Indeed, a primary challenge of\nstatistical modeling is balancing model fit against model complexity.\nExample 9. In the plot below, we fit polynomials of degree 1, 2, and 9 to bivariate data\nconsisting of 10 data points. The degree 2 model (maroon) gives a significantly better fit\nthan the degree 1 model (blue). The degree 10 model (orange) gives fits the data exactly,\nbut at a glance we would guess it is overfit. That is, we don't expect it to do a good job\nfitting the next data point we see.\nIn fact, we generated this data using a quadratic model, so the degree 2 model will tend to\nperform best fitting new data points.\nx\ny\n4.2\nR function lm\nAs you would expect we don't actually do linear regression by hand. Computationally,\nlinear regression reduces to solving simultaneous equations, i.e. to matrix calculations. The\nR function lm can be used to fit any order polynomial to data. (lm stands for linear model).\nWe will explore this in the next studio class. In fact lm can fit many types of functions\nbesides polynomials, as you can explore using R help or google.\n\n18.05 Class 26, Linear regression, Spring 2022\nMultiple linear regression\nData is not always bivariate. It can be trivariate or even of some higher dimension. Suppose\nwe have data in the form of tuples\n(yi, x1,i, x2,i, ... xm,i)\nWe can analyze this in a manner very similar to linear regression on bivariate data. That\nis, we can use least squares to fit the model\ny= β0 + β1x1 + β2x2 + ... + βmxm.\nHere each xjis a predictor variable and yis the response variable. For example, we might\nbe interested in how a fish population varies with measured levels of several pollutants, or\nwe might want to predict the adult height of a son based on the height of the mother and\nthe height of the father.\nWe don't have time in 18.05 to study multiple linear regression, but we wanted you to see\nthe name.\nLeast squares as a statistical model\nThe linear regression model for fitting a line says that the value yiin the pair (xi, yi) is\ndrawn from a random variable\nYi= axi+ b+ εi\nwhere the 'error' terms εiare independent random variables with mean 0 and standard\ndeviation σ. The standard assumption is that the εiare i.i.d. with distribution N(0, σ2).\nSo, the mean of Yiis given by:\nE[Yi] = axi+ b+ E[εi] = axi+ b.\nFrom this perspective, the least squares method chooses the values of aand bwhich minimize\nthe sample variance about the line.\nIn fact, under the assumption that εi∼N(0, σ2), the least square estimate ( a, b) coincides\nwith the maximum likelihood estimate for the parameters (a, b); that is, among all possible\ncoefficients, ( a, b) are the ones that make the observed data most probable.\nRegression to the mean\nThe reason for the term 'regression' is that the predicted response variable ywill tend to\nbe 'closer' to (i.e., regress to) its mean than the predictor variable xis to its mean. Here\ncloser is in quotes because we have to control for the scale (i.e. standard deviation) of each\nvariable. The way we control for scale is to first standardize each variable.\nui= xi-\nx\n√sxx\n,\nvi= yi-y\n√syy\n.\n\n18.05 Class 26, Linear regression, Spring 2022\nStandardization changes the mean to 0 and variance to 1:\nu=\nv= 0,\nsuu= svv= 1.\nThe algebraic properties of covariance show\nsuv=\nsxy\n√sxxsyy\n= ρ,\nthe correlation coefficient. Thus the least squares fit to v= au+ bhas\na= suv\nsuu\n= ρ\nand\nb=\nv-au= 0.\nSo the least squares line is v= ρu. Since ρis the correlation coefficient, it is between -1 and\n1. Let's assume it is positive and less than 1 (i.e., xand yare positively but not perfectly\ncorrelated). Then the formula v= ρumeans that if uis positive then the predicted value\nof vis less than u. That is, vis closer to 0 than u. Equivalently,\ny-y\n√syy\n< x-\nx\n√sxx\ni.e., yregresses to\ny. Notice how the standardization takes care of controlling the scale.\nConsider the extreme case of 0 correlation between xand y. Then, no matter what the x\nvalue, the predicted value of yis always\ny. That is, yhas regressed all the way to its mean.\nNote also that the regression line always goes through the point ( x, y).\nExample 10. Regression to the mean is important in longitudinal studies. Rice (Math-\nematical Statistics and Data Analysis) gives the following example. Suppose children are\ngiven an IQ test at age 4 and another at age 5 we expect the results will be positively\ncorrelated. The above analysis says that, on average, those kids who do poorly on the first\ntest will tend to show improvement (i.e. regress to the mean) on the second test. Thus, a\nuseless intervention might be misinterpreted as useful since it seems to improve scores.\nExample 11. Another example with practical consequences is reward and punishment.\nImagine a school where high performance on an exam is rewarded and low performance is\npunished. Regression to the mean tells us that (on average) the high performing students\nwill do slightly worse on the next exam and the low performing students will do slightly\nbetter. An unsophisticated view of the data will make it seem that punishment improved\nperformance and reward actually hurt performance. There are real consequences if those in\nauthority act on this idea.\nAppendix\nWe collect in this appendix a few things you might find interesting. You will not be asked\nto know these things for exams.\n\n18.05 Class 26, Linear regression, Spring 2022\n8.1\nProof of the formula for least square fit of a line\nThe most straightforward proof is to use calculus. The sum of the squared errors is\nS(b, a) =\nn\n∑\ni=1\n(yi-axi-b)2.\nTaking partial derivatives (and remembering that xiand yiare the data, hence constant)\n∂S\n∂b=\nn\n∑\ni=1\n-2(yi-axi-b) = 0\n∂S\n∂a=\nn\n∑\ni=1\n-2xi(yi-axi-b) = 0\nSumming this up we get two linear equations in the unknowns band a:\n(∑xi) a+ nb= ∑yi\n(∑x2\ni) a+ (∑xi) b= ∑xiyi\nSolving for aand bgives the formulas in Equation (1).\nA sneakier approach which avoids calculus is to standardize the data, find the best fit line,\nand then unstandardize. We omit the details.\nFor a slew of applications across disciplines see:\nhttps://en.wikipedia.org/wiki/Linear_regression#Applications_of_linear_regression\n8.2\nMeasuring the fit\nOnce one computes the regression coefficients, it is important to check how well the regres-\nsion model fits the data (i.e., how closely the best fit line tracks the data). A common but\ncrude 'goodness of fit' measure is the coefficient of determination, denoted R2. We'll need\nsome notation to define it. The total sum of squares is given by:\nTSS = ∑(yi-y)2.\nThe residual sum of squares is given by the sum of the squares of the residuals. When\nfitting a line, this is:\nRSS = ∑(yi-axi-b)2.\nThe RSS is the \"unexplained\" portion of the total sum of squares, i.e. unexplained by the\nregression equation. The difference TSS -RSS is the \"explained\" portion of the total sum\nof squares. The coefficient of determination R2 is the ratio of the \"explained\" portion to\nthe total sum of squares:\nR2 = TSS -RSS\nTSS\n.\nIn other words, R2 measures the proportion of the variability of the data that is accounted\nfor by the regression model. A value close to 1 indicates a good fit, while a value close to 0\n\n18.05 Class 26, Linear regression, Spring 2022\nindicates a poor fit. In the case of simple linear regression, R2 is simply the square of the\ncorrelation coefficient between the observed values yiand the predicted values axi+ b.\nExample 12. In the overfitting example (9), the values of R2 are:\ndegree\nR2\n0.3968\n0.9455\n1.0000\nNotice the goodness of fit measure increases as nincreases.\nThe fit is better, but the\nmodel also becomes more complex, since it takes more coefficients to describe higher order\npolynomials.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Reading 1a: Introduction",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_class01-prep-a.pdf",
      "content": "Introduction\nClass 1, 18.05\nJeremy Orloff and Jonathan Bloom\n1 Probability vs. Statistics\nIn this introduction we will preview what we will be studying in 18.05. Don't worry if many\nof the terms are unfamiliar, they will be explained as the course proceeds.\nProbability and statistics are deeply connected because all statistical statements are at bot\ntom statements about probability. Despite this the two sometimes feel like very different\nsubjects. Probability is logically self-contained; there are a few rules and answers all follow\nlogically from the rules, though computations can be tricky. In statistics we apply proba\nbility to draw conclusions from data. This can be messy and usually involves as much art\nas science.\nProbability example\nYou have a fair coin (equal probability of heads or tails). You will toss it 100 times. What\nis the probability of 60 or more heads? There is only one answer (about 0.028444) and we\nwill learn how to compute it.\nStatistics example\nYou have a coin of unknown provenance. To investigate whether it is fair you toss it 100\ntimes and count the number of heads. Let's say you count 60 heads. Your job as a statis\ntician is to draw a conclusion (inference) from this data. There are many ways to proceed,\nboth in terms of the form the conclusion takes and the probability computations used to\njustify the conclusion. In fact, different statisticians might draw different conclusions.\nNote that in the first example the random process is fully known (probability of heads =\n0.5). The objective is to find the probability of a certain outcome (at least 60 heads) arising\nfrom the random process. In the second example, the outcome is known (60 heads) and the\nobjective is to illuminate the unknown random process (the probability of heads).\n2 Frequentist vs. Bayesian Interpretations\nThere are two prominent and sometimes conflicting schools of statistics: Bayesian and\nfrequentist. Their approaches are rooted in differing interpretations of the meaning of\nprobability.\nFrequentists say that probability measures the frequency of various outcomes of an ex\nperiment. For example, saying a fair coin has a 50% probability of heads means that if we\ntoss it many times then we expect about half the tosses to land heads.\nBayesians say that probability is an abstract concept that measures a state of knowledge\nor a degree of belief in a given proposition. In practice Bayesians do not assign a single\nvalue for the probability of a coin coming up heads. Rather they consider a range of values\neach with its own probability of being true.\nIn 18.05 we will study and compare these approaches. The frequentist approach has long\n\n18.05 Class 1, Introduction, Spring 2022\nbeen dominant in fields like biology, medicine, public health and social sciences. The\nBayesian approach has enjoyed a resurgence in the era of powerful computers and big\ndata. It is especially useful when incorporating new data into an existing statistical model,\nfor example, when training a speech or face recognition system. Today, statisticians are\ncreating powerful tools by using both approaches in complementary ways.\n3 Applications, Toy Models, and Simulation\nProbability and statistics are used widely in the physical sciences, engineering, medicine, the\nsocial sciences, the life sciences, economics and computer science. The list of applications is\nessentially endless: tests of one medical treatment against another (or a placebo), measures\nof genetic linkage, the search for elementary particles, machine learning for vision or speech,\ngambling probabilities and strategies, climate modeling, economic forecasting, epidemiology,\nmarketing, googling...We will draw on examples from many of these fields during this course.\nGiven so many exciting applications, you may wonder why we will spend so much time\nthinking about toy models like coins and dice. By understanding these thoroughly we will\ndevelop a good feel for the simple essence inside many complex real-world problems. In\nfact, the modest coin is a realistic model for any situations with two possible outcomes:\nsuccess or failure of a treatment, an airplane engine, a bet, or even a class.\nSometimes a problem is so complicated that the best way to understand it is through\ncomputer simulation. Here we use software to run virtual experiments many times in order\nto estimate probabilities. In this class we will use R for simulation as well as computation\nand visualization. Don't worry if you're new to R; we will teach you all you need to know.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Reading 1b: Counting and Sets",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_class01-prep-b.pdf",
      "content": "Counting and Sets\nClass 1, 18.05\nJeremy Orloff and Jonathan Bloom\n1 Learning Goals\n1. Know the definitions and notation for sets, intersection, union, complement.\n2. Be able to visualize set operations using Venn diagrams.\n3. Understand how counting is used computing probabilities.\n4. Be able to use the rule of product, inclusion-exclusion principle, permutations and com\nbinations to count the elements in a set.\n2 Counting\n2.1 Motivating questions\nExample 1. A coin is fair if it comes up heads or tails with equal probability. You flip a\nfair coin three times. What is the probability that exactly one of the flips results in a head?\nSolution: With three flips, we can easily list the eight possible outcomes:\n{T T T , T T H, T HT , T HH, HT T , HT H, HHT , HHH}\nThree of these outcomes have exactly one head:\n{T T H, T HT , HT T }\nSince all outcomes are equally probable, we have\nnumber of outcomes with 1 head\nP (1 head in 3 flips) =\n= 8.\ntotal number of outcomes\nThink: Would listing the outcomes be practical with 10 flips?\nA deck of 52 cards has 13 ranks (2, 3, ... , 9, 10, J, Q, K, A) and 4 suits (♡, ♠, ♢, ♣,). A\npoker hand consists of 5 cards. A one-pair hand consists of two cards having one rank and\nthree cards having three other ranks, e.g., {2♡, 2♠, 5♡, 8♣, K♢}\nTest your intuition: the probability of a one-pair hand is:\n(a) less than 5%\n(b) between 5% and 10%\n(c) between 10% and 20%\n(d) between 20% and 40%\n(e) greater than 40%\n\n18.05 Class 1, Counting and Sets, Spring 2022\nAt this point we can only guess the probability. One of our goals is to learn how to compute\nit exactly. To start, we note that since every set of five cards is equally probable, we can\ncompute the probability of a one-pair hand as\nnumber of one-pair hands\nP (one-pair) =\ntotal number of hands\nSo, to find the exact probability, we need to count the number of elements in each of these\nsets. And we have to be clever about it, because there are too many elements to simply\nlist them all. We will come back to this problem after we have learned some counting\ntechniques.\nSeveral times already we have noted that all the possible outcomes were equally probable\nand used this to find a probability by counting. Let's state this carefully in the following\nprinciple.\nPrinciple: Suppose there are n possible outcomes for an experiment and each is equally\nprobable. If there are k desirable outcomes then the probability of a desirable outcome is\nk/n. Of course we could replace the word desirable by any other descriptor: undesirable,\nfunny, interesting, remunerative, ...\nConcept question:\nCan you think of a scenario where the possible outcomes are not\nequally probable?\nHere's one scenario: on an exam you can get any score from 0 to 100. That's 101 different\npossible outcomes. Is the probability you get less than 50 equal to 50/101?\n2.2 Sets and notation\nOur goal is to learn techniques for counting the number of elements of a set, so we start\nwith a brief review of sets. (If this is new to you, please come to office hours).\n2.2.1 Definitions\nA set S is a collection of elements. We use the following notation.\nElement: We write x ∈ S to mean the element x is in the set S.\nSubset: We say the set A is a subset of S if all of its elements are in S. We write this as\nA ⊂ S.\nComplement:: The complement of A in S is the set of elements of S that are not in A.\nWe write this as Ac or S -A.\nUnion: The union of A and B is the set of all elements in A or B (or both). We write this\nas A ∪ B.\nIntersection: The intersection of A and B is the set of all elements in both A and B. We\nwrite this as A ∩ B.\nEmpty set: The empty set is the set with no elements. We denote it ∅.\nDisjoint: A and B are disjoint if they have no common elements. That is, if A∩B = ∅.\nDifference: The difference of A and B is the set of elements in A that are not in B. We\nwrite this as A - B.\n\n18.05 Class 1, Counting and Sets, Spring 2022\nLet's illustrate these operations with a simple example.\nExample 2. Start with a set of 10 animals\nS = {Antelope, Bee, Cat, Dog, Elephant, Frog, Gnat, Hyena, Iguana, Jaguar}.\nConsider two subsets:\nM = the animal is a mammal = {Antelope, Cat, Dog, Elephant, Hyena, Jaguar}\nW = the animal lives in the wild = {Antelope, Bee, Elephant, Frog, Gnat, Hyena, Iguana, Jaguar}.\nOur goal here is to look at different set operations.\nIntersection: M ∩W contains all wild mammals: M∩W = {Antelope, Elephant, Hyena, Jaguar}.\nUnion:\nM ∪W contains all animals that are mammals or wild (or both).\nM ∪W = {Antelope, Bee, Cat, Dog, Elephant, Frog, Gnat, Hyena, Iguana, Jaguar}.\nComplement:\nMc means everything that is not in M, i.e. not a mammal. Mc =\n{Bee, Frog, Gnat, Iguana}.\nDifference:\nM -W means everything that's in M and not in W . So, M-W =\n{Cat, Dog}.\nThere are often many ways to get the same set, e.g.\nMc = S-M, M-W = M ∩W c.\nThe relationship between union, intersection, and complement is given by DeMorgan's laws:\n(A ∪ B)c = Ac ∩ Bc\n(A ∩ B)c = Ac ∪ Bc\nIn words the first law says everything not in (A or B) is the same set as everything that's\n(not in A) and (not in B). The second law is similar.\n2.2.2 Venn Diagrams\nVenn diagrams offer an easy way to visualize set operations.\nIn all the figures S is the region inside the large rectangle, L is the region inside the left\ncircle and R is the region inside the right circle. The shaded region shows the set indicated\nunderneath each figure.\nL ∪ R\nL ∩ R\nLc\nL - R\nS\nL\nR\n\n18.05 Class 1, Counting and Sets, Spring 2022\nProof of DeMorgan's Laws\n(L ∪ R)c\nLc\nRc\nLc ∩ Rc\n(L ∩ R)c\nLc\nRc\nLc ∪ Rc\nExample 3. Verify DeMorgan's laws for the subsets A = {1, 2, 3} and B = {3, 4} of the\nset S = {1, 2, 3, 4, 5}.\nSolution: For each law we just work through both sides of the equation and show they are\nthe same.\n1. (A ∪ B)c = Ac ∩ Bc:\nRight hand side: A ∪ B = {1, 2, 3, 4} ⇒ (A ∪ B)c = {5}.\nLeft hand side: Ac = {4, 5}, Bc = {1, 2, 5} ⇒ Ac ∩ Bc = {5}.\nThe two sides are equal.\nQED\n2. (A ∩ B)c = Ac ∪ Bc:\nRight hand side: A ∩ B = {3} ⇒ (A ∩ B)c = {1, 2, 4, 5}.\nLeft hand side: Ac = {4, 5}, Bc = {1, 2, 5} ⇒ Ac ∪ Bc = {1, 2, 4, 5}.\nThe two sides are equal.\nQED\nThink: Draw and label a Venn diagram with A the set of Brain and Cognitive Science\nmajors and B the set of sophomores. Shade the region illustrating the first law. Can you\nexpress the first law in this case as a non-technical English sentence?\n2.2.3 Products of sets\nThe product of sets S and T is the set of ordered pairs:\nS × T = {(s, t) | s ∈ S, t ∈ T }.\nIn words the right-hand side reads \"the set of ordered pairs (s, t) such that s is in S and t\nis in T .\nThe following diagrams show two examples of the set product.\n\n18.05 Class 1, Counting and Sets, Spring 2022\n×\n(1,1)\n(1,2)\n(1,3)\n(1,4)\n(2,1)\n(2,2)\n(2,3)\n(2,4)\n(3,1)\n(3,2)\n(3,3)\n(3,4)\n{1, 2, 3} × {1, 2, 3, 4}\n[1, 4] × [1, 3] ⊂ [0, 5] × [0, 4]\nThe right-hand figure also illustrates that if A ⊂ S and B ⊂ T then A× B ⊂S× T .\n2.3 Counting\nIf S is finite, we use |S| or #S to denote the number of elements of S.\nTwo useful counting principles are the inclusion-exclusion principle and the rule of product.\n2.3.1 Inclusion-exclusion principle\nThe inclusion-exclusion principle says\n|A∪B| = |A| + |B| -|A∩B|.\nWe can illustrate this with a Venn diagram. S is all the dots, A is the dots in the blue\ncircle, and B is the dots in the red circle.\nS\nA\nB\nA∩B\n|A| is the number of dots in A and likewise for the other sets. The figure shows that |A|+|B|\ndouble-counts |A ∩ B|, which is why |A ∩ B| is subtracted off in the inclusion-exclusion\nformula.\nExample 4. In a band of singers and guitarists, seven people sing, four play the guitar,\nand two do both. How big is the band?\n\n18.05 Class 1, Counting and Sets, Spring 2022\nSolution: Let S be the set singers and G be the set guitar players. The inclusion-exclusion\nprinciple says\nsize of band = |S ∪ G| = |S| + |G| -|S ∩ G| = 7 + 4 -2 = 9.\n2.3.2 Rule of Product\nThe Rule of Product says:\nIf there are n ways to perform action 1 and then by m ways to perform action\n2, then there are n ⋅ m ways to perform action 1 followed by action 2.\nWe will also call this the multiplication rule.\nExample 5. If you have 3 shirts and 4 pants then you can make 3 ⋅4 = 12 outfits.\nThink: An extremely important point is that the rule of product holds even if the ways to\nperform action 2 depend on action 1, as long as the number of ways to perform action 2 is\nindependent of action 1. To illustrate this:\nExample 6. There are 5 competitors in the 100m final at the Olympics. In how many\nways can the gold, silver, and bronze medals be awarded?\nSolution: There are 5 ways to award the gold. Once that is awarded there are 4 ways to\naward the silver and then 3 ways to award the bronze: answer 5 ⋅4 ⋅3 = 60 ways.\nNote that the choice of gold medalist affects who can win the silver, but the number of\npossible silver medalists is always four.\n2.4 Permutations and combinations\n2.4.1 Permutations\nA permutation of a set is a particular ordering of its elements. For example, the set {a, b, c}\nhas six permutations: abc, acb, bac, bca, cab, cba. We found the number of permutations by\nlisting them all. We could also have found the number of permutations by using the rule\nof product. That is, there are 3 ways to pick the first element, then 2 ways for the second,\nand 1 for the third. This gives a total of 3 ⋅2 ⋅1 = 6 permutations.\nIn general, the rule of product tells us that the number of permutations of a set of k elements\nis\nk! = k⋅(k-1) ⋯3 ⋅2 ⋅1.\nWe also talk about the permutations of k things out of a set of n things. We show what\nthis means with an example.\nExample 7. List all the permutations of 3 elements out of the set {a, b, c, d}.\nSolution: This is a longer list,\nabc acb bac bca cab cba\nabd adb bad bda dab dba\nacd adc cad cda dac dca\nbcd bdc cbd cdb dbc dcb\n\n18.05 Class 1, Counting and Sets, Spring 2022\nNote that abc and acb count as distinct permutations. That is, for permutations the order\nmatters.\nThere are 24 permutations. Note that the rule of product would have told us there are\n4 ⋅3 ⋅2 = 24 permutations without bothering to list them all.\n2.4.2 Combinations\nIn contrast to permutations, in combinations order does not matter: permutations are lists\nand combinations are sets. We show what we mean with an example\nExample 8. List all the combinations of 3 elements out of the set {a, b, c, d}.\nSolution: Such a combination is a collection of 3 elements without regard to order. So, abc\nand cab both represent the same combination. We can list all the combinations by listing\nall the subsets of exactly 3 elements.\n{a, b, c} {a, b, d} {a, c, d} {b, c, d}\nThere are only 4 combinations. Contrast this with the 24 permutations in the previous\nexample. The factor of 6 comes because every combination of 3 things can be written in 6\ndifferent orders.\n2.4.3 Formulas\nWe'll use the following notations.\nnPk = number of permutations (lists) of k distinct elements from a set of size n\n= (n\nnCk\nk) = number of combinations (subsets) of k elements from a set of size n\nWe emphasize that by the number of combinations of k elements we mean the number of\nsubsets of size k.\nThese have the following notation and formulas:\nn!\nPermutations:\nnPk\n= (n-k)! = n(n-1) ⋯(n-k+ 1)\nn!\nCombinations:\nnCk\n= k!(n - k)! = n\nk\nP\n!\nk\nThe notation nCk is read \"n choose k\". The formula for nPk follows from the rule of product.\nIt also implies the formula for nCk because a subset of size k can be ordered in k! ways.\nWe can illustrate the relation between permutations and combinations by lining up the\nresults of the previous two examples.\nabc acb bac bca cab cba\n{a, b, c}\nabd adb bad bda dab dba\n{a, b, d}\nacd adc cad cda dac dca\n{a, c, d}\nbcd bdc cbd cdb dbc dcb\n{b, c, d}\nPermutations: 4P3\nCombinations: 4C3\nNotice that each row in the permutations list consists of all 3! permutations of the corre\nsponding set in the combinations list.\n\n18.05 Class 1, Counting and Sets, Spring 2022\n2.4.4 Examples\nExample 9. Count the following:\n(i) The number of ways to choose 2 out of 4 things (order does not matter).\n(ii) The number of ways to list 2 out of 4 things.\n(iii) The number of ways to choose 3 out of 10 things.\nSolution: (i) This is asking for combinations: (4\n4! = 6.\n2) = 2! 2!\n(ii) This is asking for permuations: 4P2 = 4!\n2! = 12.\n(iii) This is asking for combinations: (10\n10! = 10⋅9⋅8 = 120.\n3 ) = 3! 7!\n3⋅2⋅1\nExample 10. (i) Count the number of ways to get 3 heads in a sequence of 10 flips of a\ncoin.\n(ii) If the coin is fair, what is the probability of exactly 3 heads in 10 flips?\nSolution: (i) This asks for the number sequences of 10 flips (heads or tails) with exactly\n3 heads. That is, we have to choose exactly 3 out of 10 flips to be heads. This is the same\nquestion as in the previous example.\n(10\n10!\n10 ⋅9 ⋅8\n=\n= 120.\n3 ) = 3! 7!\n3 ⋅2 ⋅1\n(ii) Each flip has 2 possible outcomes (heads or tails). So the rule of product says there are\n210 = 1024 sequences of 10 flips. Since the coin is fair each sequence is equally probable.\nSo the probability of 3 heads is\n1024 = 0.117 .\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Reading 2: Probability: Terminology and Examples",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_class02-prep.pdf",
      "content": "Probability: Terminology and Examples\nClass 2, 18.05\nJeremy Orloff and Jonathan Bloom\n1 Learning Goals\n1. Know the definitions of sample space, event and probability function.\n2. Be able to organize a scenario with randomness into an experiment and sample space.\n3. Be able to make basic computations using a probability function.\n2 Terminology\n2.1 Probability cast list\n- Experiment: a repeatable procedure with well-defined possible outcomes.\n- Sample space: the set of all possible outcomes. We usually denote the sample space by\nΩ, sometimes by S.\n- Event: a subset of the sample space.\n- Probability function: a function giving the probability for each outcome.\nLater in the course we will learn about\n- Probability density: a continuous distribution of probabilities.\n- Random variable: a random numerical outcome.\n2.2 Simple examples\nExample 1. Toss a fair coin.\nExperiment: toss the coin, report if it lands heads or tails.\nSample space: Ω = {H, T}.\nProbability function: P(H) = 0.5, P(T) = 0.5.\nExample 2. Toss a fair coin 3 times.\nExperiment: toss the coin 3 times, list the results.\nSample space: Ω = {HHH, HHT, HTH, HTT, THH, THT, TTH, TTT}.\nProbability function: Each outcome is equally likely with probability 1/8.\nFor small sample spaces we can put the set of outcomes and probabilities into a probability\ntable.\nOutcomes\nHHH\nHHT\nHTH\nHTT\nTHH\nTHT\nTTH\nTTT\nProbability\n1/8\n1/8\n1/8\n1/8\n1/8\n1/8\n1/8\n1/8\n\n18.05 Class 2, Probability: Terminology and Examples, Spring 2022\nExample 3. Measure the mass of a proton\nExperiment: follow some defined procedure to measure the mass and report the result.\nSample space: Ω = [0, inf), i.e. in principle we can get any positive value.\nProbability function: since there is a continuum of possible outcomes there is no probability\nfunction. Instead we need to use a probability density, which we will learn about later in\nthe course.\nExample 4. Taxis (An infinite discrete sample space)\nExperiment: count the number of taxis that pass 77 Mass. Ave during an 18.05 class.\nSample space: Ω = {0, 1, 2, 3, 4, ...}.\nThis is often modeled with the following probability function known as the Poisson distri\nbution. (Do not worry about mastering the Poisson distribution just yet):\nP (k) = e-λ λ\nk\nk\n! ,\nwhere λ is the average number of taxis. We can put this in a table:\nOutcome\n...\nk\n...\nProbability\ne-λ\ne-λ λ\ne-λ λ2/2\ne-λ λ3/3!\n...\ne-λ λk/k!\n...\ninf\nQuestion: Accepting that this is a valid probability function, what is ∑ e-λ λ\nk\nk\n! ?\nk=0\nSolution:\nThis is the total probability of all possible outcomes, so the sum equals 1.\ninf λn\n(Note, this also follows from the Taylor series eλ = ∑\nn=0 n! .)\nIn a given setup there can be more than one reasonable choice of sample space. Here is a\nsimple example.\nExample 5. Two dice (Choice of sample space)\nSuppose you roll one die. Then the sample space and probability function are\nOutcome\nProbability:\n1/6\n1/6\n1/6\n1/6\n1/6\n1/6\nNow suppose you roll two dice. What should be the sample space? Here are two options.\n1. Record the pair of numbers showing on the dice (first die, second die).\n2. Record the sum of the numbers on the dice. In this case there are 11 outcomes\n{2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}. These outcomes are not all equally likely.\nAs above, we can put this information in tables. For the first case, the sample space is the\nproduct of the sample spaces for each die\n{(1, 1), (2, 1), (3, 1), ... (6, 6)}\nEach of the 36 outcomes is equally likely. (Why 36 outcomes?) For the probability function\nwe will make a two dimensional table with the rows corresponding to the number on the\nfirst die, the columns the number on the second die and the entries the probability.\n\n18.05 Class 2, Probability: Terminology and Examples, Spring 2022\nDie 2\nDie 1\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\n1/36\nTwo dice in a two dimensional table\nIn the second case we can present outcomes and probabilities in our usual table.\noutcome\nprobability\n1/36\n2/36\n3/36\n4/36\n5/36\n6/36\n5/36\n4/36\n3/36\n2/36\n1/36\nThe sum of two dice\nThink: What is the relationship between the two probability tables above?\nWe will see that the best choice of sample space depends on the context. For now, simply\nnote that given the outcome as a pair of numbers it is easy to find the sum.\nNote. Listing the experiment, sample space and probability function is a good way to start\nworking systematically with probability. It can help you avoid some of the common pitfalls\nin the subject.\nEvents.\nAn event is a collection of outcomes, i.e. an event is a subset of the sample space Ω. This\nsounds odd, but it actually corresponds to the common meaning of the word.\nExample 6. Using the setup in Example ?? we would describe the event that you get\nexactly two heads in words by E = 'exactly 2 heads'. Written as a subset this becomes\nE = {HHT, HTH, THH}.\nYou should get comfortable moving between describing events in words and as subsets of\nthe sample space.\nThe probability of an event E is computed by adding up the probabilities of all of the\noutcomes in E. In this example each outcome has probability 1/8, so we have P(E) = 3/8.\n2.3 Definition of a discrete sample space\nDefinition. A discrete sample space is one that is listable, it can be either finite or infinite.\nExamples. {H, T}, {1, 2, 3}, {1, 2, 3, 4, ...}, {2, 3, 5, 7, 11, 13, 17, ...} are all discrete\nsets. The first two are finite and the last two are infinite.\nExample. The interval 0 ≤ x ≤ 1 is not discrete, rather it is continuous. We will deal\nwith continuous sample spaces in a few days.\n\n18.05 Class 2, Probability: Terminology and Examples, Spring 2022\n2.4 The probability function\nSo far we've been using a casual definition of the probability function. Let's give a more\nprecise one.\nCareful definition of the probability function.\nFor a discrete sample space S a probability function P assigns to each outcome ω a number\nP (ω) called the probability of ω. P must satisfy two rules:\n- Rule 1. 0 ≤ P(ω) ≤ 1 (probabilities are between 0 and 1).\n- Rule 2. The sum of the probabilities of all possible outcomes is 1 (something must\noccur)\nIn symbols, Rule 2 says: if S\nn= {ω1, ω2, ... , ωn} then P(ω1) + P(ω2) + ... + P(ωn) = 1. Or,\nusing summation notation: ∑ P (ωj) = 1.\nj=1\nThe probability of an event E is the sum of the probabilities of all the outcomes in E. That\nis,\nP(E) = ∑P(ω).\nω∈E\nThink: Check Rules 1 and 2 on Examples 1 and 2 above.\nExample 7. Flip until heads (A classic example)\nSuppose we have a coin with probability p of heads and we have the following scenario.\nExperiment: Toss the coin until the first heads. Report the number of tosses.\nSample space: Ω = {1, 2, 3, ...}.\nProbability function: P (n) = (1 - p)n-1p.\nChallenge 1: show the sum of all the probabilities equals 1 (hint: geometric series).\nChallenge 2: justify the formula for P (n) (we will do this soon).\nStopping problems. The previous toy example is an uncluttered version of a general\nclass of problems called stopping rule problems. A stopping rule is a rule that tells you\nwhen to end a certain process. In the toy example above the process was flipping a coin\nand we stopped after the first heads. A more practical example is a rule for ending a series\nof medical treatments. Such a rule could depend on how well the treatments are working,\nhow the patient is tolerating them and the probability that the treatments would continue\nto be effective. One could ask about the probability of stopping within a certain number of\ntreatments or the average number of treatments you should expect before stopping.\n3 Some rules of probability\nFor events A, L and R contained in a sample space Ω.\nRule 1. P (Ac) = 1 - P (A).\nRule 2. If L and R are disjoint then P(L∪R) = P(L) + P(R).\n\n18.05 Class 2, Probability: Terminology and Examples, Spring 2022\nRule 3. If L and R are not disjoint, we have the inclusion-exclusion principle:\nP(L ∪ R) = P(L) + P(R) -P(L ∩ R)\nWe visualize these rules using Venn diagrams.\nA\nAc\nΩ = A∪Ac, no overlap\nL\nR\nL∪R, no overlap\nL\nR\nL∪R, overlap = L∩R\nWe can also justify them logically.\nRule 1: A and Ac split Ω into two non-overlapping regions. Since the total probability\nP(Ω) = 1 this rule says that the probabiity of A and the probability of 'not A' are comple\nmentary, i.e. sum to 1.\nRule 2: L and R split L ∪ R into two non-overlapping regions. So the probability of L ∪ R\nis is split between P (L) and P (R)\nRule 3: In the sum P (L) + P (R) the overlap P(L ∩ R) gets counted twice. So P (L) +\nP(R) - P(L ∩ R) counts everything in the union exactly once.\nThink: Rule 2 is a special case of Rule 3.\nFor the following examples suppose we have an experiment that produces a random integer\nbetween 1 and 20. The probabilities are not necessarily uniform, i.e., not necessarily the\nsame for each outcome.\nExample 8. If the probability of an even number is 0.6 what is the probability of an odd\nnumber?\nSolution: Since being odd is complementary to being even, the probability of being odd is\n1-0.6 = 0.4.\nLet's redo this example a bit more formally, so you see how it's done. First, so we can refer\nto it, let's name the random integer X. Let's also name the event 'X is even' as A. Then the\nevent 'X is odd' is Ac. We are given that P (A) = 0.6. Therefore P(Ac) = 1 -0.6 = 0.4 .\nExample 9. Consider the 2 events, A: 'X is a multiple of 2'; B: 'X is odd and less than\n10'. Suppose P (A) = 0.6 and P (B) = 0.25.\n(i) What is A ∩ B?\n(ii) What is the probability of A ∪ B?\nSolution: (i) Since all numbers in A are even and all numbers in B are odd, these events\nare disjoint. That is, A ∩ B = ∅.\n(ii) Since A and B are disjoint P(A ∪ B) = P(A) + P(B) = 0.85.\nExample 10. Let A, B and C be the events X is a multiple of 2, 3 and 6 respectively. If\nP (A) = 0.6, P (B) = 0.3 and P(C) = 0.2 what is P (A or B)?\nSolution: Note two things. First we used the word 'or' which means union: 'A or B' =\nA ∪ B. Second, an integer is divisible by 6 if and only if it is divisible by both 2 and 3.\n\n18.05 Class 2, Probability: Terminology and Examples, Spring 2022\nThis translates into C= A ∩ B. So the inclusion-exclusion principle says\nP(A ∪ B) = P(A) + P(B) -P(A ∩ B) = 0.6 + 0.3 -0.2 = 0.7 .\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Reading 3: Conditional Probability, Independence and Bayes' Theorem",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_class03-prep.pdf",
      "content": "Conditional Probability, Independence and Bayes' Theorem\nClass 3, 18.05\nJeremy Orloff and Jonathan Bloom\n1 Learning Goals\n1. Know the definitions of conditional probability and independence of events.\n2. Be able to compute conditional probability directly from the definition.\n3. Be able to use the multiplication rule to compute the total probability of an event.\n4. Be able to check if two events are independent.\n5. Be able to use Bayes' formula to 'invert' conditional probabilities.\n6. Be able to organize the computation of conditional probabilities using trees and tables.\n7. Understand the base rate fallacy thoroughly.\n2 Conditional Probability\nConditional probability answers the question 'how does the probability of an event change\nif we have extra information'. We'll illustrate with an example.\nExample 1. Toss a fair coin 3 times.\n(a) What is the probability of 3 heads?\nSolution: Sample space Ω = {HHH, HHT, HTH, HTT, THH, THT, TTH, TTT}.\nAll outcomes are equally probable, so P (3 heads) = 1/8.\n(b) Suppose we are told that the first toss was heads. Given this information how should\nwe compute the probability of 3 heads?\nSolution: We have a new (reduced) sample space: Ω′ = {HHH, HHT, HTH, HTT}.\nAll outcomes are equally probable, so\nP (3 heads given that the first toss is heads) = 1/4.\nThis is called conditional probability, since it takes into account additional conditions. To\ndevelop the notation, we rephrase (b) in terms of events.\nRephrased (b) Let A be the event 'all three tosses are heads' = {HHH}.\nLet B be the event 'the first toss is heads' = {HHH, HHT, HTH, HTT}.\nThe conditional probability of A knowing that B occurred is written\nP (A|B)\nThis is read as\n'the conditional probability of A given B'\n\n18.05 Class 3, Conditional Probability, Independence and Bayes' Theorem, Spring 2022 2\nor\n'the probability of A conditioned on B'\nor simply\n'the probability of A given B'.\nWe can visualize conditional probability as follows. Think of P (A) as the proportion of the\narea of the whole sample space taken up by A. For P (A|B) we restrict our attention to B.\nThat is, P (A|B) is the proportion of area of B taken up by A, i.e. P (A ∩ B)/P (B).\nB\nA\nA∩B\nHTH\nHTT\nTTH\nTTT\nHHH\nHHT\nTHH\nTHT\nA= A∩B\nB\nConditional probability: Abstract visualization and coin example\nNote, A ⊂ B in the right-hand figure, so there are only two colors shown.\nThe formal definition of conditional probability catches the gist of the above example and\nvisualization.\nFormal definition of conditional probability\nLet A and B be events. We define the conditional probability of A given B as\nP(A ∩ B)\nP (A|B) =\n, provided P(B) = 0.\n(1)\nP (B)\nLet's redo the coin tossing example using the definition in Equation (1). Recall A = '3 heads'\nand B = 'first toss is heads'. We have P (A) = 1/8 and P (B) = 1/2. Since A ∩ B = A, we\nalso have P(A ∩ B) = 1/8. Now according to (1),\nP (A ∩ B)\n1/8\nP (A|B) =\n= 1/2 = 1/4,\nP (B)\nwhich agrees with our answer in Example 1 b.\nWe start with a simple example where we can find all the probabilities directly by counting.\nExample 2. Draw two cards from a deck.\nDefine the events: S1 = 'first card is a spade'\nand S2 = 'second card is a spade'. What is the P (S2|S1)?\nSolution: We'll use formula (1) to compute the conditional probability. We have to com\npute P (S1), P (S2) and P (S1 ∩ S2): We know that P (S1) = 1/4 because there are 52 equally\nprobable ways to draw the first card and 13 of them are spades. The same logic says that\nthere are 52 equally probable ways the second card can be drawn, so P (S2) = 1/4.\nAside: The probability P (S2) = 1/4 may seem surprising since the value of first card\ncertainly affects the probabilities for the second card. However, if we look at all possible\ntwo card sequences we will see that every card in the deck has equal probability of being\n\n18.05 Class 3, Conditional Probability, Independence and Bayes' Theorem, Spring 2022 3\nthe second card. Since 13 of the 52 cards are spades we get P (S2) = 13/52 = 1/4. Another\nway to say this is: if we are not given value of the first card then we have to consider all\npossibilities for the second card.\nContinuing, we compute P (S1 ∩ S2) by counting:\nNumber of ways to draw a spade followed by a second spade: 13 ⋅ 12.\nNumber of ways to draw any card followed by any other card: 52 ⋅ 51.\nThus,\n13 ⋅ 12\nP (S1 ∩ S2) =\n51 = 3/51.\n52 ⋅\nNow, using (1) we get\nP (S2 ∩ S1)\n3/51\nP (S2|S1) =\n=\n= 12/51.\nP (S1)\n1/4\nThis case is simple enough that we can check our answer by computing the conditional\nprobability directly: if the first card is a spade then of the 51 cards remaining, 12 are\nspades. So, the probability the second card is also a spade is\nP (S2|S1) = 12/51.\nWarning: In more complicated problems it will be be much harder to compute conditional\nprobability by counting. Usually we have to use Equation (1).\nThink: For S1 and S2 in the previous example, what is P (S2|S1\nc)?\n3 Multiplication Rule\nThe following formula is called the multiplication rule.\nP (A ∩ B) = P (A|B) ⋅ P (B).\n(2)\nThis is simply a rewriting of the definition in Equation (1) of conditional probability. We\nwill see that our use of the multiplication rule is very similar to our use of the rule of\nproduct in counting. In fact, the multiplication rule is just a souped up version of the rule\nof product.\nWe start by verifying the multiplication rule for the previous example.\nExample 3. Draw two cards from a deck.\nDefine the events: S1 = 'first card is a spade'\nand S2 = 'second card is a spade'. Verify the multiplication rule.\nSolution: From the previous example, we know P (S2|S1) = 12/51, P (S1 ∩ S2) = 3/51, P (S1) =\n1/4. From this it is easy to check that\nP (S2|S1) ⋅ P (S1) =\n⋅\n51 = P (S1 ∩ S2).\n51 4 =\n4 Law of Total Probability\nThe law of total probability will allow us to use the multiplication rule to find probabilities\nin more interesting examples. It involves a lot of notation, but the idea is fairly simple. We\n\n18.05 Class 3, Conditional Probability, Independence and Bayes' Theorem, Spring 2022 4\nstate the law when the sample space is divided into 3 pieces. It is a simple matter to extend\nthe rule when there are more than 3 pieces.\nLaw of Total Probability\nSuppose the sample space Ω is divided into 3 disjoint events B1, B2, B3 (see the figure\nbelow). Then for any event A:\nP(A) = P(A ∩ B1) + P(A ∩ B2) + P(A ∩ B3)\nP (A) = P (A|B1) P (B1) + P (A|B2) P (B2) + P (A|B3) P (B3)\n(3)\nThe top equation says 'if A is divided into 3 pieces then P (A) is the sum of the probabilities\nof the pieces'. The bottom equation (3) is called the law of total probability. It is just a\nrewriting of the top equation using the multiplication rule.\nΩ\nB3\nB2\nB1\nA ∩B1\nA ∩B2 A ∩B3\nThe sample space Ω and the event A are each divided into 3 disjoint pieces.\nThe law holds if we divide Ω into any number of events, so long as they are disjoint and\ncover all of Ω. Such a division is often called a partition of Ω.\nOur first example will be one where we already know the answer and can verify the law.\nExample 4. An urn contains 5 red balls and 2 green balls. Two balls are drawn one after\nthe other. What is the probability that the second ball is red?\nSolution: The sample space is Ω = {rr, rg, gr, gg}.\nLet R1 be the event 'the first ball is red', G1 = 'first ball is green', R2 = 'second ball is\nred', G2 = 'second ball is green'. We are asked to find P (R2).\nLet's compute this same value using the law of total probability (3). First, we'll find the\nconditional probabilities. This is a simple counting exercise.\nP (R2|R1) = 4/6,\nP (R2|G1) = 5/6.\nSince R1 and G1 partition Ω the law of total probability says\nP (R2) = P (R2|R1)P (R1) + P (R2|G1)P (G1)\n(4)\n=\n⋅7 + 5 ⋅\n= 42 = 7.\n\n18.05 Class 3, Conditional Probability, Independence and Bayes' Theorem, Spring 2022 5\nOf course, this example is simple enough that we could have computed P (R2) directly the\nsame way we found P (S2) directly in the card example. But, we will see that in more\ncomplicated examples the law of total probability is truly necessary.\nProbability urns\nThe example above used probability urns. Their use goes back to the beginning of the\nsubject and we would be remiss not to introduce them. This toy model is very useful. We\nquote from Wikipedia: https://en.wikipedia.org/wiki/Urn_problem\nIn probability and statistics, an urn problem is an idealized mental exercise\nin which some objects of real interest (such as atoms, people, cars, etc.) are\nrepresented as colored balls in an urn or other container. One pretends to draw\n(remove) one or more balls from the urn; the goal is to determine the probability\nof drawing one color or another, or some other properties. A key parameter is\nwhether each ball is returned to the urn after each draw.\nIt doesn't take much to make an example where (3) is really the best way to compute the\nprobability. Here is a game with slightly more complicated rules.\nExample 5. An urn contains 5 red balls and 2 green balls. A ball is drawn. If it's green\na red ball is added to the urn and if it's red a green ball is added to the urn. (The original\nball is not returned to the urn.) Then a second ball is drawn. What is the probability the\nsecond ball is red?\nSolution: The law of total probability says that P (R2) can be computed using the expres\nsion in Equation (4). Only the values for the probabilities will change. We have\nP (R2|R1) = 4/7,\nP (R2|G1) = 6/7.\nTherefore,\nP (R2) = P (R2|R1)P (R1) + P (R2|G1)P (G1) =\n⋅ 7 + 6 ⋅\n=\n49.\n5 Using Trees to Organize the Computation\nTrees are a great way to organize computations with conditional probability and the law of\ntotal probability. The figures and examples will make clear what we mean by a tree. As\nwith the rule of product, the key is to organize the underlying process into a sequence of\nactions.\nWe start by redoing Example 5. The sequence of actions are: first draw ball 1 (and add the\nappropriate ball to the urn) and then draw ball 2.\nG1\nR1\nR2\nG2\nR2\nG2\n5/7\n2/7\n4/7\n3/7\n6/7\n1/7\n\n18.05 Class 3, Conditional Probability, Independence and Bayes' Theorem, Spring 2022 6\nYou interpret this tree as follows. Each dot is called a node. The tree is organized by levels.\nThe top node (root node) is at level 0. The next layer down is level 1 and so on. Each level\nshows the outcomes at one stage of the game. Level 1 shows the possible outcomes of the\nfirst draw. Level 2 shows the possible outcomes of the second draw starting from each node\nin level 1.\nProbabilities are written along the branches. The probability of R1 (red on the first draw)\nis 5/7. It is written along the branch from the root node to the one labeled R1. At the\nnext level we put in conditional probabilities. The probability along the branch from R1 to\nR2 is P (R2|R1) = 4/7. It represents the probability of going to node R2 given that you are\nalready at R1.\nThe muliplication rule says that the probability of getting to any node is just the product of\nthe probabilities along the path to get there. For example, the node labeled R2 at the far left\nreally represents the event R1 ∩ R2 because it comes from the R1 node. The multiplication\nrule now says\nP(R1 ∩ R2) = P(R1) ⋅P(R2|R1) = 5 ⋅7\n4,\nwhich is exactly multiplying along the path to the node.\nThe law of total probability is just the statement that P (R2) is the sum of the probabilities\nof all paths leading to R2 (the two circled nodes in the figure). In this case,\nP(R2) =\n⋅7 + 2 ⋅7 =\n49,\nexactly as in the previous example.\n5.1 Shorthand vs. precise trees\nThe tree given above involves some shorthand. For example, the node marked R2 at the\nfar left really represents the event R1 ∩ R2, since it ends the path from the root through\nR1 to R2. Here is the same tree with everything labeled precisely. As you can see this tree\nis more cumbersome to make and use. We usually use the shorthand version of trees. You\nshould make sure you know how to interpret them precisely.\nG1\nR1\nR1 ∩R2\nR1 ∩G2\nG1 ∩R2\nG1 ∩G2\nP(R1) = 5/7\nP(G1) = 2/7\nP(R2|R1) = 4/7\nP(G2|R1) = 3/7\nP(R2|G1) = 6/7\nP(G2|G1) = 1/7\n6 Independence\nTwo events are independent if knowledge that one occurred does not change the probability\nthat the other occurred. Informally, events are independent if they do not influence one\nanother.\n\n18.05 Class 3, Conditional Probability, Independence and Bayes' Theorem, Spring 2022 7\nExample 6. Toss a coin twice. We expect the outcomes of the two tosses to be independent\nof one another. In real experiments this always has to be checked. If my coin lands in honey\nand I don't bother to clean it, then the second toss might be affected by the outcome of the\nfirst toss.\nMore seriously, the independence of experiments can by undermined by the failure to clean or\nrecalibrate equipment between experiments or to isolate supposedly independent observers\nfrom each other or a common influence. We've all experienced hearing the same 'fact' from\ndifferent people. Hearing it from different sources tends to lend it credence until we learn\nthat they all heard it from a common source. That is, our sources were not independent.\nTranslating the verbal description of independence into symbols gives\nA is independent of B\nif\nP (A|B) = P (A).\n(5)\nThat is, knowing that B occurred does not change the probability that A occurred. In\nterms of events as subsets, knowing that the realized outcome is in B does not change the\nprobability that it is in A.\nIf A and B are independent in the above sense, then the multiplication rule gives\nP (A ∩ B) = P (A|B) ⋅ P (B) = P (A) ⋅ P (B).\nThis justifies the following technical definition of independence.\nFormal definition of independence: Two events A and B are independent if\nP(A ∩ B) = P(A) ⋅P(B)\n(6)\nThis is a nice symmetric definition which makes clear that A is independent of B if and only\nif B is independent of A. Unlike the equation with conditional probabilities, this definition\nmakes sense even when P(B) = 0. In terms of conditional probabilities, we have:\n1. If P(B) = 0 then A and B are independent if and only if P (A|B) = P (A).\n2. If P(A) = 0 then A and B are independent if and only if P (B|A) = P (B).\nIndependent events commonly arise as different trials in an experiment, as in the following\nexample.\nExample 7. Toss a fair coin twice. Let H1 = 'heads on first toss' and let H2 = 'heads on\nsecond toss'. Are H1 and H2 independent?\nSolution: Since H1 ∩ H2 is the event 'both tosses are heads' we have\nP (H1 ∩ H2) = 1/4 = P (H1)P (H2).\nTherefore the events are independent.\nWe can ask about the independence of any two events, as in the following two examples.\nExample 8. Toss a fair coin 3 times. Let H1 = 'heads on first toss' and A = 'two heads\ntotal'. Are H1 and A independent?\nSolution: We know that P (A) = 3/8. Since this is not 0 we can check if the formula in\nEquation 5 holds. Now, H1 = {HHH, HHT, HTH, HTT} contains exactly two outcomes\n(HHT, HTH) from A, so we have P (A|H1) = 2/4. Since P (A|H1) = P (A) these events\nare not independent.\n\n18.05 Class 3, Conditional Probability, Independence and Bayes' Theorem, Spring 2022 8\nExample 9. Draw one card from a standard deck of playing cards. Let's examine the\nindependence of 3 events 'the card is an ace', 'the card is a heart' and 'the card is red'.\nDefine the events as A = 'ace', H = 'hearts', R = 'red'.\n(a) We know that P (A) = 4/52 (4 out of 52 cards are aces), P(A|H) = 1/13 (1 out of 13\nhearts are aces). Since P(A) = P(A|H) we have that A is independent of H.\n(b) P (A|R) = 2/26 = 1/13 = P (A). So A is independent of R. That is, whether the card\nis an ace is independent of whether it is red.\n(c) Finally, what about H and R? Since P(H) = 1/4 and P(H|R) = 1/2, H and R are not\nindependent. We could also see this the other way around: P (R) = 1/2 and P(R|H) = 1,\nso H and R are not independent. That is, the suit of a card is not independent of the color\nof the card's suit.\n6.1 Paradoxes of Independence\nAn event A with probability 0 is independent of itself, since in this case both sides of\nequation (6) are 0. This appears paradoxical because knowledge that A occurred certainly\ngives information about whether A occurred. We resolve the paradox by noting that since\nP(A) = 0 the statement 'A occurred' is vacuous.\nThink: For what other value(s) of P (A) is A independent of itself?\n7 Bayes' Theorem\nBayes' theorem is a pillar of both probability and statistics and it is central to the rest of\nthis course. For two events A and B Bayes' theorem (also called Bayes' rule and Bayes'\nformula) says\nP (A|B) ⋅ P (B)\nP (B|A) =\n.\n(7)\nP (A)\nComments: 1. Bayes' rule tells us how to 'invert' conditional probabilities, i.e. to find\nP (B|A) from P (A|B).\n2. In practice, P (A) is often computed using the law of total probability.\nProof of Bayes' rule\nThe key point is that A ∩ B is symmetric in A and B. So the multiplication rule says\nP (B|A) ⋅ P (A) = P (A ∩ B) = P (A|B) ⋅ P (B).\nNow divide through by P (A) to get Bayes' rule.\nA common mistake is to confuse the meanings of P (A|B) and P (B|A). They can be very\ndifferent. This is illustrated in the next example.\nExample 10. Toss a coin 5 times. Let H1 = 'first toss is heads' and let HA = 'all 5 tosses\nare heads'. Then P (H1|HA) = 1 but P (HA|H1) = 1/16.\nFor practice, let's use Bayes' theorem to compute P (H1|HA) using P (HA|H1).The terms\n\n18.05 Class 3, Conditional Probability, Independence and Bayes' Theorem, Spring 2022 9\nare P (HA|H1) = 1/16, P (H1) = 1/2, P (HA) = 1/32. So,\nP (HA|H1)P (H1)\n(1/16) ⋅ (1/2)\nP (H1|HA) =\n=\n= 1,\nP (HA)\n1/32\nwhich agrees with our previous calculation.\n7.1 The Base Rate Fallacy\nThe base rate fallacy is one of many examples showing that it's easy to confuse the meaning\nof P (B|A) and P (A|B) when a situation is described in words. This is one of the key\nexamples from probability and it will inform much of our practice and interpretation of\nstatistics. You should strive to understand it thoroughly.\nExample 11. The Base Rate Fallacy\nConsider a routine screening test for a disease. Suppose the frequency of the disease in the\npopulation (base rate) is 0.5%. The test is fairly accurate with a 5% false positive rate and\na 10% false negative rate.\nYou take the test and it comes back positive. What is the probability that you have the\ndisease?\nSolution: We will do the computation three times: using trees, tables and symbols. We'll\nuse the following notation for the relevant events:\nD+ = 'you have the disease'\nD- = 'you do not have the disease\nT + = 'you tested positive'\nT - = 'you tested negative'.\nWe are given P (D+) = 0.005 and therefore P (D-) = 0.995. The false positive and false\nnegative rates are (by definition) conditional probabilities.\nP (false positive) = P (T +|D-) = 0.05 and P (false negative) = P (T -|D+) = 0.1.\nThe complementary probabilities are known as the true negative and true positive rates:\nP (T -|D-) = 1 - P (T +|D-) = 0.95 and P (T +|D+) = 1 - P (T -|D+) = 0.9.\nTrees: All of these probabilities can be displayed quite nicely in a tree.\nD+\nD-\nT+\nT-\nT+\nT-\n0.005\n0.995\n0.9\n0.1\n0.05\n0.95\nThe question asks for the probability that you have the disease given that you tested positive,\ni.e. what is the value of P (D+|T +). We aren't given this value, but we do know P (T +|D+),\nso we can use Bayes' theorem.\nP (T +|D+) ⋅ P (D+)\nP (D+|T +) =\n.\nP (T +)\n\n18.05 Class 3, Conditional Probability, Independence and Bayes' Theorem, Spring 2022 10\nThe two probabilities in the numerator are given. We compute the denominator P (T +)\nusing the law of total probability. Using the tree, we just have to sum the probabilities for\neach of the nodes marked T +\nP (T +) = 0.995 × 0.05 + 0.005 × 0.9 = 0.05425\nThus,\n0.9 × 0.005\nP (D+|T +) =\n= 0.082949 ≈ 8.3%.\n0.05425\nRemarks: This is called the base rate fallacy because the base rate of the disease in the\npopulation is so low that the vast majority of the people taking the test are healthy, and\neven with an accurate test most of the positives will be healthy people. Ask your doctor\nfor his/her guess at the odds.\nTo summarize the base rate fallacy with specific numbers\n95% of all tests are accurate does not imply 95% of positive tests are accurate\nWe will refer back to this example frequently. It and similar examples are at the heart of\nmany statistical misunderstandings.\nOther ways to work Example 11\nTables: Another trick that is useful for computing probabilities is to make a table. Let's\nredo the previous example using a table built with 10000 total people divided according to\nthe probabilites in this example.\nWe construct the table as follows. Pick a number, say 10000 people, and place it as the\ngrand total in the lower right. Using P (D+) = 0.005 we compute that 50 out of the 10000\npeople are sick (D+). Likewise 9950 people are healthy (D-). At this point the table looks\nlike:\nD+\nD-\ntotal\nT +\nT -\ntotal\n9950 10000\nUsing P (T +|D+) = 0.9 we can compute that the number of sick people who tested positive\nas 90% of 50 or 45. The other entries are similar. At this point the table looks like the\ntable below on the left. Finally we sum the T + and T - rows to get the completed table on\nthe right.\nD+\nD-\ntotal\nT +\nT -\ntotal\n9950 10000\nD+\nD-\ntotal\nT +\nT -\ntotal\n9950 10000\nUsing the complete table we can compute\n|D+ ∩ T +|\nP (D+|T +) =\n= 543 = 8.3%.\n|T +|\nSymbols: For completeness, we show how the solution looks when written out directly in\n\n18.05 Class 3, Conditional Probability, Independence and Bayes' Theorem, Spring 2022 11\nsymbols.\nP (T +|D+) ⋅ P (D+)\nP (D+|T +) =\nP (T +)\nP (T +|D+) ⋅ P (D+)\n= P (T +|D+) ⋅ P (D+) + P (T +|D-) ⋅ P (D-)\n0.9 × 0.005\n= 0.9 × 0.005 + 0.05 × 0.995\n= 8.3%\nVisualization: The figure below illustrates the base rate fallacy. The large blue rectangle\nrepresents all the healthy people. The much smaller orange rectangle represents the sick\npeople. The shaded rectangle represents the people who test positive. The shaded area\ncovers most of the orange area and only a small part of the blue area. Even so, the most of\nthe shaded area is over the blue. That is, most of the positive tests are of healthy people.\nD-\nD+\n7.2 Bayes' rule in 18.05\nAs we said at the start of this section, Bayes' rule is a pillar of probability and statistics.\nWe have seen that Bayes' rule allows us to 'invert' conditional probabilities. When we study\nstatistics we will see that the art of statistical inference involves deciding how to proceed\nwhen one (or more) of the terms on the right side of Bayes' rule is unknown.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Reading 4a: Discrete Random Variables",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_class04-prep-a.pdf",
      "content": "Discrete Random Variables\nClass 4, 18.05\nJeremy Orloff and Jonathan Bloom\n1 Learning Goals\n1. Know the definition of a discrete random variable.\n2. Know the Bernoulli, binomial, and geometric distributions and examples of what they\nmodel.\n3. Be able to describe the probability mass function and cumulative distribution function\nusing tables and formulas.\n4. Be able to construct new random variables from old ones.\n2 Random Variables\nThis topic is largely about introducing some useful terminology, building on the notions of\nsample space and probability function. The key words are\n1. Random variable\n2. Probability mass function (pmf)\n3. Cumulative distribution function (cdf)\n2.1 Recap\nA discrete sample space Ω is a finite or listable set of outcomes {ω1, ω2 ...}. The probability\nof an outcome ω is denoted P (ω).\nAn event E is a subset of Ω. The probability of an event E is P(E) = ∑P(ω).\nω∈E\n2.2 Random variables as payoff functions\nExample 1. A game with 2 dice.\nRoll a die twice and record the outcomes as (i, j), where i is the result of the first roll and\nj the result of the second. We can take the sample space to be\nΩ = {(1, 1), (1, 2), (1, 3), ... , (6, 6)} = {(i, j) | i, j = 1, ... 6}.\nThe probability function is P (i, j) = 1/36.\n\n18.05 Class 4, Discrete Random Variables, Spring 2022\nIn this game, you win $500 if the sum is 7 and lose $100 otherwise. We give this payoff\nfunction the name X and describe it formally by\nif i+ j = 7\nX(i, j) = {500\n-100\nif i+ j = 7.\nExample 2. We can change the game by using a different payoff function. For example\nY (i, j) = ij - 10.\nIn this example if you roll (6, 2) then you win $2. If you roll (2, 3) then you win -$4 (i.e.,\nlose $4).\nQuestion: Which game is the better bet?\nSolution: We will come back to this once we learn about expectation.\nThese payoff functions are examples of random variables. A random variable assigns a\nnumber to each outcome in a sample space. More formally:\nDefinition: Let Ω be a sample space. A discrete random variable is a function\nX∶ Ω → R\nthat takes a discrete set of values. (Recall that R stands for the real numbers.)\nWhy is X called a random variable? It's 'random' because its value depends on a random\noutcome of an experiment. And we treat X like we would a usual variable: we can add it\nto other random variables, square it, and so on.\n2.3 Events and random variables\nFor any value a we write X = a to mean the event consisting of all outcomes ω with\nX(ω) = a.\nExample 3. In Example 1 we rolled two dice and X was the random variable\nif i+ j = 7\nX(i, j) = {500\n-100\nif i+ j = 7.\nThe event X = 500 is the set {(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)}, i.e. the set of all\noutcomes that sum to 7. So P(X = 500) = 1/6.\nWe allow a to be any value, even values that X never takes. In Example 1, we could look\nat the event X = 1000. Since X never equals 1000 this is just the empty event (or empty\nset)\n‵X= 1000′ = {} = ∅\nP(X = 1000) = 0.\n\n18.05 Class 4, Discrete Random Variables, Spring 2022\n2.4 Probability mass function and cumulative distribution function\nIt gets tiring and hard to read and write P(X = a) for the probability that X = a. When\nwe know we're talking about X we will simply write p(a). If we want to make X explicit\nwe will write pX(a). We spell this out in a definition.\nDefinition: The probability mass function (pmf) of a discrete random variable is the\nfunction p(a) = P (X = a).\nNote:\n1. We always have 0 ≤ p(a) ≤ 1.\n2. We allow a to be any number. If a is a value that X never takes, then p(a) = 0.\nExample 4. Let Ω be our earlier sample space for rolling 2 dice. Define the random\nvariable M to be the maximum value of the two dice, i.e.\nM(i, j) = max(i, j).\nFor example, the roll (3,5) has maximum 5, i.e. M(3, 5) = 5.\nWe can describe a random variable by listing its possible values and the probabilities asso\nciated to these values. For the above example we have:\nvalue a:\npmf p(a):\n1/36\n3/36\n5/36\n7/36\n9/36\n11/36\nFor example, p(2) = 3/36.\nQuestion: What is p(8)?\nSolution: p(8) = 0.\nThink: What is the pmf for Z(i, j) = i+ j? Does it look familiar?\n2.5 Events and inequalities\nInequalities with random variables describe events. For example X ≤a is the set of all\noutcomes ω such that X(w) ≤ a.\nExample 5. If our sample space is the set of all pairs of (i, j) coming from rolling two dice\nand Z(i, j) = i+ j is the sum of the dice then\nZ ≤ 4 = {(1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (3, 1)}\n2.6 The cumulative distribution function (cdf)\nDefinition: The cumulative distribution function (cdf) of a random variable X is the\nfunction F given by F(a) = P(X ≤ a). We will often shorten this to distribution function.\nNote well that the definition of F (a) uses the symbol less than or equal to. This will be\nimportant for getting your calculations exactly right.\nExample. Continuing with the example M, we have\nvalue a:\npmf p(a):\n1/36\n3/36\n5/36\n7/36\n9/36\n11/36\ncdf F (a):\n1/36\n4/36\n9/36\n16/36\n25/36\n36/36\n\n18.05 Class 4, Discrete Random Variables, Spring 2022\nF (a) is called the cumulative distribution function because F (a) gives the total probability\nthat accumulates by adding up the probabilities p(b) as b runs from -inf to a. For example,\nin the table above, the entry 16/36 in column 4 for the cdf is the sum of the values of the\npmf from column 1 to column 4. In notation:\nAs events: 'M ≤4' = {1, 2, 3, 4};\nF(4) = P(M ≤ 4) = 1/36+3/36+5/36+7/36 = 16/36.\nJust like the probability mass function, F (a) is defined for all values a. In the above\nexample, F(8) = 1, F(-2) = 0, F (2.5) = 4/36, and F (π) = 9/36.\n2.7 Graphs of p(a) and F (a)\nWe can visualize the pmf and cdf with graphs. For example, let X be the number of heads\nin 3 tosses of a fair coin:\nvalue a:\npmf p(a):\n1/8\n3/8\n3/8\n1/8\ncdf F (a):\n1/8\n4/8\n7/8\nThe colored graphs show how the cumulative distribution function is built by accumulating\nprobability as a increases. The black and white graphs are the more standard presentations.\na\n1/8\n3/8\na\n1/8\n3/8\nProbability mass function for X\na\n1/8\n4/8\n7/8\na\n1/8\n4/8\n7/8\nCumulative distribution function for X\n\n18.05 Class 4, Discrete Random Variables, Spring 2022\na\n1/36\n3/36\n5/36\n7/36\n9/36\n11/36\na\n1/36\n4/36\n9/36\n16/36\n25/36\n36/36\npmf and cdf for the maximum of two dice (Example 4)\na\n9 10 11 12\n1/36\n2/36\n3/36\n4/36\n5/36\n6/36\na\n9 10 11 12\n1/36\n3/36\n6/36\n10/36\n15/36\n21/36\n26/36\n30/36\n33/36\npmf and cdf for the sum of two dice\nHistograms: Later we will see another way to visualize the pmf using histograms. These\nrequire some care to do right, so we will wait until we need them.\n2.8 Properties of the cdf F\nThe cdf F of a random variable satisfies several properties:\n1. F is non-decreasing. That is, its graph never goes down, or symbolically if a ≤ b then\nF (a) ≤ F (b).\n\n18.05 Class 4, Discrete Random Variables, Spring 2022\n2. 0 ≤ F(a) ≤ 1.\n3. lim F(a) = 1,\nlim F(a) = 0.\na→inf\na→-inf\nIn words, (1) says the cumulative probability F (a) increases or remains constant as a\nincreases, but never decreases; (2) says the accumulated probability is always between 0\nand 1; (3) says that as a gets very large, it becomes more and more certain that X ≤a and\nas a gets very negative it becomes more and more certain that X > a.\nThink: Why does a cdf satisfy each of these properties?\n3 Specific Distributions\n3.1 Bernoulli Distributions\nModel: The Bernoulli distribution models one trial in an experiment that can result in\neither success or failure This is the most important distribution and is also the simplest. A\nrandom variable X has a Bernoulli distribution with parameter p if:\n1. X takes the values 0 and 1.\n2. P(X = 1) = p and P(X = 0) = 1 -p.\nWe will write X ∼ Bernoulli(p) or Ber(p), which is read \"X follows a Bernoulli distribution\nwith parameter p\" or \"X is drawn from a Bernoulli distribution with parameter p\".\nA simple model for the Bernoulli distribution is to flip a coin with probability p of heads,\nwith X = 1 on heads and X = 0 on tails. The general terminology is to say X is 1 on\nsuccess and 0 on failure, with success and failure defined by the context.\nMany decisions can be modeled as a binary choice, such as votes for or against a proposal.\nIf p is the proportion of the voting population that favors the proposal, than the vote of a\nrandom individual is modeled by a Bernoulli(p).\nHere are the table and graphs of the pmf and cdf for the Bernoulli(1/2) distribution and\nbelow that for the general Bernoulli(p) distribution.\nvalue a:\npmf p(a):\n1/2\n1/2\ncdf F (a):\n1/2\na\np(a)\n1/2\na\nF(a)\n1/2\nTable, pmf and cmf for the Bernoulli(1/2) distribution\n\n18.05 Class 4, Discrete Random Variables, Spring 2022\nvalues a:\npmf p(a):\n1-p\np\ncdf F (a):\n1-p\na\np(a)\n1 -p\np\na\nF(a)\n1 -p\nTable, pmf and cmf for the Bernoulli(p) distribution\n3.2 Binomial Distributions\nThe binomial distribution Binomial(n,p), or Bin(n,p), models the number of successes in n\nindependent Bernoulli(p) trials.\nThere is a hierarchy here. A single Bernoulli trial is, say, one toss of a coin. A single\nbinomial trial consists of n Bernoulli trials. For coin flips the sample space for a Bernoulli\ntrial is {H, T}. The sample space for a binomial trial is all sequences of heads and tails of\nlength n. Likewise a Bernoulli random variable takes values 0 and 1 and a binomial random\nvariables takes values 0, 1, 2, ..., n.\nExample 6. Binomial(1,p) is the same as Bernoulli(p).\nExample 7. The number of heads in n flips of a coin with probability p of heads follows\na Binomial(n, p) distribution.\nWe describe X ∼ Binomial(n, p) by giving its values and probabilities. For notation we will\nuse k to mean an arbitrary number between 0 and n.\nWe remind you that 'n choose k = (n\nk) = nCk is the number of ways to choose k things\nout of a collection of n things and it has the formula\n(n\nn!\n(1)\nk) = k! (n-k)!.\n(It is also called a binomial coefficient.) Here is a table for the pmf of a Binomial(n, k) ran\ndom variable. We will explain how the binomial coefficients enter the pmf for the binomial\ndistribution after a simple example.\nvalues a:\n⋯\nk\n⋯\nn\n(n\n(n\n(n\npn\npmf p(a):\n(1 - p)n\n1)p1(1 - p)\nn-1\n2)p2(1 - p)\nn-2\n⋯\nk)pk(1 - p)\nn-k\n⋯\nExample 8. What is the probability of 3 or more heads in 5 tosses of a fair coin?\nSolution: The binomial coefficients associated with n = 5 are\n(5\n(5\n5!\n5 ⋅4 ⋅3 ⋅2 ⋅1\n(5\n5!\n5 ⋅4 ⋅3 ⋅2 ⋅1\n5 ⋅4\n0) = 1,\n1) =\n= 5,\n2) =\n= 10,\n1! 4! = 4 ⋅3 ⋅2 ⋅1\n2! 3! = 2 ⋅1 ⋅3 ⋅2 ⋅1 = 2\nand similarly\n(5\n3) = 10,\n(5\n4) = 5,\n(5\n5) = 1.\n\n18.05 Class 4, Discrete Random Variables, Spring 2022\nUsing these values we get the following table for X ∼ Binomial(5,p).\nvalues a:\npmf p(a):\n(1 - p)5\n5p(1 - p)4\n10p2(1 - p)3\n10p3(1 - p)2\n5p4(1 - p)\np5\nWe were told p = 1/2 so\nP(X≥3) = 10 (1\n2) (2\n1) + 5 (1\n2) (2\n1) + (1\n2) =\n= 2.\nThink: Why is the value of 1/2 not surprising?\n3.3 Explanation of the binomial probabilities\nFor concreteness, let n = 5 and k = 2 (the argument for arbitrary n and k is identical.) So\nX ∼ binomial(5, p) and we want to compute p(2). The long way to compute p(2) is to list\nall the ways to get exactly 2 heads in 5 coin flips and add up their probabilities. The list\nhas 10 entries:\nHHTTT, HTHTT, HTTHT, HTTTH, THHTT, THTHT, THTTH, TTHHT, TTHTH,\nTTTHH\nEach entry has the same probability of occurring, namely\np2(1 - p)3.\nThis is because each of the two heads has probability p and each of the 3 tails has proba\nbility 1 - p. Because the individual tosses are independent we can multiply probabilities.\nTherefore, the total probability of exactly 2 heads is the sum of 10 identical probabilities,\ni.e.\np(2) = 10p2(1 - p)3, as shown in the table.\nThis guides us to the shorter way to do the computation. We have to count the number of\nsequences with exactly 2 heads. To do this we need to choose 2 of the tosses to be heads\nand the remaining 3 to be tails. The number of such sequences is the number of ways to\nchoose 2 out of 5 things, that is (5\n2). Since each such sequence has the same probability,\np2(1 - p)3, we get the probability of exactly 2 heads p(2) = (5\n2)p2(1 - p)3.\nHere are some binomial probability mass functions:\na\n0.1\n0.2\na\n0.1\n0.2\n0.3\n0.4\na\n0.1\n0.2\nBinomial(10, 0.5)\nBinomial(10, 0.1)\nBinomial(20, 0.1)\n\n18.05 Class 4, Discrete Random Variables, Spring 2022\n3.4 Geometric Distributions\nA geometric distribution models the number of tails before the first head in a sequence of\ncoin flips (Bernoulli trials).\nExample 9. (a) Flip a coin repeatedly. Let X be the number of tails before the first heads.\nSo, X can equal 0, i.e. the first flip is heads, 1, 2, .... In principle it takes any nonnegative\ninteger value.\n(b) Give a flip of tails the value 0, and heads the value 1. In this case, X is the number of\n0's before the first 1.\n(c) Give a flip of tails the value 1, and heads the value 0. In this case, X is the number of\n1's before the first 0.\n(d) Call a flip of tails a success and heads a failure. So, X is the number of successes before\nthe first failure.\n(e) Call a flip of tails a failure and heads a success. So, X is the number of failures before\nthe first success.\nYou can see this models many different scenarios of this type. The most neutral language\nis the number of tails before the first head.\nFormal definition. The random variable X follows a geometric distribution with param\neter p if\n- X takes the values 0, 1, 2, 3, ...\n- its pmf is given by p(k) = P (X = k) = (1 - p)kp.\nWe denote this by X ∼ geometric(p) or geo(p). In table form we have:\nvalue\na:\n...\nk\n...\npmf\np(a):\np\n(1 - p)p\n(1 - p)2p\n(1 - p)3p\n...\n(1 - p)kp\n...\nTable: X ∼ geometric(p): X = the number of 0s before the first 1.\nWe will show how this table was computed in an example below.\nThe geometric distribution is an example of a discrete distribution that takes an infinite\nnumber of possible values. Things can get confusing when we work with successes and\nfailure since we might want to model the number of successes before the first failure or we\nmight want the number of failures before the first success. To keep straight things straight\nyou can translate to the neutral language of the number of tails before the first heads.\n\n18.05 Class 4, Discrete Random Variables, Spring 2022\na\n0.0\n0.1\n0.2\n0.3\n0.4\na\n0.0\n0.2\n0.4\n0.6\n0.8\npmf and cdf for the geometric(1/3) distribution\nExample 10. Computing geometric probabilities. Suppose that the inhabitants of an\nisland plan their families by having babies until the first girl is born. Assume the probability\nof having a girl with each pregnancy is 0.5 independent of other pregnancies, that all babies\nsurvive and there are no multiple births. What is the probability that a family has k boys?\nSolution: In neutral language we can think of boys as tails and girls as heads. Then the\nnumber of boys in a family is the number of tails before the first heads.\nLet's practice using standard notation to present this. So, let X be the number of boys in\na (randomly-chosen) family. So, X is a geometric random variable. We are asked to find\np(k) = P (X = k). A family has k boys if the sequence of children in the family from oldest\nto youngest is\nBBB ... BG\nwith the first k children being boys. The probability of this sequence is just the product\nof the probability for each child, i.e. (1/2)k ⋅ (1/2) = (1/2)k+1. (Note: The assumptions of\nequal probability and independence are simplifications of reality.)\nThink: What is the ratio of boys to girls on the island?\nMore geometric confusion. Another common definition for the geometric distribution is the\nnumber of tosses until the first heads. In this case X can take the values 1, i.e. the first\nflip is heads, 2, 3, .... This is just our geometric random variable plus 1. The methods of\ncomputing with it are just like the ones we used above.\n3.5 Uniform Distribution\nThe uniform distribution models any situation where all the outcomes are equally likely.\nX ∼ uniform(N).\nX takes values 1, 2, 3, ... , N, each with probability 1/N . We have already seen this distribu\ntion many times when modeling to fair coins (N = 2), dice (N = 6), birthdays (N = 365),\nand poker hands (N = (52\n5 )).\n\n18.05 Class 4, Discrete Random Variables, Spring 2022\n3.6 Discrete Distributions Applet\nThe applet at https://mathlets.org/mathlets/probability-distributions/ gives a\ndynamic view of some discrete distributions. The graphs will change smoothly as you move\nthe various sliders. Try playing with the different distributions and parameters.\nThis applet is carefully color-coded. Two things with the same color represent the same or\nclosely related notions. By understanding the color-coding and other details of the applet,\nyou will acquire a stronger intuition for the distributions shown.\n3.7 Other Distributions\nThere are a million other named distributions arising is various contexts. We don't expect\nyou to memorize them (we certainly have not!), but you should be comfortable using a\nresource like Wikipedia to look up a pmf. For example, take a look at the info box at the\ntop right of https://en.wikipedia.org/wiki/Hypergeometric_distribution. The info\nbox lists many (surely unfamiliar) properties in addition to the pmf.\n4 Arithmetic with Random Variables\nWe can do arithmetic with random variables. For example, we can add subtract, multiply\nor square them.\nThere is a simple, but extremely important idea for counting. It says that if we have a\nsequence of numbers that are either 0 or 1 then the sum of the sequence is the number of\n1s.\nExample 11. Consider the sequence with five 1s\n1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0.\nIt is easy to see that the sum of this sequence is 5 the number of 1s.\nWe illustrate this idea by counting the number of heads in n tosses of a coin.\nExample 12. Toss a fair coin n times. Let Xj be 1 if the jth toss is heads and 0 if it's\ntails. So, Xj is a Bernoulli(1/2) random variable. Let X be the total number of heads in\nthe n tosses. Assuming the tosses are independent we know X ∼ binomial(n, 1/2). We can\nalso write\nX = X1 + X2 + X3 + ... + Xn.\nAgain, this is because the terms in the sum on the right are all either 0 or 1. So, the sum\nis exactly the number of Xj that are 1, i.e. the number of heads.\nThe important thing to see in the example above is that we've written the more complicated\nbinomial random variable X as the sum of extremely simple random variables Xj. This will\nallow us to manipulate X algebraically.\nThink: Suppose X and Y are independent and X ∼ binomial(n, 1/2) and Y ∼ binomial(m, 1/2).\nWhat kind of distribution does X + Y follow? (Answer: binomial(n + m, 1/2). Why?)\n\n18.05 Class 4, Discrete Random Variables, Spring 2022\nExample 13. Suppose X and Y are independent random variables with the following\ntables.\nValues of X\nx:\npmf\nValues of Y\npX(x):\ny:\n1/10\n2/10\n3/10\n4/10\npmf\npY (y):\n1/15\n2/15\n3/15\n4/15\n5/15\nCheck that the total probability for each random variable is 1. Make a table for the random\nvariable X + Y .\nSolution: The first thing to do is make a two-dimensional table for the product sample\nspace consisting of pairs (x, y), where x is a possible value of X and y one of Y . To help\ndo the computation, the probabilities for the X values are put in the far right column and\nthose for Y are in the bottom row. Because X and Y are independent the probability for\n(x, y) pair is just the product of the individual probabilities.\nYvalues\nXvalues\n1/150\n2/150\n3/150\n4/150\n5/150\n2/150\n4/150\n6/150\n8/150\n10/150\n3/150\n6/150\n9/150\n12/150\n15/150\n4/150\n8/150\n12/150\n16/150\n20/150\n1/15\n2/15\n3/15\n4/15\n5/15\n1/10\n2/10\n3/10\n4/10\nThe diagonal stripes show sets of squares where X + Y is the same. All we have to do to\ncompute the probability table for X + Y is sum the probabilities for each stripe.\nX + Y values:\npmf:\n1/150\n4/150\n10/150\n20/150\n30/150\n34/150\n31/150\n20/150\nWhen the tables are too big to write down we'll need to use purely algebraic techniques to\ncompute the probabilities of a sum. We will learn how to do this in due course.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Reading 4b: Discrete Random Variables: Expected Value",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_class04-prep-b.pdf",
      "content": "Discrete Random Variables: Expected Value\nClass 4, 18.05\nJeremy Orloff and Jonathan Bloom\n1 Learning Goals\n1. Know how to compute expected value (mean) of a discrete random variable.\n2. Know the expected value of Bernoulli, binomial and geometric random variables.\n2 Expected Value\nIn the R reading questions for this lecture, you simulated the average value of rolling a die\nmany times. You should have gotten a value close to the exact answer of 3.5. To motivate\nthe formal definition of the average, or expected value, we first consider some examples.\nExample 1. Suppose we have a six-sided die marked with five 5 3's and one 6. (This was\nthe red one from our non-transitive dice.) What would you expect the average of 6000 rolls\nto be?\nSolution: If we knew the value of each roll, we could compute the average by summing\nthe 6000 values and dividing by 6000. Without knowing the values, we can compute the\nexpected average as follows.\nSince there are five 3's and one six we expect roughly 5/6 of the rolls will give 3 and 1/6 will\ngive 6. Assuming this to be exactly true, we have the following table of values and counts:\nvalue:\nexpected counts:\nThe average of these 6000 values is then\n5000 ⋅3 + 1000 ⋅6\n=\n⋅3 + 1 ⋅6 = 3.5\nWe consider this the expected average in the sense that we 'expect' each of the possible\nvalues to occur with the given frequencies.\nExample 2. We roll two standard 6-sided dice. You win $1000 if the sum is 2 and lose\n$100 otherwise. How much do you expect to win on average per trial?\nSolution: The probability of a 2 is 1/36. If you play N times, you can 'expect'\n⋅ N\nof the trials to give a 2 and 35 ⋅N of the trials to give something else. Thus your total\nexpected winnings are\nN\n35N\n1000 ⋅ 36 -100 ⋅\n.\nTo get the expected average per trial we divide the total by N :\nexpected average = 1000 ⋅ 36 - 100 ⋅ 36 = -69.44.\n\n18.05 Class 4, Discrete Random Variables: Expected Value, Spring 2022\nThink: Would you be willing to play this game one time? Multiple times?\nNotice that in both examples the sum for the expected average consists of terms which are\na value of the random variable times its probabilitiy. This leads to the following definition.\nDefinition: Suppose X is a discrete random variable that takes values x1, x2, ..., xn with\nprobabilities p(x1), p(x2), ..., p(xn). The expected value of X is denoted E[X] and defined\nby\nn\nE[X] = ∑p(xj) xj = p(x1)x1 + p(x2)x2 + ... + p(xn)xn.\nj=1\nNotes:\n1. The expected value is also called the mean or average of X and often denoted by μ\n(\"mu\").\n2. As seen in the above examples, the expected value need not be a possible value of the\nrandom variable. Rather it is a weighted average of the possible values.\n3. Expected value is a summary statistic, providing a measure of the location or central\ntendency of a random variable.\n4. If all the values are equally probable then the expected value is just the usual average of\nthe values.\nExample 3. Find E[X] for the random variable X with table:\nvalues of X:\npmf:\n1/6\n1/6\n2/3\nSolution: E[X] = 1 ⋅1 + 1 ⋅3 + 2 ⋅5 = 24 = 4\nExample 4. Let X be a Bernoulli(p) random variable. Find E[X].\nSolution: X takes values 1 and 0 with probabilities p and 1 - p, so\nE[X] = p⋅1 + (1 -p) ⋅0 = p.\nImportant: This is an important example. Be sure to remember that the expected value of\na Bernoulli(p) random variable is p.\nThink: What is the expected value of the sum of two dice?\n2.1 Mean and center of mass\nYou may have wondered why we use the name 'probability mass function'. Here's one\nreason: if we place an object of mass p(xj) at position xj for each j, then E[X] is the\nposition of the center of mass. Let's recall the latter notion via an example.\nExample 5. Suppose we have two masses along the x-axis, mass m1 = 500 at position\nx1 = 3 and mass m2 = 100 at position x2 = 6. Where is the center of mass?\n\n18.05 Class 4, Discrete Random Variables: Expected Value, Spring 2022\nSolution: Intuitively we know that the center of mass is closer to the larger mass.\nm1\nm2\nx\nFrom physics we know the center of mass is\nm1x1 + m2x2\n500 ⋅ 3 + 100 ⋅ 6\nx=\n=\n= 3.5.\nm1 + m2\nWe call this formula a 'weighted' average of the x1 and x2. Here x1 is weighted more heavily\nbecause it has more mass.\nNow look at the definition of expected value E[X]. It is a weighted average of the values of\nX with the weights being probabilities p(xi) rather than masses! We might say that \"The\nexpected value is the point at which the distribution would balance\". Note the similarity\nbetween the physics example and Example 1.\n2.2 Algebraic properties of E[X]\nWhen we add, scale or shift random variables the expected values do the same. The\nshorthand mathematical way of saying this is that E[X] is linear.\n1. If X and Y are random variables on a sample space Ω then\nE[X + Y ] = E[X] + E[Y ]\n2. If a and b are constants then\nE[aX + b] = aE[X] + b.\nWe will think of aX + b as scaling X by a and shifting it by b.\nBefore proving these properties, let's see them in action with a few examples.\nExample 6. Roll two dice and let X be the sum. Find E[X].\nSolution: Let X1 be the value on the first die and let X2 be the value on the second\ndie. Since X = X1 + X2 we have E[X] = E[X1] + E[X2]. Earlier we computed that\nE[X1] = E[X2] = 3.5, therefore E[X] = 7.\nExample 7. Let X ∼ binomial(n, p). Find E[X].\nSolution: Recall that X models the number of successes in n Bernoulli(p) random variables,\nwhich we'll call X1, ... Xn. The key fact, which we highlighted in the previous reading for\nthis class, is that\nn\nX = X1 + X2 + ... + Xn = ∑Xj.\nj=1\nNow we can use the Algebraic Property (1) to make the calculation simple.\nn\nX = ∑Xj ⇒ E[X] = ∑E[Xj] = ∑p =\nj=1\nj\nj\nnp .\n\n18.05 Class 4, Discrete Random Variables: Expected Value, Spring 2022\nWe could have computed E[X] directly as\nn\nn\nE[X] = ∑kp(k) = ∑k(n\nk)pk(1 -p)n-k.\nk=0\nk=0\nIt is possible to show that the sum of this series is indeed np. We think you'll agree that\nthe method using Property (1) is much easier.\nExample 8. (For infinite random variables the mean does not always exist.)\nSuppose X\nhas an infinite number of values according to the following table.\n2k\nvalues x:\n...\n... Try to compute the mean.\npmf p(x):\n1/2\n1/22\n1/23\n...\n1/2k\n...\nSolution: The mean is\ninf\ninf\nE[X] = ∑2k\nk = ∑1 = inf.\nk=1\nk=1\nThe mean does not exist! This can happen with infinite series.\nExample 9. Mean of a geometric distribution\nLet X ∼ geo(p). Recall this means X takes values k = 0, 1, 2, ...with probabilities p(k) =\n(1 - p)kp. (X models the number of tails before the first heads in a sequence of Bernoulli\ntrials.) The mean is given by\n1 - p\nE[X] =\n.\np\nTo see this requires a clever trick. Mathematicians love this sort of thing and we hope you\nare able to follow the logic and enjoy it. In this class we will not ask you to come up with\nsomething like this on an exam.\nHere's the trick.: to compute E[X] we have to sum the infinite series\ninf\nE[X] = ∑ k(1 - p)kp.\nk=0\ninf\nNow, we know the sum of the geometric series: ∑ xk = 1 - x.\nk=0\ninf\nDifferentiate both sides:\n∑ kxk-1 = (1 - x)2 .\nk=0\ninf\nx\nMultiply by x:\n∑ kxk = (1 - x)2 .\nk=0\ninf\n1 - p\nReplace x by 1 - p:\n∑ k(1 - p)k =\n.\np2\nk=0\ninf\n1 - p\nMultiply by p:\n∑ k(1 - p)kp =\n.\np\nk=0\nThis last expression is the mean.\n1 - p\nE[X] =\n.\np\n\n18.05 Class 4, Discrete Random Variables: Expected Value, Spring 2022\nExample 10. Flip a fair coin until you get heads for the first time. What is the expected\nnumber of times you flipped tails?\nSolution: The number of tails before the first head is modeled by X ∼ geo(1/2). From the\n1/2\nprevious example E[X] = 1/2 = 1. This is a surprisingly small number.\nExample 11. Michael Jordan, perhaps the greatest basketball player ever, made 80% of\nhis free throws. In a game what is the expected number he would make before his first miss.\nSolution: Here is an example where we want the number of successes before the first\nfailure. Using the neutral language of heads and tails: success is tails (probability 1 - p)\nand failure is heads (probability = p). Therefore p = 0.2 and the number of tails (made\nfree throws) before the first heads (missed free throw) is modeled by a X ∼ geo(0.2). We\nsaw in Example 9 that this is\n1 - p\n0.8\nE[X] =\n=\np\n0.2 = 4.\n2.3 Expected values of functions of a random variable\n(The change of variables formula.)\nIf X is a discrete random variable taking values x1, x2, ...and h is a function then h(X) is\na new random variable. Its expected value is\nE[h(X)] = ∑h(xj)p(xj).\nj\nWe illustrate this with several examples.\nExample 12. Let X be the value of a roll of one die and let Y = X2. Find E[Y ].\nSolution: Since there are a small number of values we can make a table.\nX\nY\nprob\n1/6\n1/6\n1/6\n1/6\n1/6\n1/6\nNotice the probability for each Y value is the same as that of the corresponding X value.\nSo,\nE[Y] = E[X2] = 12 ⋅6 + 22 ⋅6 + ... + 62 ⋅6 = 15.167.\nExample 13. Roll two dice and let X be the sum. Suppose the payoff function is given by\nY = X2 -6X+ 1. Is this a good bet?\nSolution: We have E[Y ] = ∑(j2 - 6j + 1)p(j), where p(j) = P (X = j).\nj=2\nWe show the table, but really we'll use R to do the calculation.\nX\nY\n-7\n-8\n-7\n-4\nprob\n1/36\n2/36\n3/36\n4/36\n5/36\n6/36\n5/36\n4/36\n3/36\n2/36\n1/36\nHere's the R code I used to compute E[Y ] = 13.833.\n\n18.05 Class 4, Discrete Random Variables: Expected Value, Spring 2022\nx = 2:12\ny = x^2 - 6*x + 1\np = c(1 2 3 4 5 6 5 4 3 2 1)/36\nave = sum(p*y)\nIt gave E[Y ] = 13.833.\nTo answer the question above: since the expected payoff is positive it looks like a bet worth\ntaking.\nQuiz: If Y = h(X) does E[Y ] = h(E[X])? Solution: NO!!! This is not true in general!\nThink: Is it true in the previous example?\nQuiz: If Y = 3X+ 77 does E[Y ] = 3E[X] + 77?\nSolution: Yes. By property (2), scaling and shifting does behave like this.\n2.4 Proofs of the algebraic properties of E[X]\nWe finish by proving the algebraic properties of E[X].\n1. For random variables X and Y on a sample space Ω: E[X + Y ] = E[X] + E[Y ]>\n2. For constants a, b and random variable X: E[aX + b] = aE[X] + b.\nThe proof of Property (1) is simple, but there is some subtlety in even understanding what\nit means to add two random variables. Recall that the value of random variable is a number\ndetermined by the outcome of an experiment. To add X and Y means to add the values of\nX and Y for the same outcome. In table form this looks like:\noutcome ω:\nω1\nω2\nω3\n...\nωn\nvalue of X:\nx1\nx2\nx3\n...\nxn\nvalue of Y :\ny1\ny2\ny3\n...\nyn\nvalue of X + Y :\nx1 + y1\nx2 + y2\nx3 + y3\n...\nxn+ yn\nprob. P (ω):\nP (ω1)\nP (ω2)\nP (ω3)\n...\nP (ωn)\nThe proof of (1) follows immediately:\nE[X + Y ] = ∑(xi + yi)P(ωi) = ∑xiP(ωi) + ∑yiP(ωi) = E[X] + E[Y].\nThe proof of Property (2) only takes one line.\nE[aX + b] = ∑p(xi)(axi + b) = a∑p(xi)xi + b∑p(xi) = aE[X] + b.\nThe b term in the last expression follows because ∑ p(xi) = 1.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Reading 5a: Variance of Discrete Random Variables",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_class05-prep-a.pdf",
      "content": "Variance of Discrete Random Variables\nClass 5, 18.05\nJeremy Orloff and Jonathan Bloom\n1 Learning Goals\n1. Be able to compute the variance and standard deviation of a random variable.\n2. Understand that standard deviation is a measure of scale or spread.\n3. Be able to compute variance using the properties of scaling and linearity.\n2 Spread\nThe expected value (mean) of a random variable is a measure of location or central tendency.\nIf you had to summarize a random variable with a single number, the mean would be a good\nchoice. Still, the mean leaves out a good deal of information. For example, the random\nvariables X and Y below both have mean 0, but their probability mass is spread out about\nthe mean quite differently.\nvalues X\n-2\n-1\nvalues Y\n-3\npmf p(x)\n1/10\n2/10\n4/10\n2/10\n1/10\npmf p(y)\n1/2\n1/2\nIt's probably a little easier to see the different spreads in plots of the probability mass\nfunctions. We use bars instead of dots to give a better sense of the mass.\nx\np(x)\n-1\n-2\n1/10\n2/10\n4/10\npmf for X\ny\np(y)\n-3\n1/2\npmf for Y\npmf's for two different distributions both with mean 0\nIn the next section, we will learn how to quantify this spread.\n3 Variance and standard deviation\nTaking the mean as the center of a random variable's probability distribution, the variance\nis a measure of how much the probability mass is spread out around this center. We'll start\nwith the formal definition of variance and then unpack its meaning.\nDefinition: If X is a random variable with mean E[X] = μ, then the variance of X is\ndefined by\nVar(X) = E[(X - μ)2].\n\n18.05 Class 5, Variance of Discrete Random Variables, Spring 2022\nThe standard deviation σ of X is defined by\nσ = √Var(X).\nIf the relevant random variable is clear from context, then the variance and standard devi\nation are often denoted by σ2 and σ ('sigma'), just as the mean is μ ('mu').\nWhat does this mean? First, let's rewrite the definition explicitly as a sum. If X takes\nvalues x1, x2, ... , xn with probability mass function p(xi) then\nn\nVar(X) = E[(X -μ)2] = ∑p(xi)(xi -μ)2.\ni=1\nIn words, the formula for Var(X) says to take a weighted average of the squared distance\nto the mean. By squaring, we make sure we are averaging only non-negative values, so that\nthe spread to the right of the mean won't cancel that to the left. By using expectation,\nwe are weighting high probability values more than low probability values. (See Example 2\nbelow.)\nNote on units:\n1. σ has the same units as X.\n2. Var(X) has the same units as the square of X. So if X is in meters, then Var(X) is in\nmeters squared.\nBecause σ and X have the same units, the standard deviation is a natural measure of spread.\nLet's work some examples to make the notion of variance clear.\nExample 1. Compute the mean, variance and standard deviation of the random variable\nX with the following table of values and probabilities.\nvalue x\npmf p(x)\n1/4\n1/4\n1/2\nSolution: First we compute E[X] = 7/2. Then we extend the table to include (X - 7/2)2.\nvalue x\np(x)\n1/4\n1/4\n1/2\n(x - 7/2)2\n25/4\n1/4\n9/4\nNow the computation of the variance is similar to that of expectation:\nVar(X) =\n⋅4 + 1 ⋅4 + 9 ⋅\n4 2 = 4 .\nTaking the square root we have the standard deviation σ = √11/4.\nExample 2. For each random variable X, Y , Z, and W plot the pmf and compute the\nmean and variance.\n(i)\nvalue x\npmf p(x)\n1/5\n1/5\n1/5\n1/5\n1/5\n(ii)\nvalue y\npmf p(y)\n1/10\n2/10\n4/10\n2/10\n1/10\n\n18.05 Class 5, Variance of Discrete Random Variables, Spring 2022\n(iii)\nvalue z\npmf p(z)\n5/10\n5/10\n(iv)\nvalue w\npmf p(w)\nSolution: Each random variable has the same mean 3, but the probability is spread out\ndifferently. In the plots below, we order the pmf's from largest to smallest variance: Z, X,\nY , W .\nz\np(z)\n.5\npmf for Z\nx\np(x)\n1/5\npmf for X\ny\np(y)\n.1\n.3\n.5\npmf for Y\nW\np(w)\npmf for W\nNext we'll verify our visual intuition by computing the variance of each of the variables.\nAll of them have mean μ = 3. Since the variance is defined as an expected value, we can\ncompute it using the tables.\n(i)\npmf p(x)\n1/5\n1/5\n1/5\n1/5\n1/5\n(X - μ)2\nvalue x\nVar(X) = E[(X-μ)2] = 4\n5 + 1\n5 + 0\n5 + 5\n1 + 4\n5 = 2 .\n(ii)\nvalue y\np(y)\n1/10\n2/10\n4/10\n2/10\n1/10\n(Y - μ)2\nVar(Y) = E[(Y -μ)2] = 10\n4 + 10\n2 + 10\n0 + 10\n2 + 10\n4 =\n(iii)\nvalue z\n1.2 .\npmf p(z)\n5/10\n5/10\n(Z - μ)2\n= 20\nVar(Z) = E[(Z - μ)2]\n10 + 20 =\n4 .\n\n18.05 Class 5, Variance of Discrete Random Variables, Spring 2022\n(iv)\npmf p(w)\n(W - μ)2\nvalue w\nVar(W) = 0 . Note that W doesn't vary, so it has variance 0!\n3.1 The variance of a Bernoulli(p) random variable.\nBernoulli random variables are fundamental, so we should know their variance.\nIf X ∼ Bernoulli(p) then\nVar(X) = p(1 - p).\nProof: We know that E[X] = p. We compute Var(X) using a table.\nvalues X\npmf p(x)\n1 - p\np\n(X - μ)2\n(0 - p)2\n(1 - p)2\nVar(X) = (1 -p)p2 + p(1 -p)2 = (1 -p)p(1 -p+ p) = (1 - p)p.\nAs with all things Bernoulli, you should remember this formula.\nThink: For what value of p does Bernoulli(p) have the highest variance? Try to answer\nthis by plotting the PMF for various p.\n3.2 A word about independence\nSo far we have been using the notion of independent random variable without ever carefully\ndefining it. For example, a binomial distribution is the sum of independent Bernoulli trials.\nThis may (should?) have bothered you. Of course, we have an intuitive sense of what inde\npendence means for experimental trials. We also have the probabilistic sense that random\nvariables X and Y are independent if knowing the value of X gives you no information\nabout the value of Y .\nIn a few classes we will work with continuous random variables and joint probability func\ntions. After that we will be ready for a full definition of independence. For now we can use\nthe following definition, which is exactly what you expect and is valid for discrete random\nvariables.\nDefinition: The discrete random variables X and Y are independent if\nP(X = a, Y = b) = P(X = a)P(Y = b)\nfor any values a, b. That is, the probabilities multiply.\n3.3 Properties of variance\nThe three most useful properties for computing variance are:\n1. If X and Y are independent then Var(X + Y) = Var(X) + Var(Y ).\n\n18.05 Class 5, Variance of Discrete Random Variables, Spring 2022\n2. For constants a and b, Var(aX + b) = a2Var(X).\n3. Var(X) = E[X2] - E[X]2.\nFor Property 1, note carefully the requirement that X and Y are independent. We will\nreturn to the proof of Property 1 in a later class.\nProperty 3 gives a formula for Var(X) that is often easier to use in hand calculations. The\ncomputer is happy to use the definition! We'll prove Properties 2 and 3 after some examples.\nExample 3. Suppose X and Y are independent and Var(X) = 3 and Var(Y) = 5. Find:\n(i) Var(X + Y), (ii) Var(3X + 4), (iii) Var(X + X), (iv) Var(X + 3Y).\nSolution: To compute these variances we make use of Properties 1 and 2.\n(i) Since X and Y are independent, Var(X + Y) = Var(X) + Var(Y) = 8.\n(ii) Using Property 2, Var(3X + 4) = 9 ⋅ Var(X) = 27.\n(iii) Don't be fooled! Property 1 fails since X is certainly not independent of itself. We can\nuse Property 2: Var(X+ X) = Var(2X) = 4 ⋅ Var(X) = 12. (Note: if we mistakenly used\nProperty 1, we would the wrong answer of 6.)\n(iv) We use both Properties 1 and 2.\nVar(X + 3Y) = Var(X) + Var(3Y) = 3 + 9 ⋅5 = 48.\nExample 4. Use Property 3 to compute the variance of X ∼ Bernoulli(p).\nSolution: From the table\nX\np(x)\n1 - p\np\nX2\nwe have E[X2] = p. So Property 3 gives\nVar(X) = E[X2] - E[X]2 = p - p2 = p(1 - p).\nThis agrees with our earlier calculation.\nExample 5. Redo Example 1 using Property 3.\nSolution: From the table\nX\np(x)\n1/4\n1/4\n1/2\nX2\nwe have E[X] = 7/2 and\nE[X2] = 12 ⋅4 + 32 ⋅4 + 52 ⋅\n= 15.\n2 = 4\nSo Var(X) = 15 - (7/2)2 = 11/4 -as before in Example 1.\n3.4 Variance of binomial(n,p)\nSuppose X ∼ binomial(n, p). Since X is the sum of independent Bernoulli(p) variables and\neach Bernoulli variable has variance p(1 - p) we have\nX ∼ binomial(n, p) ⇒ Var(X) = np(1 - p).\n\n18.05 Class 5, Variance of Discrete Random Variables, Spring 2022\n3.5 Proof of properties 2 and 3\nProof of Property 2: This follows from the properties of E[X] and some algebra.\nLet μ = E[X]. Then E[aX + b] = aμ + b and\nVar(aX+b) = E[(aX+b-(aμ+b))2] = E[(aX-aμ)2] = E[a2(X-μ)2] = a2E[(X-μ)2] = a2Var(X).\nProof of Property 3: We use the properties of E[X] and a bit of algebra. Remember\nthat μ is a constant and that E[X] = μ.\nE[(X - μ)2] = E[X2 - 2μX + μ2]\n= E[X2] - 2μE[X] + μ2\n= E[X2] - 2μ2 + μ2\n= E[X2] - μ2\n= E[X2] - E[X]2.\nQED\n4 Tables of Distributions and Properties\nDistribution\nrange X\npmf p(x)\nmean E[X]\nvariance Var(X)\nBernoulli(p)\n0, 1\np(0) = 1 - p, p(1) = p\np\np(1 - p)\nBinomial(n, p)\n0, 1,..., n\np(k) = (n\nk)pk(1 - p)n-k\nnp\nnp(1 - p)\nUniform(n)\n1, 2, ..., n\np(k) = n\nn + 1\nn2 - 1\nGeometric(p)\n0, 1, 2,...\np(k) = p(1 - p)k\n1 - p\np\n1 - p\np2\nLet X be a discrete random variable with range x1, x2, ...and pmf p(xj).\nExpected Value:\nVariance:\nSynonyms:\nmean, average\nNotation:\nE[X], μ\nVar(X), σ2\nDefinition:\nE[X] = ∑ p(xj)xj\nj\nE[(X - μ)2] = ∑ p(xj)(xj - μ)2\nj\nScale and shift:\nE[aX + b] = aE[X] + b\nVar(aX + b) = a2Var(X)\nLinearity:\n(for any X, Y )\nE[X + Y ] = E[X] + E[Y ]\n(for X, Y independent)\nVar(X + Y ) = Var(X) + Var(Y )\nFunctions of X:\nE[h(X)] = ∑ p(xj) h(xj)\nAlternative formula:\nVar(X) = E[X2] - E[X]2 = E[X2] - μ2\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Reading 5b: Continuous Random Variables",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_class05-prep-b.pdf",
      "content": "Continuous Random Variables\nClass 5, 18.05\nJeremy Orloff and Jonathan Bloom\n1 Learning Goals\n1. Know the definition of a continuous random variable.\n2. Know the definition of the probability density function (pdf) and cumulative distribution\nfunction (cdf).\n3. Be able to explain why we use probability density for continuous random variables.\n2 Introduction\nWe now turn to continuous random variables. All random variables assign a number to\neach outcome in a sample space. Whereas discrete random variables take on a discrete set\nof possible values, continuous random variables have a continuous set of values.\nComputationally, to go from discrete to continuous we simply replace sums by integrals. It\nwill help you to keep in mind that (informally) an integral is just a continuous sum.\nExample 1. Since time is continuous, the amount of time Jon is early (or late) for class is\na continuous random variable. Let's go over this example in some detail.\nSuppose you measure how early Jon arrives to class each day (in units of minutes). That\nis, the outcome of one trial in our experiment is a time in minutes. We'll assume there are\nrandom fluctuations in the exact time he shows up. Since in principle Jon could arrive, say,\n3.43 minutes early, or 2.7 minutes late (corresponding to the outcome -2.7), or at any other\ntime, the sample space consists of all real numbers. So the random variable which gives the\noutcome itself has a continuous range of possible values.\nIt is too cumbersome to keep writing 'the random variable', so in future examples we might\nwrite: Let T = \"time in minutes that Jon is early for class on any given day.\"\n3 Calculus Warmup\nWhile we will assume you can compute the most familiar forms of derivatives and integrals\nby hand, we do not expect you to be calculus whizzes. For tricky expressions, we'll let the\ncomputer do most of the calculating. Conceptually, you should be comfortable with two\nviews of a definite integral.\n1. ∫\nb\nf(x) dx = area under the curve y = f(x).\na\n2. ∫\nb\nf(x) dx = 'sum of f(x) dx'.\na\n\n18.05 Class 5, Continuous Random Variables, Spring 2022\nThe connection between the two is:\nn\narea ≈ sum of rectangle areas = f(x1)Δx+ f(x2)Δx+ ... + f(xn)Δx = ∑f(xi)Δx.\nAs the width Δx of the intervals gets smaller the approximation becomes better.\nx\ny\na\nb\ny= f(x)\nx\ny\nx0 x1 x2\nxn\nΔx\n⋯\na\nb\ny= f(x)\nArea = f(xi)Δx\nArea is approximately the sum of rectangles\nNote: In calculus you learned to compute integrals by finding antiderivatives. This is\nimportant for calculations, but don't confuse this method for the reason we use integrals.\nOur interest in integrals comes primarily from its interpretation as a 'sum' and to a much\nlesser extent its interpretation as area.\n4 Continuous Random Variables and Probability Density Func\ntions\nA continuous random variable takes a range of values, which may be finite or infinite in\nextent. Here are a few examples of ranges: [0, 1], [0, inf), (-inf, inf), [a, b].\nDefinition: A random variable X is continuous if there is a function f(x) such that for\nany c ≤d we have\nd\nP(c ≤ X ≤ d) = ∫ f(x) dx.\n(1)\nc\nThe function f(x) is called the probability density function (pdf).\nThe pdf always satisfies the following properties:\n1. f(x) ≥ 0 (f is nonnegative).\n2. ∫\ninf\nf(x) dx = 1\n(This is equivalent to: P(-inf< X < inf) = 1).\n-inf\nThe probability density function f(x) of a continuous random variable is the analogue of\nthe probability mass function p(x) of a discrete random variable. Here are two important\ndifferences:\n1. Unlike p(x), the pdf f(x) is not a probability. You have to integrate it to get proba\nbility. (See section 4.2 below.)\n2. Since f(x) is not a probability, there is no restriction that f(x) be less than or equal\nto 1.\n\n18.05 Class 5, Continuous Random Variables, Spring 2022\nNote: In Property 2, we integrated over (-inf, inf) since we did not know the range of values\ntaken by X. Formally, this makes sense because we just define f(x) to be 0 outside of the\nrange of X. In practice, we would integrate between bounds given by the range of X.\n4.1 Graphical View of Probability\nIf you graph the probability density function of a continuous random variable X then\nP(c ≤ X ≤ d) = area under the graph between c and d.\nx\nf(x)\nc\nd\nP(c≤X≤d)\nThink: What is the total area under the pdf f(x)?\n4.2 The terms 'probability mass' and 'probability density'\nWhy do we use the terms mass and density to describe the pmf and pdf? What is the\ndifference between the two? The simple answer is that these terms are completely analogous\nto the mass and density you saw in physics and calculus. We'll review this first for the\nprobability mass function and then discuss the probability density function.\nMass as a sum:\nIf masses m1, m2, m3, and m4 are set in a row at positions x1, x2, x3, and x4, then the\ntotal mass is m1 + m2 + m3 + m4.\nm1\nm2\nm3\nm4\nx\nx1\nx2\nx3\nx4\nWe can define a 'mass function' p(x) with p(xj) = mj for j = 1, 2, 3, 4, and p(x) = 0\notherwise. In this notation the total mass is p(x1) + p(x2) + p(x3) + p(x4).\nThe probability mass function behaves in exactly the same way, except it has the dimension\nof probability instead of mass.\nMass as an integral of density:\nSuppose you have a rod of length L meters with varying density f(x) kg/m. (Note the units\nare mass/length.)\nx\nx1\nx2\nx3\nxi\nΔx\nxn = L\nmass of ith piece ≈ f(xi)Δx\n\n18.05 Class 5, Continuous Random Variables, Spring 2022\nIf the density varies continuously, we must find the total mass of the rod by integration:\ntotal mass = ∫\nL\nf(x) dx.\nThis formula comes from dividing the rod into small pieces and 'summing' up the mass of\neach piece. That is:\nn\ntotal mass ≈ ∑f(xi) Δx\ni=1\nIn the limit as Δx goes to zero the sum becomes the integral.\nThe probability density function behaves exactly the same way, except it has units of\nprobability/(unit x) instead of kg/m. Indeed, equation (1) is exactly analogous to the\nabove integral for total mass.\nWhile we're on a physics kick, note that for both discrete and continuous random variables,\nthe expected value is simply the center of mass or balance point.\nExample 2. Suppose X has pdf f(x) = 3 on [0, 1/3] (this means f(x) = 0 outside of\n[0, 1/3]). Graph the pdf and compute P(0.1 ≤ X ≤ 0.2) and P(0.1 ≤ X ≤ 1).\nSolution: P(0.1 ≤ X ≤ 0.2) is shown below at left. We can compute the integral:\n0.2\n0.2\nP(0.1 ≤ X ≤ 0.2) = ∫\nf(x) dx = ∫\n3 dx = 0.3.\n0.1\n0.1\nOr we can find the area geometrically:\narea of rectangle = 3 ⋅0.1 = 0.3.\nP(0.1 ≤ X ≤ 1) is shown below at right. Since there is only area under f(x) up to 1/3, we\nhave P(0.1 ≤X ≤1) = 3 ⋅(1/3 -0.1) = 0.7.\nx\nf(x)\n1/3\n.1\n.2\nP (0.1 ≤ X ≤ 0.2)\nx\nf(x)\n1/3\n.1\nP (0.1 ≤ X ≤ 1)\nThink: In the previous example f(x) takes values greater than 1. Why does this not\nviolate the rule that probabilities are always between 0 and 1?\nNote on notation. We can define a random variable by giving its range and probability\ndensity function. For example we might say, let X be a random variable with range [0,1]\n\n18.05 Class 5, Continuous Random Variables, Spring 2022\nand pdf f(x) = x/2. Implicitly, this means that X has no probability density outside of the\ngiven range. If we wanted to be absolutely rigorous, we would say explicitly that f(x) = 0\noutside of [0,1], but in practice this won't be necessary.\nExample 3. Let X be a random variable with range [0,1] and pdf f(x) = Cx2. What is\nthe value of C?\nSolution: Since the total probability must be 1, we have\n∫\nf(x) dx = 1\n⇔\n∫\nCx2 dx = 1.\nBy evaluating the integral, the equation at right becomes\nC/3 = 1 ⇒ C = 3 .\nNote: We say the constant C above is needed to normalize the density so that the total\nprobability is 1.\nExample 4. Let X be the random variable in the Example 3. Find P (X ≤ 1/2).\n1/2\nSolution: P (X ≤ 1/2) = ∫\n1/2\n3x2 dx = x3∣0\n= 8.\nThink: For this X (or any continuous random variable):\n- What is P(a ≤ X ≤ a)?\n- What is P(X = 0)?\n- Does P(X = a) = 0 mean that X can never equal a?\nIn words the above questions get at the fact that the probability that a random person's\nheight is exactly 5'9\" (to infinite precision, i.e. no rounding!) is 0. Yet it is still possible\nthat someone's height is exactly 5'9\". So the answers to the thinking questions are 0, 0, and\nNo.\n4.3 Cumulative Distribution Function\nThe cumulative distribution function (cdf) of a continuous random variable X is defined in\nexactly the same way as the cdf of a discrete random variable.\nF (b) = P (X ≤ b).\nNote well that the definition is about probability. When using the cdf you should first think\nof it as a probability. Then when you go to calculate it you can use\nF(b) = P(X ≤ b) = ∫\nb\nf(x) dx,\nwhere f(x) is the pdf of X.\n-inf\nNotes:\n1. For discrete random variables, we defined the cumulative distribution function but did\n\n18.05 Class 5, Continuous Random Variables, Spring 2022\nnot have much occasion to use it. The cdf plays a far more prominent role for continuous\nrandom variables.\n2. As before, we started the integral at -inf because we did not know the precise range of\nX. Formally, this still makes sense since f(x) = 0 outside the range of X. In practice, we'll\nknow the range and start the integral at the start of the range.\n3. In practice we often say 'X has distribution F (x)' rather than 'X has cumulative distri\nbution function F (x).'\nExample 5. Find the cumulative distribution function for the density in Example 2.\na\na\nSolution: For a in [0,1/3] we have F(a) = ∫ f(x) dx = ∫ 3 dx = 3a.\nSince f(x) is 0 outside of [0,1/3] we know F(a) = P(X ≤ a) = 0 for a < 0 and F(a) = 1\nfor a > 1/3. Putting this all together we have\n⎧0\nif a < 0\n{\nF (a) =\n3a\nif 0 ≤ a ≤ 1/3\n⎨\n{1\nif 1/3 < a.\n⎩\nHere are the graphs of f(x) and F (x).\nx\nf(x)\n1/3\nx\nF (x)\n1/3\nNote the different scales on the vertical axes. Remember that the vertical axis for the pdf\nrepresents probability density and that of the cdf represents probability.\nExample 6. Find the cdf for the pdf in Example 3, f(x) = 3x2 on [0, 1]. Suppose X is a\nrandom variable with this distribution. Find P (X < 1/2).\nSolution: f(x) = 3x2 on [0,1] ⇒ F (a) = ∫\na\n3x2 dx = a3 on [0,1]. Therefore,\n⎧0\nif a < 0\n{\nF (a) =\na3\nif 0 ≤ a ≤ 1\n⎨\n{\n⎩1\nif 1 < a\nThus, P (X < 1/2) = F (1/2) = 1/8. Here are the graphs of f(x) and F (x):\nx\nf(x)\nx\nF (x)\n\n18.05 Class 5, Continuous Random Variables, Spring 2022\n4.4 Properties of cumulative distribution functions\nHere is a summarry of the most important properties of cumulative distribution functions\n(cdf)\n1. (Definition) F(x) = P(X ≤ x)\n2. 0 ≤ F(x) ≤ 1\n3. F (x) is non-decreasing, i.e. if a ≤ b then F (a) ≤ F (b).\n4. lim F(x) = 1\nand\nlim F(x) = 0\nx→inf\nx→-inf\n5. P(a≤X ≤b) = F(b) -F(a)\n6. F ′(x) = f(x).\nProperties 2, 3, 4 are identical to those for discrete distributions. The graphs in the previous\nexamples illustrate them.\nProperty 5 can be seen algebraically:\nb\na\nb\n∫ f(x) dx = ∫ f(x) dx+ ∫ f(x) dx\n-inf\n-inf\na\nb\nb\na\n⇔ ∫ f(x) dx = ∫ f(x) dx-∫ f(x) dx\na\n-inf\n-inf\n⇔P(a≤X ≤b) = F(b) -F(a).\nProperty 5 can also be seen geometrically. The orange region below represents F (b) and\nthe striped region represents F (a). Their difference is P(a ≤ X ≤ b).\nx\na\nb\nP (a ≤ X ≤ b)\nProperty 6 is the fundamental theorem of calculus.\n4.5 Probability density as a dartboard\nWe find it helpful to think of sampling values from a continuous random variable as throw\ning darts at a funny dartboard. Consider the region underneath the graph of a pdf as a\ndartboard. Divide the board into small equal size squares and suppose that when you throw\na dart you are equally likely to land in any of the squares. The probability the dart lands\nin a given region is the fraction of the total area under the curve taken up by the region.\nSince the total area equals 1, this fraction is just the area of the region. If X represents\nthe x-coordinate of the dart, then the probability that the dart lands with x-coordinate\nbetween a and b is just\nP(a ≤ X ≤ b) = area under f(x) between a and b = ∫\nb\nf(x) dx.\na\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu\n18.05 Introduction to Probability and Statistics\nSpring 2022\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    }
  ]
}