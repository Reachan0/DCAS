{
  "course_name": "Introduction to Neural Computation",
  "course_description": "This course introduces quantitative approaches to understanding brain and cognitive functions. Topics include mathematical description of neurons, the response of neurons to sensory stimuli, simple neuronal networks, statistical inference and decision making. It also covers foundational quantitative tools of data analysis in neuroscience: correlation, convolution, spectral analysis, principal components analysis, and mathematical concepts including simple differential equations and linear algebra.",
  "topics": [
    "Engineering",
    "Biological Engineering",
    "Computational Biology",
    "Health and Medicine",
    "Biomedical Signal and Image Processing",
    "Sensory-Neural Systems",
    "Science",
    "Biology",
    "Biophysics",
    "Neuroscience",
    "Cognitive Science",
    "Engineering",
    "Biological Engineering",
    "Computational Biology",
    "Health and Medicine",
    "Biomedical Signal and Image Processing",
    "Sensory-Neural Systems",
    "Science",
    "Biology",
    "Biophysics",
    "Neuroscience",
    "Cognitive Science"
  ],
  "syllabus_content": "Meeting Times\n\nLectures: 2 sessions / week, 90 minutes / session\n\nRecitations: 1 session / week, 1 hour / session\n\nPrerequisites\n\nPhysics II\n(\n8.02\n, 8.021, or\n8.022\n),\n6.0002 Introduction to Computational Thinking and Data Science\n,\nand\n9.01 Introduction to Neuroscience\nor permission of the instructor.\n\nNote: several modules of 8.02 can be found in\nthe Open Learning Library\n.\n\nCourse Description\n\nThis course introduces quantitative approaches to understanding brain and cognitive functions. Topics include mathematical description of neurons, the response of neurons to sensory stimuli, simple neuronal networks, statistical inference and decision making. It covers foundational quantitative tools of data analysis in neuroscience: correlation, convolution, spectral analysis, principal components analysis. Mathematical concepts include simple differential equations and linear algebra.\n\nHomework Assignments\n\nThere will be a total of seven (7) homework assignments. Release and due dates are indicated on the class schedule. Assignments are due by 11:59 pm on the due date.\n\nExcused extensions on assigned work will be given only for significant illness or family crisis. If an excused extension or postponement is requested, you must notify me prior to the class period for which the work is due.\n\nYou will be allowed four (4) free days of unexcused extensions on homework assignments to flexibly manage scheduling difficulties across the semester. Once these free days have been used, late work will be penalized at 20% per day.\n\nAdditionally, the lowest problem set grade will be dropped in calculating your final grade.\n\nSoftware Requirements\n\nAssignments require the use of MATLAB\n(r)\nversion 2017b. Therefore, it is essential that you install this software on your laptop.\n\nNote: MIT OpenCourseWare does not provide student access or discounts for\nMATLAB software\n. It can be purchased from The MathWorks\n(r)\n. For more information about MATLAB Pricing and Licensing, contact\nThe MathWorks\ndirectly.\n\nPolicy on Problem Set Collaboration\n\nCollaboration is encouraged on problem sets, but you must write up your own solutions and develop your own MATLAB code. List the names of all your collaborators on the top of each problem set submission.\n\nMidterm Exam\n\nThere will be two midterm exams, which will be held in class. Bring a calculator for the exams. For the second midterm, a take-home programming exercise will be assigned. Instructions for submission will be provided with assignment.\n\nFinal Exam\n\nThe final exam will be focused on the material presented after the second midterm. However, we will include a question pertaining to the material covered in the first midterm and a question for the material covered in the second midterm.\n\nGrading\n\nGrades are not matched to a specific curve in this subject. If everyone in the class does well, everyone can get an A. Grades will be assigned based on your overall, weighted class average using the weighting scheme presented below:\n\nActivities\n\nPercentages\n\nHomework Assignments\n\n50%\n\n2 Midterm Exams\n\n30% (15% each)\n\nFinal Exam\n\n20%\n\nClass Schedule\n\nL = Lecture\n\nR = Recitation\n\nSES #\n\nTOPICS\n\nKEY DATES\n\nL1\n\nCourse Overview and Ionic Currents\n\nPSet 1 assigned\n\nR1\n\nIntro to MATLAB and Ionic Currents\n\nL2\n\nRC Circuit and Nernst Potential\n\nL3\n\nNernst Potential and Integrate and Fire Models\n\nR2\n\nRC Model, Nernst Potential\n\nL4\n\nHodgkin Huxley Model Part 1\n\nNo Class\n\nPSet 1 due\n\nPSet 2 assigned\n\nR3\n\nIntegrate and Fire Model, Hodgkin Huxley Model\n\nL5\n\nHodgkin Huxley Model Part 2\n\nL6\n\nDendrites\n\nL7\n\nSynapses\n\nPSet 2 due\n\nPSet 3 assigned\n\nMidterm Review\n\nR5\n\nReview Session\n\nMidterm Exam\n\nL8\n\nSpike Trains\n\nPSet 4 assigned\n\nR6\n\nSpike Train Analysis\n\nL9\n\nReceptive Fields\n\nPSet 3 due\n\nL10\n\nTime Series\n\nR7\n\nSpike Triggered Average, Poisson Process\n\nL11\n\nSpectral Analysis Part 1\n\nPSet 4 due\n\nL12\n\nSpectral Analysis Part 2\n\nPSet 5 assigned\n\nR8\n\nSpectral Analysis\n\nL13\n\nSpectral Analysis Part 3\n\nMidterm 2 Review\n\nR9\n\nMidterm 2 Review\n\nMidterm Exam 2\n\nR10\n\nHelp With PSet 5\n\nL14\n\nRate Models and Perceptrons\n\nPSet 5 due\n\nMidterm Programming assigned\n\nL15\n\nMatrix Operations\n\nR11\n\nPerceptons and Matrices\n\nMidterm Programming due\n\nL16\n\nBasis Sets\n\nPSet 6 assigned\n\nL17\n\nPrincipal Components Analysis\n\nR12\n\nPrincipal Components Analysis\n\nL18\n\nRecurrent Networks\n\nPSet 6 due\n\nPSet 7 assigned\n\nL19\n\nNeural Integrators\n\nR13\n\nNetworks\n\nL20\n\nHopfield Networks\n\nPSet 7 due\n\nL21\n\nSequence Generation in Songbirds\n\nR14\n\nFinal Review",
  "files": [
    {
      "category": "Assignment",
      "title": "9.40 S2018 Problem Set 3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-40-introduction-to-neural-computation-spring-2018/5262d7111bcf17c733cb80c68591ebc9_MIT9_40S18_pset3.pdf",
      "content": "9.40 Introduction to Neural Computation\nProblem Set #3\nLearning objectives and outcomes:\nThis problem set investigates the role of synapses in two-compartment cell models. In\nparticular, we will analyze how the location of an inhibitory synapse affects the\ncomputational properties of a cell in a circuit mediating an escape response. More\nfundamentally, we will analyze how these biophysical and computational changes mediate\ndifferent behaviors and are thus fundamental for survival of the species.\nThe role of synapses and the computational properties of 2-compartment models are\ndiscussed in lectures 6 & 7. Furthermore, this PSET is based on the assigned reading for\nlecture 7 (Vu and Krasne, 1992).\nThe expected learning outcomes for this PSET are:\n- Analyze two-compartment cell models and write current balance equations using\nKrichoff's and Ohm's law.\n- Be acquainted with shunting inhibition.\n- Use the current balance equations to understand and analyze the different behaviors\nthat arise when inhibition is located out on a dendrite or near the soma.\n- Make plots to visualize relative and absolute suppression in 2-compartment cell\nmodels.\n- Disentangle in which scenarios excitation can override inhibition.\n- Be able to present your results as a cohesive and well-structured report.\nMATLAB functions you will need:\nThis PSET deals mostly with analytical work and plotting functions in MATLAB. As a\nreminder the main plotting functions are: figure, plot, subplot, xlim,\nylim, title, xlabel, ylabel, legend, hold on, hold off.\nTo generate plots with a logarithmic scale on the x-axis use semilogx. To generate a\nvector whose elements are logarithmically spaced use logspace.\nFor more information on MATLAB functions and commands, check the provided cheat\nsheet or the more extensive MATLAB documentation.\nSYNAPSES AS COMPUTATIONAL DEVICES\nEscape response in the crayfish: computational roles of somatic (proximal) and\ndendritic (distal) inhibition\nVu and Krasne (1992) studied the role of inhibition at two locations relative to the soma:\nnear the soma (proximal configuration) and out in the dendrite (distal configuration). The\n\n9.40 Introduction to Neural Computation\nProblem Set #3\nmain point of their study is to understand how inhibition location affects the initiation of\nthe LG escape response in the crayfish1.\nIn their analysis they argue that proximal (called \"recurrent\" in Vu and Krasne) inhibition\ncan mediate an absolute suppression of the escape behavior. In contrast, distal (called\n\"tonic\" in Vu and Krasne) inhibition mediates a relative suppression that can always be\noverridden by sufficient strong excitatory input. These mechanisms are important to\ndistinguish different behavioral scenarios. For an animal, it is often necessary to suppress\ncertain behaviors completely, while under other conditions the threshold for initiating a\nbehavior should only be elevated but the behavior should not be completely suppressed.\nAltogether, once an escape response is initiated no other escape reflex should be triggered.\nHowever, during feeding or exploratory behaviors the threshold for an escape response\nshould be set accordingly. This gives the biological background for the observed absolute\nand relative suppression.\nIn the next exercise we follow the analysis by Vu and Krasne. We will work on proximal\nand distal inhibition configurations on a two-compartment cell model to understand how\nabsolute and relative suppression arise in each of these scenarios.\nAn inhibitory synapse produces different effects on the target cell depending on the value\nof the synaptic reversal potential (Esyn). If the synaptic reversal potential is below the target\ncell resting potential, inhibition will be hyperpolarizing and therefore will have a\nsubtractive effect. In contrast, if the synaptic reversal potential is identical (or very close)\nto the membrane resting potential, inhibition will have a \"shunting\" effect. Shunting\ninhibition is termed \"shunting\" because the synaptic conductance short-circuits currents\nthat are generated at adjacent excitatory synapses. If a shunting inhibitory synapse is\nactivated, the input resistance is reduced locally and, following Ohm's law, the amplitude\nof subsequent excitatory postsynaptic potentials (EPSPs) is reduced. In this case inhibition\nper se does not affect the membrane potential as much as in the hyperpolarizing case,\nhowever it makes it harder to excite the cell.\nFor simplicity, and to be consistent with lecture notes we will call the inhibition locations\nsomatic (proximal in the paper) and dendritic (distal in the paper). It is important to notice\nthat crayfish neurons have a ganglionic morphological organization and that action\npotentials are not initiated in the soma but rather in a dendrite near the soma. This is the\norigin of the proximal and distal nomenclature, to refer to dendritic locations near and far\nfrom the soma respectively.\nProblem 1: Somatic inhibition\nIn the circuit below we depict a two-compartment model where the inhibitory synapse is\nlocated in the somatic compartment (proximal configuration).\n1 Check https://www.jove.com/video/1297/recordings-of-neural-circuit-activation-in-freely-behaving\nanimals for a video exhibiting the behavior (Heberholz, 2009)\n\n9.40 Introduction to Neural Computation\nProblem Set #3\nNotice that G* is a longitudinal resistor linking/coupling the two compartments. Also, to\nsimplify the analysis we have set the resting membrane potential at 0 mV. As inhibition is\nof the shunting type we have set Ei to 0 mV and thus there is no need to include a battery\nfor this synapse. Vd and Vs are labels to indicate the membrane potential at the dendritic\nand somatic compartments respectively.\nWith this information answer the following questions:\n1. Using Kirchoff's and Ohm's law, write two equations, each one expressing the\nsum of currents at the dendritic and somatic compartments respectively.\n2. Combine the above two equations to get a single expression for the voltage at\nthe somatic compartment that does not depend on the voltage at the dendritic\ncompartment. ( i.e to eliminate Vd). You should reach the following\nexpression:\nG G* E\ne\nV =\ne\ns\nG*Gd + GsGd + GsG* + Ge (G* + Gi + Gs )+ Gi (Gd + G* )\n3. Next consider the following parameter values:\n= G\nG* = G\nG = 1\nGd\ns\ns\ns\nGi\nα =\n={0,0.2,0.5,1,2,5} Ee = 100mV\nGd\nwhere α controls the inhibitory strength. Simplify the above expression for Vs\nusing these parameter relations.\n\n9.40 Introduction to Neural Computation\nProblem Set #3\n4. Using the expression from question 3, plot the voltage at the soma as a function\nof Ge for each level of inhibition. Use a logarithmic scale on the x-axis and set\nits range between 0.01 and 1000. Overlay all the curves in a single panel. This\nshould look similar to figure 1A in Vu and Krasne (1992)2.\n5. Note that in the previous plot, strong inhibition is always capable of\nsuppressing the voltage at the soma. To show this, take the limit of Ge going to\ninfinity on the simplified equation for Vs. Write down the expression you got\nfor this limit. In this limit, is Vs dependent or independent of inhibition?\nExplain and relate to the plot in question 4. What type of suppression is this?\nProblem 2: Dendritic inhibition\nIn the circuit below we depict a two-compartment model where the inhibitory synapse is\nlocated in the dendritic compartment with the same considerations as in problem 1.\nWith this information answer the following questions:\n1. Using Kirchoff's and Ohm's law, write two equations, each one expressing the\nsum of currents at the dendritic and somatic compartments respectively.\n2. Combine the above two equations to get a single expression for the voltage at\nthe somatic compartment that does not depend on the voltage at the dendritic\ncompartment. (i.e. to eliminate Vd). You should reach the following\nexpression:\n2 As in the paper, consider that the threshold for initiating an escape response is 9 mV.\n\n9.40 Introduction to Neural Computation\nProblem Set #3\nG G* E\ne\ne\nV =\ns\nG*\nG*\nG*Gd + GdG + G*G + G (\n+ G )+ Gi (\n+ G )\ns\ns\ne\ns\ns\n3. Next consider the following parameter values:\nGd = G\nG* = G\nG = 1\ns\ns\ns\nGi\nα =\n={0,0.2,0.5,1,2,5} Ee = 100mV\nGd\nwhere α controls the inhibitory strength. Simplify the above expression for Vs\nusing this parameter relations.\n4. Using the expression from question 3, plot the voltage at the soma as a function\nof Ge for each level of inhibition. Use a logarithmic scale on the x-axis and set\nits range between 0.01 and 1000. Overlay all the curves in a single panel. This\nshould look similar to figure 1A in Vu and Krasne (1992)3.\n5. Note that in the previous plot, strong excitation is always capable to overcome\ninhibition. To show this, take the limit of Ge going to infinity on the simplified\nequation for Vs. Write down the expression you got for this limit. In this limit,\nis Vs dependent or independent of inhibition? Explain and relate to the plot in\nquestion 4. What type of suppression is this?\nReferences:\nVu, E. T. and Krasne, F. B. (1992). Evidence for a Computational Distinction Between\nProximal and Distal Neuronal Inhibition. Science 255(5052): 1710-2.\nHerberholz, J. (2009). Recordings of Neural Circuit Activation in Freely Behaving\nAnimals, Journal of Visualized Experiments 29, e1297.\n3 As in the paper, consider that the threshold for initiating an escape response is 9 mV.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu/\n9.40 Introduction to Neural Computation\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Lecture 1: Overview and Ionic Currents - 9.40 Introduction to Neural Computation",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-40-introduction-to-neural-computation-spring-2018/6b1b693b91378b6fa15da16eee7621d4_MIT9_40S18_Lec01.pdf",
      "content": "Introduction to Neural Computation - 9.40\n- Prof. Michale Fee, Instructor\n- Daniel Zysman, Technical instructor\n\nTexts: Selected readings\n- Berg, Random Walks in Biology\n- Dayan & Abbott, Theoretical Neuroscience.\n- Hille, Ionic Channels of Excitable Membranes\n...and others\n\nWhat is neural computation?\n- Brain and cognitive sciences are no longer primarily descriptive\n- Engineering-level descriptions of brain systems.\nDiagram C Jeff Dean (adapted from DiCarlo & Cox, 2007). All rights\nreserved. This content is excluded from our Creative Commons License.\nFor more information see https://ocw.mit.edu/help/faq-fair-use/.\nFigure courtesy of Wormbook. License: CC BY. Eisenmann, D. M., ed.\nThe C. elegans Research Community, doi/10.1895/wormbook.1.7.1.\n\nNew technologies for neuronal activity\nmeasurements\nVideo\nYaleCampus. \"Imaging Brain Activity\nAcross the Mouse Cortex.\" YouTube.\nCrair Lab, Yale Univ\n\nWhat is neural computation?\n- Brain and cognitive sciences are no longer primarily descriptive\n- Engineering-level descriptions of brain systems.\n- Use mathematical techniques to analyze neural data in a way that\nallows us to relate it to mathematical models.\n- In this course we will have the added component that we will apply\nthese techniques to understand the circuits and computational\nprinciples that underlie animal behavior.\n\nNeural circuits that control bird song\nSee Lecture 1 video recording for playback\n\nWhat is neural computation?\n- Computational and quantitative approaches are also important in\ncognitive science.\n- Importance of computation and quantitation in medical sciences\n\nCourse Goals\n- Understand the basic biophysics of neurons and networks\nand other principles underlying brain and cognitive\nfunctions\n- Use mathematical techniques to\n- analyze simple models of neurons and networks\n- do data analysis of behavioral and neuronal data (compact\nrepresentation of data)\n- Become proficient at using numerical methods to\nimplement these techniques (MATLAB(r))\n\nTopics\nNeuronal biophysics and model neurons\nDifferential equations\nNeuronal responses and tuning curves\nSpike sorting, PSTHs and firing rates\nNeural coding and receptive fields\nCorrelation and convolution\nFeed forward networks and perceptrons\nLinear algebra\nData analysis, dimensionality reduction\nPrinciple Component Analysis and SVD\nShort-term memory, decision making\nRecurrent networks, eigenvalues\nSensory integration\nBayes rule\n\nSkills you will have\n- Translate a simple model of neurons and neural circuits into a\nmathematical model\n- Be able to simulate simple models using MATLAB(r)\n- Be able to analyze neuronal data (or model output) using\nMATLAB(r)\n- Be able to visualize high dimensional data.\n- Be able to productively contribute to research in a\nneuroscience lab!\n\nProblem sets\n- MATLAB(r) will be used extensively for the problem sets.\n- Free for students. Please install on your laptop.\n- We will use live scripts for Pset submissions.\n\nIntroduction to Neural\nComputation\nMichale Fee\nMIT BCS 9.40 -- 2018\nLecture 1 - Ionic Currents\n\nA mathematical model of a neuron\n- Equivalent circuit model\ngNa\ngK\ngL\nVm\nEL\nC\nIe\nEK\n+ ENa\n+\n+\n-\nA conceptual model based on simple\ncomponents from electrical circuits\n-100\nTime (ms)\n-\nA mathematical model that we can use\nto calculate properties of neurons\nVm\n\n0.1\n0.2\nTime (ms)\n\nWhy build a model of a neuron?\n-\nNeurons are very complex.\n-\nDifferent neuron types are defined by the genes that are expressed and their\ncomplement of ion channels\n-\nIon channels have dynamics at different timescales, voltage ranges, inactivation\nFigures removed due to copyright restrictions. Left side is Figure 3a: Spectral tSNE plot of 13,079\nneurons, colored according to the results of iterative subclustering. Campbell, J., et al. \"A\nmolecular census of arcuate hypothalamus and median eminence cell types.\" Nature Neuroscience\n20, pages 484-496 (2017). Right side is Figure 1: Representation of the amino acid sequence\nrelations of the minimal pore regions of the voltage-gated ion channel superfamily. Yu, F.H. and\nW.A. Catterall. \"The VGL-Chanome: A Protein Superfamily Specialized for Electrical Signaling and\nIonic Homeostasis.\" Science's STKE05 Oct 2004: re15.\n\nNeurons are extremely complex\n-\nIon channel and morphological diversity lead to diversity\nof firing patterns\n-\nIt's hard to guess how morphology and ion channels lead\nto firing patterns\n-\n... and how firing patterns control circuit behavior\nFigures removed due to copyright restrictions. Left side source unknown. Right side is Figure 6.1:\nMultiple firing patterns in cortical neurons. In: Gerstner, W., et al. Neuronal Dynamics. Cambridge\nUniversity Press.\n\nA mathematical model of a neuron\n- Equivalent circuit model\ngNa\ngK\ngL\nVm\nEL\nC\nIe\n+ ENa\nEK\n+\n+\n- Different parts of this circuit do different interesting things\n- Power supplies\n- Integrator of past inputs\n- Temporal filter to smooth inputs in time\n- Spike generator\n- Oscillator\n\nIonic currents\ngNa\ngK\ngL\nVm\nIe\n+ ENa\nEK\nEL\nC\n+\n+\nWhat are the wires of the brain?\nIn the brain (in neurons), current flow results from the movement of\nions in aqueous solution (water).\n\nBasic electrochemistry\n- Water is a polar solvent\n- Intracellular and extracellular space is filled with salt\nsolution (~100mM)\nH+\nH+\n- 6x1019 ions per cm3 (25A spacing)\nO-\nO-\nH+H+\nO\nH+\nO- Na\nO\n+\n-\nH+\nH+\nCl-\nH+\nH+\nH+\nH+H+\nO\n- Currents flow through a salt solution by two key\nmechanisms:\n+\nI\n-\nΔV\no Diffusion\no Drift in an electric field\n\nLearning objectives for Lecture 1\n- To understand how the timescale of diffusion relates to\nlength scales\n- To understand how concentration gradients lead to\ncurrents (Fick's First Law)\n- To understand how charge drift in an electric field leads\nto currents (Ohm's Law and resistivity)\n\nThermal energy\n- Every degree of freedom comes to thermal equilibrium with\nan energy proportional to temperature (Kelvin, K)\n- The proportionality constant is the Boltzmann constant (k)\nkT = 4x10-21 Joules at 300K)\nkT\nvx\n- Kinetic energy :\n=\n2 mvx = kT\nm\n- The mass of a sodium ion is 3.8x10-26 kg\nvx\n2 = 105m2 /s2\n⇒ vx = 3.2×102 m/s\nThis would cross this 10m classroom in 3/10 second!\nHere we follow 'Random Walks in Biology'\nHoward C. Berg, Princeton Univ Press 1993\n\nWhat is diffusion?\n- A particle in solution undergoes collisions with water molecules very\noften (~1013 times per second!) that constantly change its direction\nof motion.\nCollisions produce a 'random walk' in space\n\nSpatial and temporal scales\nDiffusion is fast at short length scales and slow at long length scales.\n-\nTo diffuse across a cell body (10um) it takes an ion 50ms\n-\nTo diffuse down a dendrite (1mm) it takes about 10min\n-\nHow long does it take an ion to diffuse down a motor neuron axon (1m)?\n10 years!\n\nDistribution of particles resulting from diffusion\nin 1-D\n- On average particles stay\nclustered around initial position\n- Particles spread out around\ninitial position\n- We can compute analytically\nproperties of this distribution!\n\n- An ensemble of particles diffusing\nfrom a point acquires a Gaussian\ndistribution\n- This arises from a binomial\ndistribution for large number of\ntime-steps (The probability of the\nparticle moving exactly k steps to\nthe right in n steps will be:\n⎛ n⎞\nP(k;n, p) =\n⎠⎟ pk (1- p)n-k\n⎝⎜ k\nGaussian Distribution\n\ne- x2 /4 Dt\nlim P(k;n, p) =\nnp→inf\n4π Dt\n\nRandom walk in one dimension\n- We can mathematically analyze the properties of an ensemble of\nparticles undergoing a random walk\n- Consider a particle moving left or right at a fixed velocity vx for a τ\ntime\nbefore a collision.\n- Imagine that each collision randomly resets the direction\n- Thus, on every time-step,\n- half the particles step right by a distance δ = +vxτ\n- and half the particles step to the left by a distance δ\n\nRandom Walk in 1-D\n- Assume that we have N particles that start at position x=0 at time\nt=0\n- xi(n) = the position of the ith particle on time-step n: n = t /τ\n- Assume the movement of each particle is independent\n- Thus, we can write the position of each particle at time-step n as a\nfunction of the position at previous time-step\nxi (n) = xi (n -1) ± δ\n- Use this to compute how the distribution evolves in time!\n\nAverage displacement is zero\n- What is the average position of our ensemble?\n=\nxi (n) i\n1 ∑ xi (n)\nxi (n) = xi (n -1) ± δ\nN\ni\n= 1 ∑ [xi (n -1) ± δ ]\nN\ni\n= 1 ∑ [xi (n -1)] + 1\n±δ\n(\ni ∑\n)\nN\ni\nN\n= xi (n -1)\nxi (n) i\ni\nHere we follow 'Random Walks in Biology' Howard C. Berg, Princeton Univ Press 1993\n\nDistribution of particles resulting from diffusion\nin 1-D\n- On average particles stay\nclustered around initial\nposition\n- Particles spread out around\ninitial position\n- We can compute analytically\nproperties of this\ndistribution!\n\nHow far does a particle travel due to diffusion?\n- We want to compute an average 'absolute value' distance from\norigin... Root mean square distance\nx(n)\n→\nx2(n)\nxi (n) = xi (n -1) ± δ\nCompute variance\nxi\n2(n) = (xi (n -1) ± δ )2\n= N ∑ xi\n2(n)\n= xi\n2(n -1) ± 2δ xi (n -1) + δ 2\nx2(n)\ni\n±2δ xi (n -1)\nδ 2\nx2(n) = x2(n -1) +\n+\nx2(n) = x2(n -1) + δ 2\n\nHow far does a particle travel due to diffusion?\n+ δ 2\nx2(n) = x2(n -1)\n- Note that at each time-step, the variance grows by δ 2\n= nδ 2\nx2(n)\nx2(0) = 0 ,\nx2(1) = δ 2 , x2(2) = 2δ 2 , ...\nδ 2t\n=\n,\nn = t /τ\nxi\n2 ( )t\nτ\nxi\n2 = 2Dt, D = δ 2 / 2τ\n(Diffusion coefficient)\nx2 = 2Dt\n\nSpatial and temporal scales\nDiffusion is fast at short length scales and slow at long length scales.\nTypical diffusion constants for small molecules and ions are ~10-5\ncm2/s\n-\nL = 10μm = 10-3 cm t = 10-6(cm2)/2x10-5(cm2/s) = 50 ms\n-\nL = 1mm = 10-1 cm t = 10-2(cm2)/2x10-5(cm2/s) = 500 s\n-\nL = 1000mm = 102 cm t = 104(cm2)/2x10-5(cm2/s) =\n500,000,000 seconds!!\nL =\n2Dt\nL2 = 2Dt\nt = L2 2D\n\nFick's first law\n-\nDiffusion produces a net flow of particles from regions of high concentration to\nregions of lower concentration.\n-\nThe flux of particles is proportional to the concentration gradient.\nN(x)\nN(x+δ)\nx\nx+δ\nis the number of particles in\nthe box at position x\nN(x)\n12 N(x)\n12 N(x+δ)\nis the net number of particles moving to\nthe right in an interval of time\n12 N(x)-N(x+δ)\n[\n]\nτ\nJx =-D 1\nδ [φ(x+δ)-φ(x)]\n\nJx =-D ∂φ\n∂x\n\nDiffusion produces a net flux of particles down a\ngradient\nK+\nK+\nK+\nK+\nK+\nK+\nK+\nK+\nK+\nK+\nK+\nK+\nK+\nK+\nx\nφ(x)\nJx =-D ∂φ\n∂x\n-\nEach particle diffuses\nindependently and\nrandomly!\n-\nEventually all concentration gradients go away...\n-\nAnd yet concentration\ngradients produce currents!\n\nCurrent flow in neurons obeys Ohm's Law\nΔV\nI\n+\nR\nIn a wire, current flow is\nproportional to voltage difference\nwhere\n- I is current (Amperes, A)\n- ΔV is voltage (Volts, V)\n- R is resistance (Ohms, Ω)\nI = ΔV\nR\nOhm's Law\n\nWhere does Ohm's Law come from?\nConsider a beaker filled with salt solution, two electrodes, and a\nbattery that produces a voltage difference between the electrodes.\n+\n-\nΔV\nI\nL\n-\nThe electric field produces a force which, in a solution, causes an\nion to drift with a constant velocity -- a current\nV(x)\nx\nV-\nV+\nE = ΔV\nL\nΔV\n!E\n!F=q !E\n-\nForce:\nK+\n\nIon currents in an electric field\nCurrents are also caused by the drift of ions in the presence of an\nelectric field.\n+\n-\nΔV\nI\nL\n-\nThe electric field produces a force which, in a solution, causes an\nion to drift with a constant velocity -- a current\nK+\nV(x)\nx\nV-\nV+\nE = ΔV\nL\nΔV\n!E\n-\nWhy constant velocity?\n\nIon currents in an electric field\nCurrents are also caused by the drift of ions in the presence of an\nelectric field.\nK+\nV(x)\nx\nV-\nV+\nE = ΔV\nL\nΔV\n!E\n=\nD\nkT q !E\n(\n)\nf = kT / D\n-\nEinstein -Smoluchovski relation\n!vd = D\nkT\n!F\n-\nDrift velocity is given by\n!F = f !vd\n-\nEinstein realized that this is just a result of viscous drag (or friction)\n\nCurrents are also caused by the drift of ions in the presence of an\nelectric field.\n-\nThe electric field produces a force which, in a solution, causes an\nion to drift with a constant velocity -- a current\nIon currents in an electric field\nI ∝vd A\nI ∝E A = ΔV\nL A\nK+\nV(x)\nx\nV-\nV+\nE = ΔV\nL\nΔV\n!E\n\n+\n-\nΔV\nI\nSurface area A\nL\nIn a solution, current flow per unit area is proportional to\nvoltage gradients\n-\nThus the resistance is given by:\nR = ρL\nA\nOhm's Law in solution\nI =\nΔV\nL A\n-\nLet's make this look more like Ohm's Law\nI =\nA\nρL\n⎛\n⎝⎜\n⎞\n⎠⎟ΔV\nρ\n⎛\n⎝⎜\n⎞\n⎠⎟\n= resistivity (Ω.m)\nρ\nI = 1\nR ΔV\n\nResistivity of intra/extra cellular space\n+\n-\nV\nI\nSurface\narea A\nL\n-\nρ = 1.6 μΩ.cm for copper\n-\nρ = ~60 Ω.cm for mammalian saline - the brain\nhas lousy conductors!\n-\nResistance of a volume of\nconductive medium is given by\nR = ρL\nA\n-\nThe brain has many specializations to deal with lousy wires...\n\nLearning objectives for Lecture 1\n- To understand how the timescale of diffusion relates to\nlength scales\n- Distance diffused grows as the square root of time\n- To understand how concentration gradients lead to\ncurrents (Fick's First Law)\n- Concentration differences lead to particle flux, proportional to\ngradient\n- To understand how charge drift in an electric field leads\nto currents (Ohm's Law and resistivity)\n\nI\nA = qφvd\nφ = ion density (ions per m3)\n= ionic charge (Coulombs per ion)\n= ion valence times 1.6x10-19 Coulombs\nq = ze\nCurrent density (Coulombs per second per unit area) is just drift\nvelocity times the density of ions times the charge per ion.\n(Extra slide) Derivation of resistivity\n\nI\nA = qφ D\nkT qE\n(\n)\n-\nPlugging in drift velocity from above, we get:\n\nDerivation of resistivity\nI\nA = q2φD\nkT\nE\n-\nThus, the current density (coulombs per second per unit area is\njust proportional to the electric field:\nI\nA =\nρ\n⎛\n⎝⎜\n⎞\n⎠⎟E\nρ =\nkT\nq2φD\n= resistivity (Ω!m)\n-\nSolving for we get:\nρ\n\nExtra slides on derivation of\nFick's first law\nWe will now use a similar approach to derive a macroscopic\ndescription of diffusion - a differential equation that describes the the\nflux of particles from the spatial distribution of their concentration.\nA is the area of the\ninterface between\nthe boxes\nN(x)\nN(x+δ)\nx\nx+δ\nis the number of particles in\na box (of length δ) at position x\nN(x)\n12 N(x)\n12 N(x+δ)\nis the net number of particles moving to\nthe right in an interval of time\n12 N(x)-N(x+δ)\n[\n]\nτ\n\nJx =-1\nAτ\n2[N(x+δ)-N(x)]\nWe can calculate the flux in units of particles per second\nper area\nNote: To get density (ions/m3) from molar\nconcentration (mol/L), you have to\nmultiply by NAx10-3. (NA is Avagadro's\nNumber = 6.02x1023)\nExtra slides on derivation of\nFick's first law\nParticles per unit volume\nJx =-D 1\nδ [φ(x+δ)-φ(x)]\nDensity - particles per unit volume\nmultiply by δ2 /δ2\nJx =-δ2\n2τ\nδ\nN(x+δ)\nAδ\n-N(x)\nAδ\n⎡\n⎣\n⎢⎢\n⎤\n⎦\n⎥⎥\nJx =-D ∂φ\n∂x\nN(x)\nN(x+δ)\nx\nx+δ\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu/\n\n9.40 Introduction to Neural Computation\nSpring 2018\n\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Lecture 2: RC Circuit and Nernst Potential - 9.40 Introduction to Neural Computation",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-40-introduction-to-neural-computation-spring-2018/45909c8bf4cb591d3f8395f0ec014fb1_MIT9_40S18_Lec02.pdf",
      "content": "Introduction to Neural\nComputation\nMichale Fee\nMIT BCS 9.40 -- 2018\nLecture 2 - RC Neuron Model\n\nA mathematical model of a neuron\n- Equivalent circuit model\ngL\nEL\nIe\nVm\nC\n+\ngNa\ngK\nEK\n+ ENa\n+\n-100 0\nTime (ms)\nAlan Hodgkin\nAndrew Huxley, 1952\n\n0.1\n0.2\nTime (ms)\nVm\n\nLearning objectives for Lecture 2\n- To understand how neurons respond to injected currents\n- To understand how membrane capacitance and resistance allows\nneurons to integrate or smooth their inputs over time (RC model)\n- To understand how to derive the differential equations for the RC\nmodel\n- To be able to sketch the response of an RC neuron to different\ncurrent inputs\n- To understand where the 'batteries' of a neuron come from\n\nWhy understand how neurons respond to\ninjected current?\nVm\n- First, because nearly every aspect of computation and\nsignaling in a neuron is controlled by voltage. This control is\nalmost entirely mediated by the voltage sensitivity of ion\nchannels.\n- In the brain, neurons have current injected into them:\n- Through synapses from other neurons\n- Or as a result of sensory stimuli\n\nWhy understand how neurons respond to\ninjected current?\nVm\nVm\nNeurons can perform analog numerical\nintegration over time\ntime\nt\nVoltage(t) = Current(t) dτ\n∫\nI0\ntime\n\nPhospholipid bilayer:\n--polar head\n--non-polar tail\nintracellular\nextracellular\nIe\nC\nVm = Vin-Vout\nEquivalent circuit\nVm\nWhy is this a capacitor?\nA capacitor is two conductors\nseparated by an insulator\nA neuron is a capacitor\nWhat happens when we inject\ncurrent into our neuron?\nImage of a parallel plate\ncapacitor is in the public\ndomain. Source: Wikimedia.\n\n-\nA neuron is a capacitor\nintracellular\nextracellular\nIe\nC\nVm\nIc\nAs positive charges build up on the\ninside of the membrane, they repel\npositive charges away from the\noutside of the membrane...\nIc\nThis looks like a current flowing\nthrough the capacitor!\n+\n+\n+ -\n-\n+\n+\n-\n-\nQ: charge (Coulombs, C = 6 x 1018 charges)\nC: capacitance (Farads, F)\nV: voltage difference across capacitor (Volts, V)\nΔQ = C ⋅ΔV\nΔQ\nΔV\nCharge imbalance:\nVoltage difference:\n!E\n\nintracellular\nextracellular\nIe\nC\nVm\nIc\n-Ic + Ie = 0\nBut, Kirchoff's current law tells us that the sum of all\ncurrents into a node is zero\nIc(t) = dQ\ndt\n= C dVm\ndt\nDefinition of capacitive current\nIe(t) = C dVm\ndt\nThus, we can write the differential equation that describes the change\nin voltage of our neural capacitor with injected current\nIe has units of Amperes, which is\nCoulombs per second\nA neuron is a capacitor\nΔQ = C ⋅ΔV\n\nVm(t) = V0 + 1\nC\nIe(τ )\nt\n∫\ndτ\nWe can integrate this differential equation over time, starting with\ninitial voltage V0 at time zero.\nIe(t) = C dVm\ndt\nIe(τ )\nt\n∫\ndτ = ΔQ\nThink about the integral as adding up all the current from time 0 to\ntime t\nΔV = 1\nC ΔQ\nThus, the total change in voltage is just given by\nResponse of a neuron to injected current\ncapacitor\n\nSome examples\nVm\ntime\nI0\nVm(t) = V0 + I0\nC t\ntime\nVm\ntime\nV0 + I0\nC τ\nI0\ntime\nτ\nV0\n\nA neuron is a leaky capacitor\nintracellular\nextracellular\nIe\nC\nVm\nRL\nIc\nIL\nIL = membrane ionic current\nIc = membrane capacitive current\n\nOur equation for our model becomes:\nelectrode current\nmembrane capacitive current\nmembrane ionic current\n|{z}\noutward current\n'+' leaving the cell ⇒positive\ninward current\n'+' entering the cell ⇒negative\nIL\n+\nC dV\ndt\n=\nIe\nIL + Ic = Ie\nIe\nC\nVm\nRL\nIc\nIL\nintracellular\nextracellular\n\nSimple case: a leak\n-\nWe are going to begin by considering the simplest case of a membrane\ncurrent - a simple leak (like a hole in the membrane)\n-\nIn this case, the current through the ion channel can be modeled using\nOhm's Law\nIL = Vm\nRL\nVm\nRL\n+ C dVm\ndt\n=\nIe\nPlugging this into our equation above, we get\nMultiplying by RL, we get:\nVm + RLC dVm\ndt\n= RL Ie\nIL + Ic = Ie\n\nVm + RLC dVm\ndt\n= RL Ie\nWhat is the steady-state solution to this equation?\nWe find that:\nVm\n⇒\nVinf= RL Ie\nThus, we can rewrite our equation as follows\nVm + τ dVm\ndt\n= Vinf\nwhere τ = RLC\nSet dVm dt = 0\n\nWe can rewrite our equation in the following form:\nWe see that the derivative is\n-\nnegative if V > Vinf\n-\npositive if V < Vinf\nThus, the voltage always approaches the\nvalue Vinf\ndV\ndt\n= -1\nτ V -Vinf\n(\n)\nV(t)\ntime\nV0\nVinf\nAn aside about first-order linear differential equations\nAnd it approaches at a rate proportional\nto how far V is from Vinf\nWhat kind of function is this?\nV(t)\ntime\nV0\nVinf\nsmaller τ\nτ\n1/ e\ndV\ndt\nV\nVinf\n\nThus, under the condition that Ie is constant (and thus Vinf) is constant:\nV(t)-Vinf= (V0 -Vinf)e-t/τ\nWhile this solution applies only in the case of constant Vinf, it can be\nvery useful\n\nResponse to current injection\nIe(t) 0\nI0\nVinf(t)= RL Ie(t)\nVinf(t)\nRL I0\ntime\nVm(t)\nRL I0\ntime\nτ = RLC\nLet's see what happens when we inject current into our\nmodel neuron with a leak conductance.\n\nAn RC neuron acts like a filter\nResponding well to inputs slower than , but not to inputs faster than\nτ\nτ =10ms\nτ\nInjected current\nVoltage response\n\nThe first-order linear differential equation is fundamental to\nunderstanding many processes in physics, chemistry, biology and\nneural computation\nV(t) = Vinf+(V0 -Vinf)e-t/τ\nV +τ dV\ndt = Vinf\nEven more complex systems involve differential equations that are not\n(much) more difficult to understand and solve.\n\nOrigin of 10 millisecond time scale\nR ≈108Ω = 100 MΩ\nC ≈10-10 F\nτ = RC 10ms\n\nWe have described the relation between voltage and current using\nOhms Law (V=ILRL)\nIL = RL\n-1 V\nA closer look at membrane resistance\nRL has units of Ohms (Ω)\nGL has units of Ohms-1 or Siemens (S)\nWe can rewrite Ohm's Law in terms of a quantity called\n'conductance.'\nGL = RL\n-1\nIL = GL V\n\nConductances in parallel add\nTwice the area, twice the holes,\ntwice the conductance, twice the\ncurrent at a given voltage\n\nIL = GLVm\n= A gLVm\nMembrane area (mm2)\nSpecific leak conductance (mS/mm2)\nI1\nI2\nV\n+\nItot = I1 + I2\nItot = G1V +G2V\nItot = G1 +G2\n(\n)V\nGtot = G1 +G2\n\nICtot = IC1 + IC2\nCapacitances in parallel add!\nThus, the capacitance of a cell depends linearly on surface area\nmembrane area\nspecific capacitance (10 nF/mm2)\nC = cmA A = 4πr2\nA closer look at membrane capacitance\nICtot = C1\ndV\ndt +C2\ndV\ndt\nICtot = C1 +C2\n(\n)dV\ndt\nCtot = C1 +C2\n\nMembrane time constant\nNeuron time constant:\nτm = RLC\n= C\nGL\n= cmA\ngLA = cm\ngL\nThus, the time constant of a neuron is a property of the\nmembrane, not dependent on cell geometry (size, shape, etc!).\n\nLet's add a battery to our neuron!\nIe\nC\nintracellular\nVm\nIe 0\nV\ntime\nIe\nVm\nC\n+\nintracellular\n\nOutline of HH model\ngNa\ngK\ngL\nEL\nEK\nENa\nIe\nVm\nC\n+\n+\n+\nVoltage and time-dependent ion channels are\nthe 'knobs' that control membrane potential.\n-\nSome ion channels push the membrane\npotential positive.\n-\nOther ion channels push the membrane\npotential negative.\n-\nTogether these channels give the neural\nmachinery flexible control of voltage!\n\nWhere do the batteries of a neuron come from?\n1) Ion concentration gradients\n2) Ion-selective permeability of ion\nchannels\n\nNeurons have batteries\nTime\n[K]\n[K]out\n[K]in\n[K]0\n[K]inf=[K]0/2\n'Non-selective' pore\npasses all ions\n(a long time)\n\nNeurons have batteries\nWhy do the ions stop flowing\nfrom side 1 to side 2?\n[K]\n[K]out\n[K]in\n[K]0\nTime\n'Ion-selective' pore\npasses only K+ ions\nIK (t)\nTime\nWhy does the concentration\nstop changing here?\n(a short time)\n\nNeurons have batteries\n'Ion-selective' pore\npasses only K+ ions\nIK (t)\ntime\nΔV=V1-V2\ntime\nThe voltage difference changes in a\ndirection that opposes the flow of ions.\nEK\nIt reaches an 'equilibrium potential' at a\nvalue that gives zero net flow of ionic\ncurrent.\nThis voltage difference is a\nbattery for our model neuron!!\n\nNeurons have batteries\nThere will be some electric field strength such that the\n'drift' will exactly balance the diffusion produced by the\nconcentration gradient...\nK+\nK+\nK+\nK+\nK+\nK+\nK+\nE\nV(x)\nx\nVin\nVout\nNernst Potential\n\nNeurons have batteries\n- Where do the 'batteries' of a\nneuron come from?\n1) Ion concentration gradients\n2) Ion-selective pores (channels)\n- How big is the battery (how\nmany volts?)\nThis is determined by a balance\nbetween diffusion down a concentration\ngradient balanced by 'drift' in the\nopposing electric field.\n\nElectrodiffusion and the Nernst Potential\nOne can use Ohm's law and Fick's first law to derive the Nernst\npotential\nITot = IDrift + IDiffusion\n-- At this voltage, the drift current in the electric field exactly\nbalances current due to diffusion\n= 0\nIDrift =\nAq2φ(x)D\nkT\nΔV\nL\nOhm's Law\nIDiffusion =-AqD ∂φ\n∂x\nFick's First Law\nΔV = kT\nq ln φout\nφin\n⎛\n⎝\n⎜⎜⎜⎜\n⎞\n⎠\n⎟⎟⎟⎟\nat equilibrium\n\nDerive Nernst potential using the\nBoltzmann equation\nThe Boltzmann equation describes the ratio of probabilities of a\nparticle being in any two states, at thermal equilibrium:\nstate 1\nstate 2\nEnergy\nU2\nU1\nPstate1\nPstate2\n= 0\nkT = 0\nk =Boltzmann constant (J/K)\nT = temperature (K) = 273 + TC\nkT = thermal energy (J)\nPstate1\nPstate2\n= e\n-U1-U2\nkT\n⎛\n⎝⎜\n⎞\n⎠⎟\n\nDerive Nernst potential using the\nBoltzmann equation\nThe Boltzmann equation describes the ratio of probabilities of a\nparticle being in any two states, at thermal equilibrium:\nstate 1\nstate 2\nEnergy\nU2\nU1\n}kT\nPstate1\nPstate2\n> 0\nkT > 0\nk =Boltzmann constant (J/K)\nT = temperature (K)\nkT = thermal energy (J)\nPstate1\nPstate2\n= e\n-U1-U2\nkT\n⎛\n⎝⎜\n⎞\n⎠⎟\n\nThe Boltzmann equation describes the ratio of probabilities of a\nparticle being in any two states, at thermal equilibrium:\nPstate1\nPstate2\n≈0\nPstate1\nPstate2\n= e-2\nPstate1\nPstate2\n≈1.0\nΔU >> kT\nDerive Nernst potential using the\nBoltzmann equation\nstate 1\nstate 2\nEnergy\nU2\nU1\n}kT\nstate 1\nstate 2\nEnergy\nU1\n}kT\nΔU = 2kT\nU2\nstate 1\nstate 2\nEnergy\nU2\nU1\n}kT\nΔU < kT\nPstate1\nPstate2\n= e\n-U1-U2\nkT\n⎛\n⎝⎜\n⎞\n⎠⎟\n\nNernst Potential\nWe can compute the equilibrium potential using the Boltzmann\nequation:\nPin\nPout\n= e\n-Uin-Uout\nkT\nU = qV = electrical potential (J)\nq = 1.6x10-19C for monovalent ion\nq = charge of ion\n= e\n-q(Vin-Vout )\nkT\n\nNernst Potential\nWe can compute the equilibrium potential using the Boltzmann\nequation:\nkT\nq = 25mV for monovalent ion\nVin -Vout = -kT\nq ln\nPin\nPout\n⎛\n⎝⎜\n⎞\n⎠⎟\nΔV = Vin -Vout = 25mV ln\nPout\nPin\n⎛\n⎝⎜\n⎞\n⎠⎟\nΔV = 25mV ln\nK\n[ ]out\nK\n[ ]in\n⎛\n⎝⎜\n⎞\n⎠⎟\n= EK\nDon't get confused by this notation. EK is the\nequilibrium potential (voltage) for the K ion.\n'E' here does not refer to an electric field.\nU = qV = electrical potential (J)\nq = 1.6x10-19C for monovalent ion\nq = charge of ion\nPin\nPout\n= e\n-Uin-Uout\nkT\n= e\n-q(Vin-Vout )\nkT\n\nIon\nCytoplasm\n(mM)\nExtracellular\n(mM)\nNernst\n(mV)\nK+\n-75\nThe Nernst potential for potassium\nIntracellular and extracellular concentrations of ionic\nspecies, and the Nernst potential\nEk = kT\nq ln 20\n⎛\n⎝\n⎜⎜⎜\n⎞\n⎠\n⎟⎟⎟\n= 25mV at 300K (room temp)\nfor monovalent ion\nkT\nq\nEK = 25mV(-3.00)=-75mV\nK+\nK+\nK+\nK+\n\nIe\nC\nVm\nGK\nEK\n+\nHow to implement an ion specific conductance\nas a battery in our model neuron\n\nLearning objectives for Lecture 2\n- To understand how membrane capacitance and\nresistance allows neurons to integrate or smooth their\ninputs over time (RC model)\n- To understand how to derive the differential equations\nfor the RC model\n- To be able to sketch the response of an RC neuron to\ndifferent current inputs\n- To understand where the 'batteries' of a neuron come\nfrom.\n\nExtra notes on how to derive the Nernst potential\nusing the equations for electrodiffusion\n\nElectrodiffusion and the Nernst Potential\nIn the last lecture, we found that the relation between drift velocity\nand force for an ion in an electric field is:\n!F = kT\nD\n!vd\n= electric force on ion due to\nelectric field E\n= total ion charge in Coulombs\n= electric field (V/m)\n!F = q !E\nq!E\n=Boltzmann constant (J/K)\n=temperature (K)\n=thermal energy (J)\n=diffusion constant (m2/s)\nk\nT\nkT\nD\nwhere is just the coefficient of friction given by the Einstein-\nSmoluchovski relation.\nf = kT D\n\nElectrodiffusion and the Nernst Potential\nThus, we can write the drift velocity as:\nvd = qD\nkT E\nSubstituting vd from above, we get that:\nI\nA = q2NAD\nkT\ncE\nqD\nkT\nis the ion mobility, which describes\nhow fast an ion will move in an\nelectric field - (m/s)/(V/m)\nWe can find the total current density (amperes per unit area) as\nI\nA = q NAcvd\n= ion density (ions/m3)\n= molar ion concentration (mol/m3)\n= Avagadro's number (ions/mol)\nNAc\nc\nNA\n\nElectrodiffusion and the Nernst Potential\nNext we use the fact the the electric field is the spatial derivative of\nthe electrical potential (voltage)\nE\n\n=-∇\n\nV,\nEx = ∂V\n∂x\nWe can find the total current density (amperes per unit area) due to\nthe electric field:\nI\nA =-q2NAD\nkT\nc ∂V\n∂x\n\nElectrodiffusion and the Nernst Potential\nPut it all together and we get\nI\nA\n⎡\n⎣\n⎢⎢\n⎤\n⎦\n⎥⎥Tot\n=-qNAD q\nkT c ∂V\n∂x + ∂c\n∂x\n⎡\n⎣\n⎢⎢\n⎤\n⎦\n⎥⎥\nThis has units of current per unit area\n(Amperes/m2)\nWe know that at equilibrium, the total current is zero. Thus,\n= charge of a single ion\n= molar concentration (mol/m3)\n= Avagadro's number\nq\nNA\nc\nq\nkT c ∂V\n∂x + ∂c\n∂x = 0\nA good reference for this derivation is Hille's chapter on 'Elementary\nProperties of Ions in Solution' (p. 261-269 of the second edition)\n\nElectrodiffusion and the Nernst Potential\nDivide through by c and q/kT and we get\n∂V\n∂x + kT\nq\n⎛\n⎝\n⎜⎜⎜\n⎞\n⎠\n⎟⎟⎟⎟\nc\n∂c\n∂x = 0\nUse the fact that ∂lnc(x)\n∂x\n=\nc(x)\n∂c(x)\n∂x\n∂V\n∂x + kT\nq\n⎛\n⎝\n⎜⎜⎜\n⎞\n⎠\n⎟⎟⎟⎟\n∂lnc(x)\n∂x\n= 0\n\nElectrodiffusion and the Nernst Potential\nNow we can integrate both terms\nV(x)⎤⎦x=0\nx=l + kT\nq\n⎛\n⎝\n⎜⎜⎜\n⎞\n⎠\n⎟⎟⎟⎟lnc(x)⎤⎦x=0\nx=l = 0\nVout -Vin =-kT\nq\n⎛\n⎝\n⎜⎜⎜\n⎞\n⎠\n⎟⎟⎟⎟lncout -lncin\n[\n]\nK\n+\nK\n+\nK\n+\nK\n+\nK\n+\nK\n+\nK\n+\nV(x)\nx\nVout\nx = 0\nx = l\nVin\nc(x)\nx\nx = 0\nx = l\ncin\ncout\nc(x)= cine\n-x\nl ln cout\ncin\n⎛\n⎝\n⎜⎜⎜⎜\n⎞\n⎠\n⎟⎟⎟⎟⎟\n\nΔV = kT\nq ln cout\ncin\n⎛\n⎝\n⎜⎜⎜⎜\n⎞\n⎠\n⎟⎟⎟⎟, where ΔV =Vin -Vout\nat equilibrium\nDon't get confused by this notation. EK is the\nequilibrium potential (voltage) for the K ion.\n'E' here does not refer to an electric field.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu/\n\n9.40 Introduction to Neural Computation\nSpring 2018\n\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Lecture 3: Nernst Potential & Integrate and Fire Model - 9.40 Introduction to Neural Computation",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-40-introduction-to-neural-computation-spring-2018/16084675fd6a490a2ac0d3d6ae350072_MIT9_40S18_Lec03.pdf",
      "content": "Introduction to Neural\nComputation\nMichale Fee\nMIT BCS 9.40 -- 2018\nVideo Module on Nernst Potential\nPart 1\n\nA mathematical model of a neuron\n- Equivalent circuit model\ngNa\ngK\ngL\nEL\nEK\nENa\nIe\nVm\nC\n+\n+\n+\n-100\nTime (ms)\nVm\n0.1\n0.2\nTime (ms)\nAlan Hodgkin\nAndrew Huxley, 1952\n\nA neuron is a leaky capacitor\nintracellular\nextracellular\nIe\nC\nVm\nRL\nIc\nIL\nIL = membrane ionic current\nIc = membrane capacitive current\nVinf(t) = RL Ie(t)\nVm + τ dVm\ndt\n= Vinf\nwhere τ = RLC\nl\nl\n\nResponse to current injection\nIe(t) 0\nI0\ntime\nVm(t)\nRL I0\ntime\nτ = RLC\nLet's see what happens when we inject current into our\nmodel neuron with a leak conductance.\n\nA neuron is a leaky capacitor\nIe\nVm\nC\n+\n\nOutline of HH model\ngNa\ngK\ngL\nEL\nEK\nENa\nIe\nVm\nC\n+\n+\n+\nVoltage and time-dependent ion channels are\nthe 'knobs' that control membrane potential.\n-\nSome ion channels push the membrane\npotential positive.\n-\nOther ion channels push the membrane\npotential negative.\n-\nTogether these channels give the neural\nmachinery flexible control of voltage!\n50-\n-------------\n> o~\nE --\n>\n-501--\n~ --------------\ntime (ms)\n\nWhere do the batteries of a neuron come from?\n1) Ion concentration gradients\n2) Ion-selective permeability of ion\nchannels\n~\n\\r,s·1\ntc>\n('ri,1- e ,·c\"-e\n✓ i.,.; f ~J\n,., ..\n. -,,\n\nNeurons have batteries\nTime\n[K]\n[K]out\n[K]in\n[K]0\n[K]inf=[K]0/2\n'Non-selective' pore\npasses all ions\n(a long time)\n------------~-~-~-~-~-!::!!!~-~== --➔\nj_\n\nNeurons have batteries\nWhy do the ions stop flowing\nfrom side 1 to side 2?\n[K]\n[K]out\n[K]in\n[K]0\nTime\n'Ion-selective' pore\npasses only K+ ions\nIK (t)\nTime\nWhy does the concentration\nstop changing here?\n(a short time)\n\nNeurons have batteries\n'Ion-selective' pore\npasses only K+ ions\nIK (t)\ntime\nΔV=V1-V2\ntime\nThe voltage difference changes in a\ndirection that opposes the flow of ions.\nEK\nIt reaches an 'equilibrium potential' at a\nvalue that gives zero net flow of ionic\ncurrent.\nThis voltage difference is a\nbattery for our model neuron!!\n\nNeurons have batteries\nThere will be some electric field strength such that the\n'drift' will exactly balance the diffusion produced by the\nconcentration gradient...\nK+\nK+\nK+\nK+\nK+\nK+\nK+\nE\nV(x)\nx\nVin\nVout\nNernst Potential\n]<.\n+-\nc1,- '\nI(> cl(\nci' k~\nj_\n\\YlSi~t'\nOvf5,Je\n\nNeurons have batteries\n- Where do the 'batteries' of a\nneuron come from?\n1) Ion concentration gradients\n2) Ion-selective pores (channels)\n- How big is the battery (how\nmany volts?)\nThis is determined by a balance\nbetween diffusion down a concentration\ngradient balanced by 'drift' in the\nopposing electric field.\n\nElectrodiffusion and the Nernst Potential\nOne can use Ohm's law and Fick's first law to derive the Nernst\npotential\nITot = IDrift + IDiffusion\n-- At this voltage, the drift current in the electric field exactly\nbalances current due to diffusion\n= 0\nIDrift =\nAq2φ(x)D\nkT\nΔV\nL\nOhm's Law\nIDiffusion =-AqD ∂φ\n∂x\nFick's First Law\nΔV = kT\nq ln φout\nφin\n⎛\n⎝\n⎜⎜⎜⎜\n⎞\n⎠\n⎟⎟⎟⎟\nat equilibrium\n\nDerive Nernst potential using the\nBoltzmann equation\nThe Boltzmann equation describes the ratio of probabilities of a\nparticle being in any two states, at thermal equilibrium:\nstate 1\nstate 2\nEnergy\nU2\nU1\nPstate1\nPstate2\n= 0\nkT = 0\nk =Boltzmann constant (J/K)\nT = temperature (K) = 273 + TC\nkT = thermal energy (J)\nPstate1\nPstate2\n= e\n-U1-U2\nkT\n⎛\n⎝⎜\n⎞\n⎠⎟\n\nDerive Nernst potential using the\nBoltzmann equation\nThe Boltzmann equation describes the ratio of probabilities of a\nparticle being in any two states, at thermal equilibrium:\nstate 1\nstate 2\nEnergy\nU2\nU1\n}kT\nPstate1\nPstate2\n> 0\nkT > 0\nk =Boltzmann constant (J/K)\nT = temperature (K)\nkT = thermal energy (J)\nPstate1\nPstate2\n= e\n-U1-U2\nkT\n⎛\n⎝⎜\n⎞\n⎠⎟\n\nThe Boltzmann equation describes the ratio of probabilities of a\nparticle being in any two states, at thermal equilibrium:\nPstate1\nPstate2\n≈0\nPstate1\nPstate2\n= e-2\nPstate1\nPstate2\n≈1.0\nΔU >> kT\nDerive Nernst potential using the\nBoltzmann equation\nstate 1\nstate 2\nEnergy\nU2\nU1\n}kT\nstate 1\nstate 2\nEnergy\nU1\n}kT\nΔU = 2kT\nU2\nstate 1\nstate 2\nEnergy\nU2\nU1\n}kT\nΔU < kT\nPstate1\nPstate2\n= e\n-U1-U2\nkT\n⎛\n⎝⎜\n⎞\n⎠⎟\n\nNernst Potential\nWe can compute the equilibrium potential using the Boltzmann\nequation:\nPin\nPout\n= e\n-Uin-Uout\nkT\nU = qV = electrical potential (J)\nq = 1.6x10-19C for monovalent ion\nq = charge of ion\n= e\n-q(Vin-Vout )\nkT\n\nNernst Potential\nWe can compute the equilibrium potential using the Boltzmann\nequation:\nkT\nq = 25mV for monovalent ion\nVin -Vout = -kT\nq ln\nPin\nPout\n⎛\n⎝⎜\n⎞\n⎠⎟\nΔV = Vin -Vout = 25mV ln\nPout\nPin\n⎛\n⎝⎜\n⎞\n⎠⎟\nΔV = 25mV ln\nK\n[ ]out\nK\n[ ]in\n⎛\n⎝⎜\n⎞\n⎠⎟\n= EK\nDon't get confused by this notation. EK is the\nequilibrium potential (voltage) for the K ion.\n'E' here does not refer to an electric field.\nU = qV = electrical potential (J)\nq = 1.6x10-19C for monovalent ion\nq = charge of ion\nPin\nPout\n= e\n-Uin-Uout\nkT\n= e\n-q(Vin-Vout )\nkT\n\nIon\nCytoplasm\n(mM)\nExtracellular\n(mM)\nNernst\n(mV)\nK+\n-75\nThe Nernst potential for potassium\nIntracellular and extracellular concentrations of ionic\nspecies, and the Nernst potential\nEk = kT\nq ln 20\n⎛\n⎝\n⎜⎜⎜\n⎞\n⎠\n⎟⎟⎟\n= 25mV at 300K (room temp)\nfor monovalent ion\nkT\nq\nEK = 25mV(-3.00)=-75mV\nK+\nK+\nK+\nK+\n\nIe\nC\nVm\nGK\nIc\nIK\nEK\n+\nHow to implement an ion specific conductance\nas a battery in our model neuron\n\nIntroduction to Neural\nComputation\nMichale Fee\nMIT BCS 9.40 -- 2018\nVideo Module on Nernst Potential\nPart 2\n\nIon\nCytoplasm\n(mM)\nExtracellular\n(mM)\nNernst\n(mV)\nK+\n-75\nThe Nernst potential for potassium\nIntracellular and extracellular concentrations of ionic\nspecies, and the Nernst potential\nK+\nK+\nK+\nK+\n= 25mV at 300K (room temp)\nfor monovalent ion\nkT\nq\nEK = 25mV(-3.00)=-75mV\nΔV = kT\nq ln\nK\n⎡⎣\n⎤⎦out\nK\n⎡⎣\n⎤⎦in\n⎛\n⎝\n⎜\n⎜\n⎞\n⎠\n⎟\n⎟\n\nIe\nC\nVm\nGK\nIc\nIK\nEK\n+\nHow to implement an ion specific conductance\nas a battery in our model neuron\n\nPotassium I-V relation\nOne of the best ways to study the function of an ion channel is\nto plot the current-voltage relation (I-V curve). This can be\nmeasured as the current required to hold the neuron at a given\nvoltage.\nNote that the current reverses at the equilibrium potential, so this\nis often referred to as the 'reversal potential'\n-\nIf you hold the cell below EK, then the\ncurrent will flow into the cell.\nFor a potassium conductance\n-\nIf you hold the voltage above the\nequilibrium potential, K current will flow out\nthrough the membrane (positive current)\nIK\nV\nVm\nEK\nGK\nI e\n\nI-V relation\nIK = GK (V -EK ) ,\nGK = R-1\nK\nThis relation turns out to be monotonic and\nroughly linear for ion channels in the open\nstate. So we can write:\nVm\nWe can model this as a battery in series with a resistor! Why?\nIK\nV\nEK\ndriving potential\nΔV = EK\n\nΔV = IK\nGK\nVm = EK + IK\nGK\nIK = GK(V -EK )\n\nOur equation is now:\n\nIK +C dV\ndt = Ie\nIe\nC\nVm\nGK\nIc\nIK\nEK\n+\nV +τ dV\ndt = EK + RKIe\nGK (V -EK )+C dV\ndt = Ie , RK = Gk\n-1 , τ = RKC\nV +τ dV\ndt =Vinf, Vinf= EK + RKIe\nVinf\nl\n\nResponse to current injection\nIe(t) 0\nI0\nVinf(t)= EK + RK Ie(t)\nVinf(t)\nEK\nEK + RK I0\ntime\nEK =-75mV\nEK\nVm(t)\nEK + RK I0\ntime\n\nA mathematical model of a neuron\n- Equivalent circuit model\ngNa\ngK\ngL\nEL\nEK\nENa\nIe\nVm\nC\n+\n+\n+\n-100\nTime (ms)\nVm\n0.1\n0.2\nTime (ms)\nAlan Hodgkin\nAndrew Huxley, 1952\n\nIon\nCytoplasm\n(mM)\nExtracellular\n(mM)\nNernst\n(mV)\nK+\n-75\nNa+\n+54\nIon\nCytoplasm\n(mM)\nExtracellular\n(mM)\nNernst\n(mV)\nK+\n-75\nNa+\nThe Nernst Potential is different for\ndifferent ions\nIntracellular and extracellular concentrations of ionic\nspecies, and the Nernst potential\nENa = 25mV ln 440\n⎛\n⎝⎜\n⎞\n⎠⎟= 25mV (2.17) = 54.3mV\nNa+\nNa+\nNa+\nNa+\n\nIon\nCytoplasm\n(mM)\nExtracellular\n(mM)\nNernst\n(mV)\nK+\n-75\nNa+\n+54\nCl-\n-59\nIon\nCytoplasm\n(mM)\nExtracellular\n(mM)\nNernst\n(mV)\nK+\n-75\nNa+\n+54\nCl-\nThe Nernst Potential is different for\ndifferent ions\nIntracellular and extracellular concentrations of ionic\nspecies, and the Nernst potential\nECl = -25mV ln 560\n⎛\n⎝⎜\n⎞\n⎠⎟= -25mV (2.38) = -59.4mV\nThe negative here comes from the negative charge of the Cl- ion (q=-e)\nCl-\nCl-\nCl-\nCl-\nt\n\nThe Nernst Potential is different for\ndifferent ions\nIon\nCytoplasm\n(mM)\nExtracellular\n(mM)\nNernst\n(mV)\nK+\n-75\nNa+\n+55\nCl-\n-59\nCa++\n10-4\n+124\nIntracellular and extracellular concentrations of ionic\nspecies, and the Nernst potential\nECa =12.5mV ln\n.0001\n⎛\n⎝\n⎜⎜⎜\n⎞\n⎠\n⎟⎟⎟=124mV\nWhy is this 12.5mV?\nCa++\nCa++\nCa++\nCa++\nt\n\nOutline of HH model\ngNa\ngK\ngL\nEL\nEK\nENa\nIe\nVm\nC\n+\n+\n+\nVoltage and time-dependent ion channels are\nthe 'knobs' that control membrane potential.\n-\nNa+channels push the membrane potential\ntoward +50mV.\n-\nK+ channels push the membrane potential\ntoward -80mV.\n-\nTogether these channels give the neural\nmachinery flexible control of voltage!\n-\n- for example to generate an action potential\n50-\n-------------\n> o~\nE --\n>\n-501--\n~ --------------\ntime (ms)\n\nIntroduction to Neural\nComputation\nMichale Fee\nMIT BCS 9.40\nVideo Module on Integrate and Fire\nNeuron\n\nOutline of HH model\ngNa\ngK\ngL\nEL\nEK\nENa\nIe\nVm\nC\n+\n+\n+\nVoltage and time-dependent ion channels are\nthe 'knobs' that control membrane potential.\n-\nNa+ conductance pushes the membrane\npotential toward +55mV.\n-\nK+ conductance pushes the membrane\npotential toward -75mV.\n-\nTogether these conductances (and\nbatteries) give the neuron flexible control\nof voltage!\n-\n- for example to generate an action potential\n50-\n-------------\n> o~\nE --\n>\n-501--\n~ --------------\ntime (ms)\n\ngNa\ngK\ngL\nEL\nEK\nENa\nIe\nVm\nC\n+\n+\n+\nWe are going to replace the fancy spike generating mechanism in a\nreal neuron with a simplified 'spike generator'.\nIntegrate and Fire model of a neuron\nGL\nEL\nIe\nVm\nC\n+\nSpike\ngenerator\nLouis Lapique, 1907\nKnight, 1972\n\nA simplified model of a neuron\n-\nAll spikes are the same. (No information carried in the details of action\npotential waveforms.)\n-\nWhile APs (spikes) are important, they are not what neurons spend most of\ntheir time doing. Spikes are very fast (~1ms in duration).\n-\nThis is much shorter than the typical interval between spikes (~100ms). Most\nof the time, a neuron is 'integrating' its inputs. (Separation of timescales)\nspikes as δ -functions\n250 ms\n-\nSpikes tend to occur when the voltage in a neuron reaches a particular\nmembrane potential, called the spike threshold.\n\nIntegrate and Fire model of a neuron\nGL\nEL\nIe\nVm\nC\n+\nSpike\ngenerator\nThe spike generator is very\nsimple. When the voltage\nreaches the threshold Vth, it\nresets the neuron to a hyper-\npolarized voltage Vres.\nLouis Lapique, 1907\nVth\nVres\nVm(t)\ntime\nRemoved due to copyright restrictions: Figure 2D1: Subthreshold\nmembrane potential oscillations in RA neuron. Mooney, R. \"Synaptic\nbasis for developmental plasticity in a birdsong nucleus.\" Journal of\nNeuroscience 1 July 1992, 12 (7) 2464-2477.\n\nf .r.= 1\nΔt\nIe\nV\nC\nGL\nEL\n+\nSpike\ngenerator\nΔt\nC dV\ndt = Ie\n- Let's calculate the firing rate of our neuron\nWe'll first consider the case\nwhere there is no leak.\nf =\nΔt =\nCΔV\n⎛\n⎝\n⎜⎜⎜\n⎞\n⎠\n⎟⎟⎟Ie\nVth\nVres\nΔV\nIe\nC ΔV\nΔt = Ie\nΔV =Vth -Vres\ndV\ndt = ΔV\nΔt\nIntegrate and Fire model of a neuron\n\nIe\nV\nC\nSpike\ngenerator\n- Let's calculate the firing rate of our neuron\nWe'll first consider the case\nwhere there is no leak.\nf =\nCΔV\n⎛\n⎝\n⎜⎜⎜\n⎞\n⎠\n⎟⎟⎟Ie\nf\nIe\nCΔV\nVth\nVres\nΔV\nIe\nIntegrate and Fire model of a neuron\n\nIe\nV\nC\nGL\nEL\n+\nSpike\ngenerator\nΔt\nf .r.= 1\nΔt\nNow we'll put our leak\nconductance back in.\nThink of this GL like a small\npotassium conductance that is\nconstantly on. It has no voltage\ndependence and no time\ndependence. EL = -75mV.\nVinf\nEL\nIe\nVth\nVres\nΔV\nWhat happens when\nVinf<Vth ?\nIntegrate and Fire model of a neuron\nVinf= EL + RL Ie\n\nLets calculate the injected current\nrequired to reach threshold (rheobase).\nτ = RLC\nIntegrate and fire with leak\nVth\nVres\nΔt\nVinf\nWhat happens just at threshold?\nThe time to reach threshold ( ) is:\n-\nvery long\n-\nvery sensitive to injected current\nΔt\nVinf=Vth\nEL + RL Ie =Vth\nIe = GL(Vth -EL)\nIe\nf.r.\nIth\nIth =\n\nτ = RLC\nV(t)-Vinf= (V0 -Vinf)e-t/τ\nVth -Vinf= (Vres -Vinf)e-Δt/τ\nIntegrate and fire with leak\ne-Δt/τ = Vinf-Vth\nVinf-Vres\nVth\nVres\nΔt\nVinf\nf = Δt-1 = τ ln Vinf-Vres\nVinf-Vth\n⎛\n⎝\n⎜⎜⎜⎜\n⎞\n⎠\n⎟⎟⎟⎟\n⎡\n⎣\n⎢⎢\n⎤\n⎦\n⎥⎥\n-1\nΔt =-τ ln Vinf-Vth\nVinf-Vres\n⎛\n⎝\n⎜⎜⎜⎜\n⎞\n⎠\n⎟⎟⎟⎟\n\nVinfVth,Vres\nAt high input currents, the solution\nhas a simple approximation\nIntegrate and fire\nf = τ ln Vinf-Vres\nVinf-Vth\n⎛\n⎝\n⎜⎜⎜⎜\n⎞\n⎠\n⎟⎟⎟⎟\n⎡\n⎣\n⎢⎢\n⎤\n⎦\n⎥⎥\n-1\nIth = GL Vth -EL\n(\n)\nf =\nCΔV Ie -Ith\n(\n)\nln(1+α)∼α\n\nIntegrate and fire\nThis equation is linear in injected current Ie , just like the case of no leak!\nIe\nf.r.\nIth\nCΔV\nf =\nCΔV Ie -Ith\n(\n)\nThe F-I curve of many neurons look approximately like this!\nLuo et al 2017\nFigure courtesy of Luo, et al. License: CC BY. Source: \"Comparison of the Upper Marginal Neurons of Cortical\nLayer 2 with Layer 2/3 Pyramidal Neurons in Mouse Temporal Cortex.\" Front. Neuroanat., 21 December 2017.\n\ngNa\ngK\ngL\nEL\nEK\nENa\nIe\nVm\nC\n+\n+\n+\nWe have replaced the fancy spike generating mechanism in a real\nneuron with a simplified 'spike generator'.\nIntegrate and Fire model of a neuron\nGL\nEL\nIe\nVm\nC\n+\nSpike\ngenerator\nLouis Lapique, 1907\nKnight, 1972\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu/\n\n9.40 Introduction to Neural Computation\nSpring 2018\n\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Lecture 4: Hodgkin Huxley Model Part 1 - 9.40 Introduction to Neural Computation",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-40-introduction-to-neural-computation-spring-2018/4458dad30259d17a3f1edf9d7f803fdf_MIT9_40S18_Lec04.pdf",
      "content": "Introduction to Neural\nComputation\nProf. Michale Fee\nMIT BCS 9.40 -- 2018\nLecture 4\n\nHodgkin-Huxley model of action potential\ngeneration\nVoltage and time-dependent ion channels are\nthe 'knobs' that control membrane potential.\ngNa\ngK\nENa\n+\nEK\n+\ngL\nVm\nC\nEL\nIe\n+\nRemoved due to copyright restrictions: Figure 1a: The first intracellular\nrecording of an action potential, from squid axon. Hausser, M. \"The Hodgkin-\nHuxley theory of the action potential.\" Nature Neuroscience 3 (2000).\n\nHodgkin-Huxley model of action potential\ngeneration\nGNa\nGK\nGL\nV\n+ ENa\nEK\nEL\nC\nIe\n+\n+\n|{z}\nIm = INa + IK + IL\nThis is the total membrane ionic current, and it includes the contribution\nfrom --sodium channels, potassium channels and a 'leak' conductance.\nThe equation for our HH model neuron is\n(t)+ C dV(t)\nIm\n= Ie (t)\ndt\n\nHodgkin-Huxley model of action potential\ngeneration\nGNa\nGK\nGL\nV\nC\nEK\nEL\n+ ENa\nIe\n+\n+\n|{z}\n(t)+ C dV (t)\nIm = INa + IK + IL\nIm\ndt = Ie (t)\nINa = GNa (V,t)(V - ENa )\nIK = GK (V,t)(V - EK )\nIL = GL (V - EL )\nWe can see that the membrane potential depends on current\n...which depends on all the conductances\n...which depend on the membrane potential\n\nWe are going to write down an algorithm for\nhow a neuron spikes!\nStart with V :\nm\nCompute Vinf and τ mem\nCompute total membrane current\nCompute conductance using voltage-dependent parameters\nCompute sodium and potassium current from conductances\nCompute voltage-dependent parameters using V m\nCompute membrane potential\nVm + τ dVm\ndt = Vinf\n\nLearning objectives for Lecture 2\n-\nTo be able to draw the circuit diagram of the HH model\n-\nUnderstand what a voltage clamp is and how it works\n-\nBe able to plot the voltage and time dependence of the\npotassium current and conductance\n-\nBe able to explain the time and voltage dependence of the\npotassium conductance in terms of Hodgkin-Huxley gating\nvariables\n\nOutline of HH model\ngNa\ngK\ngL\nV\n+ ENa\nEK\nEL\nC\nIe\n+\n+\n|{z}\nIm = INa + IK + IL\nI X = GX ⋅(V - EX )\nINa = GNa (V,t)(V - ENa )\nIK = GK (V,t)(V - EK )\nIL = GL (V - EL )\nThe sodium conductance is\nThe potassium conductance\nThe leak conductance is\ntime-dependent and\nis time-dependent and\nneither time-dependent\nvoltage-dependent\nvoltage-dependent\nnor voltage-dependent\nENa = +55mV\nEK = -75mV\nEL = -50mV\n\nVoltage and Time dependence\n-\nVoltage and time-dependent ion channel conductances are the\n'knobs' that control membrane potential.\n-\nH&H studied the properties of K and Na channels in the squid giant\naxon. In particular they wanted to study the voltage and time\ndependence of the K and Na channels.\n1mm diameter!\nMost axons in our brain are 1um dia\nImage of squid giant axon CKay Cooper and Roger\nHanlon. Used with permission.\nSquid diagram from The CellularScale.\nLicense CC BY-NC-SA.\n\nOutline of HH model\nThe best way study the time and voltage dependent conductance of\nionic channels is to suddenly 'set' the voltage at different values and\nmeasure the current required to hold that voltage.\nThis not easy, because as soon as\nyou depolarize the axon, the axon\nbegins to spike!\n0.1\n0.2\nTime (ms)\n-100\nTime (ms)\nVm\nGK\nIK\nV\nEK\nThen plot the I-V curve.\n\nVoltage Clamp\nRm\n+-\nVc\nVout\nVm\nIe\nRi\nVout = G(V+ -V-)\nThe basic equation of an op-amp is:\nwhere G is the gain, typically ~105 or 106,\nThe key component is an operational amplifier\n(op-amp)\n+-\nV+\nV-\nA voltage clamp is a device that holds the membrane potential of a cell\nto any desired 'command' voltage Vc, and measures the current\nrequired to hold that voltage.\nDon't get confused here. G is\ngain, not conductance!\n\nVoltage Clamp\nIe\nRm\n+-\nVc\nVout\nVm\nRi\n-\nThus, the voltage clamp circuit drives whatever current (Ie) is necessary to\n'clamp' the voltage of the neuron to the command voltage.\n-\nDuring a voltage clamp experiment, we step the Vc around within the\nvoltage range of interest and measure Ie .\n-\nIt is easy to show that, for large gain:\nVm Vc\nThis is called 'negative feedback'.\n-\nDrives current into neuron\n-\nIncreases membrane potential\nIf Vm < Vc then Vout >> 0\nIf Vm > Vc then Vout << 0\n-\nPulls current out of neuron\n-\nDecreases membrane potential\nVout = G(Vc -Vm)\n\nIonic currents\n+-\nVc\nVm\nIe\nRemoved due to copyright restrictions: Figure 2.6 p. 36 In: Hille, Bertil. Ion Channels\nof Excitable Membranes (3rd Ed.). 2001, Sinauer / Oxford University Press.\n\nIonic currents\nHow do we figure out the contribution of Na and the contribution of K?\nIonic substitution (e.g. replace NaCl with choline chloride)\nK current\nNa current\n\nVc\n-80 mV\n-40 mV\n0 mV\n40 mV\nVc\n-80 mV\n-40 mV\n0 mV\n40 mV\nK current\nNa current\ntime (ms)\nK current (mA/cm2)\n\n-40 mV\n0 mV\n40 mV\n-2.5\n-2\n-1.5\n-1\n-0.5\n0.5\ntime (ms)\nNa current (mA/cm2)\n\n-40 mV\n0 mV\n40 mV\nIonic currents\n\nIonic currents\nK current\nNa current\n-5\n-50\nEH\nIK\nEm (mV)\nIm(mA/cm2)\nV\nIK\n-5\n-50\nEm\nEH\n(mV)\nINa\nIm(mA/cm2)\nV\nINa\ntime (ms)\nK current (mA/cm2)\n\n-40 mV\n0 mV\n40 mV\n-2.5\n-2\n-1.5\n-1\n-0.5\n0.5\ntime (ms)\nNa current (mA/cm2)\n\n-40 mV\n0 mV\n40 mV\n\nINa(V)= GNa(V) V -ENa\n(\n)\nIK(V)= GK(V) V -EK\n(\n)\nIonic currents (voltage dependence)\nWe used the voltage clamp to measure current as a function of\nvoltage.\nGK(V) ≡\nIK(V)\nV -EK\n(\n)\nGNa(V) ≡\nINa(V)\nV -ENa\n(\n)\nBut what we are really trying to extract is conductance as\na function of voltage!\n-5\n-50\nEH\nIK\nEm (mV)\nIm(mA/cm2)\nV\nIK\n-5\n-50\nEm\nEH\n(mV)\nINa\nIm(mA/cm2)\nV\nINa\n\nINa = GNa(V) V -ENa\n(\n)\nGNa(V)\nIK = GK(V) V -EK\n(\n)\nGK(V)\nIonic currents (voltage dependence)\nV -EK\n(\n)\nV\nV -ENa\n(\n)\nV\n-5\n-50\nEH\nIK\nEm (mV)\nIm(mA/cm2)\nV\nIK\n-5\n-50\nEm\nEH\n(mV)\nINa\nIm(mA/cm2)\nV\nINa\n\nIonic currents (voltage dependence)\nGNa(V)\nGNa\nMaximal\nconductance\nV\nSigmoidal voltage-dependence\nof activation\nGK(V)\nGK\nMaximal\nconductance\nV\n-5\n-50\nEH\nIK\nEm (mV)\nIm(mA/cm2)\nV\nIK\n-5\n-50\nEm\nEH\n(mV)\nINa\nIm(mA/cm2)\nV\nINa\n\nConductance\n(relative to max)\nNa conductance\nConductance\n(relative to max)\nK conductance\nIonic currents (Voltage dependence)\nGNa(V)\nGK(V)\nGK\nGNa\nMaximal\nconductance\nMaximal\nconductance\nV\nV\nG(V) ∝\n1+ eα-βVm\nG(V) ∝\n1+ eα-βVm\n\nIonic currents (time dependence)\nIK(t)= GK(t) V -EK\n(\n)\nINa(t)= GNa(t) V -ENa\n(\n)\nGK(t)\nTime after start\nof pulse (ms)\nactivation\nGNa(t)\nTime after start\nof pulse (ms)\nactivation\ninactivation\nAnnotated figures on pages 19, 20, 22, 24 and 36 (c) Hille, Bertil. Ion Channels of Excitable Membranes (3rd Ed.). 2001, Sinauer / Oxford University Press. All\nrights reserved. This content is excluded from our Creative Commons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nOutline of HH model\ngNa\ngK\ngL\nEL\nEK\nENa\nIe\nVm\nC\n+\n+\n+\nVoltage and time-dependent ion channels are\nthe 'knobs' that control membrane potential.\n-\nSome ion channels push the membrane\npotential positive.\n-\nOther ion channels push the membrane\npotential negative.\n-\nTogether these channels give the neural\nmachinery flexible control of voltage!\n\nIonic currents\n(time and voltage dependence)\nGK(V,t)\nGNa(V,t)\nTime after start of pulse (ms)\nSpecific conductance\n(full scale =20 mS/cm2)\nSpecific conductance\n(full scale =20 mS/cm2)\nDelayed\nactivation\nFast\nactivation\n\nIonic Currents\n(time and voltage dependence)\n-\nNow we are going to develop a detailed understanding of\nwhere these voltage and time-dependencies come from!\n-\nWe are going to derive equations for the voltage dependence\n-\nAnd differential equations for the time dependence.\n-\nOnce we do that, we will be able to write down a system of\nequations that allows us to simulate an action potential!\n\nEach of these\nshows the\ncurrent on a\nsingle trial\nAverage across all\nthe trials\n'ensemble average'\nSingle channel K-current\nVm(t)\n-100\n+50\nTime after start of pulse (ms)\nSingle Channels\nSo far, we have been discussing total currents into a neuron. However,\nthese total currents result from ionic flow through thousands of individual\nion channels.\nIt is possible to record\nfrom single ion channels\nusing a 'patch clamp'.\nVm\n+-\nVc\nIe\nNeher & Sakmann\nSingle channel Na-current\nVm(t)\n-80\n-40\nTime after start of pulse (ms)\n\nIndividual ion channels are either OPEN or CLOSED.\nIonic conductance in terms of single channels\nThe total conductance through a membrane is given by the total\nnumber of open channels times the conductance one ion\nchannel.\nAll of the interesting time and voltage dependence\ncomes from here\nunitary conductance\nˆgK =\nNK =\ntotal number of ion channels\nPK =\nprobability of being 'open'\n= PKNK\nNumber of 'open' ion\nchannels\nGK = PK(V, t) NK ˆgK\nTotal K conductance:\nIK = GK(V, t) (V -EK )\nRemember, K current is:\n\nHow do we describe the probability that a channel is open?\nEach subunit has a voltage sensor and gate to\nturn the channel on and off.\nsubunit\npore\nThe pore of a K+ channel is formed by 4 identical subunits.\nLets start with the K channel.\nIonic conductance in terms of single channels\nEach subunit has two states: 'open' and 'closed'.\n'n' is the probability that a subunit is open.\nFinally, all subunits must be in the 'open' state for\nthe channel to be permeable.\n\nHow do we describe the probability that a channel is open?\nsubunit\npore\nPK = n4\nAssuming\nindependence\nIonic conductance in terms of single channels\nIf 'n' is the probability that one subunit is open, then\nthe probability that all four subunits is open is given\nby:\nH & H called the\n'gating variable' for the\npotassium current\nn\nAnd we can write the K current as:\nIK = GKn4(V -EK )\nWe can now write down the conductance of our K channels as:\nGK = GKn4\nGK is the maximal open\nconductance\n\nIon selective pore\nGating charges\n(voltage sensor)\n+ + + + + +\nLipid bilayer\nGate\nOPEN\nCLOSED\nIonic conductance in terms of single channels\nIntracellular\nExtracellular\n+ + + + + +\nVin = -75mV\nVout = 0mV\nWe are going see how to derive\nthe voltage-dependence from first\nprinciples!\nGK(V)\nV\n\nIon channels are stochastic. They are either open or closed, and\nflicker back and forth between the open and closed states.\nEnergy\nΔU = w\nopen\nclosed\nIonic conductance in terms of single channels\nLets see if we can predict the voltage-dependence of an ion\nchannel!\n+ + + + + +\nOPEN\nVin = 0mV\nVout = 0mV\nΔU = Uopen -Uclosed\nVm = 0mV , Popen ≈1\n}kT ! ΔU\n\nIon channels are stochastic. They are either open or closed, and\nflicker back and forth between the open and closed states.\nopen\nclosed\nIonic conductance in terms of single channels\nLets see if we can predict the voltage-dependence of an ion\nchannel!\nVin = 0mV\nVout = 0mV\n+ + + + + +\nCLOSED\nEnergy\nVm = 0mV , Popen ≈1\nΔU = w\nΔU = Uopen -Uclosed\n}kT ! ΔU\n\nIon channels are stochastic. They are either open or closed, and\nflicker back and forth between the open and closed states.\nopen\nclosed\nIonic conductance in terms of single channels\nLets see if we can predict the voltage-dependence of an ion\nchannel!\nVin = -75mV\nEnergy\nVin\nVout\nUg = qgV(x)\nx\nVout = 0mV\n+ + + + + +\nCLOSED\n\nIon channels are stochastic. They are either open or closed, and\nflicker back and forth between the open and closed states.\nopen\nclosed\nIonic conductance in terms of single channels\nLets see if we can predict the voltage-dependence of an ion\nchannel!\nEnergy\nVin\nVout\n+ + + + + +\nOPEN\nVout = 0mV\nVin = -75mV\nUg = qgV(x)\n\nIon channels are stochastic. They are either open or closed, and\nflicker back and forth between the open and closed states.\nopen\nVm = -75mV,Popen = small\nΔU = w -ˆqgVm\n}kT ≪ΔU\nclosed\nIonic conductance in terms of single channels\nLets see if we can predict the voltage-dependence of an ion\nchannel!\nEnergy\n+ + + + + +\nOPEN\nVout = 0mV\nΔU = Uopen -Uclosed\nVin\nVout\nVin = -75mV\nEffective\ngating\ncharge\n\nIon channels are stochastic. They are either open or closed, and\nflicker back and forth between the open and closed states.\nopen\nVm = -75mV,Popen = small\n}kT ≪ΔU\nclosed\nIonic conductance in terms of single channels\nLets see if we can predict the voltage-dependence of an ion\nchannel!\nEnergy\nΔU = Uopen -Uclosed\nVin\nVout\nVin = -75mV\nVout = 0mV\n+ + + + + +\nCLOSED\nΔU = w -ˆqgVm\nEffective\ngating\ncharge\n\nTo derive the voltage dependence:\nboltzmann equation\nWe can use the Boltzmann equation to describe the ratio of\nprobabilities of being in the open or closed state:\nPopen\nPclosed\n= e\n-Uopen-Uclosed\nkT\n⎛\n⎝⎜\n⎞\n⎠⎟\nThe probability of having an open subunit is:\n=\n1+ e\n(w-qgVm )/kT\nPopen\nPclosed\n= e\n-w-qgVm\nkT\n⎛\n⎝⎜\n⎞\n⎠⎟\n=\nPo\nPo + Pc\n=\n1+ Pc\nPo\nn = P0\n\n-100\n.01\n.02\n.1\n.2\nVoltage (mV)\n-100\n.01\n.02\n.1\n.2\nVoltage (mV)\nTo derive the voltage dependence:\nboltzmann equation\nIs this the observed voltage-dependence?\nConductance\n(relative to max)\nK conductance\nPo(V) =\n1+ e\n(w-qgVm )/kT\nP0(V) measured\nP0(V) Boltzmann\nqg = 4.5e\nw = -1.7kT\nYES!\n\nNote that for any given subunit or channel, when we change the\nvoltage, the energy levels shift (nearly) instantaneously. The\nprobability of making a transition changes instantly, however the\nnumber of open channels does not changes instantly.\nWe still have to wait for thermal fluctuations to kick the channel or\nsubunit open, or wait for the subunit to make a conformational\nchange.\nThis takes time!\nWe are going to model the transitions between open and closed states\nwith a simple rate equation. We can do this because we have many\nchannels to average over.\nProbability per unit time;\nunits are 1/s\n'closed'\nαn\nβn\n\n'open' α n,βn are transition rates\n1-n\nn\nNow get the time dependence!\nvoltage-\ndependent!\n\n'closed'\nαn\nβn\n\n'open' α n,βn are transition rates\n1-n\nn\nChange in the number\nof open subunits\nThe number of closed\nsubunits that open\nThe number of open\nsubunits that close\n=\n-\nThe number of closed subunits\n(1-n)\ntimes\nthe probability that a closed\nsubunit opens\nper unit time (αn)\nThe number of open subunits\n(n)\ntimes\nthe probability that an open\nsubunit closes\nper unit time (βn)\nChange in the number of\nopen subunits\nper unit time\n=\n-\ndn\ndt\n=\nα n(1-n)\n-\nβnn\n\ndn\ndt = α n(1-n) -βnn\nThus, we can rewrite this equation in terms of the\nsteady state open probability and a time constant:\nτ n\ndn\ndt = ninf-n\nLet's rewrite this equation as follows:\n= α n -α nn -βnn\n= α n -α n + βn\n(\n)n\nα n + βn\n(\n)\ndn\ndt =\nα n\nα n + βn\n(\n)\n-n\nninf=\nα n\n(α n + βn)\nThe steady state solution!\nτ n =\n(α n + βn)\nA time constant!\nα n,βn,ninf,τ n\nare all voltage dependent\nRemember...\nBut we just derived !\nninf(V)\nRemember: n is the\nprobability that a subunit is\nopen.\n\nResponse to voltage change\nHow does the gating variable 'n' change as we step the\nmembrane potential?\nVm(t) -80\nninf(t)= ninfV(t)\n[\n]\nninf(t)\nninf(-80)\nninf(0)\nτn(V =-80)\nτn(V = 0)\ntime\nn(t)\nactivation\ndisactivation\n\nVm(t) -80\ntime\nn(t)\nHow does the 'open' probability change as we step the\nmembrane potential?\n-\nThe shape of the K conductance was well fit by a rising\nexponential raised to the fourth power.\n-\nH & H inferred from this that the K-current was governed by\nfour independent first-order processes! (They didn't know\nabout the structure of K-channels at the time!)\nResponse to voltage change\nPK(t)= n(t)\n[\n]\nGK(t)\nTime after start of pulse\n(ms)\n\nMeasuring the parameters\n-80\ntime\nn(t)\nPK(t)= n(t)\n[\n]\n-\nBy measuring the persistent\nconductance at different voltages,\nthey were able to measure ninfas a\nfunction of voltage.\nMembrane potential (mV)\n-50\n.5\n1 ninf(V)\n-\nBy measuring the time course of the\nconductance at onset and offset of\nthe voltage steps, they were able to\nmeasure τn as a function of voltage.\nMembrane potential (mV)\nTime constant (ms)\n-50\nτn(V)\n\nHodgkin and Huxley summarized their data using algebraic\nexpressions for the rate functions α n(V) , βn(V)\nα n(V) =\n1-exp -0.1(V + 55)\n(\n)\nβn(V) = 0.125exp -0.0125(V + 65)\n(\n)\nV is in mV\nα n,βn are in ms-1\nMeasuring the parameters\nninf=\nα n\n(α n + βn)\nτ n =\n(α n + βn)\nRemember that...\n\nWhy did we do all of this?\nMeasuring the parameters\nninf(V) and τ n(V)\nBecause once we have expressions for ,\nwe can integrate the differential equation for :\nτ n\ndn\ndt = ninf-n\nn\nGK = GKn4\nto get the potassium conductance:\nIK = GKn4(V -EK )\nand the potassium current:\n\ndn\ndt\nn(t)\nIntegrate one time step to get\nCompute ninf(V) and τ n(V)\nCompute K current: IK = GKn4(V -EK )\nCompute total membrane current Im = IK + INa + IL\nIntegrate to get at next time step\ndVm\ndt\nVm\nStart with at time step t\nVm\nCompute Vinf\nWe are going to write down an algorithm for\nhow a neuron spikes!\nFor now here the parts related to the potassium current...\n\nLearning objectives for Lecture 2\n-\nTo be able to draw the circuit diagram of the HH model\n-\nUnderstand what a voltage clamp is and how it works\n-\nBe able to plot the voltage and time dependence of the\npotassium current and conductance\n-\nBe able to explain the time and voltage dependence of the\npotassium conductance in terms of Hodgkin-Huxley gating\nvariables\n\nIm = INa + IK + IL\nOutline of HH model\ngNa\ngK\ngL\nEL\nEK\nENa\nIe\nV\nC\n+\n+\n+\n|{z}\nINa = GNa(V,t) V -ENa\n(\n)\nThe sodium conductance is\ntime-dependent and\nvoltage-dependent\nENa = +55mV\nIK = GK(V,t)(V -EK )\nThe potassium conductance\nis time-dependent and\nvoltage-dependent\nEK =-75mV\nIL = GL(V -EL)\nThe leak conductance is\nneither time-dependent\nnor voltage-dependent\nEL =-50mV\n\nIm(t)+C dV(t)\ndt\n= Ie(t)\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu/\n\n9.40 Introduction to Neural Computation\nSpring 2018\n\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Lecture 5: Hodgkin Huxley Model Part 2 - 9.40 Introduction to Neural Computation",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-40-introduction-to-neural-computation-spring-2018/aaa92d26e832b661a850e02fbf3bd9bc_MIT9_40S18_Lec05.pdf",
      "content": "Introduction to Neural\nComputation\nProf. Michale Fee\nMIT BCS 9.40 -- 2018\nLecture 5\n\ngNa\ngK\ngL\nEL\nEK\nENa\nIe\nVm\nC\n+\n+\n+\nVoltage and time-dependent ion channels are\nthe 'knobs' that control membrane potential.\nHodgkin-Huxley model of action potential\ngeneration\nRemoved due to copyright restrictions: Figure 1a: The first intracellular\nrecording of an action potential, from squid axon. Hausser, M. \"The Hodgkin-\nHuxley theory of the action potential.\" Nature Neuroscience 3 (2000).\n\nIm = INa + IK + IL\nIm(t)+C dV(t)\ndt\n= Ie(t)\nThis is the total membrane ionic current, and it includes the contribution\nfrom --sodium channels, potassium channels and a 'leak' conductance.\nGNa\nGK\nGL\nEL\nEK\nENa\nIe\nV\nC\n+\n+\n+\n|{z}\nThe equation for our HH model neuron is\nHodgkin-Huxley model of action potential\ngeneration\n\nVoltage and Time dependence\n-\nVoltage and time-dependent ion channels are the 'knobs' that\ncontrol membrane potential.\n-\nH&H studied the properties of K and Na channels in the squid giant\naxon. In particular they wanted to study the voltage and time\ndependence of the K and Na channels.\n1mm diameter!\nHodgkin and Huxley, 1938\nImage of squid giant axon CKay Cooper and Roger\nHanlon. Used with permission.\nSquid diagram from The CellularScale.\nLicense CC BY-NC-SA.\nRemoved due to copyright restrictions: Figure 1a: The\nfirst intracellular recording of an action potential, from\nsquid axon. Hausser, M. \"The Hodgkin-Huxley theory\nof the action potential.\" Nature Neuroscience 3 (2000).\n\nIonic currents\n+-\nVc\nVm\nIe\nRemoved due to copyright restrictions: Figure 2.6 p. 36 In: Hille, Bertil. Ion Channels\nof Excitable Membranes (3rd Ed.). 2001, Sinauer / Oxford University Press.\n\nIonic currents\nHow do we figure out the contribution of Na and the contribution of K?\nIonic substitution (e.g. replace NaCl with choline chloride)\nK current\nNa current\nAnnotated figure (c) Hille, Bertil. Ion Channels of Excitable Membranes (3rd Ed.). 2001, Sinauer / Oxford University Press. All rights reserved.\nThis content is excluded from our Creative Commons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nVc\n-80 mV\n-40 mV\n0 mV\n40 mV\nVc\n-80 mV\n-40 mV\n0 mV\n40 mV\nK current\nNa current\ntime (ms)\nK current (mA/cm2)\n\n-40 mV\n0 mV\n40 mV\n-2.5\n-2\n-1.5\n-1\n-0.5\n0.5\ntime (ms)\nNa current (mA/cm2)\n\n-40 mV\n0 mV\n40 mV\nIonic currents\n\nIonic currents\nK current\nNa current\n-5\n-50\nEH\nIK\nEm (mV)\nIm(mA/cm2)\nV\nIK\n-5\n-50\nEm\nEH\n(mV)\nINa\nIm(mA/cm2)\nV\nINa\ntime (ms)\nK current (mA/cm2)\n\n-40 mV\n0 mV\n40 mV\n-2.5\n-2\n-1.5\n-1\n-0.5\n0.5\ntime (ms)\nNa current (mA/cm2)\n\n-40 mV\n0 mV\n40 mV\n\nINa = GNa(V) V -ENa\n(\n)\nGNa(V)\nIK = GK(V) V -EK\n(\n)\nGK(V)\nIonic currents (Voltage dependence)\nV -EK\n(\n)\nV\nV -ENa\n(\n)\nV\n-5\n-50\nEH\nIK\nEm (mV)\nIm(mA/cm2)\nV\nIK\n-5\n-50\nEm\nEH\n(mV)\nINa\nIm(mA/cm2)\nV\nINa\n\nIonic currents\n(time and voltage dependence)\nGK(V,t)\nGNa(V,t)\nTime after start of pulse (ms)\nSpecific conductance\n(full scale =20 mS/cm2)\nSpecific conductance\n(full scale =20 mS/cm2)\nDelayed\nactivation\nFast\nactivation\nAnnotated figure (c) Hille, Bertil. Ion Channels of Excitable Membranes (3rd Ed.). 2001, Sinauer / Oxford University Press. All rights reserved.\nThis content is excluded from our Creative Commons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nIon selective pore\nLipid bilayer\nGate\nCLOSED\nVoltage-dependent conductance use voltage sensors\nIntracellular\nExtracellular\n+ + + + + +\nVout = 0mV\nVin = 0mV\nVin = -75mV\n\nIon selective pore\nLipid bilayer\nGate\nOPEN\nVoltage-dependent conductance use voltage sensors\nIntracellular\nExtracellular\n+ + + + + +\nVin = 0mV\nVout = 0mV\n\nK and Na conductances\nWe modeled changes in conductance as transitions between 'closed'\nand 'open' states of ion channels.\n'closed'\nαn\nβn\n\n'open'\n1-n\nn\nK-conductance\nτ n\ndn\ndt = ninf-n\nMembrane potential (mV)\n-50\n.5\n1 ninf(V)\nninf=\nα n\n(α n + βn)\n'closed'\nαm\nβm\n\n'open'\n1-m\nm\nNa-conductance\nMembrane potential (mV)\n-50\n.5\n1 minf(V)\nτ m\ndm\ndt = minf-m\nminf=\nαm\n(αm +βm)\n\nThe activation of both Na and K conductances is\nrepresented by 'gating variables' m and n\nGating variables\nVm(t) -80\ntime\nn(t)\nPK(t)= n(t)\n[\n]\nninf(t)\nPNa ∝m(t)\n[\n]\nGNa(t)\ntime\nm(t)\n-80\nminf(t)\nVm(t)\nK-conductance\nNa-conductance\nPNa = m3h\nInactivation\nInactivation\ngating variable\nActivation\ngating variable\n\nSodium channel inactivation\nCLOSED\n(intactivated)\nVm = +10mV\n+\nIntracellular\nExtracellular\nInactivation\nparticle\nOPEN\n+\nVm = -75mV\n\nE\nHH postulated an additional voltage-dependent\ninactivation gate.\n'closed'\nαh\nβh\n\n'open'\n1-h\nh\nτ h\ndh\ndt = hinf-h\n\nSodium channel inactivation\nDynamics of inactivation are captured by a new gating\nvariable 'h'.\nGNa(V,t)\nSpecific conductance\n(full scale =20 mS/cm2)\nVm(t) -80\ntime\nGNa(t)\nh(t)\ninactivation\ndisinactivation\nτ h\ndh\ndt = hinf-h\nPNa = m3h\nAnnotated figure (c) Hille, Bertil. Ion Channels of Excitable Membranes\n(3rd Ed.). 2001, Sinauer / Oxford University Press. All rights reserved.\nThis content is excluded from our Creative Commons license. For more\ninformation, see https://ocw.mit.edu/help/faq-fair-use/.\n\nMeasuring the parameters\nHow do we measure inactivation and recovery from inactivation?\nMembrane potential (mV)\n-50\n0.5\nhinf(V)\nVm(t)\n-80mV\n-70\n-60\n-50\n+40mV\nINa(t)\n1.\nHold Vm at different values\n2.\nLet the Na channels inactivate\n3.\nThen measure the Na current!\nholding\npotential\nmeasurement\nstep\nτ h\ndh\ndt = hinf-h\n\nPutting our two Na-channel gating variables together, we get:\nNote independence\nGNa = GNam3h\nThe sodium conductance is:\nINa = GNam3h(V -ENa)\nAnd the sodium current is:\nThe sodium conductance\nPNa = m3h\nThe probability of having a Na channel open is:\nNOT !\nBut it's not so\nbad\n\nPutting it all together!\nStart with initial contition\nat time step t0\nVm = V0\nCompute τ mem and Vinf\nCompute:\nninf(V) and τ n(V)\nminf(V) and τ m(V)\nhinf(V) and τ h(V)\nm(t) = m(t -1)+ dm\ndt Δt\nn(t) = n(t -1)+ dn\ndt Δt\nh(t) = h(t -1)+ dh\ndt Δt\nIK = GKn4(V -EK )\nINa = GNam3h(V -ENa)\nIL = GL(V -EL)\nTotal membrane current\nIm = IK + INa + IL\nVm(t) = Vm(t -1)+ dVm\ndt Δt\n\n-100\n-50\nV(mV)\ntime (ms)\nPutting it all together!\nm(t)\nn(t)\nh(t)\n-100\n-50\nV(mV)\ntime (ms)\ng (mS/cm2)\ntime (ms)\nGNa(t)\nGK(t)\ntime (ms)\n\nSpike refractory period\ntime(ms)\nV (mV)\n-100\n-50\nV\nIe\n\nSpike refractory period due to sodium\nchannel inactivation\ntime(ms)\nV (mV)\n-100\n-50\ntime(ms)\n0.2\n0.4\n0.6\n0.8\nh\nm\nV\nIe\n\nFainting Goats Video from National Geographic\nDiseases related to defects\nin sodium channel inactivation\n\nHyperkalemic Periodic Paralysis - Hyper PP\nDiseases related to defects\nin sodium channel inactivation\nSee Lecture video to view clip\n\nStructure of Muscle Fiber\n2-6 cm\n50-100 μm\nmyofibrils\nSarcolemma (muscle fiber membrane)\nfilament\nTransverse\ntubules\nSarcolemma (muscle fiber membrane)\nAnnotated Figure (c) Kandel, E.R, J.H. Schwartz, and T.M Jessell. Principles of Neural Science 3rd ed. 1991, McGraw-Hill.\n\nMuscle Fiber AP Leads to Ca Release in\nMyofibrils\nAnnotated Figure (c) Kandel, E.R, J.H. Schwartz, and T.M Jessell. Principles of Neural Science 3rd ed. 1991, McGraw-Hill.\n\nMyotonia and Periodic Paralysis are\nassociated with mutations of the Na channel\n(skeletal isoform only)\nDiseases related to defects\nin sodium channel inactivation\nFigure removed due to copyright restrictions. See Figure 2: Cannon, S. \"Sodum Channel Defects in Myotonia and\nPeriodic Paralysis.\" Annu. Rev. Neurosci. 19 (1996):141-44.\n\nSodium channel mutations\nwild-type\nhuman M1592V mutation\nFigure removed due to copyright restrictions. See Figure 3: Cannon, S. \"Sodum Channel Defects in Myotonia and\nPeriodic Paralysis.\" Annu. Rev. Neurosci. 19 (1996):141-44.\n\nDiseases related to defects\nin sodium channel inactivation\nSea anemone toxin (ATXII, 10uM) partially\nblocks sodium channel inactivation.\nFigure removed due to copyright restrictions. See Figure 5a: Cannon, S. \"Sodum Channel\nDefects in Myotonia and Periodic Paralysis.\" Annu. Rev. Neurosci. 19 (1996):141-64.\nSea anemone image is in the public domain.\nSource: heartypanther on Flickr.\n\nSea anemone toxin (ATXII) also prolongs\nmuscle fiber twitch duration.\nDiseases related to defects\nin sodium channel inactivation\nFigure removed due to copyright restrictions. See Figure 5b: Cannon, S. \"Sodum Channel\nDefects in Myotonia and Periodic Paralysis.\" Annu. Rev. Neurosci. 19 (1996):141-64.\n\nSea anemone toxin (ATXII) prolongs spiking\nin muscle fiber.\nDiseases related to defects\nin sodium channel inactivation\n40-\nDetubulated\n0 '\nStimulus\nmV\n-40 ,\n-80\n)\nI\n60ms\nOsmotic shock breaks T-tubules\nand eliminates myotonic run\n40-\n20.\nmv -20\n-40\n-60\n-eo,,\n60 msec\nControl\nStimulus\n40 nA\n50 nA\n80 msec\n-\nATXII\nMyotonic\nrun\nAnnotated figure (c) Cannon, S. \"Sodum Channel Defects in Myotonia and Periodic Paralysis.\" Annu. Rev. Neurosci. 19 (1996):141-64. All rights reserved.\nThis content is excluded from our Creative Commons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nDiseases related to defects\nin sodium channel inactivation\nHypothesis for how persistent sodium leads to persistent\nmuscle activation.\nT-tubule membrane\nMuscle fiber membrane\nMotor neuron\nsynapse\nNa+\nK+\nNa+\nK+\n\nDiseases related to defects\nin sodium channel inactivation\nHypothesis for how persistent sodium leads to persistent\nmuscle activation.\nT-tubule membrane\nMuscle fiber membrane\nMotor neuron\nsynapse\nK+\nK+\nK+\n25μm\nτ = x2\n2D\nτ = 300 -400ms\n\nDiseases related to defects\nin sodium channel inactivation\nHypothesis for how persistent sodium leads to persistent\nmuscle activation.\nT-tubule membrane\nMuscle fiber membrane\nMotor neuron\nsynapse\nNa+\nK+\nNa+\nK+\nFailure of Na to\ninactivate leads to...\n... extra spikes!\n\nDiseases related to defects\nin sodium channel inactivation\nHypothesis for how persistent sodium leads to persistent\nmuscle activation.\nT-tubule membrane\nMuscle fiber membrane\nMotor neuron\nsynapse\nK+\nK+\nK+\nK+\nK+\nK+\nK+\nK+\nK+\nK+\nK+\nExtra spiking leads\nto...\n... accumulation\nof extra K+ ions in\nT-tubules\n\nDiseases related to defects\nin sodium channel inactivation\nHypothesis for how persistent sodium leads to persistent\nmuscle activation.\nT-tubule membrane\nMuscle fiber membrane\nMotor neuron\nsynapse\nK+\nK+\nK+\nK+\nK+\nK+\nK+\nVm\n... leads to even\nmore spiking!\n\nsurface membrane\na)\nCZ\nL-\nE\n.0E\na)\n.0\n--a)\nFL\nRa\nI\n7I\nj\nCm\nj9Na\n9K\n9L\nV\nENaT EK\nT EL\nLL\nA\n9K\nEK\n9Na\nENa\n.VT\n-j-\n--.O-\nYIIN\nFIGURE\nEquivalent circuit diagram for the\nmodel of the electrical behavior of a muscle fiber.\nThe voltage and time dependence of the variable\nconductances are given by Eqs. 6, 7, 9, and 10.\n,y is the ratio of the T-tubular membrane area to\nsurface membrane area. The qs represent the den-\nsity of ion channels in the T-tubular membrane\nrelative to that of the surface membrane.\nDiseases related to defects\nin sodium channel inactivation\nEquivalent circuit model of muscle fiber membrane and T-\ntubule.\nCourtesy of Elsevier, Inc., https://www.sciencedirect.com. Used with permission.\n\nDiseases related to defects\nin sodium channel inactivation\nHypothesis for how persistent sodium leads to persistent\nmuscle activation.\nT-tubule membrane\nMuscle fiber membrane\nMotor neuron\nsynapse\nK+\nK+\nK+\nd K\n[ ]T\ndt\n= -K\n[ ]T -K\n[ ]o\nτK\nK\n[ ]T\nK\n[ ]o\nτK = 350ms\n\nDiseases related to defects\nin sodium channel inactivation\nHypothesis for how persistent sodium leads to persistent\nmuscle activation.\nMuscle fiber membrane\nMotor neuron\nsynapse\nK+\nK+\nK+\nK+\nd K\n[ ]T\ndt\n= 1\nξ\nF IK\nξ = volume of T-tubule\nIK = K-current into T-tubule\nd K\n[ ]T\ndt\n∝IK\nd K\n[ ]T\ndt\n= 1\nξF GK(V -EK )-K\n[ ]T -K\n[ ]o\nτK\nIK = GK(V -EK )\nEK is a function of [K]T !!\nF = Faraday constant (C/mol)\n= 9.6x104 (C/mol)\n\nComputer model of effects of defective Na-\nchannel inactivation\na\n-60\n-80 -\n100 200\nmsec\nf = 0.02\n- 500\n400 N\nE\n200 E\n.-\nc\n100 u,\n(myotonia)\nL o\nI\nI\nI\nI\nI\nI\nI\nI\n100 200 300 400 500 600 700\nmsec\nDiseases related to defects\nin sodium channel inactivation\nFraction of Na channels that fail to\ninactivate\nFailure to inactivate was modeled by\nsettting h=1 for a fraction of the\nchannels\nAnnotated figure (c) Cannon, S. \"Sodum Channel Defects in Myotonia and Periodic Paralysis.\" Annu. Rev. Neurosci. 19 (1996):141-64. All rights reserved.\nThis content is excluded from our Creative Commons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nComputer model of effects of defective Na-channel\ninactivation showing transition from myotonia to paralysis\nDiseases related to defects\nin sodium channel inactivation\nFigure removed due to copyright restrictions. Figue 6: Cannon, S. \"Sodum Channel\nDefects in Myotonia and Periodic Paralysis.\" Annu. Rev. Neurosci. 19 (1996):141-64.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu/\n\n9.40 Introduction to Neural Computation\nSpring 2018\n\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Lecture 6: Dendrites - 9.40 Introduction to Neural Computation",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-40-introduction-to-neural-computation-spring-2018/65ad58a9a37d79e863208745f30c264d_MIT9_40S18_Lec06.pdf",
      "content": "Introduction to Neural\nComputation\nProf. Michale Fee\nMIT BCS 9.40 -- 2018\nLecture 6\n\nSignal propagation in dendrites and axons\n-\nSo far we have considered a very\nsimple model of neurons - a model\nrepresenting the soma of the neuron.\n-\nWe did this because in most\nvertebrate neurons, the region that\ninitiates action potentials is at the\nsoma.\n-\nThis is usually where the 'decision' is\nmade in a neuron whether to spike or\nnot.\nRamon y Cajal\nUsed with Permission. Courtesy of the Cajal Institute (CSIC).\nLegado Cajal. Madrid.\n\nSignal propagation in dendrites and axons\nRamon y Cajal\n-\nRelatively few inputs to a neuron are\nmade onto the soma.\n-\nInputs arrive onto the dendrites -\nwhich are thin branching processes\nthat radiate from the soma.\n-\nMany synapses form onto the\ndendrite at some distance from the\nsoma (as much as 1-2 mm away)\nUsed with Permission. Courtesy of the Cajal Institute (CSIC).\nLegado Cajal. Madrid.\n\nHow does a pulse of synaptic current affect the membrane potential\nat the soma (and elsewhere in the dendrite)?\n-\n+\nVsoma\nThe\nimag\ne\nT\nh\nT\nh\nUsed with Permission. Courtesy of the Cajal Institute (CSIC). Legado Cajal. Madrid.\n\nA dendrite is like a leaky garden hose\nI0\nI0\nV0\n-\nCurrent is like water flow\n-\nVoltage is like pressure\n\nLearning objectives for Lecture 6\n-\nTo be able to draw the 'circuit diagram' of a dendrite\n-\nBe able to plot the voltage in a dendrite as a function of\ndistance for leaky and non-leaky dendrite, and understand the\nconcept of a length constant\n-\nKnow how length constant depends on dendritic radius\n-\nUnderstand the concept of electrotonic length\n-\nBe able to draw the circuit diagram a two-compartment model\n\nΔx\nx\na\nRadius\nim(x,t)Δx\nV (x,t)\nV (x -Δx,t)\nV (x + Δx,t)\nie(x,t)Δx\nInjected current per unit length\ntimes segment length\nI(x -Δx,t)\nI(x,t)\noutside\nEL\ngL\ninside\nr\nr\nr\nr\nim(x,t) = membrane current per unit length\nFinite element analysis\nExtracellular resistance is small compared to intracellular.\n\nV (x,t)\nV (x + Δx,t)\nI(x,t)\noutside\ninside\nr\nr\nr\nr\nV (x,t) -V (x + Δx,t) = r I(x,t)\nΔx V (x,t) -V (x + Δx,t)\n⎡⎣\n⎤⎦= r\nΔx I(x,t)\n-∂V\n∂x\n= Ra I(x,t)\nRa =\nr\nΔx = axial resistance\nThe cable equation\nΔV = I R\nOhm's Law\nLet's write down the relation between\nV(x,t) and I(x,t)\nNote that current flow to the right\nproduces a negative gradient\nΔx\nx\nRadius a\nThis is just the definition of a derivative!\nper unit length\n\nIf there are no membrane conductances then:\nMembrane potential changes linearly!\nV (x,t)\nV (x + Δx,t)\nI(x,t)\ninside\nΔx\nL\na\nRadius\nI(x -Δx,t)\nAnd no membrane currents ...\nI(x,t) = I(x -Δx,t) = I0\n∂V\n∂x = -Ra I0\nThe cable equation\nConsider the special case of a length L\n-∂V\n∂x = Ra I(x,t)\nL\nI0\nRaΔx\nΔV = R I0\nR = RaL\nΔV\nand steady state\n\nBoundary conditions\n-∂V\n∂x = Ra I0\nIn order to solve this equation, we need to specify two\nunknowns (boundary conditions):\nVL =V0 -Ra I0 L\nV (x) =V0 -Ra I0x\nIntegrate over x:\nIf you know any two of these\nquantities ( ), you can\ncalculate the third.\nV0 ,VL , I0\nVL\nV0\nL\nI0\nV (x,t)\nV (x + Δx,t)\ninside\nI0\nVL\nV0\nRaΔx\n\nBoundary conditions\nV0\nL\nI0\nI0\nI0\nV0\nVL = 0\nI0\nVo = Rin I0\nRin = RaL\nInput impedance\nVL =V0 -Ra I0 L = 0\n'open end'\n\nBoundary conditions\nV0\nL\nV0\nRin =Vo\nI0\nVL = V0 -Ra I0 L\nI0 = 0\n= V0\n= inf\n'closed end'\n\nCable with membrane conductance\nLeaky garden-hose analogy\nI0\nA leaky dendrite acts like a series of voltage dividers.\nI0\nRaΔx\nV0\nGmΔx\nRaΔx\nGmΔx\nRaΔx\nGmΔx\n-\nCurrent is like water flow\n-\nVoltage is like pressure\n\nV (x,t)\nV (x + Δx,t)\nI(x,t)\ninside\nI(x -Δx,t)\nDeriving the cable equation\nie(x,t) Injected current per unit length\nim(x,t)\nKirchoff's law: sum of all currents out of each node must equal zero.\nim(x,t) Δx -\nie(x,t) Δx +\nI(x,t) -\nI(x -Δx,t) = 0\nMembrane current per unit length\nim(x,t) -ie(x,t) = -1\nΔx I(x,t) -I(x -Δx,t)\n⎡⎣\n⎤⎦\nim -ie = -∂I\n∂x (x,t)\nLength of element\n∂V\n∂x = -Ra I(x,t)\nBut remember that:\nSubstitute\n∂2V\n∂x2 = -Ra\n∂I\n∂x (x,t)\nAssuming Ra is constant\n\nRa\n∂2V\n∂x2 (x,t) = im -ie\nEach element in our cable is just like our\nmodel neuron!\nim(x,t) Δx = Cm Δx dV\ndt (x,t) + Gm Δx (V -EL)\nSo, the total membrane current in our\nelement of length Δx is:\nim\nEL\nGMΔx\nCapacitance per unit length\nMembrane ionic conductance per unit length\nThis we know!!\nDeriving the cable equation\nPlug this expression for into the equation at top...\nim(x,t)\n\nRa\n∂2V\n∂x2 (x,t) = Cm\ndV\ndt (x,t)+Gm(V -EL)-ie(x,t)\nλ 2 ∂2V\n∂x2 (x,t) = τ m\n∂V\n∂t (x,t) + V (x,t) -\nGm\nie(x,t)\nτ m = Cm\nGm\nMembrane time\nconstant (sec)\nDivide both sides by Gm to get the cable equation!\nλ =\nGmRa\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟\n1/2\nSteady state space\nconstant (length, mm)\nwhere\nDeriving the cable equation\nEL is just a constant\noffset, so we ignore it\n\nLet's solve the cable equation for a simple case. What is the steady\nstate response to a constant current at a point in the middle of an\ninfinitely long cable?\nδ(x) - Dirac delta function\nIe\nx\nie(x,t) = I0δ(x)\nAn example\nδ(x)dx = ε 1\nε\n⎛\n⎝⎜⎞\n⎠⎟= 1\n-inf\ninf\n∫\ny =δ(x)\n1ε\nlimit\nε →0\nε\nx\n\nLet's solve the cable equation for a simple case. What is the steady\nstate response to a constant current at a point in the middle of an\ninfinitely long cable?\nλ 2 ∂2V\n∂x2 (x,t) = τ m\n∂V\n∂t (x,t) + V x,t\n(\n)-1\nGm\nie(x,t)\n∫δ(x)dx ≡1\nδ(x) - Dirac delta function\nIe\nx\nie(x,t) = I0δ(x)\nAn example\nλ 2 ∂2V\n∂x2 (x) = V (x) -\nGm\n2I0δ(x)\n\nV (x) = V0 e-|x| / λ\nλ 2 ∂2V\n∂x2 (x) = V (x) -\nGm\n2I0δ(x)\nAn example\nIe\nx\nie(x,t) = 2I0δ(x)\nV0\ne-1\nλ\nx\nV (x)\nI0\nλ\nx\nI(x)\n∂V\n∂x (x) = -RaI(x)\nI(x) = -1\nRa\n∂V\n∂x\nI(x) = V0\nRaλ e-|x| / λ\nI(x) = -1\nRa\n-V0\nλ\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟e\n-x /λ\n\nl\na\nA closer look at the space constant\n-\nTotal membrane conductance :\nGtot = 2πal gL\nG\ntotal area\nconductance per unit\narea (S/mm2)\n-\nMembrane conductance per unit length :\nGm = Gtot\nl\n= 2πa gL\nGm\ncircumference\nUnits are S/mm\nλ =\nGmRa\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟\n1/2\nis membrane conductance per unit length\nGm\n\nAxial resistance: the resistance along the inside of the dendrite\nSteady-state space constant\nλ =\nGmRa\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟\n1/2\n=\nS / mm Ω / mm\n⎛\n⎝⎜\n⎞\n⎠⎟\n1/2\n= mm2\n(\n)\n1/2 = mm\nA closer look at the space constant\nl\na\nλ =\nGmRa\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟\n1/2\nAxial resistance per unit length\n(Ω / mm)\nRa = Rtot\nl\n= ρi\nA = ρi\nπa2\nRtot = ρi l\nA\n~ 2000 Ω mm\nresistivity of the intracellular space\n(property of the medium )\nwhere\nTotal axial resistance along a dendrite of length l\nA = cross sectional area = πa2\nρi =\n\nTypical λ for a dendrite of a cortical\npyramidal cell\na = 2μm=2 ×10-3mm\ngL = 5×10-7S/mm2\nGm = 2πagL = 6 ×10-9 S/mm\nFirst calculate membrane conductance\n= 6 nS/mm\nl\na\nρi = 2000 Ω mm\nResistivity intracellular medium\nRa = ρi\nπa2 =160 MΩ / mm\nNow we calculate axial resistance\n= 1 mm2\n(\n)\n1/2\nλ ≈1 mm\nλ =\nGmRa\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟\n1/2\n=\n6nS/mm ⋅160MΩ/mm\n⎛\n⎝⎜\n⎞\n⎠⎟\n1/2\n\nλ =\nGmRa\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟\n1/2\n=\n2πagL\nπa2\nρi\n⎡\n⎣\n⎢\n⎤\n⎦\n⎥\n1/2\n=\na\n2ρigL\n⎛\n⎝\n⎜\n⎞\n⎠\n⎟\n1/2\nλ scales as\nradius\nNeurons need to send signals over a distance of a ~100 mm in\nthe human brain.\nWhat would a (radius) would have to be to get λ= 100 mm?\na = 20 mm!\nThis would never work! This is why signals that must be\nsent over long distances in the brain are sent by\npropagating axon potentials.\nScaling with radius\nGm = 2πagL\nRa = ρi\nπa2\n\nElectrotonic length\nl\ne-1\nλ\nx\nElectrotonic length is the physical length divided by\nthe space constant.\n\nL = l\nλ\nunitless\ne-1\nλ\nx\ne-1\nλ\nx\nThe amount of current\ninto the soma will scale\nas\ne-L\nL = 1\nL = 2\nL = 4\n\nMulti-compartment model\nUsed with Permission. Courtesy of the Cajal Institute (CSIC). Legado Cajal. Madrid.\nAnnotated figure (c) Bower, J.M. and D. Beeman. The Book of GENESIS: Exploring Realistic Neural\nModels with the GEneral NEural SImulation System 2nd ed. 1998, Springer-Verlag.\n\nTwo-compartment model\nsoma\ndendrite\nVS(t)\nGD\nGS\nCD\nCS\nRc\nVD(t)\nsomatic\ncompartment\ndendritic\ncompartment\nIe\nIe\nsomatic\ncompartment\ndendritic\ncompartment\nRc\nIe\n\nLearning objectives for Lecture 6\n-\nTo be able to draw the 'circuit diagram' of a dendrite\n-\nBe able to plot the voltage in a dendrite as a function of\ndistance for leaky and non-leaky dendrite, and understand the\nconcept of a length constant\n-\nKnow how length constant depends on dendritic radius\n-\nUnderstand the concept of electrotonic length\n-\nBe able to draw the circuit diagram a two-compartment model\n\nObviously, a big hose has less resistance to\nflow. Ie. it takes less pressure\nA small hose has more resistance and takes\nmore pressure\nExtra Slides on Input impedance\nHow much voltage does it take to produce a given current into our\ndendrite? (How much pressure does it take to get a certain water\nflow?)\nV0\nI0\nRinf\nRinf≡V0\nI0\nThis is called the 'input impedance' of the\ncable\n\nWe can calculate the input impedance\nRinf\n-1 = Ginf= Gm λ\nλ 2 =\nGm Ra\nsince\nInput impedance\nV0\nI0\nRinf≡V0\nI0\nRinf\nI(x) = V0\nRaλ e-|x| / λ\nWe calculated earlier that the current along the\ncable is\nIf we evaluate the current at x=0, we get:\nI(0) = V0\nRaλ = I0\nRinf= V0\nI0\n= Raλ\nThus,\nThus the 'input impedance' of a cable is\njust the axial resistance of a length λ of\nthe cable!\nWhat can we say about the input\nconductance?\n\nWe can exactly solve the case of a brief pulse of current in an\ninfinite cable\nie(x,t) dx dt = Q0 = total charge\n∫∫\nV (X,T) = Q0\nCλ\nIe\nx\nie(x,t) = Q0δ(x)δ(t)\nt\n4πT\ne\n-X 2\n4T e-T\nExtra Slides on Time Dependence\nQ = CV\nCλ = 2πacmλ\nX = x / λ\nT = t /τ\nwhere\n\nIe\nx\nie(x,t) = Q0δ(x)δ(t)\nPulse of charge\nV (X,T) ∝\n4πT\ne\n-X 2\n4T\nLooking at just the spatial\ndependence\nThis is just a Gaussian profile.\nσ =\n2T\nWidth increases as\nFigure removed due to copyright restrictions. See p. 39, Fig. 2.7A in\nKoch, Christof. Biophysics of Computation: Information Processing in\nSingle Neurons. 1999, Oxford University Press.\n\nIe\nx\nPropagation\nV (X,T) = Q\nCλ\n4πT\ne\n-X 2\n4T e-T\n-\n+\nVresponse\n0.001\n0.01\n0.1\nt / τ\ne\n-T\nX = 0\nX = 0.1\nX = 1\nX = 2\nV(X,T)\n\nIe\nx\nPropagation\n0 1 2 3 4 5 6 7 8 9 10\nTmax\nx / λ\nV (X,T) = Q\nCλ\n4πT\ne\n-X 2\n4T e-T\n∂V\n∂T (X,T)\nFind the peaks by calculating\nand setting it to zero.\nFor any given X, you can solve for Tmax.\nTmax = 1\n1+ 4X 2 -1\n(\n) ≈1\n2 X\ntmax\nτ\n≈1\nx\nλ\nFrom this, we can calculate the velocity!\n0 1 2 3 4 5 6 7 8 9 10\n0.2\n0.4\n0.6\n0.8\nNormalized amplitude\nt / τ\nX = 1 3 5\n-\n+\nVresponse\nv =\nx\ntmax\n= 2λ\nτ\n\nDendritic filtering\nIe\nX\n0 1 2 3 4 5 6 7 8 9 10\n0.2\n0.4\n0.6\n0.8\nNormalized amplitude\nt / τ\nX meas =\n1 3 5\nAs the voltage response propagates down a dendrite, it not only\nfalls in amplitude, but it broadens in time.\n-\n+\nVresponse\nX meas\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu/\n9.40 Introduction to Neural Computation\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Lecture 7: Synapses - 9.40 Introduction to Neural Computation",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-40-introduction-to-neural-computation-spring-2018/fff03c3125fe8d597cec46ba356862ae_MIT9_40S18_Lec07.pdf",
      "content": "Introduction to Neural\nComputation\nProf. Michale Fee\nMIT BCS 9.40 -- 2018\nLecture 7\n\nREPLACE\nRamon y Cajal\n-\n+\nVsoma\nUsed with Permission. Courtesy of the Cajal\nInstitute (CSIC). Legado Cajal. Madrid.\n\nLearning objectives for Lecture 7\n-\nBe able to add a synapse in an equivalent circuit model\n-\nTo describe a simple model of synaptic transmission\n-\nTo be able to describe synaptic transmission as a convolution\nof a linear kernel with a spike train\n-\nTo understand synaptic saturation\n-\nTo understand the different functions of somatic and dendritic\ninhibition\n\n- Structure of typical excitatory synapse\npre-synaptic terminal\ndendrite\npost-synaptic terminal\n(spine)\n~20 nm\n500 nm\n0.5μm\nsynaptic vesicle\n(30-40nm dia)\nChemical synapse\n\n- Sequence of events in synaptic transmission\ndendrite\n+50mV\n-60 mV\nCa++\nCa++\nvoltage-gated\nCa- channels\nAP\nChemical synapse\n\nChemical synapse\ndendrite\nCa++\nCa++\nNa+ Ca++\nligand-gated ion\nchannels\n'ligand' =\n'neurotransmitter'\n- Sequence of events in synaptic transmission\nIsyn\nGsyn\nVm\nLast step:\nNeurotransmitter\nreuptake\n\nAnatomy of synapses/axons/dendrites\n-\nSynapses are small - contact area~0.5μm\n-\nHigh packing density ~109 synapses/mm3\n- 1.1um on a 3D lattice\n- 4.1km of axon (0.3μm dia)\n- 500m of dendrite\n-\nA cell receives many synapses\n- 10000 synapses\n- on 4mm of dendrites (4 cm of axon)\n- 105 neurons/mm3 in mouse cortex\n\nLearning objectives for Lecture 7\n-\nBe able to add a synapse to an equivalent circuit model\n-\nTo describe a simple model of synaptic transmission\n-\nTo be able to describe synaptic transmission as a convolution\nof a linear kernel with a spike train\n-\nTo understand synaptic saturation\n-\nTo understand the different functions of somatic and dendritic\ninhibition\n\nTwo electrode voltage-clamp\nexperiment\nMagleby and Stevens, 1972\n-\n+\nVc\nVm\nIe\nFrog sartorius muscle fiber\nMotor\nneuron\nsynapse\nHow does a synapse respond?\n- Ionotropic receptors\n\nHow does a synapse respond?\n- Ionotropic receptors\nIsyn(t) = Gsyn(t) V -Esyn\n⎡⎣\n⎤⎦\nGsyn(t)\n1 ms\ntime\nV\nImax\n50 mV\n-100\nI-V Curve\nAnnotated figure on lower right (c) Hille, Bertil. Ion Channels of Excitable Membranes (3rd Ed.).\n2001, Sinauer / Oxford University Press. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nEquivalent circuit model of a synapse\n-\nCurrent flow through a synapse results from changes in\nsynaptic conductance\nEL\nIe\nV\nC\n+\nGL\nIsyn(t) = Gsyn(t) V -Esyn\n⎡⎣\n⎤⎦\nEsyn\nGsyn\nEquivalent circuit of\na synapse\n\nExcitatory synapses\n-\nIncreased synaptic conductance causes the membrane potential to\napproach the reversal potential for that synapse.\nGsyn\nEL\nIe\nV\nC\n+\nGL\nIsyn(t) = Gsyn(t) V -Esyn\n⎡⎣\n⎤⎦\nEsyn = 0mV\nExcitatory\npostsynaptic\npotential (EPSP)\nNow we can change the\n'holding potential of the cell\nby injecting a little current\n(current clamp experiment)\nVm\n15mV\n\nExcitatory and inhibitory synapses\n-\nIncreased synaptic conductance causes the membrane potential to\napproach the reversal potential for that synapse.\nGsyn\nEL\nIe\nV\nC\n+\nGL\nIsyn(t) = Gsyn(t) V -Esyn\n⎡⎣\n⎤⎦\nExcitatory synapse if\nEsyn > Vth\nEsyn = 0mV\nExcitatory\npostsynaptic\npotential (EPSP)\n15mV\nFigure from Johnston, D. and M.-S. Wu. Foundations of Cellular\nNeurophysiology. 1995. Courtesy of MIT Press.\n\n-\nIncreased synaptic conductance causes the membrane potential to\napproach the reversal potential for that synapse.\nGsyn\nEL\nIe\nV\nC\n+\nGL\nIsyn(t) = Gsyn(t) V -Esyn\n⎡⎣\n⎤⎦\nInhibitory synapse if\nEsyn < Vth\nEsyn = -75mV\nGABAergic synapse\nExcitatory and inhibitory synapses\nInhibitory\npostsynaptic\npotential (IPSP)\nFigure 13.4 from Johnston, D. and M.-S. Wu. Foundations of\nCellular Neurophysiology. 1995. Courtesy of MIT Press.\n\nEquivalent circuit model of a synapse\n-\nCurrent flow through a synapse results from changes in\nsynaptic conductance\nIsyn(t) = Gsyn(t) Vm(t)-Esyn\n⎡⎣\n⎤⎦\n-\nLigand gated ion channels 'flicker' between\nopen and closed states.\nGsyn(t) = ˆgR NR PR(t)\nˆgR =unitary 'open' conductance\nNR =number of receptors\n-\nWe can write the synaptic conductance in\nterms of the probability that a\nreceptor is 'open'.\nPR(t)\nFigure removed due to copyright restrictions. Single-\nchannel patch recording, GABAA receptor. Figure 6.13\nin: Hille, Bertil. Ion Channels of Excitable Membranes\n(3rd Ed.). 2001, Sinauer / Oxford University Press.\nSingle-channel patch recording\nGABAA receptor\n\nKinetic model of synapse gating\n-\nWe can describe the open probability using a 'kinetic' model.\n'closed'\nα\nβ\n\n'open'\n1-PR\nPR\nProbability per unit time;\nunits are 1/s\nα,β are transition rate constants\nSingle-channel patch recording\nGABAA receptor\n-\nWhat controls the rate at which channels\nopen ?\nNeurotransmitter!\nFigure removed due to copyright restrictions. Single-\nchannel patch recording, GABAA receptor. Figure 6.13\nin: Hille, Bertil. Ion Channels of Excitable Membranes\n(3rd Ed.). 2001, Sinauer / Oxford University Press.\n\nEquivalent circuit model of a synapse\n- Simplified version of Magleby-Stevens model\nd PR\ndt\n=\nα [A]n (1-PR)\n-\nβ PR\nA\nα\nβ\n(closed)\nunbound receptor\n(open)\nbound receptor complex\nunbound\nNT\nR\n\nAR*\n1-PR\nPR\nPR(t) = Pmax e-t/τ s\nPR(t)\n[A]\nT\nGsyn(t) = ˆgR NR PR(t)\n\nLearning objectives for Lecture 7\n-\nBe able to add a synapse in an equivalent circuit model\n-\nTo describe a simple model of synaptic transmission\n-\nTo be able to describe synaptic transmission as a convolution\nof a linear kernel with a spike train\n-\nTo understand synaptic saturation\n-\nTo understand the different functions of somatic and dendritic\ninhibition\n\nResponse of a synapse to a spike train input\n-\nThis simple model makes it very easy to describe the\nresponse of a synapse to a train of spikes!\nK(t) = Gmax e-t/τ s\nImpulse response\nor Linear Kernel\nS(t)\nInput\nG(t)\nResponse\nG(t) =\nK(τ )S(t -τ )\n-inf\ninf\n∫\ndτ\nConvolution\n\nt\nt\nResponse of a synapse to a spike train input\n-\nThis simple model makes it very easy to describe the\nresponse of a synapse to a train of spikes!\n-\nWe just convolve the spike train with the linear\nresponse of the synaptic conductance\nG(t) =\nK(τ )S(t -τ )\n-inf\ninf\n∫\ndτ\nt\nK(t) = Gmax e-t/τ s\nImpulse response\nτ\nS(t)\nG(t)\nK(t)\n\nResponse of a synapse to a spike train input\n-\nWe just convolve the spike train with the linear response of\nthe synaptic conductance\nG(t) =\nK(τ )S(t -τ )\n-inf\ninf\n∫\ndτ\n-\nEasy to do in MATLAB(r)\n- use the conv function\n\nLearning objectives for Lecture 7\n-\nBe able to add a synapse in an equivalent circuit model\n-\nTo describe a simple model of synaptic transmission\n-\nTo be able to describe synaptic transmission as a convolution\nof a linear kernel with a spike train\n-\nTo understand synaptic saturation\n-\nTo understand the different functions of somatic and dendritic\ninhibition\n\nSynaptic saturation\n- Let's examine how the voltage in a dendrite changes as\na function of the amount of excitatory conductance...\nGL\nVD(t)\ndendritic\ncompartment\ndendritic\ncompartment\nEL\nsomatic\ncompartment\nsomatic\ncompartment\nVS(t)\nGL\nRc\nEL\nGsyn\nEsyn\n\nSynaptic saturation\nGL\nVD(t)\ndendritic\ncompartment\ndendritic\ncompartment\nEL\nsomatic\ncompartment\nsomatic\ncompartment\nVS(t)\nGL\nRc\nEL\nGsyn\nEsyn\n- Let's examine how the voltage in a dendrite changes as\na function of the amount of excitatory conductance...\n\nVm\nGsyn(nS)\nEL\nSynaptic saturation\nGL\nVD\ndendritic\ncompartment\nEL\nGsyn\nEsyn\nEsyn\nAs synaptic input increases, the postsynaptic response saturates to a constant\nvalue\n- Let's examine how the voltage in a dendrite changes as\na function of the amount of excitatory conductance...\n\nSynaptic saturation\n- Let's examine how the voltage in a dendrite changes as\na function of the amount of excitatory conductance...\nGL\nVD\ndendritic\ncompartment\nEL\nGsyn\nEsyn\nKirchoff's current law says:\nIsyn\n+\nIL\n= 0\nIsyn\nIL\nGsyn V -Esyn\n⎡⎣\n⎤⎦+ GL V -EL\n[\n] = 0\nGsynV -GsynEsyn + GLV -GLEL = 0\nV(Gsyn + GL)-(GsynEsyn + GLEL) = 0\nV = GLEL +GsynEsyn\nGL +Gsyn\n\nVm\nGsyn(nS)\nEL\nGL\nSynaptic saturation\nGL\nVD\ndendritic\ncompartment\nEL\nGsyn\nEsyn\nV ≈EL\nV →Esyn\nEsyn\nFor GL >> Gsyn\nFor Gsyn >> GL\n+\nEsyn\nGL\n⎛\n⎝⎜\n⎞\n⎠⎟Gsyn\nV = GLEL +GsynEsyn\nGL +Gsyn\n- Let's examine how the voltage in a dendrite changes as\na function of the amount of excitatory conductance...\n\nLearning objectives for Lecture 7\n-\nBe able to add a synapse in an equivalent circuit model\n-\nTo describe a simple model of synaptic transmission\n-\nTo be able to describe synaptic transmission as a convolution\nof a linear kernel with a spike train\n-\nTo understand synaptic saturation\n-\nTo understand the different functions of somatic and dendritic\ninhibition\n\nInhibitory inputs\nGL\nVD(t)\ndendritic\ncompartment\ndendritic\ncompartment\nEL\nsomatic\ncompartment\nsomatic\ncompartment\nVS(t)\nGL\nRc\nEL\nGexc\nEexc\n- The effect of inhibitory input depends strongly on where\nthe inhibitory synapse is.\nExcitatory synapse\nGinh\nEinh\nInhibitory synapse\n\nInhibitory inputs\nGL\nVD(t)\ndendritic\ncompartment\ndendritic\ncompartment\nEL\nsomatic\ncompartment\nsomatic\ncompartment\nVS(t)\nGL\nRc\nEL\nGexc\nEexc\n- The effect of inhibitory input depends strongly on where\nthe inhibitory synapse is\nExcitatory synapse\nGinh\nEinh\nInhibitory synapse\n\nCrayfish as a model system\n- Stereotypic behavior\n- Identifiable neurons\n- Identifiable circuits\nYellow: LG neuron\n(Antonsen & Edwards, 2003)\nEdwards et al. (Trends Neurosci, 1999)\nFigure removed due to copyright restrictions.\nSee Fig. 1 in Antonsen, B.L. and D.H. Edwards.\n\"Differential Dye Coupling Reveals Lateral Giant\nEscape Circuit in Crayfish.\" J. Comp. Neurol. 466\nno. 1 (2003):1-13.\nCourtesy of Elsevier, Inc., https://www.sciencedirect.com. Used with permission.\n\nEscape behavior in crayfish\nFigure: Edwards et al. (Trends Neurosci, 1999)\n¡ MG (medial giant) escape\n¡ LG (lateral giant) escape\n¡ Non-giant escape\nLG escape\nMG escape\nCourtesy of Elsevier, Inc., https://www.sciencedirect.com. Used with permission.\n\nLG is a 'command neuron'\n- LG neuron is sufficient for LG escape.\n- Electrical stimulation of LG neuron produces tail flip.\n- LG neuron is necessary for LG escape.\n- Tail flip is not elicited if the LG neuron is hyperpolarized.\nWine & Mistick (1977)\nFigure removed due to copyright restrictions. See Fig. 1 in Wine, J.J. and D.C.\nMistick. \"Temporal Organization of Crayfish Escape Behavior: Delayed Recruitment\nof Peripheral Inhibition.\" J. Neurophysiology 40 no. 4 (1977):905-925.\n\nEscape behaviors are strongly modulated by\ninhibition\n- Escape response is suppressed while another escape\nresponse is in progress\n- Recurrent inhibition of LG neurons (and many other neurons)\nduring escape behavior\n- Escape response is suppressed when the animal\nis restrained\nKrasne & Wine (1975)\nHold off escape until timely\nmoment?\nFigure removed due to copyright restrictions. See Fig. 2\nin Krasne, F.B. and J.J. Wine. \"Extrinsic Modulation of\nCrayfish Escape Behaviour.\" J. Experimental Biology 63\n(1975): 433-450.\n\nEscape behaviors are strongly modulated by\ninhibition\n- Escape response is suppressed\nwhile the animal is eating\n- But not while the animal is\nsearching for food\nKrasne & Lee (1988)\nFigure removed due to copyright restrictions.\nSee Fig. 2 in Krasne, F.B. and S.C. Lee.\n\"Response-dedicated Trigger Neurons as\nControl Points for Behavioral Actions.\" J.\nNeuroscience 8 no. 10 (1988): 3703-3712.\n\nTwo types of modulation of LG escape\nreflex\n- Absolute inhibition: The escape is inhibited\nno matter how strong the excitation is.\n- Relative inhibition: The likelihood of escape is\nreduced, but it is still possible to override this\nkind of inhibition.\n\nLocation of inhibitory synapses\n-\nProximal inhibition:\n-\nNear the spike initiating zone\n-\nArises from motor circuits that generate the MG escape\n-\nCalled 'recurrent inhibition'\n-\nDistal inhibition:\n-\nIntermixed with excitatory afferents further out on the dendrite\n-\nArises from sensory areas\n-\nCalled 'tonic inhibition'\nPrevious hypothesis:\nDistal inhibition allows\nselective inhibition for\nparticular dendritic\nbranches\n\nMeasuring the effect of different types\nof inhibition\nSensory root stimulation\nSpike-initiating zone\nMG stimulation\nrestraint, sucrose block\nCurrent injection\nRecording\nVu and Krasne, 1992\nAnnotated figure (c) American Association for the Advancement of Science. All rights reserved. This content is excluded from our Creative Commons\nlicense. For more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nEquivalent circuit model\n- RL: longitudinal resistance\n- RP: proximal resistance\n- RD: distal resistance\n- Ee: reversal potential for excitatory synapse (100 mV)\n- Ge: excitatory conductance\n- Gi: inhibitory conductance\n\nProximal\ninhibition\nDistal\ninhibition\nProximal versus Distal inhibition\nAnnotated figure (c) American Association for the Advancement of Science. All rights reserved. This content is\nexcluded from our Creative Commons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nExcitatory strength\nProximal inhibition\n0.2\n0.5\nGi / GD\nAnnotated figure (c) American\nAssociation for the Advancement of\nScience. All rights reserved. This content\nis excluded from our Creative Commons\nlicense. For more information, see\nhttps://ocw.mit.edu/help/faq-fair-use/.\n\nExcitatory strength\nDistal inhibition\n0.2\n0.5\nGi / GD\nAnnotated figure (c) American\nAssociation for the Advancement of\nScience. All rights reserved. This content\nis excluded from our Creative Commons\nlicense. For more information, see\nhttps://ocw.mit.edu/help/faq-fair-use/.\n\nVu et al. (JNS, 1993)\nproximal inhibitory synapses\nexcitatory synapses,\ndistal inhibitory synapses\nMore 'realistic' multi-compartment\nmodel\nAnnotated figure (c) Society for Neuroscience. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\n- Two-compartment model shows that the effect of proximal\nand distal inhibition are different.\n- Proximal inhibition: absolute\n- Distal inhibition: relative\n- Qualitatively similar effects were seen when more\ncomplicated models were used.\nDifferent functions for proximal and\ndistal inhibition\n\nLearning objectives for Lecture 7\n-\nBe able to add a synapse in an equivalent circuit model\n-\nTo describe a simple model of synaptic transmission\n-\nTo be able to describe synaptic transmission as a convolution\nof a linear kernel with a spike train\n-\nTo understand synaptic saturation\n-\nTo understand the different functions of somatic and dendritic\ninhibition\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu/\n9.40 Introduction to Neural Computation\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Lecture 8: Spike Trains - 9.40 Introduction to Neural Computation",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-40-introduction-to-neural-computation-spring-2018/7a90ad5ecc7037506a34cffd3533935d_MIT9_40S18_Lec08.pdf",
      "content": "Introduction to Neural\nComputation\nProf. Michale Fee\nMIT BCS 9.40 -- 2018\nSpike trains\n\nElectrical recordings of brain activity\n- Electrical recordings in the brain are made\nwith electrodes.\n- Recordings can be made inside of single cells\n- Most often, in behaving animals, recordings\nare made of signals outside of neurons.\n\nElectrical recordings of brain activity\nWhat is the origin of 'extracellular' electrical signals?\n-\nVoltage measurements are always\nvoltage differences\n-\nExtracellular voltages are always\nmeasured between the signal\nelectrode and a local ground or\nreference electrode.\n-\nVoltage differences between the\nsignal and ground electrode are\nalways associated with current flow\nthrough the extracellular space.\n-\nBack to Ohm's Law!\n-\n+\nVext\nsurface of\nbrain\nextracellular\nrecording\nelectrode\namplifier\nneuron\nground\nelectrode\nρl\n=\nΔV = I Rext\nRext\nA\n\nOrigin of extracellular signals\nWhat happens to the voltage outside our model neuron during an\naction potential?\nNa+\nNa+\n-\nLet's start with our spherical neuron\nNa+\nNa+\n-\nThere is no spatial separation between\ncurrent flows into the neuron and current\nintracellular\nflow out of the neuron. Thus, no current\nflow outside the neuron.\nGNa\nINa\nENa\n-\nAnd thus no extracellular voltage changes!\nextracellular\n\nOrigin of extracellular signals\nWhat happens to the voltage outside our model neuron during an\naction potential?\n-\nNow let's see what happens when we add a\ndendrite.\nNa+\nNa+\nNa+\nNa+\n-\nNow there is extracellular current flow\nGNa\nINa\nΔV\n= R I\next\next ext\nRext\nENa\nsoma\nΔVext\nIext\nextracellular\n-\nAnd extracellular voltage changes!\n\nCurrent sources and sinks\nWhat happens to the voltage outside our model neuron during an\naction potential?\nNa+\nNa+\nNa+\nNa+\n-\nExtracellular current flows from current sources to\ncurrent sinks\nintracellular\nGNa\nCurrent sources are regions\nINa\nof higher extracellular\nRext\npotential\nCurrent sinks are regions of\nΔVext\nlower extracellular potential\nENa\nextracellular\nCurrent source\nCurrent sink\n\nRelation between membrane potential and\nextracellular potential\nΔV\n= R I\n= R (I + IR )\next\next ext\next\nc\n= Rext\n⎡\n⎢⎣\nC dV\n⎤\n⎥⎦\nm + G(V\n)\nm - EL\ndt\nFigure removed due to copyright restrictions. See Henze,\ndV\nD.A., et al. \"Dopamine Increases Excitability of Pyramidal\nm\nΔV\n≈ R C\nNeurons in Primate Prefrontal Cortex.\" J. Neurophys. 84\next\next\nno. 6 (2000): 2799-2809.\ndt\nV m\nHenze et al, 2000\nExtracellular voltages look a lot like the\next\nextracellular\nGNa\nsoma\nENa\nRext\nΔVext\nI\nderivative on membrane potential!\n\nOrigin of extracellular signals\nWhat happens to the voltage outside our model neuron during a\nsynaptic input?\nExcitatory synapse\nCurrent sink\nCurrent source\nVext (t)\nVext (t)\ntime\ntime\n\nOrigin of extracellular signals\nWhat happens to the voltage outside our model neuron during a\nsynaptic input?\nInhibitory synapse\nCurrent source\nCurrent sink\nVext (t)\nVext (t)\ntime\ntime\n\nLocal Field Potentials\n- Synchronous synaptic input to large populations\nof neurons\n- Depends on linear summation\n- morphology is important\nLaminar morphology\nNuclear morphology\n- large LFP\n- small LFP\n\nElectrical recordings of brain activity\nSAMPLING\n-\n+\nsurface of\nbrain\nextracellular\nrecording\nelectrode\namplifier\nneuron\nground\nelectrode\nAnalog to\nDigital\nConverter\n(ADC)\nThe analog to digital converter samples the voltage from the amplifier at\nregular intervals in time and stores the result in the computer memory.\nΔt\nsampling frequency, fs = 1\nΔt\nThe rate at which the samples are acquired is called the sampling rate or\nsampling frequency.\n\nTwo kinds of signals\n- Recording of neural activity in hippocampus of a running rat.\n200ms\n- Slow - Local Field Potentials (LFP)\n- Synaptic currents\n- Fast - Spikes\n- Action potentials\n-70\n-60\n-50\n-40\n-30\n-20\n-10\nfrequency (Hz)\nPower(dB)\nraw data power spectrum\n\nLow-pass filtering\nLow-pass filtering can be done by convolving\nthe signal with a kernel like this.\narea = 1\n\nExplanation of low pass filter\nKernel\n0.5\n0.5\nSignal\n1.5\n0.5\nProduct\n∑= 2\nSum\nFiltered output\n\nExplanation of low pass filter\nKernel\n0.5\n0.5\nSignal\n0.5\n1.5\nProduct\n∑= 2\nSum\nFiltered output\n\nExplanation of low pass filter\nKernel\n0.5\n0.5\nSignal\n1.5\n2.5\nProduct\n∑= 4\nSum\nFiltered output\n\nHigh-pass filtering\nHigh-pass filtering can be done by convolving\nthe signal with a kernel like this.\narea = -1\nTotal area = 0\n\nSpike detecton\n- If you are recording from only a single neuron, it is easy\nto extract spike times...\nSpike detection threshold\nt0\n\nSpike trains\n- Representation of all the spikes that a neuron generates in a period\nof time of interest\nSpike detection threshold\nt1\nt2 t3\nt4\nt5\nt6\n- We can represent a spike train as a list of spike times\nti for i =1 to N spikes\n\nSpike trains\n- Spike trains can also be represented a sum of delta\nfunctions.\nδ (t - t6)\nt1\nt2 t3\nt4\nt5\nt6\nρ(t) = δ (t - t1) + δ (t - t2) + δ (t - t3)\n+ δ (t - t4) + δ (t - t5) + δ (t - t6)\nρ(t) =\nδ (t - ti )\ni ∑\n- ρ(t) can be thought of as the derivative of a spike count function\nT\n∫ρ(t)dt = N (# spikes)\nρ(t) has units of spikes per second\nN(t)\nt\n\nTuning curves\n- Simple cells in primary visual cortex of the cat are\nresponsive to some orientations, but not others.\nThese neurons show 'orientation tuning.'\n\nTuning curves\nHearing / ear diagram removed due to copyright restrictions.\nUnfolded cochlea figure removed due to copyright restrictions.\nSource unknown.\nSource unknown.\nAuditory neurons show frequency tuning\n\nTuning curves\n- Relation between spiking activity of a neuron in primary\nmotor cortex and the onset of the arm movement.\nFigures removed due to copyright restrictions. See Figs. 1 & 3 in Georgopoulos, A.P.,\net al. \"On the Relations Between the Direction of Two-Dimensional Arm Movements\nand Cell Discharge in Primate Motor Cortex.\" J. Neurosci. 2 no. 11(1982): 1527-37.\n\nTuning curves\n- Relation between spiking activity of a neuron in primate\nprimary motor cortex and the onset of the arm movement.\nFigures removed due to copyright restrictions. See Figure 4 in Georgopoulos, A.P., et\nal. \"On the Relations Between the Direction of Two-Dimensional Arm Movements and\nCell Discharge in Primate Motor Cortex.\" J. Neurosci. 2 no. 11(1982): 1527-37.\nM1 neurons show tuning for movement direction.\n\nQuantifying firing rates\n- Method 1: Trial average firing rate.\n- Count the number of spikes in each trial. Average over trials.\nStimulus\ntime\nonset\noffset\nTrial #\nT\n= number of spikes on trial i\nNi i\nNi\nR =\ndenotes the average over all trials i\ni\nT\n\nQuantifying firing rates\n- We can get higher temporal resolution by breaking the\nrate calculation into smaller units in time.\ntime\nStimulus\nonset\noffset\nTrial #\nΔT\nBin 1\nTrial-average rate in time bin j\n= number of spikes on trial i in bin j\nNi\ndenotes the average over all trials i\ni\nRj =\n1 Ni, j i\nΔT\n\nQuantifying firing rates\n- Peri-Stimulus Time Histogram (PSTH)\nor Peri-Event Time Histogram (PETH)\nFigures removed due to copyright restrictions. See Figure 2 in Georgopoulos, A.P., et\nal. \"On the Relations Between the Direction of Two-Dimensional Arm Movements and\nCell Discharge in Primate Motor Cortex.\" J. Neurosci. 2 no. 11(1982): 1527-37.\nGeorgopoulos, 1982\n\nQuantifying firing rates\n- The same trick can be used to estimate firing rates in\ncontinuous spike trains (not associated with trials).\nnumber of spikes in bin j\nNj\nΔT\nNj\nRj = ΔT\nThe problem with using fixed bins here is that the answer\ndepends on where the boundaries are\n\nQuantifying firing rates\n- A continuous measure of firing rate.\nΔT\nWe count the number of spikes in a small window of width ΔT\nand shift the window in smaller steps\nHow can we describe this mathematically (you ask)?\n\nQuantifying firing rates\n- A continuous measure of firing rate.\nWe can write this process down mathematically as follows.\nFirst, we use the fact that the number of spikes in an interval t1 to\nt2\nt2 is given by\nN = ∫ρ(t)dt\nt1\nt + ΔT /2\nN\nR(t) =\n=\n∫\nρ(τ )dτ\nΔT\nΔT t -ΔT /2\nt + ΔT 2\nt - ΔT 2\nt\n\nQuantifying firing rates\n- A continuous measure of firing rate.\nt\nBut this is just a convolution! We are convolving our spike train with a\nsquare kernel of width ΔT .\nKernel\n< ΔT\n⎡ 1 / ΔT if τ\ninf\nK(τ ) = ⎢\n⎢\nR(t) = ∫ρ(t -τ )K(τ )dτ\n⎣\notherwise\n-inf\nNotation\nR(t) = ρ(t)* K\n1ΔT\n+ ΔT 2\n- ΔT 2\narea = 1\n\nQuantifying firing rates\n- A continuous measure of firing rate.\nEven better is to convolve with a Gaussian kernel\nKernel\nτ 2\nR(t) = ρ(t)* K\n-\nK(τ ) =\ne 2σ 2\nσ 2π\narea = 1\nGaussian kernel is still averaging - it is just a weighted average,\nwith less weight at the edges.\n\nQuantifying firing rates\n- Summary\nFigure courtesy MIT Press. From Dayan, P. and L. Abbott. Theoretical Neuroscience: Computational and\nMathematical Modeling of Neural Systems. 2001.\n\nKey problem\n- You have to choose a timescale to measure the firing rate.\nBut you get a different answer for every different timescale!\nσ = 4 ms\nσ = 20 ms\nσ = 100 ms\nTime (ms)\n100 ms\nFiring rate (Hz)\n\nTemporal structure of responses\n- Neuronal responses are not static. They have a strong\ntemporal structure.\nRecordings from vibrissa cortex in the rat. Response to whisker deflections.\n\nTemporal structure of responses\n- Auditory neurons can be strongly locked to the phase of\nthe sound waveform.\nFiring rate (Hz)\nThe spike timing is precisely\ncontrolled.\nThe firing rate is rapidly modulated in time.\n\nRate vs timing?\n- Sensory neurons spike more in response to some stimuli than others\nMotor neurons spike more before or during some actions than others\nTherefore, information about a stimulus (or motor action) is carried in the\nnumbers of spikes generated.\n- All neurons exhibit temporal modulation of their firing rate (or spiking\nprobability per unit time).\nIf information is carried in the slow modulations spike probability, we say\nthat the information is coded by firing rate.\n'Rate coding'\nIf information is carried in the fast modulations in spike probability, we say\nthe the information is coded by spike timing.\n'Temporal coding'\n\nRate vs timing?\n- You may occasionally hear a debate about 'rate coding' vs.\n'temporal coding'.\n- This is a false dichotomy. These are two extremes along a\nspectrum.\n- The brain uses information in spike trains at a fast timescale and\nat a slow timescale.\n- How do we determine what timescales are important?\nWe look at what the downstream neurons do with these spikes!\n- What timescale is relevant for the computation being done?\n- What are the biophysical processes in the downstream neurons.\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu/\n9.40 Introduction to Neural Computation\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Lecture 9: Receptive Fields - 9.40 Introduction to Neural Computation",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-40-introduction-to-neural-computation-spring-2018/2b84cf132d429d089f1f2709f800a9cd_MIT9_40S18_Lec09.pdf",
      "content": "Introduction to Neural\nComputation\nProf. Michale Fee\nMIT BCS 9.40 -- 2018\nLecture 9 -- Receptive fields\n\nSpatial receptive fields\nLIGHT\nVideo of visual neurons of a cat from\nHubel & Wiesel's experiments.\nAli Moeeny. \"Hubel & Wiesel - LGN\nNeuron.\" April 23, 2011. YouTube.\nFigure (annotated) from Introduction to Visual Prostheses on Webvision. License CC BY-NC.\n\nSpatial receptive fields\nLIGHT\nVideo of visual neurons of a cat - simple\nand complex cells - from Hubel &\nWiesel's experiments.\nAli Moeeny. \"Hubel & Wiesel - Cortical\nNeuron - V1.\" April 23, 2011. YouTube.\nFigure (annotated) from Introduction to Visual Prostheses on Webvision. License CC BY-NC.\n\nLearning objectives for Lecture 9\n- To be able to mathematically describe a neural response as a\nlinear filter followed by a nonlinear function.\n- A correlation of a spatial receptive field with the stimulus\n- A convolution of a temporal receptive field with the stimulus\n- To understand the concept of a Spatio-temporal Receptive\nField (STRF) and the concept of 'separability'\n- To understand the idea of a Spike Triggered Average and how\nto use it to compute a Spatio-temporal Receptive Field and a\nSpectro-temporal Receptive Field (STRF).\n\nLearning objectives for Lecture 9\n- To be able to mathematically describe a neural response as a\nlinear filter followed by a nonlinear function.\n- A correlation of a spatial receptive field with the stimulus\n- A convolution of a temporal receptive field with the stimulus\n- To understand the concept of a Spatio-temporal Receptive\nField (STRF) and the concept of 'separability'\n- To understand the idea of a Spike Triggered Average and how\nto use it to compute a Spatio-temporal Receptive Field and a\nSpectro-temporal Receptive Field (STRF).\n\nSpatial receptive fields\n- How do we represent receptive fields mathematically?\n- At the simplest level, we think of the receptive field (RF) as the region\nof visual space that causes the neuron to spike.\n- But a visual neuron doesn't respond to any stimulus within this RF. It\nresponds selectively to certain 'features' in the stimulus.\n- We can think of a neuron as having a filter (G) that passes certain\nfeatures in both space and time.\n- The better the stimulus 'overlaps' with the filter, the more the neuron\nwill spike.\n\nSpatial receptive fields\n- How do we represent receptive fields mathematically?\nStart by describing the spatial part of this filter.\noutput\nspike\nfilter\nx\nnonlinearity\ngenerator\ny\nI(x, y)\nL = [\nG I(\n)\ny\n,\nx\n]\n[\nr = r0 + L]+\nPT [n]\nstimulus\nPoisson\nprocess\nresponse\nfilter\nstimulus\nfiring rate\nImage of mouse in public domain.\n\nSpatial receptive fields\n- How do we represent receptive fields mathematically?\nWe are going to consider the simplest case in which the response\nof a neuron is given by a linear filter acting on the stimulus.\nr = r0 + ∫∫G(x, y)I(x, y)dx dy\nLet's look at this in one dimension\nx\ny\nG(x, y)\nG(x)\nx\nr = r0 + ∫G(x)I(x)dx\nLike a correlation ∑Gi Ii\ni\n\nSpatial receptive fields\n- How do we represent receptive fields mathematically?\nG(x)\nx\nx\nI(x)\nG(x)I(x)\nx\nG(x)I(x)\nG(x)\nI(x)\n∫G(x)I(x)dx big\n∫G(x)I(x)dx small\n\nLinearity\n- Response varies linearly with overlap\nG(x)\nI(x)\nG(x)I(x)\nG(x)\nx\nx\nI(x)\nG(x)I(x)\nx\n∫G(x)I(x)dx big\n∫G(x)I(x)dx half as big\n\nTemporal receptive fields\n- We can also think of the response of a neuron as some\nfunction of the temporal variations in the stimulus.\nTime-dependent\nfiring rate\nfiring rate\nr(t) = r0 + D S(t)\n[\n]\nStimulus\nFilter\nSpontaneous\n\nTemporal receptive fields\n- We can think of 'overlap' in the time domain! That there\nis a particular 'temporal profile' of a stimulus that makes a\nneuron spike.\nS(t)\ntime\nDoes this look familiar?\n\nTemporal receptive fields\ninf\nConvolution!!\nr(t) = r0 + ∫ D(τ )S(t -τ )dτ\n-inf\nD(τ )\nτ\nLinear temporal response kernel. (Or 'temporal kernel')\nIt is linear in the sense that if we make the stimulus partial, or\nweaker, the response changes linearly.\n\nLearning objectives for Lecture 9\n- To be able to mathematically describe a neural response as a\nlinear filter followed by a nonlinear function.\n- A correlation of a spatial receptive field with the stimulus\n- A convolution of a temporal receptive field with the stimulus\n- To understand the concept of a Spatio-temporal Receptive\nField (STRF) and the concept of 'separability'\n- To understand the idea of a Spike Triggered Average and how\nto use it to compute a Spatio-temporal Receptive Field and a\nSpectro-temporal Receptive Field (STRF).\n\n- This is called the spatio-temporal receptive field (STRF).\nSpatio-temporal receptive fields\n-\nLet's imagine a stimulus that is a function of space and time,\nlike the light falling on a retina: I(x,y,t)\n-\nBut now we are going to simplify things by considering only\none spatial dimension:\nI(x,t)\nr(t) = r0 +\ndxdτ D(x,τ )I(x,t -τ )\n-inf\ninf\n∫\n-\nNow we are going to put the temporal receptive field and the\nspatial receptive field together in a single object.\n\nHere we are doing a correlation and a convolution at the same time!\nCorrelation in the integral over space and a convolution in the integral\nover time!\nSpatio-temporal receptive fields\nr(t) = r0 +\ndxdτ D(x,τ )I(x,t -τ )\n∫∫\nconvolution\nr(t) = r0 +\ndτ\n-inf\ninf\n∫\ndx D(x,τ )I(x,t -τ )\n-inf\ninf\n∫\ncorrelation\n\n- If a receptive field is separable in space and time, then\nwe can decompose it into a spatial receptive field and a\ntemporal receptive field.\nSeparability\n+\n-\nGS(x)\nDT (τ )\nD(x,τ )\nτ\nx\nSeparable\nD(x,τ )\nτ\nx\nInseparable\n\n- If a receptive field is separable in space and time, then\nwe can decompose it into a spatial receptive field and a\ntemporal receptive field:\nSeparability\nD(x,τ ) =GS(x)DT (τ )\nr(t) = r0 +\ndx\n-inf\ninf\n∫\ndτ D(x,τ )I(x,t -τ )\n-inf\ninf\n∫\nS(t) =\ndxG(x)I(x,t)\n-inf\ninf\n∫\nCorrelation\nr(t) = r0 +\ndτ DT (τ )S(t -τ )\n-inf\ninf\n∫\nConvolution\nwhere\n\nRepresenting stimulus and receptive fields in\nspace and time\ntime\nspace\nSuppose our stimulus is a bar of light extending from x=2 to x=4\nand that is turned on for times from t=1 to t=6\nWe represent it on a space-time plot as follows:\nfiring\nrate\ntime\nlight\nr.f.\n\nRepresenting stimulus and receptive fields in\nspace and time\ntime\nspace\nSuppose we now consider a receptive field D(t,x) with spatial as well\nas temporal structure (but 'space-time separable': D(t,x) = D(t)D(x) )\nfiring\nrate\ntime\nlight\n\nResponse to a moving bar of light\ntime\nspace\nNow suppose our stimulus is moving:\nfiring\nrate\ntime\nlight\n\nResponse to a moving bar of light\ntime\nspace\nNow suppose our stimulus is moving in opposite direction:\nfiring\nrate\ntime\nlight\n\nLearning objectives for Lecture 9\n-\nTo be able to mathematically describe a neural response as a\nlinear filter followed by a nonlinear function.\n- A correlation of a spatial receptive field with the stimulus\n- A convolution of a temporal receptive field with the stimulus\n-\nTo understand the concept of a Spatio-temporal Receptive\nField (STRF) and the concept of 'separability'\n-\nTo understand the idea of a Spike Triggered Average and how\nto use it to compute a Spatio-temporal Receptive Field and a\nSpectro-temporal Receptive Field (STRF).\n\nSpike-Triggered Average\nS(t)\nspikes\n+\n+\n=\nτ\nK(τ )\nK(τ ) = 1\nn\nS(ti -τ )\ni=1\nn\n∑\nSum over n spikes\n\nK(τ ) =\nn\nS(ti -τ )\ni=1\nn\n∑\nTrials\nAverage over trials\n\nMeasuring STRFs in the retina\nMarcus Meister\nSlides pp 25-31 (c) Marcus Meister. All rights reserved. This content is excluded from our Creative Commons license.\nFor more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nVoltage (μV)\nTime (s)\n0.0\n0.2\nSimultaneous Recording from Retinal Ganglion Cells\nSlides pp 25-31 (c) Marcus Meister. All rights reserved. This content is excluded from our Creative Commons license.\nFor more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nRabbit ganglion cells responding to a natural movie\n\"Trees swaying in the breeze\"\nTime (s)\nFiring rate (Hz)\n\nMarcus Meister\nSlides pp 25-31 (c) Marcus Meister. All rights reserved. This content is excluded from our Creative Commons license.\nFor more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nMeasuring STRFs in the retina\nRandom flicker stimulus\nMarcus Meister\nSlides pp 25-31 (c) Marcus Meister. All rights reserved. This content is excluded from our Creative Commons license.\nFor more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nStimulus\nResponse\nSpike-triggered average\nReverse-Correlation to a Random Flicker Stimulus\nMarcus Meister\nSlides pp 25-31 (c) Marcus Meister. All rights reserved. This content is excluded from our Creative Commons license.\nFor more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nSpatio-Temporal Receptive Fields (STRF)\nMarcus Meister\nSlides pp 25-31 (c) Marcus Meister. All rights reserved. This content is excluded from our Creative Commons license.\nFor more information, see https://ocw.mit.edu/help/faq-fair-use/.\nSee Lecture 9 video to view the above clips.\n\n0.5 mm\nTime = -0.150 s\n0.004\n0.04\n-1.0\n0.0\nTime (s)\n-0.150 s\n0.000 s\n-0.075 s\n-0.225 s\n-0.300 s\n-0.375 s\n-0.450 s\n-0.525 s\nMean Effective Stimulus for an ON cell\nSlides pp 25-31 (c) Marcus Meister. All rights reserved. This content is excluded from our Creative Commons license.\nFor more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nSpectro-temporal receptive fields\nWe can use this same approach to describe the responses of\nneurons in the auditory system.\nHigh frequency sound\nLow frequency sound\nMicrophone\nsignal\nWe start by representing sounds in a spectral representation.\n\nSpectrogram\nS( f ,t)\nA spectrogram shows how much power there is in a sound at\ndifferent frequencies and at different times.\n\nSpectro-temporal receptive fields\nSpectro-temporal receptive fields from A1 in monkey.\nFigures removed due to copyright restrictions. See Lecture 9 video or Figure 1 in\ndeCharms, R.C., D.T. Blake and M.M. Merzenich. \"Optimizing Sound Features for\nCortical Neurons.\" Science 280 No. 5368 (1998): 1439-1444.\n\nSpectro-temporal receptive fields\nSpectro-temporal receptive fields from A1 in monkey.\ndeCharms, Blake, Merzenich, Science, 1998\nFigures removed due to copyright restrictions. See Lecture 9 video or Figure 1 & 2\nin deCharms, R.C., D.T. Blake and M.M. Merzenich. \"Optimizing Sound Features\nfor Cortical Neurons.\" Science 280 No. 5368 (1998): 1439-1444.\n\nLearning objectives for Lecture 9\n-\nTo be able to mathematically describe a neural response as a\nlinear filter followed by a nonlinear function.\n- A correlation of a spatial receptive field with the stimulus\n- A convolution of a temporal receptive field with the stimulus\n-\nTo understand the concept of a Spatio-temporal Receptive\nField (STRF) and the concept of 'separability'\n-\nTo understand the idea of a Spike Triggered Average and how\nto use it to compute a Spatio-temporal Receptive Field and a\nSpectro-temporal Receptive Field (STRF).\n\nExtra slides on nonlinear receptive\nfields\n\nNon-linearities\nImagine a neuron with these responses to the following\nstimuli.\nI(x)\nStrong response\nStimulus 1\nx\nNo response\nI(x)\nStimulus 2\nx\nNo response\nI(x)\nStimulus 3\nIf this response was captured by a linear kernel, then, the response to\nStimulus 2 and 3 would be half as large as to Stimulus 1. Thus...\nG1(x) = 0\nr = r0 + G1(x)I(x)dx\n∫\nIs this neuron linear?\n\nNon-linearities\nr = r0 +\ndx1 dx2 G2(x1,x2)I(x1)I(x2)\n∫\nI(x)\nx\nx\nI(x)\nI(x)\nStimulus 1\nStimulus 2\nStimulus 3\nI(x1)I(x2)\nx2\nx1\nI(x1)I(x2)\nx2\nx1\nI(x1)I(x2)\nx2\nx1\n\nNon-linearities\nr = r0 +\ndx1 dx2 G2(x1,x2)I(x1)I(x2)\n∫\nI(x1)I(x2)\nx2\nx1\nI(x1)I(x2)\nx2\nx1\nI(x1)I(x2)\nx2\nx1\nG(x1,x2)\nx2\nx1\nStimulus 1 is the only\none that has overlap\nwith this nonlinear\nkernel.\nThis kernel\nimplements an AND\noperation!\n\nThe Weiner-Volterra expansion is like a Taylor-series\nexpansion for functions:\nr = r0 + G1(x)I(x)dx\n∫\n+\ndx1 dx2 G2(x1,x2)I(x1)I(x2)\n∫\n+\ndx1 dx2 dx3G3(x1,x2,x3)I(x1)I(x2)I(x3)\n∫∫∫\n+\n...\nNon-linearities\n\nSpike-Triggered Average\nK(τ ) = 1\nn\nS(ti -τ )\ni=1\nn\n∑\n- One can show that the spike triggered average is just the\ncross correlation of firing rate and the stimulus\nK(τ ) =\nr(t)S(t -τ )dt\n-inf\ninf\n∫\nreverse correlation\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu/\n\n9.40 Introduction to Neural Computation\nSpring 2018\n\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Lecture 10: Time Series - 9.40 Introduction to Neural Computation",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/9-40-introduction-to-neural-computation-spring-2018/1bf2762c9ef029915015b61f5eaab59a_MIT9_40S18_Lec10.pdf",
      "content": "Introduction to Neural\nComputation\nProf. Michale Fee\nMIT BCS 9.40 -- 2018\nLecture 10 - Time Series\n\nSpatial receptive fields\n- How do we represent receptive fields mathematically?\nLinear-Nonlinear Model (LN Model)\noutput\nspike\nfilter\nx\nnonlinearity\ngenerator\ny\nI(x, y)\nL = [\nG I(\n)\ny\n,\nx\n]\n[\nr = r0 + L]+\nPT [n]\nStimulus\nPoisson\nprocess\nResponse\nStimulus\nFiring rate\nSpatial\nfilter\nImage of mouse in public domain.\n\nSpatial receptive fields\n- How do we represent receptive fields mathematically?\nWe are going to consider the simplest case in which the response\nof a neuron is given by a linear filter acting on the stimulus.\nr = r0 + ∫∫G(x, y)I(x, y)dx dy\nLet's look at this in one dimension\nx\ny\nG(x, y)\nG(x)\nx\nr = r0 + ∫G(x)I(x)dx\n\nSpatial receptive fields\n- How do we represent receptive fields mathematically?\nG(x)\nx\nx\nI(x)\nG(x)I(x)\nx\nG(x)I(x)\nG(x)\nI(x)\n∫G(x)I(x)dx big\n∫G(x)I(x)dx small\n\nTemporal receptive fields\n- We can also think of the response of a neuron as some\nfunction of the temporal variations in the stimulus.\nr(t) = r0 + D[S(t)]\n\nTemporal receptive fields\n- We can think of 'overlap' in the time domain! That there\nis a particular 'temporal profile' of a stimulus that makes a\nneuron spike.\nStimulus\nS(t)\ntime\nLinear Response\nKernel\n\nSpatio-temporal receptive fields\n- How do we represent receptive fields mathematically?\nCombine neural responses into a single kernel that captures\nboth spatial and temporal sensitivity.\noutput\nspike\nfilter\nx\nnonlinearity\ngenerator\ny\nI(x, y,t)\nL( )\nt = K [I(\n,y\n,\nx\nt)] r(t) = [r0 + L(t)]+\nPT [n]\nStimulus\nPoisson\nprocess\nResponse Spatio-\nStimulus\nFiring rate\ntemporal\nImage of mouse in public domain.\nfilter\n\nLearning objectives for Lecture 10\n- Spike trains are probabilistic (Poisson Process)\n- Be able to use measures of spike train variability\n- Fano Factor\n- Interspike Interval (ISI)\n- Understand convolution, cross-correlation, and\nautocorrelation functions\n- Understand the concept of a Fourier series\n\nLearning objectives for Lecture 10\n- Spike trains are probabilistic (Poisson Process)\n- Be able to use measures of spike train variability\n- Fano Factor\n- Interspike Interval (ISI)\n- Understand convolution, cross-correlation, and\nautocorrelation functions\n- Understand the concept of a Fourier series\n\nNeuronal responses are variable\n- Spike trains are often quite variable. The precise pattern of\nspikes on each presentation of a stimulus is different.\nStimulus\ntime\n100 ms\nFigure courtesy MIT Press. From Dayan, P. and L. Abbott. Theoretical Neuroscience: Computational and\nMathematical Modeling of Neural Systems. 2001. Original source: Bair, W. and C. Koch. \"Temporal Precision\nof Spike Trains in Extrastriate Cortex of the Behaving Macaque Monkey.\" Neural Computation 8 no 6 (1996):\n1185-1202.\nResponse of a neuron in area MT of the monkey to\nthe exact same stimulus replayed on each trial.\n\nNeuronal responses are variable\nStimulus\ntime\nΔt\nImagine a random process that produces spikes at an average rate of spikes\nper second during the stimulus presentation.\nμ\nBreak up the spike train into small time bins of some duration Δt. Each spike is\ngenerated independently of other spikes and with equal probability in each bin. then\nwe can write the probability that a spike occurs in any bin as\nIf Δt is small enough that most of the bins have zero spikes, we can write the\nprobability that a spike occurs in any bin as: μ ⋅Δt\nThe probability that no spike occurs in the bin is: 1- μ ⋅Δt\n\nPoisson process\nT\ntime\nM bins\nΔt\nHow many spikes land in the interval T ?\nWhat is the probability that n spikes land in the interval T ? PT [n]\nThis is just the product of three things:\n- The probability of having n bins with a spike = (μ Δt)n\n- The probability of having M-n bins with no spike = (1- μ Δt)M -n\nM !\n- The number of different ways to distribution n spikes in M bins = (M - n)!n! 12\n\nPoisson process\nWhat is the probability that n spikes land in the interval T ?\nM !\nPT [n] = lim\nΔt→0 (M - n)!n! (μ Δt)n (1- μ Δt)M -n\nT\nIn the limit that: Δt → 0\nM =\n→inf\nΔt\nPT [n] = (μT )n\nn! e- μT\nPoisson distribution!\n\nPoisson distribution\nThe Poisson Distribution gives us the probability that n\nspikes land in the interval T\nPT[n] = (μT )n\nn!\ne-μT\nn =\nnPT[n]\nn=0\ninf\n∑\n= μT\nAverage (expected) number of spikes\nThus, is also the average\nspike rate! (going to use variable r)\nμ = n\nT\nn\nn = 1\nn = 4\nn = 10\nPT[n]\nPoisson distribution plot courtesy of Skbkekas on Wikimedia. License: CC BY.\n\nLearning objectives for Lecture 10\n- Spike trains are probabilistic (Poisson Process)\n- Be able to use measures of spike train variability\n- Fano Factor\n- Interspike Interval (ISI)\n- Understand convolution, cross-correlation, and\nautocorrelation functions\n- Understand the concept of a Fourier series\n\nSpike count variability\nWhat is the variance in the number of spikes that land in the\ninterval T ?\nPT[n] = (μT )n\nn!\ne-μT\nσn\n2(T ) =\nn -n\n(\n)\n=\nn2 -2 n\n2 + n\n=\nn2 -n\nσn\n2(T ) = μT\nVariance in spike count\nFano Factor\nF = σn\n2(T )\nn\n= 1\n\nInterspike interval (ISI) distribution\nWhat is the distribution of intervals between spikes?\nti\nti+1\nΔt\nτi\nτi = ti+1 -ti\nThe probability of having the next spike land in the interval\nbetween and is:\nti+1\nti+1 + Δt\nP[τ ≤ti+1 -ti < τ + Δt] =\nPτ[n = 0] = (rτ )0\n0! e-rτ = e-rτ\ne-rτ r Δt\n\nInterspike interval (ISI) distribution\nWhat is the distribution of intervals between spikes?\nti\nti+1\nΔt\nτi\nτi = ti+1 -ti\nThe probability density (probability per unit time) is just\nΔt P[τ ] = re-rτ\nInterval\nτ\n1/ e\nr\nr-1\n\nHomogeneous vs inhomogeneous\nPoisson process\ntime\nStimulus\n100 ms\nrate = μ\nHomogeneous\nrate = μ(t)\nInhomogeneous\nAnnotated figure from Dayan, P. and L. Abbott. Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems. 2001. Original source: Bair, W. and\nC. Koch. \"Temporal Precision of Spike Trains in Extrastriate Cortex of the Behaving Macaque Monkey.\" Neural Computation 8 no 6 (1996): 1185-1202. All rights reserved. This\ncontent is excluded from our Creative Commons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.\n\nLearning objectives for Lecture 10\n- Spike trains are probabilistic (Poisson Process)\n- Be able to use measures of spike train variability\n- Fano Factor\n- Interspike Interval (ISI)\n- Understand convolution, cross-correlation, and\nautocorrelation functions\n- Understand the concept of a Fourier series\n\nConvolution\n- We have discussed the idea of convolution\ny(t) =\ndτ G(τ )x(t -τ )\n-inf\ninf\n∫\n- To model the response of membrane potential to synaptic input\n- To model the response of neurons to a time-dependent stimulus\n- To implement a low-pass or high-pass filter\n-\nIn general, convolution allows us to model the output of a system as\na linear filter acting on its input.\n\nCross-correlation function\n-\nA way to examine the temporal relation between signals.\nK(τ ) =\ndt x(t)y(t +τ )\n-inf\ninf\n∫\nx(t)\ny(t)\nTime Lag\nxc=xcorr(ShftNoisyData,NoisyData,Nlags);\nK = x y\n\nRelation between Convolution and Cross-\ncorrelation\n-\nThese are mathematically very similar, but are used differently.\nThink of as long vectors (signals)\nx(t) and y(t)\nThink of as a short vector (kernel)\nK(τ )\ny(t) =\ndτ K(τ )x(t -τ )\n-inf\ninf\n∫\nConvolution\nTake input signal and\nconvolve it with kernel K to get\noutput signal .\nx(t)\ny(t)\nK(τ ) =\ndt x(t)y(t +τ )\n-inf\ninf\n∫\nCross-correlation\nTake two signals, ,\nand cross-correlate to extract a\ntemporal 'kernel' K.\nx(t) and y(t)\nRelation to STA\n\nAutocorrelation\nK(τ ) =\ndt x(t)x(t +τ )\n-inf\ninf\n∫\n- A way to examine the temporal structure within a signal.\n50 ms\n\nAutocorrelation\n- A way to examine the temporal structure within a signal.\nxcRaw=xcorr(Data,Nlags);\n50 ms\nTime\nTime Lag\n~100 ms\n\nAutocorrelation\nK(τ ) =\ndt x(t)x(t +τ )\n-inf\ninf\n∫\n- A way to examine the temporal structure within a signal.\n50 ms\n\nAutocorrelation\n- A way to examine the temporal structure within a signal.\nxcRaw=xcorr(Data,Nlags);\n50 ms\nTime\nTime Lag\n100 ms\nTime\n1 ms\nTime Lag\n\nAutocorrelation\n- A way to examine the temporal structure within a signal.\n50 ms\nData = randn(1,N)+0.1*cos(2*pi*10*time);\nTime\nTime Lag\n100 ms\n\nLearning objectives for Lecture 10\n- Spike trains are probabilistic (Poisson Process)\n- Be able to use measures of spike train variability\n- Fano Factor\n- Interspike Interval (ISI)\n- Understand convolution, cross-correlation, and\nautocorrelation functions\n- Understand the concept of a Fourier series\n\nS( f ,t)\nA spectrogram shows how much power there is in a sound at\ndifferent frequencies and at different times.\nSpectral Analysis\n\nSpectral Analysis\nTime\nFrequency (Hz)\n100 ms\nBabbling baby bird\nTime\nFrequency (Hz)\nTime\nHippocampal theta rhythm\n100 ms\n\nFourier Series\na1 cos 2π f0t\n(\n)\n-\nWe can express any periodic function of time as sums of sine and\ncosine functions.\nWe could approximate this square wave with a cosine wave of the\nsame period T and amplitude.\nOscillation frequency\nf0 = 1\nT\nCycles per second (Hz)\nω0 = 2π\nT\nAngular frequency\nRadians per second\n-\nLet's start with an even function that is periodic with a period T\nt\nT\n\ncos ω0t\n(\n)\nt\ncos 2ω0t\n(\n)\ncos 3ω0t\n(\n)\n-\nBut we can get a better approximation if we add some more cosine\nwaves to our original one...\nFourier Series\nT\nWhy can we restrict ourselves to only frequencies that are integer\nmultiples of ω0 ?\nBecause only cosines that are integer multiples of ω0 are periodic with\na period T!\n\ncos ω0t\n(\n)\ncos 2ω0t\n(\n)\ncos 3ω0t\n(\n)\nt\nFourier Series\nT\ny(t) = a1 cos ω0t\n(\n) + a2 cos 2ω0t\n(\n) + a3 cos 3ω0t\n(\n) + ...\n-\nBut we can get a better approximation if we add some more cosine\nwaves to our original one...\n\nFourier Series\ncos ω0t\n(\n)\ncos 3ω0t\n(\n)\ncos 5ω0t\n(\n)\ncos 7ω0t\n(\n)\ncos 9ω0t\n(\n)\ncos 11ω0t\n(\n)\ncos 13ω0t\n(\n)\n\nFourier Series\ncos nω0t\n(\n)\nn =\nconstructive\ninterference\nconstructive\ninterference\ndestructive\ninterference\ndestructive\ninterference\n\ncos ω0t\n(\n)\ncos 2ω0t\n(\n)\ncos 3ω0t\n(\n)\nt\nFourier Series\nT\ny(t) =\na1 cos ω0t\n(\n) + a2 cos 2ω0t\n(\n) + a3 cos 3ω0t\n(\n) + ...\nyeven(t) = a0\n2 +\nan cos nω0t\n(\n)\nn=1\ninf\n∑\nDC term\na0\n2 +\n\nHow do we find the coefficients?\na0\n2 = 1\nT\ny(t)dt\n-T /2\nT /2\n∫\n-\nThe coefficient is just like the average of our function y(t).\na0\n-\nThe coefficient is just the overlap of our function y(t) with\na1\na1 = 2\nT\ny(t)cos ω0t\n(\n)dt\n-T /2\nT /2\n∫\ncos ω0t\n(\n)\n-\nThe coefficient is just the overlap of our function y(t) with\na2\na2 = 2\nT\ny(t)cos 2ω0t\n(\n)dt\n-T /2\nT /2\n∫\ncos 2ω0t\n(\n)\na0 = 2\nT\ny(t)cos(0ω0t)dt\n-T /2\nT /2\n∫\n-\nThe coefficient is just the overlap of our function y(t) with\nan\nan = 2\nT\ny(t)cos nω0t\n(\n)dt\n-T /2\nT /2\n∫\ncos nω0t\n(\n)\nCorrelation!\n\na0 = 2\nT\ny(t)dt\n-T /2\nT /2\n∫\na1 = 2\nT\ny(t)cos ω0t\n(\n)dt\n-T /2\nT /2\n∫\na2 = 2\nT\ny(t)cos 2ω0t\n(\n)dt\n-T /2\nT /2\n∫\nConsider the following functions y(t):\ny(t) = 1\na0 = 2\na1 = 0\na2 = 0\ny(t) = cos(ω0t)\na0 = 0\na1 = 1\na2 = 0\ny(t) = cos(2ω0t)\na0 = 0\na1 = 0\na2 = 1\ny(t) = a0\n2 + a1 cos ω0t\n(\n) + a2 cos 2ω0t\n(\n) + ...\ncos ω0t\n(\n)cos 2ω0t\n(\n)dt\n-T /2\nT /2\n∫\n= 0\ncos ω0t\n(\n)\n⎡⎣\n⎤⎦\n2dt\n-T /2\nT /2\n∫\n= T\nHow do we find the coefficients?\n\nFourier Series\n-\nIf a function has maximal overlap with one of our cosine functions,\nthen it has zero overlap with all the others!\nˆx1\nv\nˆx2\na1 ˆx1\na2 ˆx2\nv = a1 ˆx1 + a2 ˆx2\na2 = v ⋅ˆx2\nHow do we find the coefficients a1 and a2?\na1 = v ⋅ˆx1\nˆx2 = [1 , 0]\nˆx1 = [0 , 1]\n!v = [a1 , a2]\n=\nvix1\ni\ni∑\n=\nvi\ni∑\nx2\ni\n-\nWe say that our set of cosine functions form an orthogonal basis set...\nun(t) = cos(nω0t)\na1 = 2\nT\ny(t)cos ω0t\n(\n)dt\n-T /2\nT /2\n∫\n\nFourier Series\n- Now let's look an an odd (antisymmetric) function...\nT\nsin 2π\nT t\n⎛\n⎝⎜\n⎞\n⎠⎟\nsin ω0t\n(\n) =\nsin 2ω0t\n(\n)\nsin 3ω0t\n(\n)\nyodd(t) = b1sin ω0t\n(\n) + b2 sin 2ω0t\n(\n) + b3sin 3ω0t\n(\n) + ...\nyodd(t) =\nbn sin nω0t\n(\n)\nn=1\ninf\n∑\nWhy is there no DC term here?\n\nFourier Series\n- For an arbitrary function, we can write it down as the\nsum of a symmetric and an antisymmetric part.\ny(t) =\na0\n+\nan cos nω0t\n(\n)\n+\nn=1\ninf\n∑\nbn sin nω0t\n(\n)\nn=1\ninf\n∑\nsymmetric\nantisymmetric\n\nComplex Fourier Series\n- We can express any periodic function of time as sums of\ncomplex exponentials.\neiω t = cosωt +isinωt\ne-iω t = cosωt -isinωt\nθ(t) = ωt\nθ\nEuler's formula\ni = -i\ncosωt = 1\n2 eiωt +e-iωt\n(\n)\nsinωt = 1\n2i eiωt -e-iωt\n(\n)\nRewrite as follows...\n= -i\n2 eiωt -e-iωt\n(\n)\nRe eiωt\n⎡⎣\n⎤⎦\nIm eiωt\n⎡⎣\n⎤⎦\n\nFourier Series\ny(t) =\na0\n+\nan cos nω0t\n(\n) +\nn=1\ninf\n∑\nbn sin nω0t\n(\n)\nn=1\ninf\n∑\ny(t) =\nA0\n+\nAn einω0 t\nn=1\ninf\n∑\n+\nA-ne-inω0 t\nn=1\ninf\n∑\n'DC' or\n'constant'\nterm\npositive\nfrequencies\nnegative\nfrequencies\nAn = 1\n2 an -ibn\n(\n)\nA-n = 1\n2 an + ibn\n(\n)\nA0 = a0\nAn = A-n\n(\n)\n*\ncomplex conjugates\ny(t) =\na0\n+\nan\n2 einωt +e-inωt\n(\n) +\nn=1\ninf\n∑\n-ibn\neinωt -e-inωt\n(\n)\nn=1\ninf\n∑\n\nComplex Fourier Series\n- We can write this more compactly as follows:\ny(t) =\nAneinω0t\nn=-inf\ninf\n∑\nFor n = 0,\neinω0t = e0 = 1\ny(t) =\nA0\n+\nAn einω0 t\nn=1\ninf\n∑\n+\nA-ne-inω0 t\nn=1\ninf\n∑\n=\nAn einω0 t\nn=0∑\n+\nAn einω0 t\nn=1\ninf\n∑\n+\nAneinω0 t\nn=-1\n-inf\n∑\n\nLearning objectives for Lecture 10\n- Spike trains are probabilistic (Poisson Process)\n- Be able to use measures of spike train variability\n- Fano Factor\n- Interspike Interval (ISI)\n- Understand convolution, cross-correlation, and\nautocorrelation functions\n- Understand the concept of a Fourier series\n\nExtra Slides on Poisson process\nHow many spikes land in the interval T ?\nWhat is the probability that n spikes land in the interval T ?\ntime\nΔt\nT\nM bins\nThis is just the product of three things:\n-\nThe probability of having n bins with a spike =\n-\nThe probability of having M-n bins with no spike =\n-\nThe number of different ways to distribution n spikes in M bins =\n(μ Δt)n\n(1-μ Δt)M -n\nM!\n(M -n)!n!\nPT[n]\n\nWhat is the probability that n spikes land in the interval T ?\nPT[n] = lim\nΔt→0\nM!\n(M -n)!n!(μ Δt)n (1-μ Δt)M -n\nΔt →0\nM = T\nΔt →inf\nNote that as :\nM -n M\nlim\nΔt→0 (1-μ Δt)M -n = lim\nΔt→0 (1-μ Δt)\nT\nΔt\n= lim\nε→0 (1+ ε)\nε\n⎡\n⎣⎢\n⎤\n⎦⎥\n-μT\n= lim\nε→0 (1+ ε)\n-μT\nε\nε = -μ Δt\nΔt = -μ\nε\n= e -μT\nExtra Slides on Poisson process\n\nWhat is the probability that n spikes land in the interval T ?\nPT[n] = lim\nΔt→0\nM!\n(M -n)!n!(μ Δt)n e-μT\nPT[n] = 1\nn!\nT\nΔt\n⎛\n⎝⎜\n⎞\n⎠⎟\nn\n(μ Δt)n e-μT\nPT[n] = (μT )n\nn!\ne-μT\nPoisson distribution!\n=\nT\nΔt\n⎛\n⎝⎜\n⎞\n⎠⎟\nn\nM →inf\nNote that as :\nM!\n(M -n)! = M(M -1)(M -2) ⋅⋅⋅(M -n +1)\n≈M n\nn terms\nExtra Slides on Poisson process\n\nMIT OpenCourseWare\nhttps://ocw.mit.edu/\n9.40 Introduction to Neural Computation\nSpring 2018\nFor information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms."
    }
  ]
}