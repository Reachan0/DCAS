{
  "course_name": "System Dynamics II",
  "course_description": "Continuation of 15.871, emphasizing tools and methods needed to apply systems thinking and simulation modeling successfully in complex real-world settings. Uses simulation models, management flight simulators, and case studies to deepen the conceptual and modeling skills introduced in 15.871. Through models and case studies of successful applications students learn how to use qualitative and quantitative data to formulate and test models, and how to work effectively with senior executives to implement change successfully.",
  "topics": [
    "Business",
    "Management",
    "Operations Management",
    "Organizational Behavior",
    "Engineering",
    "Systems Engineering",
    "Computational Modeling and Simulation",
    "Systems Optimization",
    "Business",
    "Management",
    "Operations Management",
    "Organizational Behavior",
    "Engineering",
    "Systems Engineering",
    "Computational Modeling and Simulation",
    "Systems Optimization"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nRecitations: 1 session / week, 1.5 hours / session\n\nGeneral Information\n\n15.872 System Dynamics II\nis the second half-semester continuation of\n15.871 Introduction to System Dynamics\n. Taken in sequence the courses constitute the introductory sequence in system dynamics. Successful completion of both\n15.871\nand\n15.872\nis a prerequisite for advanced courses in system dynamics, work as a research or teaching assistant in the field, or careers using system dynamics.\n\nPrerequisites\n\n15.871 Introduction to System Dynamics\n\nCourse Objectives and Scope\n\nWhy do so many business strategies fail? Why do so many others fail to produce lasting results? Why do businesses suffer from periodic crises and fluctuations in sales, earnings, and morale? Why do some firms grow while others stagnate? And how can an organization's leadership and managers identify and design high-leverage policies, policies that are not thwarted by unanticipated side effects?\n\nAccelerating economic, technological, social, and environmental change challenges managers to learn at increasing rates. Today's economy requires us to design and manage complex systems where dynamic complexity is unavoidable, thanks to multiple feedback effects, long time delays, and nonlinear responses to our decisions. Yet learning in such environments is difficult precisely because we never confront many of the consequences of our most important decisions. Effective learning in such environments requires methods to develop systems thinking by representing and assessing dynamic complexity. It also requires tools that managers can use to accelerate learning throughout an organization.\n\n15.871\nand\n15.872\nintroduce you to system dynamics modeling for the analysis of business policy and strategy. You will learn to visualize a business organization in terms of the structures and policies that create dynamics and regulate performance. System dynamics allows us to create 'microworlds,' management flight simulators where space and time can be compressed, slowed, and stopped so we can experience the long-term side effects of decisions, systematically explore new strategies, and develop our understanding of complex systems. In these system dynamics courses we use simulation models, case studies, and management flight simulators to develop principles of policy design for successful management of complex strategies. Case studies of successful strategy design and implementation using system dynamics will be stressed. We consider the use of systems thinking to promote effective organizational learning.\n\nThe principal purpose of modeling is to improve our understanding of the ways in which an organization's performance is related to its internal structure and operating policies as well as those of customers, competitors, suppliers, and other stakeholders. During the course you will use several simulation models to explore such strategic issues as fluctuating sales, production and earnings; market growth and stagnation; the diffusion of new technologies; the use and reliability of forecasts; the rationality of business decision making; and applications in health care, energy policy, environmental sustainability, and other topics.\n\nStudents will learn to recognize and deal with situations where policy interventions are likely to be delayed, diluted, or defeated by unanticipated reactions and side effects. You will have a chance to use state of the art software for computer simulation and gaming. Assignments give hands-on experience in developing and testing computer simulation models in diverse settings.\n\nCourse Materials\n\nThe required text is:\n\nSterman, J.\nBusiness Dynamics: Systems Thinking and Modeling for a Complex World\n. McGraw-Hill / Irwin, 2000. ISBN: 9780072389159.\n\nThere are additional required readings. Articles and case studies as assigned will be listed in the\nReadings\nsection. Additional readings will be handed out on an occasional basis.\n\nModeling Software\n\nIn this course, we use modeling software. Several excellent packages for system dynamics simulation are available, including\niThink\n, from High Performance Systems,\nPowersim\n, from Powersim Corporation, and\nVensim\n, from Ventana Systems. All are highly recommended. You may wish to learn more about these packages, as all are used in the business world and expertise in them is increasingly sought by potential employers. For further information, see the following resources:\n\niThink\n: See\nthe isee Systems web site\n.\n\nPowersim\n: See\nthe Powersim web site\n.\n\nVensim\n: See\nthe Ventana Systems web site\n.\n\nThe required modeling software is VensimPLE\n. In this course, we will be using the Vensim Personal Learning Edition (\nVensimPLE\n) by Ventana Systems. The current version is 6.1. It is free for academic use and is available for Windows and Mac. VensimPLE comes with sample models, help engine, and Adobe Acrobat format User's Guide, all available from\nthe Vensim web site\n.\n\nNOTE:\nThe disc that comes with the\nBusiness Dynamics\ntextbook includes a version of VensimPLE. However, the version available online is much newer and has enhanced functionality. Be sure to download the current version of VensimPLE from the website above. Vensim models on the textbook CD will work with it.\n\nGrading\n\nACTIVITIES\n\nPERCENTAGES\n\nAssignments\n\nClass participation\n\nPeer evaluation\n\nAssignments are due by 5 pm on the due date--you should never skip class to complete an assignment. Each assignment is graded on a 10-point scale. A minimum of two points will be forfeited for assignments turned in late. Assignments handed in more than one class late receive no credit.\nThis policy is strictly enforced\n.\n\nPeer Evaluation\n\nSome assignments will be done in teams of three. At the end of the course you will have the opportunity to evaluate your teammates. The Peer Evaluation is confidential. You will assess how well your teammates contributed to your team's assignments and your individual learning. Specifically, each member of a team will assign a total of 20 points to the other two members of the team, with more points indicating higher contribution of that person to the team. Raters must differentiate some in their ratings (this means each rater must give at least one score of 11 or higher, with a maximum of 15, and at least one score of 9 or lower). As a result team peer evaluation will produce differences in grades only within teams. The best strategy is to do your best on each assignment, to help your teammates understand the concepts and to create a constructive, supportive environment that enhances everyone's learning.\n\nCalendar\n\nSES #\n\nTOPICS\n\nKEY DATES\n\nAssignment out\n\nAssignment due\n\nSystem Dynamics in Action: Re-engineering the Supply Chain in a High-velocity Industry\n\nAssignment 1 out\n\nManaging Instability Part 1: Formulating and Testing Robust Models of Business Processes\n\nManaging Instability Part 2: The Supply Line and Supply Chains\n\nManaging Instability Part 3: Forecasting and Feedback: Bounded Rationality or Rational Expectations?\n\nAssignment 2 out\n\nAssignment 1 due\n\nBoom and Bust: Real Estate, Shipbuilding, Commodities, Financial Markets\n\nCutting Corners and Working Overtime: Service Quality Management\n\nAssignment 3 out, and Dell\n\nAssignment 2 due\n\nService Quality Dynamics: Customer Service at Dell\n\nDell\n\nSystem Dynamics in Action: Applications of System Dynamics to Environmental and Public Policy Issues\n\nMeet LEW: Late, Expensive, and Wrong: The Dynamics of Project Management\n\nAssignment 4 out\n\nProject Dynamics Modeling in the Real World\n\nGetting Things Done: Firefighting, Capability Traps, and Death Spirals\n\nSystem Dynamics in Action: The Implementation Challenge\n\nConclusion: How to keep Learning; Follow-up resources; Career Opportunities\n\nAssignment 4 due",
  "files": [
    {
      "category": "Assignment",
      "title": "Assignment 1: Structure and Behavior of Delays - 15.872 Fall 2013",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-872-system-dynamics-ii-fall-2013/4a4a650965a526dcd7cd778d0483a895_MIT15_872F13_ass1.pdf",
      "content": "Originally prepared by John Sterman; last updated by JS and HR October 2013.\n\ndenotes a question for which you must hand in an answer, a model, or a plot.\n\ndenotes a tip to help you build the model or answer the question.\n\nSystem Dynamics Group\nSloan School of Management\nMassachusetts Institute of Technology\n\nSystem Dynamics II, 15.872\nProfessors Hazhir Rahmandad and John Sterman\n\nAssignment 1\nStructure and Behavior of Delays\n\nAssigned: Monday 28 October 2013; Due: Wednesday 6 November 2013\n\nThis assignment should be done in a team totaling three people and submitted digitally\n\nDelays are a critical source of dynamics in nearly all systems. Thus far in our modeling,\nhowever, we have represented delays in causal diagrams qualitatively. In this assignment you\nexplore the structure and behavior of delays and test their responses to a range of inputs. You\nwill also apply your knowledge of delays to an important real-world issue: the current glut of\nnatural gas in the US. The assignment helps you understand the dynamics of delays so that you\ncan use them appropriately in more complex models. The assignment also develops your skills\nin model formulation and analysis. To do this assignment it is essential that you first read\nchapter 11 in the text.\n\nA.\nMaterial Delays\nA1. Do the challenge on page 425 of the text (Response of Material Delays to Steps,\nRamps, and Cycles). Note that you are asked to:\n\nSketch your intuitive estimate of the response of a first order material delay to the\nfour test inputs shown in Figure 11-9.\n\nTo assist you, Figure 11-9 is reproduced on the following pages so you can\nsketch your estimates directly on the graphs by hand or digitally; you will\nneed to submit your response digitally so you can take a picture of your\nhand-drawn graphs and paste it into your write up.\n\nYour grade will not be affected by your answer to this part of the question.\nPlease be honest and sketch your responses before simulating the model;\nthis is important for building your intuition about delays.\n\nA2. After sketching your intuitive estimates, build and simulate the model for the first-order\nmaterial delay.\n\nThe structure and equations for a first-order delay are described in the text\n(section 11.2.4). Build the delay explicitly as a stock and flow structure shown in\nFigure 11-4 (that is, do not use Vensim's built-in function DELAY1).\n\nSet the initial value of the stock of letters in transit so that the delay is always\ninitialized in equilibrium, independent of the initial value of the input. In\nequilibrium, the stock in transit is unchanging, so the inflow and outflow to the\ndelay must be equal. Solve this equation for the equilibrium value for the stock\nin transit, and enter this parametric expression as the initial condition for the\nstock in your model. Run your model with a few different constant input values\nto confirm that it begins and remains in equilibrium. You do not need to hand in\nthe equilibrium run.\n\nWe have created a \"test input generator\" that will make it easy for you to\ngenerate the step, ramp, sine wave, and other test inputs you need to run the tests.\nDownload the model, TestGen.mdl, from the course website and use it to\ngenerate the input patterns you need. The test input generator is fully\ndocumented and dimensionally consistent (see appendix 1).\n\nTo learn more about the parameters for the built-in functions used in the Test\nInput Generator (specified in the Appendix) and in the following exercises, use\nthe pull-down \"Help\" menu in Vensim. Click on \"Keyword Search...\" and type\nin the function name (e.g., STEP), and hit Enter.\n\nNote that the test generator has an initial value of one, so the exponential growth\nrate you need to replicate the test input in the graphs is 0.05/day. The ramp slope\nneeded is 0.05/day.\n\nSet the initial time to -5 days, the final time to 25 days, and the Time Step (dt) to\n0.125 days. Be sure to save output every time step (in the Time Bounds tab\nunder Settings..., in the Model menu).\n\nTo show the relationship between the input and output, define a custom graph\nthat shows the input and output rate on the same graph, and with the same scale.\nAlso use the strip graph tools to examine the behavior of the stock in transit.\n\nIt may be helpful to envision the delay as the post office, with the input\nrepresenting the mailing rate, the output representing the delivery rate, and the\nstock in transit representing the letters in the post office system.\n\nAfter you run the model for each of the test inputs, compare the results to your\nintuitive estimates and comment briefly. Were your mental simulations correct?\nWhy? / Why not? In particular,\n\n1.\nConsider the linear ramp. The response of any system to a shock consists of a\ntransient, during which the relationship of input and output is changing, and a\nsteady state in which the relationship between input and output is no longer\n\nchanging (even though the input and output might both be growing, for example).\nDoes the output of the delay (e.g., the delivery rate of letters) equal the input (e.g.,\nthe mailing rate) in the steady state for the linear ramp? Explain.\n\n2.\nHow does the response to the sine wave depend on the delay time relative to the\nperiod of the cycle? In particular, as the delay time increases relative to the period\nof the cycle, what happens to the amplitude and timing of the output?\n\nA3.\n\nNext, repeat the steps above (both mental and formal simulation) for a third order\nmaterial delay with the same average delay time of 5 days.\n\nBuild the delay explicitly as shown in section 11.2.5. Do not use the Vensim\nbuilt-in function DELAY3. Figure 11-6 shows a second order material delay;\nthe third order delay is analogous but contains three intermediate stocks of\nmaterial in transit (see equation 11-4).\n\nBuild the third order delay in the same model as your first order delay and use\nthe same input test generator. This allows you to compare the behavior of the\nthird order delay to the first order delay.\n\nA4.\n\nNext, repeat the steps above (again both mental and formal simulation) for a\npipeline delay with the same average delay time of 5 days.\n\nBuild the pipeline delay using the DELAY FIXED function in Vensim. The\npipeline delay is described in section 11.2.3. To access the DELAY FIXED\nfunction from the variable definition dialog in VensimPLE, click on the tab\nlabeled Functions; you will then see an alphabetical list of all the built in\nfunctions available in PLE. The syntax for the DELAY FIXED function is:\n\nOUTPUT = DELAY FIXED(INPUT, DELAY TIME, INITIAL OUTPUT)\n\nBuild the pipeline delay in the same model as your other delays so you can\neasily compare the output of the three delay types.\n\nFour test inputs to a delay\nBefore simulating the model, sketch the response of a first order material delay to each of these inputs, assuming a 5 day average\ndelay time.\n\nFour test inputs to a delay\nBefore simulating the model, sketch the response of a third order material delay to each of these inputs, assuming a 5 day average\ndelay time.\n\nFour test inputs to a delay\nBefore simulating the model, sketch the response of a Pipeline delay to each of these inputs, assuming a 5 day average delay time.\n\nB.\nInformation Delays\n\nIn the previous sections we considered material delays, where the input to the delay is a physical\ninflow of items to a stock of units in transit, and the outflow is the physical flow of items exiting\nthe stock. Many delays, however, exist in channels of information feedback--for example, in the\nmeasurement or perception of a variable, such as the order rate for a product.\n\nIn this example, the expected order rate is management's belief about the value of the order rate.\nThe expected order rate lags behind the actual order rate due to delays in measuring and reporting\norders, and due to the time it takes managers to update their beliefs about the order rate. The\nphysical order rate does not flow into the delay; rather information about the order rate enters the\ndelay. For reasons discussed in Business Dynamics, Ch. 11, a different structure is needed to\nrepresent these information delays.\n\nAs an example of an information delay, consider the way a firm forecasts demand for its\nproducts. Why does forecasting inevitably involve a delay? Firms must forecast demand\nbecause it takes time to adjust production to changes in demand, and because it is costly to make\nlarge changes in production. They don't want to respond to temporary changes in demand but\nonly to sustained new trends. A good forecasting procedure should filter out random changes in\nincoming orders to avoid costly and unnecessary changes in output while still responding quickly\nto changes in trends to avoid costly stockouts and lost business.\n\nOne of the most widely used forecasting techniques is called \"exponential smoothing\" or\n\"adaptive expectations.\" As discussed in section 11.3.1, \"Adaptive expectations\" means the\nforecast adjusts (adapts) gradually to the actual stream of orders for the product. If the forecast is\npersistently wrong, it will gradually adjust until the error is eliminated. The structure of an\nexponential smoothing delay is shown in Figure 11-10. Specifying this structure for the case of a\ndemand forecast leads to the following:\n\nThe equation for the rate at which expectations adapt--the rate at which the forecast is revised--\nis:\n\nd(EOR)/dt = (IOR - EOR)/TEOR\n\nwhere\n\nEOR = Expected Order Rate (widgets/month)\n\nIOR = Actual Incoming Order Rate (widgets/month)\n\nTEOR = Time to adjust the Expected Order Rate (months)\n\nThe \"time constant\" TEOR represents the time required, on average, for expectations to respond\nto a change in actual conditions. The forecast, EOR, is a stock, since the forecast is a state of the\nsystem, in this case representing a state of mind of the managers regarding what the future order\nrate will be. This stock remains at its current value until there is some reason to change it. In\nadaptive expectations, the stock (the managers' belief about the future order rate) changes when\nthere is a difference between the current order rate and their belief about the expected order rate.\nThis structure is known as a first-order information delay, or 'first-order exponential smoothing'.\n\nBuild a model to represent this forecasting procedure. Your model should include the equations\nabove exactly. Do not add any additional structure.\n- Assume TEOR is 6 months.\n- Assume the order rate IOR is exogenous. Use the Test Input Generator in the Appendix\nto determine the incoming order rate. You will have to change the units for time in the\ntest generator from days to months.\n- Set the initial value of the forecast to 100 widgets/month.\n- Use a simulation time step of 0.25 months and simulate the model for 60 months.\n- Create a custom graph that traces the behavior of both IOR and EOR.\n\nB1. Run your model with a random input to demand. Set the Noise Standard Deviation to\n0.50, and the Noise Start Time to 5 months. That is, the noise should have a standard\ndeviation of 50% of the initial input.\n\nTo filter out random variation in incoming orders, should the adjustment time be long or\nshort?\n\nB2. Consider the response to a step input. Set the Step Height to 0.50 and the Step Time to 5\nmonths.\n\nTo respond quickly to a permanent change in incoming orders should the adjustment time\nbe long or short?\n\nC. 0 points--unless you don't document your model, in which case the grader may deduct an\narbitrary number of points. Hand in the dimensionally consistent and documented\nVensim model files (.mdl) along with your submission (including the first order, third\norder, and pipeline material delays, and for the information smoothing delay).\n\nD. Delays in Action: the Natural Gas Glut\n\nNow let's apply your knowledge of delays to model an important issue: the dynamics of natural\ngas supply and prices.\n\nHydraulic fracturing (fracking) has led to a boom in exploration for, and production of, natural\ngas. Modern fracking was introduced around 1998. Fracking suddenly made it possible to find\nand produce large quantities of gas that were previously too expensive or technically\nunrecoverable. As shown in the graphs below, drilling activity in the US boomed, reaching a\npeak around 2006-2008 about three times higher than the average for the 1980s-1990s. Proved\nreserves soared. However, the demand for gas is quite inelastic in the short run. Consumption\nhas been growing modestly and did not increase nearly as much as production capacity.\n\nAs a result, the price of natural gas plunged. As seen below, real prices (measured in constant\n2010 dollars) fluctuated from about $6 to over $10 per thousand cubic feet (tcf) from about 2005\nthrough 2008. Since then, however, prices have tumbled, reaching a low of less than $2/tcf, an\nenormous decline (the price has rebounded to about $3/tcf today). Some of the decline can be\nattributed to the recession and weak economy since 2008. But as you can see in the graph of\nconsumption, demand did not fall much from 2008-2010. The main source of the price crash lies\non the supply side: The US is awash in gas--At $2-3/tcf, gas is now so cheap that many\nexploration and development firms are losing money. As the New York Times reported (20\nOctober 2012; the full article is appended to this assignment):\n\n\"...while the gas rush has benefited most Americans, it's been a money loser so far for many of\nthe gas exploration companies and their tens of thousands of investors.\n\nThe drillers punched so many holes and extracted so much gas through hydraulic fracturing that\nthey have driven the price of natural gas to near-record lows. And because of the intricate\nfinancial deals and leasing arrangements that many of them struck during the boom, they were\nunable to pull their foot off the accelerator fast enough to avoid a crash in the price of natural\ngas, which is down more than 60 percent since the summer of 2008.\n\n...most of the industry has been bloodied -- forced to sell assets, take huge write-offs and shift\nas many drill rigs as possible from gas exploration to oil, whose price has held up much better.\n\nRex W. Tillerson, the chief executive of Exxon Mobil, which spent $41 billion to buy XTO\nEnergy, a giant natural gas company, in 2010, when gas prices were almost double what they are\ntoday, minced no words about the industry's plight during an appearance in New York this\nsummer.\n\n\"We are all losing our shirts today,\" Mr. Tillerson said. \"We're making no money. It's all in the\nred.\"\n\nThe US natural gas industry. From top: exploration activity (wells drilled/month); spot price (2010 $/tcf, Henry\nHub. Henry Hub is an important gas pipeline distribution center in Louisiana; Henry Hub spot and futures contracts\nare traded in commodity markets including the NY Mercantile Exchange [NYMEX]. Prices deflated using the\nProducer Price Index [PPI]); proved reserves (trillion cubic feet); total US consumption, in trillion cubic feet/year.\nSource: Energy Information Administration, US Department of Energy.\n\nWe can conceptualize the impact of fracking technology as a shift in the long run supply curve\nthat increased the quantity of gas that can be produced at a given price (a hypothetical supply\ncurve is shown in the graph below). In the short run, fracking lowered the marginal cost of\ndeveloping a new well below the market price, creating excess profits and a huge incentive to\ndrill. Standard economic theory suggests (1) price should fall to a new equilibrium at the new,\nlower marginal cost, and (2) exploration should expand capacity until the marginal cost of gas\nonce again equals the market price. Profits would rise until this happens and then return to\nnormal levels, but not drop below normal. However, as shown in the data and described above,\ndrilling expanded too much and too long, resulting in a glut and prices significantly below\nmarginal cost for many producers. The industry dramatically overshot equilibrium. How did this\nhappen? In this section, you will use your knowledge of delays to build a simple model of the\nexploration process.\n\nQuantity\n($/tcf)\nMarginal cost\nbefore Fracking\nMarginal cost\nafter Fracking\nDemand\nCurve\nEquilibrium\nbefore\nEquilibrium after\n\nNote on model boundary: To keep this assignment simple, the model is not intended to be a\ncomplete representation of the market. In particular, we will focus on the supply side, assuming\n(for simplicity) that demand is constant. We will also use a simple model for price determination\nand market clearing; a more sophisticated approach is described in Business Dynamics, Ch.\n13.2.12 and Ch. 20.2.6. Further, although gas is a nonrenewable resource, the model omits the\ndepletion of the resource base. Finally, natural gas production and combustion generate methane\nand CO2 emissions, the two most important greenhouse gases contributing to global climate\nchange. These impacts are also omitted. We also omit environmental issues around fracking\nincluding seismicity, water pollution, air quality, and other local environmental impacts.\n\nCausal Structure of the Market: The diagram below shows a causal diagram of the supply side\nof the natural gas market. There are two main stocks, Production Capacity and Capacity Under\nDevelopment. Production capacity (measured in million cubic feet/year of potential production\nfrom existing wells), increases when new exploration and development projects are completed,\nadding to the stock of producing wells, and decreases when the wells and associated production\nand distribution equipment reach the end of their useful life and are decommissioned. There are\nimportant time delays in the reaction of exploration and production to changes in market and\ntechnological conditions. When entrepreneurs believe that it is profitable to develop new\nresources, they initiate new development projects. The flow of new projects increases the stock\nof projects under development. After some time, these projects are completed and the number of\nproducing wells increases. Capacity under development captures all exploration and development\nprojects that have been initiated but not yet completed, including capital budgeting and financing,\nplanning, site location, lease acquisition, acquisition of drill rigs, and the time required to carry\nout the actual drilling. For simplicity, these stages are not explicitly separated.1\n\nThe decision to expand or contract drilling activity is based on entrepreneurs' and energy\ncompany managers' beliefs about whether new exploration will be profitable. If they believe that\nthe price of gas (in the future, when the wells they begin to develop now will begin to produce)\nexceeds the marginal cost of development, they will expand drilling activity. If they believe\nfuture prices will be below marginal costs they reduce the number of projects they start.\nMarginal costs depend on the long run supply curve: the larger the total exploited resource, the\nhigher the marginal cost will be. These relationships create the balancing Marginal Cost\nfeedback: an increase in profitability leads to more exploration and development, raising the\nmarginal costs of the next development prospects until profitability drops enough to stop further\ndevelopment. Note however that it takes time to assess the marginal cost of new wells. Each\nregion, geologic zone, and region differs, and the costs of new technologies are uncertain.\nAlthough fracking lowered the marginal cost of development in many places, its benefits were\nnot immediately obvious: Developers had to gain enough experience with the technology to\nupdate their prior beliefs about costs. Hence there is a lag in the response of developers'\nperceptions of marginal cost to changes in actual costs.\n\nThe second feedback in the diagram shows the determination of spot prices. Prices respond to\nthe balance of demand and production capacity. When there is excess capacity, prices fall,\ncausing the most expensive wells to be shut in. Price, and utilization, continue to fall until price\nreaches a level that clears the market. The result is the balancing Price feedback, shown as B2 in\nthe diagram. If profitability is high, exploration increases, eventually increasing production\ncapacity. Prices then fall, eroding profitability.\n\nExpected profitability depends not on the current spot price, but on the expected future price.\nEnergy prices are notoriously volatile--a rise in price today, or this month, may not persist long\nenough to justify an expansion in drilling. Entrepreneurs tend to wait and see whether prices will\nstay high before adjusting their price expectations upward. Research shows that people's price\nexpectations are strongly conditioned on past prices (see Business Dynamics, Chapter 16).\n\n1 A more detailed model would recognize that some wells lead to large discoveries while others are dry holes. For\nthe purpose of this model we aggregate over all development prospects. That is, we assume developers account for\nthe expected value of well productivity and the probability of dry holes when assessing expected profitability.\n\nMarginal Cost\nPerceived\nMarginal Cost\nExpected\nPrice\nExpected\nProfit\nCapacity Under\nDevelopment\nProduction\nCapacity\nProject\nInitiation\nProject\nCompletion\nDecommissioning\n+\n+\n-\n+\nAverage\nDevelopment\nDelay\nAverage Life of\nCapacity\n-\n-\nDemand\n+\nEffect of Expected\nProfit on Investment\n+\nTime to Form Price\nExpectations\nTime to Perceive\nMarginal Cost\n+\nLong Run\nSupply Curve\nSensitivity of\nInvestment to\nProfitability\n<Reference\nProduction\nCapacity>\n+\nTotal Exploited\nResource\n+\n+\nPrice\nSensitivity of\nUtilization to Profit\n-\n+\n+\n+\n+\nB1\nMarginal Cost\nB2\nPrice\n<Perceived\nMarginal Cost>\n+\n\nTasks:\n\nD1. The causal diagram above shows several delays. These are:\n\na. Time to form price expectations (affects the Expected Price)\n(the average time required for developers to adjust their expectations regarding the\nfuture price as prices vary);\n\nb. Time to perceive marginal costs (affects Perceived Marginal Cost)\n(the average time required for developers to adjust their beliefs about the marginal\ncosts of exploration and development to new information);\n\nc. Average development delay (affects Project Completion)\n(the average lag between the decision to develop new wells and project completion);\n\nd. Average life of capacity (affects Decommissioning)\n(the average useful life of a well and associated production equipment).\n\nUsing your best judgment, estimate the average duration of each delay, in years (or fractions of\nyears). Do not do any research or gather any data to respond: we are interested in your\nintuitive estimate of the average delay time for each of these processes. Provide a brief (1-2\nsentence) justification for your estimate. Hint: Consider the relationship among these delays,\nparticularly the first three. Your grade for this part does not depend on the estimate you provide,\nbut on the clarity of your explanation.\n\nD2. Consider the shape of the response of the development delay. Using the graph below,\nsketch the shape of the project completion flow to a pulse in project initiation. That is, if a\nthousand new wells were started all at once, what would the pattern of completions look like as\ntime unfolds? Provide a time axis for your sketch. Briefly explain the shape you choose. See\nBusiness Dynamics, Ch. 11.2 and Figure 11-2 for an example and discussion of the relevant\nconsiderations.\n\nPulse of wells all started at time 0\nWells/Year\nTime (Years)\n\nD3. Download and open the model, Natural Gas Market.mdl from the course website. The\nmodel corresponds to the causal diagram above. The equations and units of measure are all\nspecified for you, with the exception of the formulations for the delays. Given your responses to\nthe questions above, specify the equations for perceived costs, expected price, and the\ndevelopment delay. Hint: Pay particular attention to the type of delay (material or\ninformation), the average duration of the delay, and the order of the delay. Do not agonize\nover the order of the delays: the main choices, as described in BD Ch. 10, are first order,\nintermediate order, and a pipeline delay. You can use the built-in third order delay functions in\nVensim if you want to specify an intermediate order delay.\n\nD4. Provide documentation for your formulations (enter the explanation in the comment field\nin the equations for the delays you have specified). Check to be sure your model is\ndimensionally consistent.\n\nD5. Simulate the model with your delay formulations and values. The model is set up to\nbegin in equilibrium. The model is set up to simulate the introduction of fracking. To do so, the\nlong run supply curve shifts down by 20% in year 0 (see the Marginal Cost view in the model).\nNote: the simulation begins in year -2 and runs through year 40, with a time step of 0.015625\nyears (1/64 of a year, or a little less than 1 week).\n\nIn your write-up, provide a screenshot of the Dashboard view in the model showing the response\nof the system to the change in marginal costs with your estimated parameters. What happens,\nand why?\n\nD6. Explore how the behavior of the market and exploration changes as you vary the duration\nof the time delays (use Synthesim and the sliders for each delay time). Briefly explain the impact\nof changes in the delays on the response.\n\nD8. Briefly explain the policy implications of your results. Why has the introduction of\nfracking led to a glut of natural gas and steep losses for exploration and development firms?\nWhat do you think is likely over the next decade or more in terms of exploration, demand, and\nprices?\n\nE. 0 points--unless you don't document your model, in which case the grader may deduct an\narbitrary number of points. Hand in the dimensionally consistent and documented\nVensim model file (.mdl) along with your submission.\n\nF. What to hand in and how to submit your work\nWrite up your responses to the questions above in a word (.docx) document. In addition,\nyou need to submit the various Vensim model (.mdl) files called for in parts C and E.\nth\nUpload your team's assignment by 5 PM on November 6 . Submit your\nassignment as a single .zip file including your response document and models.\nName files with your team's name, and for multiple files of the same type, the assignment\nsection, e.g. \"Team21.docx\", \"Team21-C.mdl\", all submitted as part of \"Team21.zip\".\n\nAppendix 1: Test Inputs\n\nSystem dynamics modeling software provides functions for the most commonly used test inputs:\nstep, pulse, ramp, sine wave, exponential, and uncorrelated (or \"white\") noise, etc. These inputs\nare not necessarily intended to correspond to anything that really happened in the past or that will\nhappen in the future. Rather, the inputs are designed to help you easily explore the dynamics of a\nmodel. With a very simple input, it is easy to see the dynamics generated by the model. More\ncomplicated input patterns, such as actual historical data, make it difficult to isolate the behavior\ngenerated by the model's structure from the input pattern. Once the dynamics of the structure are\nunderstood, it is usually possible to grasp how the structure will behave with more complicated\ninputs such as the actual historical input data.\n\nA useful \"Test Input Generator\" is provided by the following structure. Note: When using this in\nother models, be sure to check the units for time and modify if necessary, along with the default\nparameters.\n\nEquations:\n\nInput=\n\n1+STEP(Step Height,Step Time)+\n\n(Pulse Quantity/TIME STEP)*PULSE(Pulse Time,TIME STEP)+\n\nRAMP(Ramp Slope,Ramp Start Time,Ramp End Time)+\n\nSTEP(1,Exponential Growth Time)*(EXP(Exponential Growth Rate*Time)-1)+\n\nSTEP(1,Sine Start Time)*Sine Amplitude*SIN(2*3.14159*Time/Sine Period)+\nSTEP(1,Noise Start Time)*RANDOM NORMAL(-4, 4, 0, Noise Standard\nDeviation, Noise Seed)\n\n~ Dimensionless\n\n~ The test input can be configured to generate a step, pulse, linear\nramp, exponential growth, sine wave, and random variation. The initial value\n\nof the input is 1 and each test input begins at a particular start time. The\nmagnitudes are expressed as fractions of the initial value.\nStep Height=0\n\n~ Dimensionless\n\n~ The height of the step increase in the input.\n\nStep Time=0\n\n~ Day\n\n~ The time at which the step increase in the input occurs.\n\nPulse Quantity=0\n\n~ Dimensionless*Day\n\n~ The quantity added to the input at the pulse time.\n\nPulse Time=0\n\n~ Day\n\n~ The time at which the pulse increase in the input occurs.\n\nRamp Slope=0\n\n~ 1/Day\n\n~ The slope of the linear ramp in the input.\n\nRamp Start Time=0\n\n~ Day\n\n~ The time at which the ramp in the input begins.\n\nRamp End Time=1e+009\n\n~ Day\n\n~ The end time for the ramp input.\n\nExponential Growth Rate=0\n\n~ 1/Day\n\n~ The exponential growth rate in the input.\n\nExponential Growth Time=0\n\n~ Day\n\n~ The time at which the exponential growth in the input begins.\n\nSine Start Time=0\n\n~ Day\n\n~ The time at which the sine wave fluctuation in the input begins.\n\nSine Amplitude=0\n\n~ Dimensionless\n\n~ The amplitude of the sine wave in the input.\n\nSine Period=10\n\n~ Day\n\n~ The period of the sine wave in the input.\n\nNoise Seed=1000\n\n~ Dimensionless\n\n~ Varying the random number seed changes the sequence of realizations\nfor the random variable.\n\nNoise Standard Deviation=0\n\n~ Dimensionless\n\n~ The standard deviation in the random noise. The random fluctuation\nis drawn from a normal distribution with min and max values of +/-\n4. The user can also specify the random number seed to replicate\n\nsimulations. To generate a different random number sequence,\nchange the random number seed.\n\nNoise Start Time=0\n\n~ Day\n\n~ The time at which the random noise in the input begins.\n\n********************************************************\n\n.Control\n********************************************************~\n\nSimulation Control Parameters\n\n|\n\nFINAL TIME = 25\n\n~ Day\n\n~ The final time for the simulation.\n\nINITIAL TIME = -5\n\n~ Day\n\n~ The initial time for the simulation.\n\nSAVEPER =\nTIME STEP\n\n~ Day\n\n~ The frequency with which output is stored.\n\nTIME STEP = 0.125\n\n~ Day\n\n~ The time step for the simulation.\n\nA Note on Random Noise\n\nThe random input is useful to simulate unpredictable shocks. The RANDOM_NORMAL\nfunction in Vensim samples from a normal distribution with parameters the user specifies. The\nfunction has the following syntax:\n\nRANDOM_NORMAL (min, max, mean, std dev, seed)\n\nVensim uses a default random number \"seed.\" You can specify a different seed by defining a\nconstant called \"Noise Seed\" in your model and setting it equal to some value (e.g. Noise Seed =\n1000). Vensim generates a single random sequence for any given seed. Let's say the sequence\nis: 0.500, 0.213, 0.678, 0.932, 0.340, 0.015. If there is a single random number function in the\nmodel it will simply yield the random sequence. If there are two or more random functions, the\nfunctions will take turns accessing the sequence. For example, if you have two functions, the\nfirst will yield 0.5, 0.678, 0.34; and the second will yield 0.213, 0.932, 0.015. If you run two\nsimulations with the same seed, you will get exactly the same sequence of random numbers.\nThis is important so that you can compare two runs with different policies and be sure the\ndifferences in behavior are due only to the policies and not to different realizations of the random\nnumber generator. When you do want to examine runs with different realizations of the random\nprocess, you need to change the value of the random number seed.\n\nNote also that the use of a function such as RANDOM_NORMAL means a new random number\nis selected every time step. Cutting the time step in half would then double the number of\nrandom shocks to which the model is subjected, and increase the highest frequency represented\nin the random signal. This is generally not good modeling practice. In realistic models, one must\nnot only select the standard deviation of any random processes, but also specify its frequency\nspectrum (or, equivalently, the autocorrelation function). Failure to do so can lead to spurious\nresults and make your model overly sensitive to the time step. These issues are discussed in\nAppendix B.\n\nA Note on the Pulse Function\n\nThe pulse function is used to simulate the effect of instantaneously adding a fixed quantity Q to a\nvariable. To ensure the entire quantity is added all at once (within a single time step, or DT [delta\ntime]), the duration of the pulse is set to the smallest interval of time in the model, that is, to the\ntime step DT. The height of the pulse is then the quantity to be added divided by the time step in\nthe model, Q/DT. The inflow increases by the height of the pulse and remains at the higher level\nfor one time step, so that the total quantity added to the accumulation is (Q/DT)*DT = Q units.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n\n15.872 System Dynamics II\nFall 2013\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Assignment 2: Understanding Business Fluctuations - 15.872 Fall 2013",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-872-system-dynamics-ii-fall-2013/1b79076bb5c16be81bb3d3a4374e2a47_MIT15_872F13_ass2.pdf",
      "content": "System Dynamics Group\nSloan School of Management\nMassachusetts Institute of Technology\n\nSystem Dynamics II, 15.872\nProfs. John Sterman and Hazhir Rahmandad\n\nAssignment 2\nUnderstanding Business Fluctuations: The Causes of Oscillations*\n\nAssigned: Wednesday 6 November 2013; Due: Monday18 November 2013\nPlease do this assignment in a group totaling three people and submit digitally.\n\nOscillations and cyclic phenomena are among the most common and important dynamic\nbehaviors. The economy as a whole and many individual industries suffer from chronic\ninstability in production, demand, employment, and profits, with resulting turnover of top\nmanagement. Tempering these industry cycles has proven difficult. This assignment develops\nyour ability to formulate dynamic models of a business situation and analyze their behavior. You\nwill build on the model of a firm we developed in class and explore the causes of and cures for\noscillations. The assignment also serves as an introduction to model-based policy analysis.\n\nCase Background: A manufacturing firm, Widgets, Inc., has experienced chronic instability in\nits inventory level and production rate.1 Specifically, production and inventory undergo large,\ncostly variations over time, variations larger than those in incoming orders. Your client,\nWidgets, Inc., wants to understand why it has experienced these persistent oscillations, and how\nto stabilize operations.\n\nThe data show oscillations in inventory and production (while specific data for the Widgets firm\nare confidential, the graphs below show large fluctuations in aggregate capacity utilization for the\nmanufacturing sector of the US economy, and also in overall civilian unemployment).\n\nOriginally prepared by John Sterman. Last revision: October 2013.\n1 For reasons of confidentiality, the name of the client firm and details of their operations have been disguised.\n\nCapacity Utilization, Total Industry (US)\n\nSource: US Federal Reserve\n\nUS Civilian Unemployment Rate\n\nSource: US Bureau of Labor Statistics\n\nYou hypothesize that the cause of the oscillations is the policy structure used by Widgets to\ncontrol inventories and production. Specifically, if inventories are below the desired level\nrequired to provide good service, the company produces more, but inventory overshoots the\ndesired level, leading to excess stocks. The company reacts by cutting production, which causes\ninventories to fall below the goal. The negative (balancing) loop in the diagram below\nsummarizes your initial hypothesis:\nInventory\nDesired Inventory\nInventory Gap\nProduction\n+\n+\n+\n-\nB\nInventory\nControl\nDELAY\n\nIn the class discussion of this issue, we developed a simple model to begin testing this\nhypothesis. Your modeling strategy is to start with the simplest possible model, expand it as\nnecessary, and never let the model's complexity exceed your understanding of it.\n\nSummary of model development thus far: Your discussions with management and workers on\nthe factory floor revealed the following information.\n\nCustomer Demand and Order Fulfillment\n\n- Widget Inc.'s customers are delivery-sensitive. To meet demand, Widgets operates a\nmake-to-stock system and carries significant inventories of finished product. The firm's\ninventory of finished goods is increased by production and decreased by shipments.\n- Whenever inventory becomes inadequate, shipments fall below the order rate: the lower\nthe average inventory level relative to the target level, the greater the probability that\nindividual items will be out of stock. Since Widgets carries many different SKUs, some\nitems are likely to be out of stock even when the maximum shipment rate equals the\nactual order rate.\n- The order fulfillment ratio is the fraction of orders that Widgets can fill (the service\nlevel). The order fulfillment ratio would be 85% when the aggregate inventory level is\nequal to the required level. To provide the higher service level customers demand,\nWidgets maintains significant safety stocks.\n- Orders that Widgets can't deliver are lost forever, as the customer finds an alternate\nsource of supply. (There is no backlog of unfilled orders.)\n\nTo keep the initial model simple--so that we can learn from it--we assume that\nfeedbacks to the customer order rate can be omitted for the purpose of this assignment.\nTherefore you and the client have agreed to assume that the customer order rate is\nexogenous. Later, you may consider adding feedbacks to customer demand (but not for\nthis assignment). The client is particularly concerned with the response of the firm's\ninventory and production to unanticipated increases in demand (that is, to situations in\nwhich the forecast of demand is erroneous, as is often the case).\n\nProduction Scheduling and Inventory Control\n- The production process involves a significant delay due to the complexity of the\nfabrication and assembly process.\n- The average manufacturing cycle time (the time between the start of the production\nprocess and its completion) is four weeks.\n- Some items in the product line can be made faster than four weeks, and some take longer.\n- There is ample plant and equipment to meet demand. Your contact at corporate\nheadquarters argues that the plant can adjust production starts rapidly to changes in\nproduction schedules.\n- Desired production (the factory production target) is determined by anticipated\n(forecasted) customer orders, modified by a correction to maintain inventory at the\ndesired level.\n- The firm continuously compares the desired inventory level to the actual level. They\nattempt to correct discrepancies between desired and actual inventory in eight weeks.\n- Desired inventory is equal to desired inventory coverage multiplied by the forecast of\ncustomer orders. Desired coverage is equal to the minimum order processing time (two\nweeks) plus a safety stock of two additional weeks of coverage. The total desired\ncoverage of four weeks provides enough inventory on average to avoid costly stockouts\nwithout incurring excessive carrying costs.\n- The desired production start rate depends on the desired rate of production and on the\nquantity of work in process (WIP). When WIP is low compared to the level needed to\nmeet the desired production rate, additional units are started. If WIP is too high relative\nto the desired WIP level, production starts are reduced.\n- The desired level of WIP is the amount required to complete production at the desired\nrate given the manufacturing cycle time.\n- The firm adjusts the level of WIP to the desired level over a two-week period.\n\nA.\nBase Model\n\nThe model is called <Widgets> and is available on the CD in the textbook in the Chapter 18\nfolder, and will a\n\nIMPORTANT: Change the Manufacturing Cycle Time in the Widgets model from 8 to 4\nweeks. Use a value of 4 weeks in your base case.\n\nThe model is described in detail in chapter 18 (pp. 709-723). Be sure to read this section\ncarefully so you understand the formulations. In the model customer orders are exogenous\nand are determined by a test generator (see Appendix 2 to this assignment). The test generator\nmakes it easy to challenge the model with a wide variety of patterns for customer orders,\nincluding steps, ramps, cycles, and random noise. Feel free to try your model with any of the\ntest inputs, particularly noise, but for the purposes of the assignment you only need to use the\nstep input.\n\nBrevity is a virtue in your write up. Unless specifically requested, do not include complete\nsets of output (graphs, tables) for each test and simulation you do. A summary table will\nsuffice. Construct a table showing the minimum/maximum values of inventory and\nproduction, the amplification ratio, the period of any oscillation, the \"settling time\" (the time\nrequired for, say, production, to settle within 2% of its equilibrium value), and any other\nindicators of performance and stability that are helpful to make your points.\n\nHowever, as always, you must document and explain the changes you make in the equations\nso that an independent third party can replicate your simulations.\n\nTo begin your analysis, confirm that your model begins in equilibrium by running the model\nwithout any changes in parameters. Do not hand in the equilibrium run.\n\nNow run the base case of the model with a 20% step input in customer orders in week 5 (set Step\nHeight = 0.20). Examine the behavior.\nThe model includes a variety of custom graphs to help you understand its behavior. You may\nwish to modify these or create new custom graphs to illustrate the points you wish to make in\nyour write up. In particular, it is interesting to make a phase plot showing the Production\nStart Rate as a function of Inventory.\n\nA1. Briefly explain the model's base case behavior. Describe in managerial terms (as if you\nwere presenting your results to the client) what happens after the step increase in\ncustomer orders. Present a minimum of graphs to make your points.\n\nA2.\na. Does the Production Start Rate change by less, the same, or more than the change in\norders? Quantify your response by calculating the amplification ratio (refer to Appendix\n1 of this assignment for a discussion of amplification and the amplification ratio).\nlso be posted. Copy the model to your hard disk.\n\nb. Identify the sources of any amplification.\n\nc. What are the specific structural reasons for the degree of amplification you find? That is,\nwhat parameters and structures control the magnitude of amplification?\n\nA3. Can this model oscillate? Briefly explain why or why not. That is, are there any\ncircumstances in which this model can generate the reference mode of oscillating\nproduction and inventory?\nTo answer, you will need to conduct sensitivity tests in which you vary the parameters of\nthe decision rules and physical structure (see the discussion on sensitivity testing in\nchapter 21).\nAs you make these tests, beware of \"DT error\"--a situation in which the time period used\nto update the stocks is too long relative to the time constants in the system (see Appendix\nA in the text on numerical integration).\nWe are interested in whether the model can endogenously generate an oscillation.\nObviously, if customer orders fluctuate, the production rate will also fluctuate, but that is\nnot an endogenous oscillation. You want to know if the model will oscillate in response\nto a single shock such as an unanticipated step increase in orders.\nA4. Can this model oscillate with plausible and realistic parameter values? It is not enough\nto generate the behavior of interest in your model. Your model must generate that\nbehavior for the right reasons. If you find that there are parameter values that cause\noscillation, are these values plausible or realistic? In answering this question consider the\nreal-life interpretation of the decision processes of the actors in your model. Given the\nparameter values needed to generate oscillations, would real firms behave in such a way?\nExplain.\n\nB.\nExtending the Model: Adding the Labor Force\nAfter exploring the behavior of your initial model, you return to the client and argue that the\ndynamic hypothesis, as stated, is not sufficient to explain the persistent fluctuations plaguing the\nfirm. You note that the model currently assumes that the production start rate is equal to the\ndesired production start rate. This assumption was justified by the statement of a member of the\nclient team from corporate headquarters that there was ample plant and equipment in the factory,\nso that production starts can be altered rapidly to equal the rate called for by the schedule.\nNevertheless, you conclude that the initial model must be revised to include a more realistic\nrepresentation of the production start rate.\n\nYou visit the plants again to observe how production starts are actually determined, focusing on\nwhether there are any time delays, and what the sources of delay might be. Your observations\nand discussions with plant personnel reveal that, while the firm indeed has ample physical plant\nand equipment, labor cannot be hired and trained instantaneously.\n\nChanging the workforce is a multi-step process.\n- First, a target or desired workforce is determined from the production targets developed by\nthe plant managers. The desired workforce is determined by desired production and average\nproductivity, which is 20 widgets per worker per week and is quite constant over time.\n- Next, the desired workforce must be reviewed and authorized by corporate headquarters, a\nprocess that requires an average of four weeks. (During this time analysts examine the latest\ncost, inventory, and sales data and try to determine whether the plants' requests are sensible.\nThen their recommendations are forwarded to the relevant management committee, which\ndeliberates before acting on the plant's requests.)\n- There are two inputs to the hiring decision. First, there are replacement hires. Any workers\nwho quit or retire are replaced immediately. Second, the hiring rate is adjusted above or\nbelow the replacement rate to close any gap between the authorized and actual workforce. Of\ncourse, hiring cannot fall below zero.\n- Eight weeks are required on average to adjust the actual workforce to the authorized level\nthrough incremental hires. (The delay is due to the time required to create and fill vacancies.)\n- The firm has a no-layoff policy, and workers stay with the firm an average of 50 weeks (one\nyear), so any reduction in workforce has to be accomplished through attrition.\nWorking with your client team, you translate the description of Widgets' labor policies into the\nfollowing policy structure diagrams.\n\nFirst, the Production Start Rate no longer depends on the Desired Production Start Rate, but\non a new variable, the Normal Production Rate. The Normal Production Rate is the rate of\noutput achievable by the current Labor Force given Normal Labor Productivity.\n\nDiscussion with your client suggests that the labor authorization process can be modeled as a\nfirst-order information delay. See Business Dynamics, Ch. 11 for explanation of the different\ntypes of delays. A first order information delay is the same structure used here to model the\ndemand forecast (expected orders). The Factory Labor Request depends, as described above,\non the Desired Production Start Rate and Normal Labor Productivity. The labor force is\nincreased by hiring and decreased by the attrition rate. The firm hires to replace attrition and also\nadjusts the labor force to the authorized level.\n\nNext, you draw a high-level diagram of the labor supply chain and hiring process.\n\nThe diagrams that you develop with your clients are not sufficiently detailed for you to begin\nwriting equations. Expand the diagrams to show explicitly the structure for the delays and\nanything else you might need to complete the model. Clearly label the important feedback loops\nadded. You will need to add a few more variables (both constants and auxiliary variables) to\ncomplete your diagram. Once you have completed the diagram, write the equations for the labor\nsector and add them to your model. Wherever possible try to use formulations and structures\nanalogous to those we have discussed in class! You will not need anything fancy or\ncomplicated.\nDo not use built-in functions such as SMOOTH or DELAY. Instead represent the\nstructures you need explicitly.\nMake sure that all units of measure balance and that the model begins in equilibrium (see\nsection 18.1.5, pp. 716-720, for guidance). Do not hand in the equilibrium run.\n\nB1. Hand in your documented new model (the mdl file), which should include in comments\nan explanation of how the equations for labor acquisition capture the verbal description\nabove.\n\nB2. Before running the model, consider how you think the model will respond to an\nunanticipated 20% step increase in customer orders in week 5. Draw by hand the pattern\nof behavior you expect for inventories, production starts, and labor. Pay attention to the\n\nphase relationships among the variables, that is, the leads and lags of the variables\nrelative to one another. Hand in your drawing or its picture. (Please do this before\nsimulating your model. Your grade is not affected by your answer to this question).\n\nB3.\nNow test the response of the model to a 20% step increase in orders in week 5. This is\nyour new base run.\na. Hand in graphs showing orders, expected orders, desired production, desired production\nstarts, production starts, and actual production, all on the same scale.\n\nb. Also hand in a plot of inventory and desired inventory, on the same scale, and a plot of\nWIP and Desired WIP, on the same scale.\n\nc. Hand in a plot of Labor and the Factory Labor Request on the same scale.\n\nd. Hand in a graph showing inventory coverage and the order fulfillment ratio.\n\ne. Finally, hand in a phase plot showing Labor vs. Inventory. The phase plot shows\ninventory on the x-axis and labor on the y-axis. Use the custom graph feature of Vensim\nto create this graph.\nB4. Explain the behavior produced by the step increase in shipments. You may find the\nexplanation emerges naturally as you answer the following questions:\n\n- Why isn't the system in equilibrium when inventory first equals desired inventory?\n\n- Why isn't the system in equilibrium when production first equals shipments?\n\n- Why does production overshoot its equilibrium value?\n\n- Why does it undershoot?\nYour analysis should proceed step by step, explaining at each juncture in the behavior\nwhat is occurring and why. You should refer to your causal-loop diagram and make use\nof your knowledge of stock and flow structures. Also, feel free to plot additional\nvariables (you may find Vensim's causal tracing feature valuable); but hand in additional\noutput only if you refer to it in your explanation. Strive for an explanation that a manager\ncould understand.\n\nB5.\na. Now that labor is represented in the model, what is the amplification of the production\nrate relative to the order rate?\n\nb. Is it larger or smaller than the amplification generated by the model without labor? Why?\n\nc. Identify any new sources of amplification.\n\nC.\nPolicy Analysis: Stabilizing the Firm\n\nNow you can begin to use your model to explore policies to stabilize the firm. Before policy\nanalysis is meaningful, however, we must be clear about our objectives.\nC1.\nHand in answers for the questions below.\nDon't present a lot of graphs. Make a table showing your predictions of the behavior for\nall the parameters you test and your simulation results. Parsimony and brevity are key.\nHand in your explanations for the different results.\na. Provide a brief explanation of what \"stability\" means in this context.\nIn assessing stability, you may want to consider the excursions of inventory away from\ndesired inventory, the amplification of production with respect to customer orders, the\nperiodicity of any fluctuations, and how long it takes the system to return to equilibrium.\n\nb. To stabilize the system, should the Time to Adjust Inventory discrepancies be increased\nor decreased? Write down your answer before simulating the model. Be honest--your\ngrade does not depend on whether your answer is correct.\n\nc. Test your intuition by simulating the model with Time to Adjust Inventory lengthened or\nshortened by 50%, according to your prediction. Is your intuition confirmed? Try other\nvalues for the Time to Adjust Inventory until you are satisfied you understand its effects\non the behavior of the firm. Explain the effect of the change on stability in terms of the\nfeedback structure of the system.\n\nd. Repeat the analysis you did for the Time to Adjust Inventory for the other important time\nconstants in the model, including the behavioral parameters (e.g. the time to authorize\nlabor, the time to adjust WIP), and the physical delays (e.g. the manufacturing cycle time\nand the time for incremental hires). Be sure to write down your prediction for each\nparameter before simulating. Briefly explain (in a sentence or two) how each parameter\naffects the model and why. Do all the parameters have the same effect on stability?\nIn the interest of brevity, you may want to summarize your results in a table.\n\ne. What policies would you recommend at this point? (Which parameters would you\nchange and how?)\n\nD.\nOvertime/Undertime\n\nThe model thus far does not permit any overtime or undertime: The Normal Production Rate\nrepresents the rate of production attainable with the current labor force and a standard workweek\n(the workweek is held constant). To test the effect of a variable workweek, modify the equation\nfor the Production Start Rate as follows.\n\nEquations:\n\nProduction Start Rate = Normal Production Rate * Relative Workweek\n\nWidgets/Week\n\nThe production rate is the normal rate modified by the relative amount of over or undertime.\n\nRelative Workweek = Table for Relative Workweek(Schedule Pressure)\n\nDimensionless\n\nThe relative workweek is a lookup function of schedule pressure\n\nSchedule Pressure = Desired Production Start Rate /Normal Production Rate\n\nDimensionless\n\nSchedule pressure is the ratio of desired production starts to normal production (the production\nachievable at the normal workweek). Values greater than one indicate a need for overtime;\nvalues less than one indicate a need for undertime.\n\nThese equations allow production to vary according to the adequacy of the current labor force.\nThe variable Relative Workweek represents the change in production above or below the\nstandard rate caused by variation in the workweek. The workweek effect is determined by\nschedule pressure, which is the ratio of the desired to standard rate of production. The\nrelationship between Schedule Pressure and Relative Workweek should incorporate both\nphysical constraints on the use of over/undertime and common practice in the use of a variable\nworkweek. If your client used overtime and undertime, you would try to represent their\n\npractices. For the base case of the model, set the relative workweek equal to unity for all values\nof schedule pressure (no over/under time). The values you use should be:\n\nThe input, schedule pressure, runs from 0 to 2 in steps of 0.2. The output, for the base case, is\nconstant at 1.0 (no over- or undertime). You can try different policies for workweek by changing\nthe values of the relationship in the simulation dialog box when you run the model.\n\nDon't change the workweek table function by editing the model. Instead, use the Set button\nin the top toolbar, then enter the values for the table by clicking on it in the diagram.\nAlternatively, you can change the table values in Synthesim mode by clicking on the table.\nExamine the response of the complete model to a 20% step increase in incoming orders, using\nthe base case parameters and the following overtime policy:\nSchedule Pressure Relative Workweek\n\n0.0\n1.00\n\n0.2\n1.00\n\n0.4\n1.00\n\n0.6\n1.00\n\n0.8\n1.00\n\n1.0\n1.00\n\n1.2\n1.10\n\n1.4\n1.20\n\n1.6\n1.25\n\n1.8\n1.28\n\n2.0\n1.30\n\nD1.\n\na. Explain (in terms a manager would understand) the relationship between Schedule\nPressure and Relative Workweek.\n\nb. What is the meaning of the line Relative Workweek = 1?\n\nc. What is the meaning of the line Relative Workweek = Schedule Pressure?\n\nD2. Explain why the behavior would be stabilized or destabilized by flexible workweeks.\n\nE.\nSynthesis of Policy Analysis and Recommendations\n\nNow, reflect on what you have learned. Think about the high-leverage points to stabilize the\nfirm. If you were responsible for reengineering the firm, what policies would you recommend?\nYou may recommend one of the policies you have tried so far, novel policies you may think of,\nor any combination of these. Try to keep your changes realistic (it is not possible to produce or\nhire instantly, for example, though you might be able to make substantial cuts in these delays).\nDesign and simulate your preferred policy.\n\nE1.\n\na. Give a managerial description of how you believe managers can stabilize their company.\nThis description differs from a model-based description. For example, managers\nprobably do not have a parameter called \"Inventory Adjustment Time\"--instead they\nthink in terms of how fast they jump on an inventory problem (words like \"panic\" or\n\"relaxed\" might be useful). A managerially oriented description moves the focus back to\nthe real system, and casts your model in its true role as an aid to decision-making, not a\ntheoretical treatise.\n\nb. After your managerial description, give the model parameters you use to represent this\nreal-world policy.\n\nE2. Hand in a very brief evaluation of how your policy works in the simulation model. You\nmight create a table comparing the base case to the your policy run. You might also\ninclude one or two plots comparing the base to the policy run. (Remember to be explicit\nabout the parameter differences between the two runs--we must be able to replicate your\nsimulations.)\n\nE3. Hand in the your full, documented model, including overtime and undertime as an mdl\nfile.\n\nF. What to hand in and how to submit your work\nWrite up your responses to the questions above in a word (.docx) document. In addition,\nyou need to submit the various Vensim model (.mdl) files called for in parts B and E.\n\nUpload your team's assig\nby 5 PM on November 18th. Submit your\nassignment as a single .zip file including your response document and models.\nMake sure you include your team-members' names in the document. Name files with\nyour team's name, and for multiple files of the same type, the assignment section, e.g.\n\"Team21.docx\", \"Team21-B.mdl\", all submitted as part of \"Team21.zip\".\nnment\n\nAppendix 1: Amplification\n\nManufacturing systems tend to amplify changes in incoming orders. \"Amplification\" is defined\nas the ratio of the change in the output of a system to the change in the input, that is A =\nOutput/Input. (Engineers: amplification is a rough measure of the closed loop gain of the\nsystem.) If A<1, then the system attenuates disturbances in the environment (the output\nfluctuates less than the input). Conversely, if A>1, the system amplifies disturbances in the\nenvironment. In the context of the current model, we are interested in the extent to which the\nfirm's production management policy amplifies or attenuates changes in incoming orders. That\nis, we are interested in the response of production to changes in incoming orders.\n\nTo figure the amplification ratio, calculate the maximum change in production as a fraction of\nthe change in incoming orders. For example, if incoming orders rise from 10,000 to 12,000, then\nInput=2,000 units. If production reached a peak of 13,150 units, then Output would be 3,150\nunits, and amplification A would equal 3,150/2,000=158%.\n\nAppendix 2: Test Inputs\n\nSystem dynamics modeling software provides functions for the most commonly used test inputs:\nstep, pulse, ramp, sine wave, exponential, and uncorrelated (or \"white\") noise, etc. These inputs\nare not necessarily intended to correspond to anything that really happened in the past or that will\nhappen in the future. Rather, the inputs are designed to help you easily explore the dynamics of a\nmodel. With a very simple input, it is easy to see the dynamics generated by the model. More\ncomplicated input patterns, such as actual historical data, make it difficult to isolate the behavior\ngenerated by the model's structure from the input pattern. Once the dynamics of the structure are\nunderstood, it is usually possible to grasp how the structure will behave with more complicated\ninputs such as the actual historical input data.\n\nA useful \"Test Input Generator\" is provided by the following structure. Note: When using this in\nother models, be sure to check the units for time and modify if necessary, along with the default\nparameters.\n\nEquations:\n\nInput=\n\n1+STEP(Step Height,Step Time)+\n\n(Pulse Quantity/TIME STEP)*PULSE(Pulse Time,TIME STEP)+\n\nRAMP(Ramp Slope,Ramp Start Time,Ramp End Time)+\n\nSTEP(1,Exponential Growth Time)*(EXP(Exponential Growth Rate*Time)-1)+\n\nSTEP(1,Sine Start Time)*Sine Amplitude*SIN(2*3.14159*Time/Sine Period)+\nSTEP(1,Noise Start Time)*RANDOM NORMAL(-4, 4, 0, Noise Standard\nDeviation, Noise Seed)\n\n~ Dimensionless\n\n~ The test input can be configured to generate a step, pulse, linear\nramp, exponential growth, sine wave, and random variation. The initial value\nof the input is 1 and each test input begins at a particular start time. The\nmagnitudes are expressed as fractions of the initial value.\nStep Height=0\n\n~ Dimensionless\n\n~ The height of the step increase in the input.\n\nStep Time=0\n\n~ Day\n\n~ The time at which the step increase in the input occurs.\n\nPulse Quantity=0\n\n~ Dimensionless*Day\n\n~ The quantity added to the input at the pulse time.\n\nPulse Time=0\n\n~ Week\n\n~ The time at which the pulse increase in the input occurs.\n\nRamp Slope=0\n\n~ 1/ Week\n\n~ The slope of the linear ramp in the input.\n\nRamp Start Time=0\n\n~ Week\n\n~ The time at which the ramp in the input begins.\n\nRamp End Time=1e+009\n\n~ Week\n\n~ The end time for the ramp input.\n\nExponential Growth Rate=0\n\n~ 1/ Week\n\n~ The exponential growth rate in the input.\n\nExponential Growth Time=0\n\n~ Week\n\n~ The time at which the exponential growth in the input begins.\n\nSine Start Time=0\n\n~ Week\n\n~ The time at which the sine wave fluctuation in the input begins.\n\nSine Amplitude=0\n\n~ Dimensionless\n\n~ The amplitude of the sine wave in the input.\n\nSine Period=10\n\n~ Week\n\n~ The period of the sine wave in the input.\n\nNoise Seed=1000\n\n~ Dimensionless\n\n~ Varying the random number seed changes the sequence of realizations\nfor the random variable.\n\nNoise Standard Deviation=0\n\n~ Dimensionless\n\n~ The standard deviation in the random noise. The random fluctuation\nis drawn from a normal distribution with min and max values of +/-\n\n4. The user can also specify the random number seed to replicate\nsimulations. To generate a different random number sequence,\nchange the random number seed.\n\nNoise Start Time=0\n\n~ Week\n\n~ The time at which the random noise in the input begins.\n\n********************************************************\n\n.Control\n********************************************************~\n\nSimulation Control Parameters\n\nFINAL TIME = <user specified>\n\n~ Week\n\n~ The final time for the simulation.\n\nINITIAL TIME = <user specified>\n\n~ Week\n\n~ The initial time for the simulation.\n\nSAVEPER =\nTIME STEP\n\n~ Week\n\n~ The frequency with which output is stored.\n\nTIME STEP = <user specified>\n\n~ Week\n\n~ The time step for the simulation.\n\nA Note on Random Noise\n\nThe random input is useful to simulate unpredictable shocks. The RANDOM_NORMAL\nfunction in Vensim samples from a normal distribution with parameters the user specifies. The\nfunction has the following syntax:\n\nRANDOM_NORMAL (min, max, mean, std dev, seed)\n\nVensim uses a default random number \"seed.\" You can specify a different seed by defining a\nconstant called \"Noise Seed\" in your model and setting it equal to some value (e.g. Noise Seed =\n1000). Vensim generates a single random sequence for any given seed. Let's say the sequence\nis: 0.500, 0.213, 0.678, 0.932, 0.340, 0.015. If there is a single random number function in the\nmodel it will simply yield the random sequence. If there are two or more random functions, the\nfunctions will take turns accessing the sequence. For example, if you have two functions, the\nfirst will yield 0.5, 0.678, 0.34; and the second will yield 0.213, 0.932, 0.015. If you run two\nsimulations with the same seed, you will get exactly the same sequence of random numbers.\nThis is important so that you can compare two runs with different policies and be sure the\ndifferences in behavior are due only to the policies and not to different realizations of the random\nnumber generator. When you do want to examine runs with different realizations of the random\nprocess, you need to change the value of the random number seed.\n\nNote also that the use of a function such as RANDOM_NORMAL means a new random number\nis selected every time step. Cutting the time step in half would then double the number of\nrandom shocks to which the model is subjected, and increase the highest frequency represented\nin the random signal. This is generally not good modeling practice. In realistic models, one must\nnot only select the standard deviation of any random processes, but also specify its frequency\nspectrum (or, equivalently, the autocorrelation function). Failure to do so can lead to spurious\nresults and make your model overly sensitive to the time step. These issues are discussed in\nAppendix B.\n\nA Note on the Pulse Function\n\nThe pulse function is used to simulate the effect of instantaneously adding a fixed quantity Q to a\nvariable. To ensure the entire quantity is added all at once (within a single time step, or DT [delta\ntime]), the duration of the pulse is set to the smallest interval of time in the model, that is, to the\ntime step DT. The height of the pulse is then the quantity to be added divided by the time step in\nthe model, Q/DT. The inflow increases by the height of the pulse and remains at the higher level\nfor one time step, so that the total quantity added to the accumulation is (Q/DT)*DT = Q units.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n\n15.872 System Dynamics II\nFall 2013\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Assignment 3: The Dynamics of Service Quality - 15.872 Fall 2013",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-872-system-dynamics-ii-fall-2013/0edab69ba8903fbab622dba4535b38b2_MIT15_872F13_ass3.pdf",
      "content": "Prepared by Kawika Pierson, Avijit Sen and John Sterman, with the assistance of Joe Hsueh, Rashmi Melgiri and\nEllen Tompsett. Last revision November 2013.\nSystem Dynamics Group\nSloan School of Management\nMassachusetts Institute of Technology\nSystem Dynamics II, 15.872\nProf. John Sterman and Hazhir Rahmandad\nAssignment 3\nThe Dynamics of Service Quality\nAssigned: Monday 18 November 2013; Due: Wednesday 27 November 2013\nPlease do this assignment in a team totaling three people and submit digitally.\n\nThis assignment focuses on the dynamics of service quality. Services are an increasingly\nimportant sector of the economy and source of competitive advantage for firms of all types. You\nwill learn about some of the dynamics of services by building a model of service delivery and\nquality using the financial services industry as an example. The assignment deepens your skills\nin modeling and analysis, including formulation of intangible variables and nonlinear\nrelationships, analyzing model behavior, and model testing.\n\nBackground\nThe service sector now represents approximately 75 percent of US GDP and employment, and\ncontinues to grow as more heavy industry and manufacturing move offshore. As important as the\nservice sector is, companies often fail to deliver high quality customer service. While the quality\nof most manufactured products has increased over the past few decades, the American Customer\nSatisfaction Index for important service industries shows great dissatisfaction: for 2012 airlines\nscored 67 (out of 100); banks 77; wireless phone service, 70; subscription television service, 66;\ninternet social media, 69; health insurers, 72; hospitals, 76. Consumers give most service\nindustry firms a grade of C or D, and satisfaction in many cases is not improving (for information\non the ACSI, see http://www.theacsi.org/).\nNowhere is this dynamic more important than in highly competitive industries in which products\nand services have been largely commoditized. In those industries, service quality becomes a\nmajor differentiator and driver of sales. The personal computer industry provides a good\nexample. In his book, Direct from Dell, Michael Dell notes \"We've found that pricing is only\none-third of our customers' decision-making process; the other two-thirds represent service and\nsupport\" (p. 143). High quality service delivery increases customer loyalty, leads to more repeat\nbusiness and favorable word of mouth referrals to others who may then become customers. Poor\nservice can destroy a firm's brand and erode sales.\nServices differ from manufacturing because they are produced in the context of a personal\ninteraction between the customer and the server. Services are intangible, and the quality of a\nservice interaction is necessarily a subjective judgment made by the individual customer.\nFeelings and emotions matter in the service encounter. Because customers have different\nbackgrounds, knowledge, needs, and expectations, services are harder to standardize than\n\nmanufacturing. Services cannot be inventoried, so balancing capacity and demand is more\ndifficult than in manufacturing. Perceptions of procedural fairness and respect are important.\nCustomers do not evaluate service quality solely in terms of the outcome of the interaction (e.g.,\ndid the doctor correctly diagnose my illness, did I get better?) but also consider the process and\nexperience (e.g., did the doctor listen to my concerns, offer empathy and understanding, take the\ntime to hear me out, and treat me with respect--or did the doctor rush through the appointment\nas quickly as possible to stay on schedule and get to the next patient?).\nMedical treatment is a classic example of a high-contact service--it involves intimate interaction\nbetween medical professionals and you, the patient. High contact services are those in which the\nprocess of service delivery and interpersonal interactions between customer and server are\nimportant to the customer's experience and judgment about service quality, and in which such\ninteraction is necessary to the delivery of the service. In this assignment, we'll focus on service\nquality in the retail financial services industry. Financial services (including retail banking,\ninvestment management, and insurance) are also high contact. For many people, financial\ntransactions are a source of anxiety. The array of account types, investment options, loans, etc. is\nbewildering. Fee structures and risks are complex, and the fine print difficult to understand.\nWhether you are investing or seeking to borrow, you want to be sure you are doing the right\nthing. How the firm's employees treat you can matter as much or more than the interest rate you\ncan get on a loan or the fees you will pay on your checking account. At the same time, many\nfinancial institutions have scaled back on direct, face-to-face service to cut costs. Managers are\ntold \"do more with less.\" More and more service functions have been moved into back offices,\nonline banking and contact centers where customer requests are increasingly handled by live\nchat, email, online FAQs, crowd-sourced help forums and telephone.\n\nCase Study: UniversalGloboBank\nConsider a large retail bank we will call \"UniversalGloboBank\" or UGB. Some years ago, to\nlower costs, UGB created a number of \"lending centers\" (LCs) to handle their retail and small\nbusiness loan operations, including credit cards, lines of credit, personal loans, etc. A typical LC\nserves several hundred thousand to about a million customers in a particular region, and operates\nmuch like a call center. Requests for service arrive at the LCs from existing or potential\ncustomers, or via referral from bankers in branches. For example, an existing customer may call\nor go online to request an increase in the credit limit on her credit card. A new potential\ncustomer may apply at a branch, by phone, or online for a personal loan or car loan; the\napplication is sent to the LC for consideration. Hence work arrives at the LCs by phone\n(customer inquiries or calls from bankers in one of UGB's branches), by mail, email and online\nchat (customer requests and communications with branches), by web (loan applications\ncompleted by customers or bankers in branches), and in the form of automated computer-\ngenerated reports identifying problematic accounts that require action, such as overdrafts,\ndelinquencies, etc. LC employees must evaluate the request, including checking credit scores,\naccount histories and references. Most requests require LC employees to produce an email, letter\nor phone conversation with the customer. Often they must call the customer or others to get\nmissing information or correct erroneous information. LC staff are also trained to use customer\ncalls to learn more about the customer's needs and financial situation, a process called\n\"profiling\", and to offer them additional products or services, a process called cross-selling. For\nexample, through profiling, the LC employee may learn that the customer runs a small business,\n\nowns a home, and has small children, then offer a line of credit for the business, a home equity\nloan, and a college-savings plan. Cross selling brings in significant revenue.\n\nAs in most call center operations, the managers of the LCs are evaluated on the basis of their\ncosts and the average time taken to respond to requests. In turn, LC managers closely monitor\ncosts, employee productivity (cases handled per employee per day) and the time taken to close\nout cases. The LCs have a strong norm that all cases are closed out in one day. Cost, closing\ntime, and productivity data are reported to the LC managers daily. LC managers also receive\nfeedback on customer satisfaction, but less often: they receive a monthly report on customer\nsatisfaction based on telephone surveys of a random sample of UGB customers. The surveys are\ndone by a large public opinion research firm, which has been on retainer to the bank for years.\nLC managers report that the customer satisfaction data are out of date by the time they get them,\nand neither reliable nor useful.\n\nLC employees report high pressure to close cases and boost productivity. They report that they\noften have to work uncompensated overtime to meet their targets. For example, two employees\nreport\n\"I don't claim it all in overtime. I tend not to claim for work I do before the eight o'clock start, nor for the\nlunch hour [an average of 5 hours/week].\"\n\"[My coworkers and I] don't always claim that overtime either. I suppose that [we're] worried that\nsomeone would say 'you are not working very cleverly' or something. I never go out to lunch; I'm giving\nthe bank five hours a week of [unpaid] overtime.\n\nHigh schedule pressure (pressure to close cases within the one day target closing time) means\nemployees often cut customer interactions short, or fail to follow all recommended procedures in\nchecking credit references. LC staffers know that they are not able to provide good service or\ncross sell when schedule pressure is high:\n\"The feedback you get back [from the customer] is 'I'm dehumanized, I just became a number. I can no\nlonger talk to you as a person, you just treat me as a number.' [we] have lost the customers along the way.\"\n\"We just don't have the relationship basis to sell effectively. The customers have said that they become a\nnumber, and in a way they have. ...It is difficult to sell that way.\"\n\nQuantitative data back up these impressions. Figure 1 (below) shows average LC revenue from\ncross-selling as a function of the time per task (time spent on each customer request). The data\nhave been normalized so that revenue from cross selling equals its reference (average) value\nwhen the time spent on each customer equals its reference (average) value. The graph also\nshows the best-fitting quadratic curve. Although there is a lot of variability in cross-selling\nrevenue, there is a highly statistically significant relationship: the longer each LC employee\nspends with each customer, the greater the revenue from cross selling.\n\nThe budget for each LC determines how many workers they can hire. The budget for each LC is\ndetermined by the revenue generated within the region served by that LC. Employee turnover in\nthe LCs is high, averaging 30-60%/year, and employee morale is often low. Absenteeism\n(workers who arrive late for their shift, or fail to arrive at all) is common.\n\nFigure 1. Dependence of Cross-Selling on Time Spent with Customers\n\nGetting Started\nA. Begin your modeling effort by replicating the model shown in Figure 14-6 of Business\nDynamics (p. 564). To speed your work, we have created a template for the model which you\nwill find on the course website. The model, called \"ServiceDelivery.mdl\", corresponds to the\ndiagram shown in Figure 14-6, but does not include the equations. Use your judgment and the\ndiscussion on pp. 563-569 to formulate the equations for the model, paying special attention to\ndimensional consistency. For instance, the workforce is measured in people, the workday in\nhours/day, and time per task in person-hours/task. Knowing this you should be able to arrive at\nan equation for Potential Completion Rate that gives you units of tasks/day. The model includes\ntwo nonlinear functions (the effect of schedule pressure on workday and on time per task).\nBefore formulating these, be sure to read pp. 551-563, with special attention to Table 14-1. Then\nread section 14.3, which details how the two nonlinear functions in the model are formulated.\nUse the values for the table functions given on pages 571 and 572 in your model for the effects of\nschedule pressure on workday and the effect of schedule pressure on time per task.\n\nTo speed your work, we have also modified the model slightly compared to Figure 14-6:\nThe model shown in Figure 14-6 measures time in weeks. For our purposes, we will measure\ntime in days. Consistent with the use of the day as the unit for time, instead of \"workweek\"\nand \"standard workweek\", the model variables have been changed to \"workday\" and\n\"standard workday\". In the model poste , we have pre-set the time step to 0.125\ndays, and the length of the simulation to 365 days (see the \"Settings\" menu).\nd\n\nBased on the description of UGB's policies above, we know that the Target Delivery Delay is\none day. Observation of LC workers shows that the Minimum Delivery Delay is one-quarter\nday. The standard workday is 8 hours/day. Standard Time per Task is one person-hour/task.\nThe nominal headcount of a typical LC is 100 people.\nTo aid model testing, the model includes two test generators: one for the Task Arrival Rate\nand one for absenteeism. The test generators are found on the Task Arrival and Absenteeism\nview of the model. Using the test generators you can select a variety of inputs for the task\narrival rate or absenteeism, including steps, pulses, ramps, cycles, and random variation. The\ntest generator for task arrivals is set up so that the initial Task Arrival Rate equals the initial\nvalue of the Standard Completion Rate: tasks arrive at the rate equal to the LC's ability to\nprocess those orders at the rate given by their initial head count and the standard workday and\nstandard time per task.\nAbsenteeism is modeled as:\nNet Labor = Labor Force * (1 - Absenteeism)\nAbsenteeism = MAX(0, Input_0)\nWhere the Labor Force is the nominal LC headcount, Absenteeism is the fractional reduction\nin the actual number of employees working at any time, and Input_0 follows pulses, steps,\nramps, cycles or noise according to your choices. The MAX function ensures that\nabsenteeism lowers net labor (people sometimes fail to appear for their scheduled shifts), but\ndoes not increase net labor (people do not show up to work when they are not scheduled to do\nso, and net labor cannot exceed the labor force).\nThe test generators for task arrivals and absenteeism allow you to include random variations\nas a test input. The structure to model noise is called \"pink noise\" because realistic noise\nprocesses are autocorrelated. To use the noise input, you must set both the standard deviation\nfor the noise and the Noise Correlation Time. The correlation time captures how much\npersistence there is in the noise from day to day. Please read Business Dynamics Appendix B\nto learn more about pink noise and autocorrelation. The data for UGB show that the noise\ncorrelation time for task arrivals is 7 days, and the correlation time for absenteeism is 14\ndays. These values have been set in the model; you can vary them to explore the sensitivity\nof the models response to different degrees of persistence in the noise inputs.\nFormulate the initial task backlog to ensure that the model always starts in equilibrium\nregardless of the initial Task Arrival Rate. To do so you must provide an algebraic\nexpression for every stock, not a number. Use the equilibrium condition for backlog (task\ncompletion = task arrival) to derive an algebraic expression for the equilibrium backlog,\nassuming that the task completion rate equals its desired rate.\nAs always, you must fully document your model by writing brief but informative comments\nin the comment field for every variable and constant in the model.\n1. Explain the shape of the table functions for the effect of schedule pressure on workweek\nand time per task using language a manager would understand. Pay special attention to\nreference lines and the behavior of the functions for extreme values of schedule pressure.\n\n2. As described above, LC managers receive frequent feedback on labor productivity and\nmonitor it closely. Add a new variable to your model to compute labor productivity. Labor\nproductivity is the number of tasks completed per day per person.\n\nB. A valuable method to explore the behavior of models is to start in equilibrium and then shock\nthe system with a known perturbation such as a sudden increase in workload. Use the test input\ngenerator to run this test by having the task arrival rate step up by 20% on day 5. Run the model\nand answer the following questions.\n\n1. How does the simulated LC respond to the sudden increase in tasks arriving? Explain this\nresponse in terms a manager familiar with the industry would understand. Specifically, how do\nLC employees respond to the increase in workload? What happens to productivity?\n\n2. What happens as the size of the step in task arrivals increases? Does the model reach\nequilibrium for all step sizes? Why or why not? Explain the relationship between task arrivals\nrelative to the organization's nominal capacity (the standard completion rate) in terms managers\nwould understand.\n\nUse Vensim's Synthesim mode to quickly find the values of equilibrium delivery delay,\nworkday, time per task, productivity, and other variables.\nLaunch Synthesim by clicking on the\nbutton in the top toolbar. Use the slider for\nStep Height in the test generator to set different size increases in the arrival rate. You can\nset exact values for any input slider by clicking on the arrow at the end of the slider, then\nentering your desired value in the dialog box that appears. Once you set the step height,\nuse the table tool to find the final (equilibrium) value of delivery delay.\nWhat do you conclude about the performance of the system in this initial model as the\nworkload increases? You may want to explore the response of the system to other test inputs,\nincluding random variations in task arrivals. Read about noise inputs in Business Dynamics\nAppendix B.\n\nC. Expanding the model boundary: The initial model does not capture a variety of important\nfeedbacks affecting the performance of the LC. In this section, you will relax some of these\nassumptions, one at a time.\nC1. Revenue: As described above, UGB derives significant revenue from cross selling. Use\nthe information below and the case description above to formulate equations for LC revenue.\nTotal LC revenue consists of a base revenue level plus revenue from cross selling. Base\nrevenue comes from account maintenance fees, other fees and interest income generated by\nthe customer base in the LC's service region, and averages $12,000 per day (about $4.4\nmillion/year).\nRevenue from cross selling is determined by the number of customer inquiries (tasks)\ncompleted each day and the average cross sell per customer inquiry (cross sell per task).\nAverage cross selling revenue per task depends on the time spent per task, as shown in Figure\n\n1. Formulate cross sell revenue per task using a table function. Formulate the table using the\nprinciples for table functions described in the text. In particular, normalize the function as\nfollows: set cross sell revenue per task so that it equals a reference value when time per task\nequals a reference value. Define these reference values as constants.\nThe data indicate that when time per task equals 1 person-hour/task, then average cross sell\nrevenue per task is $15/task. Use these as the reference values. Then use the data and best-\nfitting curve in Figure 1 to specify the values of the table function relating time per task\nrelative to the reference value to cross sell revenue per task relative to its reference value.\nYour function should correspond to the values shown by the best-fit curve in the figure, but\nbe sure to pay special attention to the shape of the curve outside the range of historical data.\nConsider extreme conditions: what must cross sell revenue per task be if time per task is\nzero? What must happen to cross sell revenue as time per task becomes many times greater\nthan the reference?\n\n1. Run the model with various size step increases in task arrivals. What happens to cross\nsell revenue? Why? Explain in terms of the feedback structure of the system, but in terms a\nmanager can understand.\n\n2. Now run the model with a constant task arrival rate, but random variations in\nabsenteeism. A standard deviation of 0.02 (2%) in the noise input to absenteeism is reasonable.\nWhat is the impact on revenue? Why? Explain.\n\nC2. Organizational Norms for Time per Task: So far the standard time per task has been\ntreated as a constant. Constant standards are appropriate in some settings, such as\nmanufacturing, where target processing times are tightly determined because so much of the\nwork is automated and routinized. In high-contact service settings, however, it is very difficult to\ndetermine an appropriate standard for the time each employee should spend with each customer.\nCustomer needs and knowledge are heterogeneous, server skill varies, and the customer's\nperception of quality depends strongly on how much time and attention they receive from the\nserver. In such settings, norms for the appropriate amount of time to spend on each case tend to\nadjust over time to the actual amount of time servers spend with each customer.\nNorms that adjust to past performance are known as \"floating goals\". Floating goals are\ncommon: when sales people exceed their sales quotas, management tends to raise them;\nstudents sometimes adjust their aspirations for grades to the actual grades they receive; your\nbelief about how much income you need to live comfortably tends to adjust to your actual\nincome; your belief about your optimal weight tends to adjust to your actual weight. Read\nmore about floating goals and how to model them in section 13.2.10, pp. 532-535.\nIn service settings, where it is difficult or impossible to determine the \"correct\" standard for\nthe time each server should spend with customers, norms for customer service tend to adjust\nstrongly to actual performance.\nModify your model to capture variations in the standard time per task. In particular, if LC\nemployees find they are consistently spending more (less) time with each customer, the\n\nstandard time per task--what they and management consider to be appropriate--will\ngradually rise (drop) until it equals the actual time they are spending.\nSpecifically, model the standard time per task as a stock that adjusts towards the current time\nper task over some adjustment time. Fieldwork suggests the average adjustment time for\nstandard time per task is 90 days. Set the initial standard time per task at 1 person-hour/task.\nDocument your formulation, check it for dimensional consistency and test it to make sure that\nit behaves plausibly.\n\n1. How does your additional structure change the response of the system to a 20% step\nincrease in task arrivals? Does the model reach equilibrium? Note: you may need to run your\nmodel longer than 365 days to determine if the system reaches a new equilibrium.\n2. Explain the behavior of the system in terms a manager can understand. Discuss how the\nequilibrium of the system changes relative to the original model with a fixed standard time per\ntask. How does the organization adjust to the increase in workload? What are the impacts of\nthese differences on productivity? On cross-selling? What do these changes represent in terms\nof customer service quality, and what other feedbacks would you expect these changes to have in\nthe real system? (A simple causal diagram will be helpful here.)\n3. Now run the model with a constant task arrival rate, but random variations in absenteeism\n(use a standard deviation of 0.02). What happens to the norm for time per task, to productivity,\nand to cross selling? Why? Explain in terms of the feedback structure of the system.\n\nC3. Budget Constraint on Hiring: The number of workers in the call center is still exogenous\nin your model. Relax this assumption by modeling the dynamics of the labor force.\n1. As described above, Lending Center managers must strive to staff their centers so that\nheadcount is sufficient to complete work at the desired rate, but they also face a budget\nconstraint. Model the labor force of the LC as a stock with explicit hiring and quits. The LC\nmanagers replace employees who quit and adjust the labor force to a desired level. The desired\nlevel is determined by the number of people needed to meet the desired task completion rate or\nthe number of people the LC can hire given their budget, whichever is less. The number of\npeople needed to meet the desired task completion rate is based on the desired completion rate,\nstandard workday and standard time per task. In modeling how many people the LC can afford\nto have on staff, assume that the average daily cost per worker is $160/day (about $58,000/year, a\nfigure that includes salary, benefits, and overhead costs). The budget for the LC is a certain\nfraction of the revenue generated in their service area. Two-thirds (67%) of that revenue is\nallocated to the budget of the LC, with the rest going to UGB to cover indirect costs and\ncontribute to profit. Note that when time per task equals its reference value, the budget should be\nsufficient to support the number of people needed.\nYou can use the labor sector you developed in the Widgets model to model the labor force.\nRemember that time is measured in weeks in your Widgets model, but days in the service\ndelivery model: make sure you set the parameters appropriately.\nCompare the head count determined by the budget with the head count required to process\nthe work at the desired rate. Be sure that the LC's initial budget is sufficient to provide\n\nenough workers to complete the work at the desired rate and with the standard workday and\ninitial standard time per task.\n\n2. Test the model with a variety of step increases in incoming tasks. Explain, in terms\nmanagers would understand, which feedback loops are most important in generating the behavior\nyou observe. Consider the behavior of productivity, revenue, head count, and other variables.\nHow do you think LC managers would interpret the results? Explain.\n3. Now test the model in response to other inputs. Try a ramp in task arrivals. A ramp with\na positive slope corresponds to growth in the overall number of customers in the LCs service\narea. Try a slope of 0.0002/day (a little more than 7%/year growth in volume). How does the LC\nmeet the increase in work volume? What other impacts does growth have?\n4. Keeping the task arrival rate constant, run the model with random absenteeism (use a\nstandard deviation of 0.02). Explain the resulting behavior, considering productivity, revenue,\nhead count, and other variables. Show graphs of model behavior to illustrate (you should plot\nthose variables needed to show why your explanation is correct; use your judgment).\n\nD. Expanding the model boundary: Identify ONE feedback process you believe to be\nimportant in the service context you have modeled but that is missing from the current model.\nProvide a causal loop diagram (properly constructed, per usual) of your loop. Then add your\nloop to the model, providing the equations for the new structure you are adding in the text of\nyour write up, including units of measure for each new variable. Select parameters and values\nfor any nonlinear relationships (table functions) representing your best judgment; it is not\nnecessary to gather any data for the purpose of this assignment. Simulate the model with your\nnew feedback and compare the behavior of the system to the identical case without your new\nstructure. Explain briefly how the inclusion of your new feedback affects the dynamics of the\nsystem.\n\nE. Policy Implications: Based on your model results, what policies do you recommend to\nUGB's senior management to avoid the pitfalls your model indicates may exist?\nYour policy recommendations must be specific and implementable. It is not acceptable to\nsay \"maintain quality standards\" or \"keep standard time per task constant.\" The first\nsuggestion is not operational: how would you maintain quality standards? The second is\ninfeasible because products and customer knowledge constantly change. For example, if\nproduct complexity grows, the standard time per task may increase; if technology allows\nmore of the work to be automated, standard times might fall without compromising quality or\ncross-selling.\nEffective policies will consist of one or more of the following:\n1. Changes in model parameters that strengthen or weaken existing feedbacks.\n2. The elimination of an existing feedback loop.\n3. The addition of a new feedback loop or loops.\n4. Changes in the goals of the different loops.\nIn all cases your policies must be implementable. Explain why you think your recommended\npolicies would work, and how they could be implemented in the real world.\n\nF. What to hand in and how to submit your work\nWrite up your responses to the questions above in a word (.docx) document. In addition, you\nneed to submit two fully documented Vensim model (.mdl) files: one is the complete model\nyou have completed by the end of part C (which includes revenue, task performance norms,\nand hiring), and another is your final model that includes extra structure from part D. Upload\nyour team's assig\nby 5 PM on November 27th. Submit your assignment as a sing\nip file including your response document and models.\nMake sure you include your team-members' names in the document. Name files with your\nteam's name, and for multiple files of the same type, the assignment section, e.g.\n\"Team21.docx\", \"Team21-C.mdl\", and \"Team21-D.mdl\" all submitted as part of\n\"Team21.zip\".\n\nnment\nle\n.z\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n\n15.872 System Dynamics II\nFall 2013\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Assignment 4: Understanding Cost & Schedule Overrun - 15.872 Fall 2013",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-872-system-dynamics-ii-fall-2013/34fad5cc8b0ea2e7c2d461b3749d96dc_MIT15_872F13_ass4.pdf",
      "content": "System Dynamics Group\nSloan School of Management\nMassachusetts Institute of Technology\n\nSystem Dynamics II, 15.872\nProfessors John Sterman and Hazhir Rahmandad\n\nAssignment 4\n\nUnderstanding Cost & Schedule Overrun In Product Development Projects*\n\nAssigned: Sunday 1 December 2013; Due: Wednesday 11 December 2013\nPlease do this assignment in a team totaling three people and submit digitally.\n\nProjects to design and build new products or services, whether large, one-of-a kind projects such\nas HealthCare.gov or Boston's Big Dig, or smaller, more routine projects such as designing a\nnew car or smartphone operating system, are notorious for cost and schedule overruns, poor\nquality, bugs and defects, and failure to meet customer requirements--they are Late, Expensive\nand Wrong (LEW). Such projects are increasingly important in our economy. For example,\nalmost every new product or service today involves development of new software. Success in\nthe market often depends on being on schedule and budget. This assignment builds your\nunderstanding of the feedback structures driving success or failure in projects.\n\nCase Background: You have been hired as a consultant to the IT department of a major\nfinancial services institution. The department is responsible for the development, extension, and\nmaintenance of software to support the growing range of products and services offered by the\ninstitution. The department has not been doing a very good job. All of its software development\nprojects over the last few years have significantly exceeded both their original budgets and\nschedule. The Vice President in charge of the IT Department put it this way: \"Our projects start\nout okay, but about halfway through we begin to discover additional work, often to correct errors\nmade on earlier work. When we realize we are behind we add additional resources as they are\nfreed from other assignments. But we barely keep pace with the workload, and end up throwing\nlots of staff at the job. And we still don't finish on time. I am beginning to wonder if there isn't\nsomething to Brooks Law.1\"\n\nYou interview some of the key project managers from the department, and come up with a long\nlist of factors that might be contributing to the problems in the IT department:\n\nUncertain specs from the operating departments (the customers for the projects)\n\n*Originally prepared by James Lyneis, April 2000. Last modified November 2013.\n1 \"Adding manpower to a late software project makes it later.\" Brooks, Frederick P. Jr. The Mythical Man-Month.\nReading, MA, Addison Wesley, 1995.\n\nLate customer specification changes\nDelays in getting additional staff\nNew staff unfamiliar with the project\nDesign errors discovered late in the project\nOverwork and fatigue among the programming staff\nTime spent getting people brought on a project up to speed\nProject staff not doing a thorough job checking their work\nPressures to promise delivery earlier than is reasonable\nBuilding design errors into coding, and not discovering the problem until testing\n\nThere is a lot of pressure on you and your team to provide implementable solutions for these\nproblems. Some suggest you should immediately begin developing a comprehensive model of\nthe IT department. However, you know that the most effective way to develop a model, and to\nengage and educate the client along the way, is to build it in small steps, making sure you and the\nclients understand each step fully before adding additional complexity. With this in mind, you\nwork through the four steps below, and at the end have some significant preliminary insights for\nyour client.\n\nBrevity is a virtue in your write up. Unless specifically requested, it is not necessary to\nhand in complete sets of output (graphs, tables) for each test and simulation you do. A\nsummary table will suffice. For example, you might construct a table showing the date of\nproject finish and cumulative effort expended. However, as always, you must explain the\nchanges you make in the equations so that an independent third party can replicate your\nsimulations.\n\nA.\nStep 1: The Rework Cycle\n\nTo begin your analysis, you hypothesize that the \"rework cycle\" is likely to be at the heart of the\nIT department's project problems. You therefore construct a rework cycle model of a typical\ndepartment project, as illustrated in the following figure. Your interviews indicated the\nfollowing:\n\n- A typical project involves 100 tasks\n- Under optimal conditions, each programmer accomplishes 1 task per month\n- Normally, programming error rates are 25%\n- It takes about 4 months to discover design errors\n- A typical project starts with 4 staff\n\n- Define the completion of work (Project Finished) when work done is 99% of original\nwork to do. Use Vensim's IF THEN ELSE function. Stop further work accomplishment\nand rework at this point.\n\nThe Rework Cycle:\n\nA1. Create your model from scratch based on the diagram above and complete the equations\nfor the rework cycle. You do not need to add any variables to the diagram. Select a\nsufficiently long time horizon for simulation and adequately small TIME STEP (See\nAppendix A in Business Dynamics). Hand in your fully documented and dimensionally\nconsistent model (.mdl file).\n\nA2. Which factors do you think are more important in determining project completion --\nproductivity, quality, or rework discovery time? Why? (Please answer this before\nsimulating your model. Your grade is not affected by your answer to this question).\n\nA3. As an extreme test case, if programmers did not make any errors, when would the project\nfinish? What happens to work to do, work done, and undiscovered rework?\n\nYou will want to create custom graphs to show all the important variables.\n\nA4. Now, set the value for normal quality to the value indicated in your interview notes.\nWhen does the project finish? What happens to undiscovered rework in this situation?\n\nWork to Do\nUndiscovered\nRework\nWork Done\nRework\nGeneration\nRework Discovery\nWork\nAccomplishment\nQuality\nPotential Work\nRate\nMaximum Work\nRate\nMinimum Time to\nPerform a Task\nStaff Level\nProductivity\nProject Finished\nTime to Discover\nRework\nCumulative\nWork Done\nRate of Doing\nWork\n<Rework\nGeneration>\n<Work\nAccomplishment>\n+\n+\n+\n+\n+\n+\n+\n+\n+\n-\n-\n-\n-\n-\nInitial Work\nCompletion\nThreshold\n\nProvide a one paragraph, simple, and non-technical explanation of why the project takes\nmore than 25 months (the length of a naive estimate with 100 tasks completed by four\npeople each having one task per month productivity).\n\nA5. Analyse the impact of productivity, work quality, and rework discovery time on project\ncompletion date and total work done. Perform this analysis by conducting sensitivity\ntests varying each parameter by plus-and-minus 33% (requiring 6 simulations in addition\nto the reference or Base Case in which the parameters are at their normal values). Explain\nyour observations in simple managerial terms.\n\nUse a quality of 0.75 as the mid-point for your analyses\n\nCreate a table reporting the completion rate and total work done for each condition to\nsummarize the results of these tests.\n\nYour explanation should be written in none-technical language and understandable to\nany client team member who has never seen your model.\n\nB.\nStep 2: Extending the Model: Adding the Quality on Quality Feedback and Variable\nRework Discovery Time\n\nAfter exploring the behavior of the rework cycle model with your client, you return to the office\nto expand the model to better reflect some important mechanisms revealed from the feedback\nyou received from your client.\n\nFirst, your discussions around the rework cycle indicate that Time to Discover Rework is not\nlikely to be constant, but falls as progress on the project progresses. Your discussions indicate\nthat rework discovery time is long initially, about 10 months, but once perceived project progress\npasses 50% complete and testing begins, rework discovery time falls below the initial value of 10\nmonths. In the latter stages of the project, when almost everything is testing and error correction,\nthe discovery time is approximately 1 month. The table below describes the best estimates of the\npeople you interviewed.\n\nFraction Complete\nEffect on Rework Discovery\n\n.1\n\n.2\n\n.3\n\n.4\n\n.5\n\n.6\n\n.95\n.7\n\n.8\n.8\n\n.45\n.9\n\n.2\n1.0\n\n.1\n\nYour client also highlights the phenomenon of errors building on errors. Specifically, the\nquality of prior work affects the quality of new work being done. New code to a code\nbase containing errors will be more likely to have errors. For example, if the code base\nincludes an erroneous label and pointer to data that needs to be retrieved from the firm's\ndata warehouse, then code developed later using the same pointer will also be in error.\nSimilarly, if programmers re-use a code object or \"subroutine\" (yes, the firm still uses\nsome legacy code programmed in Fortran) containing errors then every instance of that\nre-used code will propagate the error. The fact that errors lead to more errors is a general\nchallenge in software development, and is a particular problem in the IT department.\n\nYour discussions indicate that while average quality may be 75% or less, management\nbelieves that \"normal quality\" would be about 85% were it not for these \"quality on\nquality\" and other effects (to be discussed in Step 3). You probe project personnel on the\nlikely effect of prior quality on current quality. They estimate that normal quality would\nbe achieved if the prior work were perfect. And they estimate that if all prior work were\ndone incorrectly, then the quality of new work would be as low as 10% of normal. The\ntable below describes their best estimate.\n\nAverage Work Quality\nEffect on Current Quality\n\n.1\n.1\n\n.25\n.2\n\n.35\n.3\n\n.45\n.4\n\n.55\n.5\n\n.65\n.6\n\n.725\n.7\n\n.8\n.8\n\n.875\n.9\n\n.95\n1.0\n\n1.0\n\nAssume that staff and scheduled completion date remain constant (although actual\ncompletion date may exceed the scheduled deadline).\n\nInclude as a performance measure \"cumulative person-years on the project.\"\n\nThe fraction of the project perceived to be complete is a key performance measure\nused by management. You realize that perceived progress includes both work\nactually done and the stock of undiscovered rework. Because undiscovered rework\nis, by definition, not yet known to management, the project managers and team\nbelieve that the total amount done correctly is the sum of what has actually been done\ncorrectly and the stock of undiscovered rework.\n\nConsider reformulating quality as\n\nQuality = Normal Quality * Effect of Prior Work Quality\n\nThe Effect of Prior Work Quality should be driven by the average quality of work to\ndate, used as an input into a Lookup function. Errors embedded in past work product\ncan cause errors to be made on current work, and may lower productivity as\nambiguities are sorted out. Some definitions\n\nAverage Quality of Work done to date =\n\nWork Done/(Work Done + Undiscovered Rework)\n\nYou may need to use the IF THEN ELSE, or the XIDZ function (\"X if division by\nzero; see the Vensim Manual) function to avoid division by zero at the very beginning\nof the project when no work is completed.\n\nThe revised model diagram is shown below. Save a copy of the model you developed above\nwith a different name. Using that copy, implement the diagram below and add the equations.\nYou do not need to add any additional structure beyond the diagram:\n\nB1. Hand in your fully documented and dimensionally consistent model (the .mdl file).\nWork to Do\nUndiscovered\nRework\nWork Done\nRework\nGeneration\nRework Discovery\nWork\nAccomplishment\nQuality\nPotential Work\nRate\nMaximum Work\nRate\nMinimum Time to\nPerform a Task\nStaff Level\nProductivity\nProject Finished\nTime to Discover\nRework\nCumulative\nWork Done\nRate of Doing\nWork\n<Rework\nGeneration>\n<Work\nAccomplishment>\n+\n+\n+\n+\n+\n+\n+\n+\n+\n-\n-\n-\n-\n-\nInitial Work\nNormal Time to\nDiscover Rework\nEffect of Work\nProgress\nTable for Effect of\nWork Progress\nFraction Perceived to\nbe Complete\nWork Believed to\nbe Done\nAverage Work\nQuality\nEffect of Prior Work\nQuality on Quality\nTable for Effect of Prior\nWork Quality on Quality\nCumulative\nEffort\nExpended\nEffort Expended\n<Project\nFinished>\n<Staff Level>\n+\n-\n+\n+\n<Initial Work>\n<Work Done>\n+\n+\n+\nNormal Quality\n+\n+\n+\n-\n+\n-\n-\nCompletion\nThreshold\n\nB2. How does the inclusion of a variable rework discovery delay affect the behavior of the\nsimulated project? What happens to the completion date and the work backlog compared\nto the case when rework discovery time is constant? Compare the behavior of the model\nwith and without this feedback, and provide a non-technical explanation of the\nmechanisms behind your observations.\nB3. How does the inclusion of the 'quality on quality' feedback affect the behavior of the\nsimulated project? What happens to quality, completion date and the work backlog as\ncompared to the prior? Compare the behavior of the model with and without this\nfeedback, and provide a non-technical explanation of the mechanisms behind your\nobservations.\n\nC.\nStep 3: Extending the Model: Allowing for Increased Staff\n\nYour model is beginning to behave like a real project, and now you can begin to use it to explore\nthe consequences of the IT department's project staffing policies. To do so, you need to add the\nability to increase and decrease staff on the project. You also need to include the consequences\nof adding staff on productivity and quality. Your interviews indicate:\n\n-\nA typical project starts with a team of four experienced staff, but hiring/transfer can\ndynamically change staff levels.\n-\nBecause the IT department's programmers are fully engaged on other projects, or because\nhiring takes time, the time to increase staff via hiring or transfer averages 4 months.\n-\nExcess staff is transferred out of the project or fired, with a delay of two months on\naverage. When staff levels are reduced, the inexperienced members are cut first, and only\nthen will experienced staff leave the project.\n-\nNew staff coming onto a project require 24 months to gain full experience\n-\nInexperienced staff has lower productivity and quality. Your interviews indicate that new\nstaff produce at only 50% the productivity and quality of experienced staff.\n-\nInexperienced staff also reduces the productivity of experienced staff as a result of the\nneed to train and oversee these new staff. Management estimates that each new staff\nneeds about 40% of an experienced team member's time for training and oversight.\n-\nManagement estimates the staff level required to finish the project by dividing the\nestimated labor effort needed to complete (in person-months) by the scheduled time\nremaining. Since scheduled completion date is fixed, once the project passes the\nscheduled completion date, the scheduled time remaining can become negative, which Is\nnot realistic. Therefore in staff planning, management sets the deadline one month from\nthe current date once they pass the initial scheduled completion date. The typical project\nis scheduled to complete in 30 months.\n-\nThe IT department estimates cost to complete, which is the estimated labor effort, in\nperson-months, needed to complete the project, by dividing the level of work remaining\nto be done by average net productivity on the project; average net productivity is\nestimated by dividing work believed to be done to date by cumulative person-hours used\nto date, and starts initially from the normal productivity of experienced staff.\n\nSave your model from step B under a new name and expand it as follows:\n\nCreate two stocks, new staff and experienced staff. Assume that all hires or transfers to\nthe project enter the pool of new staff (since they don't have experience with this project),\nand that they take 24 months to become experienced.\n\nA simple weighted average can be used to calculate the average productivity and quality\nof the current staff. That is, average productivity depends on the productivity of new\nstaff, the productivity of experienced staff, and the fraction of staff of each type. These\naverages should replace the normal quality and productivity values. Be sure you take into\naccount the impact of new staff on the productivity of experienced staff created by on the\njob training and oversight.\n\nA diagram of the new staffing section is given below. A tricky formulation in this model\nregards the behavior early when little work has been accomplished. Because rework\ndiscovery takes time, early in the project work believed to be done, and average\nproductivity, appear to be high. The resulting high productivity estimate might indicate\nthat fewer staff are required, and transfers off the project would occur. However, a smart\nmanager recognizes this possibility and will not reduce staff levels based on apparent\nprogress until sufficient progress has been made to really determine where things stand,.\nYou will note in the diagram below that the \"weight on progress-based estimate\" can\nslow down staff leaving. The weight on progress-based estimate is a function of the\nfraction perceived to be complete, and is estimated based on your interviews to\napproximately follow the table below:\nFraction Complete\nWeight\n\n.1\n\n.2\n\n.3\n\n.1\n.4\n\n.25\n.5\n\n.5\n.6\n\n.75\n.7\n\n.9\n.8\n\n.9\n\n1.0\n\nC1. Complete the model equations and document your new model. You can add the staff\nsector of the model to a new view to simplify navigation. Also you will benefit from a\ncreating a control panel in which you include the key policy levers, tables, and\nparameters, along with key graphs that represent the summary of project's behavior.\nHand in your documented model (.mdl file).\nC2. Describe the behavior of the simulated project when staff are endogenously increased in\norder to meet the originally scheduled date. Does the project finish on time? Does it\nfinish sooner than when staff are not added to the project? Why or why not? (Note:\nwhile several computer runs may be necessary to get your model working properly, only\none computer run is necessary to answer this question). Explain in plain English and\nsupport your argument with relevant graphs.\n\nLook at the behavior of productivity and quality, and the factors that cause them to\nchange over time. How does the addition of staff later in the project affect productivity\nProductivity\nNormal Quality\nNew Staff\nExperienced\nStaff\nStaff Hired\nStaff Getting\nExperience\nStaff Leaving\nNew Staff Leaving\nInitial\nExperienced Staff\nHiring Delay\nExtra Staff\nNeeded\nMaximum Staff\nLevel\nStaff Level\nRequired\nTime Remaining\nMinimum Time to\nFinish Work\nScheduled\nCompletion Date\nWeight on\nProgress-Based Estimate\nTable for Weight on\nProgress Based Estimate\nEstimated Cost to\nComplete\nAverage Past Net\nProductivity\nStaff Level\nTime to Gain\nExperience\nExcess Staff\nTransfer/Firing\nDelay\nExcess New Staff\nExcess\nExperienced Staff\n<Project\nFinished>\n<Work to Do>\n<Work Believed to\nbe Done>\n<Cumulative Effort\nExpended>\n<Time>\n<Fraction Perceived\nto be Complete>\nAverage Staff\nProductivity\nNew Staff Relative\nProductivity\nFraction of Experienced\nStaff Time per New Staff\nTraining\n<Experienced\nStaff>\n<New Staff>\nAverage Staff\nQuality\nNew Staff Relative\nQuality\n\nand quality? What does this do to the total amount of work done and to cumulative\nperson-years spent on the project?\n\nC3. What policies would you recommend at this point? Explain briefly.\n\nD.\nStep 4: Policy Analysis: When Does Adding Labor Make Sense\n\nWhile the results thus far seem to be consistent with the behavior of the IT Department, you do\nnot want to end with the recommendation that staff should never be added to a late software\nproject. There must be some conditions under which adding staff makes sense. Conduct a series\nof analyses to determine when, given the model developed thus far, project performance is\nimproved when staff are added. You should consider conducting a sensitivity analysis on the\nassumptions regarding:\n\nRelative productivity of new staff\nRelative quality of new staff\nEffect of new staff on productivity of experienced staff\nTime required for new staff to become experienced\nTime required to increase staffing\nInitial number of staff\nScope/length of the project\n\nD1. Prepare a summary of the results from your sensitivity experiments using a table which\nidentifies each scenario and its impact on key performance metric(s). Besides sensitivity\nruns in which you increase/decrease one parameter at a time, conduct at least one\nsimulation where multiple assumptions are changed in a realistic range and justify\nadjustment of staff during project life. Approximately 16 computer runs should be\nsufficient for this question. Under what conditions does it make sense to add staff to get a\nproject completed on time? Why? Explain in simple English.\n\nE.\nSynthesis of Policy Analysis and Recommendations\n\nE1. Prepare a recommendation for the management of the IT department regarding its project\nmanagement and staffing policies. What would you suggest regarding initial staffing,\ntheir skills, and the schedule? How about adding staff as the project progresses? Provide\na brief summary in non-technical language that synthesizes your understanding and offers\nmanagement with clear guidance.\n\nE2. While your model is already pretty complicated, it still does not include several features\nthat may potentially be relevant to IT projects in general. Provide management with a list\nof what you think are potentially valuable additions should they decide to continue with\nthis analysis in future. You do not need to do any further modeling, only highlight what\nyou think are important potential additions.\n\nF. What to hand in and how to submit your work\nWrite up your responses to the questions above in a word (.docx) document. In addition, you\nneed to submit three fully documented Vensim model (.mdl) files, one each for parts A, B,\nand C. Upload your team's assig\ny 5 PM on the due date. Submit your assignm\nsingle .zip file including your response document and models.\nMake sure you include your team-members' names in the document. Name files with your\nteam's name, and for multiple files of the same type, the assignment section, e.g.\n\"Team21.docx\", \"Team21-A.mdl\", \"Team21-B.mdl\"and \"Team21-C.mdl\" all submitted as\npart of \"Team21.zip\".\n\nnment b\nent\nas a\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n\n15.872 System Dynamics II\nFall 2013\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Assignment",
      "title": "Mini Assignment: Service Quality at Dell - 15.872 Fall 2013",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-872-system-dynamics-ii-fall-2013/8e6e6547e3343b8064133a842b786081_MIT15_872F13_mini_ass.pdf",
      "content": "Prepared by John Sterman, November 2013.\nSystem Dynamics Group\nSloan School of Management\nMassachusetts Institute of Technology\nSystem Dynamics II, 15.872\nProf. John Sterman and Hazhir Rahmandad\nMini-Assignment\nService Quality at Dell\nAssigned: Monday 18 November 2013; Due: Wednesday 20 November 2013\nThis is an individual assignment.\n\n1. Read the case, Customer Service at Dell, Inc. (2008), to be handed out in class.\n\n2. A key concept is the way average handle time (AHT) for each customer service contact\ninteracts with costs per contact and contact volume to determine the total costs of customer\nservice, and how these both respond to and affect customer satisfaction (CSAT)\n\nDrawing on the case, create a reference mode showing, qualitatively, the behavior of the\nfollowing key ideas affecting Dell's customer service operation over time:\n\nCustomer satisfaction (CSAT)\nContact center costs (the total costs of customer service, $/month)\nAverage Handle Time for service contacts (AHT; minutes/contact)\nAverage cost per contact ($/minute)\nContact volume (contacts/month)\n\nAn appropriate time horizon for your reference modes begins in 2004 and continues a few years\nbeyond 2008, the end of the period described in the case, to capture what you think the longer-\nterm impacts of Dell's actions through 2008 might be.\n\n3. Develop a simple causal loop diagram capturing your hypothesis to explain the dynamics you\nidentify in (2). Your diagram should show the feedback processes that you believe are\nresponsible for the patterns of behavior for CSAT, total service costs, and so on.\n\nStart with the diagram on the next page. The diagram shows the basic \"physics\" determining\ntotal contact center costs, following the equation in the case:\n\nContact center costs = AHT * Cost per minute * Number of calls\n\n($/month) = (minutes/contact) * ($/minute) * (contacts/month)\n\nContact volume (the number of contacts per month) is shown as the product of contacts per\ncustomer per month and the total number of customers. CSAT is shown as being adversely\n\naffected by contacts per customer: the more times a customer must contact Dell, the lower\ntheir satisfaction is likely to be. You may of course consider other determinants of CSAT.\nNote that the diagram contains no feedback loops. Build on the diagram by adding the\ndeterminants of the inputs, thus closing the feedback loops you believe are responsible for the\ndynamics in your reference mode.\n\nService Costs\n($/month)\nAverage Handle\nTime (AHT)\n(min/contact)\nCosts per\nminute\n($/min)\nContact Volume\n(contacts/month)\nCustomers\nCustomer\nSatisfaction\n(CSAT)\nContacts per Customer\n(contacts/month/customer)\n+\n+\n+\n+\n+\n-\n\nFollow the standard rules for causal diagrams (Business Dynamics, Ch. 5), including labeling\nthe polarity of every link and loop, naming your loops, using variable names that are\nmeaningful to people who work in the real system, showing important delays, etc.\nInclude explicit stock and flow structure only if it is useful to explain the dynamics. Your\ndiagram is not intended to be the basis for a simulation model. It is intended to communicate\nyour theory to explain the dynamics of customer service at Dell.\nKeep your diagram simple. It must fit comfortably on a single page.\nSpend no more than about 2 hours or so on this assignment. Upload your write-\n\nbefore class on Wednesday 11/20.\nBring your diagrams to class on Wednesday.\nHave fun.\nup\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n\n15.872 System Dynamics II\nFall 2013\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Product Development Recitation - 15.872 Fall 2013",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-872-system-dynamics-ii-fall-2013/accc5abf95a34f15c9a97dd96473b555_MIT15_872F13_proj_over.pdf",
      "content": "Product Development Recitation\n\nWhy does this happen, and how can\nwe improve this?\n\nProduct Development\n\n- Success in the market often depends on being\non schedule and budget.\n- This assignment builds your understanding of\nthe feedback structures driving success or\nfailure in projects.\n- Cost and schedule overruns, Late, Expensive\nand Wrong (LEW).\n\nDeliverables\n\n- Brevity is a virtue in your write up.\n- Unless specifically requested, it is not necessary to\nhand in complete sets of output (graphs, tables) for\neach test and simulation you do. A summary table will\nsuffice.\n- For example, you might construct a table showing the\ndate of project finish and cumulative effort expended.\n- However, as always, you must explain the changes you\nmake in the equations so that an independent third\nparty can replicate your simulations.\n\nA. Step 1: The Rework Cycle\nA.\nStep 1: The Rework Cycle\n\nTo begin your analysis, you hypothesize that the \"rework cycle\" is likely to be at the heart of\nthe IT department's project problems. You therefore construct a rework cycle model of a\ntypical department project, as illustrated in the following figure. Your interviews indicated the\nfollowing:\n\n-\nA typical project involves 100 tasks\n-\nUnder optimal conditions, each programmer accomplishes 1 task per month\n-\nNormally, programming error rates are 25%\n-\nIt takes about 4 months to discover design errors\n-\nA typical project starts with 4 staff\n\n-\nDefine the completion of work (Project Finished) when work done is 99% of original\nwork to do. Use Vensim's IF THEN ELSE function. Stop further work accomplishment\nand rework at this point.\n\nA. Step 1: The Rework Cycle\n\nA. Step 1: The Rework Cycle\nA1. Create your model from scratch based on the diagram above and complete the equations\nfor the rework cycle. You do not need to add any variables to the diagram. Select a\nsufficiently long time horizon for simulation and adequately small TIME STEP (See\nAppendix A in Business Dynamics). Hand in your fully documented and dimensionally\nconsistent model (.mdl file).\n\nA2. Which factors do you think are more important in determining project completion --\nproductivity, quality, or rework discovery time? Why? (Please answer this before\nsimulating your model. Your grade is not affected by your answer to this question).\n\nA3. As an extreme test case, if programmers did not make any errors, when would the project\nfinish? What happens to work to do, work done, and undiscovered rework?\n\n*\nYou will want to create custom graphs to show all the important variables.\n\nA4. Now, set the value for normal quality to the value indicated in your interview notes.\nWhen does the project finish? What happens to undiscovered rework in this situation?\n\nB. Step 2: Extending the Model: Adding the Quality on\nQuality Feedback and Variable Rework Discovery Time\n- Time to Discover Rework is not likely to be constant, but falls as\nprogress on the project progresses\nFraction Complete Effect on Rework Discovery\n\n.1\n\n.2\n\n.3\n\n.4\n\n.5\n\n.6\n\n.95\n.7\n\n.8\n.8\n\n.45\n.9\n\n.2\n1.0\n\n.1\n\nC. Step 3: Extending the Model:\nAllowing for Increased Staff\n\n- The weight on progress-based estimate is a function of the\nfraction perceived to be complete:\n\nFraction Complete Weight\n\n.1\n\n.2\n\n.3\n\n.1\n.4\n\n.25\n.5\n\n.5\n.6\n\n.75\n.7\n\n.9\n.8\n\n.9\n\n1.0\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n\n15.872 System Dynamics II\nFall 2013\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    }
  ]
}