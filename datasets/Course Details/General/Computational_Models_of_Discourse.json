{
  "course_name": "Computational Models of Discourse",
  "course_description": "This course is a graduate level introduction to automatic discourse processing. The emphasis will be on methods and models that have applicability to natural language and speech processing.\nThe class will cover the following topics: discourse structure, models of coherence and cohesion, plan recognition algorithms, and text segmentation. We will study symbolic as well as machine learning methods for discourse analysis. We will also discuss the use of these methods in a variety of applications ranging from dialogue systems to automatic essay writing.\nThis subject qualifies as an Artificial Intelligence and Applications concentration subject.",
  "topics": [
    "Engineering",
    "Computer Science",
    "Artificial Intelligence",
    "Theory of Computation",
    "Humanities",
    "Linguistics",
    "Engineering",
    "Computer Science",
    "Artificial Intelligence",
    "Theory of Computation",
    "Humanities",
    "Linguistics"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nCourse Description\n\nThis course is a graduate level introduction to automatic discourse processing. The emphasis will be on methods and models that have applicability to natural language and speech processing.\n\nThe class will cover the following topics: discourse structure, models of coherence and cohesion, plan recognition algorithms, and text segmentation. We will study symbolic as well as machine learning methods for discourse analysis. We will also discuss the use of these methods in a variety of applications ranging from dialogue systems to automatic essay writing.\n\nThis subject qualifies as an Artificial Intelligence and Applications concentration subject.\n\nReadings\n\nCourse readings are available in the\nreadings\nsection.\n\nRequirements\n\nACTIVITIES\n\nPERCENTAGES\n\nClass Participation\n\n10%\n\nTerm Project\n\n90%\n\nThe project (done alone or in collaboration) on one of the topics covered in the course or some other topic related to discourse processing will be defined by each class participant in consultation with the professor. These projects will involve:\n\na survey of background literature\n\nimplementation of an algorithm for discource processing\n\nempirical evaluation of the algorithm performance\n\nAcademic Integrity\n\nCopying or paraphrasing someone's work (code included),\nor\npermitting your own work to be copied or paraphrased, even if only in part, is\nnot allowed\n, and will result in an automatic grade of 0 for the entire assignment in which the copying or paraphrasing was done.",
  "files": [
    {
      "category": "Lecture Notes",
      "title": "lec01.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/77072bd780a5d8a3d624912447b2d339_lec01.pdf",
      "content": "Computational Models of Discourse:\nOverview\nRegina Barzilay\nFebruary 4, 2004\n\nPlan for Today\nOverview of the course:\n- Discourse processing: motivation and background\nDiscourse theories\n-\n- Applications\nAdministration:\n- Requirements\nComputational Models of Discourse: Overview\n1/25\n\nNatural Language Processing\nGoal: Computers using natural language as input or\noutput\nlanguage\nlanguage\nunderstanding\ncomputer\ngeneration\nNLU example: convert an utterance into a sequence of\ncomputer instructions\nNLG example: produce a summary of a patient's record\nComputational Models of Discourse: Overview\n2/25\n\nWhy NLP?\nLots of information is in natural language format\nDocuments\n-\nNews broadcasts\n-\nUser Utterances\n-\nLots of users want to communicate in natural language.\n\"DO what I mean!\"\n-\n\"Now we are betting the company on these natural\ninterface technologies\" Bill Gates, 1997\nComputational Models of Discourse: Overview\n3/25\n\nExample: User Interfaces\nSHRDLU (Winograd, 1972): language interface for\nblock manipulation\nPerson: PICK UP A BIG RED BLOCK.\nComputer: OK. (does it)\nPerson: PUT IT NEAR THE PYRAMID.\nComputational Models of Discourse: Overview\n4/25\n\nExample: Question-Answering(1)\nQ: Who is the president of First Union Corp?\nFirst Union Corp is continuing to wrestle with sever prob-\nlems. According to industry insiders at Pine Webber, their\npresident, John R. Georgius, is planning to retire soon.\nComputational Models of Discourse: Overview\n5/25\n\nExample: Information Retrieval\nShow me the p roof of Cantor theorem\nComputational Models of Discourse: Overview\n6/25\n\nExample: Text Generation\n(Duboue&McKeown, 2003): verbalization of semantic\ndata from movie databases\nActor, born Thomas Connery on August 25, 1930, in Ed-\ninburgh, Scotland. He has a brother, Neil, born in 1938.\nConnery dropped out of school to join the British Navy.\nConnery is best known for his portrayal of the British spy,\nJames Bond, in the 1960s.\nComputational Models of Discourse: Overview\n7/25\n\nChallenges\nSentences cannot be processed in isolation\nCoreference\n-\n- Ordering\n- Segmentation\nWe need to model text and dialog structure\nComputational Models of Discourse: Overview\n8/25\n\nWhat is Discourse\nExample by Charles Fillmore:\nPlease use the toilets, not the pool.\nThe pool for members only.\nComputational Models of Discourse: Overview\n9/25\n\nDiscourse Phenomena\n- a word, phrase, and utterance whose interpretation\nis shaped by the discourse or dialogue context\nJohn arrived at an oasis. He saw the camels around the\nwater hole ...\nJohn arrived at an oasis. He left the camels around the\nwater hole ...\n- a sequence of utterances whose interpretation is\nmore than sum of its component parts\nComputational Models of Discourse: Overview\n10/25\n\nInference in Discourse Processing\n- There are several possible ways to interpret an\nutterance in context\n- We need to find the most likely interpretation\n- Discourse model provides a computational\nframework for this search\nComputational Models of Discourse: Overview\n11/25\n\nDiscourse Exhibits Structure!\n- Discourse can be partition into segments, which can\nbe connected in a limited number of ways\n- Speakers use linguistic devices to make this\nstructure explicit\ncue phrases, intonation, gesture\n- Listeners comprehend discourse by recognizing this\nstructure\n- Kintsch, 1974: experiments with recall\n- Haviland&Clark, 1974: reading time for given/new\ninformation\nComputational Models of Discourse: Overview\n12/25\n\nModels of Discourse Structure\n- Investigation of lexical connectivity patterns as the\nreflection of discourse structure\n- Specification of a small set of rhetorical relation\namong discourse segments\n- Adaption of the notion of grammar\n- Examination of intentions and relations among\nthem as the foundation of discourse structure\nComputational Models of Discourse: Overview\n13/25\n\nExample\n1. A: I'm going camping next week. Do you have a two person tent I could borrow?\n2. B: Sure. I have a two-person backpacking tent.\n3. A: The last trip I was on there was a huge storm.\n4. A: It poured for two hours.\n5. A: I had a tent, but I got soaked anyway.\n6. B: What kind of tent was it?\n7. A: A tube tent.\n8. B: Tube tents don't stand up well in a real storm.\n9. A: True.\n10.B: Where are you going on this trip?\n11.A: Up in the Minarets.\n12.B: Do you need any other equipment?\n13.A: No.\n14.B: Okay. I'll bring the tent tomorrow.\nComputational Models of Discourse: Overview\n14/25\n\nCohesion\nAssumption: Well-formed text exhibits strong lexical\nconnectivity via use of:\n- Repetitions\n- Synonyms\nCoreference\n-\nComputational Models of Discourse: Overview\n15/25\n\nCohesion\n1. A: I'm going camping next week. Do you have a two person tent I could borrow?\n2. B: Sure. I have a two-person backpacking tent.\n3. A: The last trip I was on there was a huge storm.\n4. A: It poured for two hours.\n5. A: I had a tent, but I got soaked anyway.\n6. B: What kind of tent was it?\n7. A: A tube tent.\n8. B: Tube tents don't stand up well in a real storm.\n9. A: True.\n10.B: Where are you going on this trip?\n11.A: Up in the Minarets.\n12.B: Do you need any other equipment?\n13.A: No.\n14.B: Okay. I'll bring the tent tomorrow.\nComputational Models of Discourse: Overview\n16/25\n\nRhetorical Structure Theory\nAssumption: Clauses in well-formed text are related via\npredefined rhetorical relations\nEvidence: a claim\ninformation intended to\n-\nâ†’\nincrease the readers' belief in the claim\n. . .\n-\nComputational Models of Discourse: Overview\n17/25\n\nRhetorical Structure Theory\n3. The last trip I was on there was a huge storm.\n4. It poured for two hours.\n5. I had a tent, but I got soaked anyway.\nresult\nevidence\nComputational Models of Discourse: Overview\n18/25\n\nText Grammars\nAssumption: existence of text grammar in limited\ndomains (analogous to sentence grammar)\n- Tale grammar (31 terminals) V.Propp(1920s)\n- Scientific articles: introduction, conclusions, . . ..\nComputational Models of Discourse: Overview\n19/25\n\nIntention-based Approaches\nDialogue as collaborative activity:\n- Intention of A: to get a tent\n- To achieve this goal, A:\n- Requests a tent from B\n- Convinces B in the importance of this request\nComputational Models of Discourse: Overview\n20/25\n\nComputational Approaches\n- Rule-based approaches: manually encode all the\nrequired domain and common knowledge\n- Machine-learning approaches: learn all the required\nknowledge from a corpus\n- Supervised classification\n- Hidden-Markov Models\n- Clustering\n- Reinforcement learning\nComputational Models of Discourse: Overview\n21/25\n\nApplications\nSummarization\n-\n- Anaphora resolution\n- Essay grading\n- Segmentation\n- Information ordering\n- Dailog processing\n. . .\n-\nComputational Models of Discourse: Overview\n22/25\n\nTypes of Discourse\n- Monologue (narrative, lecture)\n- Human-human dialog\n- Human-machine dialogue\nComputational Models of Discourse: Overview\n23/25\n\nDoes it work?\n- Summarization: F-scores 70% (DUC 2003)\n- Anaphora resolution: F-scores 60-70%\n(Ng&Cardie:2002)\n- RST parsing: 47% (compare with th accuracy of\nsyntactic parsers -- high 80th!)\nDiscourse processing is hard!\nComputational Models of Discourse: Overview\n24/25\n\nDoes it work?\nA day after a suicide bomber killed 10 people in a terror attack on a Jerusalem\nbus, Israeli forces conducted operations Friday in the West Bank and Gaza,\nkilling three Palestinians the Israeli army had identified as terrorists.\nThe\nleader of Hamas said Friday that his group is making every effort to seize\nIsraeli soldiers as bargaining chips for the release of Palestinians in Israeli\njails. At least 45 people were wounded in the terror attack, which Israeli of\nficials said proved the need for what Israel calls a security fence intended to\nblock terrorists from entering the country.\nComputational Models of Discourse: Overview\n25/25"
    },
    {
      "category": "Lecture Notes",
      "title": "lec02.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/434c44c4c6c8f51d1a08d5b483894c1d_lec02.pdf",
      "content": "Topic Segmentation\nRegina Barzilay\nFebruary 8, 2004\n\nWhat is Segmentation?\nSegmentation: determining the positions at which topics\nchange in a stream of text or speech.\nSEGMENT 1: OKAY\ntsk There's a farmer,\nhe looks like ay uh Chicano American,\nhe is picking pears.\nA-nd u-m he's just picking them,\nhe comes off the ladder,\na-nd he- u-h puts his pears into the basket.\nSEGMENT 2: U-h a number of people are going by,\nand one of them is um I don't know,\nI can't remember the first . . . the first person that goes by\nTopic Segmentation\n1/34\n\nMotivation\nInformation Retrieval\n-\nSummarization\n-\n- Question-Answering\n- Word-sense disambiguation and anaphora\nresolution\nTopic Segmentation\n2/34\n\nToday's Topics\n- Human Agreement on Segmentation and Evaluation\n- Segmentation Algorithms:\n- Features: word distribution, cue words, speaker,\nchange,. . .\n- Methods: classification, clustering, HMMs, . . .\n- Segmentation for different genres: text, meetings,\nbroadcasts,\nTopic Segmentation\n3/34\n\nSegmentation: Agreement\nPercent agreement -- ratio between observed\nagreements and possible agreements\nA\nB\nC\n-\n-\n-\n-\n-\n-\n+\n-\n-\n-\n+\n+\n-\n-\n-\n+\n+\n+\n-\n-\n-\n-\n-\n-\n22 = 91%\n8 âˆ— 3\nTopic Segmentation\n4/34\n\nResults on Agreement\nGrosz&Hirschbergberg'92\nnewspaper text\n74-95%\nHearst'93\nexpository text\n80%\nPassanneau&Litman'93\nmonologues\n82-92%\nTopic Segmentation\n5/34\n\nCochran's Test\nEstimate the null hypothesis that the number of subjects\nassigning a boundary at any position is randomly\ndistributed\nTopic Segmentation\n6/34\n\nEvaluation Measures\nBoundary\nNon-boundary\nAlg. Boundary\na\nb\nAlg. Non-boundary\nc\nd\na\nRecall a+c\na\nPrecision a+b\nb+c\nError a+b+c+d\nTopic Segmentation\n7/34\n\nSimple Algorithm\nPassanneau&Litman'93\nRecall\nPrecision\nError\nCue\n72%\n15%\n50%\nPause\n92%\n18%\n49%\nHumans\n74%\n55%\n11%\nTopic Segmentation\n8/34\n\nText Segmentation\nHearst'94\n- Goal: divide text into coherent segments\n- Main Idea: change in lexical connectivity patterns\nsignals topic change\n- Linguistic Theory: Text Cohesion\nTopic Segmentation\n9/34\n\nSkorochodko's Text Types\nChained\nRinged\nMonolith\nPiecewise\nTopic Segmentation\n10/34\n\nFlow model of discourse\nChafe'76:\n\"Our data ... suggest that as a speaker moves from\nfocus to focus (or from thought to thought) there\nare certain points at which they may be a more or\nless radical change in space, time, character con-\nfiguration, event structure, or even world ... At\npoints where all these change in a maximal way,\nan episode boundary is strongly present.\"\nTopic Segmentation\n11/34\n\nExample\nStargazers Text(from Hearst, 1994)\n- Intro - the search for life in space\n- The moon's chemical composition\n- How early proximity of the moon shaped it\n- How the moon helped the life evolve on earth\n- Improbability of the earth-moon system\nTopic Segmentation\n12/34\n\nExample\n-------------------------------------------------------------------------------------------------------------+\nSentence:\n95|\n-------------------------------------------------------------------------------------------------------------+\nform\n111 1\n1 1\n|\n8 scientist\n1 1\n|\nspace 11\n|\nstar\n11 22 111112 1 1 1\n11 1111\n1 |\nbinary\n11 1\n1|\ntrinary\n1|\n8 astronomer 1\n1 1\n1 1\n|\norbit\n1 1\n|\npull\n1 1\n1 1\n|\nplanet\n21 11111\n1|\ngalaxy\n1 11\n1|\nlunar\n1 1\n|\nlife 1 1 1\n11 1 11 1\n1 1\n1 111 1 1 |\nmoon\n13 1111\n1 1 22 21 21\n11 1\n|\nmove\n|\n7 continent\n2 1 1 2 1\n|\n3 shoreline\n|\ntime\n1 1 1\n1 |\nwater\n|\nsay\n1 1\n|\nspecies\n1 1 1\n|\n-------------------------------------------------------------------------------------------------------------+\nSentence:\n95|\n-------------------------------------------------------------------------------------------------------------+\nTopic Segmentation\n13/34\n\nSegmentation Algorithm\n- Preprocessing and Initial segmentation\n- Similarity Computation\n- Boundary Detection\nTopic Segmentation\n14/34\n\nPreprocessing and Initial Segmentation\nTokenization\n-\n- Morphological analysis\n- Token-sequence division\nTopic Segmentation\n15/34\n\nSimilarity Computation: Representation\nVector-Space Representation\nSENTENCE1: I like apples\nSENTENCE2: Apples are good for you\nVocabulary\nApples\nAre\nFor\nGood\nI\nLike\nyou\nSentence1\nSentence2\nTopic Segmentation\n16/34\n\nSimilarity Computation: Cosine Measure\nCosine of angle between two vectors in n-dimensional\nspace\nt\nsim(b1, b2) =\nwy,b1wt,b2\nn\n\nw\nt\nt,b1\nt=1 wt,b2\nSENTENCE1: 1 0 0 0 1 1 0\nSENTENCE2: 1 1 1 1 0 0 1\nsim(S1,S2) =\n1âˆ—0+0âˆ—1+0âˆ—1+0âˆ—1+1âˆ—0+1âˆ—0+0âˆ—1\nâˆš\n(12+02+02+02+12+12+02)âˆ—(12+12+12+12+02+02+12 = 0.26\nTopic Segmentation\n17/34\n\nSimilarity Computation: Output\n0.22\n0.33\nTopic Segmentation\n18/34\n\nGap Plot\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\nTopic Segmentation\n19/34\n\nBoundary Detection\nBased on changes in sequence of similarity scores:\nDepth Scores: relative depth (in comparison to the\nclosest maximum)\nNumber of segments: s - Ïƒ/2\nTopic Segmentation\n20/34\n\nSegmentation Evaluation\nComparison with human-annotated\nsegments(Hearst'94):\n- 13 articles (1800 and 2500 words)\n- 7 judges\n- boundary if three judges agree on the same\nsegmentation point\nTopic Segmentation\n21/34\n\nAgreement on Segmentation\nTopic Segmentation\n22/34\n\nEvaluation Results\nMethods\nPrecision\nRecall\nBaseline 33%\n0.44\n0.37\nBaseline 41%\n0.43\n0.42\nChains\n0.64\n0.58\nBlocks\n0.66\n0.61\nJudges\n0.81\n0.71\nTopic Segmentation\n23/34\n\nMore Results\n- High sensitivity to change in parameter values\n- Thesaural information does not help\nMost of the mistakes are \"close misses\"\n-\nTopic Segmentation\n24/34\n\nMeeting Segmentation\nMotivation: Facilitate information Access\n-\n- Challenges:\n- High error rate in transcription\n- Multi-thread structure\nTopic Segmentation\n25/34\n\nAlgorithm for Feature Segmentation\nSupervised ML\n(Galley&McKeown&Fosler-Lussier&Jing'03)\n- Combines multiple knowledge source:\n- cue phrases\n- silences\n- overlaps\n- speaker change\n- lexical cohesion\n- Uses probabilistic classifier (decision tree) to\ncombine them\nTopic Segmentation\n26/34\n\nCue Word Selection\nAutomatic computation of cue words:\n- Compute word probability to appear in boundary\nposition\n- Select words with the highest probability\nRemove non-cues.\n-\nTopic Segmentation\n27/34\n\nSelected Cue Words\nOKAY\n93.05\nshall\n0.44\nanyway\n0.43\nalright\n0.64\nlet's\n0.66\ngood\n0.81\nTopic Segmentation\n28/34\n\nSilences\n- Pauses -- speaker silence in the middle of her\nspeech\n- Gap -- silences not attributable to any party\nTopic boundaries are typically preceeded by gaps\nTopic Segmentation\n29/34\n\nOverlaps\n- Average overlap rate within some window\nLittle overlap in the beginning of segments\nTopic Segmentation\n30/34\n\nSpeaker Change\nTopic Segmentation\n31/34\n\nDetermination of Window Size\nFeature\nTag\nSize(sec)\nSide\nCue phrases\nCUE\nboth\nSilence (gaps)\nSIL\nleft\nOverlap\nOVR\nright\nSpeaker activity\nACT\nboth\nLexical cohesion\nLC\nboth\nTopic Segmentation\n32/34\n\nExamples of Derived Rules\nCondition\nDecision\nConf.\nLCâ‰¤0.67, CUEâ‰¥1,\nOVRâ‰¤1.20, SILâ‰¤3.42\nyes\n94.1\nLCâ‰¤0.35, SIL>3.42,\nOVRâ‰¤4.55\nyes\n92.2\nCUEâ‰¥1, ACT>0.1768,\nOVRâ‰¤1.20, LCâ‰¤0.67\nyes\n91.6\n...\ndefault\nno\nTopic Segmentation\n33/34\n\nResults\nMethod\nPk\nWD\nFeature-based\n23.00\n25.47\nCohesion-based\n31.91\n35.88\nTopic Segmentation\n34/34"
    },
    {
      "category": "Lecture Notes",
      "title": "lec03.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/e29b6ff9a4209b96df99904e434b2c87_lec03.pdf",
      "content": "Topic Segmentation\nRegina Barzilay\nFebruary 11, 2004\n\nExample\n-------------------------------------------------------------------------------------------------------------+\nSentence:\n95|\n-------------------------------------------------------------------------------------------------------------+\nform\n111 1\n1 1\n|\n8 scientist\n1 1\n|\nspace 11\n|\nstar\n11 22 111112 1 1 1\n11 1111\n1 |\nbinary\n11 1\n1|\ntrinary\n1|\n8 astronomer 1\n1 1\n1 1\n|\norbit\n1 1\n|\npull\n1 1\n1 1\n|\nplanet\n21 11111\n1|\ngalaxy\n1 11\n1|\nlunar\n1 1\n|\nlife 1 1 1\n11 1 11 1\n1 1\n1 111 1 1 |\nmoon\n13 1111\n1 1 22 21 21\n11 1\n|\nmove\n|\n7 continent\n2 1 1 2 1\n|\n3 shoreline\n|\ntime\n1 1 1\n1 |\nwater\n|\nsay\n1 1\n|\nspecies\n1 1 1\n|\n-------------------------------------------------------------------------------------------------------------+\nSentence:\n95|\n-------------------------------------------------------------------------------------------------------------+\nTopic Segmentation\n1/23\n\nSegmentation Algorithm\n- Preprocessing and Initial segmentation\n- Similarity Computation\n- Boundary Detection\nTopic Segmentation\n2/23\n\nEvaluation Results\nMethods\nPrecision\nRecall\nBaseline 33%\n0.44\n0.37\nBaseline 41%\n0.43\n0.42\nChains\n0.64\n0.58\nBlocks\n0.66\n0.61\nJudges\n0.81\n0.71\nTopic Segmentation\n3/23\n\nMore Results\n- High sensitivity to change in parameter values\n- Thesaural information does not help\nMost of the mistakes are \"close misses\"\n-\nTopic Segmentation\n4/23\n\nToday's Topics\n- Hierarchical segmentation\n- HMM-based segmentation\n- Supervised segmentation\nTopic Segmentation\n5/23\n\nAgglomerative Clustering\n- First, each data point is a singleton cluster\n- Next, closest points are merged until all points are\ncombined\nTopic Segmentation\n6/23\n\nAgglomerative Clustering\n- Complete-link -- merge the two clusters whose\nmerger has the smallest diameter\n- Single-link -- merge the two clusters whose two\nclosest members have the smallest distance\n- Average-link -- merges in each iteration the pair of\nclusters with the highest cohesion.\nTopic Segmentation\n7/23\n\nHierarchical Segmentation\n(Yaari, 1997)\n- Partition the text into elementary segments\n- While more than one segment left do\n- Find closest adjacent segments si, si+1(based on\ncosine measure)\n- Merge si, si+1 into one segment\nTopic Segmentation\n8/23\n\nBroadcast News Segmentation\nGoal: divide news stream into stories\n-\n- Assumption: news stories typically belong to one of\nseveral categories (sports, politics, . . .)\nTopic Segmentation\n9/23\n\nHMM-based Segmentation: Construction\nvan Mulbregt&Carp&Gillick&Lowe'99:\n- Each state of HMM represents a topic\n- Topics are derived via story clustering\n- Emission probabilities for a state are computed\nbased on a unigram language model\nTopic Segmentation\n10/23\n\nHMM-based Segmentation: Decoding\n- Transitions are controlled by switch penalty\n- Segmentation via Viterbi-style decoding\nTopic Segmentation\n11/23\n\nTDT Segmentation Results\n- Data: 384 shows, 6,000 stories and 2.2 million\nwords\n- Sources: ABC, CNN, . . .\nTDT Evaluation Measure:\n-\nCSeg = Î± âˆ— PMiss + (1 - Î±) âˆ— PF alseAlarm\nTopic Segmentation\n12/23\n\nTDT Performance\nInput Type\nCSeg for ABC\nASR\n0.1723\nClosed Captions\n0.1515\nTranscripts\n0.1356\nNote the impact for\nASR!\nTopic Segmentation\n13/23\n\nMeeting Segmentation\nMotivation: Facilitate information Access\n-\n- Challenges:\n- High error rate in transcription\n- Multi-thread structure\nTopic Segmentation\n14/23\n\nAlgorithm for Feature Segmentation\nSupervised ML\n(Galley&McKeown&Fosler-Lussier&Jing'03)\n- Combines multiple knowledge source:\n- cue phrases\n- silences\n- overlaps\n- speaker change\n- lexical cohesion\n- Uses probabilistic classifier (decision tree) to\ncombine them\nTopic Segmentation\n15/23\n\nCue Word Selection\nAutomatic computation of cue words:\n- Compute word probability to appear in boundary\nposition\n- Select words with the highest probability\nRemove non-cues.\n-\nTopic Segmentation\n16/23\n\nSelected Cue Words\nOKAY\n93.05\nshall\n0.44\nanyway\n0.43\nalright\n0.64\nlet's\n0.66\ngood\n0.81\nTopic Segmentation\n17/23\n\nSilences\n- Pauses -- speaker silence in the middle of her\nspeech\n- Gap -- silences not attributable to any party\nTopic boundaries are typically preceeded by gaps\nTopic Segmentation\n18/23\n\nOverlaps\n- Average overlap rate within some window\nLittle overlap in the beginning of segments\nTopic Segmentation\n19/23\n\nSpeaker Change\nTopic Segmentation\n20/23\n\nDetermination of Window Size\nFeature\nTag\nSize(sec)\nSide\nCue phrases\nCUE\nboth\nSilence (gaps)\nSIL\nleft\nOverlap\nOVR\nright\nSpeaker activity\nACT\nboth\nLexical cohesion\nLC\nboth\nTopic Segmentation\n21/23\n\nExamples of Derived Rules\nCondition\nDecision\nConf.\nLCâ‰¤0.67, CUEâ‰¥1,\nOVRâ‰¤1.20, SILâ‰¤3.42\nyes\n94.1\nLCâ‰¤0.35, SIL>3.42,\nOVRâ‰¤4.55\nyes\n92.2\nCUEâ‰¥1, ACT>0.1768,\nOVRâ‰¤1.20, LCâ‰¤0.67\nyes\n91.6\n...\ndefault\nno\nTopic Segmentation\n22/23\n\nResults\nMethod\nPk\nWD\nFeature-based\n23.00\n25.47\nCohesion-based\n31.91\n35.88\nTopic Segmentation\n23/23"
    },
    {
      "category": "Lecture Notes",
      "title": "lec04.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/5f3ebb21615d4d72406cdd36e3fc3680_lec04.pdf",
      "content": "Lexical Cohesion and Coherence\nRegina Barzilay\nFebruary 17, 2004\n\nLeftovers from Last Time\nInput Type\nCSeg for ABC\nASR\n0.1723\nClosed Captions\n0.1515\nTranscripts\n0.1356\nNote the impact for ASR!\nLexical Cohesion and Coherence\n1/34\n\nLack of Coherence\nHobbs' Example(1982)\nWhen Teddy Kennedy paid a courtesy call on Ronald Reagan recently, he\nmade only one Cabinet suggestion. Western surveillance satellites confirmed\nhuge Soviet troop concentrations virtually encircling Poland.\nLexical Cohesion and Coherence\n2/34\n\nCoherence in Automatically Generated Text\nDUC results: most of automatic summaries exhibit\n-\nlack of coherence\n- Is it possible to automatically compute text\ncoherence?\n- text representation\n- inference procedure\nLexical Cohesion and Coherence\n3/34\n\nText Representation\n-------------------------------------------------------------------------------------------------------------+\nSentence:\n95|\n-------------------------------------------------------------------------------------------------------------+\nform\n111 1\n1 1\n|\n8 scientist\n1 1\n|\nspace 11\n|\nstar\n11 22 111112 1 1 1\n11 1111\n1 |\nbinary\n11 1\n1|\ntrinary\n1|\n8 astronomer 1\n1 1\n1 1\n|\norbit\n1 1\n|\npull\n1 1\n1 1\n|\nplanet\n21 11111\n1|\ngalaxy\n1 11\n1|\nlunar\n1 1\n|\nlife 1 1 1\n11 1 11 1\n1 1\n1 111 1 1 |\nmoon\n13 1111\n1 1 22 21 21\n11 1\n|\nmove\n|\n7 continent\n2 1 1 2 1\n|\n3 shoreline\n|\ntime\n1 1 1\n1 |\nwater\n|\nsay\n1 1\n|\nspecies\n1 1 1\n|\n-------------------------------------------------------------------------------------------------------------+\nSentence:\n95|\n-------------------------------------------------------------------------------------------------------------+\nLexical Cohesion and Coherence\n4/34\n\nToday's Topics\n- Two linguistic theories of text connectivity\n- Text Cohesion (Halliday&Hasan'76)\n- Centering Theory (Grosz&Joshi&Weinstein'83)\n- Application to automatic essay scoring\nLexical Cohesion and Coherence\n5/34\n\nText cohesion\nHobbs' Example(1982)\nThe concept of cohesion refers to relations of meaning that exist within the\ntext, and that defines it as a text. Cohesion occurs where the interpretation of\nsome element in the discourse dependent on that of another.\nLexical Cohesion and Coherence\n6/34\n\nText Cohesion\nCohesion captures devices that link sentences into a text\nLexical cohesion\n-\nReferences\n-\n- Ellipsis\n- Conjunctions\nLexical Cohesion and Coherence\n7/34\n\nExample\nHalliday&Hasan(1982)\nTime flies.\n- You can't; they fly too quickly.\nFind three cohesive ties!\nLexical Cohesion and Coherence\n8/34\n\nLexical Chains: Example\n1. There was once a little girl and a little boy and a dog\n2. And the sailor was their daddy\n3. And the little doggy was white\n4. And they like the little doggy\n5. And they stroke it\n6. And they fed it\n7. And they ran away\n8. And then daddy had to go on a ship\n9. And the children misssed 'em\n10. And they began to cry\nLexical Cohesion and Coherence\n9/34\n\nLexical Chains: Applications\nSummarization\n-\n- Segmentation\n- Malapropism Detection\nInformation Retrieval\n-\nLexical Cohesion and Coherence\n10/34\n\nLexical Chains: Computation\n\"Associanist text models\"\n- Define word similarity function\n- Define \"insertion conflict\" strategy (greedy vs.\ndynamic strategy)\nLexical Cohesion and Coherence\n11/34\n\nLexical Chains: Example\nLexical Cohesion and Coherence\n12/34\n\nLexical Chains: Accuracy\nExample: Entertainment-service 1 auto-maker 1 enterprise 1\nmassachusetts-institute 1 technology-microsoft 1 microsoft 10 concern\n1 company 6\n- The accuracy bounded by the quality of a lexical\nresource\n- The need in disambiguation makes the task harder\nDisambiguation accuracy around 60%\nFor more examples see:\nhttp://www.cs.columbia.edu/nlp/summarization-test/index.html\nLexical Cohesion and Coherence\n13/34\n\nAutomatic Measurement of Text Coherence\n- Cohesive ties reflect the degree of text coherence\n- First attempts to (semi-) automate cohesion\njudgments rely on:\n- propositional modeling of text structure\n(Kintsch&van Dijk'78)\ntime consuming and requires training\n- readability measures (Flesch'48)\nweak correlation with comprehension measures\nLexical Cohesion and Coherence\n14/34\n\nVector-Based Coherence Assessment\n- Each sentence is represented as a weighted vector of\nits terms SENTENCE1: 1 0 0 0 1 1 0\nSENTENCE2: 1 1 1 1 0 0 1\n- Distance between two adjacent sentences is\nmeasured using cosine\nwy,b1wt,b2\nsim(b1, b2) =\nt\nn\nw\nw\nt\nt,b1\nt=1\nt,b2\n- Lexical continuity is measured as average distance\nbetween sentences in a paragraph\nLexical Cohesion and Coherence\n15/34\n\nTerm similarity\nLatent Semantic Analysis (Deerwester'90)\n- Goal: identification of semantically similar words\nbirth, born, baby\n- Assumption: the context surrounding a given word\nprovides important information about its meaning\n- Method: Singular Vector Decomposition\nLexical Cohesion and Coherence\n16/34\n\nExperimental Set-Up\nData from (Britton& Gulgoz'88)\nSource: text on the airwar in Vietnam from an Air\n-\nForce training textbook\n- Various revision methods to improve text\nreadability:\n- Principled (based on propositional model)\n- Heuristic (based on reader's intuition)\n- Readability (based on readability index)\nLexical Cohesion and Coherence\n17/34\n\nExperimental Set-Up\nData from (Britton& Gulgoz'88)\n- Evaluation: based on recall, efficiency recall and\nscores on a multiple choice\n- Assessment: Principled and Heuristic is better than\nReadability and Original\nLexical Cohesion and Coherence\n18/34\n\nResults\nWeighted\nNo.\nInference\nLSA\nword\nprops\nEfficiency\nmult.\nText\ncoherence overlap recalled (props/min) choice\nOriginal\n0.192\n0.047\n35.5\n3.44\n37.11\nReadability rev.\n0.193\n0.073\n32.8\n3.57\n29.74\nPrincipled rev.\n0.347\n0.204\n58.6\n5.24\n46.44\nHeuristic rev.\n0.403\n0.225\n56.2\n6.01\n48.23\nLexical Cohesion and Coherence\n19/34\n\nUnderstanding the Results\n- No significant difference between LSA and the\nbaseline model in this experiment\n- Other experiments showed that LSA may perform\nbetter, but note need in parameter estimation\n- Neither model is used for prediction\nLexical Cohesion and Coherence\n20/34\n\nCentering Theory\n(Grozs&Joshi&Weinstein'95)\n- Goal: to account for differences in perceived\ndiscourse\nFocus: local coherence\n-\nglobal vs immediate focusing in discourse\n(Grosz'77)\n- Method: analysis of reference structure\nLexical Cohesion and Coherence\n21/34\n\nPhenomena to be Explained\nJohh went to his favorite music\nstore to buy a piano.\nHe had frequented the store for\nmany years.\nHe was excited that he could fi-\nnally buy a piano.\nHe arrived just as the store was\nclosing for the day.\nJohn went to his favorite music\nstore to buy a piano.\nIt was a store John had fre-\nquented for many years.\nHe was excited that he could fi-\nnally buy a piano.\nIt was closing just as John ar-\nrived.\nLexical Cohesion and Coherence\n22/34\n\nAnalysis\n- The same content, different realization\nVariation in coherence arises from choice of\n-\nsyntactic expressions and syntactic forms\nLexical Cohesion and Coherence\n23/34\n\nAnother Example\nJohn really goofs sometimes.\nYesterday was a beautiful day and he was excited about\ntrying out his new sailboat.\nHe wanted Tony to join him on a sailing trip.\nHe called him at 6am.\nHe was sick and furious at being woken up so early.\nLexical Cohesion and Coherence\n24/34\n\nCentering Theory: Basics\n- Unit of analysis: centers\n- \"Affiliation\" of a center: utterance (U) and discourse\nsegment (DS)\n- Function of a center: to link between a given\nutterance and other utterances in discourse\nLexical Cohesion and Coherence\n25/34\n\nCenter Typology\n- Types:\n- Forward-looking Centers Cf (U, DS)\n- Backward-looking Centers Cb (U, DS)\n- Connection: Cb (Un) connects with one of Cf\n(Un-1)\nLexical Cohesion and Coherence\n26/34\n\nExample\nJohn went to his favorite music store to buy a piano.\nIt was a store John had frequented for many years.\nHe was excited that he could finally buy a piano.\nIt was closing just as John arrived.\nLexical Cohesion and Coherence\n27/34\n\nConstraints on Distribution of Centers\n- Cf is determined only by U;\n- Cf are partially ordered in terms of salience\n- The most highly ranked element of Cf (Un-1) is\nrealized as Cb (Un)\n- Syntax plays role in ambiguity resolution: subj >\nind obj > obj > others\n- Types of transitions: center continuation, center\nretaining, center shifting\nLexical Cohesion and Coherence\n28/34\n\nCenter Continuation\nContinuation of the center from one utterance not only\nto the next, but also to subsequent utterances\n- Cb(Un+1)=Cb(Un)\n- Cb(Un+1) is the most highly ranked element of\nCf(Un+1) (thus, likely to be Cb(Un+2)\nLexical Cohesion and Coherence\n29/34\n\nCenter Retaining\nRetention of the center from one utterance to the next\n- Cb(Un+1)=Cb(Un)\n- Cb(Un+1) is not the most highly ranked element of\nCf(Un+1) (thus, unlikely to be Cb(Un+2)\nLexical Cohesion and Coherence\n30/34\n\nCenter Shifting\nShifting the center, if it is neither retained no continued\n- Cb(Un+1) <> Cb(Un)\nLexical Cohesion and Coherence\n31/34\n\nCoherent Discourse\nCoherence is established via center continuation\nJohn went to his favorite music\nstore to buy a piano.\nHe had frequented the store for\nmany years.\nHe was excited that he could fi-\nnally buy a piano.\nHe arrived just as the store was\nclosing for the day.\nJohn went to his favorite music\nstore to buy a piano.\nIt was a store John had fre-\nquented for many years.\nHe was excited that he could fi-\nnally buy a piano.\nIt was closing just as John ar-\nrived.\nLexical Cohesion and Coherence\n32/34\n\nApplication to Essay Grading\n(Miltsakaki&Kukich'00)\nFramework: GMAT e-rater\n-\n- Implementation: manual annotation of coreference\ninformation\n- Grading: based on ratio of shifts\n- Data: GMAT essays\nLexical Cohesion and Coherence\n33/34\n\nStudy results\n- Correlation between shifts and low grades\n(established using t-test)\n- Improvement of score prediction in 57%\nLexical Cohesion and Coherence\n34/34"
    },
    {
      "category": "Lecture Notes",
      "title": "lec05.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/c756903d6e12fa721c9da65764fce31c_lec05.pdf",
      "content": "Reference Resolution\nRegina Barzilay\nFebruary 18, 2004\n\nReference Resolution: Example\nThe Salesgirl (Burns and Allen)\nGracie: And then Mr. and Mrs. Jones were having matrimonal\ntrouble, and my brother was hired to watch Mrs. Jones.\nGeorge: Well, I am imagine she was a very attractive woman.\nGracie: She was, and my brother watched her day and night for\nsix month.\nGeorge: Well, what happened?\nGracie: She finally got a divorce.\nGeorge: Mrs. Jones?\nGracie: No, my brother's wife.\nReference Resolution\n1/??\n\nReference Resolution\n- Task: determine which noun phrases refer to each\nreal-world entity mentioned in a document\n- Goal: partition noun phrases in a text into\ncoreference equivalence classes, with one cluster for\neach set of coreferent NPs\n- Difference between anaphora and coreference\nIn the previous example: {Mrs. J2ones, she, she, Mrs. Jones},\n{my brother, my brother}, {my brother's wife}\nReference Resolution\n3/??\n\nToday's Topics\nMotivation\n-\n- Types of referential expressions\n- Syntactic and semantic constraints on coreference\n- Preferences in coreference interpretation2\n- Algorithm's for coreference resolution\nReference Resolution\n4/??\n\nMotivation\nInformation extraction\n-\n- Question-Answering\nMachine-Translation\n-\npronoun in the Malay language is translated by its antecedent\n(Mitkov, 1999)\nSummarization\n-\nReference Resolution\n5/??\n\nWhen something goes wrong\nIn the past decade almost all Islamic revivalist movements\nhave been labeled fundamentalists, whether they be of\nextremist or moderate origin. The widespread impact of\nthe term is obvious from the following quotation from one\nof the most influential Encyclopedias under the title\n'Fundamentalist': \"The term fundamentalist has. . . been\nused to describe members of militant Islamic groups.\" Why\nwould the media use this specific word, so often with\nrelation to Muslims? Most of them are radical Baptist,\nLutheran and Presbyterian groups.\nReference Resolution\n6/??\n\nWhen something goes wrong\nWhy would the media use this specific word, so often with\nrelation to Muslims?\nBefore the term fundamentalist was branded for Muslims,\nit was, and still is, being used by certain Christian\ndenominations. Most of them are radical Baptist,\nLutheran and Presbyterian groups.\nReference Resolution\n7/??\n\nTypes of referential expressions: Nouns\nIndefinite Noun Phrases:\n-\nI saw an Acura Integra today.\nSome Acura Integras were being unloaded.\nI saw this awesome Acura Integra today.\nDefinite Noun Phrases\n-\nI saw an Acura Integra today. The Integra was white and needed\nto be washed.\nThe fastest car in the Indianapolis 500 was an Integra.\nReference Resolution\n8/??\n\nPronouns\nStronger constrains on using pronouns than on noun\nphrase references.\n- Require a high degree of activation from a referent\n- Have short activation span\na. John went to Bob's party, and parked next to a Acura Integra.\nb. He went inside and talked to Bob for more than an hour.\na. Bob told him that he recently got engaged.\nb. ??He also said that he bought it yesterday.\nReference Resolution\n9/??\n\nDemonstratives and One Anaphora\n- Demonstratives (this, that) capture spatial proximity\nI like this one, better than that\n- One Anaphora evokes a new entity into the\ndiscourse whose description is dependent of this\nnew entity\nI saw no less that 6 Acuras today. Now I was one.\nReference Resolution\n10/??\n\nTroublemakers\n- Inferrables: inferential relation to an evoked entity\nI almost bought an Acura today, but a door had a dent and the\nengine seemed noisy.\nDiscontinuous Sets: refer to entities that do not\n-\nform a set in a text\nJohn has an Acura, and Mary has a Mazda. They drive them all\nthe time.\n- Generics: refer to general set of entities (in contrast\nto a specific set mentioned in text)\nI saw no less than six Acuras today. They are the coolest cars.\nReference Resolution\n11/??\n\nSyntactic Constraints on Coreference\n- Number Agreement\n* John has a new Acura. They are red.\nJohn has three New Acuras. It is red.\n- Person and Case Agreement\n* John and Mary have Acuras. We love them.\nYou and I have Acuras. We love them.\nReference Resolution\n12/??\n\nSyntactic Constraints\n- Gender Agreement\nJohn has an Acura. It is attractive.\n- Syntactic Agreement\nJohn bought himself a new Acura.\nJohn bought him a new Acura.\nReference Resolution\n13/??\n\nSemantic Constraints\n- Selectional restrictions of the verb on its arguments\n(1) John parked his Acura in the garage. He had driven it around\nfor hours.\n(2) John parked his Acura in the garage. It is incredibly messy,\nwith old bike and car parts lying around everywhere.\n(3) John parked his Acura in downtown Beverly Hills. It is\nincredibly messy, with old bike and car parts lying around\neverywhere.\nReference Resolution\n14/??\n\nPreferences in Pronoun Interpretation\n- Recency: Entities introduced in recent utterances\nare more salient than those introduced further back\nJohn has an Integra. Bill has a Legend. Mary likes to drive it.\n- Repeated mention: Entities that have been focus on\nin the prior discourse are more likely to continue to\nbe focused on in subsequent discourse\nJohn needed a car to get his new job. He decided that he wanted\nsomething sporty. Bill went to the Acura dealership with him. He\nbought an Integra.\nReference Resolution\n15/??\n\nPreferences in Pronoun Interpretation\n- Grammatical Role: Hierarchy of candidate entities\nbased on their grammatical role\nJohn went to the Acura dealership with Bill. He bought an\nIntegra.\nBill went to the Acura dealership with John. He bought an\nIntegra.\nParallelism:\n-\nMary went with Sue to the Acura dealership. Sally went with her\nto the Mazda dealership.\nReference Resolution\n16/??\n\nPreferences in Pronoun Interpretation\nVerb Semantics: emphasis on one of verb's arguments\n- \"implicit causality\" of a verb causes change in\nsalience of verb arguments\nJohn telephoned Bill. He lost the pamphlet on Acuras.\nJohn criticized Bill. He lost the pamphlet on Acuras.\n- thematic roles (Goal, Source) cause change in\nsalience of verb arguments\nJohn seized the Acura pamphlet from Bill. He loves reading\nabout cars.\nJohn passed the Acura pamphlet to Bill. He loves reading about\ncars.\nReference Resolution\n17/??\n\nGeneric Algorithm\nIdentification of Discourse Entities\n-\nIdentify nouns and pronouns in text\nCharacterization of Discourse Entities\n-\nCompute for each discourse entity NPi a set of values from\n{Ki1, . . . , kim} from m knowledge sources\n- Anaphoricity Determination\nEliminate non-anaphoric expressions to cut search space\nGeneration of Candidate Antecedents\n-\nCompute for each anaphoric NPj a list of candidate antecedents\nCj\nReference Resolution\n18/??\n\nGeneric Algorithm(cont.)\n- Filtering\nRemove all the members of Cj that violate reference constraints\n- Scoring/Ranking\nOrder the candidates based on preferences and soft constraints\n- Searching/Clustering\nClustering of instances with the same antecedent\nReference Resolution\n19/??\n\nReference Resolution: Trends\n- Knowledge-Rich Approaches vs Knowledge-Lean\nApproaches\n- Semi-automatic Fully-Automatic Preprocessing\n- Small-scale vs Large-Scale Evaluation\nReference Resolution\n20/??\n\nKnowledge-Lean Multi-strategy Approach\n(Lappin&Leass, 1994)\n- Integrates the effects of the recency and\nsyntactically-based preferences\n- Doesn't rely on semantic or pragmatic knowledge\n- Follows greedy strategy\n- Two stages: discourse model update and pronoun\nresolution\nReference Resolution\n21/??\n\nDiscourse Model Update\n(Lappin&Leass, 1994)\n- Add every new discourse entity to discourse model\n- Update its value based on salience factors\n- Cut in half recency values when process new entity\n(recency enforcement)\nReference Resolution\n22/??\n\nSalience Factors\nSentence Recency\nSubject Emphasis\nExistential Emphasis\nAccusative\nIndirect Object\nNon-adverbial Emphasis\nHead-noun Emphasis\nReference Resolution\n23/??\n\nSyntactic Factors\nsubject > existential predicate nominal > object >\nindirect object > demarcated adverbial PP\n1. An Acura Integra is parked on the lot. (subject)\n2. There is an Acura Integra parked in the lot.\n3. . . .\n4. Inside his Acura Integra, John kissed Mary. (demarcated\nadverbial PP)\nPenalty for non-head occurrences\nScore for equivalence classes\nReference Resolution\n24/??\n\nAlgorithm\n1. Remove potential referents that do not agree in\nnumber or gender with the pronoun\n2. Remove potential referents that do not pass\nintrasentetial syntactic coreference constraints\n3. Update the total salience value of the referent\n4. Select the referent with the highest value\nAccuracy on unseen data: 86%\nReference Resolution\n25/??\n\nClustering for Coreference\n(Cardie&Wagstaff:1999)\n- Each group of coreferent noun phrases defines an\nequivalence class\n- Distance measure incorporates \"linguistic intuition\"\nabout similarity of noun phrases\n- Hard constraints enforce clustering construction\nReference Resolution\n26/??\n\nInstance Representation\nBased noun phrases (automatically computed) are\nrepresented with 11 features:\nIndividual Words\n-\nHead Word\n-\nPosition\n-\n- Pronoun type (nominative, accusative)\n- Semantic Class: Time, City, Animal, Human, Object (WordNet)\n- Gender (WordNet, specified list)\n- Animacy (based on WordNet)\nReference Resolution\n27/??\n\nDistance Metric\ndist(NPi, NPj) =\n\nwf âˆ— incompf(NPi, NPj)\nf\nReference Resolution\n28/??\n\nClustering Algorithm\n- Initialization: every noun is a singleton\n- From right to left, compare each noun to all\nproceeding clusters\n- Combine \"close enough\" clusters unless there exist\nany incompatible NP\nExample: The chairman spoke with Ms. White. He ...\nReference Resolution\n29/??\n\nResults\nMUC-6 (30 documents): Recall 48.8*%, Precision\n57.4%, F-measure 52.8%\nBaseline: 34.6%, 69.3%, 46.1% Types of Mistakes:\n- Parsing mistakes\n- Coarse entity representation and mistakes in feature\ncomputation\n- Greedy nature of the algorithm\nReference Resolution\n30/??\n\nSupervised Learning\n(Soon et al.,2001)\nDecision Tree Induction\n-\n- Shallow feature representation (12 features):\n- \"corrective\" clustering\n- Significant performance gain over rule-based\nalgorithms\nReference Resolution\n31/??\n\nAdding Linguistic Knowledge\nRich Linguistic representation for learning (Ng&Cardie\n2002)\n53 features\n-\nmanual feature selection\n-\n- significant gain in performance over (Soon et al.,\n2001)\nReference Resolution\n32/??"
    },
    {
      "category": "Lecture Notes",
      "title": "lec06.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/879f8aae163a2bf3d3e1c2d131f7355a_lec06.pdf",
      "content": "Reference Resolution\nRegina Barzilay\nFebruary 23, 2004\n\nAnnouncements\n3/3 -- first part of the projects\nExample topics\n- Segmentation\nIdentification of discourse structure\n-\nSummarization\n-\n- Anaphora resolution\n- Cue phrase selection\nReference Resolution\n1/30\n\nReference Resolution\nCaptain Farragut was a good seaman, worthy of the\nfrigate he commanded. His vessel and he were one. He\nwas the soul of it.\n- Coreference resolution: {the frigate, his vessel, it}\n- Anaphora resolution: {his vessel, it}\nCoreference is a harder task!\nReference Resolution\n2/30\n\nLast Time\n- Symbolic Multi-Strategy Anaphora Resolution\n(Lappin&Leass, 1994)\n- Clustering-based Coreference Resolution\n(Cardie&Wagstaff, 1999)\n- Supervised ML Coreference Resolution + Clustering\n(Soon et al, 2001), (Ng&Cardie, 2002)\nReference Resolution\n3/30\n\nFeatures (Soon et al, 2001)\n- distance in sentences between anaphora and antecedent?\n- antecedent in a pronoun?\n- weak string identity between anaphora and antecedent?\n- anaphora is a definite noun phrase?\n- anaphora is a demonstrative pronoun?\n- number agreement between anaphora and antecedent\n- semantic class agreement anaphora and antecedent\n- gender agreement between anaphora and antecedent\n- anaphora and antecedent are both proper names?\nan alias feature\n-\n- an appositive feature\nReference Resolution\n4/30\n\nObservations\n(Ng&Cardie'2002)\n0,76,83,C,D,C,D,D,D,D,D,I,I,C,I,I,D,N,N,D,C,D,D,N,N,N,N,N,C,Y,\nY,D,D,D,C,0,D,D,D,D,D,D,D,1,D,D,C,N,Y,D,D,D,20,20,D,D,-.\n0,75,83,C,D,C,D,D,D,C,D,I,I,C,I,I,C,N,N,D,C,D,D,N,N,N,N,N,C,Y,\nY,D,D,D,C,0,D,D,D,D,D,D,C,1,D,D,C,Y,Y,D,D,D,20,20,D,D,+.\n0,74,83,C,D,C,D,D,D,D,D,I,I,C,I,I,D,N,N,D,C,D,D,N,N,N,N,N,C,Y,\nY,D,D,D,C,0,D,D,D,D,D,D,D,1,D,D,C,N,Y,D,D,D,20,20,D,D,-.\nReference Resolution\n5/30\n\nClassification Rules\n+ 786 59 IF SOON-WORDS-STR = C\n+ 73 10 IF WNCLASS = C PROPER-NOUN = D NUMBERS = C SENTNUM <= 1 PRO-\nRESOLVE = C ANIMACY = C\n+ 40 8 IF WNCLASS = C CONSTRAINTS = D PARANUM <= 0 PRO-RESOLVE = C\n+ 16 0 IF WNCLASS = C CONSTRAINTS = D SENTNUM <= 1 BOTH-IN-QUOTES = I\nAPPOSITIVE = C\n+ 17 0 IF WNCLASS = C PROPER-NOUN = D NUMBERS = C PARANUM <= 1\nBPRONOUN-1 = Y AGREEMENT = C CONSTRAINTS = C BOTH-PRONOUNS = C\n+ 38 24 IF WNCLASS = C PROPER-NOUN = D NUMBERS = C SENTNUM <= 2 BOTH-\nPRONOUNS = D AGREEMENT = C SUBJECT-2 = Y\n+ 36 8 IF WNCLASS = C PROPER-NOUN = D NUMBERS = C BOTH-PROPER-NOUNS =\nC\n+ 11 0 IF WNCLASS = C CONSTRAINTS = D SENTNUM <= 3 SUBJECT-1 = Y SUBJECT-\n2 = Y SUBCLASS = D IN-QUOTE-2 = N BOTH-DEFINITES = I\nReference Resolution\n6/30\n\nObservations\n- Feature selection plays an important role in\nclassification accuracy: MUC-6 62.6% (Soon et al.,\n2001) â†’ Ng&Cardie, 2002) 69.1%\n- Clustering operates over the results of hard\nclustering, which may negatively influence the final\nresults\n- Machine learning techniques rely on large amounts\nof annotated data: 30 texts\n- All the methods are developed on the same corpus\nof newspaper articles\nReference Resolution\n7/30\n\nToday\n- Minimizing amounts of training data:\n- Co-training\n- Weakly-supervised learning\n- Hobbs' algorithm\n- Anaphora resolution in dialogs\nReference Resolution\n8/30\n\nCo-training\n(Blum&Mitchell, 1998)\n1. Given a small amount of training data, train two\nclassifiers based on orthogonal set of features\n2. Add to training set n instances on which both\nclassifiers agree\n3. Retrain both classifiers on the extended set\n4. Return to step 2\nReference Resolution\n9/30\n\nCo-training for Coreference\nCoreference does not support natural split of features\nAlgorithm for feature splitting\n- Train a classifier on each feature separately\n- Select the best feature and assign it to the first view,\nand the second best feature assign to the second\nview\n- Iterate over the remaining feature, and add them to\none of the views\nSeparate training for each reference type (personal\npronouns, possessives,. . .)\nReference Resolution\n10/30\n\nResults\nImprovements for some types of references\n- Definite noun phrases: from 19% to 28% (2000\ntraining instances)\n- No improvements for possessives, proper names and\npossessive pronouns\nStudy of learning curves\n- Personal and possessive pronoun can be trained\nfrom very small training data (100 instances)\n- Other types of references require large amounts of\ntraining data\nReference Resolution\n11/30\n\nAnaphora In Spoken Dialogue\nDifferences between spoken and written text\n- High frequency of anaphora\n- Presence of \"Vague anaphora\"\n(Eckert&Strube'2000) 33%\nPresence of non-NP-antecedents\n-\n(Byron&Allen'1998) TRAINS93: 50%\n(Eckert&Strube'2000) SwitchBoard: 22%\n- Presence of repairs, disfluences, abandoned\nutterances and so on...\nReference Resolution\n12/30\n\nExample of Dialog\nA1: ..[he]i's nine months old . . .\nA2: ..[He]i likes to dig around a little bit.\nA3: ..[His mother]i mother comes in and says, why\ndid you let [him]i [plays in the dirt]j.\nA4: I guess [[he]i's enjoying himself]k.\nB5: [That]k's right.\nB6: [It]j's healthy . . .\nReference Resolution\n13/30\n\nAbstract Referents\n(Webber, 1988)\n(A0) Each Fall, penguins migrate to Fiji.\n(A1) That's where they wait out the winter.\n(A2) That's when it's cold even for them.\n(A3) That's why I'm going there next month.\n(A4) It happens just before the eggs hutch.\nReference Resolution\n14/30\n\nAbstract Referents\n- Webber (1990): each discourse unit produces a\npseudo discourse entity -- \"proxy for its\npropositional content\"\n- Abstract Pronoun interpretation: requires\npresentation of fact referents\n- Walker&Whittaker (1990): in problem-solving\ndialogs, people refer to aspects of the solution that\nwere not explicitly mentioned\n(Byron, 2002)\nA1 Send engine to Elmira.\nA2 That's six hours.\nReference Resolution\n15/30\n\nSymbolic Approach\nPronominal Anaphora Resolution (Byron, 2002)\n- Mentioned Entities -- referents nouns phrases\nActivated Entities -- entire sentences and nominals\n-\n- Discourse Entity attributes:\n- Input: The surface linguistic constituent\n- Type: ENGINE, PERSON, ...\n- Composition: hetero- or homogeneous\n- Specificity: individual or kind\nReference Resolution\n16/30\n\nActivated Entities\nGeneration of Multiple Proxies\n- To load the boxcars/Loading them takes an hour\n(infinitive or gerund phrase)\n- I think he that he's an alien (the entire clause)\n- I think that he's an alien (sentential)\n- If he's an alien (Subordinate clause)\nReference Resolution\n17/30\n\nTypes of Speech Acts\nTell, Request, Wh-Questions, YN-Question, Confirm\n(1) The highway is closed (Tell)\n(2) Is the highway closed? (Y/N Question)\n(3) That's right.\n(4) Why is the highway closed? (WH-Q)\n(5) *That's right.\nReference Resolution\n18/30\n\nSemantic Constraints\n\"Heavily-typed\" system\n- Verb Senses (selectional restrictions)\n\"Load them into the boxcar\" (them has to be\nCARGO)\nPredicate NPs\n-\n\"That's a good route \" (that has to be a ROUTE)\n- Predicate Adjectives\n\"It's right\" (it has to be a proposition)\nReference Resolution\n19/30\n\nExample\nEngine 1 goes to Avon to get the oranges.\n(TELL (MOVE :theme x :dest y :reason (LOAD :theme w)))\n(the x (refers-to x ENG1))\n(the y (refers-to y AVON))\n(the w (refers-to w ORANGES))\nSo it'll get there at 3 p.m.\n(ARRIVE :theme x :dest: y :time z)\n\"get there\" requires MOVABLE-OBJECT\nReference Resolution\n20/30\n\nEvaluation\n10 dialogues, 557 utterances, 180 test pronouns\nSalience-based resolution: 37%\n-\n- Adding Semantic constraints: 43%\n- Adding Abstract referents: 67%\n\"Smart\" Search order: 72%\n-\n- Domain Independent Semantics: 51%\nReference Resolution\n21/30\n\nKnowledge-Lean Approach\n(Strube&Muller'2003)\n- Switchboard: 3275 sentences, 1771 turns, 16601\nmarkables\n- Data annotated with disfluency information\n\"Problematic\" utterances were discarded\n-\n- Approach: ML combines standard features with\ndialogue specific features\nReference Resolution\n22/30\n\nFeatures\nFeatures induced for spoken dialogue: ante-exp-type [type\nof antecedent (NP, S, VP)]\nana-np-pref [preference for NP arguments]\nmdist-3mf3p [the number of NP-markables between anaphora\nand potential antecedent]\nante-tfidf [the relative importance of the expression in the\ndialogues]\naverage-ic [information content: neg. log of the total\nfrequency of the word divided by number of words ]\nReference Resolution\n23/30\n\nFeatures\nF-measure:\n- Fem&Masc Pronoun: 17.4% baseline, 17.25%\n- Third Person Neuter Pronoun: 14.68%, 19.26%\n- Third Person Plural: 28.30%, 28.70%\nReference Resolution\n24/30\n\nObservations\n- Coreference for speech processing is hard!\n- New features for dialogue are required\nProsodic featires seems to be useful\n-\nReference Resolution\n25/30\n\nHobbs' Algorithm\nTask: Pronoun resolution\n-\n- Features: Fully Syntactic\n- Accuracy: 82%\nReference Resolution\n26/30\n\nExample\nU1: Lyn's mother is a gardener. U2: Craige likes her.\nReference Resolution\n27/30\n\nAnaphora Generation\n(Reiter&Dale'1995)\n- Application: Lexical choice for generation\nFramework:\n-\nContext Set C = a1, a2, . . . , an\nProperties: pk1, pk2, . . . , pkm\n- Goal: Distinguish Referent from the Rest\nReference Resolution\n28/30\n\nAlgorithm\n- Check Success: see if the contracted description\npicks up one entity from the context\n- Choose Property: determine which properties of the\nreferent would rule out the largest number of\nentities\n- Extend Description: add the chosen properties to\nthe description being constructed and remove\nrelevant entities from the discourse.\nReference Resolution\n29/30\n\nStatistical Generation\n- (Radev,1998): classification-based\n- (Nenkova&McKeown,2003): HMM-based\nReference Resolution\n30/30"
    },
    {
      "category": "Lecture Notes",
      "title": "lec07.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/b4dc352cd1e1271637d3e81e92ebdecc_lec07.pdf",
      "content": "Generation of Referring Expression\nRegina Barzilay\nFebruary 25, 2004\n\nToday's Topics\n- Overview of Natural Language Generation\n- Psychological Evidence regarding Generation of\nReferring Expressions\n- Selection of Referring Expressions\n- Symbolic Approaches (Dale&Reiter)\n- Corpus-Based Approaches (Radev,\nNenkova&McKeown)\nGeneration of Referring Expression\n1/34\n\nWhat is NLG?\n- Program which produces texts in natural language.\n- Input: some underlying non-linguistic\nrepresentation of information.\n- Output: documents, reports, help messages and\nother types of texts.\n- Knowledge sources required: knowledge of\nlanguage and of the domain.\nGeneration of Referring Expression\n2/34\n\nWhy to Use NLG?\n- Important information is stored in ways which are\nnot comprehensible to the end users -- databases,\nexpert system, log files.\n- NLG systems can present this information to users in\nan accessible way.\n- Data presented in textual form can be searched by\nIR systems.\nGeneration of Referring Expression\n3/34\n\nExample: Summary of basketball games\nscoring((Shaquille, O'Neal), 37)\ntime(Friday, night)\nteam((Shaquille, O'Neal), (Orlando, Magic))\nwin(Orlando, Magic), (Toronto, Raptors)\nscore(101,89)\n...\nGeneration of Referring Expression\n4/34\n\nOutput\nOrlando, FL -- Shaquille O'Neal scored 37 points Friday\nnight powering the Orlando Magic to a 101 89 victory\nover the Toronto Raptors, losers of seven in a row.\nGeneration of Referring Expression\n5/34\n\nNLG System Architecture\nContent Determination\n-\n- Discourse Planning\n- Sentence Aggregation\nLexicalization\n-\n- Syntactic and morphological realization\nGeneration of Referring Expression\n6/34\n\nContent determination\n- Input: Knowledge base.\nSchemata and Inference mechanism.\n-\n- Output: predicates to be conveyed in the text.\nExample:\nGame statistics: win(Orlando, Magic), (Toronto, Raptors)\nPlayer's records: team((Shaquille, O'Neal), (Orlando, Magic))\nTeam's record: lost(7,(Toronto, Raptors))\nGeneration of Referring Expression\n7/34\n\nDiscourse Planning\n- Text have an underlying structure in which parts are\nrelated together.\n- Rhetorical relationships.\n- Conceptual grouping.\nGeneration of Referring Expression\n8/34\n\nSentence Planning\n- A one-to-one mapping from predicates to messages\nis bad.\n- Sentence planning groups information into\nsentences (aggregation).\nBefore aggregation: Shaquille O'Neal scored 37\npoints.\nThe game was on Friday night.\nOrlando Magic defeated Toronto Raptors.\nRaptors lost seven games in a row.\nAfter aggregation: Shaquille O'Neal scored 37\npoints Friday night powering the Orlando Magic\nto a 101 89 victory over the Toronto Raptors,\nlosers of seven in a row.\nGeneration of Referring Expression\n9/34\n\nLexicalization + Syntactic Realization\n- This stage determines the particular words to be\nused to express domain concepts and relations.\n- Constraints: discourse focus, style constraints,\nsyntactic environment.\n- Implementation: decision tree.\nExample: predicate:\nwin(X, Y)\nVerb: X defeated Y, Y was defeated X, X won in the game against X, X won the game\nNoun:: victory of X over Y, victory of X, defeat of Y\n\nGeneration of Referring Expression\n10/34\n\nRealization task\nInsert function words.\n-\n- Choose correct specification of content words.\nOrder words.\n-\nFUF/SURGE input for the sentence \"John likes Mary\nnow.\"\n((cat clause)\n(proc ((type mental) (tense present) (lex \"like\")))\n(partic ((processor ((cat proper) (lex \"John\")))\n(phenomenon ((cat proper) (lex \"Mary\")))))\n(circum ((time ((cat adv) (lex \"now\"))))))\nGeneration of Referring Expression\n11/34\n\nBasics\n- A noun phrase is considered to be a referring\nexpression iff its only communicative purpose is to\nidentify an object too the hearer\nContext is the set of entities that the hearer is\n-\ncurrently assumed to be attending to\n- Referring expression satisfies the referential\ncommunicative goal if it is a distinguishing\ndescription in the given context\nExample: small black dog, large white dog, small black\ncat\nGeneration of Referring Expression\n12/34\n\nImpact of Conversational Implicature\nRedundant Information causes violations of\nConversation Implicature (Grice, 1975)\nSit by the table.\nSit by the brown wood table.\nGeneration of Referring Expression\n13/34\n\nInappropriate Modifiers\nOverly specific or unexpected classifiers violates\nprinciples of Conversation Implicature\nLook at the dog.\nLook at the pitt bull.\nGeneration of Referring Expression\n14/34\n\nPsychological Data\n(Levelt, 1989)\nPeople do include unnecessary modifiers in referring\nexpressions\na white bird, a black cup and a white cup\n- Incremental Processing Helps in Understanding\n- Redundancy Helps in Understanding\nGeneration of Referring Expression\n15/34\n\nTranscript Analysis\n(Reiter&Dale,1992) (\"assembly task\" dialog)\nWhich attribute should be used?\n-\ngender vs color vs shape\n- Is it preferable to use modifier or to use a more\nspecific head noun?\nthe small dog vs the chihuahua\n- Should relative or absolute adjectives be used?\nthe small dog vs the one foot high dog\nGeneration of Referring Expression\n16/34\n\nResults of Transcript Analysis\n- Preference for adjectives that communicate size,\nshape or color?\nthe black dog vs the male dog\n- The use of specific head nouns depends on audience\nexpertise\nthe small dog vs the chihuahua\n- Preference for relative adjectives in speech, and for\nabsolute adjectives in writing\nthe small dog vs the one foot high dog\nGeneration of Referring Expression\n17/34\n\nAlgorithm: Representation\n(Reiter&Dale,1992)\n- Input is organized in (attribute, value) pairs\n- Type is one of the attributes\n- Attributes are organized in taxonomy\nObject1: (type, chihuahua), (size, small), (color, black).\nObject2: (type, chihuahua), (size, large), (color, white).\nObject3: (type, cat), (size, small), (color, black).\n\nGeneration of Referring Expression\n18/34\n\nAlgorithm\n- Check Success: see if the contracted description\npicks up one entity from the context\n- Choose Property: determine which properties of the\nreferent would rule out the largest number of\nentities\n- Extend Description: add the chosen properties to\nthe description being constructed and remove\nrelevant entities from the discourse.\nGeneration of Referring Expression\n19/34\n\nText-to-Text Generation\n- Input: text (lack of semantic information)\n- Applications: summarization, question-answering,\nmachine translation\nGeneration of Referring Expression\n20/34\n\nSupervised Approach to Referent Selection\nGoal: Select the best entity description in a given corpus\n(Radev, 1998)\nElections (1996): \"Bill Clinton, the democratic presiden-\ntial candidate\"\nFalse bomb alert in Little Rock, Ark (1997): \"Bill Clinton,\nan Arkansas native\"\nGeneration of Referring Expression\n21/34\n\nKey Idea\n- Semantic constraints imposed on lexical choice are\nreflected in contextual indicators\n- This correlation can be learned automatically from a\nlarge collections of texts, given a feature vector and\na referent\nGeneration of Referring Expression\n22/34\n\nEntity Profile\nCollection of entity descriptions (automatically\nconstructed)\nExample: Profile of Ung Huot\na senior member, Cambodia's, Cambodian foreign minister, co-\npremier, first prime minister, foreign minister, MR., new co-\npremier, new first prime minister, newly-appointed prime minis-\nter, premier\nGeneration of Referring Expression\n23/34\n\nDistributional Properties of Profile\n- 11, 504 entities from 178 MB of newswire\n- 9, 053 have a single description\n- 2, 451 very from 2 to 24 descriptions\nGeneration of Referring Expression\n24/34\n\nFeatures\n- Context -- a bag of words surrounding the entity\n- Length of the article -- an integer\n- Name of the entity -- e.g., \"Bill Clinton\"\n- Profile -- set of all the descriptions\nWordnet -- WordNet extension for Profile members\n-\nGeneration of Referring Expression\n25/34\n\nExperimental Results\n- Training: 10, 353 Testing: 1,511\n- Precision: 88.87%, Recall 63.39%\n- Steep learning curve\n(500: 64.29%, 2.86% â†’ 50,000: 88.87%, 63.39%)\n- Positive impact of WordNet extension -- 10%\nincrease on average\nGeneration of Referring Expression\n26/34\n\nRule Examples\nIF inflation IN CONTEXT, THEN \"politician\"\nIF detective IN PROFILE AND agency in CONTEXT,\nTHEN \"policeman\"\nIF celine IN CONTEXT, THEN \"north american\"\nGeneration of Referring Expression\n27/34\n\nUnsupervised Referent Selection\nStatistical Model of Syntactic Realization\n(Nenkova&McKeown, 2003)\nHypothesis: There is a regularity in lexical realization of\nreferent chain\nGeneration of Referring Expression\n28/34\n\nExample\nMany years ago, there was an Emperor, who was so excessively\nfond of new clothes, that he spent all his money in dress. He did\nnot care to go either to the theatre or the chase, except for the\nopportunities then afforded him for displaying his new clothes.\nHe had a different suit for each hour of the day; and as of any\nother king or emperor, one is accustomed to say, \"he is sitting\nin council,\" it was always said of him, \"The Emperor is sitting in\nhis wardrobe.\" One day, two rogues, calling themselves weavers,\nmade their appearance. They gave out that they knew how to\nweave stuffs of the most beautiful colors and elaborate patterns,\nthe clothes manufactured from which should have the wonder-\nful property of remaining invisible to everyone who was extraor-\ndinarily simple in character. \"These must, indeed, be splendid\nclothes!\" thought the Emperor.\nGeneration of Referring Expression\n29/34\n\nTarget NP Features\nPremodifiers:\n-\n- Titles \"President George W. Bush\"\n- Name-external modifiers \"Irish Flutist James\nGalway\"\nPostmodifiers:\n-\n- Apposition\n- Relative Clause\n- Prepositional Phrase Modificiation\nGeneration of Referring Expression\n30/34\n\nTypes of Referents\n- Is the target named entity the head of the phrase or\nnot?\n- If it is the head what kind of pre- and post- modifier\ndies it have?\nHow was the name itself realized in the NP?\n-\nGeneration of Referring Expression\n31/34\n\nHHM Construction\n- Each state of HMM corresponds to one syntactic\nrealization\n- Transitions are estimated based on corpus counts\n(anaphoric expressions are not resolved!)\nGeneration of Referring Expression\n32/34\n\nFragment of HMM\nModification\nNo Modification\nInitial\n0.76\n0.24\nModification\n0.44\n0.56\nNo modification\n0.24\n0.75\nGeneration of Referring Expression\n33/34\n\nEvaluation\nRewriting rules based on HMM improve the\nperformance of summarization system:\nPreferences: 89% rewrite, 9% original, 2% no\npreference\nGeneration of Referring Expression\n34/34"
    },
    {
      "category": "Lecture Notes",
      "title": "lec08.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/b58aed47c9624efd962fa81a63ffeb3c_lec08.pdf",
      "content": "Domain-dependent Text Structures\nRegina Barzilay\nMarch 1, 2003\n\nWhat is Text?\nA product of cohesive ties (cohesion)\nATHENS, Greece (Ap) A strong earthquake shook the\nAegean Sea island of Crete on Sunday but caused no in\njuries or damage. The quake had a preliminary magni\ntude of 5.2 and occurred at 5:28 am (0328 GMT) on the\nsea floor 70 kilometers (44 miles) south of the Cretan\nport of Chania. The Athens seismological institute said\nthe temblor's epicenter was located 380 kilometers (238\nmiles) south of the capital. No injuries or damage were\nreported.\nDomain-dependent Text Structures\n1/44\n\nWhat is Text?\nA product of structural relations (coherence)\nS1: A strong earthquake shook the Aegean Sea island of\nCrete on Sunday\nS2: but caused no injuries or damage.\nS3: The quake had a preliminary magnitude of 5.2\nDomain-dependent Text Structures\n2/44\n\nContent-based Structure\n- Describe the strength and the impact of an\nearthquake\n- Specify its magnitude\n- Specify its location\n. . .\n-\nDomain-dependent Text Structures\n3/44\n\nRhetorical Structure\nelaboration\ncontrast\nS\nS 2\nS\nDomain-dependent Text Structures\n4/44\n\nAnalogy with Syntax\nDomain-independent Theory of Sentence Structure\n- Fixed set of word categories (nouns, verbs, . . .)\n- Fixed set of relations (subject, object, . . .)\nP(\"A is sentence this weird\")\nDomain-dependent Text Structures\n5/44\n\nTwo Approaches to Text Structure\n- Domain-dependent models (Today)\n- Content-based models\n- Rhetorical models\n- Domain-independent models\n- Rhetorical Structure Theory (Next Class)\nDomain-dependent Text Structures\n6/44\n\nMotivation\nSummarization\n-\nExtract a representative subsequence from a set of\nsentences\n- Question-Answering\nFind an answer to a question in natural language\n- Text Ordering\nOrder a set of information-bearing items into a coherent\ntext\nMachine Translation\n-\nFind the best translation taking context into account\nDomain-dependent Text Structures\n7/44\n\nToday: Domain-Specific Models\nRhetorical Models:\n-\n- Argumentative Zoning of Scientific Articles\n(Teufel, 1999)\nContent-based Models:\n-\n- Supervised (Duboue&McKeown, 2001)\n- Unsupervised (Barzilay&Lee, 2004)\nDomain-dependent Text Structures\n8/44\n\nArgumentative Zoning\nMany of the recent advances in Question Answering have\nfollowed from the insight that systems can benefit from\nby exploiting the redundancy in large corpora.\nBrill et al.\n(2001) describe using the vast amount of\ndata available on the WWW to achieve impressive per\nformance . . .\nThe Web, while nearly infinite in content, is not a com\nplete repository of useful information . . .\nIn order to combat these inadequacies, we propose a\nstrategy in which in information is extracted from . . .\nDomain-dependent Text Structures\n9/44\n\nArgumentative Zoning\nBACKGROUND\nMany of the recent advances in Question Answering have followed\nfrom the insight that systems can benefit from by exploiting the\nredundancy . . .\nOTHER WORK\nBrill et al. (2001) describe using the vast amount of data available on\nthe WWW to achieve impressive performance . . .\nWEAKNESS\nThe Web, while nearly infinite in content, is not a complete repository\nof useful information . . .\nOWN CONTRIBUTION\nIn order to combat these inadequacies, we propose a strategy in which\nin information is extracted from . . .\nDomain-dependent Text Structures\n10/44\n\nMotivation\n- Scientific articles exhibit (consistent across\ndomains) similarity in structure\n- BACKGROUND\n- OWN CONTRIBUTION\n- RELATION TO OTHER WORK\n- Automatic structure analysis can benefit:\n- Q&A\n- summarization\n- citation analysis\nDomain-dependent Text Structures\n11/44\n\nApproach\n- Goal: Rhetorical segmentation with labeling\nAnnotation Scheme:\n-\n- Own work: aim, own, textual\n- Background\n- Other Work: contrast, basis, other\n- Implementation: Classification\nDomain-dependent Text Structures\n12/44\n\nExamples\nCategory\nRealization\nAim\nWe have proposed a method of clustering words\nbased on large corpus data\nTextual\nSection 2 describes three parsers which are . . .\nContrast\nHowever, no method for extracting the relation\nship from superficial linguistic expressions was\ndescribed in their paper.\nDomain-dependent Text Structures\n13/44\n\nKappa Statistics\n(Siegal&Castellan, 1998; Carletta, 1999)\nKappa controls agreement P(A) for chance agreement\nP(E)\nP(A) - p(E)\nK =\n1 - p(E)\nKappa from Argumentative Zoning:\n- Stability: 0.83\n- Reproducibility: 0.79\nDomain-dependent Text Structures\n14/44\n\nFeatures\nPosition\n-\nVerb Tense and Voice\n-\n- History\n- Lexical Features (\"other researchers claim that\")\nDomain-dependent Text Structures\n15/44\n\nResults\n- Classification accuracy is above 70%\n- Zoning improves classification\nDomain-dependent Text Structures\n16/44\n\nSupervised Content Modeling\n(Duboue& McKeown, 2001)\n- Goal: Find types of semantic information\ncharacteristic to a domain and ordering constraints\non their presentation\n- Approach: find patterns in a set of transcripts\nmanually annotated with semantic units\nDomain: Patients records\n-\nDomain-dependent Text Structures\n17/44\n\nAnnotated Transcript\nHe is 58-year-old male.\nHistory is significant for Hodgkin's disease,\nage\ngender\npmh\ntreated with . . . to his neck, back and chest.\nHyperspadias, BPH,\npmh\npmh\nhiatal hernia and proliferative lymph edema in his right arm. No IV's\npmh\npmh\nor blood pressure down in the left arm. Medications -- Inderal, Lopid,\nmed-preop\nmed-preop\nPepcid , nitroglycerine and heparin. EKG has PAC's . . . .\nmed-preop drip-preop\nmed-preop\nekg-preop\nDomain-dependent Text Structures\n18/44\n\nSemantic Sequence\nage, gender, pmh, pmh, pmh, pmh, med-preop,\nmed-preop, med-preop, drip-preop, med-preop,\nekg-preop, echo-preop, hct-preop, procedure, . . .\nDomain-dependent Text Structures\n19/44\n\nPattern Detection\nAnalogous to motif detection\nT1: A B C D F A A B F D\nT2: F C A B D D F F\n- Scanning\n- Generalizing\n- Filtering\nDomain-dependent Text Structures\n20/44\n\nExample of Learned Pattern\nintraop-problems\nintraop-problems\nâŽ§\nâŽª\nâŽª\nâŽª\nâŽ¨\nâŽª\nâŽª\nâŽª\nâŽ©\noperation\n11.11%\ndrip\n33.33%\nintraop-problems\n33.33%\ntotal-meds-anesthetics\n22.22%\nâŽ«\nâŽª\nâŽª\nâŽª\nâŽ¬\nâŽª\nâŽª\nâŽª\nâŽ­\ndrip\nDomain-dependent Text Structures\n21/44\n\nEvaluation\nPattern confidence: 84.62%\nConstraint accuracy: 89.45%\nDomain-dependent Text Structures\n22/44\n\nContent Models\n(Barzilay&Lee, 2004)\n- Content models represent topics and their ordering\nin text.\nDomain: newspaper articles on earthquake\nTopics: \"strength\", \"location\", \"casualties\", . . .\nOrder: \"casualties\" prior to \"rescue efforts\"\n- Assumption: Patterns in content organization are\nrecurrent\nDomain-dependent Text Structures\n23/44\n\nSimilarity in Domain Texts\nTOKYO (AP) A moderately strong earthquake with a preliminary magni\ntude reading of 5.1 rattled northern Japan early Wednesday, the Central\nMeteorological Agency said. There were no immediate reports of casual\nties or damage. The quake struck at 6:06 am (2106 GMT) 60 kilometers\n(36 miles) beneath the Pacific Ocean near the northern tip of the main\nisland of Honshu. . . .\nATHENS, Greece (AP) A strong earthquake shook the Aegean Sea island\nof Crete on Sunday but caused no injuries or damage. The quake had\na preliminary magnitude of 5.2 and occurred at 5:28 am (0328 GMT)\non the sea floor 70 kilometers (44 miles) south of the Cretan port of\nChania. The Athens seismological institute said the temblor's epicenter\nwas located 380 k ilometers (238 miles) south of the capital. No injuries\nor damage were reported.\nDomain-dependent Text Structures\n24/44\n\nSimilarity in Domain Texts\nTOKYO (AP) A moderately strong earthquake with a preliminary magni\ntude reading of 5.1 rattled northern Japan early Wednesday, the Central\nMeteorological Agency said. There were no immediate reports of casual\nties or damage. The quake struck at 6:06 am (2106 GMT) 60 kilometers\n(36 miles) beneath the Pacific Ocean near the northern tip of the main\nisland of Honshu. . . .\nATHENS, Greece (Ap) A strong earthquake shook the Aegean Sea island\nof Crete on Sunday but caused no injuries or damage. The quake had\na preliminary magnitude of 5.2 and occurred at 5:28 am (0328 GMT)\non the sea floor 70 kilometers (44 miles) south of the Cretan port of\nChania. The Athens seismological institute said the temblor's epicenter\nwas located 380 k ilometers (238 miles) south of the capital. . . .\nDomain-dependent Text Structures\n25/44\n\nNarrative Grammars\n- Propp (1928): fairy tales follow a \"story grammar\"\n- Barlett (1932): formulaic text structure facilities\nreader's comprehension\n- Wray (2002): texts in multiple domains exhibit\nsignificant structural similarity\nDomain-dependent Text Structures\n26/44\n\nComputing Content Model\nImplementation: Hidden Markov Model\n- States represent topics\n- State-transitions represent ordering constraints\nRescue\nStrength\nLocation\nHistory\nefforts\nCasualties\nDomain-dependent Text Structures\n27/44\n\nModel Construction\n- Initial topic induction\n- Determining states, emission and transition\nprobabilities\nViterbi re-estimation\n-\nDomain-dependent Text Structures\n28/44\n\nInitial Topic Induction\nAgglomerative clustering with cosine similarity measure\n(Iyer&Ostendorf:1996,Florian&Yarowsky:1999, Barzilay&Elhadad:2003)\nThe Athens seismological institute said the temblor's epicenter was lo\ncated 380 kilometers (238 miles) south of the capital.\nSeismologists in Pakistan's Northwest Frontier Province said the temblor's\nepicenter was about 250 kilometers (155 miles) north of the provincial\ncapital Peshawar.\nThe temblor was centered 60 kilometers (35 miles) northwest of the\nprovincial capital of Kunming, about 2,200 kilometers (1,300 miles)\nsouthwest of Beijing, a bureau seismologist said.\nDomain-dependent Text Structures\n29/44\n\nFrom Clusters to States\n- Each large cluster constitutes a state\n- Agglomerate small clusters into an \"insert\" state\nDomain-dependent Text Structures\n30/44\n\nEstimating Emission Probabilities\nState si emission probability:\nn\npsi (w0, ..., wn) =\nj=0 psi (wj wj-1)\n|\nEstimation for a \"normal\" state:\n-\ndef fci(wwâ€²) + Î´1\npsi(wâ€²|w) = fci (w) + Î´1|V , |\nEstimation for the \"insertion\" state:\n-\ndef\n1 - maxi<m psi (wâ€² w)\n.\npsm (wâ€²|w) =\nuâˆˆV (1 - maxi<m psi\n|\n(u w))\n|\nDomain-dependent Text Structures\n31/44\n\nEstimating Transition Probabilities\n3/6\n1/5\n3/4\nci, cj) + Î´2\np(sj|si) = g(\ng(ci) + Î´2m\ng(ci, cj) is a number of adjacent sentences (ci, cj)\ng(ci) is a number of sentences in ci\nDomain-dependent Text Structures\n32/44\n\nViterbi re-estimation\nGoal: incorporate ordering information\n- Decode the training data with Viterbi decoding\n- Use the new clustering as the input to the parameter\nestimation procedure\nDomain-dependent Text Structures\n33/44\n\nApplication: Information Ordering\n- Input: set of sentences\n- Applications:\n- Text summarization\n- Natural Language Generation\n- Goal: Recover most likely sequences\n\"get marry\" prior to \"give birth\" (in some domains)\nDomain-dependent Text Structures\n34/44\n\nInformation Ordering: Algorithm\nInput: set of sentences\n- Produce all permutations of the set\nRank them based on the content model\n-\nDomain-dependent Text Structures\n35/44\n\nApplication: Summarization\n- Domain-dependent summarization:\n(Radev&McKeown:1998)\n- specify types of important information\n(manually)\n- use information extraction to identify this\ninformation (automatically)\n- Domain-independent summarization: (Kupiec et\nal:1995)\n- represent a sentence using shallow features\n- use a classifier\nDomain-dependent Text Structures\n36/44\n\nSummarization: Algorithm\nInput: source text\nTraining data: parallel corpus of summaries and source\ntexts (aligned)\n- Employ Viterbi on source texts and summaries\n- Compute state likelihood to generate summary\nsentences:\np(s âˆˆ summar y s âˆˆ source) = summar y count(s)\n|\nsource count(s)\n,\n- Given a new text, decode it and extract sentences\ncorresponding to \"summary\" states\nDomain-dependent Text Structures\n37/44\n\nEvaluation: Data\nDomain\nAverage\nLength\nVocabulary\nSize\nToken/\ntype\nEarthquake\n10.4\n13.158\nClashes\n4.464\nDrugs\n10.3\n4.098\nFinance\n13.7\n12.821\nAccidents\n11.5\n5.556\nDomain-dependent Text Structures\n38/44\n\nBaselines for Ordering\n- \"Straw\" baseline: Bigram Language model\n- \"State-of-the-art\" baseline: (Lapata:2003)\n- represent a sentence using lexico-syntactic\nfeatures\n- compute pairwise ordering preferences\n- find optimally global order\nDomain-dependent Text Structures\n39/44\n\nResults: Ordering\nDomain\nAlgorithm\nPrediction\nAccuracy\nRank\nÏ„\nContent\n72%\n2.67\n0.81\nEarthquake\nLapata '03\n24%\n(N/A)\n0.48\nBigram\n4%\n485.16\n0.27\nContent\n48%\n3.05\n0.64\nClashes\nLapata '03\n27%\n(N/A)\n0.41\nBigram\n12%\n635.15\n0.25\nContent\n38%\n15.38\n0.45\nDrugs\nLapata '03\n27%\n(N/A)\n0.49\nBigram\n11%\n712.03\n0.24\nContent\n96%\n0.05\n0.98\nFinance\nLapata '03\n17%\n(N/A)\n0.44\nBigram\n66%\n7.44\n0.74\nContent\n41%\n10.96\n0.44\nAccidents\nLapata '03\n10%\n(N/A)\n0.07\nBigram\n2%\n973.75\n0.19\nDomain-dependent Text Structures\n40/44\n\nBaselines for Summarization\n- \"Straw\" baseline: n leading sentences\n- \"State-of-the-art\" Kupiec-style classifier:\n- Sentence representation: lexical features and\nlocation\n- Classifier: BoosTexter\nDomain-dependent Text Structures\n41/44\n\nOrdering: Learning Curve\nOrdering Accuracy\nearthquake\nfinance\nclashes\naccidents\ndrugs\nTraining set size\nDomain-dependent Text Structures\n42/44\n\nResults: Summarization\nSummarizer\nExtraction accuracy\nContent-based\n88%\nSentence classifier\n76%\n(words + location)\nLeading n sentences\n69%\nDomain-dependent Text Structures\n43/44\n\nSummarization: Learning Curve\nSummarization Accuracy\nhmm-based\nword+loc\nlead\nSummary/source training set size\nDomain-dependent Text Structures\n44/44"
    },
    {
      "category": "Lecture Notes",
      "title": "lec09.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/6ea9cdfb0233fbb730442d53ef6bd312_lec09.pdf",
      "content": "Domain-independent Models of Text\nStructure\nRegina Barzilay\nMarch 3, 2003\n\nDomain-Dependent Content Models\n- Capture topics and their distribution\n- Based on pattern matching techniques\n- Motifs of semantic units\n- Distributional model\n- Useful in generation and summarization\nDomain-independent Models of Text Structure\n1/16\n\nDomain-Dependent Rhetorical Model\nDomain: Scientific Articles\n- Human exhibit high agreement on the annotation\nscheme\n- The scheme covers only a small fraction of discourse\nrelations\nDomain-independent Models of Text Structure\n2/16\n\nIs it Realistic\nDomain-independent Models of Text Structure\n3/16\n\nDomain-Independent Rhetorical Model\nModel elements:\n-\n- Binary Relations\n- Compositionality Principle\n- Requirements:\n- Stability and Reproducibility of an Annotation\nScheme\n- Expressive Power of a Model\nDomain-independent Models of Text Structure\n4/16\n\nRhetorical Structure Theory\n(Mann&Thompson:1988, Matthessen&Thompson:1988)\n- Developed in the framework of natural language\ngeneration\n- Aims to describe \"building blocks\" of text structure\n- Nucleus vs Satellites\n- Binary Relations between Discourse Units\n- Compositionality principle define how to build a\ntree from binary relations\nDomain-independent Models of Text Structure\n5/16\n\nExample\n[ No matter how much one wants to stay a non-smoker, A\n], [ the truth is that the pressure to smoke in junior high is\nB\ngreater than it will be any other time of one's life.\n] . [ We\nknow that 3,000 teens start smoking each day, C] [ although\nit is a fact that 90% of them once thought that smoking was\nD\nsomething that they'll never do.\n]\nDomain-independent Models of Text Structure\n6/16\n\nBinary Relations\n- (JUSTIFICATION, A, B)\n- (JUSTIFICATION, D, B)\n- (EVIDENCE, C, B)\n- (CONCESSION, C, D)\n- (RESTATEMENT, D, A)\nDomain-independent Models of Text Structure\n7/16\n\nRST tree\nJUSTIFICATION\nA\nB\nC\nD\nJUSTIFICATION\nCONCESSION\nDomain-independent Models of Text Structure\n8/16\n\nCompositionality\nWhenever two large text spans are connected through a\nrhetorical relation, that rhetorical relation holds\nbetween the most important parts of the constituent\nspans.\nMarcu (1997): used constraint-satisfaction approach to\nbuild discourse trees given a set of binary relations\nDomain-independent Models of Text Structure\n9/16\n\nRelations\nRelation\nNucleus\nSatellite\nBackground text whose understanding\nis being facilitated\ntext whose understanding\nis being facilitated\nElaboration basic information\nadditional information\nPreparation text to be presented\ntext which prepares the\nreader to expect and in\nterpret the text to be pre\nsented\nDomain-independent Models of Text Structure\n10/16\n\nAmbiguity\nJohn can open the safe.\nHe knows the combination.\nDomain-independent Models of Text Structure\n11/16\n\nTo see this image, go to\nhttp://images.google.com/images?q=yolady.gif\n\nAutomatic Computation\n(Marcu, 1997; Marcu&Echihabi, 2002)\nSurface cues for discourse relations:\nI like vegetables, but I hate tomatoes.\nDomain-independent Models of Text Structure\n12/16\n\nAutomatic Computation of RST Relations\n(Marcu, 1997)\n- Aggregate discourse relations to a few stable\ngroups: (contrast, elaboration, condition,\ncause-explanatuin-evidence)\n- Establish deterministic correspondence between cue\nphrases and discourse relations:\n- { But, However } â†’Contrast\n- { In addition, Moreover } â†’Elaboration\nDomain-independent Models of Text Structure\n13/16\n\nAccuracy\n- Compared against manually constructed trees\n- Tested against human-constructed trees\n- Automatically constructed trees exhibit high\nsimilarity with human-constructed trees\n- However, see (Marcu&Echihabi, 2002) CONTRAST\nvs ELABORATION: only 61 from 238 have a\ndiscourse marker (26%)\nDomain-independent Models of Text Structure\n14/16\n\nOther Words Also Count!\n(Marcu&Echihabi, 2002)\nSurface cues for discourse relations:\nI like vegetables, but I hate tomatoes.\nDomain-independent Models of Text Structure\n15/16\n\nMethod\n- Assume that certain markers unambiguously predict\ndiscourse relations\n- Create Cartesian product of words located on two\nsides of a discourse marker\n- For each pair of words, compute its likelihood to\npredict a discourse relations\n- argmaxrk P(rk|(s1, s2)) =\nargmaxrk P((s1, s2)|rk) âˆ— P(rk), where\nP((s1, s2) rk) =\ni,jâˆˆs1,s2 P((wi, wj) rk)\n|\n|\nDomain-independent Models of Text Structure\n16/16"
    },
    {
      "category": "Lecture Notes",
      "title": "lec10.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/08272bf5a87aeb36868219a513e43640_lec10.pdf",
      "content": "Summarization\nRegina Barzilay\nMarch 8, 2003\n\nToday\n- Summarization (content selection, evaluation)\n- Techniques: alignment, classification, rewriting\nSummarization\n1/39\n\nWhat is Summarizer\n- Find important information in a text\n- Learn transformation rules based on training\ninstances\n- Extract certain facts from a text, and combine them\ninto a text\nSummarization\n2/39\n\nTypes of Summarization\n- Input: speech/text, single-/multi-document\n- Output: generic/query-oriented\n- Approach: domain dependent/independent,\nextraction/generation\nSummarization\n3/39\n\nKey Questions\nContent selection\n-\n- Content organization and linguistic realization\nEvaluation\n-\nSummarization\n4/39\n\nSupervised Approaches\n++\n++\n+\n-\n--\n-\n-\n--\nSummarization\n5/39\n\nSupervised Approaches\nTraining Data\nGeneration via\nAlignment\nFeature\nSelection\nClassification\nSummarization\n6/39\n\nSupervised Approaches\n- Alignment (trivial for extraction, hard for\ngeneration)\nFeature Selection\n-\n- Classification (standard classifiers -- Naive Bayes,\nSVM, maximum entropy, Boostexter)\nSummarization\n7/39\n\nFeature Selection\nShallow Features:\n- Locational Features (in the newspaper genre, the\nfirst paragraph is a summary)\n- Presence of cue words (e.g., \"in conclusion\")\n- Sentence length\n- Number of highly weighted words in a sentence\nSummarization\n8/39\n\nZipf Distribution\nThe product of the frequency of words (f) and their\nrank(r) is approximately constant: f âˆ— R = C (where C\nis around N/10)\nFreq\nRank\n1 2 3 4 5 6 7 8\nRank = order of words' frequency of occurrence\nSummarization\n9/39\n\nWord Frequency vs Resolving Power\n(from van Rijsbergen, 1979) The most frequent words\nare not the most descriptive\nfreq\nUpper cutoff\nLower Cutoff\ndiscriminative power of significant words\nsignificant words\nrank\nSummarization\n10/39\n\nAssigning Weights\n- Raw frequencies (typically with the list of\nstop-words)\n- TF*IDF - a way to deal with the problem of the Zipf\ndistribution\n- TF - Term frequency\n- IDF - Inverse term frequency\nSummarization\n11/39\n\nTF*IDF\nwik = Tfik âˆ— log(N/nk)\nwik -- Term k in document Di\nTfik -- Frequency of term k in document Di\nN -- total number of documents in the collection C\nnk -- total number of documents in the collection C\nthat contain Tk\nSummarization\n12/39\n\nFeature Selection\n\"Deep Features\"\nRhetorical structure based\n-\n- RST (Marcu, 2000)\n- Domain-dependent argumentative structure\n(Teufel&Moens, 2000)\n- Content-based (Barzilay&Lee, 2003)\nAround 10% improvement\nSummarization\n13/39\n\nAlignment\nChampollion '1822\nFind pairs\nof\ncorresponding\nele\nments\nSummarization\n14/39\n\nTo see this image, go to\nhttp://images.google.com/images?q=rosetta_stone.jpg&imgsz=large\n\nAlignment Input\nAmsterdam is the largest city in The Netherlands and the countrys economic center. It is the official capital\nof The Netherlands, though The Hague is the home of the government. Tourists come to see Amsterdams\nhistoric attractions and collections of great art. They admire the citys scenic canals, bridges, and stately old\nhouses. Amsterdam is also famous for its atmosphere of freedom and tolerance.\nCity and port, western Netherlands, located on the IJsselmeer and connected to the North Sea. It is the\ncapital and the principal commercial and financial centre of The Netherlands. To the scores of tourists who\nvisit each year, Amsterdam is known for its historical attractions, for its collections of great art, and for the\ndistinctive colour and flavour of its old sections, which have been so well preserved. However, visitors to\nthe city also see a crowded metropolis beset by environmental pollution, traffic congestion, and housing\nshortages. It is easy to describe Amsterdam, which is more than 700 years old, as a living museum of a\nbygone age and to praise the eternal beauty of the centuries-old canals, the ancient patrician houses , and\nthe atmosphere of freedom and tolerance, but the modern city is still working out solutions to the pressing\nurban problems that confront it. Amsterdam is the nominal capital of The Netherlands but not the seat of\ngovernment, which is The Hague. The royal family, for example, is only occasionally in residence at the\nRoyal Palace, on the square known as the Dam, in Amsterdam.\nSummarization\n15/39\n\nAlignment Output\nAmsterdam is the largest city in The Netherlands and the countrys economic center. It is the official capital\nof The Netherlands, though The Hague is the home of the government. Tourists come to see Amsterdams\nhistoric attractions and collections of great art. They admire the citys scenic canals, bridges, and stately old\nhouses. Amsterdam is also famous for its atmosphere of freedom and tolerance.\nCity and port, western Netherlands, located on the IJsselmeer and connected to the North Sea. It is the\ncapital and the principal commercial and financial centre of The Netherlands. To the scores of tourists who\nvisit each year, Amsterdam is known for its historical attractions, for its collections of great art, and for the\ndistinctive colour and flavour of its old sections, which have been so well preserved. However, visitors to\nthe city also see a crowded metropolis beset by environmental pollution, traffic congestion, and housing\nshortages. It is easy to describe Amsterdam, which is more than 700 years old, as a living museum of a\nbygone age and to praise the eternal beauty of the centuries-old canals, the ancient patrician houses , and\nthe atmosphere of freedom and tolerance, but the modern city is still working out solutions to the pressing\nurban problems that confront it. Amsterdam is the nominal capital of The Netherlands but not the seat of\ngovernment, which is The Hague. The royal family, for example, is only occasionally in residence at the\nRoyal Palace, on the square known as the Dam, in Amsterdam.\nSummarization\n16/39\n\nAlignment in MT\n- Alignment task: Given bitext, identify units which\nare translations of each other.\n- Units: paragraphs, sentences, phrases, words.\n- Usage: first step for full translation(Brown et al),\nlexicography(Dagan & Church, Fung & McKeown),\naid for human transaltors(Shemtov), multi-lingual\nIR.\nSummarization\n17/39\n\nLength-based Alignment\n- Matching Predicate: Long sentences will be\ntranslated as long sentences, short sentences\ntranslated as short sentences\n- Method: Dynamic programming\nSummarization\n18/39\n\nLength-based Alignment\nLet D(i, j) be the lowest cost alignment between\nsentences s1, . . . , si and t1, . . . , tj.\nBase: D(0, 0) = 0.\nâŽ§\nâŽªD(i, j - 1) + cost(0:1 align Ï†, tj)\nâŽª\nâŽª\nâŽª\nâŽª\nâŽªD(i - 1, j) + cost(1:0 align si, Ï†)\nâŽª\nâŽª\nâŽª\nâŽ¨D(i - 1, j - 1) + cost(1:1 align si, tj)\nD(i, j) = min âŽª\nâŽª D(i - 1, j - 2) + cost(1:2 align si, tj-1, tj)\nâŽª\nâŽª\nâŽª\nâŽª\nâŽª D(i - 2, j - 1) + cost(2:1 align si-1, si, tj)\nâŽª\nâŽª\nâŽ© D(i - 2, j - 2) + cost(2:2 align si-1, si, tj-1, tj)\nSummarization\n19/39\n\nDesign Choices in Alignment\nDetermined by a Corpus Type\n- Matching predicate\n- Search strategy\nSummarization\n20/39\n\nCorpus Type\n- Language Proximity (Monolingual vs Bilingual,\ntechnical vs lay)\n- Content Proximity (comparable vs parallel)\n- Matching Granularity (1:1 vs 1:5)\nSummarization\n21/39\n\nMatching Predicate\n- Length similarity. (Gale & Church, Brown et al)\n- Lexical similarity:\n- Bilingual dictionary (Wu)\n- Words with the same distribution. (Kay &\nRoscheisen, Fung & McKeown)\n- Cognates (Simard et al, Church, Melamed)\nSummarization\n22/39\n\nMethods for Overall Alignment\n- Dynamic programming\n- Methods based on Computational Geometry\n- Signal processing Methods\nSummarization\n23/39\n\nComputational Geometry Methods\n(Melamed, 1997) Assumption: Distribution of \"true points of\ncorrespondence (TPC)\" satisfies certain geometric properties\n- Generate all the matching points satisfying the matching\npredicate (over-generation)\n- Find a subset of matching points that satisfies a pattern of\nTPC:\n- Linearity\n- Injectivity\n- Low variance of slope\nVarious heuristics are used to minimize the search space\nSummarization\n24/39\n\nComputational Geometry Methods\ndiscovered TPC\nundiscovered TPC\nnoise\nmain\ndiagonal\nnext\nTPC chain\nsearch\nfrontier\nsearch\nfrontier\nsearch\nrectangle\nprevious chain\nSummarization\n25/39\n\nSignal Processing Methods\n(Fung, 1995)\nSummarization\n26/39\n\nAlignment for Summarization\n- Always monolingual\n- Seems to be trivial (use word intersection!)\nSummarization\n27/39\n\nIt is hard!\n- Insertions, deletions, reodering\n- Weak similarity function\nSummarization\n28/39\n\nWeak Similarity Function\nPetersburg served as the capital of Russia for 200\nÂ·\nyears.\n(A)\nFor two centuries Petersburg was the capital of the\nÂ·\nRussian Empire.\nThe city is also the country's leading port and center\nÂ·\nof commerce.\n(B)\nAnd yet, as with so much of the city, the port facili-\nÂ·\nties are old and inefficient.\nSummarization\n29/39\n\nPatterns of Mapping\nSummarization\n30/39\n\nDomain-Dependent Structure-Based\nAlignment\n(Barzilay&Elhadad, 2003) Assumption: Weak similarity\nfunction augmented with structural information\nContent Structure Induction\n-\n- Learning of Structural Mapping Rules\n- Macro Alignment\n- Micro Alignment\nSummarization\n31/39\n\nContent Structure Induction\nAutomatically induced topic labeling via clustering\nLisbon has a mild and equable climate, with a mean annual temperature of 63 degree F (17 degree C). The\nproximity of the Atlantic and the frequency of sea fogs keep the atmosphere humid, and summers can be\nsomewhat oppressive, although the city has been esteemed as a winter health resort since the 18th century.\nAverage annual rainfall is 26.6 inches (666 millimetres).\nJakarta is a tropical, humid city, with annual temperatures ranging between the extremes of 75 and 93 degree\nF (24 and 34 degree C) and a relative humidity between 75 and 85 percent. The average mean temperatures\nare 79 degree F (26 degree C) in January and 82 degree F (28 degree C) in October. The annual rainfall is\nmore than 67 inches (1,700 mm). Temperatures are often modified by sea winds. Jakarta, like any other large\ncity, also has its share of air and noise pollution.\nSummarization\n32/39\n\nLearning of Structural Mapping Rules\nj2\nj1\nCorpus2\nCorpus1\ni1\ni2\nPar.2\nPar.1\nPar.3\nPar.1\nPar.2\nPar.1\nText\nText\nText\nText\nClusterB\nClusterE\nCluster1\nSummarization\n33/39\n\nLearning of Structural Mapping Rules\nClassification on cluster level\nFeatures: words, cluster type\nj2\nj1\nCorpus2\nCorpus1\ni1\ni2\nPar.2\nPar.1\nPar.3\nPar.1\nPar.2\nPar.1\nText\nText\nText\nText\nClusterB\nClusterE\nCluster1\nSummarization\n34/39\n\nMacro-Alignment\nFor unseens pair of texts applied a trained classifier to\ngenerate possible mappings\nText 1\nText 2\nPar. 2\nCluster B\nPar. 2\nPar. 7\nCluster E\nCluster 3\nPar. 13\nCluster G\nSummarization\n35/39\n\nMicro-Alignment\nâŽ§\nâŽªs(i, j-1) - skip penalty\nâŽª\nâŽª\nâŽª\nâŽª\nâŽªs(i-1, j) - skip penalty\nâŽª\nâŽª\nâŽª\nâŽ¨ s(i-1, j-1) + sim(i, j)\ns(i, j) = max âŽª\nâŽª s(i-1, j-2) + sim(i, j) + sim(i, j-1)\nâŽª\nâŽª\nâŽª\nâŽª\nâŽª s(i-2, j-1) + sim(i, j) + sim(i-1, j)\nâŽª\nâŽª\nâŽ© s(i-2, j-2) + sim(i, j-1) + sim(i-1, j)\nSummarization\n36/39\n\nEvaluation\nRange\nStruct\nCos.\nPrec.\nRec.\nPrec.\nRec.\n0%-40%\n50%\n25%\n23%\n15%\n40%-70%\n85%\n73%\n66%\n86%\n70%-100%\n95%\n95%\n90%\n95%\nSummarization\n37/39\n\nSummarization Evaluation\n- Precision/Recall or their weighted version are used\n- As a baseline, people use a \"lead\" summary\n- Human agreement is computed using Kappa\n- When evaluation results matter, it is done manually\n(DUC competition)\n- Provides large collection of human-generated\nsummaries\n- Outputs are evaluated manually\nSummarization\n38/39\n\nSemantic-based Summarization\nAssumption: In a limited domain, we know \"what is\nimportant\" (Radev&McKeown, 1995,\nElhadad&McKeown, 2001)\n- Use an information extraction system to select\n\"important information\"\n- Use a semantics-to-text generation system to\ngenerate a new text\nSummarization\n39/39"
    },
    {
      "category": "Resource",
      "title": "portable.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/acfc2e5de11ed8f737c8b7076ca2ac23_portable.pdf",
      "content": "A\nP\nortable\nAlgorithm\nfor\nMapping\nBitext\nCorresp\nondence\nI\nDan\nMelamed\nDept\nof\nComputer\nand\nInformation\nScience\nUniv\nersit\ny\nof\nP\nennsylv\nania\nPhiladelphia\nP\nA\n\nUSA\nmelamedunagicisupenne\ndu\nAbstract\nThe\nrst\nstep\nin\nmost\nempirical\nw\nork\nin\nm\nultilingual\nNLP\nis\nto\nconstruct\nmaps\nof\nthe\ncorresp\nondence\nb\net\nw\neen\ntexts\nand\ntheir\ntranslations\nbitext\nmaps\nThe\nSmo\noth\nInjectiv\ne\nMap\nRecognizer\nSIMR\nalgo\nrithm\npresen\nted\nhere\nis\na\ngeneric\npattern\nrecognition\nalgorithm\nthat\nis\nparticularly\nw\nellsuited\nto\nmapping\nbitext\ncorresp\non\ndence\nSIMR\nis\nfaster\nand\nsignican\ntly\nmore\naccurate\nthan\nother\nalgorithms\nin\nthe\nliterature\nThe\nalgorithm\nis\nrobust\nenough\nto\nuse\non\nnoisy\ntexts\nsuc\nh\nas\nthose\nresult\ning\nfrom\nOCR\ninput\nand\non\ntranslations\nthat\nare\nnot\nv\nery\nliteral\nSIMR\nencap\nsulates\nits\nlanguagesp\necic\nheuristics\nso\nthat\nit\ncan\nb\ne\np\norted\nto\nan\ny\nlanguage\npair\nwith\na\nminimal\neort\n\nIn\ntro\nduction\nT\nexts\nthat\nare\na\nv\nailable\nin\nt\nw\no\nlanguages\nbitexts\nare\nimmensely\nv\naluable\nfor\nman\ny\nnatural\nlanguage\npro\ncessing\napplications\n\nBitexts\nare\nthe\nra\nw\nma\nterial\nfrom\nwhic\nh\ntranslation\nmo\ndels\nare\nbuilt\nIn\naddition\nto\ntheir\nuse\nin\nmac\nhine\ntranslation\nSato\n\nNagao\n\nBro\nwn\net\nal\n\nMelamed\n\ntranslation\nmo\ndels\ncan\nb\ne\napplied\nto\nmac\nhine\nassisted\ntranslation\nSato\n\nF\noster\net\nal\n\ncrosslingual\ninformation\nretriev\nal\nSIGIR\n\nand\ngisting\nof\nW\norld\nWide\nW\neb\npages\nResnik\n\nBitexts\nalso\npla\ny\na\nrole\nin\nless\nauto\nmated\napplications\nsuc\nh\nas\nconcordancing\nfor\nbilin\ngual\nlexicograph\ny\nCatizone\net\nal\n\nGale\n\nCh\nurc\nh\nb\ncomputerassisted\nlanguage\nlearn\ning\nand\nto\nols\nfor\ntranslators\neg\nMac\nklo\nvitc\nh\nMultitexts\nin\nmore\nthan\nt\nw\no\nlanguages\nare\nev\nen\nmore\nv\naluable\nbut\nthey\nare\nm\nuc\nh\nmore\nrare\n\nMelamed\nb\nHo\nw\nev\ner\nbitexts\nare\nof\nlit\ntle\nuse\nwithout\nan\nautomatic\nmetho\nd\nfor\nconstruct\ning\nbitext\nmaps\nBitext\nmaps\niden\ntify\ncorresp\nonding\ntext\nunits\nb\ne\nt\nw\neen\nthe\nt\nw\no\nhalv\nes\nof\na\nbitext\nThe\nideal\nbitext\nmapping\nalgorithm\nshould\nb\ne\nfast\nand\naccurate\nuse\nlittle\nmemory\nand\ndegrade\ngracefully\nwhen\nfaced\nwith\ntranslation\nirregularities\nlik\ne\nomissions\nand\nin\nv\nersions\nIt\nshould\nb\ne\napplicable\nto\nan\ny\ntext\ngenre\nin\nan\ny\npair\nof\nlanguages\nThe\nSmo\noth\nInjectiv\ne\nMap\nRecognizer\nSIMR\nal\ngorithm\npresen\nted\nin\nthis\npap\ner\nis\na\nbitext\nmapping\nalgorithm\nthat\nadv\nances\nthe\nstate\nof\nthe\nart\non\nthese\ncriteria\nThe\nev\naluation\nin\nSection\n\nsho\nws\nthat\nSIMRs\nerror\nrates\nare\nlo\nw\ner\nthan\nthose\nof\nother\nbitext\nmapping\nalgorithms\nb\ny\nan\norder\nof\nmagni\ntude\nA\nt\nthe\nsame\ntime\nits\nexp\nected\nrunning\ntime\nand\nmemory\nrequiremen\nts\nare\nlinear\nin\nthe\nsize\nof\nthe\ninput\nb\netter\nthan\nan\ny\nother\npublished\nalgorithm\nThe\npap\ner\nb\negins\nb\ny\nl a\nying\ndo\nwn\nSIMRs\ngeomet\nric\nfoundations\nand\ndescribing\nthe\nalgorithm\nThen\nSection\n\nexplains\nho\nw\nto\np\nort\nSIMR\nto\narbitrary\nlanguage\npairs\nwith\nminimal\neort\nwithout\nrely\ning\non\ngenresp\necic\ninformation\nsuc\nh\nas\nsen\ntence\nb\noundaries\nThe\nlast\nsection\noers\nsome\ninsigh\nts\nab\nout\nthe\noptimal\nlev\nel\nof\ntext\nanalysis\nfor\nmapping\nbitext\ncorresp\nondence\n\nBitext\nGeometry\nA\nbitext\nHarris\n\ncomprises\nt\nw\no\nv\nersions\nof\na\ntext\nsuc\nh\nas\na\ntext\nin\nt\nw\no\ndieren\nt\nlanguages\nT\nranslators\ncreate\na\nbitext\neac\nh\ntime\nthey\ntrans\nlate\na\ntext\nEac\nh\nbitext\ndenes\na\nrectangular\nbitext\nspace\nas\nillustrated\nin\nFigure\n\nThe\nwidth\nand\nheigh\nt\nof\nthe\nrectangle\nare\nthe\nlengths\nof\nthe\nt\nw\no\ncomp\nonen\nt\ntexts\nin\nc\nharacters\nThe\nlo\nw\ner\nleft\ncorner\nof\nthe\nrectangle\nis\nthe\norigin\nof\nthe\nbitext\nspace\nand\nrepresen\nts\nthe\nt\nw\no\ntexts\nb\neginnings\nThe\nupp\ner\nrigh\nt\ncorner\nis\nthe\ntermin\nus\nand\nrepresen\nts\nthe\ntexts\nends\nThe\nline\nb\net\nw\neen\nthe\norigin\nand\nthe\n\nterminus\nmain\ndiagonal\ny = character position in text 2\norigin\nx = character position in text 1\nFigure\n\na\nbitext\nsp\nac\ne\ntermin\nus\nis\nthe\nmain\ndiagonal\nThe\nslop\ne\nof\nthe\nmain\ndiagonal\nis\nthe\nbitext\nslop\ne\nEac\nh\nbitext\nspace\ncon\ntains\na\nn\num\nb\ne r\nof\ntrue\np\no i n\nts\nof\ncorresp\nondence\nTPCs\nother\nthan\nthe\norigin\nand\nthe\ntermin\nus\nF\nor\nexample\nif\na\ntok\nen\nat\np\nosition\np\non\nthe\nxaxis\nand\na\ntok\nen\nat\np\nosition\nq\non\nthe\nyaxis\nare\ntranslations\nof\neac\nh\nother\nthen\nthe\nco\nordinate\np\nq\n\nin\nthe\nbitext\nspace\nis\na\nTPC\n\nTPCs\nalso\nexist\nat\ncorresp\nonding\nb\noundaries\nof\ntext\nunits\nsuc\nh\nas\nsen\ntences\nparagraphs\nand\nc\nhapters\nGroups\nof\nTPCs\nwith\na\nroughly\nlinear\narrangemen\nt\nin\nthe\nbitext\nspace\nare\ncalled\nc\nhains\nBitext\nmaps\nare\nto\nfunctions\nin\nbitext\nspaces\nA\ncomplete\nset\nof\nTPCs\nfor\na\nparticular\nbitext\nis\ncalled\na\ntrue\nbitext\nmap\nTBM\nThe\npurp\nose\nof\na\nbitext\nmapping\nalgorithm\nis\nto\npro\nduce\nbitext\nmaps\nthat\nare\nthe\nb\nest\np\nossible\nappro\nx\nimations\nof\neac\nh\nbitexts\nTBM\n\nSIMR\nSIMR\nbuilds\nbitext\nmaps\none\nc\nhain\nat\na\ntime\nThe\nsearc\nh\nfor\neac\nh\nc\nhain\nalternates\nb\ne t\nw\neen\na\ngenera\ntion\nphase\nand\na\nrecognition\nphase\nThe\ngenera\ntion\nphase\nb\negins\nin\na\nsmall\nrectangular\nregion\nof\nthe\nbitext\nspace\nwhose\ndiagonal\nis\nparallel\nto\nthe\nmain\ndiagonal\nWithin\nthis\nsearc\nh\nrectangle\nSIMR\ngenerates\nall\nthe\np\noin\nts\nof\ncorresp\nondence\nthat\nsat\nisfy\nthe\nsupplied\nmatc\nhing\npredicate\nas\nexplained\nin\nSection\n\nIn\nthe\nrecognition\nphase\nSIMR\ncalls\nthe\nc\nhain\nrecognition\nheuristic\nto\nnd\nsuitable\nc\nhains\namong\nthe\ngenerated\np\no i n\nts\nIf\nno\nsuitable\nc\nhains\nare\nfound\nthe\nsearc\nh\nrectangle\nis\nprop\nortion\nally\nexpanded\nand\nthe\ngenerationrecognition\ncycle\nSince\ndistances\nin\nthe\nbitext\nspace\nare\nmeasured\nin\nc\nharacters\nthe\np\nosition\nof\na\ntok\nen\nis\ndened\nas\nthe\nmean\np\nosition\nof\nits\nc\nharacters\nis\nrep\neated\nThe\nrectangle\nk\neeps\nexpanding\nun\ntil\nat\nleast\none\nacceptable\nc\nhain\nis\nfound\nIf\nmore\nthan\none\nc\nhain\nis\nfound\nin\nthe\nsame\ncycle\nSIMR\naccepts\nthe\none\nwhose\np\noin\nts\nare\nleast\ndisp\nersed\naround\nits\nleastsquares\nline\nEac\nh\ntime\nSIMR\naccepts\na\nc\nhain\nit\nselects\nanother\nregion\nof\nthe\nbitext\nspace\nto\nsearc\nh\nfor\nthe\nnext\nc\nhain\nSIMR\nemplo\nys\na\nsimple\nheuristic\nto\nselect\nregions\nof\nthe\nbitext\nspace\nto\nsearc\nh\nT\no\na\nrst\nappro\nxima\ntion\nTBMs\nare\nmonotonically\nincreasing\nfunctions\nThis\nmeans\nthat\nif\nSIMR\nnds\none\nc\nhain\nit\nshould\nlo\nok\nfor\nothers\neither\nab\no\nv\ne\nand\nto\nthe\nrigh\nt\nor\nb\nelo\nw\nand\nto\nthe\nleft\nof\nthe\none\nit\nhas\njust\nfound\nAll\nSIMR\nneeds\nis\na\nplace\nto\nstart\nthe\ntrace\nA\ngo\no\nd\nplace\nto\nstart\nis\nat\nthe\nb\neginning\nSince\nthe\norigin\nof\nthe\nbitext\nspace\nis\nalw\na\nys\na\nTPC\nthe\nrst\nsearc\nh\nrect\nangle\nis\nanc\nhored\nat\nthe\norigin\nSubsequen\nt\nsearc\nh\nrectangles\nare\nanc\nhored\nat\nthe\ntop\nrigh\nt\ncorner\nof\nthe\npreviously\nfound\nc\nhain\nas\nsho\nwn\nin\nFigure\n\nfrontier\nsearch\ndiagonal\nmain\nfrontier\nsearch\nnext\nTPC chain\nsearch\nrectangle\ndiscovered TPC\nundiscovered TPC\nnoise\nprevious chain\nFigure\n\nSIMRs\nexp\nanding\nr\ne\nctangle\nse\nar\nch\nstr\nate\ngy\nThe\nse\nar\nch\nr\ne\nctangle\nis\nanchor\ne\nd\na t\nt h e\nt o p\nright\nc\norner\nof\nthe\npr\neviously\nfound\nchain\nIts\ndiag\nonal\nr\nemains\np\nar\nal\nlel\nto\nthe\nmain\ndiagonal\nThe\nexpandingrectangle\nsearc\nh\nstrategy\nmak\nes\nSIMR\nrobust\nin\nthe\nface\nof\nTBM\ndiscontin\nuities\nFigure\n\nsho\nws\na\nsegmen\nt\nof\nthe\nTBM\nthat\ncon\ntains\na\nv\nertical\ngap\nan\nomission\nin\nthe\ntext\non\nthe\nxaxis\nAs\nthe\nsearc\nh\nrectangle\ngro\nws\nit\nwill\nev\nen\ntually\nin\ntersect\nwith\nthe\nTBM\nev\nen\nif\nthe\ndiscontin\nuit\ny\nis\nquite\nlarge\nMelamed\nb\nThe\nnoise\nlter\nde\nscrib\ned\nin\nSection\n\nprev\nen\nts\nSIMR\nfrom\nb\neing\nled\nastra\ny\nb\ny\nfalse\np\noin\nts\nof\ncorresp\nondence\n\nP\noin\nt\nGeneration\nSIMR\ngenerates\ncandidate\np\noin\nts\nof\ncorresp\nondence\nin\nthe\nsearc\nh\nrectangle\nusing\none\nof\nits\nmatc\nhing\npredicates\nA\nmatc\nhing\npredicate\nis\na\nheuristic\nfor\ndeciding\nwhether\na\ngiv\nen\npair\nof\ntok\nens\nare\nlik\nely\nto\nb\ne\nm\nutual\ntranslations\nTw\no\nkinds\nof\ninformation\n\nthat\na\nmatc\nhing\npredicate\ncan\nrely\non\nmost\noften\nare\ncognates\nand\ntranslation\nlexicons\nTw\no\nto k\nens\nin\na\nbitext\nare\ncognates\nif\nthey\nha\nv\ne\nthe\nsame\nmeaning\nand\nsimilar\nsp\nellings\nIn\nthe\nnon\ntec\nhnical\nCanadian\nHansards\nparliamen\ntary\ndebate\ntranscripts\na\nv\nailable\nin\nEnglish\nand\nin\nF\nrenc\nh\ncog\nnates\ncan\nb\ne\nfound\nfor\nroughly\none\nquarter\nof\nall\ntext\ntok\nens\nMelamed\n\nEv\nen\ndistan\ntly\nrelated\nlanguages\nlik\ne\nEnglish\nand\nCzec\nh\nwill\nshare\na\nlarge\nn\num\nb\ne r\nof\ncognates\nin\nthe\nform\nof\nprop\ner\nnouns\nCognates\nare\nmore\ncommon\nin\nbitexts\nfrom\nmore\nsimilar\nlanguage\npairs\nand\nfrom\ntext\ngenres\nwhere\nmore\nw\nord\nb\norro\nwing\no\nccurs\nsuc\nh\nas\ntec\nhnical\ntexts\nWhen\ndealing\nwith\nlanguage\npairs\nthat\nha\nv\ne\ndissim\nilar\nalphab\nets\nthe\nmatc\nhing\npredicate\ncan\nemplo\ny\nphonetic\ncognates\nMelamed\na\nWhen\none\nor\nb\noth\nof\nthe\nlanguages\nin\nv\nolv\ned\nis\nwritten\nin\npic\ntographs\ncognates\ncan\nstill\nb\ne\nfound\namong\npunc\ntuation\nand\ndigit\nstrings\nHo\nw\nev\ner\ncognates\nof\nthis\nlast\nkind\nare\nusually\nto\no\nsparse\nto\nsuce\nb\ny\nthem\nselv\nes\nWhen\nthe\nmatc\nhing\npredicate\ncannot\ngenerate\nenough\ncandidate\ncorresp\nondence\np\no i n\nts\nbased\non\ncognates\nits\nsignal\ncan\nb\ne\nstrengthened\nb\ny\na\ntrans\nlation\nlexicon\nT\nranslation\nlexicons\ncan\nb\ne\nex\ntracted\nfrom\nmac\nhinereadable\nbilingual\ndictionaries\nMRBDs\nin\nthe\nrare\ncases\nwhere\nMRBDs\nare\na\nv\nail\nable\nIn\nother\ncases\nthey\ncan\nb\ne\nconstructed\nauto\nmatically\nor\nsemiautomatically\nusing\nan\ny\nof\nsev\neral\nmetho\nds\nF\nung\n\nMelamed\nc\nResnik\n\nMelamed\n\nSince\nthe\nmatc\nhing\npredicate\nneed\nnot\nb\ne\np\nerfectly\naccurate\nthe\ntranslation\nlexicons\nneed\nnot\nb\ne\neither\nMatc\nhing\npredicates\ncan\ntak\ne\na d v\nan\ntage\nof\nother\ninformation\nb\nesides\ncognates\nand\ntranslation\nlex\nicons\nF\nor\nexample\na\nlist\nof\nfaux\namis\nis\na\nuse\nful\ncomplemen\nt\nto\na\ncognate\nmatc\nhing\nstrategy\nMac\nklo\nvitc\nh\n\nA\nstop\nlist\nof\nfunction\nw\nords\nis\nalso\nhelpful\nF\nunction\nw\nords\nare\ntranslated\ninconsis\nten\ntly\nand\nmak\ne\nunreliable\np\noin\nts\nof\ncorresp\nondence\nMelamed\na\n\nP\noin\nt\nSelection\nAs\nillustrated\nin\nFigure\n\nev\nen\nshort\nsequences\nof\nTPCs\nform\nc\nharacteristic\npatterns\nMost\nc\nhains\nof\nTPCs\nha\nv\ne\nthe\nfollo\nwing\nprop\nerties\n\nLinearit\ny\nTPCs\ntend\nto\nline\nup\nstraigh\nt\n\nLo\nw\nV\nariance\nof\nSlop\ne\nThe\nslop\ne\nof\na\nTPC\nc\nhain\nis\nrarely\nm\nuc\nh\ndieren\nt\nfrom\nthe\nbitext\nslop\ne\n\nInjectivit\ny\nNo\nt\nw\no\np\noin\nts\nin\na\nc\nhain\nof\nTPCs\ncan\nha\nv\ne\nthe\nsame\nx\nor\nycoordinates\nSIMRs\nc\nhain\nrecognition\nheuristic\nexploits\nthese\nprop\nerties\nto\ndecide\nwhic\nh\nc\nhains\nin\nthe\nsearc\nh\nrect\nangle\nmigh\nt\nb\ne\nTPC\nc\nhains\nThe\nheuristic\nin\nv\nolv\nes\nthree\nparameters\nc\nhain\nsize\nmaxim\num\np\noin\nt\ndisp\nersal\nand\nmaxim\num\nangle\ndeviation\nA\nc\nhains\nsize\nis\nsimply\nthe\nn\num\nb\ne r\nof\np\no i n\nts\nit\ncon\ntains\nThe\nheuristic\nconsiders\nonly\nc\nhains\nof\nexactly\nthe\nsp\necied\nsize\nwhose\np\noin\nts\nare\ninjectiv\ne\nThe\nlinearit\ny\nof\nthe\nthese\nc\nhains\nis\ntested\nb\ny\nmeasuring\nthe\nro\not\nmean\nsquared\ndistance\nof\nthe\nc\nhains\np\noin\nts\nfrom\nthe\nc\nhains\nleastsquares\nline\nIf\nthis\ndistance\nexceeds\nthe\nmaxim\num\np\no i n\nt\ndisp\nersal\nthreshold\nthe\nc\nhain\nis\nrejected\nNext\nthe\nangle\nof\neac\nh\nc\nhains\nleastsquares\nline\nis\ncompared\nto\nthe\narctangen\nt\no f\nthe\nbitext\nslop\ne\nIf\nthe\ndier\nence\nexceeds\nthe\nmaxim\num\nangle\ndeviation\nthresh\nold\nthe\nc\nhain\nis\nrejected\nThese\nlters\ncan\nb\ne\ne\ncien\ntly\ncom\nbined\nso\nthat\nSIMRs\nexp\nected\nrunning\ntime\nand\nmemory\nrequiremen\nts\nare\nlinear\nin\nthe\nsize\nof\nthe\ninput\nbitext\nMelamed\na\nThe\nc\nhain\nrecognition\nheuristic\npa\nys\nno\natten\ntion\nto\nwhether\nc\nhains\nare\nmonotonic\nNonmonotonic\nTPC\nc\nhains\nare\nquite\ncommon\nb\necause\nev\nen\nlan\nguages\nwith\nsimilar\nsyn\ntax\nlik\ne\nF\nrenc\nh\nand\nEnglish\nha\nv\ne\nw\nellkno\nwn\ndierences\nin\nw\nord\norder\nF\nor\nex\nample\nEnglish\nadjectiv\ne\nnoun\npairs\nusually\ncorre\nsp\nond\nto\nF\nrenc\nh\nnoun\nadjectiv\ne\npairs\nSuc\nh\ni n\nv\ner\nsions\nresult\nin\nTPCs\narranged\nlik\ne\nthe\nmiddle\nt\nw\no\np\noin\nts\nin\nthe\nprevious\nc\nhain\nof\nFigure\n\nSIMR\nhas\nno\nproblem\naccepting\nthe\nin\nv\nerted\np\noin\nts\nIf\nthe\norder\nof\nw\nords\nin\na\ncertain\ntext\npassage\nis\nradically\naltered\nduring\ntranslation\nSIMR\nwill\nsim\nply\nignore\nthe\nw\nords\nthat\nmo\nv\ne\nt o\no\nm\nuc\nh\nand\ncon\nstruct\nc\nhains\nout\nof\nthose\nthat\nremain\nmore\nstation\nary\n\nThe\nmaxim\num\np\noin\nt\ndisp\nersal\nparameter\nlim\nits\nthe\nwidth\nof\naccepted\nc\nhains\nbut\nnothing\nlim\nits\ntheir\nlength\nIn\npractice\nthe\nc\nhain\nrecognition\nheuristic\noften\naccepts\nc\nhains\nthat\nspan\nsev\neral\nsen\ntences\nThe\nabilit\ny\nto\nanalyze\nnonmonotonic\np\noin\nts\nof\ncorresp\nondence\no\nv\ner\nv\nariablesize\nareas\nof\nbitext\nspace\nmak\nes\nSIMR\nrobust\nenough\nto\nuse\non\ntransla\ntions\nthat\nare\nnot\nv\nery\nliteral\n\nNoise\nFilter\nP\noin\nts\nof\ncorresp\nondence\namong\nfrequen\nt\ntok\nen\nt\nyp\nes\noften\nline\nup\nin\nro\nws\nand\ncolumns\nas\nillus\ntrated\nin\nFigure\n\nT\nok\nen\nt\nyp\nes\nlik\ne\nthe\nEnglish\narticle\na\ncan\npro\nduce\none\nor\nmore\ncorresp\nondence\np\noin\nts\nfor\nalmost\nev\nery\nsen\ntence\nin\nthe\nopp\nosite\ntext\nOnly\none\np\noin\nt\nof\ncorresp\nondence\nin\neac\nh\nro\nw\nand\ncolumn\ncan\nb\ne\ncorrect\nthe\nrest\nare\nnoise\nA\nnoise\nl\nter\ncan\nmak\ne\nit\neasier\nfor\nSIMR\nto\nnd\nTPC\nc\nhains\nOther\nbitext\nmapping\nalgorithms\nmitigate\nthis\nsource\nof\nnoise\neither\nb\ny\nassigning\nlo\nw\ner\nw\neigh\nts\nto\n\na\na\na\na\na\na\na\na\na\na\na\nFrench text\nFigure\n\nF\nr\ne\nquent\ntokens\nc\nause\nfalse\np\noints\nof\nc\nor\nr\nesp\nondenc\ne\nthat\nline\nup\nin\nr\nows\nand\nc\nolumns\ncorresp\nondence\np\no i n\nts\nasso\nciated\nwith\nfrequen\nt\nto\nk\nen\nt\nyp\nes\nCh\nurc\nh\n\nor\nb\ny\ndeleting\nfrequen\nt\nto\nk\nen\nt\nyp\nes\nfrom\nthe\nbitext\naltogether\nDagan\net\nal\n\nHo\nw\nev\ner\na\ntok\nen\nt\nyp\ne\nthat\nis\nrelativ\nely\nfre\nquen\nt\no\nv\nerall\ncan\nb\ne\nrare\nin\nsome\nparts\nof\nthe\ntext\nIn\nthose\nparts\nthe\ntok\nen\nt\nyp\ne\ncan\npro\nvide\nv\naluable\nclues\nto\ncorresp\nondence\nOn\nthe\nother\nhand\nman\ny\ntok\nens\nof\na\nrelativ\nely\nrare\nt\nyp\ne\ncan\nb\ne\nconcen\ntrated\nin\na\nshort\nsegmen\nt\nof\nthe\ntext\nresulting\nin\nman\ny\nfalse\ncorresp\nondence\np\noin\nts\nThe\nv\narying\nconcen\ntra\ntion\nof\niden\ntical\ntok\nens\nsuggests\nthat\nmore\nlo\ncalized\nnoise\nlters\nw\nould\nb\ne\nmore\neectiv\ne\nSIMRs\nlo\ncal\nized\nsearc\nh\nstrategy\npro\nvides\na\nv\nehicle\nfor\na\nlo\ncalized\nnoise\nlter\nThe\nlter\nis\nbased\non\nthe\nmaxim\num\np\noin\nt\nam\nbiguit\ny\nlev\nel\nparameter\nF\nor\neac\nh\np\noin\nt\np\n\nx\ny\n\nlet\nX\nb\ne\nthe\nn\num\nb\ne r\nof\np\noin\nts\nin\ncolumn\nx\nwithin\nthe\nsearc\nh\nrectangle\nand\nlet\nY\nb\ne\nthe\nn\num\nb\ne r\nof\np\noin\nts\nin\nro\nw\ny\nwithin\nthe\nsearc\nh\nrectangle\nThen\nthe\nam\nbiguit\ny\nlev\nel\nof\np\nis\nX\n\nY\n\nIn\npartic\nular\nif\np\nis\nthe\nonly\np\no i n\nt\nin\nits\nro\nw\nand\ncolumn\nthen\nits\nam\nbiguit\ny\nl e v\nel\nis\nzero\nThe\nc\nhain\nrecogni\ntion\nheuristic\nignores\np\noin\nts\nwhose\nam\nbiguit\ny\nl e v\nel\nis\nto\no\nhigh\nWhat\nmak\nes\nthis\na\nlo\ncalized\nlter\nis\nthat\nonly\np\noin\nts\nwithin\nthe\nsearc\nh\nrectangle\ncoun\nt\nt o\nw\nard\neac\nh\nothers\nam\nbiguit\ny\nlev\nel\nThe\nam\nbiguit\ny\nl e v\nel\nof\na\ng i v\nen\np\noin\nt\ncan\nc\nhange\nwhen\nthe\nsearc\nh\nrectangle\nexpands\nor\nmo\nv\nes\nThe\nnoise\nlter\nensures\nthat\nfalse\np\noin\nts\nof\ncorre\nsp\nondence\nare\nv\nery\nsparse\nas\nillustrated\nin\nFigure\n\nEv\nen\nif\none\nc\nhain\nof\nfalse\np\noin\nts\nof\ncorresp\nondence\nslips\nb\ny\nt h e\nc\nhain\nrecognition\nheuristic\nthe\nexpand\ning\nrectangle\nwill\nnd\nits\nw\na\ny\nbac\nk\nto\nthe\nTBM\nb\ne\nfore\nthe\nc\nhain\nrecognition\nheuristic\naccepts\nanother\nEnglish text\nanchor\noff track\nfalse\nchain\nFigure\n\nSIMRs\nnoise\nlter\nensur\nes\nthat\nTPCs\nar\ne\nmuch\nmor\ne\ndense\nthan\nfalse\np\noints\nof\nc\norr\nesp\non\ndenc\ne\nA\ngo\no\nd\nsignaltonoise\nr\natio\npr\nevents\nSIMR\nfr\nom\ngetting\nlost\nc\nhain\nIf\nthe\nmatc\nhing\npredicate\ngenerates\na\nreason\nably\nstrong\nsignal\nthen\nthe\nsignaltonoise\nratio\nwill\nb\ne\nhigh\nand\nSIMR\nwill\nnot\nget\nlost\nev\nen\nthough\nit\nis\na\ngreedy\nalgorithm\nwith\nno\nabilit\ny\nto\nlo\nok\nahead\n\nP\norting\nto\nNew\nLanguage\nP\nairs\nSIMR\ncan\nb\ne\np\norted\nto\na\nnew\nlanguage\npair\nin\nthree\nsteps\n\nStep\n\nConstruct\nMatc\nhing\nPredicate\nThe\noriginal\nSIMR\nimplemen\ntation\nfor\nF\nrenc\nhEnglish\nincluded\nmatc\nhing\npredicates\nthat\ncould\nuse\ncognates\nandor\ntranslation\nlexicons\nF\nor\nlanguage\npairs\nin\nwhic\nh\nlexical\ncognates\nare\nfrequen\nt\na\ncognatebased\nmatc\nhing\npredicate\nshould\nsuce\nIn\nother\ncases\na\nseed\ntranslation\nlexicon\nma\ny\nb\ne\nused\nto\nb\no\nost\nthe\nn\num\nb\ne r\no f\ncandidate\np\noin\nts\npro\nduced\nin\nthe\ngeneration\nphase\nof\nthe\nsearc\nh\nThe\nSIMR\nimplemen\ntation\nfor\nSpanishEnglish\nuses\nonly\ncognates\nF\nor\nKoreanEnglish\nSIMR\ntak\nes\nadv\nan\ntage\nof\npunctuation\nand\nn\num\nb\ner\ncognates\nbut\nsup\nplemen\nts\nthem\nwith\na\nsmall\ntranslation\nlexicon\n\nStep\n\nConstruct\nAxis\nGenerators\nIn\norder\nfor\nSIMR\nto\ngenerate\ncandidate\np\no i n\nts\nof\ncorresp\nondence\nit\nneeds\nto\nkno\nw\nwhat\ntok\nen\npairs\ncorresp\nond\nto\ncoordinates\nin\nthe\nsearc\nh\nrectangle\nIt\nis\nthe\naxis\ngenerators\njob\nto\nmap\nthe\nt\nw\no\nh a l v\nes\nof\nthe\nbitext\nto\np\nositions\non\nthe\nx\nand\nyaxes\nof\nthe\nbitext\nspace\nb\nefore\nSIMR\nstarts\nsearc\nhing\nfor\nc\nhains\nThis\nmapping\nshould\nb\ne\ndone\nwith\nthe\nmatc\nhing\npredicate\nin\nmind\nIf\nthe\nmatc\nhing\npredicate\nuses\ncognates\nthen\nev\nery\nw\nord\nthat\nmigh\nt\nha\nv\ne\na\ncognate\nin\nthe\nother\nhalf\nof\nthe\nbitext\nshould\nb\ne\nassigned\nits\no\nwn\naxis\n\np\nosition\nThis\nrule\napplies\nto\npunctuation\nand\nn\num\nb\ners\nas\nw\nell\nas\nto\nlexical\ncognates\nIn\nthe\ncase\nof\nlexical\ncognates\nthe\naxis\ngenerator\nt\nypically\nneeds\nto\nin\nv\nok\ne\na\nlanguagesp\necic\ntok\nenization\nprogram\nto\niden\ntify\nw\nords\nin\nthe\ntext\nW\nriting\nsuc\nh\na\npro\ngram\nma\ny\nconstitute\na\nsignican\nt\npart\nof\nthe\np\nort\ning\neort\nif\nno\nsuc\nh\nprogram\nis\na\nv\nailable\nin\nadv\nance\nThe\neort\nma\ny\nb\ne\nlessened\nho\nw\nev\ner\nb\ny\nthe\nrealiza\ntion\nthat\nit\nis\nacceptable\nfor\nthe\ntok\nenization\npro\ngram\nto\no\nv\nergenerate\njust\nas\nit\nis\nacceptable\nfor\nthe\nmatc\nhing\npredicate\nF\nor\nexample\nwhen\ntok\nenizing\nGerman\ntext\nit\nis\nnot\nnecessary\nfor\nthe\ntok\nenizer\nto\nkno\nw\nwhic\nh\nw\nords\nare\ncomp\nounds\nA\nw\nord\nthat\nhas\nanother\nw\nord\nas\na\nsubstring\nshould\nresult\nin\none\naxis\np\no s i t i o n\nfor\nthe\nsubstring\nand\none\nfor\nthe\nsu\np\nerstring\nWhen\nlexical\ncognates\nare\nnot\nb\neing\nused\nthe\naxis\ngenerator\nonly\nneeds\nto\niden\ntify\npunctuation\nn\num\nb\ners\nand\nthose\nc\nharacter\nstrings\nin\nthe\ntext\nwhic\nh\nalso\napp\near\non\nthe\nrelev\nan\nt\nside\nof\nthe\ntranslation\nlexicon\n\nIt\nw\nould\nb\ne\np\noin\ntless\nto\nplot\nother\nw\nords\non\nthe\naxes\nb\necause\nthe\nmatc\nhing\npredicate\ncould\nnev\ner\nmatc\nh\nt h e m\na n\nyw\na\ny\n\nTherefore\nfor\nlanguages\nlik\ne\nChinese\nand\nJapanese\nwhic\nh\nare\nwritten\nwith\nout\nspaces\nb\net\nw\neen\nw\nords\ntok\nenization\nb\noils\ndo\nwn\nto\nstring\nmatc\nhing\nIn\nthis\nmanner\nSIMR\ncircum\nv\nen\nts\nthe\ndicult\nproblem\nof\nw\nord\niden\ntication\nin\nthese\nlanguages\n\nStep\n\nReoptimize\nP\narameters\nThe\nlast\nstep\nin\nthe\np\norting\npro\ncess\nis\nto\nreoptimize\nSIMRs\nn\numerical\nparameters\nThe\nfour\nparameters\ndescrib\ned\nin\nSection\n\nin\nteract\nin\ncomplicated\nw\na\nys\nand\nit\nis\nimp\nossible\nto\nnd\na\ngo\no\nd\nparameter\nset\nanalytically\n\nIt\nis\neasier\nto\noptimize\nthese\nparameters\nempirically\n\nusing\nsim\nulated\nannealing\nVidal\n\nSim\nulated\nannealing\nrequires\nan\nob\njectiv\ne\nfunc\ntion\nto\noptimize\nThe\nob\njectiv\ne\nfunction\nfor\nbitext\nmapping\nshould\nmeasure\nthe\ndierence\nb\net\nw\neen\nthe\nTBM\nand\nmaps\npro\nduced\nwith\nthe\ncurren\nt\nparame\nter\nset\nIn\ngeometric\nterms\nthe\ndierence\nis\na\ndis\ntance\nThe\nTBM\nconsists\nof\na\nset\nof\nTPCs\nThe\nerror\nb\ne t\nw\neen\na\nbitext\nmap\nand\neac\nh\nTPC\ncan\nb\ne\ndened\nas\nthe\nhorizon\ntal\ndistance\nthe\nv\nertical\ndis\ntance\nor\nthe\ndistance\np\nerp\nendicular\nto\nthe\nmain\ndi\nagonal\nThe\nrst\nt\nw\no\nalternativ\nes\nw\nould\nminimize\nthe\nerror\nwith\nresp\nect\nto\nonly\none\nlanguage\nor\nthe\nother\nThe\np\nerp\nendicular\ndistance\nis\na\nmore\nrobust\na\nv\nerage\nIn\norder\nto\np\nenalize\nlarge\nerrors\nmore\nhea\nv\nily\n\nro\not\nmean\nsquared\nRMS\ndistance\nis\nminimized\ninstead\nof\nmean\ndistance\nMultiw\nord\nexpressions\nin\nthe\ntranslation\nlexicon\nare\ntreated\njust\nlik\ne\na n\ny\nother\nc\nharacter\nstring\nThe\nmost\ntedious\npart\nof\nthe\np\norting\npro\ncess\nis\nthe\nconstruction\nof\nTBMs\nagainst\nwhic\nh\nSIMRs\nparam\neters\ncan\nb\ne\noptimized\nand\ntested\nThe\neasiest\nw\na\ny\nto\nconstruct\nthese\ngold\nstandards\nis\nto\nextract\nthem\nfrom\npairs\nof\nhandaligned\ntext\nsegmen\nts\nThe\nnal\nc\nharacter\np\nositions\nof\neac\nh\nsegmen\nt\nin\nan\naligned\npair\nare\nthe\ncoordinates\nof\na\nTPC\nOv\ner\nthe\ncourse\nof\nt\nw\no\np\norting\neorts\nI\nha\nv\ne\ndev\nelop\ned\nand\nrened\nto\nols\nand\nmetho\nds\nthat\nallo\nw\na\nbilingual\nannota\ntor\nto\nconstruct\nthe\nrequired\nTBMs\nv\nery\necien\ntly\nfrom\na\nra\nw\nbitext\nF\nor\nexample\na\nto\nol\noriginally\nde\nsigned\nfor\nautomatic\ndetection\nof\nomissions\nin\ntrans\nlations\nMelamed\nb\nw\nas\nadopted\nto\ndetect\nmis\nalignmen\nts\n\nP\norting\nExp\nerience\nSummary\nT\nable\n\nsummarizes\nthe\namoun\nt\nof\ntime\nin\nv\nested\nin\neac\nh\nnew\nlanguage\npair\nThe\nestimated\ntimes\nfor\nbuilding\naxis\ngenerators\ndo\nnot\ninclude\nthe\ntime\nsp\nen\nt\nto\nbuild\nthe\nEnglish\naxis\ngenerator\nwhic\nh\nw\nas\npart\nof\nthe\noriginal\nimplemen\ntation\nAxis\ngenerators\nneed\nto\nb\ne\nbuilt\nonly\nonce\np\ner\nlanguage\nrather\nthan\nonce\np\ner\nlanguage\npair\n\nEv\naluation\nSIMR\nw\nas\nev\naluated\non\nhandaligned\nbitexts\nof\nv\nari\nous\ngenres\nin\nthree\nlanguage\npairs\nNone\nof\nthese\ntest\nbitexts\nw\nere\nused\nan\nywhere\nin\nthe\ntraining\nor\np\nort\ning\npro\ncedures\nEac\nh\ntest\nbitext\nw\nas\ncon\nv\nerted\nto\na\nset\nof\nTPCs\nb\ny\nnoting\nthe\npair\nof\nc\nharacter\np\nositions\nat\nthe\nend\nof\neac\nh\naligned\npair\nof\ntext\nsegmen\nts\nThe\ntest\nmetric\nw\nas\nthe\nro\not\nmean\nsquared\ndistance\nin\nc\nharacters\nb\net\nw\neen\neac\nh\nT P C\na n d\nt h e\ni n\nterp\nolated\nbitext\nmap\npro\nduced\nb\ny\nSIMR\nwhere\nthe\ndistance\nw\nas\nmeasured\np\nerp\nendicular\nto\nthe\nmain\ndiagonal\nThe\nresults\nare\npresen\nted\nin\nT\nable\n\nThe\nF\nrenc\nhEnglish\npart\nof\nthe\nev\naluation\nw\nas\np\nerformed\non\nbitexts\nfrom\nthe\npublicly\na\nv\nailable\nBAF\ncorpus\ncreated\nat\nCITI\nSimard\n\nPlamon\ndon\n\nSIMRs\nerror\ndistribution\non\nthe\nparlia\nmen\ntary\ndebates\nbitext\nin\nthis\ncollection\nis\ngiv\nen\nin\nT\nable\n\nThis\ndistribution\ncan\nb\ne\ncompared\nto\nerror\ndistributions\nrep\norted\nin\nCh\nurc\nh\n\nand\nin\nDa\ngan\net\nal\n\nSIMRs\nRMS\nerror\non\nthis\nbitext\nw\nas\n\nc\nharacters\nCh\nurc\nhs\nchar\nalign\nalgorithm\nCh\nurc\nh\n\nis\nthe\nonly\nalgorithm\nthat\ndo\nes\nnot\nuse\nsen\ntence\nb\noundary\ninformation\nfor\nwhic\nh\ncom\nparable\nresults\nha\nv\ne\nb\neen\nrep\norted\nchar\naligns\nRMS\nerror\non\nthis\nbitext\nw\nas\n\nc\nharacters\nexactly\nten\ntimes\nhigher\nTw\no\nteams\nof\nresearc\nhers\nha\nv\ne\nrep\norted\nresults\non\nthe\nsame\nparliamen\ntary\ndebates\nbitext\nfor\nal\ngorithms\nthat\nmap\ncorresp\nondence\nat\nthe\nsen\ntence\nlev\nel\nGale\n\nCh\nurc\nh\na\nSimard\net\nal\n\nT\nable\n\nTime\nsp\nent\nin\nc\nonstructing\ntwo\ngold\nstandar\nd\nTBMs\nlanguage\npair\nmain\ninforman\nt\nfor\nmatc\nhing\npredicate\nestimated\ntime\nsp\nen\nt\nto\nbuild\nnew\naxis\ngenerator\nestimated\ntime\nsp\nen\nt\no n\nhandalignmen\nt\nn\num\nb\ne r\no f\nsegmen\nts\naligned\nSpanishEnglish\nlexical\ncognates\n\nh\n\nh\n\nKoreanEnglish\ntranslation\nlexicon\n\nh\n\nh\n\nT\nable\n\nSIMR\nac\ncur\nacy\non\ndier\nent\ntext\ngenr\nes\nin\nthr\ne\ne\nlanguage\np\nairs\nlanguage\npair\nn\num\nb\ne r\no f\ntraining\nTPCs\ngenre\nn\num\nb\ne r\no f\ntest\nTPCs\nRMS\nError\nin\nc\nharacters\nF\nrenc\nh\n\nEnglish\n\nparliamen\ntary\ndebates\nCITI\ntec\nhnical\nrep\norts\n\nother\ntec\nhnical\nrep\norts\n\ncourt\ntranscripts\nUN\nann\nual\nrep\nort\n\nILO\nrep\nort\n\nSpanish\n\nEnglish\n\nsoft\nw\nare\nman\nuals\n\nKorean\n\nEnglish\n\nmilitary\nman\nuals\n\nmilitary\nmessages\n\nT\nable\n\nSIMRs\nerr\nor\ndistribution\non\nF\nr\nenchEnglish\np\narliamentary\ndeb\nates\nbitext\nn\num\nb\ne r\no f\ntest\np\noin\nts\nerror\nrange\nin\nc\nharacters\nfraction\nof\ntest\np\noin\nts\n\nto\n\nto\n\nto\n\nto\n\nto\n\nto\n\nto\n\nto\n\nt o\n\nto\n\nto\n\nto\n\nto\n\nto\n\nto\n\nto\n\nto\n\nto\n\nto\n\nBoth\nof\nthese\nalgorithms\nuse\nsen\ntence\nb\noundary\ninformation\nMelamed\na\nsho\nw\ned\nthat\nsen\ntence\nb\noundary\ninformation\ncan\nb\ne\nused\nto\ncon\nv\nert\nthe\nSIMRs\noutput\nin\nto\nsen\ntence\nalignmen\nts\nthat\nare\nmore\naccurate\nthan\nthose\nobtained\nb\ny\neither\nof\nthe\nother\nt\nw\no\napproac\nhes\nThe\ntest\nbitexts\nin\nthe\nother\nt\nw\no\nlanguage\npairs\nw\nere\ncreated\nwhen\nSIMR\nw\nas\nb\neing\np\norted\nto\nthose\nlanguages\nThe\nSpanishEnglish\nbitexts\nw\nere\ndra\nwn\nfrom\nthe\nonline\nSun\nMicroSystems\nSolaris\nAn\nsw\nerBo\noks\nThe\nKoreanEnglish\nbitexts\nw\nere\npro\nvided\nand\nhandaligned\nb\ny\nY\noungSuk\nLee\nof\nMITs\nLincoln\nLab\noratories\nAlthough\nit\nis\nnot\np\nossible\nto\ncompare\nSIMRs\np\nerformance\non\nthese\nlanguage\npairs\nto\nthe\np\nerformance\nof\nother\nalgorithms\nT\nable\n\nsho\nws\nthat\nthe\np\nerformance\non\nother\nlanguage\npairs\nis\nno\nw\norse\nthan\np\nerformance\non\nF\nrenc\nhEnglish\n\nWhic\nh\nT\next\nUnits\nto\nMap\nEarly\nbitext\nmapping\nalgorithms\nfo\ncused\non\nsen\ntences\nKa\ny\n\nR\nosc\nheisen\n\nDebili\n\nSam\nmouda\n\nAlthough\nsen\ntence\nmaps\ndo\nnot\nha\nv\ne\nsucien\nt\nresolution\nfor\nsome\nimp\nortan\nt\nbitext\nappli\ncations\nMelamed\nb\nMac\nklo\nvitc\nh\n\nsen\ntences\nw\nere\nan\neasy\nstarting\np\no i n\nt\nb\necause\ntheir\norder\nrarely\nc\nhanges\nduring\ntranslation\nTherefore\nsen\ntence\nmapping\nalgorithms\nneed\nnot\nw\norry\nab\nout\ncrossing\ncorresp\nondences\nIn\n\nt\nw\no\nteams\nof\nre\nsearc\nhers\nindep\nenden\ntly\ndisco\nv\nered\nthat\nsen\ntences\ncan\nb\ne\naccurately\naligned\nb\ny\nmatc\nhing\nsequences\n\nwith\nsimilar\nlengths\nGale\n\nCh\nurc\nh\na\nBro\nwn\net\nal\n\nSo\non\nthereafter\nCh\nurc\nh\n\nfound\nthat\nbitext\nmapping\nat\nthe\nsen\ntence\nlev\nel\nis\nnot\nan\noption\nfor\nnoisy\nbitexts\nfound\nin\nthe\nreal\nw\norld\nSen\ntences\nare\noften\ndicult\nto\ndetect\nesp\necially\nwhere\npunc\ntuation\nis\nmissing\ndue\nto\nOCR\nerrors\nMore\nim\np\nortan\ntly\n\nbitexts\noften\ncon\ntain\nlists\ntables\ntitles\nfo\notnotes\ncitations\nandor\nmarkup\nco\ndes\nthat\nfoil\nsen\ntence\nalignmen\nt\nmetho\nds\nCh\nurc\nhs\nsolution\nw\nas\nto\nlo\nok\nat\nthe\nsmallest\nof\ntext\nunits\n\nc\nharacters\n\nand\nto\nuse\ndigital\nsignal\npro\ncessing\ntec\nhniques\nto\ngrapple\nwith\nthe\nm\nuc\nh\nlarger\nn\num\nb\ne r\nof\ntext\nunits\nthat\nmigh\nt\nmatc\nh\nb\ne t\nw\neen\nthe\nt\nw\no\nhalv\nes\nof\na\nbitext\nCharacters\nmatc\nh\nacross\nlanguages\nonly\nto\nthe\nexten\nt\nthat\nthey\nparticipate\nin\ncognates\nTh\nus\nCh\nurc\nhs\nmetho\nd\nis\nonly\napplicable\nto\nlanguage\npairs\nwith\nsimilar\nalphab\nets\nThe\nmain\ninsigh\nt\nof\nthe\npresen\nt\nw\nork\nis\nthat\nw\nords\nare\na\nhapp\ny\nmediumsized\ntext\nunit\nat\nwhic\nh\nt o\nm a p\nbitext\ncorresp\nondence\nBy\nsituating\nw\nord\np\nositions\nin\na\nbitext\nspace\nthe\ngeometric\nheuristics\nof\nsen\ntence\nalignmen\nt\nalgorithms\ncan\nb\ne\nexploited\nequally\nw\nell\nat\nthe\nw\nord\nlev\nel\nThe\ncognate\nheuristic\nof\nthe\nc\nharacterbased\nalgorithms\nw\norks\nb\netter\nat\nthe\nw\nord\nlev\nel\nb\necause\ncognateness\ncan\nb\ne\ndened\nmore\nprecisely\nin\nterms\nof\nw\nords\neg\nusing\nthe\nLongest\nCommon\nSubsequence\nRatio\nMelamed\n\nSev\neral\nother\nmatc\nhing\nheuristics\ncan\nonly\nb\ne\napplied\nat\nthe\nw\nord\nlev\nel\nincluding\nthe\nlo\ncalized\nnoise\nlter\nin\nSection\n\nlists\nof\nstop\nw\nords\nand\nlists\nof\nfaux\namis\nMac\nklo\nvitc\nh\n\nMost\nimp\nortan\ntly\n\ntrans\nlation\nlexicons\ncan\nonly\nb\ne\nused\nat\nthe\nw\nord\nlev\nel\nSIMR\ncan\nemplo\ny\na\nsmall\nhandconstructed\ntransla\ntion\nlexicon\nto\nmap\nbitexts\nin\nan\ny\npair\nof\nlanguages\nev\nen\nwhen\nthe\ncognate\nheuristic\nis\nnot\napplicable\nand\nsen\ntences\ncannot\nb\ne\nfound\nThe\nparticular\ncom\nbina\ntion\nof\nheuristics\ndescrib\ned\nin\nSection\n\ncan\ncertainly\nb\ne\nimpro\nv\ned\non\nbut\nresearc\nh\ni n\nto\nb\netter\nbitext\nmap\nping\nalgorithms\nis\nlik\nely\nto\nb\ne\nmost\nfruitfull\nat\nthe\nw\nord\nlev\nel\n\nConclusion\nThe\nSmo\noth\nInjectiv\ne\nMap\nRecognizer\nSIMR\nbitext\nmapping\nalgorithm\nadv\nances\nthe\nstate\nof\nthe\nart\non\nsev\neral\nfron\ntiers\nIt\nis\nsignican\ntly\nmore\nac\ncurate\nthan\nother\nalgorithms\nin\nthe\nliterature\nIts\nexp\nected\nrunning\ntime\nand\nmemory\nrequiremen\nts\nare\nlinear\nin\nthe\nsize\nof\nthe\ninput\nwhic\nh\nmak\nes\nit\nthe\nalgorithm\nof\nc\nhoice\nfor\nv\nery\nlarge\nbitexts\nIt\nis\nnot\nfazed\nb\ny\nw\nord\norder\ndierences\nIt\ndo\nes\nnot\nrely\non\npresegmen\nted\ninput\nand\nis\np\nortable\nto\nan\ny\npair\nof\nlanguages\nwith\na\nminimal\neort\nThese\nfeatures\nmak\ne\nSIMR\nthe\nmostly\nwidely\napplicable\nbitext\nmapping\nalgorithm\nto\ndate\nSIMR\nop\nens\nup\nsev\neral\nnew\na\nv\nen\nues\nof\nresearc\nh\nOne\nimp\nortan\nt\napplication\nof\nbitext\nmaps\nis\nthe\ncon\nstruction\nof\ntranslation\nlexicons\nDagan\net\nal\n\nand\nas\ndiscussed\ntranslation\nlexicons\nare\nan\nimp\nor\ntan\nt\ninformation\nsource\nfor\nbitext\nmapping\nIt\nis\nlik\nely\nthat\nthe\naccuracy\nof\nb\noth\nkinds\nof\nalgorithms\ncan\nb\ne\nimpro\nv\ned\nb\ny\nalternating\nb\net\nw\neen\nthe\nt\nw\no\no n\nthe\nsame\nbitext\nThere\nare\nalso\nplans\nto\nbuild\nan\nautomatic\nbitext\nlo\ncating\nspider\nfor\nthe\nW\norld\nWide\nW\neb\nso\nthat\nSIMR\ncan\nb\ne\napplied\nto\nmore\nnew\nlan\nguage\npairs\nand\nbitext\ngenres\nAc\nkno\nwledgemen\nts\nSIMR\nw\nas\np\norted\nto\nSpanishEnglish\nwhile\nI\nw\nas\nvisiting\nSun\nMicroSystems\nLab\noratories\nThanks\nto\nGary\nAdams\nCo\nokie\nCallahan\nBob\nKuhns\nand\nPhilip\nResnik\nfor\ntheir\nhelp\nwith\nthat\npro\nject\nThanks\nalso\nto\nPhilip\nResnik\nfor\nwriting\nthe\nSpanish\ntok\nenizer\nand\nhandaligning\nthe\nSpanishEnglish\ntraining\nbitexts\nP\norting\nSIMR\nto\nKoreanEnglish\nw\nould\nnot\nha\nv\ne\nb\neen\np\nossible\nwithout\nY\noungSuk\nLee\nof\nMITs\nLincoln\nLab\noratories\nwho\npro\nvided\nthe\nseed\ntranslation\nlexicon\nand\naligned\nall\nthe\ntraining\nand\ntest\nbitexts\nThis\npap\ner\nw\nas\nm\nuc\nh\nimpro\nv\ned\nb\ny\nhelpful\ncommen\nts\nfrom\nMitc\nh\nMarcus\nAdw\nait\nRatnaparkhi\nBonnie\nW\nebb\ner\nand\nthree\nanon\nymous\nreview\ners\nThis\nresearc\nh\nw\nas\nsupp\norted\nb\ny\na n\ne q u i p\nmen\nt\ngran\nt\nf r o m\nSun\nMicroSystems\nand\nb\ny\nARP\nA\nCon\ntract\nN\n\nC\n\nReferences\nP\n\nF\nBro\nwn\nJ\nC\nLai\n\nR\nL\nMercer\nAligning\nSen\ntences\nin\nP\narallel\nCorp\nora\nPr\no\nc\ne\ne\ndings\nof\nthe\nth\nA\nnnual\nMe\neting\nof\nthe\nAsso\nciation\nfor\nCom\nputational\nLinguistics\nBerk\neley\n\nCA\n\nP\n\nF\nBro\nwn\nS\nDella\nPietra\nV\nDella\nPietra\n\nR\nMercer\nThe\nMathematics\nof\nStatistical\nMa\nc\nhine\nT\nranslation\nP\narameter\nEstimation\nCom\nputational\nLinguistics\n\nR\nCatizone\nG\nRussell\n\nS\nW\narwic\nk\nDeriving\nT\nranslation\nData\nfrom\nBilingual\nT\nexts\nPr\no\nc\ne\ne\nd\nings\nof\nthe\nFirst\nInternational\nL\nexic\nal\nA\nc\nquisition\nWorkshop\nDetroit\nMI\n\nS\nChen\nAligning\nSen\ntences\nin\nBilingual\nCorp\nora\nUsing\nLexical\nInformation\nPr\no\nc\ne\ne\ndings\nof\nthe\nst\nA\nnnual\nMe\neting\nof\nthe\nAsso\nciation\nfor\nCom\nputational\nLinguistics\nC o lu m\nbus\nOH\n\nK\nW\nCh\nurc\nh\nChar\nalign\nA\nProgram\nfor\nAlign\ning\nP\narallel\nT\nexts\nat\nthe\nCharacter\nLev\nel\nPr\no\n\nc\ne\ne\ndings\nof\nthe\nst\nA\nnnual\nMe\neting\nof\nthe\nAsso\nciation\nfor\nComputational\nLinguistics\nC olum\nbus\nOH\n\nI\nDagan\nK\nCh\nurc\nh\n\nW\nGale\nRobust\nW\nord\nAlignmen\nt\nfor\nMac\nhine\nAided\nT\nranslation\nPr\no\nc\ne\ne\ndings\nof\nthe\nWorkshop\non\nV\nery\nL\nar\nge\nCorp\nor\na\nA\nc\nademic\nand\nIndustrial\nPersp\ne\nctives\nC olum\nbus\nOH\n\nF\nDebili\n\nE\nSammouda\nAppariemen\nt\ndes\nPhrases\nde\nT\nextes\nBilingues\nPr\no\nc\ne\ne\ndings\nof\nthe\nth\nInternational\nConfer\nenc\ne\non\nComputational\nLin\nguistics\nNan\ntes\nF\nrance\n\nG\nF\noster\nP\n\nIsab\nelle\n\nP\n\nPlamondon\nW\nord\nCom\npletion\nA\nFirst\nStep\nT\no\nw\nard\nT\nargetT\next\nMedi\nated\nIMT\nPr\no\nc\ne\ne\ndings\nof\nthe\nth\nInternational\nConfer\nenc\ne\non\nComputational\nLinguistics\nCop\nen\nhagen\nDenmark\n\nP\n\nF\nung\nCompiling\nBilingual\nLexicon\nEn\ntries\nfrom\na\nN o n P\narallel\nEnglishChinese\nCorpus\nPr\no\nc\ne\ne\nd\nings\nof\nthe\nThir\nd\nWorkshop\non\nV\nery\nL\nar\nge\nCor\np\nor\na\nBoston\nMA\n\nW\nGale\n\nK\nW\nCh\nurc\nh\nA\nProgram\nfor\nAligning\nSen\ntences\nin\nBilingual\nCorp\nora\nPr\no\nc\ne\ne\ndings\nof\nthe\nth\nA\nnnual\nMe\neting\nof\nthe\nAsso\nciation\nfor\nComputational\nLinguistics\nBerk\neley\n\nCA\na\nW\nGale\n\nK\nW\nCh\nurc\nh\nIden\ntifying\nW\nord\nCorre\nsp\nondences\nin\nP\narallel\nT\nexts\nPr\no\nc\ne\ne\ndings\nof\nthe\nD\nARP\nA\nSNL\nWorkshop\nb\nB\nHarris\nBiT\next\na\nNew\nConcept\nin\nT\nranslation\nTheory\n\nL\nanguage\nMonthly\n\nM\nKa\ny\n\nM\nR\nosc\nheisen\nT\nextT\nranslation\nAlign\nmen\nt\nComputational\nLinguistics\n\nE\nMac\nklo\nvitc\nh\nP\neuton\nv\nerier\nautomatiquemen\nt\nla\ncoherence\nterminologique\nPr\no\nc\ne\ne\ndings\nof\nthe\nes\nI\nV\nJourn\n\nees\nscientiques\nL\nexic\nommatique\net\nDictionnairiques\norganized\nb\ny\nA\nUPELFUREF\nLy\non\nF\nrance\n\nI\nD\nMelamed\nAutomatic\nEv\naluation\nand\nUniform\nFilter\nCascades\nfor\nInducing\nN\nb\nest\nT\nranslation\nLexicons\nPr\no\nc\ne\ne\ndings\nof\nthe\nThir\nd\nWorkshop\non\nV\nery\nL\nar\nge\nCorp\nor\na\nBoston\nMA\n\nI\nD\nMelamed\nA\nGeometric\nApproac\nh\nto\nMapping\nBitext\nCorresp\nondence\nPr\no\nc\ne\ne\ndings\nof\nthe\nFirst\nConfer\nenc\ne\non\nEmpiric\nal\nMetho\nds\nin\nNatur\nal\nL\nan\nguage\nPr\no\nc\nessing\nEMNLP\nPhiladelphia\nP\nA\na\nI\nD\nMelamed\nAutomatic\nDetection\nof\nOmissions\nin\nT\nranslations\nPr\no\nc\ne\ne\ndings\nof\nthe\nth\nInterna\ntional\nConfer\nenc\ne\non\nComputational\nLinguistics\nCop\nenhagen\nDenmark\nb\nI\nD\nMelamed\nAutomatic\nConstruction\nof\nClean\nBroadCo\nv\nerage\nT\nranslation\nLexicons\nPr\no\nc\ne\ne\nd\nings\nof\nthe\nConfer\nenc\ne\nof\nthe\nAsso\nciation\nfor\nMachine\nT\nr\nanslation\nin\nthe\nA\nmeric\nas\nMon\ntreal\nCanada\nc\nI\nD\nMelamed\nA\nW\nordtoW\nord\nMo\ndel\nof\nT\nransla\ntional\nEquiv\nalence\nPr\no\nc\ne\ne\ndings\nof\nthe\nth\nCon\nfer\nenc\ne\nof\nthe\nAsso\nciation\nfor\nComputational\nLin\nguistics\nMadrid\nSpain\n\nin\nthis\nv\nolume\nP\n\nResnik\n\nI\nD\nMelamed\nSemiAutomatic\nAcqui\nsition\nof\nDomainSp\necic\nT\nranslation\nLexicons\nPr\no\nc\ne\ne\ndings\nof\nthe\nth\nA\nCL\nConfer\nenc\ne\non\nAp\nplie\nd\nNatur\nal\nL\nanguage\nPr\no\nc\nessing\nW\nashington\nDC\n\nP\n\nResnik\nEv\naluating\nMultilingual\nGisting\nof\nW\neb\nP\nages\nUMIA\nCSTR\nUniv\nersit\ny\nof\nMary\nland\n\nS\nSato\n\nM\nNagao\nT\no\nw\nard\nMemoryBased\nT\nrans\nlation\nPr\no\nc\ne\ne\ndings\nof\nthe\nth\nInternational\nConfer\nenc\ne\non\nComputational\nLinguistics\n\nS\nSato\nCTM\nAn\nExampleBased\nT\nranslation\nAid\nSystem\nPr\no\nc\ne\ne\ndings\nof\nthe\nth\nInterna\ntional\nConfer\nenc\ne\non\nComputational\nLinguistics\nNan\ntes\nF\nrance\n\nSIGIR\nWorkshop\non\nCr\nosslinguistic\nMultilingual\nInformation\nR\netrieval\nZuric\nh\n\nM\nSimard\nG\nF\nF\noster\n\nP\n\nIsab\nelle\nUsing\nCog\nnates\nto\nAlign\nSen\ntences\nin\nBilingual\nCorp\nora\nin\nPr\no\nc\ne\ne\ndings\nof\nthe\nF\nourth\nInternational\nCon\nfer\nenc\ne\non\nThe\nor\netic\nal\nand\nMetho\ndolo\ngic\nal\nIssues\nin\nMachine\nT\nr\nanslation\nMon\ntreal\nCanada\n\nM\nSimard\n\nP\n\nPlamondon\nBilingual\nSen\ntence\nAlignmen\nt\nBalancing\nRobustness\nand\nAccuracy\n\nPr\no\nc\ne\ne\ndings\nof\nthe\nConfer\nenc\ne\nof\nthe\nAsso\nciation\nfor\nMachine\nT\nr\nanslation\nin\nthe\nA\nmeric\nas\nMon\ntreal\nCanada\n\nR\nV\nV\nVidal\nApplie\nd\nSimulate\nd\nA\nnne\naling\nSpringerV\nerlag\nHeidelb\nerg\nGerman\ny"
    }
  ]
}