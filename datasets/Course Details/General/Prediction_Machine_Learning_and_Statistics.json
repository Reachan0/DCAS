{
  "course_name": "Prediction: Machine Learning and Statistics",
  "course_description": "Prediction is at the heart of almost every scientific discipline, and the study of generalization (that is, prediction) from data is the central topic of machine learning and statistics, and more generally, data mining. Machine learning and statistical methods are used throughout the scientific world for their use in handling the “information overload” that characterizes our current digital age. Machine learning developed from the artificial intelligence community, mainly within the last 30 years, at the same time that statistics has made major advances due to the availability of modern computing. However, parts of these two fields aim at the same goal, that is, of prediction from data. This course provides a selection of the most important topics from both of these subjects.",
  "topics": [
    "Engineering",
    "Computer Science",
    "Algorithms and Data Structures",
    "Artificial Intelligence",
    "Mathematics",
    "Applied Mathematics",
    "Probability and Statistics",
    "Engineering",
    "Computer Science",
    "Algorithms and Data Structures",
    "Artificial Intelligence",
    "Mathematics",
    "Applied Mathematics",
    "Probability and Statistics"
  ],
  "syllabus_content": "Course Meeting Times:\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nCourse Description\n\nPrediction is at the heart of almost every scientific discipline, and the study of generalization (that is, prediction) from data is the central topic of machine learning and statistics, and more generally, data mining. Machine learning and statistical methods are used throughout the scientific world for their use in handling the \"information overload\" that characterizes our current digital age. Machine learning developed from the artificial intelligence community, mainly within the last 30 years, at the same time that statistics has made major advances due to the availability of modern computing. However, parts of these two fields aim at the same goal, that is, of prediction from data. This course provides a selection of the most important topics from both of these subjects.\n\nThe course will start with machine learning algorithms, followed by statistical learning theory, which provides the mathematical foundation for these algorithms. We will then bring this theory into context, through the history of ML and statistics. This provides the transition into Bayesian analysis.\n\nMajor topics:\n\nAn overview of the \"top 10 algorithms in data mining,\" following a survey conducted at the International Conference on Data Mining (including association rule mining algorithms, decision trees, k-nearest neighbors, naive Bayes, etc.)\n\nA unified view of support vector machines, boosting, and regression, based on regularized risk minimization\n\nStatistical learning theory, structural risk minimization, generalization bounds (using concentration bounds from probability), the margin theory, VC dimension and covering numbers\n\nFrameworks for knowledge discovery (KDD, CRISP-DM)\n\nNotes on the history of ML and statistics\n\nBayesian analysis (exponential families, conjugate priors, hierarchical modeling, MCMC, Gibbs sampling, Metropolis-Hastings)\n\nAudience, Prerequisites, and Related Courses\n\nThis course is aimed at the introductory graduate and advanced undergraduate level. It will provide a foundational understanding of how machine learning and statistical algorithms work. Students will have a toolbox of algorithms that they can use on their own datasets after they leave the course.\n\nThe course contains theoretical material requiring mathematical background in basic analysis, probability, and linear algebra. Functional analysis (Hilbert spaces) will be covered as part of the course, and previous knowledge of the topic is not required. There will be a project assigned, and you are encouraged to design the project in line with your own research interests.\n\nThe material in this course overlaps with\n9.520\n(which has more theory and is more advanced),\n6.867\n(which has less theory, covers different algorithms, and is less advanced), and 6.437 (which does not cover ML or statistical learning theory). This course could be used as a follow-up course to 15.077, or taken independently.\n\nStudents will be required to learn R. Knowledge of MATLAB may also be helpful.\n\nCourse Requirements\n\nACTIVITIES\n\nPERCENTAGES\n\nProblem sets, including computational exercises [not available on MIT OpenCourseWare]\n\n50%\n\nCourse Project\n\nProposal\n\n10%\n\nAdvertisement\n\n2%\n\nProgress report\n\n0%\n\nPaper and talk\n\n38%\n\nAdditional References (Optional)\n\nRussell, Stuart, and Peter Norvig.\nArtificial Intelligence: A Modern Approach\n. 3rd ed. Prentice Hall, 2009. ISBN: 9780136042594.\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman.\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction\n. 2nd ed. Springer, 2009. ISBN: 9780387848570. [Preview with\nGoogle Books\n]\n\nCristianini, Nello, and John Shawe-Taylor.\nAn Introduction to Support Vector Machines and Other Kernel-based Learning Methods\n. Cambridge University Press, 2000. ISBN: 9780521780193.\n\nGelman, Andrew, et al.\nBayesian Data Analysis\n. 2nd ed. Chapman and Hall/CRC, 2003. ISBN: 9781584883883.\n\nBousquet, Olivier, Stephane Boucheron, and Gabor Lugosi.\nIntroduction to Statistical Learning Theory\n. (PDF)\n\nWu, Xindong, et al.\n\"Top 10 Algorithms in Data Mining.\" (PDF)\n\nKnowledge and Information Systems\n14 (2008): 1-37.\n\nCourse Material\n\nMachine learning and statistics tie into many different fields, including decision theory, information theory, functional analysis (Hilbert spaces), convex optimization, and probability. We will cover introductory material from most or all of these areas.\n\nOverarching Themes\n\nFive important problems in data mining: classification, clustering, regression, ranking, density estimation\n\nThe \"top 10 algorithms in data mining\"\n\nFrameworks for knowledge discovery (CRISP-DM, KDD)\n\nPriors in statistics\n\nConcepts\n\nTraining and testing, cross-validation\n\nOverfitting/underfitting, structural risk minimization, bias/variance tradeoff\n\nRegularized learning equation\n\nConjugate priors and exponential families\n\nAlgorithms (some covered in more depth than others)\n\nApriori (for association rule mining)\n\nk-NN (for classification)\n\nk-means (for clustering)\n\nNaive Bayes (for classification)\n\nDecision trees (for classification)\n\nPerceptron (for classification)\n\nSVM (for classification)\n\nAdaBoost and RankBoost (classification and ranking)\n\nHierarchical Bayesian modeling (for density estimation), including sampling techniques\n\nHistory\n\nSelected topics from the history of machine learning and statistics\n\nTheory\n\nSVM derivation: convex optimization, Hilbert spaces, reproducing kernel Hilbert spaces\n\nLarge deviation bounds and generalization bounds: Hoeffding bounds, Chernoff bounds (derived from Markov's bound), McDiarmid's inequality, VC bounds, margin bounds, covering numbers",
  "files": [
    {
      "category": "Lecture Notes",
      "title": "15.097 Lecture 1: Rule mining and the Apriori algorithm",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-097-prediction-machine-learning-and-statistics-spring-2012/eb02afbd0a9c32637dd64cdb6b76c2f1_MIT15_097S12_lec01.pdf",
      "content": "Rule Mining and the Apriori Algorithm\nMIT 15.097 Course Notes\nCynthia Rudin\nThe Apriori algorithm - often called the \"first thing data miners try,\" but some-\nhow doesn't appear in most data mining textbooks or courses!\nStart with market basket data:\nSome important definitions:\n- Itemset: a subset of items, e.g., (bananas, cherries, elderberries), indexed\nby {2, 3, 5}.\n- Support of an itemset: number of transactions containing it,\nm\nSupp(bananas, cherries, elderberries) =\nX\nMi,2 · Mi,3\ni=1\n· Mi,5.\n- Confidence of rule a →b: the fraction of times itemset b is purchased\nwhen itemset a is purchased.\nSupp(a ∪b)\n#times a and b are purchased\nConf(a →b) =\n=\nSupp(a)\n#times a is purchased\nˆ\n= P(b|a).\n\nWe want to find all strong rules. These are rules a →b such that:\nSupp(a ∪b) ≥θ, and Conf(a →b) ≥minconf.\nHere θ is called the minimum support threshold.\nThe support has a monotonicity property called downward closure:\nIf Supp(a ∪b) ≥θ then Supp(a) ≥θ and Supp(b) ≥θ.\nThat is, if a ∪b is a frequent item set, then so are a and b.\nSupp(a ∪b) = #times a and b are purchased\n≤#times a is purchased = Supp(a).\nApriori finds all frequent itemsets (a such that Supp(a) ≥θ).\nWe can use\nApriori's result to get all strong rules a →b as follows:\n- For each frequent itemset l:\n- Find all nonempty subsets of l\n- For each subset a, output a →{l\\ a} whenever\nSupp(l)\nSupp(a) ≥minconf.\nNow for Apriori. Use the downward closure property: generate all k-itemsets\n(itemsets of size k) from (k -1)-itemsets. It's a breadth-first-search.\n\nExample:\nθ = 10\nerries\napples\nbananas\ncherries\nelderb\nes\ngrap\n1-itemsets:\na\nb\nc\nd\ne\n/f\ng\nsupp:\n\n2-itemsets:\n\n{\n}\n{\n\na,b\na,c}\n{\n}\n{a,e}\n\na,d\n...\n{\ne,g}\nsupp:\n3-itemsets: {a,c,d}\n{a,c,e}\n{b,d,g} ...\nsupp:\n4-itemsets: {a,c,d,e}\nsupp:\nApriori Algorithm:\nInput: Matrix M\nL1={frequent 1-itemsets; i such that Supp(i) ≥θ}.\nFor k = 2, while Lk-1 = ∅(while there are large k -1-itemsets), k + +\n- Ck = apriori gen(Lk\n1) generate candidate itemsets of size k\n-\n- Lk = {c : c ∈Ck, Supp(c) ≥θ} frequent itemsets of size k (loop over\ntransactions, scan the database)\nend\nOutput:\nk Lk.\n\nS\n\nThe subroutine apriori gen joins Lk-1 to Lk-1.\napriori gen Subroutine:\nInput: Lk-1\nFind all pairs of itemsets in Lk-1 where the first k -2 items are identical.\ntoo big\nUnion them (lexicographically) to get Ck\n,\ne.g.,{a, b, c, d, e, f}, {a, b, c, d, e, g} →{a, b, c, d, e, f, g}\nPrune: Ck = {\ntoo big\nc ∈Ck\n, all (k -1)-subsets cs of c obey cs ∈Lk-1}.\nOutput: Ck.\nExample of Prune step: consider {a, b, c, d, e, f, g}\ntoo big\nwhich is in Ck\n, and I want\nto know whether it's in Ck. Look at {a, b, c, d, e, f, g}, {a, b,\nc, d, e, f, g},{a, b, c, d, e, f, g},\n{a, b, c, d,\ne, f, g}, etc. If any are not in L6, then prune {a, b, c, d, e, f, g} from L7.\nExample of the aprior\ni a\nlgori\nthm.\nImage by MIT OpenCourseWare, adapted from Osmar R. Zaiane.\n\n-\nApriori scans the database at most how many times?\n- Huge number of candidate sets.\nSpa\n/\n-\nwned huge number of apriori-like papers.\nWhat do you do with the rules after they're generated?\n- Information overload (give up)\n- Order rules by \"interestingness\"\n- Confidence\nSupp(a\nˆP(b|a) =\n∪b)\nSupp(a)\n- \"Lift\"/\"Interest\"\nˆP(b|a)\nSupp(b)\n=\nˆP(b)\n-Supp(a b\n∪)\nSupp(a)\n:\n- Hundreds!\nResearch questions:\n- mining more than just itemsets (e.g, sequences, trees, graphs)\n- incorporating taxonomy in items\n- boolean logic and \"logical analysis of data\"\n- Cynthia's questions: Can we use rules within ML to get good predictive\nmodels?\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.097 Prediction: Machine Learning and Statistics\nSpring 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "15.097 Lecture 10: Boosting",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-097-prediction-machine-learning-and-statistics-spring-2012/bd1326207712d907fe3418bcc9043b55_MIT15_097S12_lec10.pdf",
      "content": "Boosting\nMIT 15.097 Course Notes\nCynthia Rudin\nCredit: Freund, Schapire, Daubechies\nBoosting started with a question of Michael Kearns, about whether a \"weak\nlearning algorithm\" can be made into a \"strong learning algorithm.\" Suppose a\nlearning algorithm is only guaranteed, with high probability, to be slightly more\naccurate than random guessing. Is it possible, despite how weak this algorithm\nis, to use it to create a classifier whose error rate is arbitrarily close to 0? (There\nare some other constraints that made the problem harder, regarding how much\ncalculation is allowed.) The answer to this question was given by Rob Schapire,\nwith an algorithm that could do it, but wasn't very practical. Rob and Yoav\nFreund developed some more algorithms, e.g., Boost-by-Majority, and then de-\nveloped AdaBoost.\nGiven:\n1. examples S = {xi, yi}m\ni=1, where yi ∈{-1, 1}\n2. easy access to \"weak\" learning algorithm A, producing \"weak classifiers\"\nh ∈H, h : X →{-1, 1}.\n3. ε > 0.\nGoal: Produce a new classifier H : X →{-1, 1} with error ≤ε. Note: H is not\nrequired to be in H.\nWhat we might do is ask the weak learning algorithm A to produce a collection\nof weak classifiers and figure out how to combine them. But running A with the\nsame input multiple times won't be useful, for instance, if A is deterministic, it\nwill always produce the same weak classifiers over and over again. So we need\nto modify A's input to give new information each time we ask for a weak clas-\nsifier. AdaBoost does this by producing a discrete probability distribution over\nthe examples, using that as input to the weak learning algorithm and changing\nit at each round.\n\nOutline of a generic boosting algorithm:\nfor t = 1...T\nconstruct dt, where dt is a discrete probability distribution\nover indices {1...m}.\nrun A on dt, producing h(t) : X →{-1, 1}.\ncalculate\nεt\n=\nerrordt(h(t)) = Pri dt[h(t)(x\n∼\ni) = yi]\n=: 2 -γt,\nwhere by the weak learning assumption, γt > γWLA.\n(Of course, A tries\nto minimize the error through the choice of h(t).)\nend\noutput H\nHow do we design the dt's? How do we create H? Let's see what AdaBoost does.\nAdaBoost (Freund and Schapire 98) is one of the top 10 algorithms in data min-\ning, also boosted decision trees rated #1 in Caruana and Niculescu-Mizil's 2006\nempirical survey.\nThe idea is that at each round, we increase the weight on the examples that are\nharder to classify - those are the ones that have been previously misclassified\nat previous iterates. So the weights of an example go up and down, depending\non how easy the example was to classify. The easy examples can eventually get\ntiny weights and the hard examples get all the weight. In our notation, dt,i is\nthe weight of the probability distribution on example i. dt is called the \"weight\nvector.\"\nd1,i =\nfor all i\nm\ndt,i\ne-αt\nif yi = h(t)(xi) (smaller weights for easy examples)\ndt+1,i =\nZt\n×\n(\neαt\nif yi = h(t)(xi) (larger weights for hard examples)\nwhere Zt is a normalization constant for the discrete distribution\nthat ensures\nX\ndt+1,i = 1\ni\ndt,i\n=\ne-yiαth(t)(xi)\nZt\n\nThat's how AdaBoost's weights are defined. It's an exponential weighting scheme,\nwhere the easy examples are down-weighted and the hard examples are up-\nweighted.\nH is a linear combination of weak classifiers:\nH(x) = sign\nX\nT\nαth(t)(x)\n.\nt=1\n!\nIt's a weighted vote, where αt is the coefficient assigned to h(t).\nHere, αt is\ndirectly related to how well the weak classifier h(t) performed on the weighted\ntraining set:\n1 -εt\nαt =\nln\nεt\n\n.\n(1)\nwhere\nεt = Pi∼dt[h(t)(xi) = yi] =\nX\ndt,i1[h(t)(xi)=yi]\n(2)\ni\nAdaBoost stands for \"Adaptive Boosting.\" It doesn't depend on the weak learn-\ning algorithm's assumption, γWLA, it adapts to the weak learning algorithm.\nDemo\nTry to remember these things about the notation: dt are the weights on the\nexamples, and αt are the coefficients for the linear combination that is used to\nmake predictions. These in some sense are both weights, so it's kind of easy to\nget them confused.\nStatistical View of AdaBoost\nI'm going to give the \"statistical view\" of AdaBoost, which is that it's a coordi-\nnate descent algorithm. Coordinate descent is just like gradient descent, except\nthat you can't move along the gradient, you have to choose just one coordinate\nat a time to move along.\n\nAdaBoost was designed by Freund and Schapire, but they weren't the ones who\ncame up with the statistical view of boosting, it was 5 different groups simulta-\nneously (Breiman, 1997; Friedman et al., 2000; R atsch et al., 2001; Duffy and\nHelmbold, 1999; Mason et al., 2000).\nStart again with {(x , y\nm\ni\ni)}i=1 and weak learning algorithm A that can produce\nweak classifiers {hj}n\nj=1, hj : X →{-1, 1}. Here n can be very large or even\ninfinite. Or, hj could be very simple and produce the jth coordinate when xi is\n(j)\na binary vector, so hj(xi) = xi .\nConsider the misclassification error:\nm\nMiscl. error = m\nX\n1[yif(xi)≤0]\ni=1\nw\ne\nm\nX\n-yif(xi).\ni=1\nChoose f to be some linear combination of weak classifiers,\nf(x) =\nX\nn\nλjhj(x).\nj=1\nWe're going to end up minimizing the exponential loss with respect to the λj's.\nhich is upper bounded by the exponential loss:\nm\nCoor\ndina\nte\nd\nesce\nnt,\nw\ni\nth\nmov\ne\ns\nin\nth\ne x and y directions.\nImage by MIT OpenCourseWare.\n\nThe notation is going to get complicated from here on in, so try to remember\nwhat is what!\nDefine an m × n matrix M so that Mij = yihj(xi).\nSo matrix M encodes all of the training examples and the whole weak learning\nalgorithm. In other words, M contains all the inputs to AdaBoost. The ijth\nentry in the matrix is 1 whenever weak classifier j correctly classifies example i.\n(Note: we might never write out the whole matrix M in practice!)\nThen\nyif(xi) =\nX\nλjyihj(xi) =\nX\nλjMij = (Mλ)i.\nj\nj\nThen the exponential loss is:\nRtrain\n(λ) =\nX\ne-yif(xi) =\nX\ne-(Mλ)i.\n(3)\nm\nm\ni\ni\nLet's do coordinate descent on the exp-loss Rtrain(λ). At each iteration we'll\nchoose a coordinate of λ, called jt, and move αt in the jth\nt\ndirection. So each\nweak classifier corresponds to a direction in the space, and αt corresponds to\na distance along that direction. To do coordinate descent, we need to find the\ndirection j in which the directional derivative is the steepest. Let ej be a vector\nthat is 1 in the jth entry and 0 elsewhere. Choose direction:\n\ne\nj\n\"\n∂Rtrain(λ\nα\nt ∈argmaxj\n-\nt +\nj)\n∂α\n\n∂\n\nα=0\n#\nm\n= argmax\n\n\"\nλ\nj\n\"\n-∂α\nm\nX\ne-(M(\n\nt+αej))i\ni=1\n#\n\"\nα=0\n#\n∂\n= argmax\n-\n\"\n1 X\nm\ne-(Mλt)\nα\n\ni\n\nj\n-(Mej)i\n∂α\nm\n\ni=1\n#\nX\nm\n=\n\"\nα=0\n#\n∂\nargmax\n-\n\"\ne-(Mλt)\n\ni\n∂α\nm\n\nj\n-αMij\ni=1\n#\nα=0\n#\n= argmax\n\"\nm\n1 X\nM e-(Mλ )\n\nt i\n\nj\nij\n.\nm\n\ni=1\n#\nCreate a discrete probability distribution (we'll see later it's the same as Ad-\naBoost's):\nm\ndt,i = e-(Mλt)i/Zt where Z\n(\nt =\nX\ne-Mλt)i\n(4)\ni=1\nMultiplying by Zt doesn't affect the argmax. Same with\n1 . So from above we\nm\nhave:\nm\njt ∈argmaxj\nX\nMijdt,i = argmax(dT\nt M)j.\ni=1\nSo that's how we choose weak classifier jt. How far should we go along direction\njt? Do a linesearch, set derivative to 0.\n∂Rtrain(λt + αej\n0 =\nt)\n∂α\n\nαt\nm\n= -\nX\nM\ne-(Mλt)i\nαtMij\nij\n-\nt\nm\nt\ni=1\n= -\nX\ne-(Mλt)ie-αt\nm i:Mij =1\n-m i:M\nt\nX\nij =\nt\n-1\n-e-(Mλt)ieαt.\n\nGet rid of the -1/m and multiply by 1/Zt:\n=\nX\nd e-αt\nα\nt,i\ni:Mij =1\n-\ndt,ie\nt\ni:Mij =\nt\nt\n-1\n=: d e-αt\n+\n-d eα\nX\n-\nt\nd eα\n-\nt\n=\nd+e-αt\nd+\ne2αt\n=\nd-\nd+\nαt\n=\nln\n=\nln\n-d-.\nd\nd\n-\n-\nSo the coordinate descent algorithm is:\nd1,i = 1/m for i = 1...m\nλ1 = 0\nloop t = 1...T\njt ∈argmax\nd\n=\n-\nαt = 1\nP\nT\nj(dt M)j\nMij =\nt\n-1 dt,i\nln\n\n1-d-\nd-\nλt+1 = λt + αte\n\njt\nd\n= e-(Mλt+1)i/\nPm\nZ\nfor each i, where Z\n=\ne-(Mλt+1)i\nt+1,i\nt+1\nt+1\ni=1\nend\nSo that algorithm iteratively minimizes the exp-loss. But how is it AdaBoost?\nStart with f(x). From coordinate descent, we notice that λt,j is just the sum of\nthe αt's where our chosen direction jt is j.\nT\nλt,j =\nαt1[jt=j]\nt=1\nIn other words, it's the total amount\nX\nwe've traveled along direction j. Thus\nn\nf(x) =\nX\nn\nT\nT\nn\nT\nλt,jhj(x) =\nX X\nαt1[jt=j]hj(x) =\nX\nαt\nX\nhj(x)1[jt=j] =\nX\nαthjt(x).\nj=1\nj=1 t=1\nt=1\nj=1\nt=1\nLook familiar? There is a slight difference in notation between this and Ad-\naBoost, but that's it. (You can just set AdaBoost's h(t) to coord descent's hjt.)\n\nLet's look at dt. AdaBoost has:\nd\ne-Mij\nt,ie-Mij αt\nα\nt\nQ\nQ\nt\nt\nt\ne-\nt Mij αt\nt\nd\nj\nt+1,i =\n=\n=\nZt\nm\nt Zt\nm\nP\nQ\n=\nt Zt\nm Q\ne-\nMijλt,j\nZt\nP\nThis means the denominator must be\ni e-P\nj Mijλtj because we know the dt+1\nvector is normalized. So the dt's for AdaBoost are the same as for coordinate\ndescent (4) as long as jt's and αt's are\nP\nthe same.\nLet's make sure AdaBoost chooses the same directions jt as the coordinate de-\nscent algorithm. AdaBoost's weak learning algorithm hopefully chooses the weak\nclassifier that has the lowest error. This combined with the definition for the error\n(2) means:\njt ∈argminj\nX\ndt,i1[hj(xi)=yi] = argminj\ni\ni:M\nX\ndt,i\nij=-1\n= argmaxj\n\n-\ni:M\nX\ndt,i\nij=-1\n\n= argmaxj 1 -2\ni:M\nX\ndt,i\nij=-1\n\n= argmaxj\n\nX\ndt,i +\nd\ni:Mij=1\ni:M\nX\nt,i\nij=-1\n\n= argmaxj\ndt,i\ndt,i =\n-2\ni:M\nX\ndt,i\nX\nij=-1\n-\nX\nargmax\nT\nj(dt M)j.\n\ni:Mij=1\ni:Mij=-1\nDoes that also look familiar? So AdaBoost chooses the same directions as coor-\ndinate descent. But does it go the same distance?\nLook at αt. Start again with the error rate εt:\nεt =\nX\ndt,i1[hj (xi)=yi] =\ndt,i =\ndt,i = d\nt\n-\ni\ni:hj (xi)=yi\ni:Mij = 1\nt\nX\nX\nt\n-\nStarting from AdaBoost,\nd\nαt =\nln\n-εt\n=\nln\n-\n-,\nεt\nd-\nas in coordinate descent.\nSo AdaBoost minimizes the exponential loss by coordinate descent.\n\nProbabilistic Interpretation\nThis is not quite the same as logistic regression. Since AdaBoost approximately\nminimizes the expected exponential loss over the whole distribution D, we can\nget a relationship between it's output and the conditional probabilities. As in\nlogistic regression, assume that there's a distribution on y for each x.\nLemma (Hastie, Friedman, Tibshirani, 2001)\nEY\ne-Y f(x)\n∼D(x)\nis minimized at:\nP(Y = 1\nf(x) =\nln\n|x) .\nP(Y = -1|x)\nProof.\nEe-Y f(x) = P(Y = 1 x)e-f(x) + P(Y =\n1 x)ef(x)\ndE(e-Y f(x)\n|\n-|\n0 =\n|x) = -P(Y = 1|x)e-f(x) + P(Y =\n1 x)ef(x)\ndf(x)\n-|\nP(Y = 1|x)e-f(x) = P(Y = -1|x)ef(x)\nP(Y = 1|x)\n(\nP\n= 1\n= e2f x)\n(Y\nf(x) =\nln\n|x)\nP(Y = -1|x)\n⇒\n.\nP(Y = -1|x)\n■\nIn logistic regression, we had\nP(Y = 1 x)\nf(x) = ln\n|\nP(Y = -1|x)\nso we're different by just a factor of 2.\nFrom the Lemma, we can get probabilities out of AdaBoost by solving for P(Y =\n1|x), which we denote by p:\nP(Y = 1\nf(x) =\nln\n|x)\np\n=:\nln\nP(Y = -1\np\n|x)\n1 -p\ne2f(x) = 1 -p\ne2f(x) -pe2f(x) = p\ne2f(x) = p(1 + e2f(x))\n\nand finally,\ne2f(x)\np = P(Y = 1|x) =\n.\n1 + e2f(x)\nRecall logistic regression had the same thing, but without the 2's.\nSo now we can get probabilities out of AdaBoost.\n- This is helpful if you want a probability of failure, or probability of spam,\nor probability of discovering oil.\n- Even though logistic regression minimizes the logistic loss and AdaBoost\nminimizes the exponential loss, the formulas to get probabilities are some-\nhow very similar.\nTraining Error Decays Exponentially Fast\nTheorem If the weak learning assumption holds, AdaBoost's misclassification\nerror decays exponentially fast:\nm\nm\nX\n[yi=H(xi)]\ni=1\n≤e-γ2\nW LAT.\nProof. Start with\nm\nm\nRtrain(λt+1) = Rtrain\n(λt + αtejt) =\ne-[M(λt+αtej )]i\nt\n=\ne-(Mλt)i-αtMijt\nm\nX\nm\ni=1\nα\nλ\nX\n= e-\nt\nX\ne-(M\nt)i + eαt 1\nX\ni=1\ne-(Mλt)i.\n(5)\nm\nm\ni:Mij =1\ni:Mij = 1\nt\nt\n-\nTo summarize this proof, we will find a recursive relationship between Rtrain(λt+1)\nand Rtrain(λt), so we see how much the training error is reduced at each iteration.\nThen we'll uncoil the recursion to get the bound.\nLet's create the recursive relationship. Think about the distribution dt that we\ndefined from coordinate descent:\nm\nd\n= e-(Mλt)i\nt,i\n/Zt where Zt =\ne-(Mλt)i\n(6)\n\nX\ni=1\n\nSo you should think of e-(Mλt)i as an unnormalized version of the weight dt,i.\nSo that means:\nZt\nZt\nd+ =\nX\nd\n(Mλt)i\nt,i =\nm\nm\nm\ni:Mij =1\ni:M\nt\nX\ne-\n.\nij =1\nt\nand we could do the same thing to show Ztd\n= 1\ne (Mλ\nm\n-\nt)i. Plugging\nm\nthat\nP\ni:Mij =\n-\nt\n-1\ninto (5),\nRtrain\nt\n(λt\n) = e-αZt\nZ\n+1\nd+ + eα\nd .\nm\nm\n-\nIt's kind of neat that Zt/m = Rtrain(λt), which we can see from the definition of\nZt in (6) and the definition of Rtrain in (3). Let's use that.\nRtrain(λt+1) = Rtrain(λ\nα\nt)[e-d+ + eαd ]\n-\n= Rtrain(λ )[e-α(1 -d ) + eα\nt\nd ].\n-\n-\nRemember:\nαt =\nln\n-\nd\nd\n-\n\n/2\n1/\nd\n, so eα =\n-\n-\nd-\n\nand e-α =\n\n-\nd\n1 -d\n-\n-\n\n.\nPlugging:\n\"\n1/2\n-\n1/2\ntrain\nd\nR\n(λ\n) = Rtrain\nd\nt+1\n(λt)\n-\n(1\nd ) +\n-\nd\n1 -d\n-\n-\n\n-\n-\n\nd-\n\n#\n= Rtrain(λt)2 [d (1\n-\n-\n1/2\nd )]\n-\n1/2\n= Rtrain(λt)2 [εt(1 -εt)]\n.\nUncoil the recursion, using for the base case λ\ntrain\n1 = 0, so that R\n(0) = 1, then\nT\nRtrain(λT) =\nY\nεt(1\nεt).\nt=1\np\n-\nFrom all the way back at the beginning (on page 2 in the pseudocode) εt =\n1/2 -γt. So,\nT\nRtrain(λT) =\nY\nt=1\ns1\n2 -γt\n1 + γt\n\n=\nY\nt\nr\n4 -γ2\nt =\nY\nt\nq\n1 -4γ2\nt .\n\nUsing the inequality 1 + x ≤ex, which is true for all x,\nY\nT q\n-\n4γ2\nt\nt ≤\nY p\ne-4γ2\nt =\nY\ne-2γ = e-\nt\nPT\nt=1 γ2\nt .\n=1\nt\nt\nNow, we'll use the weak learning assumption, which is that γt > γWLA for all t.\nWe'll also use that the misclassification error is upper bounded by the exponential\nloss:\nm\n1 X\n≤\n≤\n-PT\nγ2\n[y\n)\nt=1\n=H(x )]\nRtrain(λT\ne 2\nt\ne-2γW LAT.\nm\ni\ni\ni=1\n≤\nAnd that's our bound.\nInterpreting AdaBoost\nSome points:\n1. AdaBoost can be used in two ways:\n-\n(j)\nwhere the weak classifiers are truly weak (e.g., hj(xi) = xi ).\n- where the weak classifiers are strong, and come from another learning\nalgorithm, like a decision tree. The weak learning algorithm is then,\ne.g., C4.5 or CART, and each possible tree it produces is one of the\nhj's.\nYou can think of the decision tree algorithm as a kind of \"oracle\" where\nAdaBoost asks it at each round to come up with a tree hjt with low error.\nSo the weak learning algorithm does the argmaxj step in the algorithm.\nIn reality it might not truly find the argmax, but it will give a good di-\nrection j (and that's fine - as long as it chooses a good enough direction,\nit'll still converge ok).\n2. The WLA does not hold in practice, but AdaBoost works anyway. (To see\nwhy, you can just think of the statistical view of boosting.)\n3. AdaBoost has an interpretation as a 2-player repeated game.\n\nweak learning algorithm chooses jt ≡column player chooses a pure strategy\ndt ≡mixed strategy for row player\n4. Neither logistic regression nor AdaBoost has regularization... but AdaBoost\nhas a tendency not to overfit. There is lots of work to explain why this is, and\nit seems to be completely due to AdaBoost's iterative procedure - another\nmethod for optimizing the exponential loss probably wouldn't do as well.\nThere is a \"margin\" theory for boosting that explains a lot.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.097 Prediction: Machine Learning and Statistics\nSpring 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "15.097 Lecture 11: Convex optimization",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-097-prediction-machine-learning-and-statistics-spring-2012/3c580801bf99c95efc0559294eb9d2e2_MIT15_097S12_lec11.pdf",
      "content": "Convex Optimization Overview\nMIT 15.097 Course Notes\nCynthia Rudin\nCredit: Boyd, Ng and Knowles\nThanks: Ashia Wilson\nWe want to solve differentiable convex optimization problems of this form, which\nwe call OPT:\nminimize f(x)\nx∈Rn\nsubject to gi(x) ≤ 0, i = 1, . . . , m,\nhi(x) = 0, i = 1 . . . , p,\nwhere x ∈ Rn is the optimization variable, f : Rn → R, gi : Rn → R are differ\nentiable convex functions, and hi : Rn → R are affine functions.\nRecall that a function g : G → R is convex if G is a convex set, and for any\nx, z ∈ G and θ ∈ [0, 1], we have g(θx+(1-θ)z) ≤ θg(x)+(1-θ)g(z). A function\nT\ng is concave if -g is convex. An affine function has the form h(x) = a x + b for\nsome a ∈ Rn, b ∈ R. (Affine functions are both convex and concave.)\nWe could rewrite the OPT with the constraints in the objective:\nmin ΘP (x)\nwhere\nx\nm\np\nm\nm\n(1)\nΘP (x) :=f(x) + inf\n1[gi(x)>0] + inf\n1[hi(x) =0].\ni=1\ni=1\nBut this is hard to optimize because it is non-differentiable and not even contin\nuous. Why don't we replace inf× 1[u≥0] with something nicer? A line αu seems\nlike a dumb choice...\n\nbut for α ≥ 0 the penalty is in the right direction (we are penalized for con\nstraints being dissatisfied, and αu is a lower bound on 1[gi(x)>0]).\nSimilarly, βu is a lower bound for inf× 1[u\nno matter what the sign of β is.\nFor each constraint i, replacing inf× 1[u≥0] by αu, where α ≥ 0 in the objective,\nand similarly replacing the inf× 1[u\nterm by βu, gives the Lagrangian\nm\np\nm\nm\nL(x, α, β) = f(x) +\nαigi(x) +\nβihi(x).\n(2)\ni=1\ni=1\nWe refer to x ∈ Rn as the primal variables of the Lagrangian. The second ar\ngument of the Lagrangian is a vector α ∈ Rm . The third is a vector β ∈ Rp.\nElements of α and β are collectively known as the dual variables of the La\ngrangian, or Lagrange multipliers.\nIf we take the maximum of L with respect to α and β, where αi ≥ 0, we recover\nOPT. Let's show this.\nFor a particular x, let's say the constraints are satisfied. So gi(x) ≤ 0, and\nto make the term αigi(x) as high as it can be, we set αi = 0 ∀i. Also, since\nhi(x) = 0 ∀i, the βi's can be anything and it won't change L. To summarize,\nif the constraints are satisfied, the value of maxα,β L is just f(x), which is the\nsame as the value of ΘP .\n=0]\n=0]\n\nIf any constraint is not satisfied, then we can make L(x, α, β) infinite by fiddling\nwith αi or βi, and then again the value of maxα,β L is the same as the value of ΘP .\nHow?\nSo we have formally:\nΘP (x) =\nmax L(x, α, β).\nα,β;αi≥0,∀i\nRemember that we want to minimize ΘP (x). This problem is the primal problem.\nSpecifically, the primal problem is:\n\nmin\nx\nmax\nα,β:αi≥0,∀i L(x, α, β) = min\nx\nΘP (x).\n(3)\nIn the equation above, the function ΘP : Rn → R is called the primal objective.\nWe say that a point x ∈ Rn is primal feasible if gi(x) ≤ 0, i = 1, . . . , m and\n∗\nhi(x) = 0, i = 1, . . . , p. The vector x ∈ Rn denotes the solution of (3), and\n∗\np = ΘP (x ∗) denotes the optimal value of the primal objective.\nIt turns out that ΘP (x) is a convex function of x. Why is that? First, f(x)\nis convex. Each of the gi(x)'s are convex functions in x, and since the αi's are\nconstrained to be nonnegative, then αigi(x) is convex in x for each i. Similarly,\neach βihi(x) is convex in x (regardless of the sign of βi) since hi(x) is linear.\nSince the sum of convex functions is always convex, L is convex for each α and\nβ. Finally, the maximum of a collection of convex functions is again a convex\nfunction, so we can conclude that ΘP (x) = maxα,β L(α, β, x) is a convex function\nof x.\nBy switching the order of the minimization and maximization above, we obtain\nan entirely different optimization problem.\n\nThe dual problem is:\n\nmax\nmin L(x, α, β) =\nmax\nΘD(α, β).\n(4)\nα,β:αi≥0,∀i\nx\nα,β:αi≥0,∀i\nFunction ΘD : Rm × Rp → R is called the dual objective.\nWe say that (α, β) are dual feasible if αi ≥ 0, i = 1 . . . , m.\nDenote (α∗ , β∗) ∈ Rm × Rp as the solution of (4), and d∗ = ΘD(α∗ , β∗) denote\nthe optimal value of the dual objective.\nThe dual objective ΘD(α, β), is a concave function of α and β. The dual objec\ntive is:\n\nm\np\nm\nm\nΘD(α, β) = min L(x, α, β) = min f(x) +\nαigi(x) +\nβihi(x) .\nx\nx\ni=1\ni=1\nFor any fixed value of x, the quantity inside the brackets is an affine function\nof α and β, and hence, concave. The f(x) is just a constant as far as α and\nβ are concerned. Since the minimum of a collection of concave functions is also\nconcave, we can conclude that ΘD(α, β) is a concave function of α and β.\nInterpreting the Dual Problem\nWe make the following observation:\n∗\nLemma 1. If (α, β) are dual feasible, then ΘD(α, β) ≤ p\nProof. Because of the lower bounds we made, namely\nαigi(x) ≤inf× 1[gi(x)≥0]\nβihi(x) ≤inf× 1[hi(x\n\nwe have, when α and β are dual feasible,\nL(x, α, β) ≤ ΘP (x) for all x.\nTaking the minx of both sides:\n)=0]\n\nmin L(x, α, β) ≤ min ΘP (x) .\nx\nx\n\"\n\n\"\n\n∗\np\nΘD(α, β)\n-\nThe lemma shows that given any dual feasible (α, β), the dual objective ΘD(α, β)\n∗\nprovides a lower bound on the optimal value p of the primal problem.\nSince the dual problem is maxα,β ΘD(α, β), the dual problem can be seen as a\n∗\nsearch for the tightest possible lower bound on p . This gives rise to a property\nof any primal and dual optimization problem pairs known as weak duality:\n∗\nLemma 2. For any pair of primal and dual problems, d∗≤ p .\nRewritten,\nmax min L(x, α, β) ≤ min max L(x, α, β).\nα,β:αi≥0,∀i x\nx α,β:αi≥0,∀i\nIntuitively, this makes sense: on the left, whatever the α, β player does, the x\nplayer gets to react to bring the value down. On the right, whatever the x player\ndoes, the α, β player reacts to bring the value up. And of course the player who\nplays last has the advantage.\nProve it? (Hint: place your hand over the leftmost max to see it.)\nα,β\nFor some primal/dual optimization problems, an even stronger result holds,\nknown as strong duality.\nLemma 3 (Strong Duality). For any pair of primal and dual problems which\n∗\nsatisfy certain technical conditions called constraint qualifications, then d∗ = p .\nA number of different constraint qualifications exist, of which the most com\nmonly invoked is Slater's condition: a primal/dual problem pair satisfy Slater's\ncondition if there exists some feasible primal solution x for which all inequality\nconstraints are strictly satisfied (i.e. gi(x) < 0, i = 1 . . . , m). In practice, nearly\nall convex problems satisfy some type of constraint qualification, and hence the\nprimal and dual problem have the same optimal value.\n\nKKT Conditions\nFor an unconstrained convex optimization problem, we know we are at the global\nminimum if the gradient is zero. The KKT conditions are the equivalent condi\ntions for the global minimum of a constrained convex optimization problem.\n∗\nIf strong duality holds and (x , α∗ , β∗) is optimal, then x ∗ minimizes L(x, α∗ , β∗)\ngiving us the first KKT condition, Lagrangian stationarity:\nm\nm\nα ∗\nβ ∗\nrxL(x, α ∗ , β ∗ )|x ∗ = rxf(x)|x ∗ +\ni rxgi(x)|x ∗ +\ni rxhi(x)|x ∗ = 0\ni\ni\nWe can interpret this condition by saying that the gradient of the objective\nfunction and constraint function must be parallel (and opposite). This concept\nis illustrated for a simple 2D optimization problem with one inequality constraint\nbelow.\nThe curves are contours of f, and the line is the constraint boundary. At x ∗, the\ngradient of f and gradient of the constraint must be parallel and opposing so\nthat we couldn't move along the constraint boundary in order to get an improved\nobjective value.\n\nOne interesting consequence of strong duality is next:\nLemma 3 (Complementary Slackness). If strong duality holds, then αi\n∗ gi(x ∗) =\n0 for each i = 1 . . . , m.\nProof. Suppose that strong duality holds.\n∗\np = d ∗ = ΘD(α ∗ , β ∗ )\n= min L(x, α ∗ , β ∗ )\nx\n∗\n≤L(x , α ∗ , β ∗ )\n∗\n≤\nmax L(x , α, β)\nα,β;αi≥0,∀i\n∗\n= ΘP (x ∗ ) = f(x ∗ ) = p .\nThe second last inequality is just from (1). This means that all the inequalities\nare actually equalities. In particular,\nm\np\nm\nm\n∗\nL(x , α ∗ , β ∗ ) = f(x ∗ ) +\nαi\n∗ gi(x ∗ ) +\nβi\n∗ hi(x ∗ ) = f(x ∗ ).\ni\ni\nSo,\nm\np\nm\nm\nαi\n∗ gi(x ∗ ) +\nβi\n∗ hi(x ∗ ) = 0.\ni\ni\n∗\n- Since x is primal feasible, each hi(x ∗) = 0, so the second terms are all 0.\n∗\n- Since αi\n∗'s are dual feasible, α∗≥ 0, and since x is primal feasible, gi(x ∗) ≤\ni\n0.\nSo each αi\n∗ gi(x ∗) ≤ 0, which means they are all 0.\nαi\n∗ gi(x ∗ ) = 0 ∀i = 1, . . . , m.\nWe can rewrite complementary slackness this way:\nαi\n∗ > 0 =⇒ gi(x ∗ ) = 0 (active constraints)\ngi(x ∗ ) < 0 =⇒\nα ∗ = 0.\ni\nIn the case of support vector machines (SVMs), active constraints are known as\nsupport vectors.\n\nWe can now characterize the optimal conditions for a primal dual optimization\npair:\n∗\nTheorem 1 Suppose that x ∈ Rn , α∗∈ Rm, and β∗∈ Rp satisfy the following\nconditions:\n- (Primal feasibility) gi(x ∗) ≤ 0, i = 1, ..., m and hi(x ∗) = 0, i = 1, ..., p.\n- (Dual feasibility) α∗≥ 0, i = 1, . . . , m.\ni\n- (Complementary Slackness) αi\n∗ gi(x ∗) = 0, i = 1, . . . , m.\n∗\n- (Lagrangian stationary)\nxL(x , α∗ , β∗) = 0.\n∗\nThen x is primal optimal and (α∗ , β∗) are dual optimal. Furthermore, if strong\n∗\nduality holds, then any primal optimal x and dual optimal (α∗ , β∗) must satisfy\nall these conditions.\nThese conditions are known as the Karush-Kuhn-Tucker (KKT) conditions.1\n1Incidentally, the KKT theorem has an interesting history. The result was originally derived by Karush in\nhis 1939 master's thesis but did not catch any attention until it was rediscovered in 1950 by two mathematicians\nKuhn and Tucker. A variant of essentially the same result was also derived by John in 1948.\n∇\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.097 Prediction: Machine Learning and Statistics\nSpring 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "15.097 Lecture 12: Support vector machines",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-097-prediction-machine-learning-and-statistics-spring-2012/39f6c97e482b96aba75c59b4ac0d99b8_MIT15_097S12_lec12.pdf",
      "content": "Support Vector Machines\nMIT 15.097 Course Notes\nCynthia Rudin\nCredit: Ng, Hastie, Tibshirani, Friedman\nThanks: S eyda Ertekin\nLet's start with some intuition about margins.\nThe margin of an example xi = \"distance\" from example to decision boundary\n= yif(xi)\nThe margin is positive if the example is on the correct side of the decision bound-\nary, otherwise it's negative.\nHere's the intuition for SVM's:\n- We want all examples to have large margins, want them to be as far from\ndecision boundary as possible.\n- That way, the decision boundary is more \"stable,\" we are confident in all\ndecisions.\n\nMost other algorithms (logistic regression, decision trees, perceptron) don't gen-\nerally produce large margins. (AdaBoost generally produces large margins.)\nAs in logistic regression and AdaBoost, function f is linear,\nm\nf(x) =\nX\nλ(j)x(j) + λ0.\nj=1\nNote that the intercept term can get swept into x by adding a 1 as the last\ncomponent of each x. Then f(x) would be just λTx but for this lecture we'll\nkeep the intercept term separately because SVM handles that term differently\nthan if you put the intercept as a separate feature. We classify x using sign(f(x)).\nIf xi has a large margin, we are confident that we classified it correctly. So we're\nessentially suggesting to use the margin yif(xi) to measure the confidence in our\nprediction.\nBut there is a problem with using yif(xi) to measure confidence in prediction.\nThere is some arbitrariness about it.\nHow? What should we do about it?\n\nSVM's maximize the distance from the decision boundary to the nearest training\nexample - they maximize the minimum margin. There is a geometric perspective\ntoo. I set the intercept to zero for this picture (so the decision boundary passes\nthrough the origin):\nThe decision boundary are x's where λTx = 0. That means the unit vector for\nλ must be perpendicular to those x's that lie on the decision boundary.\nNow that you have the intuition, we'll put the intercept back, and we have to\ntranslate the decision boundary, so it's really the set of x's where λTx = λ0.\nThe margin of example i is denoted γi:\nB is the point on the decision boundary closest to the positive example xi. B is\nλ\nB = xi -γi||λ||2\n\nsince we moved -γi units along the unit vector to get from the example to B.\nSince B lies on the decision boundary, it obeys λTx + λ0 = 0, where x is B. (I\nwrote the intercept there explicitly). So,\nλT\n\nλ\nxi -γi\n\n+ λ0 = 0\n||λ||2\nλTxi -\ni\n||λ\nγ\n||2 + λ = 0\n||λ||\nSimplifying,\nλTxi + λ0\nγi =\n||λ||2\n\n=: f(xi)\n(this is the normalized version of f)\n\n= yif(xi) since yi = 1.\nNote that here we normalized so we wouldn't have the arbitrariness in the mean-\ning of the margin.\nIf the example is negative, the same calculation works, with a few sign flips (we'd\nneed to move γi units rather than -γi units).\nSo the \"geometric\" margin from the picture is the same as the \"functional\"\n\nmargin yif(xi).\nMaximize the minimum margin\nSupport vector machines maximize the minimum margin. They would like to\nhave all examples being far from the decision boundary. So they'll choose f this\nway:\nmax max γ\ns.t.\nyif(xi) ≥γ\ni = 1 . . . m\nf\nγ\nλTxi + λ0\nmax γ\ns.t.\nyi\n≥γ\ni = 1 . . . m\nγ,λ,λ0\n||λ||2\nmax γ\ns.t.\nyi(λTxi + λ0)\nγ,λ,λ0\n≥γ ||λ||2\ni = 1 . . . m.\n\nFor any λ and λ0 that satisfy this, any positively scaled multiple satisfies them\ntoo, so we can arbitrarily set ||λ||2 = 1/γ so that the right side is 1.\nNow when we maximize γ, we're maximizing γ = 1/ ||λ||2. So we have\nmax\ns.t.\nyi(λTx\nλ,λ0 ||λ||\ni + λ0)\n≥1\ni = 1 . . . m.\nEquivalently,\nmin\nλ,λ0 2 ||λ||2\ns.t.\nyi(λTxi + λ0) -1 ≥0\ni = 1 . . . m\n(1)\n(the 1/2 and square are just for convenience) which is the same as:\nmin\n||λ||2\ns.t.\n-yi(λTxi + λ0) + 1\nλ,λ0 2\n≤0\ni = 1 . . . m\nleading to the Lagrangian\nn\nm\nL ([λ, λ0], α) =\nX\nλ(j)2 +\nX\nαi\n\n-yi(λTxi + λ0) + 1\n2 j=1\ni=1\n\nWriting the KKT conditions, starting with Lagrangian stationarity, where we\nneed to find the gradient wrt λ and the derivative wrt λ0:\nm\nm\n∇λL ([λ, λ0], α) = λ -\nX\nαiyixi = 0 =⇒λ =\nX\nαiyixi.\ni=1\ni=1\nm\nm\n∂L ([λ, λ0], α) =\nαiyi = 0 =\n∂λ0\n-\ni=1\n⇒\nαiyi = 0.\ni=1\n\nαi ≥0\nX\n∀i\n(dual feasibilit\nX\ny)\nαi -yi(λTxi + λ0) + 1\n\n= 0\nT\n∀i\n(complementary slackness)\n-yi(λ xi + λ0) + 1 ≤0.\n(primal feasibility)\n\nUsing the KKT conditions, we can simplify the Lagrangian.\nm\nm\nm\nL ([λ, λ0], α) =\n∥λ∥2\nT\n2 + λ\nX\n(-αiyixi) +\nX\n(-αiyiλ0) +\nαi\ni=1\ni=1\ni=1\n(We just expanded terms. Now we'll plug in the first KKT condition.)\nX\nm\n=\n||λ||2\n2 -|| ||2\nλ 2 -λ0\nX\nm\n(αiyi) +\ni\nX\nα\ni=1\ni=1\n(Plug in the second KKT condition.)\nm\n= -\nX\nm\nλ(j)2\n+\n+\n2 j=1\nX\nαi\n(2)\ni=1\nAgain using the first KKT condition, we can rewrite the first term.\nX\nm\nX\nm\n-\nλ(j)2 =\n2 j=1\n-2 j=1\nX\nm\n(j)\nαiyixi\ni=1\n!\nm\n= -\nX X\nm X\nm\n(j)\n(j)\nαiαkyiykxi x\nk\nj=1 i=1 k=1\nm\n= -\nX X\nm\nαiαky\nT\niykx\ni xk.\ni=1 k=1\nPlugging back into the Lagrangian (2), which now only depends on α, and\nputting in the second and third KKT conditions gives us the dual problem;\nmax\nα\nL (α)\nwhere\nX\nm\n1 X\nT\nαi\ni = 1 . . . m\nL (α) =\nαi -\nαiαkyiykx\ni xk\ns.t.\n\nm≥\n(3)\ni=1 αiyi = 0\ni=1\ni,k\nWe'll use the last two KKT conditions in what follows,\nP\nfor instance to get con-\nditions on λ0, but what we've already done is enough to define the dual problem\nfor α.\nWe can solve this dual problem. Either (i) we'd use a generic quadratic pro-\ngramming solver, or (ii) use another algorithm, like SMO, which I will discuss\n\nlater. For now, assume we solved it. So we have α1\n∗, . . . , αm\n∗. We can use the\nsolution of the dual problem to get the solution of the primal problem. We can\nplug α∗into the first KKT condition to get\nm\nλ∗=\nX\nαi\n∗yixi.\n(4)\ni=1\nWe still need to get λ∗\n0, but we can see something cool in the process.\nSupport Vectors\nLook at the complementary slackness KKT condition and the primal and dual\nfeasibility conditions:\nαi\n∗> 0 ⇒yi(λ∗Txi + λ0\n∗) = 1\nα\nT\nαi\n∗< 0 (Can't happen)\ni\n∗-yi(λ∗xi + λ0\n∗) + 1 = 0 ⇒\n\n-yi(λ∗Txi + λ∗\n0) + 1 < 0 ⇒αi\n∗= 0\n-yi(λ∗Txi + λ∗\n0) + 1 > 0 (Can't happen)\nDefine the optimal (scaled) scoring function: f ∗(x ) = λ∗T\ni\nxi + λ∗\n0, then\n\nαi\n∗> 0\n⇒yif ∗(xi) = scaled margini = 1\n1 < yif ∗(xi) ⇒\nαi\n∗= 0\nThe examples in the first category, for which the scaled margin is 1 and the\nconstraints are active are called support vectors. They are the closest to the\ndecision boundary.\n\nFinish What We Were Doing Earlier\nTo get λ∗\n0, use the complementarity condition for any of the support vectors (in\nother words, use the fact that the unnormalized margin of the support vectors\nis one):\n1 = yi(λ∗Txi + λ∗\n0).\nIf you take a positive support vector, yi = 1, then\nλ∗= 1 -λ∗T\nxi.\nWritten another way, since the support vectors have the smallest margins,\nλ∗\n0 = 1 -min λ∗Txi.\ni:yi=1\nSo that's the solution! Just to recap, to get the scoring function f ∗for SVM,\nyou'd compute α∗from the dual problem (3), plug it into (4) to get λ∗, plug that\ninto the equation above to get λ∗\n0, and that's the solution to the primal problem,\nand the coefficients for f ∗.\nGraph with supp\nor\nt\nv\nectors indicated.\nImage by MIT OpenCourseWare.\n\nBecause of the form of the solution:\nm\nλ∗=\nX\nαi\n∗yixi.\ni=1\nit is possible that λ∗is very fast to calculate.\nWhy is that? Think support vectors.\nThe Nonseparable Case\nIf there is no separating hyperplane,\nthere is no feasible solution to the problem we wrote above. Most real problems\nare nonseparable.\nLet's fix our SVM so it can accommodate the nonseparable case. The new for-\nmulation will penalize mistakes the farther they are from the decision boundary.\nSo we are allowed to make mistakes now, but we pay a price.\n\nLet's change our primal problem (1) to this new primal problem:\nm\ny λT\nX\ni(\nxi + λ0) ≥1\nξ\nmin\nλ,λ0,ξ 2 ||λ||2 + C\nξi\ns.t.\n-\ni\n(5)\nξi ≥0\ni=1\nSo the constraints allow some slack of size ξi, but we pay a price for it in the\nobjective. That is, if yif(xi) ≥1 then ξi gets set to 0, penalty is 0. Otherwise,\nif yif(xi) = 1 -ξi, we pay price ξi.\nParameter C trades offbetween the twin goals of making the\nλ\n|| ||2 small (making\nwhat-was-the-minimum-margin 1/ ||λ||2 large) and ensuring that most examples\nhave margin at least 1/ ||λ||2.\nGoing on a Little Tangent\nRewrite the penalty another way:\nIf yif(xi) ≥1, zero penalty. Else, pay price ξi = 1 -yif(xi)\nThird time's the charm:\nPay price ξi = ⌊1 -yif(xi)⌋+\nwhere this notation ⌊z⌋+ means take the maximum of z and 0.\nEquation (5) becomes:\nm\nmin\n||λ||2\n2 + C\nX\n⌊1 -yif(xi)\nλ,λ0 2\ni=1\n⌋+\nDoes that look familiar?\n\nThe Dual for the Nonseparable Case\nForm the Lagrangian of (5):\nm\nm\nm\nL(λ\n, α, r) = 2 ||λ||2\n, b, ξ\nT\n2 + C\nX\nξi\nαi yi(λ xi + λ0)\n1 + ξi\nriξi\ni=1\n-\nX\ni=1\n\n-\n\n-\nX\ni=1\nwhere αi's and ri's are Lagrange multipliers (constrained to be ≥0). The dual\nturns out to be (after some work)\nm\nmax\nX\nαi -\nX\nm\nα\nC\ni =\n. .\nαiαkyiykxT\n1 .\nm\ni xk\ns.t.\n≤\ni ≤\nα\ni=1\ni,k=1\n\nm\n(6)\ni=1 αiyi = 0\nSo the only difference from the original problem'\nP\ns Lagrangian (3) is that 0 ≤αi\nwas changed to 0 ≤αi ≤C. Neat!\nSolving the dual problem with SMO\nSMO (Sequential Minimal Optimization) is a type of coordinate ascent algo-\nrithm, but adapted to SVM so that the solution always stays within the feasible\nregion.\nStart with (6). Let's say you want to hold α2, . . . , αm fixed and take a coordinate\nstep in the first direction. That is, change α1 to maximize the objective in (6).\nCan we make any progress? Can we get a better feasible solution by doing this?\nm\nTurns out, no. Look at the constraint in (6),\ni=1 αiyi = 0. This means:\nm\nα\nα\nP\n1y1 = -\nX\niyi, or multiplying by y1,\ni=2\nm\nα1 = -y1\nX\nαiyi.\ni=2\nSo, since α2, . . . , αm are fixed, α1 is also fixed.\n\nSo, if we want to update any of the αi's, we need to update at least 2 of them\nsimultaneously to keep the solution feasible (i.e., to keep the constraints satis-\nfied).\nStart with a feasible vector α. Let's update α1 and α2, holding α3, . . . , αm fixed.\nWhat values of α1 and α2 are we allowed to choose?\nAgain, the constraint is: α1y1 + α2y2 = -\ni=3 αiyi =: ζ (fixed constant).\nm\nP\nWe are only allowed to choose α1, α2 on the line, so when we pick α2, we get α1\nautomatically, from\nα1 =\n(ζ\ny1\n-α2y2)\n= y1(ζ -α2y2)\n(y1 = 1/y1 since y1 ∈{+1, -1}).\nAlso, the other constraints in (6) say 0 ≤α1, α2 ≤C. So, α2 needs to be within\n[L,H] on the figure (in order for α1 to stay within [0, C]), where we will always\nhave 0 ≤L,H ≤C. To do the coordinate ascent step, we will optimize the\nobjective over α2, keeping it within [L,H]. Intuitively, (6) becomes:\nmax\n\nα1 + α2 + constants -\nX\nαiαkyiykxT\ni xk\nwhere α1 = y1(ζ\nα2y2).\nα2∈[L,H]\ni,k\n\n-\n(7)\nThe objective is quadratic in α2. This means we can just set its derivative to 0\nto optimize it and get α2 for the next iteration of SMO. If the optimal value is\n\noutside of [L,H], just choose α2 to be either L or H for the next iteration.\nFor instance, if this is a plot of (7)'s objective (sometimes it doesn't look like\nthis, sometimes it's upside-down), then we'll choose :\nNote: there are heuristics to choose the order of αi's chosen to update.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.097 Prediction: Machine Learning and Statistics\nSpring 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "15.097 Lecture 13: Kernels",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-097-prediction-machine-learning-and-statistics-spring-2012/c80897854282a1fb65c3c99488f6ae50_MIT15_097S12_lec13.pdf",
      "content": "Kernels\nMIT 15.097 Course Notes\nCynthia Rudin\nCredits: Bartlett, Sch olkopf and Smola, Cristianini and Shawe-Taylor\nThe kernel trick that I'm going to show you applies much more broadly than\nSVM, but we'll use it for SVM's.\nWarning: This lecture is technical. Try to get the basic idea even if you don't\ncatch all the details.\nBasic idea: If you can't separate positives from negatives in a low-dimensional\nspace using a hyperplane, then map everything to a higher dimensional space\nwhere you can separate them.\nThe picture looks like this but it's going to be a little confusing at this point:\nor it might look like this:\nT\nw\no\n\ng\nr\na\np\nh\ns\ncon\nnect\ned b\ny a\ndott\ned l\nine,\nsho\nwing a low-dimensional space mapped to a higher dimensional space.\nImage by MIT OpenCourseWare.\n\nSay I want to predict whether a house on the real-estate market will sell today\nor not:\nx =\n\nx(1)\n|{z}\nhouse's list price\n,\nx(2)\n|{z}\nestimated worth\n,\nx(3)\n|{z}\nlength of time on market\n,\nx(4)\n|{z}\nin a good area\n, ...\n\n.\nWe might want to consider something more complicated than a linear model:\nExample 1: [x(1), x(2)] →Φ\n[x(1), x(2)]\n\n=\n\nx(1)2, x(2)2, x(1)x(2)\nThe 2d space gets mapped to a 3d space. We could have the inner product in\nthe 3d space:\nΦ(x)TΦ(z) = x(1)2z(1)2 + x(2)2z(2)2 + x(1)x(2)z(1)z(2).\nExample 2:\n[x(1), x(2), x(3)] →Φ\n\n[x(1), x(2), x(3)]\n\n= [x(1)2, x(1)x(2), x(1)x(3), x(2)x(1), x(2)2, x(2)x(3), x(3)x(1), x(3)x(2), x(3)2]\nand we can take inner products in the 9d space, similarly to the last example.\n2-dimension\nal input space and 3-dimensional feature space.\nImage by MIT OpenCourseWare.\n\nRather than apply SVM to the original x's, apply it to the Φ(x)'s instead.\nThe kernel trick is that if you have an algorithm (like SVM) where the examples\nappear only in inner products, you can freely replace the inner product with a dif-\nferent one. (And it works just as well as if you designed the SVM with some map\nΦ(x) in mind in the first place, and took the inner product in Φ's space instead.)\nRemember the optimization problem for SVM?\nmax\nα\nm\nX\ni=1\nαi -1\nm\nX\ni,k=1\nαiαjyiyk xT\ni xk\n←inner product\ns.t. 0 ≤αi ≤C, i = 1, ..., m and\nm\nX\ni=1\nαiyi = 0\nYou can replace this inner product with another one, without even knowing Φ.\nIn fact there can be many different feature maps that correspond to the same\ninner product.\nIn other words, we'll replace xTz (i.e., ⟨x, z⟩Rn) with k(xi, xj), where k happens\nto be an inner product in some feature space, ⟨Φ(x), Φ(x)⟩Hk. Note that k is\nalso called the kernel (you'll see why later).\nExample 3: We could make k(x, z) the square of the usual inner product:\nk(x, z) = ⟨x, z⟩2\nRn =\nn\nX\nj=1\nx(j)z(j)\n!2\n=\nn\nX\nj=1\nn\nX\nk=1\nx(j)x(k)z(j)z(k).\nBut how do I know that the square of the inner product is itself an inner product\nin some feature space? We'll show a general way to check this later, but for now,\nlet's see if we can find such a feature space.\nWell, for n = 2, if we used Φ\n[x(1), x(2)]\n\n=\n\nx(1)2, x(2)2, x(1)x(2), x(2)x(1)\nto map\ninto a 4d feature space, then the inner product would be:\nΦ(x)TΦ(z) = x(1)2z(1)2 + x(2)2z(2)2 + 2x(1)x(2)z(1)z(2) = ⟨x, z⟩2\nR2.\n\nSo we showed that k is an inner product for n = 2 because we found a feature\nspace corresponding to it.\nFor n = 3 we can also find a feature space, namely the 9d feature space from\nExample 2 would give us the inner product k.\nThat is,\nΦ(x) = (x(1)2, x(1)x(2), ..., x(3)2), and Φ(z) = (z(1)2, z(1)z(2), ..., z(3)2),\n⟨Φ(x), Φ(z)⟩R9 = ⟨x, z⟩2\nR3.\nThat's nice.\nWe can even add a constant, so that k is the inner product plus a constant\nsquared.\nExample 4:\nk(x, z) = (xTz + c)2 =\nn\nX\nj=1\nx(j)z(j) + c\n! n\nX\nl=1\nx(l)z(l) + c\n!\n=\nn\nX\nj=1\nn\nX\nl=1\nx(j)x(l)z(j)z(l) + 2c\nn\nX\nj=1\nx(j)z(j) + c2\n=\nn\nX\nj,l=1\n(x(j)x(l))(z(j)z(l)) +\nn\nX\nj=1\n(\n√\n2cx(j))(\n√\n2cz(j)) + c2,\nand in n = 3 dimensions, one possible feature map is:\nΦ(x) = [x(1)2, x(1)x(2), ..., x(3)2,\n√\n2cx(1),\n√\n2cx(2),\n√\n2cx(3), c]\nand c controls the relative weight of the linear and quadratic terms in the inner\nproduct.\nEven more generally, if you wanted to, you could choose the kernel to be any\nhigher power of the regular inner product.\nExample 5: For any integer d ≥2\nk(x, z) = (xTz + c)d,\n\nwhere the feature space Φ(x) will be of degree\nn+d\nd\n\n, with terms for all monomi-\nals up to and including degree d. The decision boundary in the feature space (of\ncourse) is a hyperplane, whereas in the input space it's a polynomial of degree\nd. Now do you understand that figure at the beginning of the lecture?\nBecause these kernels give rise to polynomial decision boundaries, they are called\npolynomial kernels. They are very popular.\nBeyond these examples, it is possible to construct very complicated kernels, even\nones that have infinite dimensional feature spaces, that provide a lot of modeling\npower:\nCourtesy of Dr. Hagen Knaf. Used with permission.\n\nSVMs with these kinds of fancy kernels are among the most powerful ML algo-\nrithms currently (a lot of people say the most powerful).\nBut the solution to the optimization problem is still a simple linear combination,\neven if the feature space is very high dimensional.\nHow do I evaluate f(x) for a test example x then?\nIf we're going to replace xT\ni xk everywhere with some function of xi and xk that is\nhopefully an inner product from some other space (a kernel), we need to ensure\nthat it really is an inner product. More generally, we'd like to know how to\nconstruct functions that are guaranteed to be inner products in some space. We\nneed to know some functional analysis to do that.\n(c) mlpy Developers. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/fairuse.\n\nRoadmap\n1. make some definitions (inner product, Hilbert space, kernel)\n2. give some intuition by doing a calculation in a space with a finite number\nof states\n3. design a general Hilbert space whose inner product is the kernel\n4. show it has a reproducing property - now it's a Reproducing Kernel Hilbert\nspace\n5. create a totally different representation of the space, which is a more intu-\nitive to express the kernel (similar to the finite state one)\n6. prove a cool representer theorem for SVM-like algorithms\n7. show you some nice properties of kernels, and how you might construct them\nDefinitions\nAn inner product takes two elements of a vector space X and outputs a number.\nAn inner product could be a usual dot product: ⟨u, v⟩= u′v = P\ni u(i)v(i), or\nit could be something fancier. An inner product ⟨·, ·⟩must satisfy the following\nconditions:\n1. Symmetry\n⟨u, v⟩= ⟨v, u⟩∀u, v ∈X\n2. Bilinearity\n⟨αu + βv, w⟩= α⟨u, w⟩+ β⟨v, w⟩∀u, v, w ∈X, ∀α, β ∈R\n3. Strict Positive Definiteness\n⟨u, u⟩≥0 ∀x ∈X\n⟨u, u⟩= 0 ⇐⇒u = 0.\n\nAn inner product space (or pre-Hilbert space) is a vector space together with an\ninner product.\nA Hilbert space is a complete inner product space. ('Complete' means sequences\nconverge to elements of the space - there aren't any \"holes\" in the space.)\nExamples of Hilbert spaces include:\n- The vector space Rn with ⟨u, v⟩Rn = uTv, the vector dot product of u and\nv.\n- The space l2 of square summable sequences, with inner product ⟨u, v⟩l2 =\nPinf\ni=1 uivi\n- The space L2(X, μ) of square integrable functions, that is, functions f such\nthat\nR\nf(x)2dμ(x) < inf, with inner product ⟨f, g⟩L2(X,μ) =\nR\nf(x)g(x)dμ(x).\nFinite States\nSay we have a finite input space {x1, ..., xm}. So there's only m possible states for\nthe xi's. (Think of the bag-of-words example where there are 2(#words) possible\nstates.) I want to be able to take inner products between any two of them using\nmy function k as the inner product. Inner products by definition are symmetric,\nso k(xi, xj) = k(xj, xi). In other words, in order for us to even consider k as a\nvalid kernel function, the matrix:\nneeds to be symmetric, and this means we can diagonalize it, and the eigende-\ncomposition takes this form:\nK = VΛV′\nwhere V is an orthogonal matrix where the columns of V are eigenvectors, vt,\nand Λ is a diagonal matrix with eigenvalues λt on the diagonal. This fact (that\n\nreal symmetric matrices can always be diagonalized) isn't difficult to prove, but\nrequires some linear algebra.\nFor now, assume all λt's are nonnegative, and consider this feature map:\nΦ(xi) = [\np\nλ1v(i)\n1 , ...,\np\nλtv(i)\nt , ...,\np\nλmv(i)\nm ].\n(writing it for xj too):\nΦ(xj) = [\np\nλ1v(j)\n1 , ...,\np\nλtv(j)\nt , ...,\np\nλmv(j)\nm ].\nWith this choice, k is just a dot product in Rm:\n⟨Φ(xi), Φ(xj)⟩Rm =\nm\nX\nt=1\nλtv(i)\nt v(j)\nt\n= (VΛV′)ij = Kij = k(xi, xj).\nWhy did we need the λt's to be nonnegative? Say λs < 0. Form a point in\nfeature space that is a special linear combination of the Φ(xi)'s:\nz =\nm\nX\ni=1\nv(i)\ns Φ(xi).\n(coeffs are elements of vs)\nThen calculate\n∥z∥2\n2 = ⟨z, z⟩Rm =\nX\ni\nX\nj\nv(i)\ns Φ(xi)TΦ(xj) v(j)\ns\n=\nX\ni\nX\nj\nv(i)\ns Kijv(j)\ns\n= vT\ns Kvs = λs < 0\nwhich conflicts with the geometry of the feature space.\nWe just showed that if k has any chance of being an inner product in a feature\nspace, then matrix K needs to be positive semi-definite (needs to have non-\nnegative eigenvalues).\nIn fact, we'll just define kernels in the first place to be positive semi-definite,\nsince they can't be inner products without that. In the infinite state case, we\ncan't write out a Gram matrix (like K in the finite state case) for the whole\nspace, because the x's can take infinitely many values. We'll just get to pick m\nexamples - we want to make sure the Gram matrix for those examples is positive\nsemi-definite, no matter what examples we get!\nLet us officially define a kernel. A function k : X × X →R is a kernel if\n\n- k is symmetric: k(x, y) = k(y, x).\n- k gives rise to a positive semi-definite \"Gram matrix,\" i.e., for any m ∈N\nand any x1, ..., xm chosen from X, the Gram matrix K defined by Kij =\nk(xi, xj) is positive semidefinite.\nAnother way to show that a matrix K is positive semi-definite is to show that\n∀c ∈Rm, cTKc ≥0.\n(1)\n(This is equivalent to all the eigenvalues being nonnegative, which again is not\nhard to show but requires some calculations.)\nHere are some nice properties of k:\n- k(u, u) ≥0 (Think about the Gram matrix of m = 1.)\n- k(u, v) ≤\np\nk(u, u)k(v, v) (This is the Cauchy-Schwarz inequality.)\nThe second property is not hard to show for m = 2. The Gram matrix\nK =\nk(u, u) k(u, v)\nk(v, u) k(v, v)\n\nis positive semi-definite whenever cTKc ≥0 ∀c. Choose in particular\nc =\nk(v, v)\n-k(u, v)\n\n.\nThen since K is positive semi-definite,\n0 ≤cTKc = [k(v, v)k(u, u) -k(u, v)2]k(v, v)\n(where I skipped a little bit of simplifying in the equality) so we must then have\nk(v, v)k(u, u) ≥k(u, v)2. That's it!\nBuilding a Rather Special Hilbert Space\nDefine RX := {f : X →R}, the space of functions that map X to R. Let's\ndefine the feature map Φ : X →RX so that it maps x to k(·, x) :\nΦ : x 7-→k(·, x).\nSo, Φ(x) is a function, and for a point z ∈X, the function assigns k(z, x) to it.\n\nSo we turned each x into a function on the domain X. Those functions could be\nthought of as infinite dimensional vectors. These functions will be elements of\nour Hilbert space.\nWe want to have k be an inner product in feature space. To do this we need to:\n1. Define feature map Φ : X →RX, which we've done already. Then we need\nto turn the image of Φ into a vector space.\n2. Define an inner product ⟨, ⟩Hk.\n3. Show that the inner product satisfies:\nk(x, z) = ⟨Φ(x), Φ(z)⟩Hk.\nThat'll make sure that the feature space is a pre-Hilbert space.\nLet's do the rest of step 1. Elements in the vector space will look like this, they'll\nbe in the span of the Φ(xi)'s:\nf(·) =\nm\nX\ni=1\nαik(·, xi) ←\"vectors\"\nwhere m, αi and x1...xm ∈X can be anything. (We have addition and multipli-\ncation, so it's a vector space.) The vector space is:\nspan ({Φ(x) : x ∈X}) =\n(\nf(·) =\nm\nX\ni=1\nαik(·, xi) : m ∈N, xi ∈X, αi ∈R\n)\n.\nMapp\ni\nng x,\n\nshown as a dot, to phi(x), shown as a bell curve.\nImage by MIT OpenCourseWare.\n\nFor step 2, let's grab functions f(·) = Pm\ni=1 αik(·, xi) and g(·) = Pm′\nj=1 βjk(·, x′\nj),\nand define the inner product:\n⟨f, g⟩Hk =\nm\nX\ni=1\nm′\nX\nj=1\nαiβjk(xi, x′\nj).\nIs it well-defined?\nWell, it's symmetric, since k is symmetric:\n⟨g, f⟩Hk =\nm′\nX\nj=1\nm\nX\ni=1\nβjαik(x′\nj, xi) = ⟨f, g⟩Hk.\nIt's also bilinear, look at this:\n⟨f, g⟩Hk =\nm′\nX\nj=1\nβj\nm\nX\ni=1\nαik(xi, x′\nj) =\nm′\nX\nj=1\nβjf(x′\nj)\nso we have\n⟨f1 + f2, g⟩Hk =\nm′\nX\nj=1\nβj\nf1(x′\nj) + f2(x′\nj)\n\n=\nm′\nX\nj=1\nβjf1(x′\nj) +\nm′\nX\nj=1\nβjf2(x′\nj)\n= ⟨f1, g⟩Hk + ⟨f2, g⟩Hk (look just above)\n(We can do the same calculation to show ⟨f, g1 + g2⟩Hk = ⟨f, g1⟩Hk + ⟨f, g2⟩Hk.)\nSo it's bilinear.\nIt's also positive semi-definite, since k gives rise to positive semi-definite Gram\nmatrices. To see this, for function f,\n⟨f, f⟩Hk =\nm\nX\nij=1\nαiαjk(xi, xj) = αTKα\nand we said earlier in (1) that since K is positive semi-definite, it means that for\nany αi's we choose, the sum above is ≥0.\n\nSo, k is almost a inner product! (What's missing?) We'll get to that in a minute.\nFor step 3, something totally cool happens, namely that the inner product of\nk(·, x) and f is just the evaluation of f at x.\n⟨k(·, x), f⟩Hk = f(x).\nHow did that happen?\nAnd then this is a special case:\n⟨k(·, x), k(·, x′)⟩Hk = k(x, x′).\nThis is why k is called a reproducing kernel, and the Hilbert space is a RKHS.\nOops!\nWe forgot to show that one last thing, which is that ⟨, ⟩Hk is strictly\npositive definite, so that it's an inner product. Let's use the reproducing property.\nFor any x,\n|f(x)|2 = |⟨k(·, x), f⟩Hk|2 ≤⟨k(·, x), k(·, x)⟩Hk · ⟨f, f⟩Hk = k(x, x)⟨f, f⟩Hk\nwhich means that ⟨f, f⟩Hk = 0 ⇒f = 0 for all x. Ok, it's positive definite, and\nthus, an inner product.\nNow we have an inner product space. In order to make it a Hilbert space, we\nneed to make it complete. The completion is actually not a big deal, we just\ncreate a norm:\n∥f∥Hk =\np\n⟨f, f⟩Hk\nand just add to Hk the limit points of sequences that converge in that norm.\nOnce we do that,\nHk = {f : f =\nX\ni\nαik(·, xi)}\nis a Reproducing Kernel Hilbert space (RKHS).\nHere's a formal (simplified) definition of RKHS:\n\nWe are given a (compact) X ⊆Rd and a Hilbert space H of functions f : X →R.\nWe say H is a Reproducing Kernel Hilbert Space if there exists a k : X →R,\nsuch that\n1. k has the reproducing property, i.e., f(x) = ⟨f(·), k(·, x)⟩H\n2. k spans H, that is, H = span{k(·, x) : x ∈X}\nSo when we do the kernel trick, we could think that we're implicitly mapping x\nto f = P\ni αik(·, x) in the RKHS Hk. Neat, huh?\nThe RKHS we described is the one from the Moore-Aronszajn Theorem (1950)\nthat states that for every positive definite function k(·, ·) there exists a unique\nRKHS.\nWe could stop there, but we won't. We're going to define another Hilbert space\nthat has a one-to-one mapping (an isometric isomorphism) to the first one.\nMercer's Theorem\nThe inspiration of the name \"kernel\" comes from the study of integral operators,\nstudied by Hilbert and others. Function k which gives rise to an operator Tk via:\n(Tkf)(x) =\nZ\nX\nk(x, x′)f(x′)dx′\nis called the kernel of Tk.\nThink about an operator Tk : L2(X) →L2(X). If you don't know this notation,\ndon't worry about it. Just think about Tk eating a function and spitting out\nanother one. Think of Tk as an infinite matrix, which maps infinite dimensional\nvectors to other infinite dimensional vectors. Remember earlier we analyzed the\nfinite state case? It's just like that.1\nAnd, just like in the finite state case, because Tk is going to be positive semi-\ndefinite, it has an eigen-decomposition into eigenfunctions and eigenvalues that\n1If you know the notation, you'll see that I'm missing the measure on L2 in the notation. Feel free to put it\nback in as you like.\n\nare nonnegative.\nThe following theorem from functional analysis is very similar to what we proved\nin the finite state case. This theorem is important - it helps people construct\nkernels (though I admit myself I find the other representation more helpful).\nBasically, the theorem says that if we have a kernel k that is positive (defined\nsomehow), we can expand k in terms of eigenfunctions and eigenvalues of a pos-\nitive operator that comes from k.\nMercer's Theorem (Simplified) Let X be a compact subset of Rn. Suppose k\nis a continuous symmetric function such that the integral operator Tk : L2(X) →\nL2(X) defined by\n(Tkf)(·) =\nZ\nX\nk(·, x)f(x)dx\nis positive; which here means ∀f ∈L2(X),\nZ\nX×X\nk(x, z)f(x)f(z)dxdz ≥0,\nthen we can expand k(x, z) in a uniformly convergent series in terms of Tk's\neigenfunctions ψj ∈L2(X), normalized so that ∥ψ∥L2 = 1, and positive associated\neigenvalues λj ≥0,\nk(x, z) =\ninf\nX\nj=1\nλjψj(x)ψj(z).\nThe definition of positive semi-definite here is equivalent to the ones we gave\nearlier.2\nSo this looks just like what we did in the finite case. So we could define a feature\nmap as in the finite case this way:\nΦ(x) = [\np\nλ1ψ1(x), ...,\np\nλjψj(x), ...].\n2To see the equivalence, for this direction ⇐, choose f as a weighted sum of δ functions at each x.\nFor\nthis direction ⇒, say that\nR\nX×X k(x, z)f(x)f(z)dxdz ≥0 doesn't hold for some f and show the contrapositive.\nApproximate the integral with a finite sum over a mesh of inputs {x1, ..., xm} chosen sufficiently finely, then let v\nbe the values of f on the mesh, and as long as the mesh is fine enough, we'll have v′Kv < 0, so K isn't positive\nsemi-definite.\n\nSo that's a cool property of the RKHS that we can define a feature space ac-\ncording to Mercer's Theorem. Now for another cool property of SVM problems,\nhaving to do with RKHS.\nRepresenter Theorem\nRecall that the SVM optimization problem can be expressed as follows:\nf ∗= argminf∈HkRtrain(f)\nwhere\nRtrain(f) :=\nm\nX\ni=1\nhingeloss(f(xi), yi) + C∥f∥2\nHk.\nOr, you could even think about using a generic loss function:\nRtrain(f) :=\nm\nX\ni=1\nl(f(xi), yi) + C∥f∥2\nHk.\nThe following theorem is kind of a big surprise. It says that the solutions to any\nproblem of this type - no matter what the loss function is! - all have a solution\nin a rather nice form.\nRepresenter Theorem (Kimeldorf and Wahba, 1971, Simplified) Fix a set X,\na kernel k, and let Hk be the corresponding RKHS. For any function l: R2 →R,\nthe solutions of the optimization problem:\nf ∗∈argminf∈Hk\nm\nX\ni=1\nl(f(xi), yi) + ∥f∥2\nHk\ncan all be expressed in the form:\nf ∗=\nm\nX\ni=1\nαik(xi, ·).\nThis shows that to solve the SVM optimization problem, we only need to solve\nfor the αi, which agrees with the solution from the Lagrangian formulation for\nSVM. It says that even if we're trying to solve an optimization problem in an\ninfinite dimensional space Hk containing linear combinations of kernels centered\n\non arbitrary x′\nis, then the solution lies in the span of the m kernels centered on\nthe xi's.\nI hope you grasp how cool this is!\nProof. Suppose we project f onto the subspace:\nspan{k(xi, ·) : 1 ≤i ≤m}\nobtaining fs (the component along the subspace) and f⊥(the component per-\npendicular to the subspace). We have:\nf = fs + f⊥⇒∥f∥2\nHk = ∥fs∥2\nHk + ∥f⊥∥2\nHk ≥∥fs∥2\nHk.\nThis implies that ∥f∥Hk is minimized if f lies in the subspace. Furthermore,\nsince the kernel k has the reproducing property, we have for each i:\nf(xi) = ⟨f, k(xi, ·)⟩Hk = ⟨fs, k(xi, ·)⟩Hk+⟨f⊥, k(xi, ·)⟩Hk = ⟨fs, k(xi, ·)⟩Hk = fs(xi).\nSo,\nm\nX\ni=1\nl(f(xi), yi) =\nm\nX\ni=1\nl(fs(xi), yi).\nIn other words, the loss depends only on the component of f lying in the subspace.\nTo minimize, we can just let ∥f⊥∥Hk be 0 and we can express the minimizer as:\nf ∗(·) =\nm\nX\ni=1\nαik(xi, ·).\nThat's it! ■\nDraw some bumps on xi's\nConstructing Kernels\nLet's construct new kernels from previously defined kernels. Suppose we have k1\nand k2. Then the following are also valid kernels:\n1. k(x, z) = αk1(x, z) + βk2(x, z) for α, β ≥0\n\nProof. k1 has its feature map Φ1 and inner product ⟨⟩Hk1 and k2 has its\nfeature map Φ2 and inner product ⟨⟩Hk2. By linearity, we can have:\nαk1(x, z) = ⟨√αΦ1(x), √αΦ1(z)⟩Hk1 and βk2(x, z) = ⟨\np\nβΦ2(x),\np\nβΦ2(z)⟩Hk2\nThen:\nk(x, z)\n=\nαk1(x, z) + βk2(x, z)\n=\n⟨√αΦ1(x), √αΦ1(z)⟩Hk1 + ⟨\np\nβΦ2(x),\np\nβΦ1(z)⟩Hk2\n=: ⟨[√αΦ1(x),\np\nβΦ2(x)], [√αΦ1(z),\np\nβΦ2(z)]⟩Hnew\nand that means that k(x, z) can be expressed as an inner product.\n2. k(x, z) = k1(x, z)k2(x, z)\nProof omitted - it's a little fiddly.\n3. k(x, z) = k1(h(x), h(z)), where h : X →X.\nProof. Since h is a transformation in the same domain, k is simply a different\nkernel in that domain:\nk(x, z) = k1(h(x), h(z)) = ⟨Φ(h(x)), Φ(h(z))⟩Hk1 =: ⟨Φh(x), Φh(z)⟩Hnew\n4. k(x, z) = g(x)g(z) for g : X →R.\nProof omitted, again this one is a little tricky.\n5. k(x, z) = h(k1(x, z)) where h is a polynomial with positive coefficients\nProof. Since each polynomial term is a product of kernels with a positive\ncoefficient, the proof follows by applying 1 and 2.\n6. k(x, z) = exp(k1(x, z))\nProof Since:\nexp(x) = lim\ni→inf\n\n1 + x + · · · + xi\ni!\n\nthe proof basically follows from 5.\n7. k(x, z) = exp\n-∥x-z∥2\nl2\nσ2\n\n\"Gaussian kernel\"\n\nProof.\nk(x, z) = exp\n\n-∥x -z∥2\nl2\nσ2\n!\n= exp\n\n-∥x∥2\nl2 -∥z∥2\nl2 + 2xTz\nσ2\n!\n=\n\nexp\n-∥x∥l2\nσ2\n\nexp\n-∥z∥l2\nσ2\n\nexp\n2xTz\nσ2\n\ng(x)g(z) is a kernel according to 4, and exp(k1(x, z)) is a kernel according\nto 6. According to 2, the product of two kernels is a valid kernel.\nNote that the Gaussian kernel is translation invariant, since it only depends on\nx -z.\nThis picture helps with the intuition about Gaussian kernels if you think of the\nfirst representation of the RKHS I showed you:\nΦ(x) = k(x, ·) = exp\n\n-∥x -·∥2\nl2\nσ2\n!\nGaussian kernels can be very powerful:\nMapp\ni\nng x,\n\nshown as a dot, to phi(x), shown as a bell curve.\nImage by MIT OpenCourseWare.\n\nDon't make the σ2 too small or you'll overfit!\nBernhard Scholkopf, Christopher J.C. Burges, and Alexander J. Smola, ADVANCES IN KERNEL\nMETHODS: SUPPORT VECTOR LEARNING, published by the MIT Press. Used with permission.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.097 Prediction: Machine Learning and Statistics\nSpring 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "15.097 Lecture 14: Statistical learning theory",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-097-prediction-machine-learning-and-statistics-spring-2012/3f3332b76e8248226fb2285b91cfc6db_MIT15_097S12_lec14.pdf",
      "content": "Introduction to Statistical Learning Theory\nMIT 15.097 Course Notes\nCynthia Rudin\nCredit: A large part of this lecture was taken from an introduction to learning\ntheory of Bousquet, Boucheron, Lugosi\nNow we are going to study, in a probabilistic framework, the properties of learning\nalgorithms. At the beginning of the semester, I told you that it was important\nfor our models to be \"simple\" in order to be able to generalize, or learn from\ndata. I didn't really say that precisely before, but in this lecture I will.\nGeneralization = Data + Knowledge\nFinite data cannot replace knowledge. Knowledge allows you to choose a simpler\nset of models.\nPerhaps surprisingly, there is no one universal right way to measure simplicity or\ncomplexity of a set of models - simplicity is not an absolute notion. But we'll give\nseveral precise ways to measure this. And we'll precisely show how our ability\nto learn depends on the simplicity of the models. So we'll make concrete (via\nproof) this philosophical argument that learning somehow needs simplicity.\nIn classical statistics, the number of parameters in the model is the usual mea-\nsure of complexity. Here we'll use other complexity measures, namely the Growth\nFunction and VC dimension (which is a beautiful combinatorial quantity), cov-\nering number (the one I usually use), and Rademacher average.\nAssumptions\nTraining and test data are drawn iid from the same distribution. If there's no\nrelationship between training and test, there's no way to learn of course. (That's\nlike trying to predict rain in Africa next week using data about horse-kicks in\nthe Prussian war) so we have to make some assumption.\n\nEach learning algorithm encodes specific knowledge (or a specific assumption,\nperhaps about what the optimal classifier must look like) and works best when\nthis assumption is satisfied by the problem to which it is applied.\nNotation\nInput space X, output space Y = {-1, 1}, unknown distribution D on X × Y.\nWe observe m iid pairs {(xi, yi)}m\ni=1 drawn iid from D. The goal is to construct\na function f : X →Y that predicts y from x.\nWe would like the true risk to be as small as possible, where the true risk is:\nRtrue(f) := P(X,Y ) D(f(X) = Y ) = E\n∼\n(X,Y )∼D[1f(X)=Y ].\nDid you recognize this nice thing that comes from the definition of expecta-\ntion and probability? We can flip freely between notation for probability and\nexpectation.\nPZ\nD(Z = blah) =\nX\n1[outcome=blah]PZ\nD(Z = outcome) = E\n∼\n∼\nZ∼D1[Z=blah].\noutcomes\nWe introduce the regression function\nη(x) = E(X,Y ) D(Y |X = x)\n∼\nand the target function (or Bayes classifier)\nt(x) = sign η(x).\nThink of the distribution D, which looks sort of like this:\n\nHere's the function η:\nNow take the sign of it:\nAnd that's t:\n\nThe target function achieves the minimum risk over all possible measurable func-\ntions:\nRtrue(t) = inf Rtrue(f).\nf\nWe denote the value Rtrue(t) by R∗, called the Bayes Risk.\nOur goal is to identify this function t but since D is unknown, we cannot evaluate\nt at any x.\nThe empirical risk that we can measure is:\nm\nemp\nR\n(f) =\ny\nm\nX\n1[f(xi)= i].\ni=1\nAlgorithm\nMost of the calculations don't depend on a specific algorithm, but you can think\nof using regularized empirical risk minimization.\nf\n∈argmin\nRemp(f) + C∥f∥2\nm\nf∈F\nfor some norm. The regularization term will control the complexity of the model\nto prevent overfitting. The class of functions that we're working with is F.\n\nBounds\nRemember, we can compute fm and Remp(fm), but we cannot compute things\nlike Rtrue(fm).\nThe algorithm chooses fm from the class of functions F. Let us call the best\nfunction in the class f ∗, so that\nRtrue(f ∗) = inf Rtrue(f).\nf∈F\nThen, I would like to know how far Rtrue(fm) is from R∗. How bad is the function\nwe chose, compared to the best one, the Bayes Risk?\nRtrue(fm) -R∗=\n[Rtrue(f ∗) -R∗]\n+\n[Rtrue(fm) -Rtrue(f ∗)]\n=\nApproximation Error\n+\nEstimation Error .\nThe Approximation Error measures how well functions in F can approach the\ntarget (it would be zero if t ∈F). The Estimation Error is a random quantity\n(it depends on data) and measures how close is fm to the best possible choice in\nF.\nDraw Approximation Error and Estimation Error\nFiguring out the Approximation Error is usually difficult because it requires\nknowledge about the target, that is, you need to know something about the\ndistribution D. In Statistical Learning Theory, generally there is no assumption\nmade about the target (such as its belonging to some class). This is probably the\nmain reason why this theory is so important - it does not require any knowledge\nof the distribution D.\nAlso, even if the empirical risk converges to the Bayes risk as m gets large (the\nalgorithm is consistent), it turns out that the convergence can be arbitrarily slow\nif there is no assumption made about the regularity of the target. On the other\nhand, the rate of convergence of the Estimation Error can be computed without\nany such assumption. We'll focus on the Estimation Error for this class.\nWe would really like to understand how bad the true risk of our algorithm's\noutput, Rtrue(fm), could possibly be. We want this to be as small as possible of\n\ncourse. We'll consider another way to look at Rtrue(fm):\nRtrue(fm) = Remp(f\ntrue\nm) + [R\n(fm) -Remp(fm)],\n(1)\nwhere remember we can measure Remp(fm).\nWe could upper bound the term Rtrue(fm) -Remp(fm), to make something like\nthis:\nRtrue(fm) ≤Remp(fm) + Stuff(m, F).\nThe \"Stuff\" will get more interesting as this lecture continues.\nA Bound for One Function f\nLet's define the loss g corresponding to a function f. The loss at point (x, y) is:\ng(x) = 1f(x)=y.\nGiven F, define the loss class, which contains all of the loss functions coming\nfrom F.\nG = {g : (x, y) →1f(x)=y : f ∈F}.\nSo g doesn't look at predictions f, instead it looks at whether the predictions\nwere correct. Notice that F contains functions with range in {-1, 1} while G\ncontains functions with range {0, 1}.\nThere's a bijection between F and G.\nYou can go from an f to its g by\ng(x, y) = 1f(x)=y. You can go from a g to its f by saying that if g(x, y) = 1\nthen set f(x) = -y, otherwise set f(x) = y. We'll use the g notation whenever\nwe're bounding the difference between an empirical average and its mean because\nthe notation is slightly simpler.\nDefine this notation:\nP trueg = E(X,Y )∼D[g(X, Y )]\n(true risk again)\nm\nP emp\ng =\nX\ng(Xi, Yi)\n(empirical risk again)\nm i=1\nso that we have another way to write the true risk and empirical risk directly\nin terms of the loss.\nP emp is called the empirical measure associated to the\n\ntraining sample.\nIt just computes the average of a function at the training\npoints. Remember, we are interested in the difference between the true risk and\nempirical risk, same thing as in the right side of (1), which we're going to upper\nbound:\nP truegm -P empgm.\n(2)\n(gm is the loss version of fm.)\nHoeffding's Inequality\nFor convenience we'll define Zi = (Xi, Yi) and Z = (X, Y ), and probabilities will\nbe taken with respect to Z1 ∼D, ..., Zm ∼D which we'll write Z ∼Dm.\nLet's rewrite the quantity we're interested in, for a general g this time:\nX\nm\nP trueg -P empg = EZ∼Dm[g(Z)] -\ng(Zi).\nm i=1\nIt's a difference between an empirical mean and its expectation. By the law of\nlarge numbers we know asymptotically that the mean converges to the expecta-\ntion in probability. So with probability 1, with respect to Z ∼Dm,\nm\nlim\nX\ng(Zi) = EZ\nm→infm\n∼Dm[g(Z)].\ni=1\nSo with enough data, the empirical risk is a good approximation to its true risk.\nThere's a quantitative version of the law of large numbers when variables are\nbounded:\nTheorem 1 (Hoeffding). Let Z1...Zm be m iid random variables, and h is a\nbounded function, h(Z) ∈[a, b]. Then for all ε > 0 we have:\n\"\nX\nm\nPZ∼Dm\n\n2mε\n\nh(Zi) -EZ∼Dm[h(Z)]\n\n≥ε\n#\n\n≤2 exp\nm\n\n-\n.\n(b -a)2\ni=1\n\nThe probability that the empirical average and expectation are far from each\nother is small. Let us rewrite the formula to better understand its consequences.\n\nLet the right hand side be δ, so\n\n2mε2\nδ = 2 exp\n-(b -a)2\n\n.\nThen if I solve for ε, I get:\ns\nlog 2\nε = (b -a)\nδ\n2m\nSo Hoeffding's inequality, applied to the function g becomes:\nP\nemp\ntrue\nlog 2\nδ\nZ Dm\n\n|P\ng -P\ng| ≥(b -a)\ns\n\n≤δ.\n∼\n2m\nThere's a technique called \"inversion\" that we'll use a lot.\nInversion\nUsing inversion, we get that with probability at least 1 -δ:\n|P emp\nlog\ng -P trueg| ≤(b -a)\ns\nδ .\n2m\nThe expression above is \"2-sided\" in that there's an absolute value on the left.\nThis considers whether P empg is larger than P trueg or smaller than it. There's\nalso 1-sided versions of Hoeffding's inequality where we look at deviations in one\ndirection or the\n\"\nother, for instance here is a 1-sided version of Hoeffding's:\nm\n2mε2\nPZ Dm\nEZ Dm[h(Z)]\n∼\n∼\n-m\nX\nh(Zi) ≥ε\n#\n≤exp\n\n-\n.\n(b\n=1\n-a)2\ni\n\nIf we again set the right side to δ and solve for ε, and invert, we get that with\nprobability at least 1 -δ,\nm\nlog 1\nEZ∼Dm[h(Z)] -m\nX\nh(Zi)\n(b\na)\nδ .\n2m\ni=1\n≤\n-\ns\nHere is the inverted one applied to g: with probabilit\ns\ny at least 1 -δ,\nlog 1\nP trueg -P empg ≤(b -a)\nδ .\n2m\n\nMoving the empirical term to the right, we have that with probability at least\n1 -δ,\nP trueg ≤P emp\nlog\ng + (b -a)\ns\nδ .\n2m\nRemember that g is the loss, so g(Z) = 1f(X)=Y and that way we have an upper\nbound for the true risk, which we want to be small.\nThis expression seems very nice, but guess what? It doesn't apply when f (i.e.,\ng) comes from any reasonable learning algorithm!\nWhy not?\nLimitations\nThe result above says that for each fixed function g ∈G, there is a set S of\n\"good\" samples z1, ..., zm, for which\ntrue\nemp\ns\nlog 1\nP\ng -P\ng ≤(1 -0)\nδ\n2m\nand this set of samples has measure PZ∼Dm[Z ∈S] ≥1 -δ. However, these sets\nmay be different for different functions g. In other words, for the sample S we\nactually observe, there's no telling how many of the functions in G will actually\nsatisfy this inequality!\nThis figure might help you understand. Each point on the x-axis is a different\nfunction. The curve marked Rtrue(f) is the true risk, which is a constant for each\nf since it involves the whole distribution (and not a sample).\n\nIf you give me a sample and a function f, I can calculate Remp(f) for that sample,\nwhich gives me a dot on the plot. So, for each sample we get a different curve on\nthe figure. For each f, Hoeffding's inequality makes sure that most of the time,\nthe Remp(f) curves lie within a small distance of Rtrue(f), though we don't know\nwhich ones. In other words, for an observed sample, only some of the functions\nin F will satisfy the inequality, not all of them.\nBut remember, our algorithms choose fm knowing the data. They generally try\nto minimize the regularized Remp(f). Consider drawing a sample S, which cor-\nresponds to a curve on the figure. Our algorithm could (on purpose) meander\nalong that curve until it chooses a fm that gives a small value of Remp. This value\ncould be very far from Rtrue(fm). This could definitely happen, and if there are\nmore f's to choose from (if the function class is larger), then this happens more\neasily - uh oh! In other words, if F is large enough, one can find, somewhere\nalong the axis, a function f for which the difference between the two curves\nRemp(f) and Rtrue(f) will be very large.\n\nWe don't want this to happen!\nUniform Bounds\nWe really need to make sure our algorithm doesn't do this - otherwise it will\nnever generalize. That's why we're going to look at uniform deviations in order\nto upper bound (1) or (2):\nRtrue(fm) -Remp(fm) ≤sup(Rtrue(f)\nf\n-Remp(f))\n∈F\nwhere we look at the worst deviation over all functions in the class.\nLet us construct a first uniform bound, using Hoeffding's inequality and the\nunion bound. Define:\nCj = {z1, ..., zm : P truegj -P empgj ≥ε}.\nThis set contains all the \"bad\" samples, those for which the bound fails for\nfunction gj. From Hoeffding's Inequality, for each j,\nPZ Dm[Z ∈Cj] ≤δ.\n∼\nConsider two functions g1 and g2. Say we want to measure how many samples\nare \"bad\" for either one of these functions or the other. We're going to use the\nunion bound to do this, which says:\nP[C1 ∪C2] ≤P[C1] + P[C2] ≤2δ,\nthe probability that we hit a bad sample for either g1 or g2 is\n≤\nprob to hit a bad sample for C1 + prob to hit a bad sample for C2.\nMore generally, the union bound is:\nN\nP[C1 ∪... ∪CN] ≤\nX\nP[Cj]\nj=1\n≤Nδ\n\nSo this is a bound on the probability that our chosen sample will be bad for any\nof the functions g1, ..., gN. So we get:\nPZ Dm[∃g ∈{g1, ..., gN\n: P trueg\nP empg\nε]\n∼\nX\nN\n}\n-\n≥\n≤\nPZ\nm[\n∼D\nP truegj\nP empgj\nε]\nj=1\n-\n≥\nN\n≤\nX\nexp(-2mε2)\nWhere did this come from?\nj=1\n= N exp(-2mε2).\nIf we define a new δ so we can invert:\nδ := N exp(-2mε2)\nand solve for ε, we get:\ns\nlog N + log 1\nε =\nδ .\n2m\nPlugging that in and inverting, we find that with probability at least 1 -δ,\n∀g ∈{g1, ..., gN} : P trueg -\nemp\nlog N + log\nP\ng ≤\ns\nδ .\n2m\nChanging g's back to f's, we've proved the following:\nTheorem. (Hoeffding + Union Bound)\nFor F = {f1...fN}, for all δ > 0 with probability at least 1 -δ,\n∀f ∈F, Rtrue\nlog N + log 1\n(f) ≤Remp(f) +\ns\nδ .\n2m\nJust to recap the reason why this bound is better than the last one, if we know\nour algorithm only picks functions from a finite function class F, we now have a\nbound that can be applied to fm, even though it depends on the data.\nNote the main difference with plain Hoeffding's inequality is the extra log N term\non the right hand side. This term is the one saying we want N bounds to hold\nsimultaneously.\n\nEstimation Error\nLet's say we're doing empirical risk minimization, that is, fm is the minimizer of\nthe empirical risk Remp.\nWe can use the theorem above (combined with (1)) to get an upper bound on\nthe Estimation Error. Start with this:\nRtrue(fm) = Rtrue(fm) -Rtrue(f ∗) + Rtrue(f ∗)\nThen we'll use the fact that Remp(f ∗) -Remp(fm) ≥0. Why is that?\nWe'll add that to the expression above:\nRtrue(f ) ≤[Remp(f ∗) -Remp\ntrue\nm\n(fm)] + R\n(fm) -Rtrue(f ∗) + Rtrue(f ∗)\n= Remp(f ∗) -Rtrue(f ∗) -Remp(fm) + Rtrue(fm) + Rtrue(f ∗)\n≤|Rtrue(f ∗) -Remp(f ∗)| + |Rtrue(fm) -Remp(fm)| + Rtrue(f ∗)\n≤2 sup Rtrue(f)\nRemp(f) + Rtrue(f ∗).\nf\n|\n-\n|\n∈F\nWe could use a 2-sided version of the theorem (with an extra factor of 2 some-\nwhere) that with probability 1 -δ, that first term is bounded by the square root\nterm in the theorem. Specifically, we know that with probability 1 -δ:\nRtrue\nlog N + log\n(fm) ≤2\ns\nδ + Rtrue(f ∗).\n2m\nActually, if you think about it, both terms in the right hand side depend on the\nsize of the class F. If this size increases, the first term will increase, and the\nsecond term will decrease. Why?\nSummary and Perspective\n- Generalization requires knowledge (like restricting f to lie in a restricted\nclass F).\n- The error bounds are valid with respect to the repeated sampling of training\nsets.\n\n- For a fixed function f, for most of the samples,\nRtrue(f) -Remp(f) ≈1/√m.\n- For most of the samples if the function class if finite, |F| = N,\nsup[Rtrue(g)\nlog\ng\n-Remp(g)] ≈\n∈G\np\nN/m.\nThe extra term is because we choose fm in a way that changes with the\ndata.\n- We have the Hoeffding + Union Bound Theorem above, which bounds the\nworst difference between empirical risk and true risk among functions in the\nclass.\nThere are several things that could be improved. For instance Hoeffding's in-\nequality only uses the boundedness of the functions, not their variance, which is\nsomething we won't deal with here. The supremum over F of Rtrue(f)-Remp(f)\nis not necessarily what the algorithm would choose, so the upper bound could\nbe loose. The union bound is in general loose, because it is as bad as if all the\nfj(Z)'s are independent.\nInfinite Case: VC Dimension\nHere we'll show how to extend the previous results to the case where the class\nF is infinite.\nWe'll start with a simple refinement of the union bound that allows to extend\nthe previous results to the (countably) infinite case.\nRecall that by Hoeffding's inequality for a single function g, for each δ > 0, where\npossibly we could choose δ depending on g, which we write δ(g), we have:\nPZ Dm\n\nlog δ(g)\nP trueg\nempg\n∼\n-P\n≥\ns\n2m\n\n≤δ(g).\nHence if we have a countable\n\nset\n\nG, the union bound gives:\nlog\nP\nδ\nZ Dm\n\n∃g ∈G\n(g)\n: P trueg\ng\n∼\n-P emp\n≥\ns\n2m\n\n≤\nX\nδ(g).\ng∈G\n\nIf we choose the δ(g)'s so that they add up to a constant total value δ, that is,\nδ(g) = δ p(g) where P\ng\np(g) = 1, then the right hand side is just δ and we get\n∈G\nthe following with inversion: with probability at least 1 -δ,\nlog\n∀∈G\ntrue\n≤\nemp\ns\n+ log 1\np(g)\nδ\ng\n, P\ng\nP\ng +\n.\n2m\nIf G is finite with size N, and we take a uniform p(g) = 1 , we get the log N term\nN\nas before.\nGeneral Case\nWhen the set G is uncountable, the previous approach doesn't work because p(g)\nis a density, so it's 0 for a given g and the bound will be vacuous. We'll switch\nback to the original class F rather than the loss class for now. The general idea\nis to look at the function class's behavior on the sample. Given z1, ..., zm, we\nconsider\nFz1,...,zm = {f(z1), ..., f(zm) : f ∈F}.\nFz1,...,zm is the set of ways the data z1, ..., zm are classified by functions from F.\nSince the functions f can only take two values, this set will always be finite, no\nmatter how big F is.\nDefinition (Growth Function) The growth function is the maximum number\nof ways into which m points can be classified by the function class:\nS (m) =\nsup\nF\n(z1,...,zm)\n|Fz1,...,zm|.\nIntuition for Growth Function and Example of Halfplanes\nWe defined the growth function in terms of the initial class F but we can\ndo the same with the loss class G since there's a 1-1 mapping, so we'll get\nS (m) = S (m).\nG\nF\nThis growth function can be used as a measure of the 'size' of a class of functions\nas demonstrated by the following result:\n\nTheorem-GrowthFunction (Vapnik-Chervonenkis) For any δ > 0, with\nprobability at least 1 -δ with respect to a random draw of the data,\nlog S (2m) + log 4\n∀f ∈F Rtrue(f) ≤Remp(f) + 2\ns\nF\nδ\nm\n(proof soon).\nThis bound shows nicely that simplicity implies generalization. The simpler the\nfunction class, the better the guarantee that Rtrue will be small. In the finite\ncase where |F| = N (we have N possible classifiers), we have S (m)\nF\n≤N (at\nworst we use up all the classifiers when we're computing the growth function). So\nthis bound is always better than the one we had before (except for the constants).\nBut we need to figure out how to compute S (m).\nWe'll do that using VC\nF\ndimension.\nVC dimension\nSince f ∈{-1, 1}, it is clear that S (m)\nF\n≤2m.\nIf S (m) = 2m there is a data set of size m points such that F can generate any\nF\nclassification on these points (we say F shatters the set).\nThe VC dimension of a class F is the size of the largest set that it can shatter.\nDefinition. (VC dimension) The VC dimension of a class F is the largest m\nsuch that\nS (m) = 2m.\nF\nWhat is the VC dimension of halfplanes in 2 dimensions?\nCan you guess the VC dimension of halfplanes in d dimensions?\nIn the example, the number of parameters needed to define the half space in Rd\nis the number of dimensions, d. So a natural question to ask is whether the VC\ndimension is related to the number of parameters of the function class. In other\n\nwords, VC dimension is supposed to measure complexity of a function class -\ndoes it just basically measure the number of parameters?\nIs the VC dimension always close to the number of parameters?\nSo how can VC dimension help us compute the growth function? Well, if a class\nof functions has VC dim h, then we know that we can shatter m examples when\nm ≤h, and in that case, S (m) = 2m. If m > h, then we know we can't shatter\nF\nthe points, so S (m) < 2m otherwise.\nF\nThis doesn't seem very helpful perhaps, but actually an intriguing phenomenon\noccurs for m ≥h, shown below.\nThe plot below shows for m ≥h (where we can't shatter) the number of ways\nwe can classify - that's the growth function. The growth function which is expo-\nnential up until the VC dimension, becomes polynomial afterwards!\nTypical behavior of the log growth function.\nThis is captured by the following lemma.\nLemma. (Vapnik and Chervonenkis, Sauer, Shelah) Let F be a class of\n\nfunctions with finite VC dimension h. Then for all m ∈N,\nh\nS (m)\nF\n≤\nX\ni=0\nm\ni\n\nIntuition\nand for all m ≥h\nS (m)\nF\n≤\nUsing this lemma for m\nh along with Theo\nem\n.\nh\nh\n≥\nrem-GrowthFunction, we get:\nTheorem VC-Bound. If F has VC dim h, and for m ≥h, with prob. at least\n1 -δ,\n∀f ∈F Rtrue\nh log 2em + log 4\n(f) ≤Remp(f) + 2\ns\nh\nδ .\nm\nWhat is important to remember from this result is that the difference between\nthe true and empirical risk is at most of order\nr\nh log m.\nm\nBefore we used VC dim, the bound was infinite, i.e., vacuous!\nRecap\nWhy is Theorem VC-Bound important? It shows that limiting the complexity of\nthe class of functions leads to better generalization. An interpretation of VC dim\nand growth functions is that they measure the \"effective\" size of the class, that is,\nthe size of the projection of the class onto finite samples. This measure doesn't\njust count the number of functions in the class, but depends on the geometry\nof the class, that is, the projections onto the possible samples. Also since the\nVC dimension is finite, our bound shows that the empirical risk will converge\nuniformly over the class F to the true risk.\nBack to Margins\nHow is it that SVM's limit the complexity? Well, the choice of kernel controls\nthe complexity. But also the margin itself controls complexity. There is a set of\n\nlinear classifiers called \"gap-tolerant classifiers\" that I won't define precisely (it\ngets complicated) that require a margin of at least ∆between points of the two\ndifferent classes. The points are also forced to live inside a sphere of diameter D.\nSo the class of functions is fairly limited, since they not only need to separate\nthe points with a margin of ∆, but also we aren't allowed to move the points\noutside of the sphere.\n\"Theorem\" VC-Margin.\n(Vapnik) For data in Rd, the VC dimension h\nof (linear) gap-tolerant classifiers classifiers with gap ∆belong to a sphere of\ndiameter D, is bounded by the inequality:\nh ≤min\nD2\n\n, d\n\n+ .\n∆\nSo the VC dimension (of the set of functions that separate points with some\nmargin) is less than 1/margin. If we have a large margin, we necessarily have a\nsmall VC-dimension.\nWhat does this say about halfspaces in Rd?\n(Think about the VC dimension example we did earlier.)\nSymmetrization\nWe'll do the proof of Theorem-GrowthFunction. The key ingredient is the sym-\nmetrization lemma. We'll use what's called a \"ghost sample\" which is an extra\n(virtual) data set Z1\n′, ..., Zm\n′ . Denote P ′emp the corresponding empirical measure.\n(Lemma-Symmetrization) For any t > 0, such that mt2 ≥2,\nPZ∼Dm\n\nsup(P true\ng\n-P emp)g ≥t\n∈G\n\n≤2PZ∼Dm,Z′∼Dm\n\nsup(P ′emp\ng\n-P emp)g ≥t/2\n∈G\n\n.\nThat is, if we can bound the difference between the behavior on one sample ver-\nsus another, it gives us a bound on the behavior of a sample with respect to the\ntrue risk.\nProof. Let gm be the function achieving the supremum in the lhs term, which\ndepends on Z1, ...Zm. Think about the event that: (P true\nP emp)gm\nt (the\n-\n≥\n\nsample's loss is far from the true loss) and (P true -P ′emp)gm < t/2 (the ghost\nsample's loss is close to the true loss). If this event were true, it sort of means that\nthings didn't generalize well for Z1, ..., Zm but that they did generalize well for\nZ1\n′, ..., Zm\n′ . If we can show that this event happens rarely, then the ghost sample\ncan help us. Again, the event that we want to happen rarely is (P true-P emp)gm ≥\nt and (P true -P ′emp)gm < t/2.\n1(P true-P emp)gm≥t1(P true-P ′emp)gm<t/2\n= 1(P true-P emp)g ≥t and (P true\nemp\nm\n-P ′\n)gm<t/2\n= 1(P true-P emp)gm≥t and (P ′emp-P true)gm>-t/2\n≤1(P true-P emp+P ′emp-P true)g >t-t/2=t/2 = 1(-P emp\nemp\nm\n+P ′\n)gm>t/2.\nThe inequality came from the fact that the event on the second last line ((P true-\nP emp)gm ≥t and (P ′emp -P true)gm > -t/2) implies the event on the last line\n((P true -P emp + P ′emp -P true)gm > t -t/2), so the event on the last line could\nhappen more often.\nTaking expectations with respect to the second sample, and using the trick to\nchange expectation into probability,\nP\n[(P true -P ′emp\n(P true\nP emp)gm\nt\nZ′\nDm\n)gm < t/2]\n-\n≥\n∼\n≤PZ′\nDm[(P ′emp -P emp)gm > t/2].\n(3)\n∼\nDo you remember Chebyshev's Inequality? It says P[|X -EX| ≥t] ≤VarX/t2.\nWe'll apply it now, to that second term on the left, inverted:\n4V\nPZ′\nDm[(P true -P ′emp\nargm\n)gm ≥t/2]\n∼\n≤\n.\nmt2\nI hope you'll believe me when I say that any random variable that has range\n[0, 1] has variance less than or equal to 1/4. Hence,\nPZ′\nDm[(P true -P ′emp)gm ≥t/2]\n∼\n≤\n.\nmt2\nInverting back, so that it looks like the second term on the left of (3) again:\nPZ′\nDm[(P true -P ′emp)gm < t/2] ≥1\n∼\n-\n.\nmt2\nMultiplying both sides by 1(P true-P emp\n\n)gm≥t I get back to the left of (3):\n1 -\n\n≤1\nP\n[(P true\nemp)g\nt\n′emp\n(P true\nP emp)gm\nt\n(P true\nP\nZ′\nDm\nm\n-P\n)gm < t/2]\n-\n≥\nmt2\n-\n≥\n∼\n≤PZ′∼Dm[(P ′emp -P emp)gm > t/2]\nfrom (3).\n\nTaking the expectation with respect to the first sample, the term\nEZ∼Dm1\ntrue\nemp\n(P true-P emp)gm≥t\nbecomes\nPZ∼Dm[(P\n-P\n)gm ≥t].\nAnd now we get:\nP\n[(P true\nemp\nZ Dm\n-P\n)gm ≥t]\n\n1 -\n\n≤PZ′\nDmZ Dm[(P ′emp\n∼\nmt2\n∼\n∼\n-P emp)gm > t/2]\nP\ntrue\n[(P\n-P emp)g\n≥t] ≤\n\nP\n[(P ′emp -P emp\nZ∼Dm\nm\nZ′∼DmZ∼Dm\n)g\n1 -\nm > t/2].\nmt2\nOnly one more step, which uses our assumption mt2 ≥2.\nmt2 ≥2\nmt2\n≤2\n1 -mt2\n≥1 -\n=\n\n1 -\nmt2\n\n≤2\nPlug:\nPZ Dm[(P true -P emp)g\nemp\nm ≥t] ≤2PZ′\nDmZ Dm[(P ′emp -P\n)g\n>\n∼\n∼\nm\nt/2]\n∼\n≤2P\nemp\nZ Dm,Z′\nDm sup(P ′\n-P emp)g > t/2\n∼\n∼\ng∈G\n\n.\nWe have an upper bound by changing the strict inequality \">\" to a \"≥.\" Then\nthe result is the same as the statement of the lemma. ■\nRemember, we're still in the middle of proving Theorem-GrowthFunction. The\nsymmetrization is just a step in that proof. This symmetrization lemma allows us\nto replace the expectation P trueg by an empirical average over the ghost sample.\nAs a result, the proof will only depends on the projection of the class G on the\ndouble sample\nGZ1...Zm,Z1\n′...Zm\n′ ,\nwhich contains finitely many different vectors. In other words, an element of this\nset is just the vector [g(x1), ..., g(xm)], and there are finitely many possibilities\nfor vectors like this. So we can use the union bound that we used for the finite\n\ncase. The other ingredient that we need to prove Theorem-GrowthFunction is\nthis one:\nP\n[P empg -P ′empg ≥t] ≤2e-mt /2\nZ∼DmZ′∼Dm\n.\n(4)\nThis one comes itself from a mix of Hoeffding's with the union bound:\nP\n[P empg -P ′emp\nZ∼DmZ′∼Dm\ng ≥t]\n= P\nemp\ntrue\ntrue\nZ DmZ′\nDm[P\ng -P\ng + P\ng -P ′empg\nt\n∼\n∼\n≥]\n≤PZ∼Dm[P empg -P trueg\nt/2] + P\ntrue\nemp\nZ′\n[\n∼Dm P\ng\nP ′\ng\nt/2]\n≤e-2m(t/2)2 + e-2m(t/2)2\n≥\n-\n≥\n= 2e-mt2/2.\nWe just have to put the pieces together now:\nP\ntrue\nZ∼Dm[sup(P\ng\n-P emp)g ≥t]\n∈G\n≤2PZ DmZ′\nDm[sup(P ′emp\n∼\ng\n-P emp)g ≥t/2]\nLemma-Symmetrization\n∼\n∈G\n= 2P\nemp\nemp\nZ∼DmZ′∼Dm[\nsup\n(P ′\nP\n)g\nt/2]\n(restrict to data)\nX\ng∈GZ ,...,Zm,Z′ ,...,Z\nm\n′\n-\n≥\n≤2\nPZ DmZ′\nDm[(P ′emp\n∼\n∼\ng\n-P emp)g ≥t/2]\n(union bound)\n∈GZ ,...,Z\nm,Z′ ,...,Z\nm\n′\n≤2\nX\n2e-m(t/2)2/2\ng∈GZ ,...,Z\nm,Z′ ,...,Z\nm\n′\n= 4e-mt /8\ng∈GZ ,...,Z\nX\nm,Z1\n′ ,...,Zm\n′\n= 4S (2m) e-mt2/8.\nG\nAnd using inversion,\nP\ntrue\nemp\nmt2/8\nZ∼Dm[sup(P\n)\ng\n-P\ng ≤t] ≥1 -4S (2m) e-\n.\nG\n∈G\nLetting δ = 4S (2m) e-mt2/8, solving for t yields:\nG\nt =\nr\n4S (2m)\nlog\nG\nm\nδ\n\nPlug:\nPZ∼Dm\n\nS\n+\nsup(\ntrue\ng\n-P emp\nlog\n(2m)\nlog\nP\n)g ≤2\n∈G\ns\nG\nδ\nm\n\n≥1 -δ.\nSo, with probability at least 1 -δ,\n\n∀g ∈G\n(P true -P emp)g ≤2\ns\nlog S (2m) + log 4\nG\nδ .\nm\nThat's the result of Theorem-GrowthFunction. ■\nOther kinds of capacity measures\nOne important aspect of VC dimension is that it doesn't depend on D, so it\nis a distribution independent quantity. The growth function is also distribution\nindependent. That's nice in some ways, because it allows us to get bounds that\ndon't depend on the problem at hand: the same bound holds for any distribu-\ntion. Although this may seem like an advantage, it could also be a drawback\nsince, as a result, the bound may be loose for most distributions.\nIt turns out there are several different quantities that are distribution dependent,\nwhich we can use in generalization bounds.\nVC-entropy\nOne quantity is called the (annealed) VC entropy. Recall the notation |Gz1,...,zm|\nwhich is the number of ways we can correctly/incorrectly classify z1, ..., zm.\nVC-entropyG(m) := log EZ Dm[\n]\n∼\n|GZ1,...,Zm| .\nIf the VC-entropy is large, it means that a lot of the time, there are a lot of\ndifferent ways to classify m data points. So the capacity of the set of functions\nis somehow large. There's a bound for the VC-entropy that's very similar to the\none for Theorem-GrowthFunction (which I won't go into here).\nHow does the VC-entropy relate to the Growth Function?\n\nCovering Number\nAnother quantity is called the covering number. Covering numbers can be de-\nfined in several different ways, but we'll just define one of them. Let's start by\nendowing the function class G with the following (random) metric:\nm\ndm(g, g′) =\nX\n1[g(Z\nm\ni)=g′(Zi)],\ni=1\nthe fraction of times they disagree on the (random) sample.\nAlso denote B(gj, ε) as ball of radius ε around gj, using the metric dm. In other\nwords, B(gj, ε) contains all the functions in G that are within distance ε of gj\naccording to our metric. We say that a set g1, ..., gN covers G at radius ε if:\nN\nG ⊂\nj\n[\nB(gj, ε)\n=1\nthat is, if G is contained within the collection of balls centered at the gj's. We\nthen define the covering number.\nDefinition. The covering number of G at radius ε with respect to dm, denoted\nby N(G, ε, m) is the minimum size of a cover of radius ε.\nIllustration\n(Remember of course that since there's a bijection between F and G that N(G, ε, m) =\nN(F, ε, m)).\nIf the covering number is finite, it means we can approximately represent G by\na finite set of functions that cover G. This allows us to use the (finite) union\nbound. Basically, the proof technique involves showing that things don't change\ntoo much within an ε ball, so we can characterize the whole ball of functions by\nits center, then union bound over the centers. This kind of proof technique is my\nfavorite for creating generalization bounds - just because to me it seems more\nstraightforward (of course this is highly subjective).\n\nA typical result (stated without proof) is:\nTheorem-Covering. For any t > 0,\nP\n[∃f ∈F : Rtrue(f) ≥Remp(f) + t] ≤8E\n[N(F, t, m)]e-mt2/128\nZ∼Dm\nZ∼Dm\n.\nWe can relate the covering number to the VC dimension.\nLemma (Haussler). Let F be a class of VC dimension h. Then for all ε > 0,\nall m, and any sample,\nN(F, ε, m) ≤Ch(4e)hεh.\n(where C is a constant). One thing about this result is that the upper bound\ndoes not depend on the sample size m. Probably it is a loose upper bound, but\nit's nice to be able to get this independent of m!\nRademacher Averages\nAnother way to measure complexity is to see how well functions from the class\nF can classify random noise. So if I arbitrarily start flipping the labels, how well\ncan functions from my class fit those arbitrary labels. If the functions from F can\nfit those arbitrary labels really well, then F must have high complexity. That's\nthe intuition behind the Rademacher complexity measure that we're going to\nintroduce.\nI'm going to introduce some notation,\nm\nRmg :=\nX\nσig(Zi)\nm i=1\nwhere the σi's are independent {+1, -1}-valued random variables with proba-\nbility 1/2 of taking either value. You can think of them as random coin flips.\nDenote Eσ the expectation taken with respect to the σi's.\nDefinition. (Rademacher Averages) For a class G of functions, the Rademacher\naverage is defined as:\nR(G) := Eσ,Z sup Rmg.\ng∈G\n\nIn other words, for each set of flips of the coin, you consider them as labels for\nyour data, and find a function from G that matches them the best. If you can\nmatch a lot of random coin flips really well using functions from G, then G has\nhigh complexity.\nHere's a bound involving Rademacher averages:\nTheorem-RademacherBound. With probability at least 1 -δ,\nlog 2\n∀g ∈G, Rtrue(g) ≤Remp(g) + 2R(G) +\ns\nδ .\n2m\nThe proof of this requires a powerful tool called a concentration inequality. Ac-\ntually, Hoeffding's inequality is a concentration inequality, so you've seen one al-\nready. Concentration inequalities say that as m increases, the empirical average\nis concentrated around the expectation. McDiarmid's concentration inequality\nis my favorite, and it generalizes Hoeffding's in that applies to functions that\ndepend on m iid random variables:\nTheorem. (McDiarmid's Inequality) Assume for all i = 1, ..., m\nsup\n|F(z1, ..., zi, ..., zm) -F(z1, ..., zi\n′, ..., zm)\nz1,...,zm,zi\n′\n| ≤c\nthen for all ε > 0,\n2ε2\nP[|F -E[F]| ≥ε] ≤2 exp\n\n-mc2\n\n.\nThis is a Hoeffding-like inequality that holds for any function of m variables, as\nlong as replacing one of those variables by another one won't allow the function\nto change too much.\nProof of Theorem-RademacherBound\nWe'll follow two steps:\n- concentration to relate supg\n(P trueg\n∈G\n-P empg) to its expectation,\n\n- symmetrization to relate the expectation to the Rademacher average.\nWe want to use McDiarmid's inequality on supg\n(P trueg -P empg), so we'll need\n∈G\nto show that if we modify one training example, it doesn't change too much.\nDenote P emp,i as the empirical measure obtained by replacing zi by zi\n′ of the\nsample. The following holds:\n| sup(P trueg\n)\ng\n-P empg -sup(P trueg\n∈G\ng\n-P emp,ig)| ≤sup\n∈G\ng\n|P emp,ig -P empg|.\n(5)\n∈G\nThis isn't too hard to check. For instance, let's say that the first term is larger\nthan the second so we can remove the absolute value.\nThen say g∗achieves\nsupg\n(P trueg\n∈G\n-P empg). Then\nsup(P trueg -P empg) -sup(P trueg\ng\n∈G\ng\n-P emp,i )\ng\n∈G\n= (P trueg∗-P empg∗) -sup(P trueg\ng\n-P emp,ig)\n∈G\n≤(P trueg∗-P empg∗) -(P trueg∗\nP emp,ig∗)\n= P emp,ig∗-P empg∗≤sup(P emp,i\n-\ng\ng\n-P empg)\n∈G\nAnd if the second term is larger than the first, we do an identical calculation.\nSo (5) holds.\nP emp,ig-P emp\nm\ng is a difference of two averages, since remember P empg is 1\nm\ni=1 g(Zi)\nand P emp,ig is the same thing except that the ith term got replaced with\nP\ng(Zi\n′).\n|P emp,ig -P empg| =\n|g(Zi\n′)\nm\n-g(Zi)| ≤m\nwhere we used that g ∈{0, 1} for the last inequality.\nThis means from (5) that we have\n| sup(P trueg\ng\n-P empg) -sup(P trueg\nP emp,i\ng)\n,\n(6)\n∈G\ng\n-\n| ≤m\n∈G\nthe function F = supg\n(P trueg -P empg) can't change more than\n1 when we\n∈G\nm\nfiddle with one of the Zi's.\nThis means we can directly apply McDiarmid's\ninequality to it with c = 1 .\nm\n2ε2\nPZ Dm[| sup(P trueg-P empg)-E\ntrue\nZ Dm[sup(P\ng-P empg)]| ≥ε] ≤2 exp\n\n-\n\n.\n∼\ng\n∼\ng\nm 1\n∈G\n∈G\nm2\n\nThat's the first part of the proof (the concentration part).\nNow for symmetrization, which we'll use to prove that the expected difference\nbetween the true and empirical risks is bounded by twice the Rademacher aver-\nage.\nLemma.\nEZ Dm sup[P trueg\n∼\ng\n-P empg] ≤2Eσ,Z sup Rmg = 2R(\n∈G\ng\nG)\n∈G\nTo prove it, we introduce a ghost sample and it's corresponding measure P ′emp.\nWe're going to use that E\ntrue\nZ\nP emp\n∼Dm\n′\ng = P\ng in the second line below.\nEZ Dm sup[P trueg\n∼\ng\n-P empg]\n∈G\n= E\nsup[E\n[P ′empg -P emp\nZ∼Dm\nZ′\ng\n∼Dm\ng]]\n∈G\n≤E\nemp\nemp\nZ DmZ′\nDm sup[P ′\ng -P\ng] (uses Jensen's Inequality, sup is convex)\n∼\n∼\ng∈G\n= EZ,Z′\n\"\nm\nsup\nX\n(g(Zi\n′) -g(Zi))\ng\nm\n∈G\ni=1\n#\n= Eσ,Z,Z′\n\"\nsup\nX\nm\nσi(g(Zi\n′) -g(Zi))\n#\nWhy?\n≤Eσ,Z\n\"\ng\nm\n∈G\ni=1\nm\nm\nsup\nX\nσig(Z′)\nm\ni\nsup\n∈G\n#\n+ Eσ,Z\ng\n\"\ng\nm\ni=1\n∈G\nX\nσig(Zi)\ni=1\n-\n#\n= 2Eσ,Z sup Rmg.\ng∈G\nThe last step uses that σig(Zi) and -σig(Zi) have the same distribution. ■\nLet's put the two parts of the proof of Theorem-RademacherBound together.\nThe first part said:\nP\n[| sup(P trueg-P empg)-E\n[sup(P true\nemp\n2ε\nZ∼Dm\nZ\ng\n∼Dm\ng\ng\n-P\ng)]| ≥ε] ≤2 exp\n\n-1/m\n∈G\n∈G\n\n.\nThe second part said:\nEZ∼Dm sup[P trueg\n]\ng\n-P empg ≤2Eσ,Z sup Rmg = 2R( )\n∈G\ng\nG\n∈G\n\nLet's fiddle with the first part. First, let\n:= 2 exp\n\n2ε\nlog 2\nδ\n-\nso that\nε =\nδ\n1/m\n\ns\n2m\nSo the first part looks like\nlog\nPZ Dm\n\nsup(P trueg\n∼\n-P empg) -E\ntrue\nemp\nδ\nZ∼Dm\ng\n\nsup(P\ng\n)]\nδ.\ng\n-P\ng\n\ns\n≥\n2m\n∈G\n∈G\n\n≤\nUsing inversion, with probability at least 1 -δ,\n\nlog 2\nsup(P trueg\ng\n-P empg) -EZ D\n\nsup\nP emp\nm\n(P trueg -\ng)\n∼\n∈G\ng\n\n≤\n∈G\ns\nδ .\n2m\nI can remove the absolute value and the left hand side only gets\n\nsmaller. Then,\nsup(P trueg -P empg) ≤E\n\nsup(P trueg -P empg)]\n\n+\ns\nlog 2\nδ\nZ Dm\n.\ng\n∼\n∈G\ng\n2m\n∈G\nUsing the second part,\nlog 2\nsup(P trueg -P empg) ≤2R( )\ng\nG +\n∈G\ns\nδ .\n2m\nWriting it all together, with probability at least 1 -δ, for all g ∈G,\nP trueg ≤P empg + 2R(G) +\ns\nlog 2\nδ ,\n2m\nand that's the statement of Theorem-RademacherBound.■\nRelating the Rademacher Complexity of the Loss Class\nand the Initial Class\nTheorem RademacherBound is nice, but it only applies to the class G, and we\nalso care about class\n, so we need to relate the Rademacher average of\nto\nF\nG\n\nthat of class F. We do this as follows, using the fact that σi and Yiσi have the\nsame distribution.\nR(G) = Eσ,Z\n\"\nm\nsup\nX\nσi1[f(Xi)=Yi]\ng\nm\n∈G\ni=1\n#\n= Eσ,Z\n\"\nm\nsup\nX\nσi (1 -Yif(Xi))\ng\nm\n∈G\ni=1\n#\nWhy?\nm\n=\nEσ,Z sup\nσiYif(Xi)\n=\n(\n). Why?\n\"\ng\nm\n2R\n∈G\nX\ni=1\n#\nF\nOk so we've related the two classes' Rademacher averages. So, with probability\nat least 1 -δ,\n∀f ∈F, Rtruef ≤Remp\nlog 2\nf + R(F) +\ns\nδ .\n2m\nComputing the Rademacher Averages\nLet's assess the difficulty of actually computing Rademacher averages.\nStart\nhere:\nm\nR(F) =\nEσ,Z\n\"\nsup\nX\nσif(Xi)\nf\nm\n∈F\ni=1\n#\n=\n+ Eσ,Z\n\"\nm\nσi\nsup\n-\nf(Xi)\n\"\nf\nm\nX\n=1\n-\n∈F\ni\n#\nm\n1 -σ\n-E\nX\nif(Xi)\n=\nσ,Z\ninf\nf∈F m\ni=1\n#\nm\n=\n-Eσ,Z\n\"\ninf\nX\n1f(Xi)=σi\n#\n. (1/2 comes through the inf)\nf∈F m i=1\nLook closely at that last expression. The thing on the inside says to find the\nempirical risk minimizer, where the labels are the σi's. This expression shows\nthat computing R(F) is not any harder than computing the expected empirical\n\nrisk minimizer over random labels. So we could design a hypothetical procedure,\nwhere we generate the σi's randomly, and minimize the empirical error in G with\nrespect to the labels σi. (Of course we technically also have to do this over the\npossible Xi's, but there are some ways to get around this that I won't go into\nhere.)\nIt's also true that\nh log em\nR(F) ≤2\nr\nh\nm\nSo we could technically use the Rademacher complexity bound to get another\nVC-bound.\nSo we're done! We showed formally that limiting the complexity leads to better\ngeneralization. We gave several complexity measures for a set of models (Growth\nFunction, VC-dimension, VC-entropy, Covering Number, Rademacher averages).\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.097 Prediction: Machine Learning and Statistics\nSpring 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "15.097 Lecture 15: Bayesian analysis",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-097-prediction-machine-learning-and-statistics-spring-2012/553a0822984b08bc611306c93533a0a3_MIT15_097S12_lec15.pdf",
      "content": "15.097: Probabilistic Modeling and Bayesian Analysis\nBen Letham and Cynthia Rudin\nCredits: Bayesian Data Analysis by Gelman, Carlin, Stern, and Rubin\nIntroduction and Notation\nUp to this point, most of the machine learning tools we discussed (SVM,\nBoosting, Decision Trees,...) do not make any assumption about how the\ndata were generated. For the remainder of the course, we will make distri\nbutional assumptions, that the underlying distribution is one of a set. Given\ndata, our goal then becomes to determine which probability distribution gen\nerated the data.\nWe are given m data points y1, . . . , ym, each of arbitrary dimension. Let\ny = {y1, . . . , ym} denote the full set of data. Thus y is a random variable,\nwhose probability density function would in probability theory typically be\ndenoted as fy({y1, . . . , ym}). We will use a standard (in Bayesian analysis)\nshorthand notation for probability density functions, and denote the proba\nbility density function of the random variable y as simply p(y).\nWe will assume that the data were generated from a probability distribution\nthat is described by some parameters θ (not necessarily scalar). We treat θ\nas a random variable. We will use the shorthand notation p(y|θ) to represent\nthe family of conditional density functions over y, parameterized by the ran\ndom variable θ. We call this family p(y|θ) a likelihood function or likelihood\nmodel for the data y, as it tells us how likely the data y are given the model\nspecified by any value of θ.\nWe specify a prior distribution over θ, denoted p(θ). This distribution rep\nresents any knowledge we have about how the data are generated prior to\n\nobserving them.\nOur end goal is the conditional density function over θ, given the observed\ndata, which we denote as p(θ|y). We call this the posterior distribution, and\nit informs us which parameters are likely given the observed data.\nWe, the modeler, specify the likelihood function (as a function of y and θ)\nand the prior (we completely specify this) using our knowledge of the system\nat hand. We then use these quantities, together with the data, to compute\nthe posterior. The likelihood, prior, and posterior are all related via Bayes'\nrule:\np(y|θ)p(θ)\np(y|θ)p(θ)\np(θ|y) =\n=\n,\n(1)\np(y)\np(y|θ')p(θ')dθ'\nwhere the second step uses the law of total probability. Unfortunately the\nintegral in the denominator, called the partition function, is often intractable.\nThis is what makes Bayesian analysis difficult, and the remainder of the notes\nwill essentially be methods for avoiding that integral.\nCoin Flip Example Part 1. Suppose we have been given data from a se\nries of m coin flips, and we are not sure if the coin is fair or not. We might\nassume that the data were generated by a sequence of independent draws\nfrom a Bernoulli distribution, parameterized by θ, which is the probability of\nflipping Heads.\nBut what's the value of θ? That is, which Bernoulli distribution generated\nthese data?\nWe could estimate θ as the proportion of the flips that are Heads. We will\nsee shortly that this is a principled Bayesian approach. Let yi = 1 if flip i\nm\nwas Heads, and yi = 0 otherwise. Let mH =\ni=1 yi be the number of heads\nin m tosses. Then the likelihood model is\np(y|θ) = θmH (1 - θ)m-mH .\n(2)\n1.1\nA note on the Bayesian approach\nThe problem formulation we have just described has historically been a source\nof much controversy in statistics. There are generally two subfields of statis\n\ntics: frequentist (or classical) statistics, and Bayesian statistics. Although\nmany of the techniques overlap, there is a fundamental difference in phi\nlosophy. In the frequentist approach, θ is an unknown, but deterministic\nquantity. The goal in frequentist statistics might then be to determine the\nrange of values for θ that is supported by the data (called a confidence in\nterval). When θ is viewed as a deterministic quantity, it is nonsensical to\ntalk about its probability distribution. One of the greatest statisticians of\nour time, Fisher, wrote that Bayesian statistics \"is founded upon an error,\nand must be wholly rejected.\" Another of the great frequentists, Neyman,\nwrote that, \"the whole theory would look nicer if it were built from the start\nwithout reference to Bayesianism and priors.\" Nevertheless, recent advances\nin theory and particularly in computation have shown Bayesian statistics to\nbe very useful for many applications. Machine learning is concerned mainly\nwith prediction ability. A lot of the methods we discussed do not worry about\nexactly what the underlying distribution is - as long as we can predict, we are\nhappy, regardless of whether we even have a meaningful estimate for p(y|θ).\nPoint estimates\nRather than estimate the entire distribution p(θ|y), sometimes it is sufficient\nto find a single 'good' value for θ. We call this a point estimate. For the sake\nof completeness, we will briefly discuss two widely used point estimates, the\nmaximum likelihood (ML) estimate and the maximum a posteriori (MAP)\nestimate.\n2.1\nMaximum likelihood estimation\nThe ML estimate for θ is denoted θˆML and is the value for θ under which the\ndata are most likely:\nˆθML ∈ arg max p(y|θ).\n(3)\nθ\nAs a practical matter, when computing the maximum likelihood estimate it\nis often easier to work with the log-likelihood, R(θ) := log p(y|θ). Because the\nlogarithm is monotonic, it does not affect the argmax:\nθˆML ∈ arg max R(θ).\n(4)\nθ\n\nThe ML estimator is very popular and has been used all the way back to\nLaplace. It has a number of nice properties, one of which is that it is a\nconsistent estimator. Let's explain what that means.\nDefinition 1 (Convergence in Probability). A sequence of random variables\nX1, X2, . . . is said to converge in probability to a random variable X if, ∀E > 0,\nlim 1 (|Xn - X| ≥ E) = 0.\nn→inf\nP\nWe denote this convergence as Xn -→ X.\nDefinition 2 (Consistent estimators). Suppose the data y1, . . . , ym were gen\nerated by a probability distribution p(y|θ0). An estimator θˆ is consistent if\nP\nit converges in probability to the true value: θˆ -→ θ0 as m →inf.\nWe said that maximum likelihood is consistent. This means that if the distri\nbution that generated the data belongs to the family defined by our likelihood\nmodel, maximum likelihood is guaranteed to find the correct distribution, as\nm goes to infinity. In proving consistency, we do not get finite sample guar\nantees like with statistical learning theory; and data are always finite.\nCoin Flip Example Part 2. Returning to the coin flip example, equation\n(2), the log-likelihood is\nR(θ) = mH log θ + (m - mH ) log(1 - θ).\nWe can maximize this by differentiating and setting to zero, and doing a few\nlines of algebra:\ndR(θ)\n= mH - m - mH\nθˆ\nθ\n1 - θ\nˆθ\n0 = dθ\nmH (1 - θˆML) = (m - mH )θˆML\nmH - θˆMLmH = mθˆML - θˆMLmH\nmH\nˆθML =\n.\n(5)\nm\n(It turns out not to be difficult to verify that this is indeed a maximum).\nIn this case, the maximum likelihood estimate is exactly what we intuitively\nthought we should do: estimate θ as the observed proportion of Heads.\n\n2.2\nMaximum a posteriori (MAP) estimation\nThe MAP estimate is a pointwise estimate with a Bayesian flavor. Rather\nthan finding θ that maximizes the likelihood function, p(y|θ), we find θ that\nmaximizes the posterior, p(θ|y). The distinction is between the θ under which\nthe data are most likely, and the most likely θ given the data.\nWe don't have to worry about evaluating the partition function\np(y|θ ' )p(θ ' )dθ '\nbecause it is constant with respect to θ. Again it is generally more convenient\nto work with the logarithm.\np(y|θ)p(θ)\nˆθMAP ∈ arg max p(θ|y) = arg max\nθ\nθ\np(y|θ ' )p(θ ' )dθ '\n= arg max p(y|θ)p(θ) = arg max (log p(y|θ) + log p(θ)) .\n(6)\nθ\nθ\nWhen the prior is uniform, the MAP estimate is identical to the ML estimate\nbecause the log p(θ) is constant.\nOne might ask what would be a bad choice for a prior. We will see later that\nreasonable choices of the prior are those that do not assign zero probability to\nthe true value of θ. If we have such a prior, the MAP estimate is consistent,\nwhich we will discuss in more detail later. Some other properties of the MAP\nestimate are illustrated in the next example.\nCoin Flip Example Part 3. We again return to the coin flip example.\nSuppose we model θ using a Beta prior (we will see later why this is a good\nidea): θ ∼ Beta(α, β). The Beta distribution is:\nBeta(θ; α, β) =\nθα-1(1 - θ)β-1 ,\n(7)\nB(α, β)\nwhere B(α, β) is the beta function, and is constant with respect to θ:\nZ 1\nB(α, β) =\ntα-1(1 - t)β-1dt.\n(8)\nThe quantities α and β are parameters of the prior which we are free to set\naccording to our prior belief about θ. By varying α and β, we can encode\na wide range of possible beliefs, as is shown in this figure taken from the\nWikipedia article on the Beta distribution:\nR\nR\n\nThe MAP estimate for θ we can get from the formula for computing θˆMAP\nin (6), plugging in the formula for the likelihood we found in part 2, and the\ndefinition of the Beta distribution for the prior (7):\nˆθMAP ∈ arg max (log p(y|θ) + log p(θ))\nθ\n= arg max (mH log θ + (m - mH ) log(1 - θ)\nθ\n+(α - 1) log θ + (β - 1) log(1 - θ) - log B(α, β)) .\nDifferentiating and setting to zero at θˆMAP,\nmH\nm - mH\nα - 1\nβ - 1\n-\n+\n-\n= 0\nˆ\n1 - ˆ\nˆ\n1 - ˆ\nθMAP\nθMAP\nθMAP\nθMAP\nmH + α - 1\nˆθMAP =\n.\n(9)\nm + β - 1 + α - 1\nThis is a very nice result illustrating some interesting properties of the MAP\nestimate. In particular, comparing the MAP estimate in (9) to the ML esti\nmate in (5) which was\nmH\nˆθML =\n,\nm\nwe see that the MAP estimate is equivalent to the ML estimate of a data set\nwith α - 1 additional Heads and β - 1 additional Tails. When we specify,\nfor example, a prior of α = 7 and β = 3, it is literally as if we had begun the\n(c) Krishnavedala on Wikipedia. CC BY-SA. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/fairuse.\n\ncoin tossing experiment with 6 Heads and 2 Tails on the record. If we truly\nbelieved before we started flipping coins that the probability of Heads was\naround 6/8, then this is a good idea. This can be very useful in reducing the\nvariance of the estimate for small samples.\nFor example, suppose the data contain only one coin flip, a Heads. The ML\nestimate will be θˆML = 1, which predicts that we will never flip tails! How\never, we, the modeler, suspect that the coin is probably fair, and can assign\nα = β = 3 (or some other number with α = β), and we get θˆMAP = 3/5.\nQuestion How would you set α and β for the coin toss under a strong prior\nbelief vs. a weak prior belief that the probability of Heads was 1/8?\nFor large samples it is easy to see for the coin flipping that the effect of the\nprior goes to zero:\nˆ\nˆ\nlim θMAP = lim θML = θtrue.\nWhy?\nm→inf\nm→inf\nRecall what know about regularization in machine learning - that data plus\nknowledge implies generalization. The prior is the \"knowledge\" part. One\ncould interpret the MAP estimate as a regularized version of the ML estimate,\nor a version with \"shrinkage.\"\nExample 1. (Rare Events) The MAP estimate is particularly useful when\ndealing with rare events. Suppose we are trying to estimate the probabil\nity that a given credit card transaction is fraudulent. Perhaps we monitor\ntransactions for a day, and there are no fraudulent transactions. The ML\nestimate tells us that the probability of credit card fraud is zero. The MAP\nestimate would allow us to incorporate our prior knowledge that there is some\nprobability of fraud, we just haven't seen it yet.\n2.3\nPoint estimation and probabilistic linear regression\nWe will now apply point estimation to a slightly more interesting problem,\nlinear regression, and on the way will discover some very elegant connections\nbetween some of the machine learning algorithms we have already seen and\nour new probabilistic approach. Suppose now that we have m data points\n(x1, y1), . . . , (xm, ym), where xi ∈ Jd are the independent variables and yi ∈ J\n\n!\nare the dependent variables. We will use a likelihood model under which yi\ndepends linearly on xi, and the yi's are all independent. Specifically, we\nmodel\nyi ∼ θT xi + E,\nwhere θ ∈ Jd are the parameters we are interested in, and E represents noise.\nWe will assume that the noise is distributed normally with zero mean and\nsome known variance σ2: E ∼N (0, σ2). This, together with our assumption of\nindependence, allows us to express the likelihood function for the observations\ny = {y1, . . . , ym}:\nm\nm\nm\nm\np(y|x, θ) =\np(yi|xi, θ) =\nN (yi; θT xi, σ2)\ni=1\ni=1\nm\n\nm\n=\n√ 1\nexp\n- 1 (yi - θT xi)2\n,\n2σ2\ni=1\n2πσ2\nwhere the first line follows from independence. As usual, it is more convenient\nto work with the log-likelihood:\nm\n\nm\nR(θ) = log\n√ 1\nexp\n- 1 (yi - θT xi)2\n2σ2\ni=1\n2πσ2\nm\nm\n= - m log(2πσ2) - 1\n(yi - θT xi)2 .\n2σ2\ni=1\nThe ML estimate is\nm\nm\nθˆML ∈ arg max R(θ) = arg max - 1\n(yi - θT xi)2\n(10)\nθ\nθ\n2σ2\ni=1\nm\nm\n= arg min\n(yi - θT xi)2 .\n(11)\nθ\ni=1\nThe ML estimate is equivalent to ordinary least squares!\nNow let us try the MAP estimate. We saw in our coin toss example that the\nMAP estimate acts as a regularized ML estimate. For probabilistic regression,\nwe will use a multivariate normal prior (we will see later why this is a good\nidea) with mean 0. The covariance matrix will be diagonal, it is I (the\n\nZ\nZ\n\nidentity matrix) times σ2/C, which is the known, constant variance of each\ncomponent of θ. So the prior is:\n-1\nθ ∼N (0, Iσ2/C) =\nexp - 1 θT Iσ2/C\nθ .\n(2π)d/2|σ2/C|1/2\nHere we are saying that our prior belief is that the \"slope\" of the regression\nline is near 0. Now,\nˆθMAP ∈ arg max log p(y|x, θ) + log p(θ)\nθ\nm\n= arg max -\nθ\n2σ2\ni=1\nm\n(yi - θT xi)2 + log p(θ)\nfrom (10). At this point you can really see how MAP is a regularized ML.\nContinuing,\nm\nm\nσ2\nd\n(yi - θT xi)2 -\nθT (IC/σ2)θ\n-\nlog 2π - log\n-\n= arg max 2σ2\nC\nθ\ni=1\nm\nm\n(yi - θT xi)2 - 1 CθT θ\n2σ2\n= arg max -\nθ\n2σ2\ni=1\nm\nm\nT\nI I\n-\n= arg min\n(\nθ\n)\nC θ\n(12)\n+\ny\nx\n.\ni\ni\nθ\ni=1\nWe see that the MAP estimate corresponds exactly to R2-regularized linear\nregression (ridge regression), and that the R2 regularization can be interpreted\nas a Gaussian prior. Increasing C corresponds to increasing our certainty that\nθ should be close to zero.\nConjugate priors\nAlthough point estimates can be useful in many circumstances (and are used\nin many circumstances), our true goal in Bayesian analysis is often to find\nthe full posterior, p(θ|y). One reason for wanting the posterior is to be able\nto use the posterior predictive distribution of a yet unobserved data point:\np(ym+1|y) =\np(ym+1|y, θ)p(θ|y)dθ =\np(ym+1|θ)p(θ|y)dθ,\n(13)\nZ\nZ\n\nbecause ym+1 and y are conditionally independent given θ by assumption.\nWe saw earlier that the posterior can be obtained in principle from the prior\nand the likelihood using Bayes' rule, but that there is an integral in the\ndenominator which often makes this intractable. One approach to circum\nventing the integral is to use conjugate priors.\nThe appropriate likelihood function (Binomial, Gaussian, Poisson, Bernoulli,...)\nis typically clear from the data. However, there is a great deal of flexibility\nwhen choosing the prior distribution. The key notion is that if we choose\nthe 'right' prior for a particular likelihood function, then we can compute\nthe posterior without worrying about the integrating. We will formalize the\nnotion of conjugate priors and then see why they are useful.\nDefinition 3 (Conjugate Prior). Let F be a family of likelihood functions\nand P a family of prior distributions. P is a conjugate prior to F if for\nany likelihood function f ∈F and for any prior distribution p ∈P, the\n∗\n∗\ncorresponding posterior distribution p satisfies p ∈P.\nIt is easy to find the posterior when using conjugate priors because we know\nit must belong to the same family of distributions as the prior.\nCoin Flip Example Part 4. In our previous part of coin flip example, we\nwere very wise to use a Beta prior for θ because the Beta distribution is the\nconjugate prior to the Bernoulli distribution. Let us see what happens when\nwe compute the posterior using (2) for the likelihood and (7) for the prior:\np(y|θ)p(θ)\nθmH (1 - θ)m-mH θα-1(1 - θ)β-1\np(θ|y) =\n=\n1 p(y|θ ' )p(θ ' )dθ '\n(normalization)\nθmH +α-1(1 - θ)m-mH +β-1\n=\n.\n1 θ'mH +α-1(1 - θ')m-mH +β-1dθ'\nWe can recognize the denominator as a Beta function from the definition in\n(8):\np(θ|y) =\nθmH +α-1(1 - θ)m-mH +β-1 ,\nB(mH + α, m - mH + β)\nand we recognize this as being a Beta distribution:\np(θ|y) ∼ Beta (mH + α, m - mH + β) .\n(14)\nR\nR\n\nAs with the MAP estimate, we can see the interplay between the data yi and\nthe prior parameters α and β in forming the posterior. As before, the exact\nchoice of α and β does not matter asymptotically, the data overwhelm the\nprior.\nIf we knew that the Beta distribution is the conjugate prior to the Bernoulli,\nwe could have figured out the same thing faster by recognizing that\np(θ|y) ∝ p(y|θ)p(θ) = θmH +α-1(1 - θ)m-mH +β-1 ,\n(15)\nand then realizing that by the definition of a conjugate prior, the posterior\nmust be a Beta distribution. There is exactly one Beta distribution that\nsatisfies (15): the one that is normalized correctly, and that is equation (14).\nThe parameters of the prior distribution (α and β in the case of the Beta\nprior) are called prior hyperparameters. We choose them to best represent\nour beliefs about the distribution of θ. The parameters of the posterior dis\ntribution (mH + α and m - mH + β) are called posterior hyperparameters.\nAny time a likelihood model is used together with its conjugate prior, we\nknow the posterior is from the same family of the prior, and moreover we have\nan explicit formula for the posterior hyperparameters. A table summarizing\nsome of the useful conjugate prior relationships follows. There are many more\nconjugate prior relationships that are not shown in the following table but\nthat can be found in reference books on Bayesian statistics1 .\n1Bayesian Data Analysis by Gelman, Carlin, Stern, and Rubin is an excellent choice.\n\nPrior\nPosterior\nConjugate\nLikelihood\nHyper-\nHyper-\nPrior\nparams\nparams\nBernoulli\nBeta\nα, β\nα +\nm yi, β + m -\nm yi\ni=1\ni=1\nm\nm\nm\nBinomial\nBeta\nα, β\nα +\nyi, β +\nmi -\nyi\ni=1\ni=1\ni=1\nm\nPoisson\nGamma\nα, β\nα +\ni=1 yi, β + m\nm\nGeometric\nBeta\nα, β\nα + m, β +\ni=1 yi\nUniform on [0, θ]\nPareto\nxs, k\nmax{max yi, xs}, k + m\nm\nExponential\nGamma\nα, β\nα + m, β +\ni=1 yi\nNormal\n\n-1\nμ0\nm\nm\nm\nunknown mean\nNormal\nμ0, σ2\n+\nyi\n/\n+\n,\n+\nσ2\nσ2\ni=1\nσ2\nσ2\nσ2\nσ2\nknown variance σ2\nWe will now discuss a few of these conjugate prior relationships to try to gain\nadditional insight. For Bernoulli-Beta, we saw that the prior hyperparame\nters can be interpreted as starting the tossing with a certain number of Heads\nand Tails on the record. The posterior hyperparameters then simply add the\nobserved number of heads (mH ) to the prior hyperparameter for number of\nHeads (α), and the observed number of tails (m - mH ) to the prior hyperpa\nrameter for number of tails (β).\nFor Binomial-Beta, we now have m Binomial experiments, each of which\nconsists of a certain number of coin tosses (mi) and a certain number of\nHeads (yi). As before, the first prior hyperparameter corresponds to number\nof \"hallucinated\" Heads, and in the posterior we combine the prior hyper-\nparameter α with the total number of observed Heads across all Binomial\ntrials. Similarly, for the second posterior hyperparameter, we compute the\ntotal number of Tails observed across all Binomial trials and combine it with\nthe corresponding prior hyperparameter (β).\nIn the Uniform-Pareto example, the data come from a uniform distribution\non [0, θ], but we don't know θ. We choose a prior for θ that is a Pareto\ndistribution with prior hyperparameters xs and k. Here, xs and k can be in\nterpreted as beginning the experiment with k observations, whose maximum\nvalue is xs (so we believe θ is at least xs). In the posterior, we replace the\nmaximum value xs with the new maximum value max{max yi, xs} including\nP\nP\nP\nP\nP\nP\nP\nP\nP\n\n!\n\nthe observed data, and update the number of observations to k + m.\nFor Normal-Normal, we see that the posterior mean is a weighted combination\nof the data and the prior, where the weights are the variances and correspond\nto our certainty in the prior vs. the data. The higher the variance in the\ndata, the less certain we are in them and the longer it will take for the data\nto overwhelm the effect of the prior. (Might be helpful to think of μ0 as one\nprior example rather than a bunch of them.)\n3.1\nExponential families and conjugate priors\nThere is an important connection between exponential families (not to be\nconfused with the exponential distribution) and conjugate priors.\nDefinition 4. The family of distributions F is an exponential family if every\nm\nmember of F has the form:\np(yi|θ) = f(yi)g(θ) exp φ(θ)T u(yi) ,\n(16)\nfor some f(·), g(·), φ(·), and u(·).\nm\nEssentially all of the distributions that we typically work with (normal, ex\nponential, Poisson, beta, gamma, binomial, Bernoulli,...) are exponential\nfamilies. The next theorem tells us when we can expect to have a conjugate\nprior.\nTheorem 1 (Exponential families and conjugate priors). If the likeli\nhood model is an exponential family, then there exists a conjugate prior.\nProof. Consider the likelihood of our iid data y = {y1, . . . , ym}:\nm\nm\nφ(θ)T u(yi)\np(y|θ)\np(yi|θ) =\nf(yi)g(θ) exp\n=\nm\nm\nii=1\nm\nm\n=\nf(yi) g(θ)m exp φ(θ)T\nu(yi) .\ni=1\ni=1\nTake the prior distribution to be:\ng(θ)η exp φ(θ)T ν\np(θ) =\n,\n(17)\ng(θ')η exp (φ(θ')T ν) dθ'\ni=1\n\n!\n\nR\n\nwhere η and ν are prior hyperparameters. Then the posterior will be\np(θ|y) ∝ p(y|θ)p(θ)\nm\n=\nim\nf(yi)\ni=1\n#\ng(θ)m\n\nT\nexp\n\nφ(θ)\nm\nm\nm\n!!\n)\nR g(θ η exp\nφ(θ)T ν\nu(yi)\ng(θ ' )η exp (φ(θ ' )T ν)\n\ndθ '\ni=1\n\n∝ g(θ)η+m\n\nT\nexp\nφ(θ)\n\nν +\nm\nu(yi)\n!!\n\n!!\n(dropping non-θ terms),\ni=1\nwhich is in the same family P\n\nas the prior (17), with the posterior hyperparam\nm\neters being η + m and ν +\ni=1 u(yi).\nAlthough this proof yields the form of the conjugate prior, we may not always\nbe able to compute the partition function. So we can't always gain something\nin practice from it.\nIt turns out that in general, the converse to Theorem 1 is true, and exponen\ntial families are the only distributions with (non-trivial) conjugate priors.\nCoin Flip Example Part 5. Returning again to the coin flip example, let\nus first verify that the Bernoulli distribution is an exponential family:\np(yi|θ) = θyi (1 - θ)1-yi\n= exp (yi log θ + (1 - yi) log(1 - θ))\n= exp (yi log θ - yi log(1 - θ) + log(1 - θ))\nθ\n= (1 - θ) exp\n\nyi log\n,\n1 - θ\n\nand we see that the Bernoulli distribution is an exponential family according\nto (16) with f(yi) = 1, g(θ) = 1 - θ, u(yi) = yi, and φ(θ) = log θ . Thus,\n1-θ\naccording to (17), the conjugate prior is\n∝\nη\n)\n\np(θ\ng(θ) exp\nφ( θ)Tν\n= (1 - θ)η\nθ\nexp ν log\n\n1 - θ\n\nν\n\n= (1 -\n\nθ\nθ)η\n- θ)\n\nη-ν\n- θ\n= θν(1\n\n.\n\nReparameterizing by defining ν = α - 1 and η - ν = β - 1 gives us the Beta\ndistribution that we expect.\nExample 2. (Normal Distribution is an Exponential Family) Con\nsider a normal distribution with known variance but unknown mean:\np(yi|θ) = √\nexp -\n(yi - θ)2 .\n2σ2\n2πσ2\n√ 1\nLet f(yi) =\nexp -yi /2σ2 , u(yi) = yi/σ, φ(θ) = θ/σ, and g(θ) =\n2πσ2\nexp -θ2/2σ2 . Then,\nf(yi)g(θ) exp (φ(θ)u(yi)) = √\nexp - 2σ2 yi\nexp - 2σ2 θ2 exp 2σ2 2θyi\n2πσ2\n= √ 1\nexp - 1 (yi - θ)2\n2σ2\n2πσ2\n= p(yi|θ).\nThus the normal distribution with known variance is an exponential family.\nExample 3. (Exponential Distribution is an Exponential Family)\nConsider an exponential distribution:\np(yi|θ) = θe-θyi .\nLet f(yi) = 1, u(yi) = yi, φ(θ) = -θ, and g(θ) = θ. Then,\nf(yi)g(θ) exp (φ(θ)u(yi)) = θe-θyi = p(yi|θ).\nThus the exponential distribution is an exponential family.\nPosterior asymptotics\nUp to this point, we have defined a likelihood model that is parameterized\nby θ, assigned a prior distribution to θ, and then computed the posterior\np(θ|y). There are two natural questions that arise. First, what if we choose\nthe 'wrong' likelihood model? That is, what if the data were actually gener\nated by some distribution q(y) such that q(y) = p(y|θ) for any θ, but we use\np(y|θ) as our likelihood model? Second, what if we assign the 'wrong' prior?\nWe can answer both of these questions asymptotically as m →inf. First we\n\nZ\n\nZ\nZ\n\nZ\n\nZ\nmust develop a little machinery from information theory.\nA useful way to measure the dissimilarity between two probability distribu\ntions is the Kullback-Leibler (KL) divergence, defined for two distributions\np(y) and q(y) as:\n\nq(y)\nq(y)\nD(q(·)||p(·)) := 1y∼q(y) log\n=\nq(y) log\ndy.\np(y)\np(y)\nThe KL divergence is only defined if q(y) > 0 for any y such that p(y) > 0.\nIt is sometimes referred to as the KL distance, however it is not a metric in\nthe mathematical sense because in general it is asymmetric: D(q(·)||p(·))\n\nD(p(·)||q(·)). It is the average of the logarithmic difference between the prob\nability distributions p(y) and q(y), where the average is taken with respect\nto q(y). The following property of the KL divergence will be very important\nfor us.\nTheorem 2 (Non-negativity of KL Divergence). D(q(·)||p(·)) ≥ 0 with equal\nity if and only if q(y) = p(y) ∀y.\nProof. We will rely on Jensen's inequality, which states that for any convex\nfunction f and random variable X,\n1[f(X)] ≥ f(1[X]).\nWhen f is strictly convex, Jensen's inequality holds with equality if and only\nif X is constant, so that 1[X] = X and 1[f(X)] = f(X). Take y ∼ q(y) and\np(y)\ndefine the random variable X =\n. Let f(X) = - log(X), a strictly convex\nq(y)\nfunction. Now we can apply Jensen's inequality:\n1y∼q(y)[f(X)] ≥ f(1y∼q(y)[X])\np(y)\np(y)\n-\nq(y) log\ndy ≥- log\nq(y)\ndy\nq(y)\nq(y)\n≥- log\np(y)dy\n≥- log 1 = 0 so,\nq(y)\nq(y) log\ndy ≥ 0,\np(y)\nwith equality under the same conditions required for equality in Jensen's\ninequality: if and only if X is constant, that is, q(y) = p(y) ∀y.\nZ\n=\nZ\nZ\n\nZ\n\nZ\n\nWe will use the KL divergence to find the distribution from the likelihood\nfamily that is 'closest' to the true generating distribution:\nθ ∗ = arg min D(q(·)||p(·|θ)).\n(18)\nθ∈Θ\nFor convenience, we will suppose that the arg min in (18) is unique. The\nresults can be easily extended to the case where the arg min is not unique. The\nmain results of this section are two theorems, the first for discrete parameter\nspaces and the second for continuous parameter spaces. The intuition for the\nm\ntheorem is that as long as there is some probability in the prior that θ = θ∗ ,\nthen as m →inf the whole posterior will be close to θ∗ .\nTheorem 3 (Posterior consistency in finite parameter space). Let F be a\nfinite family of likelihood models, with Θ = {θ : p(·|θ) ∈ F} the finite pa\nrameter space. Let y = (y1, . . . , ym) be a set of independent samples from\nan arbitrary distribution q(·), and θ∗ be as in (18). If p(θ = θ∗) > 0, then\np(θ = θ∗|y) → 1 as m →inf.\nProof. Consider any θ\nθ∗ :\nm\np(θ|y)\np(θ)p(y|θ)\n= log\np(θ∗|y)\np(θ∗)p(y|θ∗)\np(θ)\np(yi|θ)\np(yi|θ∗)\nlog\n= log\n+\nlog\n(19)\np(θ∗)\ni=1\nm\nConsider the sum. By the strong law of large numbers, with probability 1,\nm\np(yi|θ)\np(y|θ)\n→ 1y∼q(y) log\np(yi|θ∗)\np(y|θ∗)\nlog\n.\n(20)\nm i=1\nWell,\nm\np(y|θ)\np(y|θ)q(y)\n1y∼q(y) log\n= 1y∼q(y) log\n(1 in disguise)\np(y|θ∗)\np(y|θ∗)q(y)\nq(y)\nq(y)\n= 1y∼q(y) log\n- log\np(y|θ∗)\np(y|θ)\n= D(q(·)||p(·|θ ∗ )) - D(q(·)||p(·|θ))\n< 0.\nWhy?\nBy (20), with probability 1, as m →inf,\nm\ni=1\np(yi|θ)\np(y|θ)\nlog\n→ m1y∼q(y) log\n= -inf.\np(yi|θ∗)\np(y|θ∗)\n\n=\n\nLet's plug back into (19). By supposition, p(θ∗) > 0 (and the prior doesn't\nchange as m →inf).Thus as m →inf, (19) obeys\nm\nm\np(θ|y)\np(θ)\np(yi|θ)\nlog\n= log\n+\nlog\n→ -inf,\np(θ∗|y)\np(θ∗)\np(yi|θ∗)\ni=1\nand\np(θ|y)\np(θ|y)\nlog\n→ -inf implies\n→ 0 which implies p(θ|y) → 0.\np(θ∗|y)\np(θ∗|y)\nThis holds for every θ\nθ∗, thus p(θ∗|y) → 1.\nAgain, this theorem tells us that the posterior eventually becomes concen\ntrated on the value θ∗ . θ∗ corresponds to the likelihood model that is 'closest'\nin the KL sense to the true generating distribution q(·). If q(·) = p(·|θ0) for\nsome θ0 ∈ Θ, then θ∗ = θ0 is the unique minimizer of (18) because p(·|θ∗) is\nclosest to p(·|θ0) when θ∗ = θ0. The theorem tells us that the posterior will\nbecome concentrated around the true value. This is only the case if in the\nprior, p(θ∗) > 0, which shows the importance of choosing a prior that assigns\nnon-zero probability to every plausible value of θ.\nWe now present the continuous version of Theorem 3, but the proof is quite\ntechnical and is omitted.\nTheorem 4 (Posterior consistency in continuous parameter space). If Θ\nis a compact set, θ∗ is defined as in (18), A is a neighborhood of θ∗ , and\np(θ∗∈ A) > 0, then p(θ ∈ A|y) → 1 as m →inf.\nThese theorems show that asymptotically, the choice of the prior does not\nmatter as long as it assigns non-zero probability to every θ ∈ Θ. They also\nshow that if the data were generated by a member of the family of likelihood\nmodels, we will converge to the correct likelihood model. If not, then we will\nconverge to the model that is 'closest' in the KL sense.\nWe give these theorems along with a word of caution. These are asymptotic\nresults that tell us absolutely nothing about the sort of m we encounter in\npractical applications. For small sample sizes, poor choices of the prior or\nlikelihood model can yield poor results and we must be cautious.\n=\n\nCoin Flip Example Part 6. Suppose that the coin flip data truly came from\na biased coin with a 3/4 probability of Heads, but we restrict the likelihood\nmodel to only include coins with probability in the interval [0, 1/2]:\np(y|θ) = θmH (1 - θ)m-mH ,\nθ ∈ [0, 1/2].\nThis time we will use a uniform prior, p(θ) = 2, θ ∈ [0, 1/2]. The posterior\ndistribution is then\nθmH (1 - θ)m-mH 2\np(θ|y) =\n.\n1/2 θ 'mH (1 - θ ' )m-mH 2dθ '\nThe partition function is an incomplete Beta integral, which has no closed\nform but can easily be solved numerically. In the following figure, we draw\nsamples iid from the true distribution (3/4 probability of Heads) and show\nhow the posterior becomes increasingly concentrated around θ = 0.5, the\nclosest likelihood function to the true generating distribution.\nHierarchical modeling\nHierarchical models are a powerful application of Bayesian analysis. They al\nlow us to reason about knowledge at multiple levels of abstraction. Basically,\nR\n\nthe prior parameters at the lower level come from the same distribution at\nthe higher level, and there is a prior at that level, and so on. This is best\nexplained by example, so we will develop a hierarchical model for topic mod\neling, an important information retrieval problem.\n5.1\nTopic models\nSuppose we have been given a collection of m documents and we wish to\ndetermine how the documents are related. For example, if the documents\nare all news articles, the article \"Patriots game canceled due to hurricane\"\nis related to the article \"New York Giants lose superbowl\" because they are\nboth about football. The article \"Patriots game canceled due to hurricane\"\nis also related to the article \"Record snowfall in May\" because they are both\nabout the weather. We will now develop a hierarchical model for finding topic\nrelationships between documents in an unsupervised setting. The method is\ncalled Latent Dirichlet Allocation (LDA) and it was developed by David Blei,\nAndrew Ng, and Michael Jordan.\n5.1.1\nLDA formulation\nThe model has several components. The data are m documents, with docu\nment i consisting of ni words. Each word in the document will be associated\nwith one of K topics. We let zi,j denote the topic of word j in document i.\nWe model zi,j ∼ Multinomial(θi), where θi ∈ JK describes the topic mixture\nof document i.\nFor each topic, we define a multinomial distribution over all possible words.\nFor example, given the topic is \"Sports\", the probability of having the word\n\"football\" might be high; if the topic were \"Weather\", the probability of hav\ning the word \"football\" might be lower. Other words, like \"the\" will have a\nhigh probability regardless of the topic. If words are chosen from a set of W\npossible words, then we let φk ∈ JW be the multinomial parameter over words\nfor topic k. Word j of document i, denoted wi,j, will be generated by the dis\ntribution over words corresponding to the topic zi,j: wi,j ∼ Multinomial(φzi,j ).\nFinally, we give prior distributions for the parameters θi and φk. The multi\nnomial distribution is a generalization of the binomial distribution, and its\n\nconjugate prior is a generalization of the beta distribution: the Dirichlet\ndistribution. Thus we model the data with the following generative model:\n1. For document i = 1, . . . , m, choose the document's topic distribution\nθi ∼ Dirichlet(α), where α ∈ JK is the prior hyperparameter.\n2. For topic k = 1, . . . , K, choose the topic's word distribution φk ∼\nDirichlet(β), where β ∈ JW is the prior hyperparameter.\n3. For document i = 1, . . . , m:\nFor word j = 1, . . . , ni:\nChoose the topic for this word zi,j ∼ Multinomial(θi).\nChoose the word wi,j ∼ Multinomial(φzi,j ).\n5.2\nGraphical representation\nHierarchical models are illustrated with a node for every variable and arcs\nbetween nodes to indicate the dependence between variables. Here's the one\nfor LDA:\nGraphs representing hierarchical models must be acyclic. For any node x,\nwe define Parents(x) as the set of all nodes with arcs to x. The hierarchical\nmodel consists of, for every node x, the distribution p(x|Parents(x)). Define\nDescendants(x) as all nodes that can be reached from x and Non-descendants(x)\nas all other nodes. Because the graph is acyclic and the distribution for each\nnode depends only on its parents, given Parents(x), x is conditionally inde\npendent from Non-descendants(x). This is a powerful fact about hierarchical\nmodels that is important for doing inference. In the graph for LDA, this\nmeans that, for example, zi,j is independent of α, given θi.\n\nIn addition to the graph structure, we use plates to denote repeated, inde\npendent draws from the same distribution. The plates are like the 'for' loops\nin the pseudocode of how the data is generated in the text description above.\nCoin Flip Example Part 7. Even simple models like the coin flip model\ncan be represented graphically. The coin flip model is:\n5.3\nInference in hierarchical models\nHierarchical models are useful because they allow us to model interactions\nbetween the observed variables (in this case the words in each document)\nthrough the use of latent (or hidden) variables.\nWhat are the latent variables for LDA?\nDespite introducing a larger number of latent variables than we have observed\nvariables, because of their additional structure hierarchical models are gen\nerally not as prone to overfitting as you might expect.\nWe are interested in infering the posterior distribution for the latent vari\nables. Let Z = {zi,j}i=1,...,m,j=1,...,ni , θ = {θi}i=1,...,m, φ = {φk}k=1,...,K , and\nW = {wi,j}i=1,...,m,j=1,...,ni . Then, by Bayes' rule, ignoring the constant de\nnominator, we can express the posterior as:\np(Z, θ, φ|w, α, β) ∝ p(w|Z, φ, θ, α, β)p(Z, θ, φ|α, β)\n(21)\nWe will look at each of these pieces and show that they have a compact\n\nanalytical form.\np(w|Z, φ, θ, α, β) =\np(wi,j|Z, φ, θ, α, β) By iid assumption\ni=1 j=1\nn\nm\ni\nm\nm\n=\np(wi,j|zi,j, φ) By conditional independence\ni=1 j=1\nn\nm\ni\nm\nm\nm\nni\n=\nMultinomial(wi,j; φzi,j ) By definition.\ni=1 j=1\nAlso,\np(Z, θ, φ|α, β) = p(Z, θ|α)p(φ|β)\nby conditional independence. Again considering each term, by the definition\nof conditional probability:\np(Z, θ|α) = p(Z|θ, α)p(θ|α),\nm\nwhere\nm\nm\nm\nm\nm\nm\nni\nm\nni\np(Z|θ, α) = p(Z|θ) =\np(zi,j|θi) =\nMultinomial(zi,j; θi),\ni=i j=1\ni=i j=1\nby conditional independence, iid, and definition as before. Further,\nm\nm\nm\nm\np(θ|α) =\np(θi|α) =\nDirichlet(θi; α),\ni=1\ni=1\nand\nm\nm\nK\nK\np(φ|β) =\np(φk|β) =\nDirichlet(φk; β).\nk=1\nk=1\nPlugging all of these pieces back into (21), we obtain\nm\nm\nm\nm\nm\nni\nm\nni\np(Z, θ, φ|w, α, β) ∝\nMultinomial(wi,j; φzi,j )\nMultinomial(zi,j; θi)×\ni=1 j=1\ni=i j=1\nm\nm\nm\nK\nDirichlet(θi; α)\nDirichlet(φk; β).\n(22)\ni=1\nk=1\n\nFor any given Z, θ, φ, w, α, and β, we can easily evaluate (22). (All the nor\nmalizations are known so it's easy.) We will see in the next section that this\nis sufficient to be able to simulate draws from the posterior.\nEven using conjugate priors, in general it will not be possible to recover\nthe posterior analytically for hierarchical models of any complexity. We will\nrely on (among a few other options) sampling methods like the Monte Carlo\nMarkov Chains (MCMC) that we discuss in the next section. What the\nstatistics community call Bayesian hierarchical models are in the machine\nlearning community often treated as a special case of Bayesian graphical\nmodels (specifically, directed acyclic graphs). There is at least one entire\ncourse at MIT on inference in Bayesian graphical models (6.438).\nMarkov Chain Monte Carlo sampling\nAs we have seen with hierarchical models, even with conjugate priors we are\nunable to express the posterior analytically. The reason Bayesian statistics\nis so widely used is because of the development of computational methods\nfor simulating draws from the posterior distribution. Even though we are\nunable to express the posterior analytically, with a large enough sample of\nsimulated draws we can compute statistics of the posterior with arbitrary\nprecision. This approach is called Monte Carlo simulation. We will describe\nthe two most commonly used Monte Carlo methods, which both fall under the\numbrella of Markov Chain Monte Carlo (MCMC) methods: the Metropolis-\nHastings algorithm, and Gibbs' sampling.\n6.1\nMarkov chains\nThe results from MCMC depend on some key results from the theory of\nMarkov chains. We will not do a thorough review of Markov Chains and will\nrather present the necessary results as fact. A continuous state Markov chain\nis a sequence θ0, θ1 , . . . with θt ∈ Jd that satisfies the Markov property:\np(θt|θt-1, . . . , θ1) = p(θt|θt-1).\nWe will be interested in the (unconditioned) probability distribution over\nstates at time t, which we denote as πt(θ) := Pr(θt = θ). Under some\n\nconditions that will be satisfied by all of the chains we are interested in,\nthe sequence of state distributions π0(θ), π1(θ), . . . will converge to a unique\ndistribution π(θ) which we call the stationary distribution, or equilibrium dis\ntribution or steady-state distribution.\nWe define the transition kernel to be the probability of transitioning from\nstate θ to state θ ' : K(θ, θ ' ) = p(θ '|θ). We then have the following important\nfact.\nFact: if the distribution π(·) satisfies the detailed balance equation:\nK(θ, θ ' )π(θ) = K(θ ' , θ)π(θ ' ), for all θ, θ ' ,\n(23)\nthen π(·) is the stationary distribution. The interpretation of the detailed\nbalance equation is that the amount of mass transitioning from θ ' to θ is the\nsame as the amount of mass transition back from θ to θ ' . For a stationary\ndistribution, we cannot have mass going from state to state.\n6.2\nMetropolis-Hastings algorithm\nThe goal in MCMC is to construct a Markov Chain whose stationary dis\ntribution is the posterior p(θ|y). We now present the Metropolis-Hastings\nalgorithm. In addition to the distributions we have already used (likelihood\nand prior), we will need a proposal distribution (or jumping distribution)\nJ(θ, θ ' ) which will propose a new state θ ' given the current state θ.\nThere are many options when choosing a proposal distribution which we will\ndiscuss later. The proposal distribution will yield a random walk over the\nparameter space, proposing steps θ → θ ' . We accept or reject each step\ndepending on the relative posterior probabilities for θ and θ ' . When we run\nthe random walk for long enough, the accepted values will simulate draws\nfrom the posterior.\n6.2.1\nSome intuition into the Metropolis-Hastings algorithm\nSuppose we are considering the transition θ → θ ' . If p(θ '|y) is larger than\np(θ|y), then for every accepted draw of θ, we should have at least as many\naccepted draws of θ ' and so we should always accept the transition θ → θ ' .\n\nIf p(θ '|y) is less than p(θ|y), then for every accepted draw θ, we should have\np(θ0|y)\non average\naccepted draws of θ ' . We thus accept the transition with\np(θ|y)\np(θ0|y)\nprobability\n. Thus for any transition, we accept the transition with\np(θ|y)\nprobability\n\np(θ '|y)\nmin\n, 1\n.\n(24)\np(θ|y)\n6.2.2\nSteps of the algorithm\nWe now give the steps of the Metropolis-Hastings algorithm.\nStep 1. Choose a starting point θ0 . Set t = 1.\nStep 2. Draw θ∗ from the proposal distribution J(θt-1 , ·). The proposed move\nfor time t is to move from θt-1 to θ∗ .\nStep 3. Compute the following:\n\np(θ∗|y)J(θ∗, θt-1)\nα(θt-1, θ ∗ ) := min\n, 1\np(θt-1|y)J(θt-1, θ∗)\n\np(y|θ∗)p(θ∗)J(θ∗, θt-1)\n= min\n, 1\n(25)\np(y|θt-1)p(θt-1)J(θt-1, θ∗)\nWe'll explain this more soon. The fact that we can compute ratios of\nposterior probabilities without having to worry about the normalization\nintegral is the key to Monte Carlo methods.\nStep 4. With probability α(θt-1, θ∗), accept the move θt-1 → θ∗ by setting θt =\nθ∗ and incrementing t ← t + 1. Otherwise, discard θ∗ and stay at θt-1 .\nStep 5. Until stationary distribution and the desired number of draws are reached,\nreturn to Step 2.\nEquation (25) reduces to what we developed intuition for in (24) when the\nproposal distribution is symmetric: J(θ, θ ' ) = J(θ ' , θ). We will see in the\nnext theorem that the extra factors in (25) are necessary for the posterior to\nbe the stationary distribution.\n\n6.2.3\nConvergence of Metropolis-Hastings\nBecause the proposal distribution and α(θt-1, θ∗) depend only on the cur\nrent state, the sequence θ0, θ1 , . . . forms a Markov chain. What makes the\nMetropolis-Hastings algorithm special is the following theorem, which shows\nthat if we simulate the chain long enough, we will simulate draws from the\nposterior.\nTheorem 5. If J(θ, θ ' ) is such that that the Markov chain θ0, θ1 , . . . produced\nby the Metropolis-Hastings algorithm has a unique stationary distribution,\nthen the stationary distribution is p(θ|y).\nProof. We will use the fact given in (23) that if the posterior p(θ|y) satisfies\nthe detailed balance equation, then it is the stationary distribution. Thus we\nwish to show:\nK(θ, θ ' )p(θ|y) = K(θ ' , θ)p(θ '|y), for all θ, θ ' .\nwhere the transition kernel comes from Metropolis-Hastings:\nK(θ, θ ' ) = (probability of proposing θ ' )×\n(probability of accepting θ ' given it was proposed)\n= J(θ, θ ' )α(θ, θ ' ).\nTo show that the detailed balance equation holds, take any θ and θ ' , and\nwithout loss of generality, suppose that α is less than or equal to 1 for the\ntransition θ to θ ' , which means\nJ(θ, θ ' )p(θ|y) ≥ J(θ ' , θ)p(θ '|y),\nso that\nJ(θ ' , θ)p(θ '|y)\nα(θ, θ ' ) =\n,\nJ(θ, θ ' )p(θ|y)\nand α(θ ' , θ) = 1. Now let's plug:\nK(θ, θ ' )p(θ|y) =J(θ, θ ' )α(θ, θ ' )p(θ|y)\nJ(θ ' , θ)p(θ '|y)\n=J(θ, θ ' )p(θ|y) J(θ, θ')p(θ|y)\n=J(θ ' , θ)p(θ '|y) (cancel terms)\n=J(θ ' , θ)α(θ ' , θ)p(θ '|y) (1 in disguise)\n=K(θ ' , θ)p(θ '|y).\nAnd that's the detailed balance equation.\n\nWe have now proven that the Metropolis-Hastings algorithm simulations will\neventually draw from the posterior distribution. However, there are a number\nof important questions to be addressed. What proposal distribution should\nwe use? How many iterations will it take for the chain to be sufficiently close\nto the stationary distribution? How will we know when the chain has reached\nits stationary distribution? We will discuss these important issues after we\nintroduce the Gibbs' sampler.\n6.3\nGibbs' Sampler\nThe Gibbs' sampler is a very powerful MCMC sampling technique for the spe\ncial situation when we have access to conditional distributions. It is a special\ncase of the Metropolis-Hastings algorithm that is typically much faster, but\ncan only be used in special cases.\nLet us express θ ∈ Jd as θ = [θ1, . . . , θd]. Suppose that although we are not\nable to draw directly from p(θ|y) because of the normalization integral, we\nare able to draw samples from the conditional distribution\np(θj|θ1, . . . , θj-1, θj+1, . . . , θd, y).\nIn fact, it is often the case in hierarchical models that their structure al\nlows us to determine analytically these conditional posterior distributions.\nThis can be done for LDA, and the derivation is quite lengthy but can\nbe found on the Wikipedia article for LDA. The Gibbs' sampler updates\nthe posterior variables θ1, θ2, . . . , θd one at a time. At each step, all of\nthem are held constant in their current state except for one (j), which is\nthen updated by drawing from its (known) conditional posterior distribu\ntion, p(θj|θ1, . . . , θj-1, θj+1, . . . , θd, y). We then hold it in its state and move\non to the next variable to update it similarly. When we do this iterative up\ndating for long enough, we eventually simulate draws from the full posterior.\nThe full algorithm is:\nStep 1. Initialize θ0 = [θ1\n0, . . . , θ0]. Set t = 1.\nd\nStep 2. For j ∈{1, . . . , d}, sample θt from p(θj|θ1\nt , . . . , θt , θt-1 , . . . , θt-1 , y).\nj\nj-1\nj+1\nd\nStep 3. Until stationary distribution and the desired number of draws are reached,\nincrement t ← t + 1 and return to Step 2.\n\nIn each iteration of the Gibbs' sampler, we sequentially update each compo\nnent of θt . We could do that updating in any order, it does not have to be\n1, . . . , d.\nThe Gibbs' sampler is a special case of Metropolis-Hastings where the pro\nposal distribution is taken to be the conditional posterior distribution. In\nfact, it is easy to show (but notationally extremely cumbersome) that when\nwe use these conditional posterior distributions as proposal distributions in\nMetropolis-Hastings, the probability of accepting any proposed move is 1,\nhence in the Gibbs' sampler we accept every move.\n6.4\nPractical considerations\nNow that we have seen the general idea of MCMC algorithms and some theory\nbehind them, let us dive into some details.\n6.4.1\nProposal distributions\nTo guarantee existence of a stationary distribution, all that is required (with\nrare exceptions) is for the proposal distribution J(·, ·) to be such that there is\na positive probability of eventually reaching any state from any other state.\nA typical proposal distribution is a random walk J(θ, θ ' ) = N (θ, σ2) for some\nσ2 . There are several important features of proposal distributions that work\nwell in practice. First, we must be able to sample from it efficiently. Second,\nwe must be able to compute the ratio α(θ, θ ' ) in an efficient way. Third, the\njumps should not be too large or we will reject them frequently and the chain\nwill not move quickly. Fourth, the jumps should not be too small or it will\ntake a long time to explore the whole space. The balance between 'too small'\nand 'too large' is the subject of hundreds of papers on 'adaptive MCMC',\nbut there is really no good way to know which proposal distribution to use.\nIn practice, we often try several proposal distributions to see which is most\nappropriate, for example, by adjusting σ2 in the above proposal distribution.\n6.4.2\nOn reaching the stationary distribution\nUnfortunately, it is impossible to know how many iterations it will take to\nreach the stationary distribution, or even to be certain when we have arrived.\n\nThis is probably the largest flaw in MCMC sampling. There are a large num\nber of heuristics which are used to assess convergence that generally involve\nlooking at how θt varies with time. In general, the initial simulations depend\nstrongly on the starting point and are thrown away. This is referred to as\nburn-in, and often involves discarding the first half of all draws.\nTo reduce the effect of autocorrelations between draws, we typically only\nstore a small fraction of them, for example we store only 1 out of every 1000\naccepted draws. This process is called thinning.\nFinally, we can assess convergence by comparing the distribution of the first\nhalf of stored draws to the distribution of the second half of stored draws.\nIf the chain has reached its stationary distribution, these should have sim\nilar distributions, as assessed by various statistics. These (and similar) ap\nproaches have been shown to be successful in practice. Unfortunately, there\nis no way to be entirely certain that we are truly drawing from the posterior\nand we must be very cautious.\nCoin Flip Example Part 8. Let us return to our coin flip example. We\nwill draw from the posterior using the Metropolis-Hastings algorithm. Our\nmodel has a scalar parameter θ ∈ [0, 1] which is the probability of heads.\nThe proposal distribution J(θt-1, θ∗) will be uniform on an interval of size r\naround θt-1:\nif θ∗∈ [θt-1 8 r , θt-1 ⊕ r ]\nr\nJt(θt-1, θ ∗ ; r) =\notherwise\nwhere ⊕ and 8 represent modular addition and subtraction on [0, 1], e.g.,\n0.7 ⊕ 0.5 = 0.2. Notice that this proposal distribution is symmetric:\nJ(θt-1, θ ∗ ; r) = J(θ ∗ , θt-1; r).\nWe accept the proposed θ∗ with probability\np(y|θ∗)p(θ∗)J(θ∗, θt-1; r)\nα(θt-1, θ ∗ ) = min\n, 1\np(y|θt-1)p(θt-1)J(θt-1, θ∗ ; r)\np(y|θ∗)p(θ∗)\n= min\n, 1\np(y|θt-1)p(θt-1)\n(θ∗)mH +α-1(1 - θ∗)m-mH +β-1\n= min\n, 1 ,\n(θt-1)mH +α-1(1 - θt-1)m-mH +β-1\n\nwhich we can easily compute. This formula and a uniform random number\ngenerator for the proposal distribution are all that is required to implement\nthe Metropolis-Hastings algorithm.\nConsider the specific case of m = 25, mH = 6, and α = β = 5. The following\nthree figures show the time sequence of proposals θ∗ for chains with r = 0.01,\nr = 0.1, and r = 1 respectively, with the colors indicating whether each\nproposed θ∗ was accepted or not, and time along the x-axis. In this first\nfigure we see that with r = 0.01, the step sizes are too small and after 2000\nproposals we have not reached the stationary distribution.\nIn the next figure, with r = 1 the steps are too large and we reject most of\nthe proposals. This leads to a small number of accepted draws after 2000\nproposals.\n\nThe chain with r = 0.1 is the happy medium, where we rapidly reach the\nstationary distribution, and accept most of the samples.\nTo compare this to the analytic distribution that we obtained in (14), namely\np(θ|y) ∼ Beta (mH + α, m - mH + β) .\nwe run a chain with r = 0.1 until we have collected 25,000 accepted draws. We\nthen discard the initial 200 samples (burn-in) and keep one out of every 100\nsamples from what remains (thinning). The next figure shows a normalized\n\nhistogram of the resulting draws, along with the analytical posterior. The\nrunning time to generate the MCMC draws was less than a second, and they\nare a reasonable approximation to the posterior.\nMCMC with OpenBUGS\nThere is a great deal of \"art\" to MCMC simulation, and a large body of\nresearch on the \"right\" way to do the simulations. OpenBUGS is a nice\nsoftware package that has built-in years of research in MCMC and can draw\nposterior samples in a fairly automated way. It is great for doing Bayesian\nanalysis without having to get your hands too dirty with MCMC details.\nHere we demonstrate OpenBUGS using two examples.\nOpenBUGS is old and is infrequently updated, but is still functional and\npowerful. The graphical interface version of OpenBugs is available only as\na Windows executable, but for Linux and Mac users, we had no difficulties\nrunning the Windows executable with Wine.\n\n7.1\nOpenBUGS example 1: Coin flip example\nBy this point it should come as no surprise that our first example will be the\ncoin flip example.\nWhen you first open OpenBUGS,\nyou will be greeted with a screen\nthat looks something like this.\nFile -> New will open what is es\nsentially a blank text file. There are\nthree items that must be specified to\ndo simulations in OpenBUGS: The\nmodel, the data, and initial condi\ntions. All of these are to be specified\nin this text file.\nSpecifying the model in OpenBUGS is very natural, and uses two main sym\nbols: \"<-\" means deterministic assignment, as in R, and \"∼\" means distribu\ntional assignment. OpenBUGS includes many different distributions, a full\nlist of how to specify them is at:\nhttp://mathstat.helsinki.fi/openbugs/Manuals/ModelSpecification.html#ContentsAI\nWe specify the coin flip model as:\nmodel{\nY ∼ dbin(theta,m)\ntheta ∼ dbeta(alpha,beta)\nScreenshots (c) OpenBUGS. Some rights reserved; the OpenBUGS software is available under the GNU\nGeneral Public License. This content is excluded from our Creative Commons license. For more information,\nsee http://ocw.mit.edu/fairuse\n. .\n\n}\nHere dbin and dbeta are the binomial and beta distributions respectively.\nNotice that in the model we do not specify the constants m, α, and β.\nTo load the model into OpenBUGS,\nwe first enter it into the text en\ntry space in OpenBUGS. Then from\nthe \"Model\" menu, we go Model\n-> Specification. This opens the\nmodel specification window, seen to\nthe right. To set the model as the\ncurrent working model, we highlight\nthe word \"model\" that we typed\nin, as in the image on the right,\nand while it is highlighted we select\n\"check model.\" When we do this,\nthe text \"model is syntatically\ncorrect\" appears along the bottom.\nWe then specify the data, including all constants. Data is specified in Open-\nBUGS as an R list. Here, for 10 Heads out of 40 flips and priors α = β = 5,\nwe write:\nlist(Y=10,m=40,alpha=5,beta=5)\nTo load the data into OpenBUGS,\nwe first type the data list into the\ntext entry space in OpenBUGS. We\nthen highlight the word \"list\", as\nin the image on the right, and\nwhile it is highlighted we click \"load\ndata\" in the model specification\nwindow. When we do this, \"data\nloaded\" appears along the bottom\nof the screen.\nScreenshots (c) OpenBUGS. Some rights reserved; the OpenBUGS software is available under the GNU\nGeneral Public License. This content is excluded from our Creative Commons license. For more information,\nsee http://ocw.mit.edu/fairuse.\n\nWe are now ready to specify the\nnumber of chains and compile the\nmodel. We use multiple chains from\nmultiple initial conditions to test\nconvergence; it is usually sufficient\nto use 2 chains. We input 2 chains\nand press \"compile\" in the model\nspecification window. Then, \"model\ncompiled\" appears along the bot\ntom. If we did not specify one of\nthe variables (for instance, if we had\nmistakenly left beta out of the data\nlist) then it will give an error when\nwe try to compile.\nFinally, we specify the initial parameter values for each chain, each as an R\nlist. Here we will start one chain from θ = 1, and the other from θ = 0.\nlist(theta=0)\nlist(theta=1)\nFor the first initial value we high\nlight the word \"list\", and click\n\"load inits\" in the model speci\nfication. The number scroll to the\nright of the \"load inits\" button\nindicates that we are loading the\ninitial values for chain 1.\nWhen\nwe do this, it says on the bottom\nof the screen \"initial values\nloaded and chain initialized\nbut another chain,\" referring to\nthe fact that chain 2 still has not\nbeen initialized.\nScreenshots (c) OpenBUGS. Some rights reserved; the OpenBUGS software is available under the GNU\nGeneral Public License. This content is excluded from our Creative Commons license. For more information,\nsee http://ocw.mit.edu/fairuse.\n\nTo initialize chain 2, we simply fol\nlow the same procedure with the sec\nond list, and with the number scroll\nset to 2. At the bottom of the screen\nit then says \"model initialized.\"\nAlternatively, instead of loading intial values for both chains, we could have\npressed the button \"gen inits\" in the model specification window, which\ngenerates random initial values for both chains.\nWe may now close the model specification window.\nOpenBUGS will only store whatever results we tell it to. Before simulating\nthe chains, we must tell OpenBUGS to store the values of theta that it\nsimulates.\nTo specify which variables\nto store, go Inference ->\nSamples.\nHere we set\n\"nodes,\" which is the Open-\nBUGS term for stored vari\nables.\nTo tell it to store\ntheta, we type \"theta\" into\nthe node entry box, and\npress \"set\" below.\nThe\nother entry boxes will be\nused for things like burn-in\nand thinning, which we will\nworry about later.\nNow we are ready to simulate the chains! We may close the sample monitor\nScreenshots (c) OpenBUGS. Some rights reserved; the OpenBUGS software is available under the GNU\nGeneral Public License. This content is excluded from our Creative Commons license. For more information,\nsee http://ocw.mit.edu/fairuse.\n\ntool for now.\nTo simulate the chains, we go Model\n-> Update. This opens the update\ntool which we use to run the simu\nlations. We simply enter the desired\nnumber of draws (we choose 10,000),\nand the amount of thinning (we en\nter 10, meaning keep only one out\nof every 10 draws). The \"refresh\"\nentry refers to how often it will re\nfresh the \"iteration\" count during\nthe simulation and is not very im\nportant.\nWhen we have entered\nin the desired parameters, we press\n\"update\". The iteration count in\ncreases from 0 to 10,000, and finally\nat the bottom of the screen it reports\n\"10000 updates took 5 s.\"\nTo view the results, we close the update tool and return to the sample monitor\ntool (Inference -> Samples). Here there are a few useful quantities to look\nat.\nWe type in \"theta\" as the\nnode we are interested in.\nWe set \"501\" in the \"beg\"\nentrybox to specify that we\nwill begin inference at in\nteration 501, thus ignoring\nthe first 500 iterations as\nburn-in. The buttons we are\nmost interested in are den\nsity, coda, trace, history, and\naccept.\nScreenshots (c) OpenBUGS. Some rights reserved; the OpenBUGS software is available under the GNU\nGeneral Public License. This content is excluded from our Creative Commons license. For more information,\nsee http://ocw.mit.edu/fairuse.\n\nPressing \"density\" produces the\nMCMC estimte of the posterior den\nsity, what we are most interested in.\nIt specifies that the density was cre\nated using 19,000 samples (10,000\nfor each chain minus 500 burn-in).\nPressing \"trace\" shows the values\nfor the last 200 draws for each chain,\nso we can verify that the chains are\nstationary and that they are similar\nas a check of convergence. This im\nage is shown on the right. Pressing\n\"history\" provides a similar plot,\nwith all 10,000 iterations instead of\nonly the last 200. Pressing \"coda\"\ndumps the values at each iteration\nto a text file which you can then load\ninto R for further analysis.\nPressing \"accept\" shows the acceptance rate vs. iteration number, which is\nthe following figure.\nOpenBUGS is smart enough to notice the Binomial-Beta conjugacy and uses\na Gibbs' sampler, hence the acceptance rate is uniformly 1.\n\n7.2\nOpenBUGS example 2: estimating pump failure rates\nThe following example is from the OpenBUGS manual, and can be found at:\nhttp://www.openbugs.info/Examples/Pumps.html\nThis example is a hierarchical model for pump failure. There are 10 pumps,\npump i being in operation for a certain amount of time ti and having had a\ncertain number of failures xi. We wish to estimate the failure rate for each\npump, and we will do so with a Bayesian hierarchical model. We suspect that\nthe failure rates are related for all of the pumps, so we include a common prior.\nWe give flexibility to the prior by sampling it from a diffuse distribution. The\ngenerative model for the observed number of pump failures is then:\n1. Choose the first prior hyperparameter α ∼ Exponential(1).\n2. Choose the second prior hyperparameter β ∼ Gamma(0.1, 1.0).\n3. For pump i = 1, . . . , 10:\nChoose this pump's failure rate θi ∼ Gamma(α, β).\nChoose the number of failures xi ∼ Poisson(θiti).\nWe input this model into OpenBUGS as\nmodel{\nalpha ∼ dexp(1)\nbeta ∼ dgamma(0.1, 1.0)\nfor (i in 1 : 10) {\ntheta[i] ∼ dgamma(alpha, beta)\nlambda[i] <- theta[i]*t[i]\nx[i] ∼ dpois(lambda[i])\n}\n}\nNotice in the above that OpenBUGS does not allow mathematical operations\ninside of calls to distributions like dpois(theta[i]*t[i]), so we had to use\na deterministic relationship to specify the new variable lambda[i]. The data\nto be specified are \"t\" and \"x\", and we do this with an R list:\n\nlist(t = c(94.3, 15.7, 62.9, 126, 5.24, 31.4, 1.05, 1.05, 2.1, 10.5),\nx = c( 5, 1, 5, 14, 3, 19, 1, 1, 4, 22))\nWe will allow OpenBUGS to generate the initial values for us randomly. We\nsimulate two chains exactly as in the coin flip example, and easily recover the\nposterior densities for the failure rates for each pump. In the following figure\nwe show some of the posterior distributions:\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.097 Prediction: Machine Learning and Statistics\nSpring 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "15.097 Lecture 2: R for machine learning",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-097-prediction-machine-learning-and-statistics-spring-2012/be146ce1af9c60a50ee7d07f0e3b421f_MIT15_097S12_lec02.pdf",
      "content": "R for Machine Learning\nAllison Chang\nIntroduction\nIt is common for today's scientific and business industries to collect large amounts of data, and the ability to\nanalyze the data and learn from it is critical to making informed decisions. Familiarity with software such as R\nallows users to visualize data, run statistical tests, and apply machine learning algorithms. Even if you already\nknow other software, there are still good reasons to learn R:\n1. R is free. If your future employer does not already have R installed, you can always download it for free,\nunlike other proprietary software packages that require expensive licenses. No matter where you travel, you\ncan have access to R on your computer.\n2. R gives you access to cutting-edge technology. Top researchers develop statistical learning methods\nin R, and new algorithms are constantly added to the list of packages you can download.\n3. R is a useful skill. Employers that value analytics recognize R as useful and important. If for no other\nreason, learning R is worthwhile to help boost your r esum e.\nNote that R is a programming language, and there is no intuitive graphical user interface with buttons you can\nclick to run different methods. However, with some practice, this kind of environment makes it easy to quickly\ncode scripts and functions for various statistical purposes. To get the most out of this tutorial, follow the examples\nby typing them out in R on your own computer. A line that begins with > is input at the command prompt. We\ndo not include the output in most cases, but you should try out the commands yourself and see what happens.\nIf you type something at the command line and decide not to execute, press the down arrow to clear the line;\npressing the up arrow gives you the previous executed command.\n1.1\nGetting Started\nThe R Project website is http://www.r-project.org/. In the menu on the left, click on CRAN under \"Download,\nPackages.\" Choose a location close to you. At MIT, you can go with University of Toronto under Canada. This\nleads you to instructions on how to download R for Linux, Mac, or Windows.\nOnce you open R, to figure out your current directory, type getwd(). To change directory, use setwd (note\nthat the \"C:\" notation is for Windows and would be different on a Mac):\n> setwd(\"C:\\\\Datasets\")\n1.2\nInstalling and loading packages\nFunctions in R are grouped into packages, a number of which are automatically loaded when you start R. These\ninclude \"base,\" \"utils,\" \"graphics,\" and \"stats.\" Many of the most essential and frequently used functions come\nin these packages. However, you may need to download additional packages to obtain other useful functions. For\nexample, an important classification method called Support Vector Machines is contained in a package called\n\n\"e1071.\" To install this package, click \"Packages\" in the top menu, then \"Install package(s)...\" When asked to\nselect a CRAN mirror, choose a location close to you, such as \"Canada (ON).\" Finally select \"e1071.\" To load\nthe package, type library(e1071) at the command prompt. Note that you need to install a package only\nonce, but that if you want to use it, you need to load it each time you start R.\n1.3\nRunning code\nYou could use R by simply typing everything at the command prompt, but this does not easily allow you to save,\nrepeat, or share your code. Instead, go to \"File\" in the top menu and click on \"New script.\" This opens up a new\nwindow that you can save as a .R file. To execute the code you type into this window, highlight the lines you\nwish to run, and press Ctrl-R on a PC or Command-Enter on a Mac. If you want to run an entire script, make\nsure the script window is on top of all others, go to \"Edit,\" and click \"Run all.\" Any lines that are run appear in\nred at the command prompt.\n1.4 Help in R\nThe functions in R are generally well-documented. To find documentation for a particular function, type ?\nfollowed directly by the function name at the command prompt. For example, if you need help on the \"sum\"\nfunction, type ?sum. The help window that pops up typically contains details on both the input and output for\nthe function of interest. If you are getting errors or unexpected output, it is likely that your input is\ninsufficient or invalid, so use the documentation to figure out the proper way to call the function.\nIf you want to run a certain algorithm but do not know the name of the function in R, doing a Google search\nof R plus the algorithm name usually brings up information on which function to use.\nDatasets\nWhen you test any machine learning algorithm, you should use a variety of datasets. R conveniently comes with\nits own datasets, and you can view a list of their names by typing data() at the command prompt. For instance,\nyou may see a dataset called \"cars.\" Load the data by typing data(cars), and view the data by typing cars.\nAnother useful source of available data is the UCI Machine Learning Repository, which contains a couple\nhundred datasets, mostly from a variety of real applications in science and business. The repository is located at\nhttp://archive.ics.uci.edu/ml/datasets.html. These data are often used by machine learning researchers\nto develop and compare algorithms. We have downloaded a number of datasets for your use, and you can find\nthe text files\n\n.\nThese\n\ninclude:\n\nName\nRows\nCols\nData\nIris\nReal\nWine\nInteger, Real\nHaberman's Survival\nInteger\nHousing\nCategorical, Integer, Real\nBlood Transfusion Service Center\nInteger\nCar Evaluation\nCategorical\nMushroom\nBinary\nPen-based Recognition of Handwritten Digits\nInteger\nin the Datasets section\n\nYou are encouraged to download your own datasets from the UCI site or other sources, and to use R to study the\ndata. Note that for all except the Housing and Mushroom datasets, there is an additional class attribute column\nthat is not included in the column counts. Also note that if you download the Mushroom dataset from the UCI\nsite, it has 22 categorical features; in our version, these have been transformed into 119 binary features.\nBasic Functions\nIn this section, we cover how to create data tables, and analyze and plot data. We demonstrate by example how\nto use various functions. To see the value(s) of any variable, vector, or matrix at any time, simply enter its name\nin the command line; you are encouraged to do this often until you feel comfortable with how each data structure\nis being stored. To see all the objects in your workspace, type ls(). Also note that the arrow operator <- sets\nthe left-hand side equal to the right-hand side, and that a comment begins with #.\n3.1\nCreating data\nTo create a variable x and set it equal to 1, type x <- 1. Now suppose we want to generate the vector [1, 2, 3, 4, 5],\nand call the vector v. There are a couple different ways to accomplish this:\n> v <- 1:5\n> v <- c(1,2,3,4,5)\n# c can be used to concatenate multiple vectors\n> v <- seq(from=1,to=5,by=1)\nThese can be row vectors or column vectors. To generate a vector v0 of six zeros, use either of the following.\nClearly the second choice is better if you are generating a long vector.\n> v0 <- c(0,0,0,0,0,0)\n> v0 <- seq(from=0,to=0,length.out=6)\nWe can combine vectors into matrices using cbind and rbind. For instance, if v1, v2, v3, and v4 are vectors of\nthe same length, we can combine them into matrices, using them either as columns or as rows:\n> v1 <- c(1,2,3,4,5)\n> v2 <- c(6,7,8,9,10)\n> v3 <- c(11,12,13,14,15)\n> v4 <- c(16,17,18,19,20)\n> cbind(v1,v2,v3,v4)\n> rbind(v1,v2,v3,v4)\nAnother way to create the second matrix is to use the matrix function to reshape a vector into a matrix of the\nright dimensions.\n> v <- seq(from=1,to=20,by=1)\n> matrix(v, nrow=4, ncol=5)\nNotice that this is not exactly right--we need to specify that we want to fill in the matrix by row.\n> matrix(v, nrow=4, ncol=5, byrow=TRUE)\nIt is often helpful to name the columns and rows of a matrix using colnames and rownames. In the following,\nfirst we save the matrix as matrix20, and then we name the columns and rows.\n\n> matrix20 <- matrix(v, nrow=4, ncol=5, byrow=TRUE)\n> colnames(matrix20) <- c(\"Col1\",\"Col2\",\"Col3\",\"Col4\",\"Col5\")\n> rownames(matrix20) <- c(\"Row1\",\"Row2\",\"Row3\",\"Row4\")\nYou can type colnames(matrix20)/rownames(matrix20) at any point to see the column/row names for matrix20.\nTo access a particular element in a vector or matrix, index it by number or by name with square braces:\n> v[3]\n# third element of v\n> matrix20[,\"Col2\"]\n# second column of matrix20\n> matrix20[\"Row4\",]\n# fourth row of matrix20\n> matrix20[\"Row3\",\"Col1\"]\n# element in third row and first column of matrix20\n> matrix20[3,1]\n# element in third row and first column of matrix20\nYou can find the length of a vector or number of rows or columns in a matrix using length, nrow, and ncol.\n> length(v1)\n> nrow(matrix20)\n> ncol(matrix20)\nSince you will be working with external datasets, you will need functions to read in data tables from text files.\nFor instance, suppose you wanted to read in the Haberman's Survival dataset (from the UCI Repository). Use\nthe read.table function:\ndataset <- read.table(\"C:\\\\Datasets\\\\haberman.csv\", header=FALSE, sep=\",\")\nThe first argument is the location (full path) of the file. If the first row of data contains column names, then the\nsecond argument should be header = TRUE, and otherwise it is header = FALSE. The third argument contains\nthe delimiter. If the data are separated by spaces or tabs, then the argument is sep = \" \" and sep = \"\\t\"\nrespectively. The default delimiter (if you do not include this argument at all) is \"white space\" (one or more\nspaces, tabs, etc.). Alternatively, you can use setwd to change directory and use only the file name in the\nread.table function. If the delimiter is a comma, you can also use read.csv and leave off the sep argument:\ndataset <- read.csv(\"C:\\\\Datasets\\\\haberman.csv\", header=FALSE)\nUse write.table to write a table to a file. Type ?write.table to see details about this function. If you need to\nwrite text to a file, use the cat function.\nA note about matrices versus data frames: A data frame is similar to a matrix, except it can also include\nnon-numeric attributes. For example, there may be a column of characters. Some functions require the data\npassed in to be in the form of a data frame, which would be stated in the documentation. You can always coerce\na matrix into a data frame using as.data.frame. See Section 3.6 for an example.\nA note about factors: A factor is essentially a vector of categorical variables, encoded using integers. For\ninstance, if each example in our dataset has a binary class attribute, say 0 or 1, then that attribute can be\nrepresented as a factor. Certain functions require one of their arguments to be a factor. Use as.factor to encode\na vector as a factor. See Sections 4.5 and 4.9 for examples.\n3.2\nSampling from probability distributions\nThere are a number of functions for sampling from probability distributions. For example, the following commands\ngenerate random vectors of the user-specified length n from distributions (normal, exponential, poisson, uniform,\nbinomial) with user-specified parameters. There are other distributions as well.\n\n> norm_vec <- rnorm(n=10, mean=5, sd=2)\n> exp_vec <- rexp(n=100, rate=3)\n> pois_vec <- rpois(n=50, lambda=6)\n> unif_vec <- runif(n=20, min=1, max=9)\n> bin_vec <- rbinom(n=20, size=1000, prob=0.7)\nSuppose you have a vector v of numbers. To randomly sample, say, 25 of the numbers, use the sample function:\n> sample(v, size=25, replace=FALSE)\nIf you want to sample with replacement, set the replace argument to TRUE.\nIf you want to generate the same random vector each time you call one of the random functions listed above,\npick a \"seed\" for the random number generator using set.seed, for example set.seed(100).\n3.3\nAnalyzing data\nTo compute the mean, variance, standard deviation, minimum, maximum, and sum of a set of numbers, use mean,\nvar, sd, min, max, and sum. There are also rowSum and colSum to find the row and column sums for a matrix.\nTo find the component-wise absolute value and square root of a set of numbers, use abs and sqrt. Correlation\nand covariance for two vectors are computed with cor and cov respectively.\nLike other programming languages, you can write if statements, and for and while loops. For instance, here\nis a simple loop that prints out even numbers between 1 and 10 (%% is the modulo operation):\n> for (i in 1:10){\n+ if (i %% 2 == 0){\n+\ncat(paste(i, \"is even.\\n\", sep=\" \"))\n# use paste to concatenate strings\n+\n}\n+ }\nThe 1:10 part of the for loop can be specified as a vector. For instance, if you wanted to loop over indices 1, 2,\n3, 5, 6, and 7, you could type for (i in c(1:3,5:7)).\nTo pick out the indices of elements in a vector that satisfy a certain property, use which, for example:\n> which(v >= 0)\n# indices of nonnegative elements of v\n> v[which(v >= 0)]\n# nonnegative elements of v\n3.4\nPlotting data\nWe use the Haberman's Survival data (read into data frame dataset) to demonstrate plotting functions. Each\nrow of data represents a patient who had surgery for breast cancer. The three features are: the age of the patient\nat the time of surgery, the year of the surgery, and the number of positive axillary nodes detected. Here we plot:\n1. Scatterplot of the first and third features,\n2. Histogram of the second feature,\n3. Boxplot of the first feature.\nTo put all three plots in a 1 × 3 matrix, use par(mfrow=c(1,3)). To put each plot in its own window, use\nwin.graph() to create new windows.\n> plot(dataset[,1], dataset[,3], main=\"Scatterplot\", xlab=\"Age\", ylab=\"Number of Nodes\", pch=20)\n> hist(dataset[,2], main=\"Histogram\", xlab=\"Year\", ylab=\"Count\")\n> boxplot(dataset[,1], main=\"Boxplot\", xlab=\"Age\")\n\nScatterplot\nHistogram\nBoxplot\n\nNumber of Nodes\nCount\n\nAge\nYear\nAge\nFigure 1: Plotting examples.\nThe pch argument in the plot function can be varied to change the marker. Use points and lines to add extra\npoints and lines to an existing plot. You can save the plots in a number of different formats; make sure the plot\nwindow is on top, and go to \"File\" then \"Save as.\"\n3.5\nFormulas\nCertain functions have a \"formula\" as one of their arguments. Usually this is a way to express the form of a\nmodel. Here is a simple example. Suppose you have a response variable y and independent variables x1, x2, and\nx3. To express that y depends linearly on x1, x2, and x3, you would use the formula y ∼ x1 + x2 + x3, where\ny, x1, x2, and x3 are also column names in your data matrix. See Section 3.6 for an example. Type ?formula\nfor details on how to capture nonlinear models.\n3.6\nLinear regression\nOne of the most common modeling approaches in statistical learning is linear regression. In R, use the lm function\nto generate these models. The general form of a linear regression model is\nY = β0 + β1X1 + β2X2 + · · · + βkXk + ε,\nwhere ε is normally distributed with mean 0 and some variance σ2 .\nLet y be a vector of dependent variables, and x1 and x2 be vectors of independent variables. We want to find\nthe coefficients of the linear regression model Y = β0 + β1X1 + β2X2 + ε. The following commands generate the\nlinear regression model and give a summary of it.\n> lm_model <- lm(y ∼ x1 + x2, data=as.data.frame(cbind(y,x1,x2)))\n> summary(lm_model)\nThe vector of coefficients for the model is contained in lm model$coefficients.\nMachine Learning Algorithms\nWe give the functions corresponding to the algorithms covered in class. Look over the documentation for\neach function on your own as only the most basic details are given in this tutorial.\n\n4.1\nPrediction\nFor most of the following algorithms (as well as linear regression), we would in practice first generate the model\nusing training data, and then predict values for test data. To make predictions, we use the predict function. To\nsee documentation, go to the help page for the algorithm, and then scroll through the Contents menu on the left\nside to find the corresponding predict function. Or simply type ?predict.name, where name is the function\ncorresponding to the algorithm. Typically, the first argument is the variable in which you saved the model, and\nthe second argument is a matrix or data frame of test data. Note that when you call the function, you can just\ntype predict instead of predict.name. For instance, if we were to predict for the linear regression model above,\nand x1 test and x2 test are vectors containing test data, we can use the command\n> predicted_values <- predict(lm_model, newdata=as.data.frame(cbind(x1_test, x2_test)))\n4.2\nApriori\nTo run the Apriori algorithm, first install the arules package and load it. See Section 1.2 for installing and\nloading new packages. Here is an example of how to run the Apriori algorithm using the Mushroom dataset.\nNote that the dataset must be a binary incidence matrix; the column names should correspond to the \"items\"\nthat make up the \"transactions.\" The following commands print out a summary of the results and a list of the\ngenerated rules.\n> dataset <- read.csv(\"C:\\\\Datasets\\\\mushroom.csv\", header = TRUE)\n> mushroom_rules <- apriori(as.matrix(dataset), parameter = list(supp = 0.8, conf = 0.9))\n> summary(mushroom_rules)\n> inspect(mushroom_rules)\nYou can modify the parameter settings depending on your desired support and confidence thresholds.\n4.3\nLogistic Regression\nYou do not need to install an extra package for logistic regression. Using the same notation as in Section 3.6, the\ncommand is:\n> glm_mod <-glm(y ∼ x1+x2, family=binomial(link=\"logit\"), data=as.data.frame(cbind(y,x1,x2)))\n4.4\nK-Means Clustering\nYou do not need an extra package. If X is the data matrix and m is the number of clusters, then the command is:\n> kmeans_model <- kmeans(x=X, centers=m)\n4.5\nk-Nearest Neighbor Classification\nInstall and load the class package. Let X train and X test be matrices of the training and test data respectively,\nand labels be a binary vector of class attributes for the training examples. For k equal to K, the command is:\n> knn_model <- knn(train=X_train, test=X_test, cl=as.factor(labels), k=K)\nThen knn model is a factor vector of class attributes for the test set.\n\n4.6\nNa ıve Bayes\nInstall and load the e1071 package. Using the same notation as in Section 3.6, the command is:\n> nB_model <- naiveBayes(y ∼ x1 + x2, data=as.data.frame(cbind(y,x1,x2)))\n4.7\nDecision Trees (CART)\nCART is implemented in the rpart package. Again using the formula, the command is\n> cart_model <- rpart(y ∼ x1 + x2, data=as.data.frame(cbind(y,x1,x2)), method=\"class\")\nYou can use plot.rpart and text.rpart to plot the decision tree.\n4.8\nAdaBoost\nThere are a number of different boosting functions in R. We show here one implementation that uses decision\ntrees as base classifiers. Thus the rpart package should be loaded. Also, the boosting function ada is in the ada\npackage. Let X be the matrix of features, and labels be a vector of 0-1 class labels. The command is\n> boost_model <- ada(x=X, y=labels)\n4.9\nSupport Vector Machines (SVM)\nThe SVM algorithm is in the e1071 package. Let X be the matrix of features, and labels be a vector of 0-1 class\nlabels. Let the regularization parameter be C. Use the following commands to generate the SVM model and view\ndetails:\n> svm_model <- svm(x=X, y=as.factor(labels), kernel =\"radial\", cost=C)\n> summary(svm_model)\nThere are a variety of parameters such as the kernel type and value of the C parameter, so check the documentation\non how to alter them.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.097 Prediction: Machine Learning and Statistics\nSpring 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "15.097 Lecture 3: Fundamentals of learning",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-097-prediction-machine-learning-and-statistics-spring-2012/fe0066918443f4a2b929edd72f03355f_MIT15_097S12_lec03.pdf",
      "content": "Fundamentals of Learning\nMIT 15.097 Course Notes\nCynthia Rudin\nImportant Problems in Data Mining\n1. Finding patterns (correlations) in large datasets\n-e.g. (Diapers → Beer). Use Apriori!\n2. Clustering - grouping data into clusters that \"belong\" together - objects\nwithin a cluster are more similar to each other than to those in other clusters.\n- Kmeans, Kmedians\n- Input: {xi}m\ni=1, xi ∈X ⊂ Rn\n- Output: f : X →{1, . . . , K} (K clusters)\n- clustering consumers for market research, clustering genes into families,\nimage segmentation (medical imaging)\n3. Classification\n- Input: {(xi, yi)}m \"examples,\" \"instances with labels,\" \"observations\"\ni=1\n- xi ∈X , yi ∈ {-1, 1} \"binary\"\n- Output: f : X → R and use sign(f) to classify.\n- automatic handwriting recognition, speech recognition, biometrics, doc\nument classification\n- \"LeNet\"\n4. Regression\n\n- Input: {(xi, yi)}m\ni=1, xi ∈X , yi ∈ R\n- Output: f : X → R\n- predicting an individual's income, predict house prices, predict stock\nprices, predict test scores\n5. Ranking (later) - in between classification and regression. Search engines\nuse ranking methods\n6. Density Estimation - predict conditional probabilities\n- {(xi, yi)}m\ni=1, xi ∈X , yi ∈ {-1, 1}\n- Output: f : X → [0, 1] as \"close\" to P (y = 1|x) as possible.\n- estimate probability of failure, probability to default on loan\nRule mining and clustering are unsupervised methods (no ground truth),\nand classification, ranking, and density estimation are supervised methods\n(there is ground truth). In all of these problems, we do not assume we know the\ndistribution that the data are drawn from!\nTraining and Testing (in-sample and out-of-sample) for supervised learning\nTraining: training data are input, and model f is the output.\n{(xi, yi)}m\ni=1 =⇒ Algorithm =⇒ f.\nTesting: You want to predict y for a new x, where (x, y) comes from the same\ndistribution as {(xi, yi)}m\ni=1.\nThat is, (x, y) ∼ D(X , Y) and each (xi, yi) ∼ D(X , Y).\nCompute f(x) and compare it to y. How well does f(x) match y? Measure\ngoodness of f using a loss function R : Y × Y → R:\nRtest(f) = E(x,y)∼DR(f(x), y)\n\n=\nR(f(x), y)dD(x, y).\n(x,y)∼D\n\nRtest is also called the true risk or the test error.\nCan we calculate Rtest?\nWe want Rtest to be small, to indicate that f(x) would be a good predictor (\"es\ntimator\") of y.\nFor instance\nR(f(x), y) = (f(x) - y)2\nleast squares loss, or\nR(f(x), y) = 1[sign(f(x))\n(mis)classification error\n=y]\nWhich problems might these loss functions correspond to?\nHow can we ensure Rtest(f) is small?\nLook at how well f performs (on average) on {(xi, yi)}i.\nm\nm\nRtrain(f) =\nR(f(xi), yi).\nm i=1\nRtrain is also called the empirical risk or training error. For example,\nm\nm\nRtrain(f) =\n=yi].\n1[sign(f(xi))\nm i=1\n(How many handwritten digits did f classify incorrectly?)\nSay our algorithm constructs f so that Rtrain(f) is small. If Rtrain(f) is small,\nhopefully Rtest(f) is too.\nWe would like a guarantee on how close Rtrain is to Rtest . When would it be close\nto Rtest?\n- If m is large.\n- If f is \"simple.\"\n\nIllustration\nIn one of the figures in the illustration, f:\n- was overfitted to the data\n- modeled the noise\n- \"memorized\" the examples, but didn't give us much other useful information\n- doesn't \"generalize,\" i.e., predict. We didn't \"learn\" anything!\nComputational Learning Theory, a.k.a. Statistical Learning Theory, a.k.a.,\nlearning theory, and in particular, Vapnik's Structural Risk Minimization\n(SRM) addresses generalization. Here's SRM's classic picture:\nWhich is harder to check for, overfitting or underfitting?\n\nComputational learning theory addresses how to construct probabilistic guaran\ntees on the true risk. In order to do this, it quantifies the class of \"simple models.\"\nBias/Variance Tradeoff is related to learning theory (actually, bias is related to\nlearning theory).\nInference Notes - Bias/Variance Tradeoff\n\nRegularized Learning Expression\nStructural Risk Minimization says that we need some bias in order to learn/generalize\n(avoid overfitting). Bias can take many forms:\n\n- \"simple\" models f(x) =\nλjx(j) where IλI2 =\nλ2 < C\nj\nj\nj\n- \"prior\" in Bayesian statistics\n- connectivity of neurons in the brain\nRegularized Learning Expression:\nm\nR(f(xi), yi) + CRreg(f)\ni\nThis expression is kind of omnipresent. This form captures many algorithms:\nSVM, boosting, ridge regression, LASSO, and logistic regression.\nIn the regularized learning expression, the loss R(f(xi), yi) could be:\n- \"least squares loss\" (f(xi) - yi)2\n- \"misclassification error\" 1[yi\nf(xi))] = 1[yif(xi)≤0]\n\n- Note that minimizing\n1[yif(xi)≤0] is computationally hard.\ni\n\n-yif(xi)\n- \"logistic loss\" log2 1 + e\n⇐= logistic regression\n=sign(\n(c) Source unknown. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/fairuse.\n\n- \"hinge loss\" max(0, 1 - yif(xi)) ⇐= SVM\n-yif(xi) ⇐\n- \"exponential loss\" e\n= AdaBoost\nIn the regularized learning expression, we define a couple of options for Rreg(f).\nUsually f is linear, f(x) =\nj λjx(j). We choose Rreg(f) to be either:\n- IλI2\n2 =\nj λj\n2 ⇐= ridge regression, SVM\n- IλI1 =\n|λj| ⇐= LASSO, approximately AdaBoost\nj\nP\nP\nP\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.097 Prediction: Machine Learning and Statistics\nSpring 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "15.097 Lecture 4: Inference",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-097-prediction-machine-learning-and-statistics-spring-2012/dec694eb34799f6bea2e91b1c06551a0_MIT15_097S12_lec04.pdf",
      "content": "Bias/Variance Tradeoff\nA parameter is some quantity about a distribution that we would like to know.\nWe'll estimate the parameter θ using an estimator θˆ. The bias of estimator θˆ for\nparameter θ is defined as:\n- Bias(θˆ,θ) := E(θˆ) - θ, where the expectation is with respect to the distribution\nthat θˆ is constructed from.\nAn estimator whose bias is 0 is called unbiased. Contrast bias with:\n- Var(θˆ) = E(θˆ- E(θˆ))2 .\nOf course, we'd like an estimator with low bias and low variance.\nA little bit of decision theory\n(The following is based on notes of David McAllester.)\nLet's say our data come from some distribution D on X × Y, where Y ⊂ R.\nUsually we don't know D (we instead only have data) but for the moment, let's\nsay we know it. We want to learn a function f : X →Y.\nThen if we could choose any f we want, what would we choose? Maybe we'd\nchoose f to minimize the least squares error:\nEx,y∼D[(y - f(x))2].\nFigure s\nhowing the f\nour comb\ninations of l\now and hi\ngh bias, and\nlow and\nhigh variance.\nImage by MIT OpenCourseWare.\n\nIt turns out that the f ∗ that minimizes the above error is the conditional expec\ntation!\nDraw a picture\nProposition.\nf ∗ (x) = Ey[y|x].\nProof. Consider each x separately. For each x there's a marginal distribution on\ny. In other words, look at Ey[(y - f(x))2|x] for each x. So, pick an x. For this\nx, define y to be Ey[y|x]. Now,\nEy[(y - f(x))2|x]\n= Ey[(y - y + y - f(x))2|x]\n= Ey[(y - y )2|x] + Ey[( y - f(x))2|x] + 2Ey[(y - y )( y - f(x))|x]\n= Ey[(y - y )2|x] + ( y - f(x))2 + 2( y - f(x))Ey[(y - y )|x]\n= Ey[(y - y )2|x] + ( y - f(x))2\nwhere the last step follows from the definition of y.\nSo how do we pick f(x)? Well, we can't do anything about the first term, it\ndoesn't depend on f(x). The best choice of f(x) minimizes the second term,\nwhich happens at f(x) = y , where remember y = Ey[y|x].\nSo we know for each x what to choose in order to minimize Ey[(y - f(x))2|x].\nTo complete the argument, note that:\nEx,y[(y - f(x))2] = Ex[Ey[(y - f(x))2|x]]\nand we have found the minima of the inside term for each x. -\nNote that if we're interested instead in the absolute loss Ex,y[|y - f(x)|], it\nis possible to show that the best predictor is the conditional median, that is,\nf(x) = median[y|x].\n\nBack to Bias/Variance Decomposition\nLet's think about a situation where we created our function f using data S. Why\nwould we do that of course?\nWe have training set S = (x1, y1), . . . , (xm, ym), where each example is drawn iid\nfrom D. We want to use S to learn a function fS : X →Y.\nWe want to know what the error of fS is on average. In other words, we want to\nknow what Ex,y,S[(y-fS(x))2] is. That will help us figure out how to minimize it.\nThis is going to be a neat result - it's going to decompose into bias and variance\nterms!\nFirst, let's consider some learning algorithm (which produced fS) and its ex\npected prediction error:\nEx,y,S[(y - fS(x))2].\nRemember that the estimator fS is random, since it depends on the randomly\ndrawn training data. Here, the expectation is taken with respect to a new ran\ndomly drawn point x, y ∼ D and training data S ∼ Dm .\nLet us define the mean prediction of the algorithm at point x to be:\nf(x) = ES[fS(x)].\nIn other words, to get this value, we'd get infinitely many training sets, run the\nlearning algorithm on all of them to get infinite predictions fS(x) for each x.\n\nThen for each x we'd average the predictions to get f(x).\nWe can now decompose the error, at a fixed x, as follows:\nEy,S[(y - fS(x))2]\n= Ey,S[(y - y + y - fS(x))2]\n= Ey(y - y )2 + ES( y - fS(x))2 + 2Ey,S[(y - y )( y - fS(x))].\nThe third term here is zero, since Ey,S[(y-y )( y-fS(x))] = Ey(y-y )ES( y-fS(x)),\nand the first part of that is Ey(y - y ) = 0.\n\nThe first term is the variance of y around its mean. We don't have control over\nthat when we choose fS. This term is zero if y is deterministically related to x.\nLet's look at the second term:\nES( y - fS(x))2\n\n= ES( y - f(x) + f(x) - fS(x))2\n\n= ES( y - f(x))2 + ES(f (x) - fS(x))2 + 2ES[( y - f(x))(f (x) - fS(x))]\n\nThe last term is zero, since ( y - f(x)) is a constant, and f(x) is the mean of\n\nfS(x) with respect to S. Also the first term isn't random. It's ( y - f(x))2 .\nPutting things together, what we have is this (reversing some terms):\nEy,S[(y - fS(x))2] = Ey(y - y )2 + ES(f (x) - fS(x))2 + ( y - f (x))2 .\nIn this expression, the second term is the variance of our estimator around its\nmean. It controls how our predictions vary around its average prediction. The\nthird term is the bias squared, where the bias is the difference between the av\nerage prediction and the true conditional mean.\nWe've just proved the following:\nTheorem.\nFor each fixed x, Ey,S[(y - fS(x))2] = vary|x(y) + varS(fS(x)) + bias(fS(x))2 .\nSo\nEx,y,S[(y - fS(x))2] = Ex[vary|x(y) + varS(fS(x)) + bias(fS(x))2].\nThat is the bias-variance decomposition.\nThe \"Bias-Variance\" tradeoff: want to choose fS to balance between reducing the\nsecond and third terms in order to make the lowest MSE. We can't just minimize\none or the other, it needs to be a balance. Sometimes, if you are willing to inject\nsome bias, this can allow you to substantially reduce the variance. E.g., modeling\nwith lower degree polynomials, rather than higher degree polynomials.\n\nQuestion: Intuitively, what happens to the second term if fS fits the data per\nfectly every time (overfitting)?\nQuestion: Intuitively, what happens to the last two terms if fS is a flat line\nevery time?\nThe bottom line: In order to predict well, you need to strike a balance between\nbias and variance.\n- The variance term controls wiggliness, so you'll want to choose simple func\ntions that can't yield predictions that are too varied.\n- The bias term controls how close the average model prediction is close to\nthe truth, y. You'll need to pay attention to the data in order to reduce the\nbias term.\n- Since you can't calculate either the bias or the variance term, what we\nusually do is just impose some \"structure\" into the functions we're fitting\nwith, so the class of functions we are working with is small (e.g., low degree\npolynomials). We then try to fit the data well using those functions. Hope\nfully this strikes the right balance of wiggliness (variance) and capturing the\nmean of the data (bias).\n- One thing we like to do is make assumptions on the distribution D, or\nat least on the class of functions that might be able to fit well. Those\nassumptions each lead to a different algorithm (i.e. model). How well the\nalgorithm works or not depends on how true the assumption is.\n- Even when we're not working with least squares error, we hope a similar\nidea holds (and will work on proving that later in the course). We'll use\nthe same type of idea, where we impose some structure, and hope it reduces\nwiggliness and will still give accurate predictions.\nGo back to the other notes!\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.097 Prediction: Machine Learning and Statistics\nSpring 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "15.097 Project Suggestions",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-097-prediction-machine-learning-and-statistics-spring-2012/0c1cbff3a11cf9517c44e304425d6b94_MIT15_097S12_project.pdf",
      "content": "15.097 Prediction: Machine Learning and Statistics\nProject Suggestions\n\nHere are some ideas for the final project. Of course, you are also encouraged to come up with your own\nideas that do not necessarily fall into any of these categories.\n\nI.\nStudy and prepare lecture notes on a set of topics not covered in class\nOur goal was to give you an overview of the major topics in machine learning and statistics, but here\nare some topics we did not have time to cover:\na. Expectation-‐Maximization and PageRank (the two top 10 algorithms not covered)\nb. One-‐class classification (anomaly detection) and multi-‐class classification\nc. Recommender systems and collaborative filtering\ni.\nYou see this on online sites that recommend products as you make purchases\nii.\nImplement an algorithm for low-‐rank matrix factorization and test it on data similar to that\nfrom the Netflix competition (http://www.netflixprize.com/).\niii.\nInvestigate other matrix completion methods as well.\nd. Neural networks\ne. Use of first-‐order optimization methods in large scale machine learning\n\nII. Address a problem from a competition\nThere are a variety of data mining competitions. Try to obtain their data and come up with solutions\nto their tasks. Possible competitions include the following (there are others; just do a Google search):\na. KDD Cup (http://www.sigkdd.org/kddcup/index.php)\nb. Kaggle (http://www.kaggle.com/competitions) - Sign up for an account to download the data.\nYou might want to take a look at \"What Do You Know?\" or \"Photo Quality Prediction.\"\nc. Kaggle in Class (http://inclass.kaggle.com/competitions)\nd. Computer Cooking Contest (http://liris.cnrs.fr/ccc/ccc2011/doku.php?id=welcome) - Figure out\na way to predict missing ingredients of recipes. Association rules might be a good place to start.\n\nIII. Design a new algorithm to solve a machine learning problem\nThose of you who have training in optimization methods may put it to good use in designing new\nalgorithms and showing how they compare against conventional methods. Think of alternative\ntechniques, such as:\na. First-‐order methods\nb. Linear programming\nc. Integer programming\nFor example, you might be able to use math programming for clustering.\n\nIV. Acquire an interesting dataset and learn something insightful from it\nData are all around us. See if you can use machine learning techniques to tell you something\ninteresting. For instance:\na. Crawl Amazon for product reviews and determine associations between reviews.\nb. Find statistics on NCAA teams and develop a model to predict the Final Four.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.097 Prediction: Machine Learning and Statistics\nSpring 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "15.097 Student Project: DC Programming",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-097-prediction-machine-learning-and-statistics-spring-2012/06d402cb88ff2cdb8e69b66a5c864b92_MIT15_097S12_proj5.pdf",
      "content": "DC Programming: The Optimization Method\nYou Never Knew You Had To Know\nMay 18, 2012\nIntroduction\n1.1\nWhat is DC Programming?\nRecall that in the lectures on Support Vector Machines and Kernels, we repeat\nedly relied on the use of convex optimization to ensure that solutions existed and\ncould be computed. As we shall see later in the lecture, there are many cases in\nwhich the assumption that the objective function and constraints are convex (or\nquasi-convex) are invalid, and so the methods on convex functions that we have\ndeveloped in class prove insufficient.\nTo deal with these problems, we develop a theory of optimization for a superclass\nof convex functions, called DC - Difference of Convex - functions. We now define\nsuch functions formally.\nDefinition 1.1. Let f be a real valued function mapping Rn to R. Then f is a\nDC function if there exist convex functions, g, h : Rn → R such that f can be\ndecomposed as the difference between g and h:\nf(x) = g(x) - h(x) ∀x ∈ Rn\nIn the remainder of this lecture, we will discuss the solutions to the following - the\nDC Programming Problem (DCP):\nminimize\nf0(x)\nx∈Rn\n(1)\nsubject to fi(x) ≤ 0, i = 1, . . . , m.\n\nwhere fi : Rn → R is a differentiable DC function for i = 0, . . . , m.\n1.2\nSome Intuition About DC Functions\nBefore we continue with the discussion of the solution to (1), we develop some in\ntuition regarding DC functions. Recall that a function f : Rn → R is convex if for\nevery x1, x2 ∈ Rn and every α ∈ [0, 1] , f(αx1 +(1-α)x2) ≤ αf(x1)+(1-α)f(x2).\nIn particular, as you may recall, if f is a twice-differential function, then it is con\nvex if and only if its Hessian matrix is positive-semidefinite. To get a sense of\nwhat DC functions can look like, we will look at some common convex functions\nand the DC functions they can form.\nExample 1.2. Consider the convex functions f1(x) = 1 and f2(x) = x .\nx\n(a) f1(x) = 1\n(b) f2(x) = x\nx\nExample 1.3. Consider the convex functions f1(x) = abs(x) and f2(x) = -log(x).\n(c) f =\n- x2\nx\n(d) f1(x) = abs(x)\n(e) f2(x) = -log(x)\n(f) f = abs(x) + log(x)\nNotice that while in these examples, the minimum is easy to find by inspection in\nthe convex functions, it is less clear in the resulting DC function. Clearly, then,\nhaving DC functions as part of an optimization problem adds a level of complexity\nto the problem that we did not encounter in our dealings with convex functions.\nFortunately, as we shall soon see, this complexity is not unsurpassable.\n\n(\nP\n1.3\nHow Extensive Are These Functions?\n1.3.1\nHartman\nTheorem 1.4. The three following formulations of a DC program are equivalent:\n1. sup{f(x) : x ∈ C}, f, C convex\n2. inf{g(x) - h(x) : x ∈ Rn}, g, h convex\n3. inf{g(x) - h(x) : x ∈ C, f1(x) - f2(x) ≤ 0}, g, h, f1, f2, C all convex.\nProof.\n- We will show how to go from formulation (1) to formulation (2).\nDefine an indicator function to be:\nif x ∈ C\nIC (x) =\n(2)\ninf otherwise\nThen sup{f(x) : x ∈ C} = inf{IC (x) - f(x) : x ∈ Rn}.\n- We will show how to go from formulation (3) to formulation (1).\nWe have that inf{g(x) - h(x) : x ∈ C, f1(x) - f2(x) ≤ 0}, g, h, f1, f2, C all\nconvex. Then we can write:\nαt = inf{g(x) + tmax{f1(x), f2(x)} - h(x) - tf2(x) : x ∈ C}\nfor some value of t such that α = αt' for all tf > t. It can be shown that such\na t always exists.\n- Finally, it is clear that (2) is a special case of (3). Thus, we have shown\nconversions (1) → (2) → (3) → (1), indicating that the three formulations\nare equivalent.\nNext we demonstrate just how large the class of DC functions is.\nTheorem 1.5 (Hartman). A function f is locally DC if there exists an ε-ball on\nwhich it is DC. Every function that is locally DC, is DC.\nProposition 1.6. Let fi be DC functions for i = 1, . . . , m. Then the following\nare also DC:\n1.\nλifi(x), for λi ∈ R\ni\n2. maxifi(x)\n\n3. minifi(x)\n\n4.\nfi(x)\ni\n5. fi, twice-continuously differentiable\n6. If f is DC and g is convex, then the composition (g * f) is DC.\n7. Every continuous function on a convex set, C is the limit of a sequence of\nuniformly converging DC functions.\nOptimality Conditions\n2.1\nDuality\nBefore we can discuss the conditions for global and local optimality in the canonical\nDC Programming problem, we need to introduce some notions regarding duality\nin the DCP, and develop some intuition of what this gives us. To begin with this,\nwe introduce conjugate functions and use them to demonstrate the relationship\nbetween the DCP and its dual.\nDefinition 2.1. Let g : Rn → R. Then the conjugate function of g(x) is\nT\ng ∗ (y) = sup{x y - g(x) : x ∈ Rn}\nTo understand what why conjugate functions are so important, we need just one\nmore definition.\nDefinition 2.2. The epigraph of a function, g : Rn → R is the set of points lying\non or on top of its graph:\nepi(g) = {(x, t) ∈ Rn × R : g(x) ≤ t}\nNote that g is convex if and only if epi(g) is a convex set.\nHaving defined the epigraph, we can now give a geometric interpretation of the\n∗\nconjugate function: The conjugate function g 'encloses' the convex hull of epi(g)\nwith g's supporting hyperplanes. In particular, we can see that when f is differ\nentiable,\ndf\nT\n= argsup{x y - g ∗ (y) : y ∈ Rn} = y(x)\ndx\n\nThat is, y is a dual variable for x, and so can be interpreted as (approximately)\nthe gradient (or slope in R2) of f at x. This should be a familiar result from the\nconvex analysis lecture.\n(g) A Convex fn f(x) and the affine function\nmeeting f at x. The conjugate, f ∗(y) of f(x)\nis the point of intersection.\n(h) A convex set being enclosed by supporting hyper\nplanes. If C is the epigraph of a function f ∗ , then the\nintersections of C with the hyperplanes is the set of values\nthat f ∗ takes.\nTheorem 2.3. Let g : Rn → R such that g(x) is lower semi-continuous and convex\non Rn . Then\nT\ng(x) = sup{x y - g ∗ (y) : y ∈ Rn}\nwhere g ∗(y) is the conjugate of g(x).\nWe provide this theorem without proof, and omit further discussion of lower semi-\ncontinuity, but we can safely assume that the functions that we will deal with\n∗∗\nsatisfy this. Note that this condition implies that g\n= g, that is, the conjugate\n∗\nof g conjugate is g, which means that the definitions of g and g are symmetric.\n(What does this mean for minimizing g?)\nWe now demonstrate the relationship between DCP and its dual problem. First\nnote the form of the conjugate function f ∗ for f ≡ g - h:\nT\n(f(x)) ∗ = ((g - h)(x)) ∗ = sup{x y - (g - h)(x) : x ∈ Rn} = h ∗ (y) - g ∗ (y)\nLet α be the optimum value to DCP.\nT\nα = inf{g(x) - h(x) : x ∈ X} = inf{g(x) - sup{x y - h ∗ (y) : y ∈ Y } : x ∈ X}\nT\n= inf{inf{g(x) - x y + h ∗ (y) : x ∈ X} : y ∈ Y }\n= inf{h ∗ (y) - g ∗ (y) : y ∈ Y }\nCourtesy of Dimitri Bertsekas. Used with permission.\nSupporting\n\nhyperplanes\nin three colors, enclosing a convex set C.\nImage by MIT OpenCourseWare.\n\nThus, we have that the optimal value to DCP is the same as the optimal value for\nits dual! Now that is symmetry! This means that we can solve either the primal\nor the dual problem and obtain the solution to both - the algorithm that we will\nemploy to solve the DCP will crucially rely on this fact.\n2.2\nGlobal Optimality Conditions\nDefinition 2.4. Define an ε-subgradient of g at x0 to be\n∂εg(x 0) = {y ∈ Rn : g(x) - g(x 0) ≥ (x - x 0)T y - ε ∀x ∈ Rn}\nDefine a differential of g at x0 to be\n\n∂g(x 0) =\n∂εg(x 0)\nε>0\nGiven these two definitions, we have the following conditions for global optimality:\nTheorem 2.5 (Generalized Kuhn-Tucker). .\n∗\nLet x be an optimal solution to the (primal) DCP. Then ∂h(x ∗) ⊂ ∂g(x ∗).\n∗\nLet y be an optimal solution to the dual DCP. Then ∂g∗(y ∗) ⊂ ∂h∗(y ∗)\nProof. This condition essentially follows from the equivalence of the primal and\ndual optima. We showed before that if α is the optimum value of the DCP, then\nα = inf{g(x) - h(x) : x ∈ X} = inf{h ∗ (y) - g ∗ (y) : y ∈ Y }\nThen if α is finite, we must have that dom g ⊂ dom h and dom h∗⊂ dom g ∗ where\ndom g = {x ∈ Rn : g(x) < inf}, the domain of g. That is, h (respectively, g ∗) is\nfinite whenever g (respectively, h∗) is finite. Note that we require this inclusion\nbecause we are minimizing the objective function, and so if there existed an x ∈ R∗\nsuch that g(x) < inf, h(x) = inf, then g(x)-h(x) would be minimized at x, yielding\nan objective value of -inf. Note also that this statement is not an in and only if\nstatement, as we work under the convention that inf-inf = inf.\n∗\n∗\nThus, we have that if x is an optimum to the primal DCP, then x ∈ dom g, and\nby weak duality,\ng(x ∗ ) - h(x ∗ ) ≤ h ∗ (y) - g ∗ (y), ∀y ∈ dom h ∗\n\n∗\n∗\nand so (for x ∈ dom h) if x ∈ ∂h(x ∗), then\nT\nx y ≥ h(x ∗ ) + h ∗ (y) ≥ g(x ∗ ) + g ∗ (y)\nwhere the first inequality is by definition of ∂h(x ∗) and the second inequality\nfollows from the weak duality inequality presented.\nWe can also think of this in terms of the interpretations of the dual problem as\nwell. We have that if g, h are differentiable, and so ∂h(x\n\n∅= ∂g(x ∗), then\n∗) =\n\n∂h(x ∗) is just the set of gradients of h at x ∗, and by equality of the primal and dual\n∗\noptima, it is the set of y that optimize the dual problem. Thus, this optimality\ncondition in terms of subdifferentials is analogous to the one we discussed in terms\nof domains.\nCorollary 2.6. Let P and D be the solution sets of the primal and dual problems\nof the DCP, respectively. Then:\n∗\nx ∈ P if and only if ∂εh(x ∗) ⊂ ∂εg(x ∗) ∀ε > 0.\n∗\ny ∈ D if and only if ∂εg ∗(y ∗) ⊂ ∂εh∗(y ∗) ∀ε > 0.\nTheorem 2.7. Let P and D be the solution sets of the primal and dual problems\nof the DCP, respectively. Then:\n\n{∂h(x) : x ∈ P} ⊂ D ⊂ dom h ∗\nand\n\n{∂g ∗ (y) : y ∈ D} ⊂ P ⊂ dom g\nNote that this theorem implies that solving the primal DCP implies solving the\ndual DCP.\n2.3\nLocal Optimality Conditions\nWe would like to construct an algorithm to find global optimal solutions based on\nthe conditions discussed in the previous section. However, finding an algorithm\nthat does this efficiently in general is an open problem, and most approaches are\ncombinatorial, rather than convex-based, and so rely heavily on the formulation\nof a given problem, and are often inefficient. Thus, we present local optimality\nconditions, which (unlike the global optimality conditions) can be used to create\na convex-based approach to local optimization. We present these theorems with\nout proof as, although they are crucially important to solving DC programming\n\nproblems, their proofs do not add much more insight. Thus, we refer further in\nvestigation to either Hurst and Thoai or Tao and An.\n∗\nTheorem 2.8 (Sufficient Local Optimality Condition 1). Let x be a point that\nadmits a neighborhood U(x) such that\n∂h(x) ∩ ∂g(x ∗ )\n∅\n∀x ∈ U(x) ∩ dom g\n∗\nThen x is a local minimizer of g - h.\nTheorem 2.9 (Sufficient Local Optimality Condition 2: Strict Local Optimal\n∗\nity). Let int(S) refer to the interior of set S. Then if x ∈ int(dom h) and\n∗\n∂h(x ∗) ⊂ int(∂g(x ∗)), then x is a strict local minimizer of g - h.\n∗\nTheorem 2.10 (DC Duality Transportation of a Local Minimizer). Let x ∈\n∗\n∗\ndom ∂h be a local minimizer of g - h and let y ∈ ∂h(x ∗). Then if g is differ\n∗\n∗\n∗\n∗\nentiable at y , y is a local minimizer of h∗- g . More generally, if y satisfies\n∗\n∗\nTheorem 2.9, then y is a local minimizer of h∗- g .\nAlgorithms\nAs discussed earlier, the conditions for global optimality in DC programs do not\nwield efficient general algorithms. Thus, while there are a number of popular tech\nniques - among them, branch-and-bound and cutting planes algorithms, we omit\ndiscussion of them and instead focus on the convex-based approach to local opti\nmization. In fact, although there has not been an analytic result to justify this,\naccording to the DC Programming literature, the local optimization approach of\nten yields the global optimum, and a number of regularization and starting-point\nchoosing methods exist to assist with incorporating the following local optimiza\ntion algorithm to find the global optimum in different cases.\n3.1\nDCA-Convex Approach to Local Optimization\nWe now present an algorithm to find local optima for a general DC program. First,\nwe offer the algorithm in raw form, then we will explain each step in an iteration,\nand finally, we will state a few results regarding the effectiveness and efficiency of\nthe algorithm.\n=\n\n3.1.1\nDCA\n1. Choose x0 ∈ dom g\n2. for k ∈ N do:\n3.\nchoose yk ∈ ∂h(xk)\n4.\nchoose xk+1 ∈ ∂g∗(yk)\n5.\nif min{|(xk+1 - xl)i|, | (xk+1-xl)i |} ≤ δ:\n(xk)i\n6.\nthen return xk+1\n7.\nend if\n8. end for\n3.1.2\nDCA Explanation and Intuition\nLet us go over each step of the DCA algorithm in more detail. The overarching\nmethod of the algorithm is to create two sequences of variables, {xk}k, {yk}k so that\n{xk} converges to a local optimum of the primal problem, x ∗, and {yk} converges\n∗\nto the local optimum of the dual problem, y . The key idea is to manipulate the\nsymmetry of the primal and dual problem in order to follow a variation on the\ntypical sub gradient-descent method used in convex optimization.\nLet us now consider each step individually.\n- Choose x0 ∈ dom g:\nSince we are utilizing a descent approach, the convergence of the algorithm\nis independent of the starting point of the sequences that the algorithm\ncreates. Thus, we can instantiate the algorithm with an arbitrary choice of\nx0 so long as it is feasible.\n- Choose yk ∈ ∂h(xk):\nT\nWe have that ∂h(xk) = arg min{h∗(y) - g ∗(yk-1) - x (y - yk-1) : y ∈ Rn}.\nk\nMoreover, since this is a minimization over y, we hold yk-1, xk constant, and\nT\nso we have that ∂h(xk) = arg max{xk y - h∗(y) : y ∈ Rn}. Computing\nthis, however is just an exercise in convex optimization, since by the local\noptimality conditions, xk is (approximately) a subgradient of h∗- g ∗, and so\nserves a role similar to that in a typical subgradient descent algorithm, which\nwe solve quickly and efficiently. Since we maximize given xk (which improves\ntogether with yk-1), we guarantee that (h∗- g ∗)(yk - yk-1) ≤ 0 ∀k ∈ N, and\n\ndue to the symmetry of duality, that yk converges to a critical point of\n(h∗- g ∗), i.e., a local minimizer.\n- Choose xk+1 ∈ ∂g∗(yk):\nGiven the symmetry of the primal and dual problem, this is entirely sym-\nT\nmetric to the step finding yk. Thus, we choose xk+1 ∈ arg max{x yk - g(x) :\nx ∈ Rn}.\n- If min{|(xk+1 - xl)i|, | (xk+1-xl)i |} ≤ δ: then return xk+1:\n(xk)i\nAlthough we can guarantee convergence in the infinite limit of k, complete\nconvergence may take a long time, and so we approximate an optimal solu\ntion within a predetermined bound, δ. Once the solution (the change in xk\nor yk) is small enough, we terminate the algorithm and return the optimal\nvalue for xk+1 - recall that solving for xk is equivalent to solving for yk.\nFigure 1: An example for the Proximal Point subgradient descent method. This\nhas been shown to be equivalent to a regularized version of DCA, and so offers\nvaluable intuition into how DCA works.\n3.1.3\nWell-Definition and Convergence\nHere we present a few results regarding the effectiveness and efficiency of the DCA\nalgorithm. We will give the results without proof, and direct anyone interested in\nfurther delving into this matter to Thoai.\nLemma 3.1. The sequences {xk} and {yk} are well defined if and only if\ndom ∂g ⊂ dom ∂h\ndom ∂h ∗⊂ dom ∂g ∗\nRn\nLemma 3.2. Let h be a lower semi-continuous function on\nand {xk} be a\nsequence of elements in Rn such that (i) xk → x ∗; (ii) There exists a bounded\nCourtesy of Dimitri Bertsekas. Used with permission.\n\nsequence {yk} such that yk ∈ ∂h(xk); (iii) ∂h(x ∗)\n∅. Then\nlim h(xk) = h(x ∗ )\nk→inf\nApplications to Machine Learning\nThe literature has references to many uses of DC Programming in Operations\nResearch, Machine Learning, and Economics.\nOne interesting use of DC Programming is discussed in the 2006-paper \"A DC-\nProgramming Algorithm for Kernel Selection\". In this paper, the authors discuss\na greedy algorithm to learn a kernel from a convex hull of basic kernels. While\nthis approach had been popularized before, it was limited to a finite set of basic\nkernels. The authors comment that the limitation was due to the non-convexity\nof a critical maximization involved in conducting the learning, but find that the\noptimization problem can be formulated as a DC Program. In particular, the\nobjective function used to weigh basic kernels is DC as the limit of DC functions.\nAnother interesting use of DC Programming is discussed in the 2008-paper \"A\nDC programming approach for feature selection in support vector machines learn\ning\". Here, DC Programming is employed in an SVM algorithm that attempts\nto choose optimally representative features in data while constructing an SVM\nclassifier simultaneously. The authors equate this problem to minimizing a zero-\nnorm function over step-k feature vectors. Using the DC-decomposition displayed\nbelow, the authors employ the DCA algorithm to find local minima and applied\nit to ten datasets - some of which were particularly sparse - to find that the DCA\nalgorithm created consistently good classifiers that often had the highest correct\nness rate among the tested classifiers (including standard SVMs, for example).\nDespite this, DCA consistently used less features than the standard SVM and\nother classifiers, and as a result, was more efficient and used less CPU capacity\nthan many of the other commonplace classifiers. Thus, all in all, DCA proved\na greatly attractive algorithm for classifying data, and particularly excellent for\nvery large and sparse datasets.\n=\n\nConclusion\nAs we can see, DC Programming is quite young. Although the oldest result\npresented in this lecture dates back to the 1950s (Hartman's), many of the results\nand algorithms discussed here were developed in the 1990s, and their applications\nare still very new to the scientific community. However, as can be seen from\nthe examples that we presented, DC Programming has tremendous potential to\nexpand and expedite many of the algorithms and techniques that are central to\nMachine Learning, as well as other fields. Thus, it is likely that the near future\nwill bring many more algorithms inspired by DCA and DCA-related approaches\nas well as the various combinatoric global approaches that are used to solve DC\nProgramming problems, but were not discussed here.\nReferences\n[1] H. A. L. T. H. M. L. V. V. Nguyen and T. P. Dinh. A dc programming\napproach for feature selection in support vector machines learning. Advances\nin Data Analysis and Classification, 2(3):259-278, 2008.\n[2] A. A. R. H. C. A. M. M. Pontil. A dc-programming algorithm for kernel selec\ntion. Proceedings of the 23rd International Conference on Machine Learning,\n2006.\n[3] P. D. TAO and L. T. H. AN. Convex analysis approach to d. c. programming:\nTheory, algorithms and applications. ACTA MATHEMATICA VIETNAM\nICA, 22(1):289-355, 1997.\n[4] R. H. N. Thoai. Dc programming: An overview. Journal of Optimization\nTheory and Application, 193(1):1-43, October 1999.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.097 Prediction: Machine Learning and Statistics\nSpring 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "15.097 Student Project: Improving Tools for Medical Statistics",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-097-prediction-machine-learning-and-statistics-spring-2012/6b2d2647dfcdf0728337c31a1d92f4b3_MIT15_097S12_proj4.pdf",
      "content": "Jacqueline Soegaard\n15.097 -Prediction: Machine Learning and Statistics\nProf. Cynthia Rudin\nCourse Project\nMay 17, 2012\n\nComparing the performance of propensity scores and support vector machines at\nestimating the effect of treatment on outcomes in observational data\n\nPropensity scores have become a popular method for removing bias in the estimation of\ntreatment effect when working with observational data. However, there are many issues and\nlimitations associated with propensity scores. This project compares the performance of pairs\nof support vector machines (SVMs) as an alternative, and aims to evaluate whether these\npaired SVMs can perform better than propensity scores when estimating the effect of treatment\non outcomes in non-randomized data. Both problems were evaluated using synthetically\ngenerated datasets that modeled 20 different models for the degree of the relationships between\nbaseline variables, treatment, and outcomes. Although neither method performed particularly\nwell at recovering the true treatment effect, it was possible to learn when the different methods\nfail by looking at the data for the different scenarios.\n\n0. Abstract\n\n1. Introduction\n\n2. Overview of Propensity Scores\n\na. Calculation and use as a balancing score\n\nb. Issues and limitations\n\n3. Methodology\n\n3.a. Synthetic data generation\n\n3.b. Propensity scores\n\n3.c. Support vector machines\n\n4. Results\n\n4.a. Propensity scores\n\n4.b. Support vector machines\n\n5. Conclusion\n\n6. References\n\n- Appendix I: Code\n\n1. Average treatment effect (ATE)\n\n2. Propensity score\n\n3. True propensity score model\n\n4. Outcome model\n\n5. Estimate of the ATE using inverse probability of treatment weighting (IPTW)\n6. Net reclassification improvement\n\n7. Net reclassification improvement, two-class formulation\n\n1. Comparison of randomized and observational medical trials\n\n2. Coefficient combinations for the generation of 20 datasets\n\n3. Plot of average treatment effect for different true treatment effects under different\nscenarios\n\n4. Plot of the net reclassification improvement for different true treatment effects under\ndifferent scenarios\n\n1. Synthetic dataset profiles\n\n2. Average treatment effect, estimated using the propensity score\n\n3. Net reclassification improvement for the SVM pairs\n\n4. Change in sensitivity for the SVM pairs\n\nMost medical studies seek to determine the effect of a certain intervention or treatment on\nclinical outcomes. One measure is the average treatment effect (ATE), which is the effect of\nmoving a population from being untreated to being treated. Considering the case where\npatients, each with a set of baseline variables, are each assigned to either the treatment\ngroup,\n\n, or the control group,\n\n, the average treatment effect is:\n\n(1)\n\nIdeally, to measure the ATE, one conducts a randomized controlled trial (RCT). In the RCT\nexperimental setup, experimenters randomly assign subjects to either the treatment or the\ncontrol group. This eliminates the confounding effect of measured and unmeasured baseline\nvariables on the treatment status.[1]\nUnfortunately, it is often either infeasible or unethical to conduct a RCT. For example,\nconsider the case where the treatment under evaluation is a surgical intervention. Due to the\ninvasive nature of the treatment, one would only assign patients to the treatment group if their\nmedical condition necessitated the surgery. Another example would be a study of the effect of\npsychotherapy on drug addiction. Again, the treatment group would be composed of people\nwho are already addicted to the drug, because it would be unethical to expose other individuals\nto addictive, harmful, and often illegal substances. In both these scenarios, an individual's\ntreatment group assignment is now conditionally dependent on their past history and baseline\nvariables (Figure 1).\nFigure 1: Comparison of randomized and observational clinical trials. Note how treatment assignment is\nconditionally independent and dependent of the baseline covariates for randomized and observational studies,\nrespectively.\n\nThinking back on the surgical study example, one might imagine that only the sickest\npatients would be given the invasive surgery. In the case where the treatment group has a\nmuch lower rate of survival than the control group, it is difficult to determine whether the\ndiscrepancy in survival rates occurred because the surgery had an adverse effect on survival.\nThe same discrepancy could have also arisen because the patients in the treatment group were\ninherently sicker than the patients in the control group. How, then, can the true effect of the\nRandomized Controlled Trials\n(RCT)\nStudy Subjects, w/ baseline\nvariables Xi\nTreatment\nGroup (Z = 1)\nY=0\nY=1\nControl\nGroup (Z = 0)\nY=0\nY=1\nNon-randomized\nObservational Trials\n\nsurgery on an outcome like survival be assessed? When working with non-randomized\nobservational data, one must first remove the confounding effect of the baseline covariates\non the association between the treatment and the outcome in order to determine the true\ntreatment effect.\nIn their 1983 papers, Rosembaum and Rubin proposed using the method of propensity\nscore analysis to reduce bias in observational studies.[2] They defined the propensity scores as\nthe probability that a patient receives a treatment given the distribution baseline covariates.\nSince being introduced in 1983, propensity scores have become a widely used tool for the\nevaluation of non-randomized observational studies in medicine and surgery. Despite the\nmethod's growing popularity, however, there remain various disadvantages that might limit its\nusefulness and appropriateness. The inevitability and importance of observational studies\nmakes it imperative that either these shortcomings are addressed, or that a better alternative be\nproposed. This project proposes using pairs of support vector machines (SVMs) as such an\nalternative, and aims to evaluate whether these paired SVMs can perform better than\npropensity score when estimating the effect of treatment on outcomes in non-randomized data.\n\n! \"#$ %\n\nA propensity score represents the probability of being assigned to the treatment group given a\npatient's baseline variables. Formally, this is represented as:\n\n(2)\n\nIn Equation 2 is the propensity score, is the binary treatment assignment, and is the\nvector of baseline variables. The propensity score is most commonly calculated via logistic\nregression, which uses baseline variables to predict the probability between 0 and 1 that a\nperson is assigned to the treatment group ( ) as opposed to the control group ).[3]\nThese propensity scores can then be used as a balancing score that accounts for the\nconfounding effect of the baseline variables so that the outcomes of the treatment and control\ngroups can be compared. The underlying idea is that among subjects with the same propensity\nscore, the same distribution of observed baseline variables will be the same between treated\nand untreated subjects. Three methods exist that help achieve this \"balance\" between the\ntreatment and control groups: matching on the propensity score, stratification on the\npropensity score, and inverse probability of treatment weighting (IPTW) using the propensity\nscore.[1]\nMatching on the propensity score involves paring each treated subject to a control\nsubject such that the both members of the matched pair have identical or highly similar values\nof the propensity scores. In contrast, stratification on the propensity score assigns subjects to\none of various ranges, or strata, of the propensity score. For example, the subjects could be\nclassified into the quintiles of the propensity score. Once the propensity scores have been used\neither to match or to stratify patients, the effect of treatment on outcome can be estimated as\nthe difference in the fraction of patients experiencing a certain outcome in the treatment group\nversus in the control groups in within the pairs or strata.[1,4] Inverse probability of treatment\nweighting (IPTW) differs from either matching or stratification in that IPTW uses the\npropensity score to weight the treatment and control subjects so that the final distribution of\n\nbaseline covariates is independent of treatment assignment.[1,5] The ATE can then be estimated\nusing these weights as described in section 4.b.\n\nDespite the prevalent use and growing popularity of propensity scores, several limitations and\nissues exist regarding their implementation and calculation. One of the major problems regards\nthe assumption of strongly ignorable treatment assignment described by Rosembaum and\nRubin in their original paper. They defined treatment assignment to be \"strongly ignorable\" if\n(a) the treatment assignment is independent of the outcome Y given the baseline variables,\nand (b) any subject has a nonzero probability of being assigned to either treatment. For\npropensity score methods to be admissible for the unbiased estimation of treatment effects, the\nassumption of strongly ignorable treatment assignment described must be satisfied. [1,2]\nHowever, some users overlook this requirement when using propensity scores, and instead\n\"interpret the mathematical proof of Rosembaum and Rubin as a guarantee that, in each strata\nof the [propensity score], matching treated and untreated subjects somehow eliminates\nconfounding from the data and contributes therefore to overall bias reduction.\" [6]\nOther problems stems from the prevailing use of the logistic regression to calculate\npropensity scores. The popularity of logistic regression for this purpose stems from its\nconvenient generation of probability values within the range [0,1] and from its accessibility and\nfamiliarity to users. However, the requirements of logistic regression create the inconvenience\nthat, for studies involving rare outcomes, there needs to be a high number of events per\nbaseline variable to avoid imbalance in the data. When working with high-dimensional data\nwith many baseline variables, the minimum number of events could become\nprohibitively large.[4] More problematic is many users' failure to account for the underlying\nassumptions of proper logistic regression modeling, including that of the linearity of the risk\nwith respect to the log-odds parametric transformation. It also seems that the common\nimplementation of the logistic regression fails to include higher order and interaction terms\nthat would be necessary for accurate model fit.[3] To eliminate these issues, Westreich et. al.\nsuggested that machine learning methods that make fewer assumptions might be suitable\nalternatives for logistic regression in the calculation of propensity scores. Of the methods they\nevaluated, they concluded that boosting, a method whereby many weak classifiers are\ncombined to make a strong classifier, showed the most promise.\n\n& '\n%\n\nThe propensity scores and support vector machines were calculated using synthetic data\ndatasets. Their performances were then evaluated using average treatment effect estimation\nbased on the inverse probability of treatment weighting, and the net reclassification index,\nrespectively.\n\nSynthetic datasets were used for experimentation so that the true effect of the treatment\nvariable on the outcome variable would be known. This then allow for the adequate\nassessment of whether the performance of propensity scores or support vector machines had\nclosely estimated the true effect of treatment on outcomes. The approach for synthetic dataset\n\ngeneration was based from the framework outlined by Setoguchi et. al.[7] To summarize their\napproach:\n- They generated 10 baseline covariates from independent standards normal random\nvariables whose relationships were defined by a correlation matrix. Four of the\nvariables were left as continuous variables, and 6 were binarized. Of the 10 variables, 3\nwere only associated with the treatment assignment only, 3 were only associated with\nthe outcome only, and four were associated with both the treatment assignment and the\noutcome.\n- They used a fixed set of covariate coefficients to model 7 different scenarios with\ndifferent degrees of non-linearity and non-additivity in the associations between the\ncovariates and the treatment assignment. These models generated true propensity\nscores, from which treatment labels were derived using Monte-Carlo simulations.\n- Their outcome was modeled using logistic regression as a function of the baseline\ncovariates and the treatment labels, where the coefficient for the treatment label was the\ntrue effect of treatment . Once more, these models generated outcome\nprobabilities, from which outcome labels were derived using Monte-Carlo\nsimulations.\n\nTheir synthetic generation framework was adapted to the needs of this study. The\nmethodology for generating the covariates was emulated exactly. However, rather than using\nthe variable-treatment relationship scenarios, only Setoguchi's first scenario was used. In\nthis scenario, the true propensity score given the covariates,\n, was modeled by a\nlinear and additive relationship between the covariates:\n\n(3)\n\nAfter obtaining the treatment labels from the Monte-Carlo simulations, the outcome was\nmodeled using the following equation:\n\n(4)\n\nInstead of using Setoguchi et. al's coefficients, I created different scenarios for the covariate\ncoefficients. These scenarios use scenarios use extremes of the coefficients to study the\nintuitive effect of having low/high effect of covariates on probability of treatment assignment;\nlow/high correlation between the baseline covariates and outcome; and varying degrees of\neffect of treatment on outcome. This would help to determine whether propensity scores or\nSVMs would fail to estimate treatment effect in any of these scenarios. The combinations of\ncoefficients are described in Figure 2.\n\nFigure 2: Coefficient combinations for the generation of 20 datasets. The betas correspond to the coefficients in true\npropensity score model (equation 3) that specify the effect of the covariates on the probability of treatment assignment.\nSimilarly, the alphas correspond to the coefficients for the covariates in the outcome model (equation 4) that specify the\neffect of those covariates on the outcome probabilities. Within each scenario, all of the alpha coefficients will be the\nsame; the same is true for the beta coefficients. The alphas and betas can either take on low or high values. Finally, the\ngammas correspond to the coefficient for the treatment label Z in the outcome model (equation 3) that specifies effect of\ntreatment on outcome. This coefficient can take on any value within the set {-0.9, -0.5, 0, 0.5, 0.9}.\nAll of the above steps were followed to create a 20 dataset, each with n = 20,000 data points.\nThe information and composition of each dataset are outlined in Table 1.\n\nTable 1: Synthetic datasets profiles.\n\n!\n\"\n\n!\n\"\n\n#\n\n$\n\n%\n\n&\n\n'\n\n(\n\n)\n\n*\n\n+\n\n#\n\n$\n\n%\n\n&\n\n'\n\n(\n\n)\n\n!\"##\n##$\n\n\"##\n##$\n\n!\"##\n##$\n\n!\"##\n##$\n\n*\n\n#+\n\nAfter considering the limitations of logistic regression for the calculation of propensity scores,\nthe propensity scores were instead calculated using boosting. Specifically, R's AdaBoost\nfunction, ada, was applied to each data example to obtain it's probability of treatment class\nmembership, i.e.\n\n. This value is equivalent to the propensity score . The\npropensity scores were then used to estimate the average treatment effect (ATE) using the\nfollowing formula for inverse probability of treatment weighting [1]:\n\n(5)\n\nThis formula uses the propensity score to give higher weight to the data examples that were\nassigned to the control group despite having a high propensity score, or vice versa. Once\nestimated, the ATE was then compared to the known treatment effect to calculate the percent\nbias of the estimator.\n\nTo determine the effect of treatment on outcome, the performance of pairs of SVMs at\npredicting outcomes from the data was compared. The framework for the training and testing\nthe SVMs was as follows:\n- One of the SVMs in the pair was trained to predict outcomes using solely the baseline\ncovariates as features, whereas the second SVM was trained to predict outcomes using\nboth the baseline covariates and the treatment labels as features. These will be called\nthe \"without-SVM\" and the \"with-SVM,\" respectively.\n- Using the svm() function from the R e1071 package, a pair of SVMs was generated for\neach of the 21 scenarios described in the dataset generation section.\n- After a preliminary evaluation of the performance of the algorithm on the \"real\"\ndataset, the SVM's cost parameter was set to . The radial basis (Gaussian) kernel\nfunction was used with gamma parameter left as the default value of\n\n- In each of these cases, the SVMs were trained on the first half of the data, and tested on\nthe remaining half.\n\nThe general idea is that if the treatment affects outcomes, then the treatment labels\ncontain information that is important for prediction. Therefore, adding the treatment labels to\nthe group of features would be expected to improve the classifier's performance. To compare\nthe two SVMs and learn how much information the treatment contributes to the prediction of\noutcome, the net reclassification improvement (NRI) metric was used. The NRI summarizes\nthe reclassification table, which itself shows how many data examples would be reclassified\ninto higher or lower risk or outcome categories upon the addition of new information into a\nprediction model. Once the reclassification table has been computed, the NRI can be\n\ncalculated as the difference in the proportions of data examples that are moving up and down\nbetween the treatment and control groups[8]:\n\n(6)\n\nFor the case where the outcomes are , this expression can be rewritten as:\n\n(7)\n\nwhere the label on the left hand side of the arrow is that given by the \"without-SVM,\" and the\nlabel on the right hand side of the arrow is that given by the \"with-SVM.\"\n\n( )\n\nThe results from the propensity score tests described in section 3.b. are summarized in Table 2.\nOverall, the known treatment effect from the data generation models was not recovered by the\npropensity score estimate of the ATE. This is not wholly unexpected, since the outcomes in\nthe data could have resulted from many different models of the relationship between the\ncovariates and the treatment assignment. In order to better visualize these results, a plot of the\nATE against the true treatment effect gamma was generated (Figure 3).\n\nTable 2: Average treatment effect, estimated using the propensity score. For each combination of betas and alphas,\nand for each value of gamma, the ATE (left) and the percent bias (right) are shown.\n\n,-.\n\n/0 -01 0\n\n/0 -01\n\n/ -01 0\n\n/ -01\n\nFrom looking at the plot, it is evident that the ATE estimates did not follow a consistent pattern\nwithin the different beta/alpha combination scenarios. However, there was an interesting failure of\nthe low beta/high alpha case, which remained insensitive to the variations in the true treatment\neffect. This is most likely because the low betas lead to an approximately random chance of being\nassigned to either treatment. This random chance then confounds the ATE formula captures when\nthere is strong correlation between the treatment and the outcome, i.e. for extremes of the gamma\nvalue. Therefore, in the case when the betas are low but the alphas are high, the propensity score\nwill work well when the true treatment effect , but will fail when .\n\nThe results from the SVM tests described in section 3.c. are summarized in Tables 3 and 4.\nFrom Table 3, it is evident that in almost every case there was a positive net reclassification\nimprovement. This indicates that the information added by the treatment label variable\nimprove classification. Moreover, sensitivity almost always increased from the \"without-\nSVM\" to the \"with-SVM.\" However, the magnitude of the improvement in NRI and in\nsensitivity was relatively small, especially if you consider that the initial \"without-SVM\"\nperformance was not impressive. In order to better visualize these results, the NRI was plotted\nagainst the true treatment effect (Figure 4).\n\nTable 3: Net reclassification improvement (NRI) for the SVM pairs. Shown for each combination of betas and alphas\nagainst each value of gamma.\n\n/0 -01 0\n\n/0 -01\n\n/ -01 0\n\n/ -01\n\n-0.1\n0.0\n0.1\n0.2\n0.3\nAverage Treatment Effect (from Propensity Scores)\nATE\nbeta=low/alpha=low\nbeta=low/alpha=high\nbeta=high/alpha=low\nbeta=high/alpha=high\n-0.5\n0.0\n0.5\nTrue Effect of Treatment\nFigure 3: Plot of the average treatment effect for different true treatment effects under different scenarios.\nNote how the beta=low/alpha=high case fails to capture the change in the true treatment effect.\n\nTable 4: Change in sensitivity from the \"without-SVM\" to the \"with-SVM\". Shown for each combination of betas\nand alphas against each value of gamma.\n\n/0 -01 0\n\n/0 -01\n\n/ -01 0\n\n/ -01\n\nNoticing trends in the data, it is evident that when the betas are low, meaning that the effect of\nthe covariates on treatment assignment is small, the improvement was larger than when the\nbetas were high. This finding is important because it corroborates the idea that when there is a\nhigh correlation between the covariates and the treatment assignment, the treatment\ninformation is almost redundant with the covariate information. Therefore, for cases such as\nthose with high betas, the net reclassification improvement will not be a useful metric for\ndetermining treatment effect on outcome because the improvement might be small despite the\ntreatment effect being large (as is the case when beta is high, alpha is low, and gamma = 0.9).\n\n-0.5\n0.0\n0.5\n-0.005\n0.005\n0.010\n0.015\n0.020\n0.025\nNet Reclassification Improvement (from SVMs)\nNRI\nbeta=low/alpha=low\nbeta=low/alpha=high\nbeta=high/alpha=low\nbeta=high/alpha=high\nTrue Effect of Treatment\nFigure 4: Plot of the net reclassification improvement for different true treatment effects under different\nscenarios. Note how the NRI was higher for the datasets with low betas than it was for the datasets with\nhigh betas.\n\n*\n\nAfter evaluating the results, it seems that neither method performed particularly well at the\nestimation of treatment effect in these scenarios where the datasets were constructed using\ncoefficients with extreme values. However, even though the magnitude of the improvements\nwas small, the SVMs showed a consistent positive NRI across all cases whereas the patter on\nthe ATE estimations from the propensity score applied to different models was erratic.\nThe extreme value cases were also informative in that they demonstrated how the estimation\nof the ATE using IPTW with the propensity score fails to capture the variation in treatment\neffect when the covariates have a low correlation with the treatment assignment but a high\ncorrelation with the outcome. Likewise, the results showed that using paired SVMs evaluated\nwith the NRI fails when the baseline covariates are highly correlated with the treatment\nassignment.\n\nTo learn more from these results, further experimentation and validation would be\nnecessary. These future studies could address some of the limitations of this current project.\nIn particular, one of the greatest limitations of this study was insufficient computation power\nfor performing the parameter sweeps necessary for the optimization of the SVMs'\nperformance. Had the SVMs been tuned more meticulously, their performance might have\nbeen much better than that which was observed. Another improvement would be to develop a\nbetter metric for capturing the treatment effect from the result of the SVM pairs. Moreover,\neven though other methods for generating synthetic datasets could be explored, it would\nprobably be most useful to perform these tests on real data to validate the results of the\nexperiment.\n\n+ )\n\n1. Austin PC. An introduction to propensity score methods for reducing the effects of\nconfounding in observational studies. Multivariate Behavioral Research 46, 399-424\n(2011).\n2. Rosembaum PR, Rubin DB. Reducing bias in observational studies using\nsubclassification on the propensity score. Journal of the American Statistical Association 79,\n516-524 (1984).\n3. Westreich D, Lessler J, Jonsson-Funk M. Propensity score estimation: neural\nnetworks, support vector machines, decision trees (CART), and meta-classifiers as\nalternatives to logistic regression. Journal of Clinical Epidemiology 63, 826-833 (2010).\n4. Adamina M, Guller U, Weber WP, Oertli D. Propensity scores and the surgeon.\nBritish Journal of Surgery 93, 389-394 (2006).\n5. Lee BK, Lessler J, Stuart EA. Improving propensity score weighting using machine\nlearning. Statistics in Medicine 29, 337-346 (2010).\n6. Pearl J. Understanding propensity scores. Causality: Models, Reasoning, and Inference,\nSecond Edition. Cambridge University Press, p. 348-352 (2009).\n7. Setoguchi S, Schneeweiss S, Brookhart MA, Glynn RJ, Cook EF. Evaluating uses of\ndata mining techniques in propensity score estimation: a simulation study.\nPharmacoepidimemiology and Drug Safety 17, 546-555 (2008).\n8. Cook, NR. Statistical evaluation of prognostic versus diagnostic models: Beyond the\nROC curve. Clinical Chemistry 54, 17-23 (2008).\n\n,\n\n%%%%%%%%%%%NOTE: This procedure for synthetic dataset generation modified\n%%%%%%%%%%%from Setoguchi et. al., 2008.\n\nndatasets = 1;\nncohort1 = 20000;\n\ni = 0;\nsetStore = cell(ndatasets,2);\n\nwhile i < ndatasets\n[real_set, this_set] = generateSyntheticDatasets(ncohort1);\nsetStore{i+1,1} = real_set;\nsetStore{i+1,2} = this_set;\ni = i+1;\nend\n\n% Save the dataset as a .mat file\nsave('project097_dataset.mat', 'setStore')\n\n% Concatenate all of the dimensions of the structure into one array\ndataset = setStore{1,2};\ndata_mat = [dataset.W, dataset.TPS, dataset.A, dataset.pY, dataset.Y];\n\nrealset = setStore{1,1};\nreal_mat = [realset.W, realset.TPS, realset.A, realset.pY, realset.Y];\n\n% Save the array as a .csv function for (that can be loaded into R)\n\nfname1 =\n['/Users/jacquelinesoegaard/Documents/Spring2012/15.097/CourseProject/SynthData/datas\net_097.csv'];\ncsvwrite(fname1, data_mat);\n\nfname2 =\n['/Users/jacquelinesoegaard/Documents/Spring2012/15.097/CourseProject/SynthData/reals\net_097.csv'];\ncsvwrite(fname2, real_mat);\n\nfunction [realdata, dataset] = generateSyntheticDatasets(ncohort)\n\nn_cov = 10; % number of covariates in model\n\nncohort1 = ncohort;\n\n% A. Create the correlation matrix for the covariates\n\ncorr_mat = zeros(n_cov,n_cov);\n\nfor i = 1:n_cov\ncorr_mat(i,i) = 1;\n\nend\n\ncorr_mat(5,1) = 0.2;\ncorr_mat(1,5) = 0.2;\ncorr_mat(6,2) = 0.9;\ncorr_mat(2,6) = 0.9;\ncorr_mat(8,3) = 0.2;\ncorr_mat(3,8) = 0.2;\ncorr_mat(9,4) = 0.9;\ncorr_mat(4,9) = 0.9;\n\n% B. Generate the covariates, store in a ncohort x n_covariate matrix\n\nV = zeros(ncohort1,n_cov); % matrix of base covariates\nW = NaN(ncohort1, n_cov); % matrix of final covariates\n\ni_base = [1,2,3,4,5,6,8,9];\ni_final = [7,10];\ni_binary = [1,3,5,6,8,9];\n\n% 1. Generate 8 base covariates V_i (i = 1...6, 8,9) and two final\n% covariates W_i (i=7,10) as independent standard normal r.v. ~ N(μ=0,var=1)\n\nfor i = 1:length(i_base)\nV(1:ncohort1, i_base(i)) = random('Normal',0,1,ncohort1,1);\nend\nfor i = 1:length(i_final)\nW(1:ncohort1, i_final(i)) = random('Normal',0,1,ncohort1,1);\nend\n\n% 2. Model the final 8 covariates W_i for i = 1...6,8,9 from linear\n% combinations of the V_i, using the correlations from the correlation\n% matrix. Also, binarize variables i = 1,3,5,6,8,9\n\nfor i = 1:ncohort1\nfor j = 1:length(i_base)\nW(i, i_base(j)) = dot(V(i,:), corr_mat(i_base(j),:), 2);\nend\nend\n\nvar_medians = median(W, 1);\n\nfor i = 1:ncohort1\nfor j = 1:length(i_binary)\nvar_id = i_binary(j);\nif W(i,var_id) > var_medians(var_id)\nW(i,i_binary(j)) = 1 ;\nelse\nW(i,i_binary(j)) = 0 ;\nend\nend\nend\n\n% C. Model the binary exposure for each of seven scenarios\n\n[truePropScore, exposureA] = calculateTruePropensityScores2(W, ncohort1);\n\n% D. Model the binary outcome Y.\n\n[tpY_real, outcomeY_real trueProbY, outcomeY] =\ncalculateTrueOuctomeProb2(W,exposureA, ncohort1);\n\n% Store this dataset's information in a struct object\n\nrealdata.W = W;\nrealdata.TPS = truePropScore(:,1);\nrealdata.A = exposureA(:,1);\n\nrealdata.pY = tpY_real;\nrealdata.Y = outcomeY_real;\n\ndataset.W = W;\ndataset.TPS = truePropScore(:,2:3);\ndataset.A = exposureA(:,2:3);\ndataset.pY = trueProbY;\ndataset.Y = outcomeY;\nend\n\nfunction [TPS, A] = calculateTruePropensityScores2(W, ncohort)\n\n% Set coefficient values for three different cases: the Setoguchi\n% et.al. coefficients from real experimental studies, low coefficients,\n% and high coefficients.\nb0 = 0;\nb_real = [0.8, -0.25, 0.6, -0.4, -0.8, -0.5, 0.7];\nb_low = [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05];\nb_high = [0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95];\n\nTPS = NaN(ncohort,3); % P(A|W_i), true propensity score calculated from model\nA = NaN(ncohort,3); % binary exposure A\n\n% Calculate the true propensity scores (TPS) (i.e. P(A|W_i) ) and the dichotomous\n% exposure A for each of the three scenarios. All scenarios are linear\n% and additive.\n\nfor i = 1:ncohort\n%Scenario A , with the \"real\" world coefficients.\nTPS(i,1) = (1 + exp( -( b0 + b_real(1)*W(i,1) + b_real(2)*W(i,2) +\nb_real(3)*W(i,3) + ...\nb_real(4)*W(i,4) + b_real(5)*W(i,5) + b_real(6)*W(i,6) +\nb_real(7)*W(i,7) )))^-1;\n%Scenario B, with \"low\" coefficients for all of the covariates.\nTPS(i,2) = (1 + exp( -( b0 + b_low(1)*W(i,1) + b_low(2)*W(i,2) +\nb_low(3)*W(i,3) + ...\nb_low(4)*W(i,4) + b_low(5)*W(i,5) + b_low(6)*W(i,6) +\nb_low(7)*W(i,7) )))^-1;\n%Scenario C, with \"high\" coefficients for all of the covariates.\nTPS(i,3) = (1 + exp( -( b0 + b_high(1)*W(i,1) + b_high(2)*W(i,2) +\nb_high(3)*W(i,3) + ...\nb_high(4)*W(i,4) + b_high(5)*W(i,5) + b_high(6)*W(i,6) +\nb_high(7)*W(i,7) )))^-1;\n\nend\n\nfor i = 1:ncohort\nfor j = 1:3\n% generate a random number between [0,1] from the uniform\n% distribution\nrand_num = random('Uniform',0,1);\nif rand_num < TPS(i,j)\nA(i,j) = 1;\nelse\nA(i,j) = 0;\nend\nend\nend\nend\n\nfunction [pY_real, Y_real, pY, Y] = calculateTrueOuctomeProb2(W, A, ncohort)\n\n% Here, A has values for real, low, and high beta coeff cases.\n\n% Number of sets of values for betas, alphas, and gamma.\nnum_b = 2;\nnum_a = 2;\nnum_g = 5;\n\n% Set coefficient values:\n% The alpha (a0...a7) coefficients are for the outcome model\na0 = -3.85;\na_real = [0.3, -0.36, -0.73, -0.2, 0.71, -0.19, 0.26];\na_low = [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05];\na_high = [0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95];\n\n% The treatment labels for the three \"beta\" cases\nA_real = A(:,1);\nA_low = A(:,2);\nA_high = A(:,3);\n\n% The gamma (g) coefficient is the effect of exposure on the outcome\ng_real = [-0.4];\ng_range = [-0.9, -0.5, 0, 0.5, 0.95];\n\n% Instantiate arrays to hold the results of the calculations below\n\npY_real = NaN(ncohort,1); % P(Y|W_i, A), probability of the outcome for real\nvalued coeff scenario\nY_real = NaN(ncohort, 1); % binary outcome Y for real valued coeff scenario\n\npY = NaN(ncohort,num_b*num_a*length(g_range)); % P(Y|W_i, A), probability of the\noutcome ...\n% given the covariates and the\nexposure\nY = NaN(ncohort,num_b*num_a*length(g_range)); % binary outcome Y\n\n% calculate the pY and Y values for the \"real\" Setoguchi coefficient\n% case\nfor i = 1:ncohort\npY_real(i) = (1 + exp( - (a0 + a_real(1)*W(i,1) + a_real(2)*W(i,2) + ...\na_real(3)*W(i,3) + a_real(4)*W(i,4) +\na_real(5)*W(i,8) + ...\na_real(6)*W(i,9) + a_real(7)*W(i,10) +\ng_real*A_real(i) )))^-1 ;\n% generate a random number between [0,1] from the uniform\n% distribution\nrand_num = random('Uniform',0,1);\n\nif rand_num < pY_real(i)\nY_real(i) = 1;\nelse\nY_real(i) = 0;\nend\nend\n\n% Calculate the pY and Y values for the low and high alpha coeff case,\n% with each of the gamma values:\nfor i = 1:ncohort\nfor gamma = 1:length(g_range)\n\n% beta_low and alpha_low\npY(i,gamma) = (1 + exp( - (a0 + a_low(1)*W(i,1) + a_low(2)*W(i,2) + ...\na_low(3)*W(i,3) + a_low(4)*W(i,4) + a_low(5)*W(i,8) + ...\na_low(6)*W(i,9) + a_low(7)*W(i,10) +\ng_range(gamma)*A_low(i) )))^-1 ;\n% beta_low and alpha_high\npY(i,(gamma+5)) = (1 + exp( - (a0 + a_high(1)*W(i,1) + a_high(2)*W(i,2)\n+ ...\na_high(3)*W(i,3) + a_high(4)*W(i,4) + a_high(5)*W(i,8) + ...\na_high(6)*W(i,9) + a_high(7)*W(i,10) +\ng_range(gamma)*A_low(i) )))^-1 ;\n% beta_high and alpha_low\npY(i,(gamma+5*2)) = (1 + exp( - (a0 + a_low(1)*W(i,1) + a_low(2)*W(i,2)\n+ ...\na_low(3)*W(i,3) + a_low(4)*W(i,4) + a_low(5)*W(i,8) + ...\na_low(6)*W(i,9) + a_low(7)*W(i,10) +\ng_range(gamma)*A_high(i) )))^-1 ;\n% beta_low and alpha high\npY(i,(gamma+5*3)) = (1 + exp( - (a0 + a_high(1)*W(i,1) + a_high(2)*W(i,2)\n+ ...\na_high(3)*W(i,3) + a_high(4)*W(i,4) + a_high(5)*W(i,8) + ...\na_high(6)*W(i,9) + a_high(7)*W(i,10) +\ng_range(gamma)*A_high(i) )))^-1 ;\nend\nend\n\nfor i = 1:ncohort\nfor j = 1:(size(pY,2))\n\n%generate a random number between [0,1] from the uniform\n%distribution\nrand_num = random('Uniform',0,1);\n\nif rand_num < pY(i,j)\nY(i,j) = 1;\nelse\nY(i,j) = 0;\nend\nend\nend\nend\n\n# Use SVMs to generate and apply two classifiers:\n# minus and plus treatment variables\n\n#Load the necessary packages\nlibrary(e1071)\n\n################\n# Function for calculating the net reclassification improvement (NRI)\n\ncalculateNRI <- function(model1_labels, model2_labels, true_labels){\n# This function calculates the net reclassification improvement, which is a summary\nstatistic\n# that describes the reclassification, thus allowing us to compare the clinical impact\n\nof two models by\n# determining how many individuals would be reclassified with new\nbaseline informaiton.\n\n# NOTE: Here, cases are those examples with outcome Y=1 and noncases are examples with\nY=0,\n# as defined by the true (original) labels\n\nnum_cases <- sum(true_labels ==1)\nnum_noncases <- sum(true_labels == 0)\n\ncase_ix <- (true_labels ==1)\nnoncase_ix <- (true_labels ==0)\n\ncase_ix <- case_ix - 10000\nnoncase_ix <- noncase_ix - 10000\n# Look at the reclassification movement.\n# the \"movement\" vector will have values in {-1,0,1}\n# -1 : \"down\" movement, reclassified from 1 to 0 with treatment info\n# 0 : no reclassification\n# +1 : \"up\" movement, reclassified from 0 to 1 with treatment info\n\nmovement <- model2_labels - model1_labels\n\npUp_cases <- sum(movement[case_ix]==1)/num_cases # = P(up|Y=1)\npDown_cases <- sum(movement[case_ix] == -1)/num_cases # P(down|Y=1)\n\npUp_noncases <- sum(movement[noncase_ix]==1)/num_noncases # P(up|Y=0)\npDown_noncases <- sum(movement[noncase_ix] == -1)/num_noncases # P(up|Y=0)\n\nnetgains_cases <- pUp_cases - pDown_cases\nnetgains_noncases <- pUp_noncases - pDown_noncases\n\nNRI = netgains_cases - netgains_noncases\n\nsensitivity1 <- sum(model1_labels == test_outcome & test_outcome == 1)/sum(test_outcome\n== 1)\nspecificity1 <- sum(model1_labels == test_outcome & test_outcome == 0)/sum(test_outcome\n== 0)\n\nsensitivity2 <- sum(model2_labels == test_outcome & test_outcome == 1)/sum(test_outcome\n== 1)\nspecificity2 <- sum(model2_labels == test_outcome & test_outcome == 0)/sum(test_outcome\n== 0)\n\ndelta_sens <- sensitivity2-sensitivity1\ndelta_spec <- specificity2-specificity1\n\nreturn(c(NRI, netgains_cases, netgains_noncases, delta_sens, delta_spec))\n}\n\n################\n# Read a practice synthetic dataset\ndataset <-\nas.data.frame(read.csv(\"/Users/jacquelinesoegaard/Documents/Spring2012/15.097/CourseProj\n\nect/SynthData/dataset_097.csv\", header = FALSE))\n\nn_data <- nrow(dataset)\nhalf_data <- n_data/2 # half of the data will be used for training and half for testing\nn_beta <- 2\nn_scenario <- 20\n\n# These are the indeces for the different types of data\ncovariate_ix <- c(1:10)\ntps_ix <- c(11:12)\ntreatmentA_ix <- c(13:14)\npY_ix <- c(15:34)\noutcome_ix <- c(35:54)\n\n# Set aside the training data\ntrain_features <- dataset[1:(half_data), covariate_ix]\ntrain_TPS <- dataset[1:(half_data), tps_ix ]\ntrain_treatment <- dataset[1:(half_data), treatmentA_ix ]\ntrain_pOutcome <- dataset[1:(half_data), pY_ix ]\ntrain_outcome <- dataset[1:(half_data), outcome_ix ]\n\n# Set aside the test data\ntest_features <- dataset[(half_data+1):n_data, covariate_ix]\ntest_TPS <- dataset[(half_data+1):n_data, tps_ix ]\ntest_treatment <- dataset[(half_data+1):n_data, treatmentA_ix ]\ntest_pOutcome <- dataset[(half_data+1):n_data, pY_ix ]\ntest_outcome <- dataset[(half_data+1):n_data, outcome_ix ]\n\n##############\n### Generate SVM models\n\n# We want to train 40 SVMs, with each of the 20 scenarios having two SVMs:\n# Model 1 will correspond to the SVMs trained on the baseline covariates (X_is) to\npredict the outcome Y\n# Model 2 will correspond to the SVMs trained on BOTH the baseline covariates (X_is) and\nthe treatment labels (Z_is) to predict the outcome Y.\n\n# Make lists to hold the svm models\nmodel1_holder <- vector(mode = \"list\", length = n_scenario)\nmodel2_holder <- vector(mode = \"list\", length = n_scenario)\nperformance_stats <- NULL # will fill up later with performance statistics\n\nbeta_low_ix <- c(1,2,5,6,9,10,13,14,17,18)\nbeta_high_ix <- c(3,4,7,8,11,12,15,16,19,20)\nC = 100\n\nfor (i in 1:n_scenario){\n# train the model that uses only the baseline covariates as features\nX1 <- train_features #features are the n=10 covariates\nY1 <- train_outcome[1:nrow(train_outcome),i] # outcome\nsvm_model_1 <- svm(X1, Y1, scale = FALSE, kernel = \"radial\", cost = C, decision.values =\nFALSE)\nmodel1_holder[[i]] <- svm_model_1\n\n# train the model that uses both the covariates and the treatment labels as features\nif (sum(beta_low_ix == i) == 1){\n\nX2 <- cbind(train_features, train_treatment[1:nrow(train_treatment), 1])\n\ntest_features2 <- cbind(test_features, test_treatment[1:nrow(test_treatment), 1])\n\nprint(\"im here\")\n}\nif (sum(beta_high_ix == i) == 1){\n\nX2 <- cbind(train_features, train_treatment[1:nrow(train_treatment), 2])\n\ntest_features2 <- cbind(test_features, test_treatment[1:nrow(test_treatment), 2])\n\nprint(\"now I'm there\")\n}\nY2 <- train_outcome[1:nrow(train_outcome),i]\nsvm_model_2 <- svm(x = X2, y=Y2, scale = FALSE, kernel = \"radial\", cost = C,\ndecision.values = FALSE)\nmodel2_holder[[i]] <- svm_model_2\n\n# Apply the SVM classifiers to the test data to obtain predicted labels\n\nmodel1_labels <- sign(predict(svm_model_1,newdata = test_features, probability = FALSE))\nmodel2_labels <- sign(predict(svm_model_2,newdata = test_features2, probability =\nFALSE))\n\n# Convert the +1/-1 labeling convention to +1/0 so that it matches the original labels\nmodel1_labels[model1_labels == 1] <- 0\nmodel2_labels[model2_labels == 1] <- 0\n\nmodel1_labels[model1_labels == -1] <- 1\nmodel2_labels[model2_labels == -1] <- 1\n\n# Calculate the NRI and other performance statistics\nstats <- calculateNRI(model1_labels, model2_labels, test_outcome)\nperformance_stats <- rbind(performance_stats, stats)\nprint(stats)\n\nwrite.table(performance_stats, file = \"svm_NRI.csv\", sep = \",\", col.names = NA, qmethod\n= \"double\")\n}\n\n# Perform adaBoost to calculate the propensity scores\n\nlibrary(rpart)\nlibrary(ada)\n\n# Read a practice synthetic dataset\ndataset <-\nas.data.frame(read.csv(\"/Users/jacquelinesoegaard/Documents/Spring2012/15.097/CourseProj\nect/SynthData/dataset_097.csv\", header = FALSE))\n\nn_data <- nrow(dataset)\nhalf_data <- n_data/2 # half of the data will be used for training and half for testing\n\nn_beta <- 2\nn_scenario <- 20\n\n# These are the indeces for the different types of data\ncovariate_ix <- c(1:10)\ntps_ix <- c(11:12)\ntreatmentA_ix <- c(13:14)\npY_ix <- c(15:34)\noutcome_ix <- c(35:54)\n\n# Set aside the training data\ntrain_features <- dataset[1:(half_data), covariate_ix]\ntrain_TPS <- dataset[1:(half_data), tps_ix ]\ntrain_treatment <- dataset[1:(half_data), treatmentA_ix ]\ntrain_pOutcome <- dataset[1:(half_data), pY_ix ]\ntrain_outcome <- dataset[1:(half_data), outcome_ix ]\n\n# Set aside the test data\ntest_features <- dataset[(half_data+1):n_data, covariate_ix]\ntest_TPS <- dataset[(half_data+1):n_data, tps_ix ]\ntest_treatment <- dataset[(half_data+1):n_data, treatmentA_ix ]\ntest_pOutcome <- dataset[(half_data+1):n_data, pY_ix ]\ntest_outcome <- dataset[(half_data+1):n_data, outcome_ix ]\n\n##############\n### Propensity Score calculation using boosting\n\n# We want to train 7 boosting classifiers for calculating the propensity score (i.e.\nprobability of receiving treatment), one for each of the scenarios\nmodel_holder <- vector(mode = \"list\", length = 2)\nfor (i in 1:n_beta){\nX <- train_features #features are the n=10 covariates\nY <- train_treatment[1:nrow(train_treatment),i] #the \"outcome\" is treamtment label\nboost_model_i <- ada(x=X, y=Y)\nmodel_holder[[i]] <- boost_model_i\n}\n\n# Make a matrix for storing the propensity scores for each individual under\n# The matrix dimensions (n x m) will be : n = number of instances ; m = number of beta\nscenarios\n# Each entry (i,j) will denote the propensity score of instance i in scenario j\nprop_scores <- NULL\nfor (i in 1:n_beta) { # Loop over the two beta coefficient cases\n# the predict function will return a vector with the probability of class membership\nclass_prob <- predict(model_holder[[i]], newdata = test_features, type = \"prob\")\nprop_scores <- cbind(prop_scores,class_prob[1:nrow(class_prob), 2])\n}\n\nbeta_low_PS <- prop_scores[1:nrow(prop_scores),1]\nbeta_high_PS <- prop_scores[1:nrow(prop_scores),2]\n\n##############\n### Now, use the propensity scores to calculate the ATE for the 20 outcome scenarios\n\nbeta_low_ix <- c(1,2,5,6,9,10,13,14,17,18)\nbeta_high_ix <- c(3,4,7,8,11,12,15,16,19,20)\nholderATE <- c(from = 0, to = 0, length.out = n_scenario )\n\nfor (i in 1: n_scenario){ #Loop over the 20 scenarios\n\n# Select the outcome coefficients for scenario i out of the 20 scenarios.\noutcome <- test_outcome[1:nrow(test_outcome),i]\n\n# Case where the low beta coefficients were used for the treatment and outcome models\nif (sum(beta_low_ix == i) == 1){\n\ntreatment <- test_treatment[1:nrow(test_treatment),1]\n\nholderATE[i] <- (1/half_data)*sum((treatment*outcome)/beta_low_PS) -\n(1/half_data)*sum(((1-treatment)*outcome)/(1-beta_low_PS))\n}\n# Case where the high beta coefficients were used for the treatment and outcome models\nelse{\n\ntreatment <- test_treatment[1:nrow(test_treatment),2]\n\nholderATE[i] <- (1/half_data)*sum((treatment*outcome)/beta_high_PS) -\n(1/half_data)*sum(((1-treatment)*outcome)/(1-beta_high_PS))\n}\n}\n\nwrite.table(holderATE, file = \"averageTreatemntEffect.csv\", sep = \",\", col.names = NA,\nqmethod = \"double\")\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.097 Prediction: Machine Learning and Statistics\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "15.097 Student Project: Online k-Means Clustering",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-097-prediction-machine-learning-and-statistics-spring-2012/29119365aa01195c89609e2895eed4f3_MIT15_097S12_proj1.pdf",
      "content": "15.097 Prediction Project Report\nOnline k-Means Clustering of\nNonstationary Data\nAngie King\nMay 17, 2012\n\nIntroduction and Motivation\nMachine learning algorithms often assume we have all our data before we try to learn from\nit; however, we often need to learn from data as we gather it over time. In my project I\nwill focus on the case where we want to learn by clustering. The setting where we have all\nthe data ahead of time is called \"batch clustering\"; here we will be interested in \"online\nclustering.\" Specifically, we will investigate algorithms for online clustering when the data is\nnon-stationary.\nConsider a motivating example of a t-shirt retailer that receives online data about their sales.\nThe retailer sells women's, men's, girls' and boys' t-shirts. Fashion across these groups is\ndifferent; the colors, styles, and prices that are popular in women's t-shirts are quite different\nfrom those that are popular in boys' t-shirts. Therefore the retailer would like to find the\n\"average\" set of characteristics for each demographic in order to market appropriately to\neach. However, as fashion changes over time, so do the characteristics of each demographic.\nAt every point in time, the retailer would like to be able to segment its market data into\ncorrectly identified clusters and find the appropriate current cluster average.\nThis is a problem of online clustering of non-stationary data. More formally, consider d-\ndimensional data points which arrive online and need to be clustered. While the cluster\nshould remain intact, its center essentially shifts in d-space over time.\nIn this paper I will argue why the standard k-means cost objective is unsuitable in the case of\nnon-stationary data, and will propose an alternative cost objective. I will discuss a standard\nalgorithm for online clustering, and its shortcomings when data is non-stationary. I will in-\ntroduce a simple variant of this algorithm which takes into account nonstationarity, and will\ncompare the performance of these algorithms with respect to the optimal clustering for a sim-\nulated data set. I will then discuss performance guarantees, and provide a practical, rather\nthan a theoretical, way of measuring an online clustering algorithm's performance.\nBackground and Literature Review\nBefore we delve into online clustering of time-varying data, we will build a baseline for this\nproblem by providing background and reviewing relevant literature.\nAs in the t-shirt example, the entire data set to be processed may not be available when we\nneed to begin learning; indeed, the data in the t-shirt example is not even finite. There are\nseveral modes in which the data may be available and we now define precisely what those are.\nIn batch mode, a finite dataset is available from the beginning. This is the most commonly\nanalyzed setting. Streaming mode refers to a setting where there is a finite amount of data\nto be processed but it arrives one data point at a time, and the entire dataset cannot be\nstored in memory. The online setting, which we will be concerned with here, departs from\nthe previous two settings in that the there is an infinite amount of data to be analyzed, and\n\nit arrives one data point at a time. Since there is an infinite amount of data, it is impossible\nto store all the data in memory.\nWhen clustering data in the batch setting, several natural objectives present themselves. The\n\"k-center\" objective is to minimize the maximum distance from any point to its cluster's\ncenter. The \"k-means\" objective is to minimize the mean squared distance from all points\nto their respective cluster centers. The \"k-median\" objective is to minimize the distance\nfrom all points to their respective cluster centers. Depending on the data being analyzed,\ndifferent objectives are appropriate in different scenarios.\nIn our case we will focus on the k-means objective. Even in the batch setting, finding the\noptimal k-means clustering is an NP-hard problem [1]. Therefore heuristics are often used.\nThe most common heuristic is often simply called \"the k-means algorithm,\" however we will\nrefer to it here as Lloyd's algorithm [7] to avoid confusion between the algorithm and the\nk-clustering objective.\nIn the batch setting, an algorithm's performance can be compared directly to the optimal\nclustering as measured with respect to the k-means objective. Lloyd's algorithm, which is\nthe most commonly used heuristic, can perform arbitrarily badly with respect to the cost of\nthe optimal clustering [8]. However, there exist other heuristics for the k-means objective\nwhich have a performance guarantee; for example, k-means++ is a heuristic with an O(logk)-\napproximation for the cost of the optimal clustering [2]. Another algorithm, developed by\nKanungo et al. in [6], gives a constant factor 50-approximation to the cost of the optimal\nclustering.\nHowever, in the online setting, it is much less clear how to analyze an algorithm's perfor-\nmance. In [5], Dasgupta proposed two possible methods for evaluating the performance of\nan online clustering scheme. The first is an approximation algorithm style bound: if at time\nt an algorithm sees a new data point xt and then outputs a set of k centers Zt then an\napproximation algorithm would find some α ≥1 such that Cost(Zt) ≤α OPT, where OPT\nis the cost for the best k centers for x1, ..., xt.\nAnother type of performance metric is regret, which is a common framework in online learn-\ning. Under a regret framework, an algorithm would announce a set of centers Zt at time t,\nthen recieve a new data point xt and incur a loss for that point equal to the squared distance\nfrom xt to its closest center in Zt. Then the cumulative regret up to time t is given by\nX\nmin ||xτ -z\nz Z\nτ≤t\n∈\nτ\n||2\nand we can compare this to the regret that would be achieved under the best k centers for\nx1, .., xt.\nDasgupta acknowledges that \"it is an open problem to develop a good online algorithm for\nk-means clustering\" [5] under either of these performance metrics. Indeed, although several\nonline algorithms exist, almost nothing theoretical is known about performance.\n\nThere is a standard online variant of Lloyd's algorithm which we will describe in detail in\nSection 4; like Lloyd's algorithm in the batch case it can perform arbitrarily badly under\neither of the performance frameworks outlined above.\nThe only existing work that proves performance guarantees for online k-means clustering is\nby Choromanska and Monteleoni [4]. They consider a traditional online learning framework\nwhere there are experts that the algorithm can learn from. In their setting, the experts are\nbatch algorithms with known performance guarantees. To get an approximation guarantee\nin the online sense, they use an analog to Dasgupta's concept of regret defined above [5]\nand prove the approximation bounds by comparing the online performance to the batch\nperformance, which has a known performance guarantee with respect to the cost of the\noptimal clustering.\nAlthough [4] is the only paper which provides performance bounds for online clustering, there\nis a growing literature in this area which focuses on heuristics for particular applications.\nIn [3], rather than trying to minimize the k-means objective, Barbakh and Fyfe consider how\nto overcome the sensitivity of Lloyd's algorithm to initial conditions when Lloyd's algorithm\nis extended to an online setting.\nAs motivated in Section 1, here we will be concerned with developing a heuristic for online\nclustering which performs well on time-varying data.\nThe traditional k-means objective is inadequate in the non-stationary setting, and it is not\nobvious what it should be replaced by. In this project, we will propose a performance ob-\njective for the analog of k-means clustering in the the non-stationary setting and provide\njustification for the departure from the traditional k-means performance objective. We then\nintroduce a heuristic algorithm, and in lieu of proving a performance guarantee (which it\nshould be clear by now is an open, hard problem in the online setting, let alone when the\ndata is non-stationary) we will test empirical performance of the algorithm on a simulated\nnon-stationary data set. By using a simulated data set we have ground truth and can there-\nfore calculate the optimal clairvoyant clustering of the data with respect to our performance\nmeasure. However, the findings would apply to any real data set which has similar prop-\nerties of clusterability and nonstationarity; we discuss this in more detail in Section 4 and\nSection 5.\nCost objective in the non-stationary setting\nThe traditional batch k-means cost function is given by\nCost(C1, ..., C\nk, z1, ..., zk) =\nX\nxi\nzk\nk\n{i:x\nX\n||\n-\ni∈C\n||\nk}\nwhere C1, ..., Ck are the k clusters and z1, ..., zk are the cluster centers [8].\n\nIn the online setting, we must consider the cost at any point in time. At time t, we have\ncenters zt\n1, ..., zt\nk and clusters Ct\n1, ..., Ct\nk. Then\nCostt(Ct\n1, ..., Ct\nk, zt\n1, ..., zt\nk) =\nX\nk\n{i:x\nX\n||x\nt||2\ni -zk\ni∈Ct\nk}\nI claim that this objective is inappropriate in the non-stationary setting; this objective\nimplicitly assumes that we want to find the best online clustering for all of the points seen\nso far, and at time t all points x1, ..., xt contribute equally to the cost of the algorithm.\nHowever, in the non-stationary setting, old data points should not be worth as much as new\ndata points; the data may have shifted significantly over time.\nIn order to take this into account, I propose the following cost function for the non-stationary\nsetting:\nCostt(Ct\n1, ..., Ct\nk, zt\n1, ..., zt\nk) =\nX\nX\nδt-i||x\nt\ni\nk\n{i:x ∈\nt\n-zk||2\ni Ck}\nwhere δ ∈(0, 1).\nUnder this cost function, we want to find k clusters which minimize\nthe weighted squared distance from each point to its cluster center, where the weight is\nexponentially decreasing in the age of the point.\nAlgorithms\nSince we are in an online setting with a theoretically infinite stream of data, it is impossible\nto store all of the data points seen so far. Therefore we assume that from one time period\nto the next, we can only keep O(k) pieces of information where k is the number of cluster\ncenters.\nIn Online Lloyd's algorithm, at time step t, each point x1, ..., xt contributes equally to de-\ntermine the updated centers. Here is the psuedocode for Online Lloyd's Algorithm:\ninitialize the k cluster centers z1, ..., zk in any way\ncreate counters n1, ..., nk and initialize them to zero\nloop\nget new data point x\ndetermine the closest center zi to x\nupdate the number of points in that cluster:\nni ←ni + 1\nupdate the cluster center:\nzi ←z\ni +\n(x\nni\n-zi)\nend loop\nI propose an alternative to updating the centers by the update rule zi ←z\ni +\n(x\nni\n-zi).\nInstead, I suggest using a discounted updating rule zi\nzi + α(x\nzi) for α\n(0, 1).\n←\n-\n∈\n\nThis exponential smoothing heuristic should work well when the data is naturally clusterable\nand the cluster centers are moving over time. However, as with Lloyd's algorithm, it may\nperform arbitrarily badly and is particularly sensitive to the initial cluster centers. Also,\nit may not be a good choice of algorithm if you do not know the number of clusters a\npriori.\nEmpirical Tests\nIn order to run empirical tests, I simulated a data set. I wanted the data set to have the\nproperties of having natural clusters and for these clusters to drift over time. Since I needed\na data set with ground truth, I modified the well known data set, iris, in order to have the\ntime series property. Therefore the modified iris data had four dimensions, with 3 natural\nclusters whose centers drift over time.\nI tested Online Lloyd's algorithm on this dataset, as well as my exponentially smoothed\nversion of Online Lloyd's. Additionally, I calculated the optimal clairvoyant cost. A plot of\ncosts appears below for α = 0.5:\n\nAs expected, Online Lloyd's performs poorly as time goes on, but my exponential smooth-\ning version of Online Lloyd's forgets the initial points and performs close to the optimal\nclairvoyant solution.\n\"Practical Performance Guarantee\"\nIt is a hard, open problem to make performance guarantees for online clustering, let alone\nwhen the data is non-stationary. However, if we can keep track of the cost that an algorithm\nis incurring over time, we can give a practical performance guarantee by comparing costs of\ndifferent algorithms against each other. I claim that it is possible to keep track of the cost\nwithout holding all points ever seen in memory. Specifically, I give the following result:\nResult 1. It is possible to calculate the cost in time period t + 1 recursively with only O(k)\npieces of information from the previous time period.\nProof. We will need to keep track of the following pieces of information:\n- Costt = (Costt\n1, Costt\n2, ...Ct\nk) = the cost at time t for each center\n- zt = (zt\n1, zt\nt\n2, ...zk), the cluster centers at time t\n- sum-deltat = (sum-delta1\nk\nk\nt\ni\nt, ..., sum-deltat ) where sum-deltat =\ni:x\nCt δ -is a scalar\n{\ni∈\nk}\nkeeping track of the sum of the discount factors of every point that has ever entered\ncluster k up to time period t. At every time period we will multiply\nP\nthis number by δ.\n- sum-delta-xt = (sum-delta-x1\nt, ..., sum-delta-xk\nt ) where sum-delta-xk\nt = P\ni\n{i:x ∈Ct δt-x\n}\ni\ni\nk\nis a vector keeping track of the sum of every point that has ever entered cluster k up\nto time period t, weighted by its discount factor.\nLet d be the dimension of the data points.\nNote that Costt\nk = P\ni\n{i:xi∈C\nδt\nt\n-\nk}\n||xi -zt+1\nk\n||2\n2 =\nd\n{i:x ∈Ct δt-i\n}\n=1(x\nt\nj\nij\n)\ni\n-zkj\nfor every\nk\ncenter k. Suppose that at time t + 1 a new data poin\nP\nt xt+1 arrives\nP\nto cluster k.\nFirst we update:\nsum-deltat+1 ←δsum-deltat\nsum-deltak\nt+1 ←δsum-deltak\nt+1 + 1\nsum-delta-xt+1 ←δsum-delta-xt\nsum-delta-xk\nt+1 ←δsum-delta-xk\nt+1 + xt+1\nThe new centerPzt+1\nk\ncan be chosen in any way. Then the cost in time period t + 1 is given\nby Costt+1\nk\n+ δ\nj=k Costt\nj. We can calculate Costt+1\nk\nas follows.\n\nCostt+1\nk\n=\nX\nδt+1-i\nk\n||xi -zt+1||2\nk\n{\nt\ni:xi\nX\n∈\n+1\nCk\n}\n=\n{\nX\nd\nδt+1-i\n(x\nij\nzt+1\nk\n)\nj\n∈\nt+1\nj=1\n-\ni:xi Ck\n}\nX\n=\nX\nd\nδt+1-i X\n(x\n-zt + zt -zt+1 2\nij\nkj\nkj\nk\n)\nj\n{\n+1}\nj=1\ni:xi∈\nt\nCk\nX\nX\nd\n=\nδt+1-i\n(x\n-zt )2 + 2(x\n-zt )(zt\nt\nk\nzt\ni\n(\nt+1\nk\n)2\nj\nkj\nij\nj\nkj -z +1\nk\n) +\nkj -z\nj\nj\n{i:xi∈\nt+1\nj=1\nCk\n}\n= δ\n\nX\nd\nδt-i X\nd\n(x\nt\nt\nij -zkj)\n+\n(xt+1j -zkj)\n{i:x ∈Ct\nj=1\n\ni\nk}\n\nX\nj=1\n+\nX\nd\nd\nδt+1-i X\n2(x\n-\nt\nt\nt+1\nt\ni\nt\nt+1 2\nij\nz\n+1\nkj)(zkj\nzk\n) +\nδ\nj\n-\n(zkj\nzk\n)\nj\n{i:xi∈Ct}\nj=1\n-\n{i\nX\n-\n:x\nCt\ni\nX\nj=1\nk\n∈\nk}\nd\nd\n= δCostt +\n(\nk\nk\nX\nx\nt\nj -zt\nt\nt+1\n+1\nkj) + sum-deltat+1\nX\n(zkj -zk\n)\nj\nj=1\nj=1\n!\nd\n+ 2\nX\n(zt\nt+1 2\nk\nt\nk\nkj\nk\n)\nj\nj\n-z\n\nsum-delta-xt+1 -zkjsum-deltat+1\n=1\n\nTherefore, counting a point as a single piece of information, we can calculate the cost recur-\nsively from time t to time t + 1 by storing 2 + 2k = O(k) pieces of information from one\nperiod to the next.\nTherefore, even without knowing the optimal cost, it is possible to run several online clus-\ntering algorithms in parallel, calculate the cost each one is incurring, and choose the best\none for a specific dataset.\nConclusion\nIn this paper we have explored online clustering, specifically for non-stationary data. Online\nclustering is a hard problem and performance guarantees are rare. When non-stationarity is\n\nintroduced, there is not even a standard agreed-upon framework for measuring performance.\nHere, I have defined a cost function for the non-stationary setting, and demonstrated a\nsimple exponential smoothing algorithm which performs well in practice. There is lots of\nroom to invent algorithms for online clustering of non-stationary data, but little theoretical\nmachinery to give performance guarantees. In lieu of this, I have shown that at the very\nleast, we can compare algorithms against each other by keeping track of the cost with only\nO(k) pieces of information. This gives a \"practical performance guarantee.\"\nThis paper scratches the surface of online clustering for non-stationary data. In general, this\ncorner of clustering research contains many open questions.\n\nReferences\n[1] Daniel Aloise and Amit Deshpande, Pierre Hansen, and Preyas Popat. \"Np-hardness of\neuclidean sum-of- squares clustering\". Machine Learning, 75:245-248, May 2009.\n[2] D. Arthur and S Vassilvitskii.\n\"k-means++: the advantages of careful seeding\".\nIn\nProceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms,\npages 1027-2035, 2007.\n[3] W. Barbakh and C. Fyfe. \"Online Clustering Algorithms\". International Journal of\nNeural Systems (IJNS), 18(3):1-10, 2008.\n[4] Anna Choromanska and Claire Monteleoni. \"Online Clustering with Experts\". In Journal\nof Machine Learning Research (JMLR) Workshop and Conference Proceedings, ICML\n2011 Workshop on Online Trading of Exploration and Exploitation 2, 2011.\n[5] Sanjoy Dasgupta. \"Clustering in an Online/Streaming Setting\". Technical report, Lec-\nture notes from the course CSE 291: Topics in Unsupervised Learning, UCSD, 2008.\nRetrieved at cseweb.ucsd.edu/~dasgupta/291/lec6.pdf.\n[6] T. Kanungo and D. Mount, N. Netanyahu, C. Piatko, R. Silverman and A. Wu. \"A Local\nSearch Approximation Algorithm for k-Means Clustering\". Computational Geometry:\nTheory and Applications, 2004.\n[7] S. P. Lloyd. \"Least square quantization in pcm\". Bell Telephone Laboratories Paper,\n1957.\n[8] Cynthia Rudin and Seyda Ertekin. \"Clustering\". Technical report, MIT 15.097 Course\nNotes, 2012.\n[9] Shi Zhong. \"Efficient Online Spherical K-means Clustering\". In Journal of Machine\nLearning Research (JMLR) Workshop and Conference Proceedings, IEEE Int. Joint Conf.\nNeural Networks (IJCNN 2005), pages 3180-3185, Montreal, August 2005.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.097 Prediction: Machine Learning and Statistics\nSpring 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "15.097 Student Project: Operator-Defined SUAV Classification Tree Algorithms",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-097-prediction-machine-learning-and-statistics-spring-2012/ce13ee1e33ee9792b978754aa6b51629_MIT15_097S12_proj3.pdf",
      "content": "\"Operator-defined SUAV Classification Tree Algorithms for\nWaypoint Prediction and Increased Use of Autonomy\"\nPhil Root\n16 May 2012\nAbstract\nThe growing divide between the potential of fully automated teams of Unmanned Aerial Vehicles\n(UAVs) and the very limited autonomy seen in operational use deserves attention. This paper proposes a\nlimited step to realize increased autonomy use in operational settings by applying decision tree learning\nalgorithms to UAV operator's flight patterns. These algorithms learn and then propose UAV trajectories\nbased on the individual preferences and patterns of each operator. This approach is distinct from previous\nmethods in that it seeks to tailor autonomy suitable to each operator rather than seek a panacea for\nall. We show that C4.5 classification trees have numerous advantages over competing algorithms, and\nlimited experimental results have provided great promise for future research.\nIntroduction\nSmall Unmanned Aerial Vehicles (SUAV) hold great promise for those operators in austere environments.\nAlready used ubiquitously throughout the US military, SUAVs are increasing used by domestic law enforce\nment agencies. Soldiers and Marines rely on the Raven SUAV pictures above, for example, to perform a\nvariety of missions to include surveillance of bases, reconnaissance of convoy routes, and aerial patrolling in\na variety of combat locations. SUAVs differ from their larger brethren in that they can be assembled and\nlaunched without any tools or runways. Soldiers have launched these aircraft from rooftops, remote bases,\nand hillsides. Their use is limited only by their batteries and the operator's imagination.\n1.1\nCurrent Use of Small UAVs\nSUAVs are commonly used by small groups of Soldiers and Marines to conduct reconnaissance in advance\nof some military operation. Figure 1a depicts the notional reconnaissance of a village by a SUAV launched\nfrom a nearby base. The SUAV operator chooses this flight trajectory for some set of reasons based on\nhis/her knowledge of the terrain and the target. The challenge for engineers and researchers is how to infer\nthis rationale from a flight trajectory depicted in figure 1b that is devoid of this informaiton.\nThere exists a spate of research that discusses optimal search patterns for such SUAV intent on patrolling\nan area such as [3] or servicing targets that arrive via a given distribution such as [4]. Their work greatly\n\n(a) Notional reconnaissance of a village\n(b) UAV trajectory as seen by flight planner\nFigure 1: Current operational SUAV use\nadvances the technical capability of collaborative SUAVs to achieve a common mission, but in practice we see\nvirtually no such use of autonomy. It is perhaps most compelling to demonstrate this dichotomy graphically.\nFigure 2a depicts the state of the art persistent patrolling technique from [3] that iteratively links points via\na Traveling Salesman Problem (TSP) solver where the points are selected according to some distribution.\nFigure 2b, on the other hand, depicts a typical \"lawnmower\" search pattern as used in practice. Moreover,\nwe typically see the best SUAV operators capable of controlling a single SUAV aircraft at a time whereas the\nmore advanced search and patrolling algorithms assume that a single operator can control multiple aircraft.\n(a) Persistent patrolling algorithm for multiple\nSUAVs\n(b) Typical patrolling technique used in practice\nFigure 2: Comparison of theoretical versus actual patrolling algorithms used in practice\nThis difference between theoretical and actual SUAV operation is well-known and studied as in [2]. What\nhas generally not been acknowledged is that every operator is different with varying degrees of experience,\neducation, risk tolerance, and willingness to trust autonomy. Any attempt to improve the autonomy for all\nusers could benefit from the realization that not all methods will be universally adopted. Hence this work\nproposes a limited first step toward increased autonomy by first learning a particular operator's preferences\nand patterns, and then proposing sequential waypoints based on the learned model.\nWe summarize the intent of this research as to answer the following central question.\n\nCentral problem: What is the best method to learn an operator's patterns for SUAV\nreconnaissance so as to propose future waypoints with a high probability of user\nacceptance?\n1.2\nRationale for Classification Tree Implementation\nThere are several proposed advantages to a classification tree learning algorithm compared to other alter\nnatives for this particular application. Any algorithm that addresses the central question for this research\nmust have two key qualities: first it must be a sufficiently powerful algorithm so as to describe a wide variety\nof operator patterns, and secondly, the algorithm must lead to increased rates of user adoption. We submit\nthat C4.5 classification trees are well-suited to address both of these requirements as we will discuss.\nClearly classification schemes are sufficiently powerful to address the problem as posed. Most operators\nwill only provide on the order of hundreds of training data examples, and these examples will have dimensions\non the order of tens and perhaps hundreds. C4.5 classification trees are more than sufficiently powerful for\nthis class of problem.\nOne key advantage of the family of decision tree algorithms is that their resulting model is much more\ntransparent and interpretable that other machine learning algorithms. The graphical decision tree is simple\nto understand even without a background in the material, and the decision tree provides rationale as to\nwhy the algorithm proposed a particular waypoint. Breiman et al outlined the utility for Classification And\nRegression Trees (CART) in [1]. Although C4.5 classification trees are implemented differently than CART,\nthey enjoy the same benefits particularly when applied to this problem. Breiman begins by providing a\nmedical example to motivate his work where a binary classification tree applied to nineteen variables predicted\nhigh risk patients with only three splits. The resulting classification tree is hence very easy to understand\nand implement particularly because is represents the logic that most medical professionals follow when\nmaking decisions. Similarly, in section 3 we will show that applying the C4.5 algorithm to a \"lawnmower\"\nsearch trajectory with a forty-two variables requires only four nodes or splits to accurately predict all future\npoints, and the graphical representation of this classification tree is very simple to grasp. This paper will\nalso explore the utility of other learning algorithms such as Support Vector Machine (SVM) algorithms.\nSVM also provides graphical interpretations for their solutions as they seek to graphically maximize the\nmargin between categorical data. But a plot of the SVM separation oracle is really only interpretable in two\ndimensions, and the problem at hand will have a much higher dimensionality where SVM graphical methods\nare insufficient.\nPerhaps one of the greatest advantages of our proposed modifications to CART and C4.5 is that our\nalgorithm does not propose the unique best solution, but rather selects the answer probabilistically based on\nthe distribution of labels in the optimal leaf, as we will show. Figure 3a depicts a notional reconnaissance\nmission where the SUAV launches from a base to a nearby set of waypoints. Perhaps the operator directs the\nSUAV to visit the point that has been visited least recently; stated differently visit the point with the largest\namount of time since the last visit. But the operator doesn't follow this pattern with probability 1. Consider\nthe far right leaf where point 4 has had the longest time since being visited last. In this case, the operator\nhas visited point 4 eight times, but he/she has also visited point 1 once, and point 2 once. This information\nabout points 1 and 2 is valuable as it reflects the some real information about the operator's mental decision\nmodel. While it is feasible to alter many learning algorithms to take advantage of this variance of the\noutcome, it is very simple for decision trees to both depict this information and take advantage of it.\nWe claim that the final advantage of decision trees over alternative learning algorithms is the malleability\nof the model. Returning to figure 3, suppose that the operator flies thirty waypoints as shown in figure 3b\nand then decides that fuel is too low and commands the SUAV to return to base. This one data point to\ncommand the SUAV to base reflects only one point of the thirty previous training examples. Hence, this\ndecision to return to base is likely to be misclassified as some other leaf in the tree. It is far simpler to append\na new heuristic to the existing decision that reflects this low density decision (\"Return to base whenever\nfuel < 5%\") than it would be to alter the output of a SVM or logistical regression model. Thus, users can\nmore easily modify or tailor their learned algorithm to further personalize the resulting recommendations.\nAll this would yield a higher user acceptance rate given the ease of modification and personalization.\n\n(b) Notional decision tree\n(a) Notional reconnaissance mission\nFigure 3: Notional reconnaissance mission and decision tree\nImplementation\nOur implementation method consisted of creating a Graphical User Interface (GUI) to allow data collection,\napplying learning algorithms to the collected data, and seeking feedback from users as to the viability of this\napproach.\n2.1\nExperimental Setup\nWe created a MATLAB GUI that allowed us to gather data quickly and reliably over a wide range of\ndifferent mission profiles. We discretized a square environment into N points. We chose N = 25 arbitrarily,\nand the GUI allows for increased or decreased discretization. Figure 4a depicts the MATLAB GUI upon\ninitialization. We always \"launch\" the simulated SUAV from the origin in the lower left corner.\nDepressing the \"Start Training\" button causes the GUI to display a set of crosshairs so that the operator\ncan select points on the map representing SUAV waypoints. Each additional waypoint adds another training\nexample for the learning algorithms to be described below. Each example captures the state vector at the\ntime the operator chose that point. We designed the state vector in an attempt to capture data that would\nbe useful given an unknown mission profile. Stated differently, the learning algorithm has no knowledge a\npriori as to the type of reconnaissance or mission so the state vector must contain sufficient data to amply\ndescribe a wide range of operator types. We chose to collect forty-two attributes for each training example\nas follows.\n⎤\n⎡\n⎤\n⎡\nx1\ntime elapsed\n⎢⎢⎢⎢⎢⎢⎣\nx2\nx3 - x7\nx8 - x12\nx13 - x17\n⎥⎥⎥⎥⎥⎥⎦\n=\n⎢⎢⎢⎢⎢⎢⎣\nfuel remaining\nlast M = 5 points visited\nlatitude of last M = 5 points\nlongitude of last M = 5 points\n⎥⎥⎥⎥⎥⎥⎦\nx18 - x42\nΔT for N = 25 points\nVariable x1 and x2 are the time elapsed and fuel remaining, respectively. Variables x3 to x7 are the\nlast M = 5 discretized waypoints closest to the selected coordinate. Similarly, we collected the latitude and\nlongitude of the last M coordinates visited. Finally, variables x18 to x42 are the length of time since that\npoint was last visited, ΔT . As stated previously, we chose N = 25 as a balance between prediction fidelity\nand problem complexity, and we felt that twenty-five points were sufficient for our experimental work. We\nchose to track the previous M = 5 points for similar reasons although we can adjust both N and M should\nwe ever require higher fidelity. In this training mode, the GUI updates the elapsed time, fuel remaining, and\nnumber of waypoints after each selection.\nWe took great care to ensure we accurately determined the time that a simulated SUAV passed by each\nof the N points. Figure 4b depicts a set of training data, and the path often passes from point 5 to point 25.\n\n(a) GUI upon initialization\n(b) GUI with training data\nFigure 4: GUI upon initialization and after collecting some training data\nWe modified the Bresenham algorithm to rapidly determine which of the N points we intersected passing\nfrom point 5 to point 25, and updated their resulting ΔT . This step was nontrivial but vital to the accuracy\nof the remainder of the work.\nThe \"Return to Base\" button commands the simulated aircraft to return to the origin but does not stop\nthe training session. The operator stops submitting training examples at some point, and depresses the\n\"Predict Next Point\" button. This button submits the previously collected training data and the current\nstate to various learning algorithms to predict the next point using the following decision tree algorithms.\n2.2\nDecision Tree Algorithms\nWe first implemented a binary classification tree in MATLAB due to the simplicity to do so as an initial\nstep. The following code excerpt implemented the actual algorithm.\n1 x=[h . t , h . fuel , h . IntLast , h . LatLast , h . LongLast , h . deltaT ] ;\n2 y=[h . l a b e l ( 1 : end-1)]\n3 DT = c l a s s r e g t r e e (x ( 1 : end- 1 ,:) , y , ' method ' , ' c l a s s i f i c a t i o n ' , . . .\n' c a t e g o r i c a l ' , [3:2+h .M] , ' prune ' , ' on ' , ' names ' ,h . names ) ;\n5 view (DT)\n6 h . prednext=eval (DT, x(end , : ) ) ;\nLine 1 concatenates the state variables to form the example data, and line 2 contains the training labels. It\nis of note that the last row of x contains the state vector used for predicting the next point, so when we call\nthe classregtree command in line 3, we reference all but the last line of x. We define the last M points\nas \"categorical\" data as compared with \"numeric\" data as part of the classregtree command call. Line 5\nallows us to see the resulting model, and line 6 evaluates the resulting DT model given the current state in\nthe last row of x to predict the next point.\nWe then implemented a C4.5 classification tree algorithm using Weka APIs available to MATLAB as\nshown in the following code.\ntrain = matlab2weka ( ' route -train ' ,h . names , train , classindex ) ;\n\nmodel = t r a i n W e k a C l a s s i f i e r ( t r a i n , ' t r e e s . J48 ' )\nt e s t = matlab2weka ( ' route -t r a i n ' , h . names , t e s t , c l a s s i n d e x ) ;\n[ p r e d i c t e d , c l a s s ] = w e k a C l a s s i f y ( t e s t , model ) ;\np i k r = rand ( 1 ) ; count =1;\nwhile p i k r > sum( c l a s s (end , 1 : count ) )\ncount = count +1;\n8 end\nnext = char ( t e s t . c l a s s A t t r i b u t e . val u e ( count -1));\nThe matlab2weka function uses the Weka APIs to create a JAVA object for the training data in line 1,\nand line 2 forms a C4.5 classification model using this training data. The test variable contains the current\nstate vector for use in predicting the next point in line 3. We then tailored the wekaClassify function to\noutput the probability distribution of the labels within the selected leaf rather than simply the classification\nlabel with the maximum probability in line 4. The code in lines 5 to 8 then picks a random variable, pikr,\nand we select the label associated with this value within the probability distribution. Line 9 determines\nwhich label to assign based on this stochastic process.\n2.3\nSupport Vector Machine Algorithm\nIn addition to the binary and C4.5 classification algorithms outlines above, we implemented a Support Vector\nMachine (SVM) algorithm for purposes of comparison. More specifically, we implemented a \"one-vs-all\"\nclassification scheme using the code highlights below.\nk l a b e l s = unique ( data . l a b e l ( 1 : end-1));\n2 for i = 1: length ( k l a b e l s )\nk=k l a b e l s ( i ) ;\ntmplabel = y ;\ntmplabel ( tmplabel==k)=1;\ntmplabel ( tmplabel =1)=0;\nsvmStruct = svmtrain ( train , tmplabel ) ;\nyout = [ yout svmeval (x(end , : ) , svmStruct ) ] ;\n9 end\nout = k l a b e l s ( find ( yout==max( yout ) ) ) ;\nFor each unique label in the training data, the SVM algorithm alternatively converts the training labels to\nbinary data, and determines the optimal support vectors and weights in lines 1 to 7. The svmeval command\na\nin line 8 evaluates the classifier as c =\nλik(si, x) + bi where the SVM model contains the optimal weights\ni\nλi, bias bi, and support vectors si for the selected kernel k(·, ·) and training data x. Line 10 then chooses\nthe classifier with the highest real value.\nResults and Feedback\nWe found the MATLAB GUI a highly effective tool to gather data and evaluate all the classification schemes\nlisted above. We used the GUI to input several common patrolling techniques such as orbiting around a\npoint and a \"lawnmower\" pattern. The orbit pattern depicted in figure 5a just follows a sequence of four\npoints. The resulting binary classification tree depicted in figure 5b required thirty-two training examples\nbefore it accurately predicted the next point in the pattern. The C4.5 classification scheme required only\neleven training examples before it converged to the correct pattern. This is not an accurate comparison\nbecause the binary classification tree was deterministic in that it always chose the label that constitutes\nthe majority of the leaf. We altered the C4.5 algorithm to randomly select the label from the probability\ndistribution of the leaf, so the C4.5 algorithm only \"converges\" given this repeated orbit pattern after all\nthe leaves are pure.\nThe lawnmower pattern depicted in figure 6a is more complex. It follows the pattern {5, 25, 24, 4, 3,\n23, 22, 2, 1, 21, 22, 2, 3, 23, 24, 4, 5}. The binary classification tree required ninety training examples to\nconverge to the model shown in figure 6b. Given a smaller training set, the binary classification algorithm\n\n(a) Orbit patrol trajectory\n(b) Orbit classification tree\nFigure 5: Orbit trajectory and classification tree\ncould correctly predict several elements of the sequence, but not the entire sequence. The modified C4.5\nalgorithm required only fifty-eight training examples to converge to a static and predictive model.\n(a) Orbit patrol trajectory\n(b) Orbit classification tree\nFigure 6: Orbit trajectory and classification tree\nIn all cases, the decision tree algorithms converged to accurate predictions with a limited number of\ntraining examples. Importantly, at no time did the classification schemes suggest an errant point such as\na point that had yet to be visited. While this may seem like a trivial achievement, it is key to establish\noperators' trust.\nInterestingly, the SVM algorithm never predicted either of these two test patterns reliably. One possible\nexplanation deals with missing values in the training data. Each training example contains the ΔT for each\npoint, and the orbit pattern never visits some points such as point 5, for example. We initialize all ΔT to\n-1000, but this initialization could cause numeric issues. Consider the \"lawnmower\" pattern which visits\n\neach point in two \"directions\". The pattern visits point 1, for example, both following point 2 and following\npoint 21 depending on the location within the sequence. This is trivial for classification trees to address,\nbut SVM seeks a separation hyperplane that separates all point 1 labels from all others, but point 1 labels\nhave two distinct states when they occur. Trying linear, quadratic, and higher order polynomial kernels\nsimilarly failed to achieve satisfactory results. It is possible that these two distinct states are too distant in\nthe 42-dimensional state space to separate from other points.\nIt is challenging to draw any objective conclusions about the predictive nature of the classification trees\nin general because we don't know what the \"correct\" waypoint would be in all cases. These simple test cases\nconfirm that indeed the classification trees can replicate an operator's decision process, but we hesitate to\noverextend this conclusion. Instead we had the opportunity to present this research to the US Army Small\nUAV Program Office in Huntsville, Alabama. As representatives of the users, they were able to provide us\nfeedback as to the veracity of our claim. They were very supportive of this work as a means to capture\noperator preferences and patterns, and they validated that this approach was the best they had seen to\nachieve this end.\nConclusions\nOur limited experiments make it clear that this approach in general performs well as a means to learn\noperator preferences and present them with the results so as to increase user acceptance rates.\nWe find that the modified C4.5 classification tree has several advantages over the binary classification\ntree. First, it converges more quickly to a stable model in our trials. Secondly, we were able to easily modify\nthe algorithm such that it selects the label stochastically based on the probability distribution of the selected\nleaf which more closely mirrors the operator's mental process.\nWe also find that either classification tree performs better than SVM in predicting the sample patterns.\nWe struggle to believe that SVM has no reliable predictive nature when applied to this problem, although\nit may indeed be poorly conditioned to address the problem. We imagine that our SVM implementation is\nfaulty in some regard leading to these results. Regardless, one of the key advantages of the classification\ntrees is that they output a model that can be displayed graphically to users for their interpretation and\nacceptance. An SVM model is much more challenging to present in higher dimensions. Thus we posit that\nusers will more readily adopt a classification tree scheme compared to SVM due simply to this difference.\nFuture Work\nWe believe that this work has potential across many fields where operators could benefit from increased\nautonomy but are reticent to do so due to potential safety issues. Consider the Air Traffic Control (ATC)\nproblem where ATC controllers must route aircraft through a busy corridor. This problem has been well-\nstudied as in [5], but in general proposed solutions fail to account for individual differences between ATC\ncontrollers in the way that they prefer to route traffic and deal with weather factors. This work could easily\nbe extended to this problem.\n\nReferences\n[1] Leo Breiman, Jerome H. Friedman, Richard Al Olshen, and Charles J. Stone. Classification and Regres\nsion Trees. Chapman & Hall, 1984.\n[2] M.L. Cummings, S. Bruni, S. Mercier, and P.J. Mitchell. Automation architecture for single operator,\nmultiple uav command and control. The International C2 Journal, 1(2):1-24, 2007.\n[3] Vu Anh Huynh, J.J. Enright, and E. Frazzoli. Persistent patrol with limited-range on-board sensors. In\nDecision and Control (CDC), 2010 49th IEEE Conference on, pages 7661-7668, December 2010.\n[4] Marco Pavone, Ketan Savla, and Emilio Frazzoli. Sharing the load. Robotics & Automation Magazine,\n16(2):52-61, 2009.\n[5] Thomas Prevot, Paul Lee, Nancy Smith, and Everett Palmer. Atc technologies for controller-managed\nand autonomous flight operations. In AIAA Guidance, Navigation, and Control Conference and Exhibit,\n2005.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.097 Prediction: Machine Learning and Statistics\nSpring 2012\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "15.097 Student Project: Security Analysis using Machine Learning",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/15-097-prediction-machine-learning-and-statistics-spring-2012/ade34baea92a97d2df4491faa6e8aa2b_MIT15_097S12_proj2.pdf",
      "content": "A Profitable Approach to Security Analysis Using Machine Learning: An\nApplication to the Prediction of Market Behavior Following Earnings Reports\n\nI. Background\n\nFour times per year, companies release earnings reports. Often the day following such\nreports, the stocks of these companies make significant moves either up or down from the\nresulting information. Such moves are often attributed to a large surprise between the analyst\nestimates for the stock and the actual earnings amounts. Unfortunately for the average trader,\nthese jumps usually occur in a very short window of time following the earnings release, making\nit difficult to profit on either bullish or bearish earnings. Since earnings reports are not usually\nreleased during market hours, a method for determining which stocks will continue to move\neither up or down throughout the day would allow a trader to create a strategy that takes actual\nearnings information into account without being limited by this extremely short window of\nopportunity. Further, with the hundreds of earnings reports that are released each day, this type\nof method could parse and interpret an enormous amount of information--far beyond the\ncapability of a single person. This study endeavors to create such a method that could utilize\npublicly available stock data in order to make trading recommendations for the entire day\nfollowing the release of earnings reports.\nThe traditional method that is used to measure the strength of earnings reports is earnings\nper share (EPS)--or the ratio of a company's profit to the number of outstanding shares of its\ncommon stock [5]. Outstanding shares of common stock simply represent the number of shares\nof a company that are held publicly [5]. Many fundamental financial analysts often give their\nown predications for what a given company's EPS will be in any given quarter, and as such, data\nconcerning EPS estimates and actual realizations are much less sparse than other measurements\nof earnings strength (one such possibility could be company revenue). In addition, EPS has an\nintuitive interpretation which makes its use easy--EPS simply represents the average dollar\n\namount of total gain or loss of publicly owned stock during a specified time period. Thus, this\nproject will attempt to learn patterns of market behavior following EPS news reports by\nexamining behavior of past data. Further, this newly learned information will be used to create a\nmachine-learning based algorithm for trading following earnings reports. In turn, this discovery\nwill provide a foundation for algorithms that can process the information of hundreds of earnings\nreports each day in a meaningful manner.\n\nII. Data Collection\nThe first step of the project was to determine an appropriate data set. Earnings reports\nare released quarterly, so each observation was chosen to describe the earnings release for a\nsingle stock. In addition, for each observation, the only attributes that were considered were\nrequired to be freely and publicly available on the internet. To reduce variability in the\nobservations, the data only include those reports that released just quarterly earnings and not\nannual earnings (this could be relaxed for future research). The actual attributes that were\nchosen to describe a single observation in the data set are shown in Appendix A. Past\nobservations containing all of the attributes listed in Appendix A were obtained by matching data\nfrom the Thompson Reuters I/B/E/S and Compustat Databases--both available freely to the MIT\ncommunity through Wharton Research Data Services [10]. The I/B/E/S database contained all\ninformation about past analyst EPS estimates and actual EPS realizations necessary for\ncomputing items 5-7 in Appendix A. The Compustat database contained the close, high, low,\nand open prices for each stock by day, as well as daily volume (number of stocks traded) and\nshares outstanding, used to calculate items 8-19 and 22-24, while items 20-21 required both\ndatabases.\n\nMany of the attributes were dependent upon the number of shares available for any given\nstock. Unfortunately, the number of shares can be chosen arbitrarily for an initial public\noffering, meaning that many attributes in the aforementioned data set did not have a standard\nbasis for comparison across the observations. Even though the number of shares can be chosen\narbitrarily, the market capitalization of a stock is absolute. Thus, any attribute that is expressed\nas some ratio \"per share\" was transformed into a ratio \"per dollar of Market Capitalization\" by\nsimply dividing the current values by the value in Appendix A, thus fixing the\nproblem of standardization. For example, suppose that two different observations are being\ncompared, one for stock A and the other for stock B. Let stock A have a value of\n$50 and a value of $100, and stock B a value of $25 but a\nvalue of $200. Suppose that the actual realized EPS for these observations were $2/share for\nstock A and $1/share for stock B. By definition, market capitalization is the number of\noutstanding shares multiplied by the price per share [5], meaning that stock A has 2 shares, and\nstock B has 8 shares. Thus, stock A has a total earnings (profit) of $4, and stock B has a total\nearnings of $8, both of which are 4% of their respective market capitalizations.\nThis example shows that between stocks, differences in EPS do not imply differences in\nprofit or market capitalization. In fact, the percentage increase of total assets in terms of market\ncapitalization is the true desired measure of absolute earnings. Using the definition of market\ncapitalization, it is easy to see that this value is simply the EPS divided by (which\nreturns 4% for both A and B). This idea generalizes for all of the other \"per share\" ratios (items\n5, 6, and 8-10 in Appendix A), which allows for complete standardization by simply dividing\nthese attributes by . Volume also suffers from the arbitrary number of shares, but\nhas units of shares/day rather than dollars/share. Thus, to standardization of volume involves\n\nmultiplying by rather than dividing, giving the total absolute number of dollars\ntraded per day, as well as standard deviation in dollars, for that stock (this must be done for items\n15-18). By performing these transformations, the attributes can be compared on an absolute\nbasis between observations.\n\nIII. Methodology\nThe first objective was to use the completely cleaned, standardized dataset to predict the\nvalues of for out-of-sample observations. Successful completion of this objective\nwould immediately open the doors for many successful trading strategies. Naturally, this can be\nframed as a regression problem, where the dependent variable is regressed on the\nindependent variables 1-21 in Appendix A. However, regression requires real-valued\nindependent variables, so the categorical attributes in Appendix A were first converted to binary\nattributes before running regression. In addition, there were a total of 91,726 collected\nobservations of quarterly earnings release data, which meant that each of the 13 sectors had\nabout 7,056 observations--more than enough data to allow separate regression analysis by\nsector. Another added benefit of splitting sectors is that the industries within each sector are\nunique. Thus, controlling by sector vastly reduces the number of possible industries in each of\nthe 13 data sets from over 200 to around 15-40 per data set. For simplicity, the data and analysis\nfor this project only come from the Capital Goods sector.\nFor this regression, the methods employed included Classification and Regression Trees\n(CART), Random Forests, additive nonparametric regression, and backpropagation neural\nnetwork regression. The reader is referred to [11], [2], [3], and [4], respectively, to obtain\nbackground pertaining to these methods. Due to the large number of binary variables and\n\nnonlinearities in the data, multiple linear regression and its variants (such as Ridge Regression)\nwere not considered.\nFor any given sector, each method was given the same training and testing data. The training\nset for each sector was created by randomly selecting 70% of the data in a manner that kept\napproximately the same distribution of values in both the training and test sets. All\nmodels were implemented in R using packages rpart (CART) [8], randomForest (Random\nForests) [1], mgcv (additive nonparametric regression) [9], and nnet (backpropagation neural\nnetworks) [7]. Each algorithm was trained carefully to reduce the chance of overfitting. For\nCART, this involved varying the complexity parameter in R to be one of 0.00005, 0.0001,\n0.0005, 0.001, 0.005, and 0.5 (see [8] for information about the complexity parameter). For\nRandom Forests, the number of decision trees was set to 500 to make sure that every input\nobservation was predicted enough times by multiple trees, which helps to prevent overfitting.\nAdditive nonparametric regression was implemented using the package mgcv in R, which has\na built-in method called \"generalized cross-validation\" that helps to decrease the chance of\noverfitting [3]. Thus, no extra controls were implemented for this method. The next method,\nbackpropagation neural networks, could have the problem of \"memorizing\" the data without\nlearning general patterns if either the number of nodes in the hidden layer or the number of\nlearning iterations were too large. However, these parameters must also be large enough to\nallow the network to learn patters from the data. Thus, the number of nodes in the hidden layer\nwere varied from 3 to 15, and the number of learning iterations was varied from 10 to 100, with\nthe best model being chosen via 10-fold cross validation on the training data using the package\ne1071 in R (see [4] for neural network theory).\n\nUnfortunately, these regressions performed poorly. As a result, another easier problem was\ndeveloped to accomplish the high-level goal of finding a profitable trading strategy. This\ninvolved creating a classifier that predicted the general direction of price movement for a stock\nduring the session following earnings releases. However, it was not immediately clear how to\ndefine the \"general direction of price movement.\" This ambiguity stemmed from the fact that in\nthe absence of a good regression price predictor, different trading strategies motivated creation of\ndifferent classification levels. While there were many possible strategies from which to choose,\nfour simple strategies were constructed as motivation for classifiers in a fashion that brought to\nlight a few important insights about the data. These strategies were compared against a basic\nnaive strategy to illustrate the improvement gained by using machine learning techniques. These\nare described here as strategies (1)-(4) and \"naive\" (variables in italics are defined in Appendix\nA).\n(1) The first strategy considered was to purchase on open and sell on close. Intuitively, this\nstrategy motivated a classifier which labeled \"1\" if the closing price is predicted to be\nabove some threshold value (i.e. ), and \"-1\" otherwise.\n(2) Another slightly more complicated strategy involved setting both a limit sell order and a\nstop sell order at specified threshold values. This ensured that the stock was sold if either\na desired amount of gain or a maximum acceptable loss were attained. In the event that\nneither level was reached, the stock was assumed to be sold on close. This strategy\nmotivated a classifier which labeled \"1\" if and\nfor some real thresholds (recall that\nis non-positive), and \"-1\" otherwise.\n\n(3) A variant of the same strategy that was considered was to only have a stop sell order but\nno limit order, so that stock would be sold before close only if the maximum allowable\nloss was attained during the day. This strategy motivated the same classifier as used in\n(2) because an investor would want to be confident that his/her choices would likely have\na high maximum possible return with a minimum return that was no worse than some\nacceptable loss.\n(4) The final motivational strategy was derived from a \"long straddle\" in options trading,\nwhich benefits when the underlying stock price moves a large amount in either direction.\nIn this case, a sensible classification would be to give a label of \"1\" if\nor , and \"-1\" otherwise. (Note that the\nthreshold values were preset in this strategy because option pricing data was not a part of\nthis dataset, so an appropriate objective function that maximized profit could not be\nconstructed.)\n(5) (naive) This strategy simply assumed that a trader would buy if the actual EPS from the\nearnings report was greater than the consensus mean analyst EPS estimate. Thus, its\ncorresponding classifier labeled \"1\" when and \"-1\" otherwise.\nThis was provided to illustrate the benefit of using machine-learning based classification\nmethods over simple, non-technical strategies.\nThis method of choosing classifiers was named strategy-based classification. For the sake of\nbrevity, the analysis of (1)-(3) was performed on the assumption of buying long at open rather\nthan selling short, although the methodology that would be employed in the latter case is\ncompletely analogous to the former. The analysis was performed by maintaining an equal\nproportion of both classes in the training and testing sets, so the separation of the data differed\n\nslightly for the different classifiers. Though this introduced a small amount of variability into the\nproblem, it ensured that more appropriate classifiers were developed. The threshold values for\nstrategies (1)-(3) were then chosen as follows. Let be the set of observations in the test data\nsuch that the classifier associated with strategy assigned a label of \"1\" to , and let be the\nreturn obtained if the stock associated with observation was purchased on open and sold\naccording to strategy during the day following the earnings release, for all . Then the\nthreshold values or (depending on the strategy) were chosen so that the resulting\nclassifiers maximized the objective function ∏\n\n, representing the total return that would be\nobtained by investing all money sequentially in each of the observations during the sessions\nfollowing the releases of earnings. This objective function worked well because it appropriately\nmodeled the compromise between having a high geometric average return and creating a\nclassifier that offered a decent amount of trading opportunities (recall that trading only occurred\nif an observation had a predicted label of \"1\").\nComparison of the results of the various classifiers was then used to provide valuable\ninsights into the structure of price movements following earnings releases. This contribution\naided in determining which of the aforementioned strategies was best, assuming the values of\nand created near-optimal evaluations of the objective function. It is important to note that the\ngoals here were deeper than simply finding the most appropriate classification algorithm for this\ndata. Also, fitting the values of and required constructing a very large amount of classifiers.\nThus, Random Forests were chosen to perform all of the classifications as they have been shown\nto work very well on large datasets (on par with boosting) while also being very computationally\nefficient [2].\n\nFollowing this analysis, some of the insights obtained suggested that regression on a\nsubset of the original data set could be more effective than the regression that was previously\nperformed on the entire data set (see analysis and results section). To test this hypothesis, the\ntraining set was divided into two sets of observations based on whether or not\n. In each set, Random Forest regression was applied to train a predictive model for\n. For the test set, the classifier for strategy (1) was then applied to predict whether\nor not . For those that were predicted to have , the first\nregression model trained on observations that actually had was applied,\notherwise the second regression model was utilized. For this analysis, Random Forests were the\nonly regression method considered, although future research should include other methods as\nwell. The reason for this was similar to why Random Forests were also used for the\nclassifications--the purpose of this analysis was simply to see if this form of regression could\nprovide any sort of benefit. Thus, time was spent in testing this method rather than comparing\nthe different regression algorithms. In addition, an added benefit of Random Forests was that\nthey did very well at finding nonlinear trends in the data without overfitting because they\nclassified based on linear combinations of the underlying decision-tree classifications.\n\nIV. Analysis and Results\n\nThe first objective, trying to predict the values of from the attributes, did\nnot yield quality results for any of the models considered. There were four measures of\nperformance that were employed to analyze these methods. The first of these, least squares error\nof the test set residuals (LSE), provided a relative measure of the performance between\nalgorithms. To gain insight into the absolute performance of each method, the actual and\n\npredicted values of the test set were compared in a scatterplot (see Figure B.1 in Appendix B for\none example of these scatterplots, done for additive nonparametric regression). In such a plot, a\ngood predictor should have most of its data points clustered along the line through the\norigin. To see if this was the case, the actual values were regressed on the\npredicted using linear regression, and the resulting , slope, and intercept values\nwere reported in table 1. These provide absolute measures of performance where a good\npredictor should have this value close to 1, a slope of approximately 1, and an intercept\nthrough the origin.\n\nMethod\nBest LSE on\ntest set\n\nSlope\nIntercept\nCART\n9.315226\nNot defined (best\ntree had same\nprediction for all\ndata)\nNot defined (best\ntree had same\nprediction for all\ndata)\nNot defined (best\ntree had same\nprediction for all\ndata)\nRandom Forests\n7.304142\n0.05453\n1.258\n-0.000251\nAdditive\nNonparametric\nRegression\n4.423454\n0.0001189\n0.0612\n0.03258\nBackpropagation\nNeural Networks\n9.315226\nNot defined (best\nnetwork had same\nprediction for all\ndata)\nNot defined (best\nnetwork had same\nprediction for all\ndata)\nNot defined (best\ntree had same\nprediction for all\ndata)\n\nLooking at table 1, there were two methods for which the best model in terms of LSE\nsimply predicted a constant value for each input observation--CART and backpropagation\nneural networks. Thus, no unique regression line could be drawn for these, although clearly\npredicting the same value for any observation was not a good predictor. In addition, though\nRandom Forests and additive nonparametric regression do not predict a single constant value of\nTable 1--Results of nonlinear regression methods when predicting ReturnPost\n\nfor any observation, the statistics of the linear regression of actual versus predicted\nshowed that these methods also performed poorly. Both of these latter methods had values\nwell below 0.1 which meant that the predictors did very little to explain the behavior of the\nactual values. Also, both had slope and intercept values that were clearly different\nthan 1 and 0, respectively, when considering that the values have magnitudes on\naverage around 0 to 0.06. These statistics suggested that the data may have had such a large\namount of noise that quality predictions were not possible.\n\nDespite the poor performance of the regression methods for determining ,\nthe strategy-based classification methods were much more successful at capturing patterns in the\ndata. Recalling the structure of these methods, a value of \"1\" meant that the stock was expected\nto perform in a manner that was likely to make the strategy profitable, and \"-1\" in a manner that\nwas likely to be unprofitable. These methods were compared on four measures of\nperformance--percent of the test data classified correctly, percent predicted to be \"1,\" percent of\nthose predicted to be \"1\" that were correct, and percent of those predicted to be \"-1\" that were\ncorrect. Of these, the most important two were the percent predicted to be \"1\" and the percent of\nthose predicted to be \"1\" that were correct. This was due to the construction of the classifier,\nwhich was designed to indicate \"1\" when there was high confidence that its respective strategy\nwould work. Thus, more \"1\" values meant more opportunities to trade. This also implied that an\nerror of predicting a \"1\" instead of a \"-1\" was very bad, for this implied that a loss would be\nincurred. The results of this comparison, as well as the threshold values which maximized the\nobjective function ∏\n\non the test set (see Methodology), are summarized in table 2.\n\nStrategy\n% classified\ncorrectly\n% of values\nPredicted as\n\"1\"\n% of Predicted\n\"1\" values that\nwere correct\n% of Predicted\n\"-1\" values that\nwere correct\nBest fitted\nthreshold values\n70.96%\n20.02%\n74.90%\n69.98%\n\n77.88%\n10.67%\n81.08%\n77.50%\nand\n\n73.27%\n31.18%\n72.13%\n73.79%\nand\n\n77.35%\n61.29%\n77.42%\n77.23%\nand\n(not\nfitted--no option\npricing data)\nNaive\n53.05%\n63.55%\n51.13%\n56.38%\nN/A\n\nThe arithmetic and geometric means for strategies (1)-(3), as well as the naive strategy,\nare shown in table 3 (strategy 4 is not included as actual pricing data for options were not\nincluded in this data set--this was left for future research). From these data, it can be seen that\nstrategies (3), (4), and the naive strategy would provide the most opportunities to trade, even\nthough (1) and (2) had higher average returns. Taking all of this data into account, the objective\nfunction determined that strategy (3) should produce the most profit among the stock trading\nstrategies (recall that strategy 4 could not be evaluated for profitability since option pricing was\nnot in the data set). In addition, (1)-(3) all outperformed the naive strategy of simply investing in\nthose stocks for which the actual return was higher than the consensus analyst return, which\nsuggests that employing machine learning was useful. The fact that this naive strategy\nperformed so poorly as compared to the other methods suggests the difference between\nconsensus analyst EPS estimates and actual EPS values may not have had as significant of an\neffect on the price shift following earnings releases as originally hypothesized. This evidence\nsupports recent findings that managers purposefully distort the perceived success of their\nTable 2--Performance of strategy-based classification methods\n\ncompanies in order to receive positive differences between actual and estimated EPS values,\nbecause this would cause EPS surprises to be less influential in the market [6].\nStrategy\nArithmetic Average Return on\nTest Set\nGeometric Average Return on\nTest Set\n3.214%\n3.038%\n3.274%\n3.179%\n2.305%\n2.018%\nNaive\n0.358%\n0.180%\n\nStrategy-based classification was also used to improve the regression results obtained at\nthe outset of the study (see methodology for a description of how this was done). The results of\nthis regression for Random Forests are shown in table 4. Comparing these results to those\nobtained earlier, the LSE obtained by separating the data first via classification actually increased\nwith this method, suggesting that the data separation may have caused the new regression\nmethod to perform even worse than the original ones. However, further analysis showed more\npromising results. The value of the actual values regressed on the predicted\nvalues was much higher in this method, and almost above 0.1. In addition, the\nslope and intercept of this regression line were much closer to 1 and 0, respectively, than any of\nthe former methods employed. This suggests that while there was more total error in the\nprediction, separating the data using classification prior to performing regression allowed the\nmethods to better capture the trends that existed in the data, even if only slightly. Figure B.2 in\nAppendix B contains a scatterplot of actual versus predicted, which verifies this analysis.\nBest\nBest LSE on\ntest set\n\nSlope\nIntercept\n-0.05\n10.0157\n0.0934\n0.777\n-0.00145\n\nTable 3--Average returns of strategy-based classification methods on the test set\nTable 4-- Results of Random Forest regression derived from strategy-based classification when\npredicting ReturnPost\n\nV. Conclusions and Further Research\nThe overall objective of creating a potentially profitable machine-learning based trading\nstrategy to capture information in earnings reports was accomplished throughout the course of\nthis study. Although regression techniques proved to be unsuccessful in predicting the difference\nbetween open and close prices during the trading session following earnings releases, strategy-\nbased classification was able to find some underlying patterns that generalized to out-of-sample\ntest data. The success of these methods imply that very often a high push in momentum the day\nfollowing earnings reports can be predicted before the trading session begins. However, these\nconclusions do come with some reservations. Although the prediction algorithms were trained\non data that was completely separate from the test data, they were all drawn at random. Thus,\nsome data points in the testing set may have temporally occurred before data points in the\ntraining set. It would be useful to see how training the algorithm on all data before a given time\nwould perform when predicting later observations. It is expected that this type of study should\nnot drastically change the results because the data used were not true time series data. However,\nto be safe, the strategies should be tested both analytically as well as in a paper trading account\nbefore being implemented. Also, it is not immediately clear how much transactions costs would\ndecrease the mean returns of each strategy--another item to be investigated via paper trading.\nFuture research into this area may provide interesting new statistical insights. The most\nnatural extensions of the work done in this study would involve using strategy-based\nclassification to create analogous short-selling strategies, as well broadening the dataset beyond\nthe Capital Goods sector. In addition, the prediction ability of the classifier designed to aid a\nstraddle option strategy seemed to be very strong, so analysis into option prices on the day\nfollowing earnings reports could potentially be very useful. The strategy-based classification\n\nalso provided a promising new outlook for how to perform regression on noisy data, which could\nbe eventually refined and generalized to other data sets. Outside of regression, this study\nfocused on how to successfully optimize threshold values for strategy-based classification rather\nthan comparing the utility of different learning algorithms. Thus, a future project that extends\nthe Random Forest techniques used here to other classifiers could provide more insight into the\ndata. In light of the success of strategy-based classification, as well as the prospects for further\nresearch in this area, a bright future exists for the use of machine learning to analyze financial\nearnings.\n\nAppendix A--Dataset Attributes\n\nPrices in dollars and EPS values are quarterly.\nAttribute\nID\nAttribute\nDescription\nPossible Values\nTicker\nOfficial Ticker for the observed stock\nText, ~5,000\nanalyzed\nSector\nStock sector of the observation\nText, 13 possible\nsectors\nIndustry\nStock industry of the observation\nText, over 200\npossibilities\nMonth\nMonth of earnings release\n1-12\nActual\nActual EPS for the observation quarter\nReal values\nMeanEstimate\nConsensus mean EPS estimate for the\nobservation quarter\nReal values\nBeforeMkt\nTRUE if the earnings release occurred\nbefore the market opened, FALSE if after\nthe market closed\nBoolean TRUE or\nFALSE\nClosePrior\nClosing price the trading session\nimmediately before the earnings release\nPositive real values\nClose10\nMean closing price of the 10 days\nimmediately before the earnings release\nPositive real values\nClose60\nMean closing price of the 60 days\nimmediately before the earnings release\nPositive real values\nMeanIntra10\nArithmetic mean percent return if stock\nwas purchased on open and sold on close\nfor the 10 days immediately before the\nearnings release\nReal values\nMeanIntra60\nArithmetic mean percent return if stock\nwas purchased on open and sold on close\nfor the 60 days immediately before the\nearnings release\nReal values\nSDIntra10\nSample standard deviation of the percent\nreturn if stock was purchased on open and\nsold on close for the 10 days immediately\nbefore the earnings release\nPositive real values\nSDIntra60\nSample standard deviation of the percent\nreturn if stock was purchased on open and\nsold on close for the 60 days immediately\nbefore the earnings release\n\nPositive real values\n\nVol10\nMean number of shares traded per day for\nthe 10 days immediately before the\nearnings release\nPositive real values\nVol60\nMean number of shares traded per day for\nthe 60 days immediately before the\nearnings release\nPositive real values\nSDVol10\nStandard deviation of the number of\nshares traded per day for the 10 days\nimmediately before the earnings release\nPositive real values\nSDVol60\nStandard deviation of the number of\nshares traded per day for the 60 days\nimmediately before the earnings release\nPositive real values\nMarketCap\nMarket capitalization of the stock the\nsession immediately before the earnings\nrelease\nPositive real values\nNormProfit\n\nReal values\nNormSurprise\nReal values\nReturnPost\nRealized return if stock was purchased on\nopen and sold on close the day following\nthe release\nReal values\nMinReturnPost\nRealized return if stock was purchased on\nopen and sold at the low value the day\nfollowing the release\nNon-positive real\nvalues\nMaxReturnPost Realized return if stock was purchased on\nopen and sold at the high value the day\nfollowing the release\nNon-negative real\nvalues\n\nAppendix B--Actual vs. Predicted Figures\n\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n-0.2\n-0.1\n0.0\n0.1\n0.2\n0.3\nReturnPost Actual vs. Predicted\nPredicted\nActual\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n-0.3\n-0.2\n-0.1\n0.0\n0.1\n0.2\n0.3\n0.4\nReturnPost Actual vs. Predicted\nPredicted\nActual\nFigure B.1--Actual vs. predicted\nscatterplot for additive\nnonparametric regression.\nFigure B.2--Actual vs. predicted\nscatterplot for Random Forest\nregression derived from strategy-\nbased classification.\n\nReferences\n[1] Breiman, Leo, et. al. \"Breiman and Cutler's Random Forests for Classification and\nRegression.\" CRAN R Project. 15 February 2012. <http://cran.r-\nproject.org/web/packages/randomForest/randomForest.pdf>.\n[2] Breiman, Leo. \"Random Forests.\" Machine Learning 45 (2001): 5-32.\n[3] Fox, John. \"Nonparametric Regression.\" CRAN R Project. January 2002. <http://cran.r-\nproject.org/doc/contrib/Fox-Companion/appendix-nonparametric-regression.pdf>.\n[4] Hecht-Nielsen, Robert. \"Theory of the Backpropagation Neural Network.\" International\nJoint Conference on Neural Networks, 1989. vol. 1: 593-605.\n[5] \"Investopedia Dictionary.\" Investopedia.com. 13 November 2007. Available from\n<http://www.investopedia.com/dictionary/#axzz1v3szuOmA>.\n[6] Matsumoto, Dawn A. \"Management's Incentives to Avoid Negative Earnings Surprises.\"\nThe Accounting Review 77.3 (2002): 483-514.\n[7] Ripley, Brian. \"Feed-Forward Neural Networks and Multinomial Log-Linear Models.\"\nCRAN R Project. 14 February 2012. < http://cran.r-\nproject.org/web/packages/nnet/nnet.pdf>.\n[8] Therneau, Terry M. and Beth Atkinson. \"Recursive Partitioning.\" CRAN R Project. 4\nMarch 2012. < http://cran.r-project.org/web/packages/rpart/rpart.pdf>.\n[9] Wood, Simon. \"Mixed GAM Computation Vehicle with GCV/AIC/REML Smoothness\nEstimation.\" CRAN R Project. 30 April 2012. < http://cran.r-\nproject.org/web/packages/mgcv/mgcv.pdf>.\n[10] \"Wharton Research Data Services.\" Wharton--University of Pennsylvania. Available from\n<https://wrds-web.wharton.upenn.edu/wrds/>.\n\n[11] Wu, Xindong and Vipin Kumar, eds. The Top Ten Algorithms in Data Mining. Boca\nRaton: Chapman and Hall/CRC Press. 2009.\n\nNote: Wharton Research Data Services (WRDS) was used in preparing this paper. This service\nand the data available thereon constitute valuable intellectual property and trade secrets\nof WRDS and/or its third-party suppliers.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n15.097 Prediction: Machine Learning and Statistics\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    }
  ]
}