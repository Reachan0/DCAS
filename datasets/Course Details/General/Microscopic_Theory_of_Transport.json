{
  "course_name": "Microscopic Theory of Transport",
  "course_description": "Transport is among the most fundamental and widely studied phenomena in science and engineering. This subject will lay out the essential concepts and current understanding, with emphasis on the molecular view, that cut across all disciplinary boundaries. (Suitable for all students in research.)\n\nBroad perspectives of transport phenomena\nFrom theory and models to computations and simulations\nMicro/macro coupling\nCurrent research insights",
  "topics": [
    "Engineering",
    "Chemical Engineering",
    "Transport Processes",
    "Science",
    "Chemistry",
    "Physical Chemistry",
    "Engineering",
    "Chemical Engineering",
    "Transport Processes",
    "Science",
    "Chemistry",
    "Physical Chemistry"
  ],
  "syllabus_content": "WEEK #\n\nTOPICS\n\nOverview\n\nTransport Coefficients - Green Kubo Relations\n\nDensity and Self Correlations, Botlzmann Equation\n\nNavier-Stokes Equations\n\nCollisions and Transport Models\n\nFluid Transport - Molecular Dynamics Simulations\n\nRadiation Transport - Monte Carlo Methods\n\nLinear Response Theory\n\nMicro/Macro Coupling (Multiscale Modeling)\n\nSolids and Soft Matter\n\n11-14\n\nSpecial Topics to Illustrate Diverse Applications - Subject to Class Interest\n\nClass Presentation\n\nThe course will be taught through class lectures. Written\nlecture notes\nwill be posted on the server.\n\nThere will be a few problem sets, a written quiz (after week 8), and at the end of the term a\nterm project\n(with presentation) and an oral exam.\n\nGeneral References\n\nBoon, J. P., and S. Yip.\nMolecular Hydrodynamics.\nMcGraw-Hill, 1980, Dover edition, 1990. ISBN: 0486669491.\n\nMcQuarrie, D. A.\nStatistical Mechanics.\nHarper & Row, 1976. ISBN: 1891389157.\n\nDuderstadt, J. J., and W. R. Martin.\nTransport Theory.\nWiley, 1979. 047104492X\n\nBird, R. B., W. E. Stewart, and E. N. Lightfoot.\nTransport Phenomena\n. Wiley, 1960. ISBN: 0470115394.",
  "files": [
    {
      "category": "Resource",
      "title": "hw1.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/22-103-microscopic-theory-of-transport-fall-2003/67709f59f54dd0ba9b5adb32d74a6ff8_hw1.pdf",
      "content": "22.103 Microscopic Theory of Transport\n(Fall 2003)\nProblem Set No. 1\n\nDue: September 19, 2003\n\nProblem 1\n\n(a) Discuss the physical meaning of the diffusion coefficient D through the\nphenomenological expression, Fick's law, where you carefully define the quantities that\nappear in this expression. Extend your discussion to the other two transport coefficients,\nthe shear viscosity η and thermal conductivity κ , using expressions analogous to the\nFick's law.\n\n(b) For a simple fluid, discuss briefly whether you expect the three transport coefficients\nto show the same dependence on density (pressure) and temperature. What physical\nprocesses are responsible for the variation of these transport coefficients with density and\ntempera? How would one go about calculating (predicting) these variations, in other\nwords, what theoretical approach is available?\n\nProblem 2\n\nDerive the mean squared displacement function\n2 ( )\nr t\n< ∆\n> for an ideal gas of particles\nwith mass M at temperature T.\n\nProblem 3\n\nSketch the behavior of\nfor an ideal gas, a liquid, and a solid, and discuss the\nqualitative features of this function for the three states of matter (differences and\nsimilarities). Based on these features, what can you infer about the atomic motions in\nthese three idealized models of physical systems?\n2 ( )\nr t\n< ∆\n>\n\nProblem 4\n\nDerive the Green-Kubo expression for the diffusion coefficient D in terms of the\nnormalized velocity autocorrelation function\n( )t\nψ\n. Repeat Problem 3 for\n( )t\nψ\ninstead of\n.\n2 ( )\nr t\n< ∆\n>"
    },
    {
      "category": "Resource",
      "title": "hw2.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/22-103-microscopic-theory-of-transport-fall-2003/5aa5114bddab73664c633443c79b6ea4_hw2.pdf",
      "content": "22.103 Microscopic Theory of Transport\n(Fall 2003)\nProblem Set No. 2\n\nDue: September 29, 2003\n\nRead Boon-Yip, Chap 1 and selected parts of Chaps 2, 3, and 4 (those sections\ndealing with the velocity autocorrelation function and the Van Hove self correlation\nfunction)\n\nProblem 1\n\nEvaluate the following thermal averages for a simple fluid,\n\n( , )\nn r t\n<\n> ,\n( , )\nsn r t\n<\n> ,\n(0)\nV\n<\n>\n\n( ,0)\ns\nG r\n,\n( ,0)\nG r\n,\n( , ,0)\nsf r v\n\nProblem 2\n\nCalculate\n( , )\ns\nG r t for the following models,\n(1) An ideal gas by direct evaluation using its definition\n(2) A fluid by direct solution of the time-dependent diffusion equation\n(3) An oscillator\n\nProblem 3\n\nDiscuss the connection between the mean square displacement function, the velocity\nautocorrelation function, the Van Hove self correlation function, and the corresponding\nphase-space distribution function."
    },
    {
      "category": "Resource",
      "title": "hw3.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/22-103-microscopic-theory-of-transport-fall-2003/d87a34594239e45a57bbab318762c3e2_hw3.pdf",
      "content": "22.103 Microscopic Theory of Transport\n(Fall 2003)\nProblem Set No. 3\n\nDue: October 14, 2003\n\nProblem 1\n\nDerive the three equations of continuum mechanics, the continuity equation for the mass\ndensity\n( , )\nr t\nρ\n, the momentum equation for mass current density\n( , )\n( , ) ( , )\nM\nj\nr t\nr t v r t\nρ\n=\n,\nand the energy equation for the temperature density ( , )\nT r t . List and discuss briefly the\nassumptions invoked in deriving these results, in particular, point out how the transport\ncoefficients, viscosity and conductivity, enter into the description.\n\nProblem 2\n\nObtain the set of linearized hydrodynamics equations from the results of Problem 1.\nTransform the dependent variables from ( , , )\nv t\nρ\nto\n( , , ,\n,\n)\ns p w μ μ\n, where s is the entropy\ndensity, p the pressure,\nthe longitudinal current, and\nw\n,\nμ μ the transverse currents.\n\nProblem 3\n\nUse the linearized hydrodynamics equations of Problem 2 to calculate the dynamic\nstructure factor ( ,\n)\nS k ω . Show your result can be put into the form Eq.(5.3.15) in Boon-\nYip.\n\nProblem 4\n\nDiscuss the line shape of the dynamic structure factor by giving physical meaning to the\npeaks in the spectrum, and discussing what physical quantities are associated with the\nheights, positions and widths of the peaks."
    },
    {
      "category": "Resource",
      "title": "hw4.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/22-103-microscopic-theory-of-transport-fall-2003/9bfe74ab58f49fc62daa20bc8d839514_hw4.pdf",
      "content": "22.103 Microscopic Theory of Transport\n(Fall 2003)\nProblem Set No. 4\n\nDue: October 24, 2003\n\nProblem 1\n\n(1) Derive the streaming term in the Boltzmann equation using the control volume\napproach where you take into account the difference between the number of particles\nflowing in and flowing out of the control volume.\n\n(2) Derive the collisional terms in the Boltzmann equation, and by combining with your\nresult for part (1) obtain the Boltzmann equation. Define all the quantities in this\nequation and briefly give physical meaning to each whenever possible.\n\n(3) What are the basic limitations of the Boltzmann equation as a general description of\ntransport phenomena in matter?\n\nProblem 2\n\nConsider a two-body collision where the interaction is a central force potential.\n\n(1) Show that the two-body problem can be reduced to an effective one-body problem\ninvolving a particle with an effective mass and moving with the relative velocity by\ntransforming the Newton's equations of motion for the two particles into an equation for\nthe center-of-mass and an equation for the relative position between the two particles.\n\n(2) Derive the relation between the angular differential cross section for the scattering\nand the impact parameter and the angle of scattering.\n\nProblem 3\n\nDiscuss the meaning of collisional (or summational) invariants in the context of kinetic\ntheory of gases. Derive the collisional invariants for the collision operator in the\nBotlzmann equation and discuss their significance."
    },
    {
      "category": "Resource",
      "title": "hw5.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/22-103-microscopic-theory-of-transport-fall-2003/4fc6ba44f140b31b5503e0d783d031c0_hw5.pdf",
      "content": "22.103 Microscopic Theory of Transport\n(Fall 2003)\nProblem Set No. 5\n\nDue: October Nov. 7, 2003\n\nProblem 1\n\nSummarize the discussion of the H-theorem and the equilibrium solution to the\nBoltzmann equation given in Chap 18 of McQuarrie (and repeating enough of the\nderivational steps for yourself). Itemize the main results of Sec. 18-5 and their\nimplications as part of your study notes on understanding the Btolzmann equation.\n\nProblem 2\n\nSummarize the paper, M. Nelkin and A. Ghatak, \"Simple Binary Collision Model for Van\nHove's Gs(r,t)\", Phys. Rev. 135, A4 (1964), so far as the calculation of\n( ,\n)\nsS k ω using the\nBoltzmann equation is concerned. Follow the details to derive Eq.(28) and then obtain\nthe two limits, the ideal gas and simple diffusion. Relative to these two limiting\nbehavior, what is the significance of the kinetic model description of Gs(r,t)?\n\nProblem 3\n\nRelative to the paper of Nelkin and Ghatak, what is the significance of the paper, S. Yip\nand M. Nelkin, \"Application of a Kinetic Model to Time-Dependent Density Correlation\nin Fluids\", Phys. Rev. 135, A1241 (1964) from the standpoint of calculating density\ncorrelation functions using kinetic theory?"
    },
    {
      "category": "Exam",
      "title": "quiz.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/22-103-microscopic-theory-of-transport-fall-2003/31ff3300561368fef0b5c9401d239343_quiz.pdf",
      "content": "22.103 Microscopic Theory of Transport\n(Fall 2003)\nQUIZ (closed book)\n\nNov. 14, 2003\n\nQuestion 1 (60% total - 12% each)\n\nAnswer briefly each of the following (answer all parts in the order asked, stick to the\nessentials and do not write more than is necessary)\n\na. What is the Green-Kubo relation in the case of diffusion? What assumptions are\ninvolved, if any? Is this relation valid for all physical systems? In what way is it useful?\n.\nb. What do we know about the dynamic structure factor in the hydrodynamic limit?\nGive a sketch of the spectrum and discuss the characteristic features?\n\nc. What is the equation governing the phase-space density correlation function for a\nlow-density gas? What is meant by collisional invariants, and what are the invariants\nassociated with this equation?\n\nd. What is the approximation made to the linearized Boltzmann equation that gives the\nNelkin-Ghatak kinetic model? Discuss the physical meaning of this approximation.\n\ne. What is the difference between the Boltzmann equation as derived in class and the\nneutron transport equation? What is the closest analogue to the neutron transport\nequation that one can obtain from the conventional Boltzmann equation in the study of\nrarefied gas dynamics?\n\nQuestion 2 (20%)\n\nThe Nelkin-Ghatak kinetic model can be used to describe the velocity autocorrelation\nfunction of a gas. Define this function (which is not normalized) as\n\n( )\n'\n' ( , ', )\nt\nd vd v v v f v v t\nψ\n=\n⋅\n∫\n\nwhere\n( , ', )\nf v v t is the distribution function satisfying the Nelkin-Ghatak model,\n\n( , ', )\n( )\n\" ( \", ', )\nM\nf v v t\nf\nv\nd v f v v t\nt\nα\nα\n∂\n⎛\n⎞\n+\n=\n⎜\n⎟\n∂\n⎝\n⎠\n∫\n\n(1)\n\nwhere α is the collision frequency, and\n( )\nM\nf\nv is the Maxwellian velocity distribution.\nNotice that equation (1) is an equation in the variable v , but not the variable 'v Use\nequation (1) to calculate\n( )t\nψ\nsubject to the initial condition,\n( , ',0)\n(\n')\n( )\nM\nf v v\nv\nv f\nv\nδ\n=\n-\n.\nDiscuss whether your result is surprising considering what we have discussed in class\nabout the nature of the approximation in the Nelkin-Ghatak model.\n\nQuestion 3 (20%)\n\nYou are given the velocity autocorrelation function as\nexp(\n/ )\nov\nt τ\n-\n, where\nis a\nthermal speed, and\nov\nτ is a relaxation time (a constant). Use this result to determine the\ncorresponding mean square displacement\n2 ( )\nr t\n< ∆\n> . Inspect your result for short and\nlong times, and discuss whether these are the proper limiting behavior for a fluid."
    },
    {
      "category": "Lecture Notes",
      "title": "lec1.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/22-103-microscopic-theory-of-transport-fall-2003/9888b4e4adf1fe6114c82925f6a270f0_lec1.pdf",
      "content": "22.103 Microscopic Theory of Transport (Fall 2003)\nLecture 1 (9/8/03)\n\nCourse Overview and Intro\n_______________________________________________________________________\n\nReferences --\nS. Yip, \"Synergistic Science\", Nature Materials 2, 3 (2003). A pdf file of this\ncommentary is available to anyone in the class.\n________________________________________________________________________\n\nGoal of this course: To collect and deliver, in an understandable way, a set of ideas\n(concepts and models) along with an introduction to the 'implementation tools'\n(simulation techniques) for applying these ideas to current research problems.\n\nThere are several ways to introduce 22.103. We begin by elaborating on the course\nannouncement that was sent to all students:\n\nTransport is among the most fundamental and widely studied phenomena in\nscience and engineering. This subject will lay out the essential concepts and current\nunderstanding, with emphasis on the molecular view, that cut across all disciplinary\nboundaries. (Suitable for all students in research.)\n\no broad perspectives of transport phenomena\n\no from theory and models to computations and simulations\n\no micro/macro coupling\n\no current research insights\n\nWhile it is not stated explicitly in the announcement, there is actually a challenge offered\nto the students taking this course. 'Take It In' in your own way. We imagine two\npossible ways of learning from this course, (A) gain a general appreciation of transport\ntheory and simulations, and (B) dig in as hard as you can. We expect most will follow\n(A), but hopefully there will be one or two doing (B).\n\nHow general is transport? Transport phenomena are pervasive in many areas of studies\nand research relevant to NED, although there is no single course where transport is\ndiscussed from the general perspective that the topic deserves.\n\nIt is easy to conjure up transport phenomena familiar to our everyday experience. In\nweather prediction one has transport of a body of moisture on length scales of tens to\nhundreds of miles. The local and global conditions of temperature and pressure clearly\nhave important effects on the phenomenon. In blood flow the relevant length scale is\nmillimeters to centimeters. Here the fluid viscosity is important as are the fluid-vessel\nsurface interactions and the dilatation or constriction of the opening. What is common\nhere? In both cases the fluid in motion can be described in terms of a transport equation\nthat takes into account the relevant physical processes affecting the flow.\n\nThink of the distributions of particles (or radiation) or a fluid element in space, energy,\ndirection of propagation, and time. Many research problems involve the study of these\ndistributions, which reflect the transport processes taking place in the system under study.\nThe problems familiar to us in the fission area of the department are the neutron transport\nequation in various forms of approximation, or the Navier-Stokes equations under special\nconditions of flow or heat transfer. In RST the students usually do not work with the\nneutron transport equation, instead many use MCNP to calculate the neutron or gamma\ndistributions in radiation therapy problems. In the fusion area, the students are interested\nin plasma transport, which can be discussed in terms of the Vlasov equation and the\nMaxwell's equations. There are connections between charged particle transport and the\nproblems of interest to the fission and RST students. We will look for opportunities to\nbring out such connections during the course.\n\nAlthough transport problems occur throughout NED we have no classes where one looks\nat transport phenomena from a more unified point of view. For example, is there a class\nwhere neutron and fluid transport are treated on equal footing? Yet, we believe that a\nbroad appreciation of transport is part of the fundamental knowledge all students should\nhave. The basis of this belief lies in the importance of transport in much of what we do\nand that the basic concepts will endure no matter how the nuclear technologies may\nevolve in the future. In 22.103 we set out to provide this understanding and appreciation.\nThis is basically what we mean by the first bullet above.\n\nHow do we study transport? There are fundamental concepts and models which form\nthe basis of description of transport. We will focus on them in the first part (7 or 8\nweeks) of the course. We will begin with a discussion of transport coefficients and how\nthey are related to correlation functions which are basic concepts in statistical mechanics.\nThen we show that time correlation functions can be calculated from either kinetic\n(transport) equations or continuum equations, leading to the distinction of micro versus\nmacro transport. This discussion makes it clear that transport phenomena belong to a\nbroad class of dynamical processes studied in non-equilibrium statistical mechanics.\nBesides considering transport at several levels (micro vs. macro), one can also distinguish\nbetween initial-value and boundary value problems. The flow of topics goes something\nlike the following for the first part of the course.\n\nD\nGreen-Kubo\nG, Gs\nBoltzmann/Navier Stokes\nLinear → multiscale\n→\n→\n→\n→\nrelations time correlation micro/macro\nresponse modeling\n\nIn addition to the different formulations and models of transport, we will introduce the\ncomputational techniques that can be used to address the different kinds of transport\nproblems. We will concentrate on the particle simulation methods of molecular\ndynamics and Monte Carlo. While we will not go into the technical details of these\nsimulations, we will discuss the reasons why these methods can be the most powerful\nway to study transport phenomena. We will see that the complications of many-body\ncollisions are such that empirical models and approximate theories are simply inadequate\nto deal with the relevant degrees of freedom of the problem. In these situations\n\nsimulations are the only way to proceed. This is what we mean by the second bullet\nabove. See the Commentary by SY in Nature Materials mentioned above. A lot more\nwill be said throughout the course about the capabilities of modeling and simulation.\n\nMicro-macro coupling in transport. When one performs calculations using the\nmacroscopeic description of transport, such as the Navier-Stokes equations in fluid\ndynamics, the constants in the equations such as the transport coefficients have to be\nspecified externally. This can come from either experimental data or theoretical\ncalculations using a microscopic description, such as the Boltzmann kinetic equation for\na particular intermolecular interaction. The transport coefficients therefore play the role\nof linking the micro (Boltzmann) and macro (Navier-Stokes) descriptions of transport.\nHow this comes about is an interesting issue, having to do with reducing the degrees of\nfreedom of the description. At the micro level we have a distribution function in\nconfiguration space and velocity (and also time which is present at both levels), while at\nthe macro level we have only densities that vary in configuration space, with the densities\nthemselves being the velocity moments of the distribution function. This reduction in\ndescription is appropriate when the phenomena of interest vary slowly in space (and\ntime), but it can also break down for problems where there are strong spatial gradients.\nThus the conditions under which one can get away with using the simpler macro\ndescription need to be understood. This is what we mean by the third bullet.\n\nHow do we make the course useful to all students? We offer the course in two modes\n- (A) is for students to gain a general appreciation of transport without going into all the\ntechnical details, and (B) is for students who want an in-depth study. Both will get the\nsame lectures and the written quiz, the difference will be in the HW and the term project.\n(B) will not necessarily get a higher grade, so any extra work is for self-satisfaction and\nthe benefit of learning as much as you can from the course. In either case we anticipate\nall the students in their research, sooner or later, will encounter problems where the\nphysical insight derived from the applications discussed in class can prove to be helpful.\nWe feel this is likely because we plan to examine a range of applications with the\nintention of bringing out general implications. These applications should serve as good\nillustrations of what one can do with the concepts and simulation methods discussed in\nclass. This is what we mean by the fourth bullet above.\n\nWe end this intro with the following course description which appears in the catalog.\nDescription of particle and field distributions in terms of intermolecular collisions and\natomic-level transport. Discusses the relation between microscopic formulations such as\nthe Boltzmann equation with corresponding coarse-grained descriptions in statistical\nmechanics, fluid dynamics and neutronics. Examines the common basis of assumptions\nand approximations applied in different areas of Nuclear Engineering. Introduction to\ncontemporary discrete-particle simulation methods for model system studies where such\ncomputational approaches have unique advantages.\n\nThis is yet another way of saying what we plan to accomplish in this class. All questions\nand suggestions at any time are appreciated."
    },
    {
      "category": "Lecture Notes",
      "title": "lec2.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/22-103-microscopic-theory-of-transport-fall-2003/5d00042019ee7016e625ea28b70fcfc3_lec2.pdf",
      "content": "22.103 Microscopic Theory of Transport (Fall 2003)\nLecture 2 (9/10/03)\n\nDiffusion and the Mean Square Displacement\n_______________________________________________________________________\n\nReferences --\nBoon and Yip, Chap.2.\nMcQuarrie, Chap. 16.\nBird, Stewart, Lightfoot, Chap 16.\n________________________________________________________________________\n\nWe begin with the simplest transport process, that of mass or particle diffusion.\nThe basic quantity in this discussion is the self-diffusion coefficient D which is a material\nconstant usually introduced in a phenomenological expression called the Fick's law,\n\nJ\nD n\n= -\n∇\n\n(2.1)\n\nwhere J is the particle current, and n is the particle density (or concentration). In other\nwords, D is the proportionality constant between a current (or flux) and a gradient of a\ndensity. A similar relation may be written between the momentum flux and the velocity\ngradient, in which case the constant is the viscosity η (sometimes the symbol μ is used),\nor between the heat current and the temperature gradient, in which case the constant is the\nthermal conductivity\n(sometimes denoted as\nκ\nλ ).\n\nFick's law is useful because by invoking it we convert the continuity equation into a time-\ndependent diffusion equation. This is fine if what we want to do is solve the diffusion\nequation subject to an initial condition and appropriate boundary condition (we will do\nthis later). On the other hand, (2.1) tells us nothing about what is D (how to find it), how\ndoes D depend on the motions of the molecules that are undergoing diffusion, or how\ndoes D vary with the state of the system.\n\nConsider a simple fluid (only one type of molecule) in equilibrium at some pressure P\nand temperature T. One can ascribe to this system a diffusion coefficient D, which is a\nfunction of P and T. Usually, P is replaced by the fluid number density n or the mass\ndensity ρ . For now we will not be concerned about the density and temperature\ndependence of D; it should be understood that later D really means\n( , )\nD\nT\nρ\n.\n\nFor now we focus on the relation between D and the atomic motions of the particles in\nthe fluid. If we know D, what information does that give about the motions, and vice\nversa? Another question is how can we calculate D, i.e., what expression can we use and\nwhat information about the fluid do we need for such a calculation? Asking such\nquestions means that we are starting to think about diffusion as a transport phenomenon\ntaking place on the molecular level. Suppose we imagine we can watch all the molecules\nmoving around, and we have the ability, in principle, of following the trajectory of any\nsingle molecule that we pick. Since all the molecules are identical, the trajectory of any\n\nzzmolecule, statistically, has the same importance as any other molecule. It then follows\nthat all the molecules must contribute equally to the estimation of the self-diffusion\ncoefficient. (In a one-component fluid the diffusion process is called self-diffusion, in\ncontrast to mutual diffusion in a multicomponent system. In the latter one keeps of the\nparticle species as diffusion takes place.)\n\nThere are two ways of defining the diffusion coefficient D which are more useful than\n(2.1) for understanding diffusion on a molecular level. The goal of this and the next\nlecture is to introduce them by explaining how they come about. We first define the\nmean square displacement function,\n\n( )\n[\n( )\n(0)]\nN\ni\ni\ni\nr t\nR t\nR\nN\n=\n< ∆\n>=\n<\n-\n>\n∑\n\n(2.2)\n\nwhere\n( )\ni\nR t is the position of molecule i in the fluid, which we take to be a finite system\nof volume V containing N identical molecules, see Fig. 2.1. The angular brackets < > in\n\nFig. 2.1. A system of N particles and volume V. The instantaneous (at time t) position\nand velocity of the ith particle are denoted as\n( )\niR t and velocity\n( )\ni\nV\nt .\n\n(2.2) indicate an average over initial (at time t = 0) positions and velocities of the\nmolecules. Notice that the quantity inside the angular brackets is the square of the\ndisplacement that molecule has undergone during a time interval t. Summing over all the\nmolecules and dividing by the number of molecules gives the mean value, so (2.2) is the\nmean square displacement of the molecules in the fluid over a period of time t. The\nangular brackets in (2.2) denotes what is known as a thermal or ensemble average. It\nmeans that if we were doing an actual measurement of the mean square displacement by\nstarting the molecules off in an initial distribution of positions of the molecules, with\neach having some initial velocity (according to the temperature of the fluid), we should\nrepeat the measurement, each time choosing for initial positions and velocities of the\nparticles a distribution drawn from an ensemble of distributions. After repeating a\nnumber of these measurements we then average the results. In each measurement, the\ntime interval over which we let molecule i wander around in the fluid is the same, and\nwhat we record is the square of the vector displacement of molecule i from its initial\nposition\n(0)\ni\nR\n(when we start over again with a different initial distribution,\n(0)\ni\nR\nwill be\ndifferent from the preceding run).\n\nTo make clear what is involved in the thermal average, we take any dynamical variable\nA(t), which is a function of the particle positions at time t, and define\n\n( )\n(0)\n(0)\n(\n(0),\n(0)) (\n( ))\nN\nN\nN\nN\nN\neq\nA t\nd R\nd V\nf\nR\nV\nA R\nt\n<\n>= ∫\n\n(2.3)\n\nIn (2.3)\n( )\nN\nR\nt stands for a collection of postions,\n( ),\n( ),...,\n( )\nN\nR t R\nt\nR\nt , all at time t. The\nintegral, however, is taken over the initial positions and velocities,\n(0)\nN\nR\nand\n(0)\nN\nV\n, with\neq\nf being an appropriate ensemble distribution. Typically we will take\neq\nf to be the\ncanonical distribution in thermodynamics, then\n\n(\n)/\n(\n,\n)\n(\nN\nB\nU R\nk T\nN\nN\nN\neq\nM\ne\n)\nf\nR\nV\nf\nV\nZ\n-\n=\n\n(2.4)\n\nwhere U is the system potential energy, kB the Boltzmann's constant, T the system\ntemperature, and Z is the partition function,\n\n(\n)/\nN\nB\nU R\nk T\nN\nZ\nd R e-\n= ∫\n\n(2.5)\n\nMoreover, in (2.4) fM is the normalized Maxwell distribution of velocities. Except for the\nvelocity distribution we will manipulate\neq\nf only formally.\n\nSince the thermal average is involved in the mean square displacement function, the\nquantity of our interest in this lecture, it is instructive to see how one can perform the\nindicated integration. To do the integral in (2.3) means we need to know what is the\nrelation between the particle positions and velocities at the initial time t = 0 and their\npositions at time t later. This relation can be very simple, as in the case of an ideal gas\nwhere all particles move in straight lines,\n\n( )\n(0)\n(0)\ni\ni\ni\nR t\nR\nV\nt\n=\n+\n\n(2.4)\n\nOr it can be very complicated, as in the case of a liquid where one can only write it down\nformally (which is not that useful for seeing what is going on). Let us now work out the\nmean square displacement function for the ideal gas. Inserting (2.4) into (2.2) we get\n\n( )\n(0)\n(0)\nr t\nV\nV\nt\n< ∆\n>=<\n⋅\n>\n\n(2.5)\n2 2\n3 ov t\n=\n\nwith\n,\nis known as the thermal speed. This is a simple and exact result\nwhich says that the mean square displacement of a gas in which there are no collisions\ngrows quadratically in time. Since any collision will have the effect of slowing down the\n/\no\nB\nv\nk T M\n=\nov\n\nmolecules, we can expect that when collision effects are taken into account\n\nwill not grow as quickly at t2.\n2 ( )\nr t\n< ∆\n>\n\nIt is instructive to compare the behavior of\n2 ( )\nr t\n< ∆\n> for three idealized systems, one of\nthem being the ideal gas we have just discussed. The other two are a simple liquid and a\nsolid. These three systems represent the three states of matter that we usually encounter,\nso it is important to see how\nvaries in each of these idealized systems.\nKnowing the behavior in these models can help us understand what kinds of atomic\nmotions take place in a particular physical situation. We will defer the derivation of\nfor a liquid and a solid to a later occasion. For now, we show in Fig.2.2 the\ncharacteristic behavior of the three systems.\n2 ( )\nr t\n< ∆\n>\n2 ( )\nr t\n< ∆\n>\n\nOne can see that at short times\nincreases like t2 for all three systems. This can\nbe seen by the following argument. For any system we can make a Taylor series\nexpansion of the displacement of a particle by writing\n2 ( )\nr t\n< ∆\n>\n\n( )\n(0)\n(0)\n(0)\n( )\nR t\nR\nR\nt\nR\nt\nO t\n=\n+\n+\n+\n\n(2.6)\n\nInserting this into (2.2) we get a time expansion of\n2 ( )\nr t\n< ∆\n> ,\n\n(2.7)\n2 2\n( )\n( )\no\nr t\nv t\nO t\n< ∆\n>=\n+\n\nThis says that for any system,\nbehaves initially like an ideal gas up to the\nsecond order term in t. We can understand this as just the inertial part of the motion of\nany physical system. We see in Fig. 2 that the curves for the liquid and the solid rather\nquickly deviate from the t2 behavior once the dynamics of the system set in. One way to\nthink about the dynamics is through particle collisions with their neighbors, this is quite\nintuitive for a liquid environment. For the solid, one can think about restoring forces on\neach atom provided by the crystal binding. This is what holds each atom in its\nequilibrium position in the crystal lattice. In a simplified way we can say that atomic\nmotions in a liquid is like Brownian motion, a particle undergoes continuous collisions\nwith its neighbors as it diffuses through the liquid. On the other hand, a particle in the\nsolid cannot diffuse very far from its lattice site, its motions are vibratory rather than\ndiffusive. This difference is seen in Fig. 2. At long times, the diffusive behavior in the\nliquid gives rise to a linear time dependence in\n2 ( )\nr t\n< ∆\n>\n2 ( )\nr t\n< ∆\n> . In contrast,\nreaches\na plateau value because it cannot grow indefinitely, no matter how long one waits.\n2 ( )\nr t\n< ∆\n>\n\nThe connection between\nand the conventional picture of diffusion lies in the\nbehavior of\nat long times. Fig. 2.2 shows that for times long compared to\nsome typical collision or correlation time tc (we will come back to make this more\nprecisely later),\nincreases linearly with time for a liquid. This linear time\n2 ( )\nr t\n< ∆\n>\n2 ( )\nr t\n< ∆\n>\n2 ( )\nr t\n< ∆\n>\n\ndependence is what is characteristic of diffusive motion. In this case the slope of\nis proportional to the diffusion coefficient D,\n2 ( )\nr t\n< ∆\n>\n\nt >> tc\n\n(2.8)\n2 ( ) ~ 6\nr t\nDt\n< ∆\n>\n\nSince\ngoes to a plateau value at long times in a solid, D is effectively zero for a\nsolid according to the definition,\n2 ( )\nr t\n< ∆\n>\n\n( )\nc\nt\nt\nD\nr t\nt\n>>\n⎡\n⎤\n=\n∆\n⎢\n⎥\n⎣\n⎦\n\n(2.9)\n\nEq.(2.9) also shows that for an ideal gas D would have infinite diffusion coefficient. In\nreality, we know that there is diffusion in the solid, although D(solid) is smaller than\nD(liquid) typically by several orders of magnitude, and D(gas) is large but finite at any\nnonzero pressure.\n\nFig.2.2. Mean squared displacement of three idealized systems, an ideal gas (no\nintermolecular collisions), a liquid, and a solid. The characteristic features of\nare a quadratic time dependence for all times for the gas, with\n,\nwhere T is the temperature and m the particle mass, a long-time linear behavior with a\nslope given by 6D, where D is diffusion coefficient, for the liquid, and a long-time\nplateau value of\n2 ( )\nr t\n< ∆\n>\n/\no\nB\nv\nk T m\n=\n/\nB\nk T mω , where ω the characteristic vibrational frequency, for the solid.\n\nIn the next lecture we will show that\n2 ( )\nr t\n< ∆\n> can be expressed in terms of the velocity\nautocorrelation function of the system. Combining this with (2.8) we then obtain a\nrelation between the diffusion coefficient D and the velocity autocorrelation function.\nThis relation turns out to be quite a general result of statistical mechanics and is known as\nthe Green-Kubo expression for the diffusion coefficient. Unlike the Fick's law (2.1), the\nGreen-Kubo relation, or (2.8), can be used to directly calculate the diffusion coefficient."
    },
    {
      "category": "Lecture Notes",
      "title": "lec3.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/22-103-microscopic-theory-of-transport-fall-2003/42a7a0ddcb94eb23f6c096ae04da0162_lec3.pdf",
      "content": "22.103 Microscopic Theory of Transport (Fall 2003)\nLecture 3 (9/12/03)\n\nDiffusion and the Velocity Autocorrelation - Green-Kubo Relations\n_______________________________________________________________________\n\nReferences --\nMcQuarrie, Sec. 21-8.\nBoon and Yip, Sec. 2.5\n________________________________________________________________________\n\nTime Correlation Function Expressions for Transport Coefficients - Green-Kubo\nFormulas\n\nTransport coefficients like the diffusion coefficient D, the viscosity coefficient η , and the\nthermal conductivity coefficient λ all can be expressed as time integrals of appropriate\ntime correlation functions. This formulation is known as the Green-Kubo formulas, the\nresults of linear response theory in statistical mechanics. To illustrate the main features\nof the Green-Kubo formalism we will treat explicitly the most straightforward case, that\nof diffusion.\n\nWe begin with the expression for the mean squared displacement of a typical particle in a\nfluid at equilibrium,\n[ ( )\n(0)]\n( )\nR t\nR\nr t\n< ∆\n<\n-\n>\n>=\n. Using the simple relation\n\n( )\n(0)\n' ( ')\nt\nR t\nR\ndt v t\n-\n= ∫\n\n(3.1)\nwe can write for the mean squared displacement function\n\n'\n\"\n( ')\n( \")\n( )\nt\nt\ndt\ndt\nv t\nv t\nr t\n< ∆\n>=\n<\n⋅\n>\n∫\n∫\n\n(3.2)\n\nWhat appears in the integrand is the velocity autocorrelation function, involving the\nvelocities of the particle at two different times, t' and t\". For a fluid system in equilibrium\none can invoke the property of time translation invariance. That is to say the correlation\nfunction depends only the time difference,\n\n( ')\n( \")\n( '\n\")\n(0)\nv t\nv t\nv t\nt\nv\n<\n⋅\n>=<\n-\n⋅\n>\n\n(3.3)\n\nThis is essentially a statement that one can shift the time origin by any amount without\naffecting the correlation function. The double integral in (3.2) extends over a square\nregion as shown in Fig. 3.1(a). To take advantage of (3.3) we transform the two integrals\nso that one of the variables of integration is the time difference t'-t\". Then the other time\nvariable would not appear in (3.3), which is unspecified, and the integration over this\n\nvariable can be carried out without knowing the velocity autocorrelation function.. We\ntherefore introduce a change of variable,\n'\n\"\nt\nt\nτ =\n-\nand\n\"\nd\ndt\nτ = -\n. Eq. (3.2) becomes\n\n=\n2 ( )\nr t\n< ∆\n>\n'\n'\n'\n(\nt\nt\nt\nt\ndt\nd\n)\nτα τ\n-\n∫\n∫\n\n(3.4)\n\nwith\n( )\n( )\n(0)\nt\nv t\nv\nα\n=<\n⋅\n> being the velocity autocorrelation function. After the coordinate\ntransformation, the region of integration now has the shape of a parallelogram, see Fig.\n3.1(b). The order of integration in (3.4) is over τ first and then t', or covering the\nintegration region first with a horizontal strip, extending from t'-t to t', and then moving\nthe strip from t' = 0 to t' = t.\n\nFig. 3.1. The region of integration for the double integral in (3.2), (a), and for the double\nintegral in (3.4), (b). Exchanging the order of integration, as (3.5), means that one\nintegrates first over vertical strips as opposed to integrating over horizontal strips\noriginally.\n\nSuppose we interchange the order of integration, and use instead a vertical strip which\nextends from t' = 0 to t' = t+τ when -t < τ <0, and another strip extending from t' = τ to\nt' = t when 0 < τ < t. Thus,\n\n=\n2 ( )\nr t\n< ∆\n>\n( )\n'\n( )\n'\nt\nt\nt\nt\nd\ndt\nd\nτ\nτ\nτα τ\nτα τ\n+\n-\n+\ndt\n∫\n∫\n∫\n∫\n\n(3.5)\n\n↓\n↓\n\nt + τ\nt - τ\n\nIn the first integral we can change τ to -τ , and make use of the fact that\n( )\n(\n)\nα τ\nα\nτ\n=\n-\n, a\nproperty of classical time correlation functions. Then the two integrals are the same, so\nthat (3.5) becomes\n\n[ ( )\n(0)]\n'(\n')\n( ')\n(0)\nt\nR t\nR\ndt t\nt\nv t\nv\n<\n-\n>=\n-\n<\n⋅\n∫\n>\n\n=\n\n(3.6)\n'(\n') (\nt\nov\ndt t\nt\nt\nψ\n-\n∫\n')\n\nwhere\n( )\n( )\n(0)\n/\n(0)\n(0)\nt\nv t\nv\nv\nv\nψ\n=<\n⋅\n> <\n⋅\n> is the normalized velocity autocorrelation\nfunction,\n(0)\n(0)\n3 o\nv\nv\nv\n<\n⋅\n>=\n, and\n,\nbeing the thermal speed.\n/\no\nB\nv\nk T m\n=\nov\n\nEq.(3.6) is the desired relation between the mean squared displacement and the velocity\nautocorrelation function. We know that the self-diffusion coefficient D is related to the\nmean squared displacement according to\n\n[ ( )\n(0)]\nt\nR t\nR\nD\nt\n→inf\n<\n-\n=\n⎡\n⎤\n⎢\n⎥\n⎣\n⎦\n\n(3.7)\n\nThen combining this with (3.6) we obtain\n\n(3.8)\n( )\nov\ndt\nt\nD\nψ\ninf\n= ∫\n\nEq.(3.8) is one of the Green-Kubo formulas for the transport coefficient of a fluid. It\nrelates a transport coefficient to a time integral of a time correlation function, in this the\nrelation is between the self-diffusion coefficient and the velocity autocorrelation function.\nAnalogous relations exist for the shear viscosity η and the thermal conductivity λ , and\nthe corresponding time correlation functions, the transverse component of the stress-\nstress correlation and the heat flux correlation function, respectively.\n\nThe significance of the Green-Kubo formulas is that one can use them to calculate the\ntransport coefficients which describe the relaxation or dissipation response of a fluid (to\nan external perturbation) in terms of time correlation functions which describe the\nthermal fluctuations in the fluid at equilibrium. This connection between the dissipation\nin the fluid out of equilibrium and the fluctuations in the fluid at equilibrium is a general\nfeature of linear response theory in statistical mechanics; it is expressed by the so-called\nfluctuation-dissipation theorem. One should keep in mind that this equivalence holds\nonly in the linear response regime, meaning that the perturbation to the fluid is\nsufficiently small that one need only to consider the first term in the deviation from\nequilibrium, the linear response.\n\nJust as it was instructive in Lec 2 to examine how the mean squared displacement\nfunction\nvaries from one idealized system to another, it is also helpful to our\nunderstanding of atomic motions in physical systems to see the behavior of\n2 ( )\nr t\n< ∆\n>\n( )t\nψ\nfor\nidealized systems. Fig. 3.2 shows the typical behavior for an ideal gas, a dense gas, a\nliquid and a solid. Since all collisions are neglected in the ideal gas model, the velocity\nautocorrelation function does not change from its initial value. As we increase the gas\npressure or density, collisional effects become important, so\n( )t\nψ\nstarts to decrease with\n\nFig. 3.2. The velocity autorccorelation function for an ideal gas, a dense gas, a liquid,\nand a solid. Compare these behavior with those of the mean squared displacement shown\nin Fig. 2.2.\n\ntime. For a dense gas there will be many collisions during the time interval shown in the\nsketch, the decay is then expected to be an exponential with a characteristic relaxation\ntime τ . This is what is expected for the Brownian motion model. As we approach liquid\ndensity, the molecules are very tightly packed, the density of a liquid is only a few\npercent larger than that of the corresponding solid. This means that a molecule in a liquid\nenvironment will be in continuous interaction with its near neighbors, the forces exerted\nby these neighbors are sufficiently strong to keep the molecule rattling around in a local\nregion - as if the neighbors are forming a cage. Because of the large fluctuations in a\nliquid, such cages have a rather short life time, so after a while the trapped molecule is\nable to diffuse away. In terms of\n( )t\nψ\nthe cage effect is revealed by\n( )t\nψ\nbecoming\nnegative during a certain interval, typically picoseconds for a simple liquid. As can be\nseen in Fig. 3.2, the effect is also present in the solid.\n\nA comparison of Fig. 3.2 with the mean squared displacement behavior given in Fig. 2.2\nshould reinforce our physical intuition of the dynamics of molecules in the three states of\nmatter. Since the diffusion coefficient is given by the time integral (3.8), or the area\nunder the curve of\n( )t\nψ\n, we see that the infinite diffusion coefficient previously noted for\nthe ideal gas based on (2.9), or Fig. 2.2, now shows up as a diverging integral in (3.8), or\na non-decaying\n( )t\nψ\n. As the system becomes more dense,\n( )t\nψ\ndecreases faster in time\nbecause of more frequent collisions. This has the effect of giving a smaller D. In the\ncase of a solid, the greatly reduced D means that the cancellation between the positive\nand negative regions of\n( )t\nψ\nis nearly complete. All these features are simple and rather\nintuitive. The student should keep them in mind as we proceed to consider other ways of\ndescribing dynamical systems, from the point of view of fundamental concepts as well as\nmethods of quantitative calculations."
    },
    {
      "category": "Lecture Notes",
      "title": "lec4.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/22-103-microscopic-theory-of-transport-fall-2003/dc5324c3741c71199e01ff8d9c12b7d1_lec4.pdf",
      "content": "22.103 Microscopic Theory of Transport (Fall 2003)\nLecture 4 (9/15/03)\n\nDiffusion and the Van Hove Self Correlation Functions\n_______________________________________________________________________\n\nReferences --\nBoon and Yip, Secs 2.2, 2.5\nL. van Hove, \"Correlations in Space and Time and Born Approximation Scattering in\nSystems of Interacting Particles\", Phys. Rev. 95, 249-262 (1954).\nG. Vineyard, \"Scattering of Slow Neutrons by a Liquid\", Phys. Rev. 110, 999-1010\n(1958).\n________________________________________________________________________\n\nThe mean squared displacement function and the velocity autocorrelation function are\ntwo functions which describe the dynamical behavior of a particle as it interacts with the\nother particles in the system. Because we are always focused on the same particle, albeit\nat different times, the behavior is called single-particle motion. The word 'single-particle'\ndoes not mean the particle is moving in isolation (like in a vacuum), rather it means that\nthe correlation over time is localized on the same particle. There is another function\nwhich, like\nand\n2 ( )\nr t\n< ∆\n>\n( )t\nψ\n, provides a fundamental description of single-particle\ndynamics. This function is the correlation of particle density. Let us define\n\n(\n', )\n( '\n(0)) (\n( ))\ns\nG r\nr t\nV\nr\nR\nr\nR t\nδ\nδ\n-\n=\n<\n-\n-\n>\n\n(4.1)\n\nwhere ( )\nr\nδ\nis the Dirac delta function. The function ( )\nr\nδ\nis essentially a bookkeeping\ndevice; it is a singular function which is zero everywhere except at the position r = 0 (the\norigin of the coordinate system) where it is infinite. Some basic properties are:\n\n( )\nd r\nr\nδ\n=\n∫\n\n(\n)\n( )\nax\nx\na\nδ\nδ\n=\n\n(4.2)\n( ) (\n)\n( )\ndxf x\nx\na\nf a\nδ\n-\n=\n∫\n\nFor the present discussion we can give the following interpretation to the two delta\nfunctions in (4.1),\n\n(\n(\nr\nR t))\nδ\n-\n~ probability that the particle whose coordinate\n(4.3)\n\nis\n( )\nR t is at the position r at time t\n\nTherefore the physical meaning of\n(\n',\ns\nG r\nr t)\n-\nis the conditional probability that given\nthe particle was at 'r initially (at time t = 0) it is at r at time t later. Another way to see\nthat Gs is a probability is from its spatial integral,\n\n( , )\ns\nd rG r t =\n∫\n\n(4.4)\n\nwhich is the statement that the probability of finding the particle somewhere in the\nsystem is certainty (particle cannot created or lost).\n\nThis simple interpretation is depicted in Fig. 4.1. We will call Gs the self correlation\nfunction because it involves two positions of the same particle, positions separated by an\narbitrary time t. It is also named after L. Van Hove for showing that this is the quantity\none measures in incoherent thermal neutron scattering. We will see later that Gs is\nfurthermore an important function from the standpoint of atomistic simulation of gases,\nliquids and solids. Thus Gs is a function that is meaningful for theory, experiment, and\nsimulation.\n\nFig. 4.1. Defining\n(\n',\ns\nG r\nr t\n-\n) as conditioning probability that given the particle is\ninitially at 'r , it will be at r at time t. The particle under consideration is particle i.\nNotice that a different particle, particle j, who was initially somewhere else, can also\nmigrate to position r at time t. The contribution from this particle is not included in Gs,\nbut it will be included in another density correlation function\n(\n',\nG r\nr t)\n-\n, to be discussed\nlater.\n\nIt is worthwhile to think about what is the dynamical information contained in Gs,\nespecially what connection, if any, it has with molecular diffusion. We first note that the\ninitial value of this function follows directly from its definition, (4.1),\n\n( ,0)\n( )\ns\nG r\nr\nδ\n=\n\n(4.5)\n\na result which is physically very intuitive - the probability of finding the particle at time\nt=0 anywhere other than the origin is zero. Secondly, from Fig. 4.1 we see that by\nlooking at Gs at incremental values of t, we can trace out the path that the particle takes as\nit moves through the system (the solid wiggly line in Fig. 4.1). Therefore, whatever\nmotion the particle undergoes, including diffusion, it is directly reflected in\n( , )\ns\nG r t .\nWith\n( , )\ns\nG r t being well-defined for any value of t, we expect that only its long-time\nbehavior will show diffusion (if such motions are taking place), while at short times it\nshould show inertial behavior corresponding to the t2 variation in the mean square\ndisplacement function (recall Lecs 2 and 3).\n\nIn discussing the mean square displacement function, the velocity autocorrelation\nfunction, and now the self correlation function we focus on a single particle as if it has a\n\nlabel so we can distinguish it from all the other (N-1) particles in the system. This is no\nproblem in computer simulation since every particle does have a label, so we can tell\nwhich is particle i at any given time. In theory this is also not a problem because we can\ngive particle i, the particle of interest, an imaginary tag (typically calling it the trace or\ntagged particle). In experiment, such as neutron scattering, we cannot always any particle\nthat we like. It is sometimes possible to isolate the single particle motions from the\ncollective motions which involve two or more particles. This is the case with incoherent\nneutron scattering where the scattering nucleus can be tagged if it has a nonzero spin or if\nthere are different isotopes in the sample. This is why we can measure single-particle\nmotions like diffusion with incoherent inelastic scattering, and also collective motions\nwith coherent inelastic scattering. This is essentially the important contribution of Van\nHove to the theory of thermal neutron scattering in 1954.\n\nIn interpreting (4.1) we have said that the delta function (4.3) can be regarded as the\nprobability of finding the particle at position r at time t. It turns out that we can also\nregard the delta function as a density variable,\n\n( , )\n(\n( ))\nsn r t\nr\nR t\nδ\n=\n-\n\n(4.6)\n\nNormally, we would say the number of particles in an element of volume centered about\nthe position r at time t is the time dependent density ( , )\nn r t . With only one tagged\nparticle in the system, the density becomes (4.6) which is also the probability that at time\nt the particle is at r . Dimensionally this also works out because the dimension of\n( )\nr\nδ\nis\nreciprocal volume (see this from the first property in (4.2)). With the interpretation of the\ndelta function as a density, we can rewrite (4.1) as\n\n(\n', )\n( ',0)\n( , )\ns\ns\ns\nG r\nr t\nV\nn r\nn r t\n-\n=\n<\n>\n\n(4.7)\n\nThis shows that the Van Hove self correlation function is actually a density correlation\nfunction for a tagged particle.\n\nWe will have more to say about Gs in the following lectures. For now we will close this\nlecture by suggesting a connection between the mean squared displacement function\nand\n2 ( )\nr t\n< ∆\n>\n( , )\ns\nG r t . The connection is that\n2 ( )\nr t\n< ∆\n> is the second spatial moment of\n( , )\ns\nG r t ,\n\n( )\n( , )\ns\nr t\nd rr G r t\n< ∆\n>= ∫\n\n(4.8)\n\nWhile this may or may not be intuitively reasonable to the reader, it is worth thinking\nabout."
    },
    {
      "category": "Lecture Notes",
      "title": "lec5.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/22-103-microscopic-theory-of-transport-fall-2003/124df307eb754bde72eb9a08073b8a63_lec5.pdf",
      "content": "22.103 Microscopic Theory of Transport (Fall 2003)\nLecture 5 (9/17/03)\n\nThe Density Correlation Functions and Neutron Scattering\n_______________________________________________________________________\n\nReferences --\nBoon and Yip, Sec. 2.2\n________________________________________________________________________\n\nIn the last chapter we discussed the interpretation of the Van Hove self correlation\nfunction, (4.1), by regarding the Dirac delta function (\n(\nr\nR t))\nδ\n-\nas a measure of the\nprobability that the tagged particle is located at the position r at time t (cf. (4.3)). A\nrelated interpretation one can give to the delta function is that of a time-dependent\ndensity. This is easier to see by considering the following sum of delta functions,\n\n( , )\n(\n( ))\nN\ni\ni\nn r t\nr\nR t\nN\nδ\n=\n=\n-\n∑\n\n(5.1)\n\nwhere the coefficient 1/\nN , with N being the number of particles in the system, appears\nfor reasons of normalization (as we will see later). The interpretation of (5.1) is\n\n( , )\nn r t d r = no. of particles in the volume element d3r about r at time t\n\nSince the delta function is just a bookkeeping device for locating particles and each term\nin (5.1) is the probability that particle is at r at time t, the sum therefore gives the number\nof particles found at r at time t. Put another way, imagine we can set up a volume\nelement d3r (a pill box) at r and count the number of particles inside the element, and we\ndo this by checking every particle in the system. Aside from the normalization factor,\nthis is exactly the number of particles is just what is given by (5.1). Defining the density\nas (5.1) means that all the particles are identical.\n\nIn the case of the Van Hove self correlation function we are interested (or want to follow)\nonly in one of the particles in the system (which we have been calling the tagged\nparticle). So the corresponding 'density' to (5.1) is just one delta function,\n\n( , )\n(\n( ))\nsn r t\nr\nR t\nδ\n=\n-\n\n(5.2)\n\nwhere\n( )\nR t is the position of the tagged particle at time t. With this definition, we can\nrewrite the self correlation function in terms of the tagged-particle density (5.2).\nHowever, instead of\n( , )\nsn r t we will take the deviation from the mean,\n\n( , )\n( , )\n( , )\ns\ns\ns\nn r t\nn r t\nn r t\nδ\n=\n-<\n>\n\n(5.3)\n\nwith\n( , )\n1/\nsn r t\nV\n<\n>=\n. The reason for taking (5.3) instead of (5.2) to define the self\ncorrelation function is that in linear response theory it is conventional to study\ncorrelations of variables which fluctuate with zero mean (one reason for this is that\nFourier transforms of these variables are well-defined). Thus the Van Hove self\ncorrelation function is defined to be\n\n(\n', )\n( ',0)\n( , )\ns\ns\nG r\nr t\nV\nn r\nn r t\ns\nδ\nδ\n-\n=\n<\n>\n\n=\n( ',0)\n( , )\ns\ns\nV\nn r\nn r t\nV -\n<\n> -\n\n(5.4)\n\nComparing (5.4) with (4.1) we see that our earlier definition differs by the second term in\n(5.4). Strictly speaking, (5.4) is the more proper definition, while in practice, the 1/V is\nnegligible because we always take the thermodynamic limit where N, V\nwith N/V =\nn, the equilibrium number density.\n\n→inf\n\nCorrespondingly we will define the time-dependent density correlation function as\n\n(\n', )\n( ',0)\n( , )\nG r\nr t\nV\nn r\nn r t\nδ\nδ\n-\n=\n<\n>\n\n= 1\n( ',0) ( , )\nn r\nn r t\nn\nn\n<\n> -\n\n=\n,\n( '\n(0)) (\n( ))\nN\ni\nj\ni j\nr\nR\nr\nR t\nn\nn\nδ\nδ\n=\n<\n-\n-\n>\n∑\n-\n\n(5.5)\n\nwith\n( , )\n/\nn r t\nN V\n<\n>=\n. From the third line in (5.5) we see that\n( , )\nG r t d r has the\ninterpretation of being the expected number of particles in the element of volume d3r\nabout r, given that a particle was at the origin at time zero. It is obvious from their\ndefinitions that the pair of functions G and Gs are space-time density correlation\nfunctions. They are particularly important members of a class of space-time correlation\nfunctions, known simply as time correlation functions, because the density is such a\nfundamental variable in the description of any physical system. Other members which\nwe will encounter are momentum and energy density correlation functions. There are a\nnumber of basic properties of time correlation functions which we will discuss the next\nlecture.\n\nWe have already emphasized previously the central role of correlation functions in the\ntheoretical understanding of transport phenomena; they are meaningful quantities from\nthe standpoint of theory, experiment, and simulation. In the case of thermal neutron\nscattering, the density correlation functions are intimately connected to the double\ndifferential scattering cross section, a connection which has been thoroughly discussed in\nthe subject 22.54, Neutron Interactions and Applications, and also in 22.51, Interaction of\nRadiation with Matter. It is worthwhile to remind ourselves of this connection, in the\ncontext of the significance of time correlation functions to experiments.\n\nThere are two types of thermal neutron scattering, coherent and incoherent. The former\nis the more common process, whereas the latter requires the scattering nucleus to have\nnonzero spin, or the scattering sample have a distribution of isotopes. Incoherent\nscattering is important because hydrogen, which is a nucleus frequently encountered in\nall areas of nuclear engineering, has an exceptionally large incoherent scattering cross\nsection (and a rather small coherent scattering cross section). The connection between\nthe double differential scattering cross section, the quantity that one measures in an\ninelastic scattering experiment, and the density correlation function is\n\n1/2\n( , )\nf\ni t\nik r\ni\nE\nd\na\ndte\nd re\nd\nd\nE\nω\nσ\nω\nπ\ninf\n-\n-\n⋅\n-inf\n⎛\n⎞\n=\n⎜\n⎟\nΩ\n⎝\n⎠\n∫\n∫\nG r t\n\n(5.6)\n\nTo define the various quantities in this expression, it is useful to refer to a schematic of\nthe inelastic scattering experiment. As indicated in Fig. 5.1 the incident (initial) beam of\n\nFig.5.1. Geometry of inelastic scattering of an incoming beam of thermal neutrons with\nenergy Ei and wave vector ki in a sample S at an angle θ . The intensity of the outgoing\nbeam with energy Ef and wave vector kf, measured at the detector D, is proportionality to\nthe double differential scattering cross section.\n\nthermal neutrons with energy\niE and momentum\nik\n=\nis scattered in the sample at a\nscattering angle θ , and the outgoing (final) beam has energy\nf\nE and momentum\nf\nk\n=\n.\nThe intensity measured at the detector is proportional to the double differential cross\nsection, the left hand side of (5.6). Here\nsin\nd\nd d\nθ θ φ\nΩ=\nis the element of solid angle,\nand\ni\nf\nE\nE\nω =\n-\n=\nis the energy transfer as a result of the scattering. On the right hand side\nof (5.6), a is the scattering length which is either coherent or incoherent. For coherent\nscattering the integrand in (5.6) is the density correlation function given by (5.5), whereas\nfor incoherent scattering it is the Van Hove self correlation function (5.4). In either case\nwe see that the measured scattering intensity, aside from the scattering length and the\nfactor\nwhich comes from the ratio of outgoing to incoming currents, is just the\ndouble Fourier transform of the density correlation function, where\n1/ 2\n(\n/\n)\nf\ni\nE\nE\ni\nk\nk\nk\n=\n-\n=\n=\n=\nf is the\nmomentum transfer. For\n( , )\nG r t the double Fourier transform,\n\n( ,\n)\n( , )\ni t\nik r\nS k\ndte\nd re\nG r t\nω\nω\nπ\ninf\n-\n⋅\n-inf\n=\n∫\n∫\n\n(5.7)\n\nis known as the dynamic structure factor. For the Van Hove self correlation function\nthere is a corresponding double Fourier transform,\n( ,\n)\nsS k ω . Since what one measures in\nthe scattering experiment are the scattering intensity and the energy and momentum\ntransfers, this information is seen to be just the Fourier components (in frequency and\nwavelength) of the density correlation function.\n\nThe connection between the density correlation function, which is fundamental in the\ntheoretical description of structure and dynamics of matter, and the dynamic structure\nfactor, which is the most important quantity measured in inelastic scattering of thermal\nneutrons, through a double Fourier transform is a particularly significant result. In (5.7)\nit may appear that the wave vector k and frequency ω can take on any value that one\nwants to specify. While this is true if (5.7) were regarded as a formal definition, it would\nnot be true if we take S to be the experimentally measured quantity in which case the\nmagnitudes of the momentum and energy transfers would be set by the instrumental\nresolutions of the neutron scattering experiment. That is to say, there are practical limits\nto what are the practical ranges of momentum and energy transfers that one measure in\nneutron scattering. This is not a trivial issue in comparing the information that one can\nobtain from neutron scattering with that given by x-ray or light scattering, for example.\nFor now, we will not be concerned with this aspect, and regard (5.7) as a mathematical\nrelation between two functions, one in space-time and the other in wave vector and\nfrequency.\n\nBefore closing we remind the reader that Gs gives dynamical information about tagged-\nparticle motions, where G gives structural and dynamical information about collective\nmotions. For the study of diffusion we will be concerned with Gs. One should also keep\nin mind that G is the full density correlation function, as such it contains the information\nthat is expressed in Gs. By this we mean that from their definitions one can write G as\nG = Gs + Gd, where Gd is known as the distinct correlation function, that is, it correlates a\nparticle at the orgin at time zero with a different particle at r and time t. While this\ndecomposition is exact, it is not always a simple matter to extract Gs when given G.\nNonetheless, we can say that Gs describes the tagged particle motion whereas Gd\ndescribes the motions between particles (interference effects?), and there is no overlap\nbetween the two. In contrast, there is overlap between G and Gs. Since G and Gs are\nrelated to coherent and incoherent neutron scattering respectively, the overlap also exists\nbetween coherent and incoherent scattering.\n\nIn the next lecture we will discuss the basic properties of time correlation functions in\ngeneral, and those of the density correlation functions in particular. In terms of diffusion,\nthe reader should pay special attention to the connection between the quantities which we\nhave discussed up to now, the diffusion coefficient, the mean square displacement\nfunction, the velocity autocorrelation function, and now the Van Hove self correlation\nfunction and the density correlation function."
    },
    {
      "category": "Lecture Notes",
      "title": "lec6.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/22-103-microscopic-theory-of-transport-fall-2003/7a4e269d651b188a78eb1306ea1d9155_lec6.pdf",
      "content": "22.103 Microscopic Theory of Transport (Fall 2003)\nLecture 6 (9/19/03)\n\nStatic and Short Time Properties of Time Correlation Functions\n_______________________________________________________________________\n\nReferences --\nBoon and Yip, Chap 2\n________________________________________________________________________\n\nThere are a number of properties of time correlation functions which follow from their\ndefinitions, sometimes in a straightforward manner and other times not so simply. It is\nimportant to make note of them because not only do they have useful physical meanings,\nbut also they are often used to formulate model descriptions of time correlation functions.\nActually we have already encountered several examples in our discussion of diffusion,\nfor example, the relation between the long-time behavior of the mean-squared\ndisplacement function and the diffusion coefficient.\n\nStatic Correlation Functions\n\nSince a time correlation function is a function of time, one can always ask about its value\nat time t = 0, also known as the initial value. This value is the correlation of two\nvariables (which may be different or the same) at the same time but at different spatial\npositions. Correlation functions which have spatial variations but do not depend on time\nare called static. We have seen that the initial value of the Van Hove self correlation\nfunction is just the delta function, (4.5). It is an interesting exercise to work out the\ncorresponding initial value for the density correlation function\n( , )\nG r t . Setting t = 0 in\n(5.5) we find\n\n( ,0)\n( )\n[ ( ) 1]\nG r\nr\nn g r\nδ\n=\n+\n-\n\n(6.1)\n\nwhere\n\n,\n( )\n(\n) (\n)\nN\ni\nj\ni j\nn g r\nr\nR\nR\nδ\nδ\n=\n<\n-\n>\n∑\n\n(0, ,\n, ,\n)/\n(\n1)\nN\nB\nU\nr R\nR\nk T\nN\nN\nN N\nd R\nd R e\nQ\n-\n⋅⋅⋅\n-\n=\n⋅⋅⋅\n∫\n\n(6.2)\n\nThe function g(r) is well-known in liquid-state theory as the equilibrium pair distribution\nor the radial distribution function (RDF). This is the quantity that one obtains from an x-\nray or neutron diffraction measurement. Recall from the previous lecture that G is the\nsum of Gs and Gd. Eq.(6.1) shows that the first term is the initial value of Gs as we\nalready know from (4.5). The second term is therefore the initial value of Gd.\n\nBecause g(r) is the fundamental quantity in the equilibrium theory of liquids [see\nMcQuarrie], we digress somewhat from the present discussion of time-dependent density\ncorrelation functions to examine its relation to the canonical distribution function.\nSuppose we define\n\n(\n)\n( ...\n)\n...\n...\nN\nU\nN\nN\nN\nN\nN\ne\nP\nr\nr\nd r\nd r\nd r\nd r\nZ\nβ\n-\n=\nN\n\n(6.3)\n\nas the probability that atom 1 is in d3r1 about\n1r , ..., atom N is in d3rN about rN.\nIntegrating this over the positions of atoms n+1 through N gives\n\n( )\n( ...\n)\n...\n...\nN\nU\nn\nn\nN\nn\nN\nP\nr\nr\nd r\nd r e\nZ\nβ\n-\n+\n=\n∫∫\nN\n\n(6.4)\n\ngives the probability that atom 1 is in d3r1 about r1, ..., atom n is in d3rn about rn\nirrespective of the positions of atoms n+1 through N. Now the probability that any atom\nis in d3rn, any other atom is in d3r2, any atom other than the first two is in d3r3, ..., and\nany atom other than the previous n-1 atoms is in d3rn is\n\n( )\n( )\n!\n( ...\n)\n( ...\n)\n(\n)!\nn\nn\nN\nN\nr\nr\nP\nr\nr\nN\nn\nn\n=\n-\nn\nn\n\n(6.5)\n\nwhere\n!/(\n)!\nN\nN\nn\n-\nis the number of ways one can pick n objects out of N possible\nchoices. For example, the number of ways of picking two atoms out of an N-particle\nsystem is N!/(N-2)! = N(N-1). Given (6.5), we have\n\n(1)\n(\n)\nN\nd\nr\nV\nV\nrn\nρ\n=\n≡\n∫\n\n(6.6)\n\nwhich is the number density of the system (usually denoted as n and taken to be a\nconstant). We next define the n-particle correlation function\n( )\n( ...\n)\nn\nn\ng\nr\nr\nby writing\n\n( )\n( )\n( ...\n)\n( ...\n)\nn\nn\nn\nn\nr\nr\nn g\nr\nr\nn\n=\nn\nn\n\n(6.7)\n\nNotice that if the atoms are not correlated, then\n( )\nn\nn\nn\n=\n, and\n( )\nn\ng\n≡. This is what we\nmean operationally when we say there is no spatial correlation among the n-particles in\nthe system. Combining (6.4), (6.5), and (6.77) we obtain a definition of the n-particle\ncorrelation function in terms of the canonical distribution,\n\n( )\n!\n( ...\n)\n...\n...\n(\n)!\nN\nn\nU\nn\nn\nn\nN\nn\nN\nV\nN\ng\nr\nr\nd r\nd r e\nN\nN\nn\nZ\nβ\n-\n+\n=\n-\n∫∫\n\n(6.8)\n\nwhich is the desired relation.\n\nTo reduce to the pair distribution function g(r) we note that in a liquid composed\nspherical atoms,\n(2)\n( ,\n)\ng\nr r\ndepends only on the separation distance\nr\nr\nr\n=\n-\n. Thus we\ncan write\n(2)\n( ,\n)\n( )\ng\nr r\ng r\n≡\n, or\n\n(2)\n( ,\n)\n( )\nr r\nn g r\nn\n=\n\n(6.9)\n\nwhich is a definition of g(r) in terms of the n-particle correlation function (158) with n=2.\nIt then follows from the above probability distributions that\n\nprobability (not normalized) of finding an atom\n( )\nng r d r =\n\nin\nabout\nd r\nr given an atom is at the origin\n\nTo find the normalization we take\n\n(6.10)\n( )\n( )4\nd r g r\nng r\nr dr\nN\nN\nn\nπ\ninf\n=\n=\n∫\n∫\n\n-\n\nThus,\n\nnumber of atoms in a shell of radius r and thickness dr\n( )\nr\ng r dr\nn\nπ\n=\n\ncentered about an atom at the origin\n\nA sketch of g(r), which is also known as the pair correlation function or the radial\ndistribution function, for a typical liquid shows its most significant features - a prominent\npeak at the distance of nearest neighbor separation, and a weaker peak at the second-\nnearest separation, see Fig. 6.1. Below a certain separation distance g(r) vanishes,\nsignifying a hard core in the interatomic interaction which gives rise to an excluded\nvolume in the spatial distribution of particles. In other words, particles repeal each other\nat very short range, a basic property associated with the existence of a certain density of\nthe system. At the opposite extreme of large separations, g approaches unity, signifying\nthe loss of spatial correlation (particles are uncorrelated when they are far apart). It is\nuseful to compare the behavior of g(r) to that of a typical pair potential like the Lennard-\nJones. Indeed at low densities a virial expansion of g(r) gives\n, where V(r) is\nthe pair potential. This leads to the definition of a 'potential of mean force'\n( )\n( )\nV r\ng r\ne\nβ\n-\n≈\n( )\n( ) /\neff\nV\nr\nng r\nβ\n≈-A\nwhich one can apply even to hard-sphere fluids. Since the static\nstructure factor S(k) and g(r) are related by Fourier transform, an osciallatory behavior in\none function means the other function will also show oscillations. The small wiggles in\ng(r) at small r are numerical artifact of the Fourier transformation. The value of S(k) in\n\nthe limit of zero k is the compressibility factor, which becomes large in the vicinity of the\ncritical point.\n\nFig. 6.1. Radial distribtution function g(r) of liquid argon as obtained by Fourier\ntransform of the measured static structure S(Q). Data points are neutron diffraction\nmeasurements, the curve is the result of molecular dynamics simulation using the\nLennard-Jones potential (Yarnell et al., Phys. Rev. A7, 3130 (1973)).\n\nThe oscillatory shape of g(r) has a simple physical meaning. Suppose we sit on any one\nof the particles and look around to see how the neighboring particles are distributed. We\ncan do this by drawing a sphere of radius r around us, with some thickness dr, and ask\nhow many of the neighbors lie inside this spherical shell, doing this repeatedly with\nincreasing r. Since for a given r and dr, the number of neighbors is just\n( )\nr ng r dr\nπ\n, the\nfact that g(r) is an oscillatory suggests that the neighbors are not uniformly distributed,\nrather they like to arrange themselves in a shell-like structure. Thus the first peak in g(r),\nwhich is by far the strongest, signifies the position of the nearest-neighbors, the second\npeak, the second nearest neighbors, and so on. The peaks are seen to damp out fairly\nquickly, this is a reflection of the fact that there is only local order in a liquid and no\nlong-range order. One expects that for a crystal the corresponding g(r) would be a set of\nvery sharp lines, each sitting precisely at the position for the particular neighbors.\n\nImage Removed\nYarnell et al., Physical Review A7, p.2130 (1973)\ntop graph is fig 6 in paper; bottom graph is fig 4.\n\nReturning to the density correlation functions, we note that just as g(r) can be measured\nby neutron and x-ray diffraction, G and\ns\nG can be directly measured by neutron inelastic\nscattering and light scattering. Neutron inelastic scattering experiments actually give the\ndouble Fourier transforms of the two density correlation functions,\n\n(\n( ,\n)\n) ( , )\ni k r\nt\nS k\ndt d re\nG r t\nω\nω\ninf\n⋅-\n-inf\n= ∫\n∫\n\n(6.11)\n\nand the same relation exists between\n( ,\n)\nsS k ω and\n( , )\ns\nG r t . ( ,\n)\nS k ω is generally known as\nthe dynamic structure factor, which is reasonable since the Fourier transform of g(r),\n\n( )\n[ ( )\n1]\nik r\nS k\nn d re\ng r\n⋅\n= +\n-\n∫\n\n(6.12)\n\nis called the static structure factor.\n\nTo see the connection between (6.11) and (6.12), we introduce another quantity which is\nrelated to ( ,\n)\nS k ω (and therefore\n( , )\nG r t ) by writing (6.11) as\n\n(6.13)\n( ,\n)\n( , )\ni t\nS k\ndte\nF k t\nω\nω\ninf\n-\n-inf\n= ∫\n\nso that the intermediate scattering function F is just the Fourier spatial transform of G(r,t)\n(for isotropic systems such as a simple fluid, G depends only the magnitude of r ),\n\n( , )\n( , )\nik r\nF k t\nd re\nG r t\n⋅\n= ∫\n\n*(0)\n( )\n(2 )\n( )\nk\nk\nn\nn t\nk\nπ\nδ\n=<\n> -\n\n(6.14)\nwith\n\n( )\n( )\ni\nN\nik R\nt\nk\ni\nn t\ne\nN\n⋅\n=\n=\n∑\n\n(6.15)\n\nWe now ask what is the initial value of F(k,t). At t = 0, (6.14) gives\n\n( ,0)\n( ,0)\nik r\nF k\nd re\nG r\n⋅\n= ∫\n\n(6.16)\nwhere\n\n, '\n( ,0)\n( '\n(0)) (\n(0))\nnG r\nr\nR\nr\nR\nn\nδ\nδ\n=<\n-\n-\n> -\n∑\nA\nA\nA A\n\n(6.17)\n\nSetting 'r = 0 without any loss of generality, we can write out the double sum explicitly,\n\n( ,0)\n(\n) (\n)\n(\n1)\n(\n) (\n)\nnG r\nN\nR\nr\nR\nN N\nR\nr\nR\nn\nδ\nδ\nδ\nδ\n=\n<\n-\n> +\n-\n<\n-\n> -\n\n(\n1)\n...\n(\n) ( )\n...\n(\n) (\n)\nU\nU\nN\nN\nN\nN\nN\nN N\nd R\nd R e\nR\nr\nd R\nd R e\nR\nr\nR\nn\nQ\nQ\nβ\nβ\nδ\nδ\nδ\nδ\n-\n-\n-\n=\n+\n∫\n∫\n-\n-\n\n( )\n( )\nN\nr\nn g r\nn\nV\nδ\n=\n+\n-\n\n(6.18)\n\nInserting this result into (167) we obtain\n\n( ,0)\n[ ( )\n1]\n( )\nik r\nF k\nn d re\ng r\nS\n⋅\n= +\n-\n≡\n∫\nk\n\n(6.19)\n\nSince the information needed to calculate the density correlation functions are the time-\ndependent particle positions, the output of MD simulations, it follows that MD provides a\ndirect means of determining the correlation functions and their Fourier transforms. In\nthis way, MD is able to provide results that can be used to interpret experiments, as well\nas results that can be used to test various dynamical models of atomic motions in gases\nand liquids. Examples of both kinds of applications will be discussed in class.\n\nShort-Time Behavior and Sum Rules\n\nThe initial value of a time correlation is the leading term in a Taylor series expansion in\ntime. Because the Fourier transform of a time correlation function is often also of\ninterest, we can make use of the connection between the coefficients of the Taylor\nexpansion and the frequency moments of the corresponding Fourier transform of the time\ncorrelation function. Take the example of the density correlation function where we\nwrite\n\n( , )\n( , )\nik r\nF k t\nd re\nG r t\n⋅\n= ∫\n\n=\n( ,\n)\ni t\ndte\nS k\nω\nω\ninf\n-inf∫\n\n(6.20)\n\nThe Taylor series expansion is\n\n( , )\n( , )\n( , )\n( ,0)\n...\n2!\nt\nt\ndF k t\nd F k t\nF k t\nF k\nt\nt\ndt\ndt\n=\n=\n⎛\n⎞\n⎛\n⎞\n=\n+\n+\n⎜\n⎟\n⎜\n⎟\n⎝\n⎠\n⎝\n⎠\n+\n\n(6.21)\n\nwhile from (6.20) we have\n\n( )\n( , )\n( ,\n)\n( , )\n( ,\n)\n...\n2!\nit\nF k t\nd S k\nit d\nS k\nd\nS k\nω\nω\nωω\nω\nωω\nω\n=\n+\n+\n∫\n∫\n∫\n+\n\n(6.22)\n\nMatching terms with the same power of t in (6.21) and (6.22) gives relation between the\nfrequency moments of S, known as sum rules, to the time derivatives of F, the\ncoefficients in the Taylor series in (6.21). The first few such relations are:\n\n( )\n( ,\n)\no k\nd S k\nω\nω\nω\nπ\ninf\n-inf\n=\n∫\n= S(k)\n\n(6.23)\n\n( ,\n)\no\ns\ns\nd S k\nω\nω\nω\nπ\ninf\n-inf\n=\n∫\n=\n\n(6.24)\n\n( )\n( ,\n)\n(\n)\no\nk\nd\nS k\nk\nω\nωω\nω\nπ\ninf\n-inf\n=\n=\n∫\nv\n\n=\n( )\n( ,\n)\ns k\nd\nS k\ns\nω\nωω\nω\nπ\ninf\n-inf\n=\n∫\n\n(6.25)\n\nwhere\n, with\nbeing the thermal speed of the particle.\n/\no\nB\nv\nk T m\n=\nov\n\nThe relations between the coefficients of the time expansion and the frequency moments\nof the Fourier transform of the time correlation function is quite general; it is applicable\nto any pair of functions which are Fourier transforms of each other. Since we are dealing\nwith classical time correlation functions, they are even functions in time and frequency.\nSo terms that are odd in t vanish in (6.21) and (6.22). There are a number of properties of\nclassical correlation functions which do not carry over to quantum mechanical definitions\nof correlation functions. We do not take the time to discuss them here except to refer the\nreader to Sec. 2,7 Linear Response Theory for some of the details. To finish up this\nlecture, we note that there are properties such as long time behavior and relation to\ntransport coefficients, the concept of a memory function, linear response theory, and\nkinetic theory which are treated in Chap 2 of Boon and Yip. We will return to them at\nvarious times in future lectures."
    },
    {
      "category": "Lecture Notes",
      "title": "lec25.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/22-103-microscopic-theory-of-transport-fall-2003/af9780fa7b3af28fbd9975b0b8c9f0d4_lec25.pdf",
      "content": "22.103 Microscopic Theory of Transport (Fall 2003)\nLecture 25 (11/17/03)\n\nRole of Atomistic Simulation in Transport - Basic Molecular Dynamics\n_______________________________________________________________________\n\nReferences --\n\nS. Yip, \"Atomistic Modeling of Liquids\", in Encyclopedia of Advanced Materials, D.\nBloor, R. Brook, M. Flemings, S. Mahajan, eds. (Pergamon, 1994) and references therein.\n________________________________________________________________________\n\nWith this lecture we begin a new topic in this course, that of atomistic simulation as a\ntool for the investigation of transport phenomena. We have seen how transport processes\ncan be discussed in various ways, as transport coefficients, correlation functions, and in\nterms of phase-space distribution functions which satisfy transport equations. In the\ndiscussion of the Boltzmann equation we saw that one has to restrict himself to\nuncorrelated binary collisions in order to arrive at tractable approximations. This\nrestriction to low-density gas means that we are not able to discuss transport phenomena\nthat occur in liquids and solids, systems for which the Boltzmann equation cannot\ndescribe. In order to treat liquids and solids one can try to extend the Boltzmann\napproach based on particle collisions. This line of research has led to very complex\nmathematical problems involving correlated binary collisions and triple collisions. The\nmathematics become so heavy that physical insights become obscure. The alternative is\nto go to atomistic simulations where one studies particle trajectories numerically. This\nturns out to be a very powerful, though theoretically much less elegant approach.\n\nWe will develop the simulation approach to transport in the next few lectures. First we\nnote that there are two major methods of atomistic simulation, molecular dynamics (MD)\nand Monte Carlo (MC). MD is very suited to the study of transport or dynamical\nprocesses, whereas MC also can be used, although not in as natural a manner as MD. For\na comparison of these two methods, see the reference given above.\n\nThe point to emphasize again is that the basic information that we seek to obtain and use\nto build up our basic understanding lies in the particle trajectories { ( )}\nR t\n. Simulation\nprovides the means to directly generate this data, using only the interatomic potential as\nthe input. Simulation can be used for practically any system, whether it is a gas, liquid or\nsolid. There are other significant features of simulation which are discussed below. We\nleave it to the individual student to decide how much details he wants to learn about\nsimulation. We expect that some will want to know more than others. What every\nstudent should take away is the appreciation of what simulation can do, and some\nexamples of what it has done already. We think it is safe to predict that the role of\nsimulation in understanding transport will only become more and more important.\n\nBasic Molecular Dynamics (MD)\n\nThis lecture is intended to be a concise, self-contained discussion of the basic elements of\nmolecular dynamics (MD) simulation, serving as a read-me primer for the newcomer as\nwell as a summary of essentials for the initiated student who is not yet an expert. We will\ndevelop the subject according to the following list of topics.\n\n1. Defining the MD Method\n2. The Pair Potential\n3. Bookkeeping Matters\n4. Properties Which Make MD Unique\n5. Hands-On MD\n6. Understanding Crystals and Liquids - An Example of an Application\n\n1. Defining the MD Method\n\nA working defintion of MD is: The process by which one generates the atomic\ntrajectories of a system of N particles by direct numerical integration of Newton's\nequations of motion (with appropriate specification of an interatomic potential and\nsuitable initial and boundary conditions).\n\nFor an even shorter definition, one can leave out the words in the parenthesis and\nconsider them as detailed qualifications. What is meant by the few words in this\nstatement? Consider a simulation model (system) to be N particles contained in a region\nof volume V at temperature T, shown schematically in Fig. 1.\n\nFig.1. A model for molecular dynamics simulation, a system of N particles, each having\nan instantaneous position\n( )\nj\nand velocity\nr\nt\n( )\nj\n, with all the particles interact with each\nother through a potential energy U. Simulation means letting the system evolve in time\nand recording the particle positions and velocities at small incremental time steps. How\nthis is done is explained in this lecture.\nv\nt\n\nThe positions of the N particles are specified by a set of N vectors,\nN\n, with\n{ ( )}\n(\n( ),\n( ),...,\n( ))\nr t\nr t\nr\nt\nr\nt\n=\n( )\nj\nbeing the position of particle j at time t. Knowledge of\nr\nt\n{ ( )}\nr t\nat various time instants allows us to follow the particle trajectories as they move\naround. Our model system of particles has a certain energy E which is the sum of kinetic\nand potential energies of the particles, E = K + U, where K is the sum of individual\nkinetic energies\n\nN\nj\nj\nK\nm\nv\n=\n=\n∑\n\n(1)\n\nand U is a prescribed interatomic potential mentioned above,\n(\n,\n,...,\n)\nN\nU r\nr\nr\n. In general U\ndepends on the positions of all the particles in a complicated fashion. We will soon\nintroduce a simplifying approximation, the assumption of two-body or pairwise\ninteraction, which makes this most important quantity much easier to handle.\n\nTo find the atomic trajectories in our model we need to solve the equations that specify\nthe particle positions; this is just what the Newton's equations of motion do, as everyone\nknows F = ma from simple mechanics. While we are all familiar with the equations of\nmotion of a pendulum or a rolling body, the equations of motion for our N-particle model\nis more complicated because the equations for different particles are all coupled to each\nother through the potential energy U. We can see this readily when we write out the\nequations explicitly,\n\n({ })\nj\nj\nr\nd r\nm\nU\ndt\n= -∇\nr\n, j = 1, ..., N\n\n(2)\n\nThat the motion of one particle depends on where the others are is not surprising, since\nthe force acting on one particle changees whenever one of its neighbors moves. Eq.(2)\nlooks deceptively simple, but it is as complicated as the famous N-body problem which\nwe cannot solve exactly when N is greater than 2. It is a system of coupled second-order,\nnon-linear ordinary differential equations. On the other hand, (2) can be solved\nnumerically, which is what one does in molecular dynamics simulation.\n\nWhen we say integrate (2) to obtain the atomic trajectories, we have in mind dividing a\ntime interval of interest into many small segments, each being a time step of size\n.\nGiven the system conditions at some initial time to,\nt\n∆\n{ ( )}\no\nr t\n, integration means we advance\nthe system successively by increments of\nt\n∆ ,\n\n{ ( )}\n{ (\n)}\n{ (\n)}\n...{ (\n)}\no\no\no\no\nt\nr t\nr t\nt\nr t\nt\nr t\nN\nt\n→\n+ ∆\n→\n+ ∆\n→\n+\n∆\n\n(3)\n\nwhere Nt is the number of time steps making up the interval of integration.\n\nHow do we numerically integrate (2) for a given U? A simple way is to write a Taylor\nseries expansion,\n\n(\n)\n( )\n( )\n( )(\n)\n...\nj\nj\nj\nj\no\no\nr\nt\nt\nr\nt\nv\nt\nt\na\nt\nt\n+ ∆\n=\n+\n∆+\n∆\n+\n\n(4)\nand a similar expansion for\n(\nj\no\nr\nt\nt)\n-∆\n. Adding the two expansions gives\n\n(\n)\n(\n)\n( )\n( )(\n)\n...\nj\nj\nj\nj\no\no\no\no\nr t\nt\nr\nt\nt\nr t\na t\nt\n+ ∆\n= -\n-∆\n+\n+\n∆\n+\n\n(5)\n\nNotice that the left-hand side is what we want, namely, the position of particle j at the\nnext time step\n, whereas all the terms on the right-hand side are quantities evaluated at\ntime to . We already know the positions at to and the time step before, so to use (5) we\nneed the acceleration of particle j at time to. For this we can make use of (2) and\nsubstitute\nt\n∆\n({ ( )}) /\nj\no\nF\nr t\nm in place of the acceleration, where F is just the right-hand side of\n(2). Thus through Eq.(5) one performs the integration of (2) in successive time\nincrements, following the system evolution in discrete time steps. . Although there are\nmore elaborate ways of doing the integration, this is basic idea of generating the atomic\ntrajectories, the essence of MD. The particular procedure we have described is the Verlet\ncentral difference method. In the MD code given to the class, a more accurate method\ncalled the Gear Predictor-Corrector is used. Higher accuracy of integration allows one to\ntake a larger value of\n, desirable because one can cover a longer time interval. On the\nother hand, the tradeoff is that one needs more memory relative to the simpler method.\nt\n∆\n\nThe time integrator is at the heart of MD simulation, with the sequence of positions and\nvelocities (trajectories) being the raw output. A typical flow-chart for an MD code would\nlook something like the following.\n\n(a) → (b) → (c) → (d) → (e) → (f)\n(g)\n→\n\na = set particle positions\n\nb = assign particle velocities\n\nc = calculate force on each particle\n\nd = move particles by timestep ∆t\n\ne = save current positions and velocities\n\nf = if reach preset no. of timesteps, stop, otherwise go back to (c)\n\ng = analyze data and print results\n\n2. The Pair Potential\n\nTo make the simulation tractable, it is common to assume the interatomic potential U can\nbe represented as the sum of two-body interactions,\n\n(\n,...,\n)\n( )\nN\nij\ni\nj\nU r\nr\nV r\n=\n≅∑\n\n(6)\nwhere\nij is the separation distance between particles i and j. V is the pairwise interaction;\nit is a central force potential, being a function only of the separation distance between the\ntwo particles,\nr\ni\nj\nij\n. A very common two-body interaction energy used in atomistic\nsimulations, known as the Lennard-Jones potential, is\nr\nr\nr\n=\n-\n\n( )\n(\n/ )\n(\n/ )\nV r\nr\nr\nε\nσ\nσ\n=\n-\n⎡\n⎤\n⎣\n⎦\n\n(7)\n\nwhere ε and σ are parameters of the potential. Like all pair potentials, this interaction\nenergy rises sharply (with inverse power of 12) at close interatomic separations, has a\nminimum, and decays to zero at large separations. See Fig. 2 which also shows the\nbehavior of the interatomic force,\n\n( )\n( )\ndV r\nF r\ndr\n= -\n\n(8)\n\nwhich is repulsive at short separations and attractive at large separations. We can\nunderstand the repulsion as arising from the overlap of the electron clouds, while the\nattraction is due to the interaction between the induced dipole in each atom. The value of\n12 for the first exponent in V(r) has no special significance, as the repulsive term could\njust as well be replaced by an exponential. The second exponent results from quantum\nmechanical calculations (the so-called London dispersion force) and therefore is not\narbitrary. Regardless of whether one uses (7) or some other interaction potentials, a\nshort-range repulsion is necessary to give the system a certain size or volume (density),\nwithout which the particles will collapse onto each other. A longer range attraction is\nalso necessary for the cohesion of the system, without which the particles will not stay\ntogether as they must in all condensed states of matter. Both are necessary for modeling\nthe physical properties of solids and liquids which we know from everyday experience.\n\nFig. 2. The Lennard-Jones interatomic potential V(r). The potential vanishes at r\nσ\n=\n\nand has a depth equal to ε .\nAlso shown is the corresponding force F(r) between the two particles (dashed curve)\nwhich vanishes at ro. At separations less or greater than ro the force is repulsive or\nattractive respectively. Arrow at nn and 2nn indicate typical separation distances of\nnearest and second nearest neighbors in a solid.\n\n3. Bookkeeping Matters\n\nOur simulation system is typically a cubical cell in which particles are placed either in a\nvery regular manner, as in modeling a crystal lattice, or in some random manner, as in\nmodeling a gas or liquid. The number of particles in the simulation cell is quite small.\nFor the homework assignment only certain discrete values, 32, 108, 256, 500, 864, should\nbe specified. These come about because the class code (which we call hailecode) is\ndesigned for a face-centered cubic lattice which has 4 atoms in each primitive cell. Thus,\nif our cube has s cells along each side, then the number of particles in the cube will be\n4s3. The above numbers then correspond to cubes with 2, 3, 4, 5, and 6 cells along each\nside respectively.\n\nOnce we choose the number of particles we want to simulate, the next step is to choose\nwhat system density we want to study. Choosing the density is equivalent to choosing\nthe system volume since density n = N/V, where N is the number of particles and V is the\n\nvolume. Hailecode uses dimensionless reduced units. The reduced density DR has\ntypical values around 0.9 - 1.2 for solids, and 0.6 - 0.85 for liquids. For reduced\ntemperature TR we recommend values of 0.4 - 0.8 for solids, and 0.8 - 1.3 for liquids.\nNotice that assigning particle velocities in (b) above is tantamount to setting the system\ntemperature.\n\nFor simulation of bulk systems (no free surfaces) it is conventional to use the periodic\nboundary condition (pbc). This means the cubical cell is surrounded by 26 identical\nimage cells. For every particle in the simulation cell, there corresponds an image particle\nin each image cell. The 26 image particles move in exactly the same manner as the actual\nparticle, so if the actual particle should happen to move out of the simulation, the image\nparticle in the image cell opposite to the exit side will move in (and becomes the actual\nparticle, or the particle in the simulation cell) just as the original particle moves out. The\nnet effect is with pbc particles cannot be lost (destroyed) or gained (created). In other\nwords, the particle number is conserved, and if the simulation cell volume is not allowed\nto change, the system density remains constant.\n\nSince in the pair potential approximation, the particles interact two at a time, a procedure\nis needed to decide which pair to consider among the pairs between actual particles and\nbetween actual and image particles. The minimum image convention is a procedure\nwhere one takes the nearest neighbor to an actual particle, irregardless of whether this\nneighbor is an actual particle or an image particle. Another approximation which is\nuseful to keep the computations to a manageable level is to introduce a force cutoff\ndistance beyond which particle pairs simply do not see each other (see the force curve in\nFig. 2). In order not to have a particle interact with its own image, it is necessary to\nensure that the cutoff distance is less than half of the simulation cell dimension.\n\nAnother bookkeeping device often used in MD simulation is a Neighbor List which\nkeeps track of who are the nearest, second nearest, ... neighbors of each particle. This is\nto save time from checking every particle in the system every time a force calculation is\nmade. The List can be used for several time steps before updating. Each update is\nexpensive since it involves NxN operations for an N-particle system. In low-temperature\nsolids where the particles do not move very much, it is possible to do an entire simulation\nwithout or with only a few updating, whereas in simulation of liquids, updating every 5\nor 10 steps is quite common. For further discussions of bookkeeping matters, the student\nshould see the MD Primer of J. M. Haile (1980).\n\n4. Properties Which Make MD Unique\n\nThere is a great deal that can be said about why MD is such a useful simulation\ntechnique. Perhaps the most important statement is that in this method (consider classical\nMD for the moment, as opposed quantum MD) one follows the atomic motions according\nto the principles of classical mechanics as formulated by Newton and Hamilton. Because\nof this, the results are physically as meaningful as the potential U that is used. One does\nnot have to apologize for any approximation in treating the N-body problem. Whatever\nmechanical, thermodynamic, and statistical mechanical properties that a system of N\n\nparticles should have, they are all still present in the simulation data. Of course how one\nextracts these properties from the output of the simulation - the atomic trajectories -\ndetermines how useful is the simulation. Before any conclusions can be drawn, one\nneeds to consider how the various properties are to be obtained from the simulation data.\nWe can regard MD simulation as an 'atomic video' of the particle motion (one which we\ncan display as a movie). While there is a great deal of realistic details in the motions\nthemselves, how to extract the information in a scientifically meaningful way is up to the\nviewer. It is to be expected that an experienced viewer can get much more useful\ninformation than an inexperienced one!\n\nThe above comments aside, we list here a number of basic reasons why MD simulation is\nso useful (or unique). These are meant to guide the thinking of the student and encourage\nthe student to discover and appreciate the many interesting and thought-provoking\naspects of this technique on your own.\n\n(a) Unified study of all physical properties. Using MD one can obtain thermodynamic,\nstructural, mechanical, dynamic and transport properties of a system of particles\nwhich can be a solid, liquid, or gas. One can even study chemical properties and\nreactions which are more difficult and will require using quantum MD.\n\n(b) Several hundred particles are sufficient to simulate bulk matter. While this is not\nalways true, it is rather surprising that one can get quite accurate thermodynamic\nproperties such as equation of state in this way. This is an example that the law of\nlarge numbers takes over quickly when one can average over several hundred degrees\nof freedom.\n\n(c) Direct link between potential model and physical properties. This is really useful\nfrom the standpoint of fundamental understanding of physical matter. It is also very\nrelevant to the structure-property correlation paradigm in materials science.\n\n(d) Complete control over input, initial and boundary conditions. This is what gives\nphysical insight into complex system behavior. This is also what makes simulation\nso useful when combined with experiment and theory.\n\n(e) Detailed atomic trajectories. This is what one can get from MD, or other atomistic\nsimulation techniques, that experiment often cannot provide. This point alone makes\nit compelling for the experimentalist to have access to simulation.\n\nWe should not leave this discussion without reminding ourselves that there are significant\nlimitations to MD. The two most important ones are:\n\n(a) Need for sufficiently realistic interatomic potential functions U. This is a matter of\nwhat we really know fundamentally about the chemical binding of the system we\nwant to study. Progress is being made in quantum and solid-state chemistry, and\ncondensed-matter physics; these advances will make MD more and more useful in\nunderstanding and predicting the properties and behavior of physical systems.\n\nThere exists a very useful writeup - A Primer on the Computer Simulation of Atomic\nFluids by Molecular Dynamics, J. M. Haile (1980), which is effectively a User's\nManual for the code. We will put a copy on reserve in the Reserve Library. See Dion if\nyou want to order a personal copy.\n\n5. Understanding Crystals and Liquids - An Example of an MD Application\n\nThere are many ways one can study the structure and dynamics of solids and liquids at\nthe atomistic level using MD. In fact, this is one of main reasons why MD has become so\nwell respected for what it can tell about the distribution of atoms and molecules in\nvarious states of matter, and the way they move about in response to thermal excitations\nor external stress such as pressure. We will encounter many examples of this kind of\ndiscussion, for now we will focus on two basic properties of matter, structure and motion\nat the atomic level. Fig. 3 shows the simulated trajectories of molecules as they move\naround in a typical solid, liquid, and gas. The most striking features that one can readily\nobserve are that the atomic structure is highly ordered in the solid (crystalline) state, quite\ndisordered (random but with some degree of local packing) in the liquid state, and very\nrandom in the gas state. The corresponding particle motions are small-amplitude\nvibrations about the lattice site, diffusive movements over a local region, and long free\nflights interrupted by a collision every now and then. The question we now ask is how to\nquantify this information in a way that it can be used to better understand the many\nphysical properties of systems of atoms and molecules. In other words, if we can\ngenerate the data, how do we analyze them?\n\n(b) Computational-capability constraints. No computers will ever be big enough and fast\nenough. On the other hand, things will keep on improving as far as we can tell. Current\nlimits on how big and how long are a billion atoms and about a microsecond in brute\nforce simulation.\n5 Hands-On MD\n'Talk is cheap' when it comes to modeling and simulation. What is not so easy is to 'just\ndo it'. In this spirit we want everyone to get your hands on an MD simulation code and\njust play with it.\n\nFig. 3. Atomic trajectories of a two-dimensional crystal, liquid, and gas simulated by\nmolecular dynamics [J. A. Barker and D. Henderson, Scientific American, Nov. 1981].\n\nImagine we are doing a simulation with the hailecode where we specify the following\ninput:\n\nNP = number of particles\nNEQ = number of timesteps for equilibration\nMAXKB = number of timesteps for the actual simulation run\nTR = reduced temperature\nDR = reduced density\n\nThe output of hailecode for this set of input parameters can be plotted in Matlab by\nfollowing the above instructions. What you will get are three plots. Fig. 4 is a composite\nof three graphs showing the variation of pressure, potential energy, and temperature with\ntime as the simulation evolves. This information is useful to make sure that the system is\nwell equilibrated and that nothing strange is happening during the entire simulation.\nThese graphs are like the meters on the wall of a reactor control room, showing how the\npressure and temperature of the reactor are varying instantaneously during operation.\nAlthough important, they do not tell us anything about what is going on with the atoms\ninside the reactor. That is, they are macroscopic properties which are not sensitive to the\ndetails at the microscopic level. The only way you can tell whether you are dealing with\na solid, liquid or gas is to examine the values of the equilibrium pressure, volume, and\ntemperature, and use them to locate the system in the phase diagram. We will not do this\nhere. Fig. 4, however, is useful for seeing that the system has reached equilibrium, as\nindicated by the convergence of the properties to steady-state values. The transients give\nus a feeling for how quickly the initial perturbations are damped out by the molecules\ninteracting with each other.\n\nFig. 4. Time variation of system pressure, energy, and temperature in an MD simulation\nof a solid. The initial behavior are transients which decay in time as the system reaches\nequilibrium.\n\nFig. 5. Results of MD simulation similar to Fig. 4 except the system is now in a liquid\nstate. Note the longer transients and the slower convergence to equilibrium.\n\nThe Radial Distribution Function\n\nThis quantity is defined as\n\n( )\n( ) /\ng r\nr\nρ\nρ\n=\n\n(9)\n\nwhere\n( )\nr\nρ\nis the local number density. For the hailecode, DR is the dimensionless\ndensity\nρσ . By the way, the dimensionless temperature TR is just\nB\n/\nk T ε , where\nBk is the\nBoltzmann's constant. Recall that\n,\nσ ε are the two parameters of the Lennard-Jones\npotential model. Hailecode calculates g(r) according to the expression\n\n(\n/ 2)\n( )\n(\n/ 2)\nN r\nr\ng r\nr\nr\nρ\n<\n± ∆\n>\n=\nΩ\n± ∆\n\n(10)\n\nwhere the numerator is the average number of particles in a spherical shell of radius r and\nthickness\n, with the shell centered on one of the particles (any particle is as good as any\nother) in the system, and\nin the denominator is the volume of this shell. A simple way\nto express the physical meaning of g(r) is:\nr\n∆\nΩ\n( )2\ng r\nr dr\nπ\n= number of particles lying in a\nspherical shell of radius r and thickness dr given that a particle is at the origin.\n\nWhat should g(r) look like if one plots it as a function of r? Two typical distributions are\nshown in Fig. 6. The function g(r) shows several peaks, which can be very sharp or quite\nbroad depending on the state of the system. Physically a peak indicates a particularly\nfavored separation distance for the neighbors to a given particle. The first peak\ncorresponds to the nearest neighbor shell, the second peak to the second nearest neighbor\nshell, etc. Thus, g(r) is the function that reveals the atomic structure of the system being\nsimulated.\n\nFig. 6. Comparison of radial distribution functions simulated by molecular dynamics for\na crystal (a) and a liquid (b) using the Lennard-Jones potential.\n\nIn the case of the output from hailecode we can even predict where the peaks should be\nlocated in the case of a low-temperature crystal. This is because the atoms are put into\nthe simulation cell in the positions of a face-centered cubic lattice. It is known that in the\nprimitive unit cell of fcc, one has 4 atoms in the cell. Sitting on any of the particles one\ncan look around the surroundings in the lattice and count up the number of nearest\nneighbor, second nearest neighbor, third neighbor, ..., as well as the distances between the\ncentral particle and its various neighbors. The numbers should be 12, 6, 24, and 12 for\nthe first four neighbors. Thus the four peaks in Fig. 6(a) correspond to these four\nneighbor rings. If one were to plot\ninstead of g(r), the areas under the four peaks\nwill also match the number of neighbors. Looking now at the g(r) for a liquid, Fig. 6(b),\nwe still see a prominent nearest neighbor peak which is considerably broadened by\nthermal motion at the higher temperature. We also see a broad second-neighbor at\ndistances where the third and fourth neighbor peaks occur in the solid. In changing from\nsolid to liquid, it appears that the second nearest peak has dissolved (implication is\nseparation from the central particle at this distance is no longer favored in the liquid\nenvironment), while the third and fourth neighbor peaks merge to form what is now the\nsecond neighbor peak in the liquid.\n(\nr g r\nπ\n)\nThe Square Displacement function\n2r\n< ∆\n>\n\nThis quantity, which we have already encountered in the lecture on random walk, is\ndefined as\n\n[\n( )\n(0)]\ni\ni\ni\nr\nr t\nr\nN\n< ∆\n>=\n-\n∑\n\n(11)\n\nHere\n( )\ni\nis the position of particle i at time t, so the square of the vector difference is the\ndistance that particle i has moved during the time interval t. If we average over all the\nparticles this then gives the mean square distance that the particles have moved on\naverage during time t.\nr t\nk T m\n\nBy definition\nmust start at zero at t = 0 and grows like t2 with a coefficient\nproportional to\nB\n, where m is the particle mass. Recall from Fig. 3 that all the atoms\nin a solid are bound to some local position. So for a solid we expect\n2r\n< ∆\n>\n/\n2r\n< ∆\n> to grow to a\ncharacteristic value, which should be determined by how tightly are the atoms being to\ntheir lattice site, and fluctuate weakly about this level. In contrast, if the system were a\nliquid, then we expect all the atoms to be able to diffuse continuously through the liquid,\nas in Brownian motion. Diffusive motion manifest in\n2r\n< ∆\n> as a linear variation with t,\nwhich is the signature of classical diffusion. These simple features of\ncan be quite\nhelpful when it comes to interpreting the simulation results.\n2r\n< ∆\n>\n\nFig. 7. Mean square displacement in a liquid (a) and a crsytal (b) as simulated by\nmolecular dynamics. The temperature and density conditions are those in Fig. 6.\n\nIn closing, we have given some hints here as to how one can learn about the structure and\ndynamics of systems of particles by doing MD simulation. We hope the students will\nexplore further on your own. You should play around with using different values for the\ninput parameters. Consider the following suggestions.\n\nNP = 32, 108, 256, 500, 864. Any one of the these values will work. Obviously a small\nsystem will run faster which means you get the results back right away if you use a 32-\nparticle simulation cell as opposed to an 864-particle cell. The latter generally takes less\nthan 5 minutes according to my experience.\n\nNEQ = 1000. Use this default value. We can talk about doing something different later.\n\nMAXKB = 2000 for a short run, 10000 for a longish run.\n\nTR = 0.4 - 0.7 for crystal, 0.9 - 1.2 for liquid\n\nDR = 0.9 - 1.2 for crystal, 0.6 - 0.9 for liquid\n\nYou should keep in mind that using more particles and running to longer times will give\nyou smoother and higher-quality results. The cost is that the simulation will take longer.\nIf you pick combinations of TR and DR values which are not in the part of the phase\ndiagram that is clearly solid or liquid, then the system may have a hard time deciding\nwhich phase it should go into. Then you can get results that are not as clearcut as the\nbehavior we havediscussed. There is much that you will learn by exploring on your own.\nHave fun and let us know how you are doing!"
    }
  ]
}